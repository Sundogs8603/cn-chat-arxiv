<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2312.15101</link><description>&lt;p&gt;
&#20462;&#22797;-Con&#65306;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#30340;&#33258;&#21160;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.15101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#27493;&#39588;&#65292;&#21487;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#27169;&#22411;&#22312;&#35774;&#22791;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#21033;&#29992;&#21487;&#33021;&#21482;&#22312;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#25552;&#20379;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36716;&#25442;&#36807;&#31243;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#65292;&#23548;&#33268;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#26080;&#27861;&#37096;&#32626;&#25110;&#23384;&#22312;&#38382;&#39064;&#65292;&#20005;&#37325;&#38477;&#20302;&#20102;&#20854;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;Fix-Con&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26102;&#20351;&#29992;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#22312;&#36716;&#25442;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#30340;&#25925;&#38556;&#12290;Fix-Con&#20351;&#29992;&#20174;&#35843;&#26597;&#36716;&#25442;&#38382;&#39064;&#20013;&#25366;&#25496;&#20986;&#30340;&#19968;&#32452;&#25925;&#38556;&#31867;&#22411;&#26469;&#23450;&#20301;&#36716;&#25442;&#27169;&#22411;&#20013;&#28508;&#22312;&#30340;&#36716;&#25442;&#25925;&#38556;&#65292;&#24182;&#36866;&#24403;&#20462;&#22797;&#23427;&#20204;&#65292;&#20363;&#22914;&#20351;&#29992;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26367;&#25442;&#30446;&#26631;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36825;&#19968;&#36807;&#31243;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#19978;&#36827;&#34892;&#36845;&#20195;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.   We propose an automated approach for fault localization and repair, Fix-Con, during model conversion between deep learning frameworks. Fix-Con is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion.   Fix-Con uses a set of fault types mined from surveying conversion issues raised to localize potential conversion faults in the converted target model, and then repairs them appropriately, e.g. replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the datas
&lt;/p&gt;</description></item><item><title>SLEDGE&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#20197;&#21450;Diffusion Transformer&#26469;&#29983;&#25104;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#25311;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.17933</link><description>&lt;p&gt;
SLEDGE: &#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#39550;&#39542;&#26234;&#33021;&#20307;&#30340;&#27169;&#25311;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17933
&lt;/p&gt;
&lt;p&gt;
SLEDGE&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#20197;&#21450;Diffusion Transformer&#26469;&#29983;&#25104;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#25311;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SLEDGE&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#35760;&#24405;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#12290;&#20854;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#26234;&#33021;&#20307;&#36793;&#30028;&#26694;&#21644;&#36710;&#36947;&#22270;&#12290;&#35813;&#27169;&#22411;&#30340;&#36755;&#20986;&#20316;&#20026;&#20132;&#36890;&#27169;&#25311;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;&#38024;&#23545;SLEDGE&#24453;&#29983;&#25104;&#30340;&#23454;&#20307;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#36830;&#25509;&#24615;&#21644;&#27599;&#20010;&#22330;&#26223;&#30340;&#21487;&#21464;&#25968;&#37327;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26420;&#32032;&#24212;&#29992;&#21464;&#24471;&#19981;&#31616;&#21333;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38500;&#20102;&#23545;&#29616;&#26377;&#36710;&#36947;&#22270;&#34920;&#31034;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#12290;&#23427;&#23558;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#32534;&#30721;&#20026;&#26629;&#26684;&#21270;&#28508;&#22312;&#26144;&#23556;&#20013;&#30340;&#19981;&#21516;&#36890;&#36947;&#12290;&#36825;&#26377;&#21161;&#20110;&#36710;&#36947;&#26465;&#20214;&#19979;&#30340;&#26234;&#33021;&#20307;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;&#25193;&#25955;&#21464;&#25442;&#22120;&#21516;&#26102;&#29983;&#25104;&#36710;&#36947;&#21644;&#26234;&#33021;&#20307;&#12290;&#22312;SLEDGE&#20013;&#20351;&#29992;&#29983;&#25104;&#30340;&#23454;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#27169;&#25311;&#65292;&#20363;&#22914;&#19978;&#37319;&#26679;&#36716;&#24367;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17933v1 Announce Type: cross  Abstract: SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE). It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPTIN&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#27425;&#24615;&#20462;&#21098;&#25216;&#26415;&#21644;&#20013;&#38388;&#29305;&#24449;&#33976;&#39311;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;Transformer&#26550;&#26500;&#30340;&#25928;&#29575;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17921</link><description>&lt;p&gt;
&#38656;&#35201;&#36895;&#24230;&#65306;&#29992;&#19968;&#31181;&#26041;&#27861;&#23545;Transformer&#36827;&#34892;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
The Need for Speed: Pruning Transformers with One Recipe
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17921
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OPTIN&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#27425;&#24615;&#20462;&#21098;&#25216;&#26415;&#21644;&#20013;&#38388;&#29305;&#24449;&#33976;&#39311;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;Transformer&#26550;&#26500;&#30340;&#25928;&#29575;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;$\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$)&#26694;&#26550;&#65292;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#30340;Transformer&#26550;&#26500;&#30340;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#25913;&#36827;Transformer&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#25110;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#26550;&#26500;&#29305;&#24449;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;OPTIN&#26694;&#26550;&#21033;&#29992;&#20013;&#38388;&#29305;&#24449;&#33976;&#39311;&#65292;&#25429;&#33719;&#27169;&#22411;&#21442;&#25968;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#65288;&#31216;&#20026;$\textit{trajectory}$&#65289;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#22270;&#20687;&#20998;&#31867;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#32473;&#23450;&#30340;FLOP&#32422;&#26463;&#19979;&#65292;OPTIN&#26694;&#26550;&#23558;&#21387;&#32553;&#32593;&#32476;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17921v1 Announce Type: new  Abstract: We introduce the $\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained transformer architectures $\textit{without requiring re-training}$. Recent works have explored improving transformer efficiency, however often incur computationally expensive re-training procedures or depend on architecture-specific characteristics, thus impeding practical wide-scale adoption. To address these shortcomings, the OPTIN framework leverages intermediate feature distillation, capturing the long-range dependencies of model parameters (coined $\textit{trajectory}$), to produce state-of-the-art results on natural language, image classification, transfer learning, and semantic segmentation tasks $\textit{without re-training}$. Given a FLOP constraint, the OPTIN framework will compress the network while maintaining competitiv
&lt;/p&gt;</description></item><item><title>&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17919</link><description>&lt;p&gt;
LISA&#65306;&#29992;&#20110;&#39640;&#25928;&#20869;&#23384;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17919
&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35832;&#22914;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#20043;&#31867;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#26080;&#27861;&#19982;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#30456;&#21305;&#37197;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#30340;&#36880;&#23618;&#29305;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#23618;&#20043;&#38388;&#26435;&#37325;&#33539;&#25968;&#30340;&#24322;&#24120;&#20559;&#26012;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#31616;&#21333;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35760;&#24518;&#25104;&#26412;&#20302;&#20110;LoRA&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#20248;&#20110;LoRA&#21644;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;Layerwise Importance Sampled AdamW&#65288;LISA&#65289;&#65292;&#36825;&#26159;LoRA&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17916</link><description>&lt;p&gt;
CMP&#65306;&#20855;&#26377;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#30340;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CMP: Cooperative Motion Prediction with Multi-Agent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#21457;&#23637;&#21644;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#30340;&#25104;&#29087;&#65292;&#21512;&#20316;&#36830;&#25509;&#30340;&#33258;&#21160;&#21270;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#21151;&#33021;&#21464;&#24471;&#21487;&#33021;&#12290;&#26412;&#25991;&#22522;&#20110;&#21512;&#20316;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CMP&#20197;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#22686;&#24378;&#36319;&#36394;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#19982;&#36807;&#21435;&#19987;&#27880;&#20110;&#21512;&#20316;&#24863;&#30693;&#25110;&#36816;&#21160;&#39044;&#27979;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#35299;&#20915;CAVs&#22312;&#24863;&#30693;&#21644;&#39044;&#27979;&#27169;&#22359;&#20013;&#20849;&#20139;&#20449;&#24687;&#30340;&#32479;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20013;&#36824;&#34701;&#20837;&#20102;&#33021;&#22815;&#23481;&#24525;&#29616;&#23454;V2X&#24102;&#23485;&#38480;&#21046;&#21644;&#20256;&#36755;&#24310;&#36831;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#22788;&#29702;&#24222;&#22823;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#39044;&#27979;&#32858;&#21512;&#27169;&#22359;&#65292;&#32479;&#19968;&#20102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;R2D2&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17905</link><description>&lt;p&gt;
&#20855;&#26377;R2D2&#30340;&#21487;&#25193;&#23637;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#25104;&#20687;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17905
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;R2D2&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#31515;&#21345;&#23572;&#30913;&#20849;&#25391;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#22312;&#22825;&#25991;&#25104;&#20687;&#20013;&#24341;&#20837;&#30340;&#8220;&#29992;&#20110;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#32423;&#32852;DNN&#31995;&#21015;&#65288;R2D2&#65289;&#8221;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;R2D2&#30340;&#37325;&#24314;&#34987;&#24418;&#25104;&#20026;&#27531;&#24046;&#22270;&#20687;&#30340;&#31995;&#21015;&#65292;&#34987;&#36845;&#20195;&#22320;&#20272;&#35745;&#20026;&#25509;&#21463;&#19978;&#19968;&#27425;&#36845;&#20195;&#30340;&#22270;&#20687;&#20272;&#35745;&#21644;&#30456;&#20851;&#25968;&#25454;&#27531;&#24046;&#20316;&#20026;&#36755;&#20837;&#30340;DNN&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17905v1 Announce Type: cross  Abstract: We propose a new approach for non-Cartesian magnetic resonance image reconstruction. While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability. To address this scalability challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)" approach recently introduced in astronomical imaging. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration's image estimate and associated data residual as inputs. The method can be interpreted as a learned version of the Matching Pursuit algo
&lt;/p&gt;</description></item><item><title>Serpent&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#24674;&#22797;&#26550;&#26500;&#65292;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#20840;&#23616;&#24863;&#21463;&#37326;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#20294;&#35745;&#31639;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2403.17902</link><description>&lt;p&gt;
Serpent&#65306;&#36890;&#36807;&#22810;&#23610;&#24230;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#23454;&#29616;&#21487;&#25193;&#23637;&#39640;&#25928;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17902
&lt;/p&gt;
&lt;p&gt;
Serpent&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#24674;&#22797;&#26550;&#26500;&#65292;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#20840;&#23616;&#24863;&#21463;&#37326;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#20294;&#35745;&#31639;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22270;&#20687;&#24674;&#22797;&#26550;&#26500;&#30340;&#35745;&#31639;&#24314;&#31569;&#22359;&#39046;&#22495;&#65292;&#20027;&#35201;&#30001;&#21367;&#31215;&#22788;&#29702;&#21644;&#21508;&#31181;&#27880;&#24847;&#26426;&#21046;&#30340;&#32452;&#21512;&#25152;&#20027;&#23548;&#12290;&#28982;&#32780;&#65292;&#21367;&#31215;&#28388;&#27874;&#22120;&#26412;&#36136;&#19978;&#26159;&#23616;&#37096;&#30340;&#65292;&#22240;&#27492;&#22312;&#24314;&#27169;&#22270;&#20687;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#27880;&#24847;&#26426;&#21046;&#25797;&#38271;&#25429;&#33719;&#20219;&#24847;&#22270;&#20687;&#21306;&#22495;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20114;&#20316;&#29992;&#65292;&#20294;&#23545;&#22270;&#20687;&#23610;&#23544;&#30340;&#20108;&#27425;&#25104;&#26412;&#36739;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Serpent&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#22312;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#20316;&#20026;&#20854;&#26680;&#24515;&#35745;&#31639;&#27169;&#22359;&#30340;&#26550;&#26500;&#12290;SSMs&#26368;&#21021;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#65292;&#21487;&#20197;&#36890;&#36807;&#26377;&#21033;&#30340;&#36755;&#20837;&#23610;&#23544;&#30340;&#32447;&#24615;&#32553;&#25918;&#26469;&#32500;&#25345;&#20840;&#23616;&#24863;&#21463;&#37326;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;Serpent&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#21516;&#26102;&#38656;&#35201;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#37327;&#36739;&#23569;&#65288;&#22312;FLOPS&#19978;&#39640;&#36798;150&#20493;&#30340;&#20943;&#23569;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17902v1 Announce Type: cross  Abstract: The landscape of computational building blocks of efficient image restoration architectures is dominated by a combination of convolutional processing and various attention mechanisms. However, convolutional filters are inherently local and therefore struggle at modeling long-range dependencies in images. On the other hand, attention excels at capturing global interactions between arbitrary image regions, however at a quadratic cost in image dimension. In this work, we propose Serpent, an architecture that leverages recent advances in state space models (SSMs) in its core computational block. SSMs, originally introduced for sequence modeling, can maintain a global receptive field with a favorable linear scaling in input size. Our preliminary results demonstrate that Serpent can achieve reconstruction quality on par with state-of-the-art techniques, while requiring orders of magnitude less compute (up to $150$ fold reduction in FLOPS) an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#21033;&#29992;&#20998;&#23618;&#26631;&#31614;&#25552;&#39640;&#26410;&#30693;&#25925;&#38556;&#26816;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#28909;&#36711;&#38050;&#36807;&#31243;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17891</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#22522;&#20110;&#22270;&#20687;&#30340;&#26032;&#22411;&#25925;&#38556;&#26816;&#27979;&#65292;&#20351;&#29992;&#20998;&#23618;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Image-based Novel Fault Detection with Deep Learning Classifiers using Hierarchical Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#21033;&#29992;&#20998;&#23618;&#26631;&#31614;&#25552;&#39640;&#26410;&#30693;&#25925;&#38556;&#26816;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#28909;&#36711;&#38050;&#36807;&#31243;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#36739;&#22909;&#30340;&#21487;&#22797;&#21046;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25925;&#38556;&#20998;&#31867;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#26159;&#22312;&#38754;&#23545;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#25925;&#38556;&#31867;&#22411;&#26102;&#33021;&#22815;&#26631;&#35760;&#31995;&#32479;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25925;&#38556;&#20998;&#31867;&#22120;&#30340;&#26410;&#30693;&#25925;&#38556;&#26816;&#27979;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#22312;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;&#26377;&#20851;&#25925;&#38556;&#20998;&#31867;&#27861;&#30340;&#26631;&#31614;&#26469;&#25552;&#39640;&#26410;&#30693;&#25925;&#38556;&#26816;&#27979;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#36719;&#26631;&#31614;&#25216;&#26415;&#26469;&#25913;&#21892;&#29616;&#26377;&#30340;&#28145;&#24230;&#26032;&#22411;&#25925;&#38556;&#26816;&#27979;&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20197;&#21450;&#29992;&#20110;&#22312;&#32447;&#26032;&#22411;&#25925;&#38556;&#26816;&#27979;&#30340;&#23618;&#27425;&#19968;&#33268;&#26816;&#27979;&#32479;&#35745;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28909;&#36711;&#38050;&#36807;&#31243;&#20013;&#26816;&#26597;&#22270;&#20687;&#20013;&#26032;&#22411;&#25925;&#38556;&#26816;&#27979;&#30340;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#65292;&#32467;&#26524;&#22312;&#22810;&#31181;&#24773;&#20917;&#21644;&#22522;&#32447;&#26816;&#27979;&#26041;&#27861;&#20013;&#37117;&#26377;&#24456;&#22909;&#30340;&#37325;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17891v1 Announce Type: cross  Abstract: One important characteristic of modern fault classification systems is the ability to flag the system when faced with previously unseen fault types. This work considers the unknown fault detection capabilities of deep neural network-based fault classifiers. Specifically, we propose a methodology on how, when available, labels regarding the fault taxonomy can be used to increase unknown fault detection performance without sacrificing model performance. To achieve this, we propose to utilize soft label techniques to improve the state-of-the-art deep novel fault detection techniques during the training process and novel hierarchically consistent detection statistics for online novel fault detection. Finally, we demonstrated increased detection performance on novel fault detection in inspection images from the hot steel rolling process, with results well replicated across multiple scenarios and baseline detection methods.
&lt;/p&gt;</description></item><item><title>IgBert&#21644;IgT5&#26159;&#36804;&#20170;&#20026;&#27490;&#21457;&#23637;&#30340;&#26368;&#20339;&#34920;&#29616;&#30340;&#25239;&#20307;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#37197;&#23545;&#21644;&#26080;&#37197;&#23545;&#21487;&#21464;&#21306;&#24207;&#21015;&#65292;&#24182;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#20102;&#36229;&#36807;20&#20159;&#20010;&#26080;&#37197;&#23545;&#24207;&#21015;&#21644;&#20004;&#30334;&#19975;&#20010;&#37197;&#23545;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.17889</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#37197;&#23545;&#25239;&#20307;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large scale paired antibody language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17889
&lt;/p&gt;
&lt;p&gt;
IgBert&#21644;IgT5&#26159;&#36804;&#20170;&#20026;&#27490;&#21457;&#23637;&#30340;&#26368;&#20339;&#34920;&#29616;&#30340;&#25239;&#20307;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#37197;&#23545;&#21644;&#26080;&#37197;&#23545;&#21487;&#21464;&#21306;&#24207;&#21015;&#65292;&#24182;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#20102;&#36229;&#36807;20&#20159;&#20010;&#26080;&#37197;&#23545;&#24207;&#21015;&#21644;&#20004;&#30334;&#19975;&#20010;&#37197;&#23545;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#34507;&#30333;&#36136;&#65292;&#21487;&#20197;&#35782;&#21035;&#21644;&#20013;&#21644;&#21508;&#31181;&#25239;&#21407;&#65292;&#20855;&#26377;&#39640;&#29305;&#24322;&#24615;&#21644;&#20146;&#21644;&#21147;&#65292;&#24182;&#26500;&#25104;&#26368;&#25104;&#21151;&#30340;&#29983;&#29289;&#27835;&#30103;&#31867;&#21035;&#12290; &#38543;&#30528;&#19979;&#19968;&#20195;&#27979;&#24207;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#36817;&#24180;&#26469;&#24050;&#25910;&#38598;&#20102;&#25968;&#21313;&#20159;&#20010;&#25239;&#20307;&#24207;&#21015;&#65292;&#23613;&#31649;&#36825;&#20123;&#24207;&#21015;&#22312;&#35774;&#35745;&#26356;&#22909;&#30340;&#27835;&#30103;&#26041;&#26696;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#25968;&#25454;&#37327;&#21644;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36804;&#20170;&#24320;&#21457;&#30340;&#34920;&#29616;&#26368;&#20339;&#30340;&#25239;&#20307;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;IgBert&#21644;IgT5&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#21487;&#20197;&#25345;&#32493;&#22788;&#29702;&#20316;&#20026;&#36755;&#20837;&#30340;&#37197;&#23545;&#21644;&#26080;&#37197;&#23545;&#21487;&#21464;&#21306;&#24207;&#21015;&#12290; &#36825;&#20123;&#27169;&#22411;&#20840;&#38754;&#22320;&#20351;&#29992;&#20102;&#8220;&#35266;&#27979;&#21040;&#30340;&#25239;&#20307;&#31354;&#38388;&#8221;&#25968;&#25454;&#38598;&#20013;&#30340;&#36229;&#36807;20&#20159;&#20010;&#26080;&#37197;&#23545;&#24207;&#21015;&#21644;&#20004;&#30334;&#19975;&#20010;&#36731;&#38142;&#21644;&#37325;&#38142;&#30340;&#37197;&#23545;&#24207;&#21015;&#36827;&#34892;&#22521;&#35757;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#29616;&#26377;&#30340;&#25239;&#20307;&#21644;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17889v1 Announce Type: cross  Abstract: Antibodies are proteins produced by the immune system that can identify and neutralise a wide variety of antigens with high specificity and affinity, and constitute the most successful class of biotherapeutics. With the advent of next-generation sequencing, billions of antibody sequences have been collected in recent years, though their application in the design of better therapeutics has been constrained by the sheer volume and complexity of the data. To address this challenge, we present IgBert and IgT5, the best performing antibody-specific language models developed to date which can consistently handle both paired and unpaired variable region sequences as input. These models are trained comprehensively using the more than two billion unpaired sequences and two million paired sequences of light and heavy chains present in the Observed Antibody Space dataset. We show that our models outperform existing antibody and protein language m
&lt;/p&gt;</description></item><item><title>&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#22823;&#37096;&#20998;&#23618;&#30340;&#31227;&#38500;&#32780;&#20445;&#25345;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.17887</link><description>&lt;p&gt;
&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#23618;&#21098;&#26525;&#30340;&#19981;&#21512;&#29702;&#26080;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Ineffectiveness of the Deeper Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17887
&lt;/p&gt;
&lt;p&gt;
&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#22823;&#37096;&#20998;&#23618;&#30340;&#31227;&#38500;&#32780;&#20445;&#25345;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#31616;&#21333;&#30340;&#23618;&#21098;&#26525;&#31574;&#30053;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#31227;&#38500;&#22823;&#37096;&#20998;&#23618;&#65288;&#26368;&#39640;&#36798;&#19968;&#21322;&#65289;&#20043;&#21069;&#65292;&#19981;&#21516;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#21098;&#26525;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#23618;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#21098;&#26525;&#23618;&#22359;&#65307;&#28982;&#21518;&#65292;&#20026;&#20102;&#8220;&#20462;&#22797;&#8221;&#25439;&#23475;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23569;&#37327;&#24494;&#35843;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#20855;&#20307;&#21253;&#25324;&#37327;&#21270;&#21644;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;QLoRA&#65289;&#65292;&#36825;&#26679;&#25105;&#20204;&#30340;&#27599;&#20010;&#23454;&#39564;&#37117;&#21487;&#20197;&#22312;&#21333;&#20010;A100 GPU&#19978;&#25191;&#34892;&#12290;&#20174;&#23454;&#38469;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#34917;&#20805;&#20854;&#20182;PEFT&#31574;&#30053;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20943;&#23569;&#24494;&#35843;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21478;&#19968;&#26041;&#38754;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;&#20174;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#30340;&#21098;&#26525;&#27809;&#26377;&#22826;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17887v1 Announce Type: new  Abstract: We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to "heal" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#30340;&#22810;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#21387;&#32553;&#29575;&#19982;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21462;&#24471;&#20102;&#25968;&#25454;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17886</link><description>&lt;p&gt;
&#21387;&#32553;&#22810;&#20219;&#21153;&#23884;&#20837;&#29992;&#20110;&#22320;&#29699;&#35266;&#27979;&#20013;&#25968;&#25454;&#39640;&#25928;&#19979;&#28216;&#35757;&#32451;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17886
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#30340;&#22810;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#21387;&#32553;&#29575;&#19982;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21462;&#24471;&#20102;&#25968;&#25454;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#20013;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#23384;&#20648;&#24211;&#22686;&#38271;&#65292;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#36716;&#31227;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#65292;&#28040;&#32791;&#20102;&#22823;&#37327;&#36164;&#28304;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#65288;NEC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#23545;&#25968;&#25454;&#20351;&#29992;&#32773;&#20256;&#36755;&#21387;&#32553;&#30340;&#23884;&#20837;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#31070;&#32463;&#21387;&#32553;&#26469;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#65292;&#29983;&#25104;&#22810;&#20219;&#21153;&#23884;&#20837;&#65292;&#21516;&#26102;&#22312;&#21387;&#32553;&#29575;&#21644;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#20165;&#38024;&#23545;FM&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;10%&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#36827;&#34892;&#30701;&#26102;&#38388;&#35757;&#32451;&#65288;&#39044;&#35757;&#32451;&#36845;&#20195;&#30340;1%&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;EO&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;NEC&#65306;&#22330;&#26223;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;&#19982;&#23558;&#20256;&#32479;&#21387;&#32553;&#24212;&#29992;&#20110;&#21407;&#22987;&#25968;&#25454;&#30456;&#27604;&#65292;NEC&#22312;&#20943;&#23569;&#25968;&#25454;&#37327;&#26041;&#38754;&#21487;&#23454;&#29616;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#20102;75%&#21040;90%&#30340;&#25968;&#25454;&#37327;&#12290;&#21363;&#20351;&#22312;99.7%&#30340;&#21387;&#32553;&#19979;&#65292;&#22312;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#19978;&#24615;&#33021;&#20165;&#19979;&#38477;&#20102;5%&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;NEC&#26159;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17886v1 Announce Type: new  Abstract: As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#32593;&#26684;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#25968;&#25454;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25152;&#26377;&#26435;&#20998;&#24067;&#21040;&#27599;&#20010;&#25968;&#25454;&#39046;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#32852;&#21512;&#27835;&#29702;&#65292;&#20197;&#20811;&#26381;&#25968;&#25454;&#28304;&#28608;&#22686;&#21644;&#21450;&#26102;&#20998;&#26512;&#22788;&#29702;&#38656;&#27714;&#22686;&#38271;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.17878</link><description>&lt;p&gt;
&#29992;&#32852;&#37030;&#23398;&#20064;&#22686;&#24378;&#25968;&#25454;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Empowering Data Mesh with Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17878
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32593;&#26684;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#25968;&#25454;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25152;&#26377;&#26435;&#20998;&#24067;&#21040;&#27599;&#20010;&#25968;&#25454;&#39046;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#32852;&#21512;&#27835;&#29702;&#65292;&#20197;&#20811;&#26381;&#25968;&#25454;&#28304;&#28608;&#22686;&#21644;&#21450;&#26102;&#20998;&#26512;&#22788;&#29702;&#38656;&#27714;&#22686;&#38271;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26550;&#26500;&#30340;&#28436;&#21464;&#35265;&#35777;&#20102;&#25968;&#25454;&#28246;&#30340;&#20852;&#36215;&#65292;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#31649;&#29702;&#30340;&#29942;&#39048;&#24182;&#25512;&#21160;&#26234;&#33021;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38598;&#20013;&#21270;&#26550;&#26500;&#21463;&#21046;&#20110;&#25968;&#25454;&#28304;&#30340;&#28608;&#22686;&#21644;&#23545;&#21450;&#26102;&#20998;&#26512;&#22788;&#29702;&#30340;&#26085;&#30410;&#22686;&#38271;&#38656;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#33539;&#24335;&#65292;&#25968;&#25454;&#32593;&#26684;&#12290;&#25968;&#25454;&#32593;&#26684;&#23558;&#39046;&#22495;&#35270;&#20026;&#39318;&#35201;&#20851;&#27880;&#28857;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25152;&#26377;&#26435;&#20174;&#20013;&#22830;&#22242;&#38431;&#20998;&#21457;&#21040;&#27599;&#20010;&#25968;&#25454;&#39046;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#32852;&#21512;&#27835;&#29702;&#26469;&#30417;&#25511;&#39046;&#22495;&#21450;&#20854;&#25968;&#25454;&#20135;&#21697;&#12290;&#20687;Paypal&#12289;Netflix&#21644;Zalando&#31561;&#35768;&#22810;&#20159;&#32654;&#20803;&#32452;&#32455;&#24050;&#32463;&#22522;&#20110;&#36825;&#31181;&#26032;&#26550;&#26500;&#36716;&#21464;&#20102;&#20182;&#20204;&#30340;&#25968;&#25454;&#20998;&#26512;&#27969;&#31243;&#12290;&#22312;&#36825;&#31181;&#21435;&#20013;&#24515;&#21270;&#26550;&#26500;&#20013;&#65292;&#25968;&#25454;&#30001;&#27599;&#20010;&#39046;&#22495;&#22242;&#38431;&#26412;&#22320;&#20445;&#23384;&#65292;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#26080;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#26377;&#25928;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17878v1 Announce Type: new  Abstract: The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24471;&#20986;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#20013;&#30340;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#21453;&#38169;&#35823;&#27010;&#29575;&#30340;&#23545;&#25968;&#21644;&#20445;&#30495;&#24230;&#30340;&#36127;&#23545;&#25968;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.17868</link><description>&lt;p&gt;
&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sample complexity of quantum hypothesis testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24471;&#20986;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#20013;&#30340;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#21453;&#38169;&#35823;&#27010;&#29575;&#30340;&#23545;&#25968;&#21644;&#20445;&#30495;&#24230;&#30340;&#36127;&#23545;&#25968;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#30740;&#31350;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#20204;&#23545;&#38169;&#35823;&#27010;&#29575;&#30340;&#26368;&#20248;&#34928;&#20943;&#36895;&#29575;&#24863;&#20852;&#36259;&#65292;&#36825;&#20010;&#36895;&#29575;&#26159;&#26410;&#30693;&#29366;&#24577;&#30340;&#26679;&#26412;&#25968;&#37327;&#20989;&#25968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#26088;&#22312;&#30830;&#23450;&#36798;&#21040;&#25152;&#38656;&#38169;&#35823;&#27010;&#29575;&#25152;&#38656;&#30340;&#26368;&#23569;&#26679;&#26412;&#25968;&#37327;&#12290;&#36890;&#36807;&#21033;&#29992;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#35774;&#32622;&#20013;&#30340;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#12290;&#26356;&#35814;&#32454;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#31216;&#20108;&#36827;&#21046;&#37327;&#23376;&#20551;&#35774;&#26816;&#39564;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23545;&#21453;&#38169;&#35823;&#27010;&#29575;&#30340;&#23545;&#25968;&#21644;&#20445;&#30495;&#24230;&#30340;&#36127;&#23545;&#25968;&#30340;&#23545;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17868v1 Announce Type: cross  Abstract: Quantum hypothesis testing has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state. In this paper, we study the sample complexity of quantum hypothesis testing, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability. By making use of the wealth of knowledge that already exists in the literature on quantum hypothesis testing, we characterize the sample complexity of binary quantum hypothesis testing in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple quantum hypothesis testing. In more detail, we prove that the sample complexity of symmetric binary quantum hypothesis testing depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#29983;&#25104;&#31070;&#32463;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI)&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#23545;&#35805;&#32467;&#26500;&#35782;&#21035;&#65292;&#24212;&#23545;&#35757;&#32451;&#35821;&#26009;&#26377;&#38480;/&#22024;&#26434;&#20197;&#21450;&#27979;&#35797;&#23545;&#35805;&#39046;&#22495;&#20998;&#24067;&#36716;&#21464;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17853</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#36890;&#36807;&#31070;&#32463;&#27010;&#29575;&#36719;&#36923;&#36753;&#24341;&#23548;&#23545;&#35805;&#32467;&#26500;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17853
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#29983;&#25104;&#31070;&#32463;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25552;&#20986;&#20102;Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI)&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#23545;&#35805;&#32467;&#26500;&#35782;&#21035;&#65292;&#24212;&#23545;&#35757;&#32451;&#35821;&#26009;&#26377;&#38480;/&#22024;&#26434;&#20197;&#21450;&#27979;&#35797;&#23545;&#35805;&#39046;&#22495;&#20998;&#24067;&#36716;&#21464;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#32467;&#26500;&#35782;&#21035;&#65288;DSI&#65289;&#26159;&#25512;&#26029;&#32473;&#23450;&#30446;&#26631;&#23548;&#21521;&#23545;&#35805;&#30340;&#28508;&#22312;&#23545;&#35805;&#32467;&#26500;&#65288;&#21363;&#19968;&#32452;&#23545;&#35805;&#29366;&#24577;&#21450;&#20854;&#26102;&#38388;&#36716;&#25442;&#65289;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#29616;&#20195;&#23545;&#35805;&#31995;&#32479;&#35774;&#35745;&#21644;&#35805;&#35821;&#20998;&#26512;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#20316;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#27010;&#29575;&#36719;&#36923;&#36753;&#23545;&#35805;&#32467;&#26500;&#35782;&#21035;&#65288;NEUPSL DSI&#65289;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#31526;&#21495;&#30693;&#35782;&#27880;&#20837;&#29983;&#25104;&#31070;&#32463;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;NEUPSL DSI&#23398;&#20064;&#23545;&#38544;&#34255;&#34920;&#31034;&#36136;&#37327;&#30340;&#24433;&#21709;&#30340;&#24443;&#24213;&#23454;&#35777;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17853v1 Announce Type: new  Abstract: Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17852</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#19982;&#20559;&#35265;&#27491;&#20132;&#30340;&#26041;&#24335;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Fairness through Transforming Data Orthogonal to Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17852
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#30340;&#22797;&#26434;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#21487;&#33021;&#34920;&#29616;&#20986;&#26377;&#20559;&#35265;&#30340;&#20915;&#31574;&#65292;&#23548;&#33268;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#24453;&#36935;&#19981;&#24179;&#31561;&#12290;&#23613;&#31649;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#24191;&#27867;&#65292;&#20294;&#22810;&#20803;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#23545;&#20915;&#31574;&#32467;&#26524;&#30340;&#24494;&#22937;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#21363;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#26088;&#22312;&#28040;&#38500;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#20013;&#32852;&#21512;&#27491;&#24577;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#21363;&#21487;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;OB&#31639;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17852v1 Announce Type: new  Abstract: Machine learning models have shown exceptional prowess in solving complex issues across various domains. Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups. Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied. We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications. Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables. The OB algorithm is model-agnostic, catering to a wide array of machine 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20505;&#32454;&#21270;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#22359;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#38477;&#27700;&#25968;&#25454;&#65292;&#20026;&#32531;&#35299;&#27668;&#20505;&#21464;&#21270;&#24102;&#26469;&#30340;&#24433;&#21709;&#21644;&#25552;&#39640;&#27700;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.17847</link><description>&lt;p&gt;
&#27668;&#20505;&#32454;&#21270;&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24102;&#26377;&#27880;&#24847;&#21147;&#22359;&#21644;&#36339;&#36291;&#36830;&#25509;&#30340;&#38477;&#27700;&#25968;&#25454;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20505;&#32454;&#21270;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#22359;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#26088;&#22312;&#26377;&#25928;&#39044;&#27979;&#38477;&#27700;&#25968;&#25454;&#65292;&#20026;&#32531;&#35299;&#27668;&#20505;&#21464;&#21270;&#24102;&#26469;&#30340;&#24433;&#21709;&#21644;&#25552;&#39640;&#27700;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#21152;&#36895;&#20102;&#21270;&#30707;&#29123;&#26009;&#30340;&#28040;&#32791;&#24182;&#20135;&#29983;&#20102;&#28201;&#23460;&#27668;&#20307;&#65292;&#23548;&#33268;&#20102;&#24403;&#20170;&#36843;&#22312;&#30473;&#30571;&#30340;&#38382;&#39064;&#65306;&#20840;&#29699;&#21464;&#26262;&#21644;&#27668;&#20505;&#21464;&#21270;&#12290;&#36825;&#20123;&#38388;&#25509;&#36896;&#25104;&#20005;&#37325;&#30340;&#33258;&#28982;&#28798;&#23475;&#65292;&#35768;&#22810;&#29983;&#21629;&#21463;&#33510;&#20197;&#21450;&#20892;&#19994;&#36130;&#20135;&#30340;&#24040;&#22823;&#25439;&#22833;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#25105;&#20204;&#22303;&#22320;&#30340;&#24433;&#21709;&#65292;&#31185;&#23398;&#23478;&#20204;&#27491;&#22312;&#24320;&#21457;&#21487;&#20877;&#29983;&#12289;&#21487;&#37325;&#22797;&#20351;&#29992;&#21644;&#28165;&#27905;&#33021;&#28304;&#65292;&#27668;&#20505;&#23398;&#23478;&#27491;&#35797;&#22270;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#12290;&#21516;&#26102;&#65292;&#21508;&#22269;&#25919;&#24220;&#27491;&#22312;&#20844;&#24067;&#33410;&#32422;&#36164;&#28304;&#30340;&#25919;&#31574;&#65292;&#20197;&#24314;&#31435;&#26356;&#29615;&#20445;&#30340;&#31038;&#20250;&#24182;&#21796;&#36215;&#29615;&#22659;&#24847;&#35782;&#12290;&#20854;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#22240;&#32032;&#20043;&#19968;&#23601;&#26159;&#38477;&#27700;&#65292;&#23558;&#20957;&#32467;&#30340;&#27700;&#33976;&#27668;&#24102;&#21040;&#22303;&#22320;&#19978;&#12290;&#27700;&#36164;&#28304;&#26159;&#31038;&#20250;&#20013;&#26368;&#37325;&#35201;&#20294;&#22522;&#26412;&#30340;&#38656;&#27714;&#65292;&#19981;&#20165;&#25903;&#25345;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#20063;&#25903;&#25345;&#32463;&#27982;&#12290;&#22312;&#21488;&#28286;&#65292;&#23613;&#31649;&#24179;&#22343;&#24180;&#38477;&#27700;&#37327;&#39640;&#36798;2500&#27627;&#31859;&#65292;&#20294;&#27599;&#20154;&#30340;&#27700;&#20998;&#37197;&#37327;&#20302;&#20110;&#20840;&#29699;&#24179;&#22343;&#27700;&#24179;&#65292;&#36825;&#26159;&#30001;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17847v1 Announce Type: cross  Abstract: Human activities accelerate consumption of fossil fuels and produce greenhouse gases, resulting in urgent issues today: global warming and the climate change. These indirectly cause severe natural disasters, plenty of lives suffering and huge losses of agricultural properties. To mitigate impacts on our lands, scientists are developing renewable, reusable, and clean energies and climatologists are trying to predict the extremes. Meanwhile, governments are publicizing resource-saving policies for a more eco-friendly society and arousing environment awareness. One of the most influencing factors is the precipitation, bringing condensed water vapor onto lands. Water resources are the most significant but basic needs in society, not only supporting our livings, but also economics. In Taiwan, although the average annual precipitation is up to 2,500 millimeter (mm), the water allocation for each person is lower than the global average due to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.17846</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20195;&#34920;&#22810;&#23618;&#24314;&#31569;&#24182;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#20854;&#20013;&#31359;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24320;&#25918;&#35789;&#27719;&#26426;&#22120;&#20154;&#26144;&#23556;&#26041;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#20016;&#23500;&#20102;&#23494;&#38598;&#20960;&#20309;&#22320;&#22270;&#12290;&#34429;&#28982;&#36825;&#20123;&#22320;&#22270;&#20801;&#35768;&#22312;&#26597;&#35810;&#26576;&#31181;&#35821;&#35328;&#27010;&#24565;&#26102;&#39044;&#27979;&#36880;&#28857;&#26174;&#33879;&#24615;&#22320;&#22270;&#65292;&#20294;&#22823;&#35268;&#27169;&#29615;&#22659;&#21644;&#36229;&#20986;&#23545;&#35937;&#32423;&#21035;&#30340;&#25277;&#35937;&#26597;&#35810;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#65292;&#26368;&#32456;&#38480;&#21046;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOV-SG&#65292;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20998;&#23618;&#24320;&#25918;&#35789;&#27719;3D&#22330;&#26223;&#22270;&#26144;&#23556;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;3D&#31354;&#38388;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#27573;&#32423;&#22320;&#22270;&#65292;&#28982;&#21518;&#26500;&#24314;&#20102;&#30001;&#22320;&#26495;&#12289;&#25151;&#38388;&#21644;&#23545;&#35937;&#27010;&#24565;&#32452;&#25104;&#30340;3D&#22330;&#26223;&#22270;&#23618;&#27425;&#32467;&#26500;&#65292;&#27599;&#20010;&#37117;&#21253;&#21547;&#24320;&#25918;&#24615;&#35789;&#27719;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#34920;&#31034;&#22810;&#23618;&#24314;&#31569;&#65292;&#24182;&#19988;&#20801;&#35768;&#26426;&#22120;&#20154;&#20351;&#29992;&#36328;&#23618;Voronoi&#22270;&#31359;&#36234;&#36825;&#20123;&#24314;&#31569;&#12290;HOV-SG&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17846v1 Announce Type: cross  Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluat
&lt;/p&gt;</description></item><item><title>TractOracle &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RL&#32420;&#32500;&#26463;&#36861;&#36394;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#35299;&#21078;&#23398;&#30693;&#35782;&#30340;&#22870;&#21169;&#20989;&#25968;&#25552;&#39640;&#20102;&#30495;&#38451;&#24615;&#27604;&#29575;&#24182;&#38477;&#20302;&#20102;&#34394;&#20551;&#38451;&#24615;&#27604;&#29575;</title><link>https://arxiv.org/abs/2403.17845</link><description>&lt;p&gt;
TractOracle: &#20026;&#22522;&#20110;RL&#30340;&#32420;&#32500;&#26463;&#36861;&#36394;&#25552;&#20379;&#19968;&#20010;&#35299;&#21078;&#23398;&#30693;&#35782;&#39537;&#21160;&#30340;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
TractOracle: towards an anatomically-informed reward function for RL-based tractography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17845
&lt;/p&gt;
&lt;p&gt;
TractOracle &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RL&#32420;&#32500;&#26463;&#36861;&#36394;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#35299;&#21078;&#23398;&#30693;&#35782;&#30340;&#22870;&#21169;&#20989;&#25968;&#25552;&#39640;&#20102;&#30495;&#38451;&#24615;&#27604;&#29575;&#24182;&#38477;&#20302;&#20102;&#34394;&#20551;&#38451;&#24615;&#27604;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22522;&#20110;&#30340;&#32420;&#32500;&#26463;&#36861;&#36394;&#26159;&#19968;&#31181;&#31454;&#20105;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#20197;&#36739;&#39640;&#30340;&#35299;&#21078;&#23398;&#20934;&#30830;&#24615;&#36827;&#34892;&#25805;&#20316;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#29992;&#20110;&#35757;&#32451;RL&#20195;&#29702;&#30340;&#22870;&#21169;&#21151;&#33021;&#24182;&#19981;&#21253;&#21547;&#35299;&#21078;&#30693;&#35782;&#65292;&#36825;&#23548;&#33268;&#20195;&#29702;&#29983;&#25104;&#34394;&#20551;&#38451;&#24615;&#30340;&#32420;&#32500;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;RL&#32420;&#32500;&#26463;&#36861;&#36394;&#31995;&#32479;&#65292;TractOracle&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#30340;&#36861;&#36394;&#32593;&#32476;&#35757;&#32451;&#30340;&#22870;&#21169;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26082;&#29992;&#20316;&#22870;&#21169;&#20989;&#25968;&#65292;&#21448;&#29992;&#20316;&#26089;&#26399;&#20572;&#27490;&#36861;&#36394;&#36807;&#31243;&#30340;&#25163;&#27573;&#65292;&#20174;&#32780;&#20943;&#23569;&#34394;&#20551;&#38451;&#24615;&#32420;&#32500;&#26463;&#30340;&#25968;&#37327;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#31995;&#32479;&#25104;&#20026;&#19968;&#31181;&#21516;&#26102;&#35780;&#20272;&#21644;&#37325;&#24314;WM&#32420;&#32500;&#26463;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#25253;&#21578;&#20102;&#30495;&#38451;&#24615;&#27604;&#29575;&#20960;&#20046;&#25552;&#39640;&#20102;20\%&#65292;&#20551;&#38451;&#24615;&#27604;&#29575;&#20943;&#23569;&#20102;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17845v1 Announce Type: new  Abstract: Reinforcement learning (RL)-based tractography is a competitive alternative to machine learning and classical tractography algorithms due to its high anatomical accuracy obtained without the need for any annotated data. However, the reward functions so far used to train RL agents do not encapsulate anatomical knowledge which causes agents to generate spurious false positives tracts. In this paper, we propose a new RL tractography system, TractOracle, which relies on a reward network trained for streamline classification. This network is used both as a reward function during training as well as a mean for stopping the tracking process early and thus reduce the number of false positive streamlines. This makes our system a unique method that evaluates and reconstructs WM streamlines at the same time. We report an improvement of true positive ratios by almost 20\% and a reduction of 3x of false positive ratios on one dataset and an increase 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#29702;&#35774;&#35745;&#21644;&#23610;&#24230;&#21464;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#26032;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.17844</link><description>&lt;p&gt;
&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#30340;&#26426;&#29702;&#35774;&#35745;&#21644;&#23610;&#24230;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Mechanistic Design and Scaling of Hybrid Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17844
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#29702;&#35774;&#35745;&#21644;&#23610;&#24230;&#21464;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#26032;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24320;&#21457;&#26159;&#19968;&#20010;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#35774;&#35745;&#31354;&#38388;&#24191;&#38420;&#12289;&#21407;&#22411;&#21046;&#20316;&#26102;&#38388;&#38271;&#20197;&#21450;&#19982;&#35268;&#27169;&#21270;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#20197;&#31471;&#21040;&#31471;&#26426;&#26800;&#24335;&#26550;&#26500;&#35774;&#35745;&#65288;MAD&#65289;&#31649;&#32447;&#20026;&#22522;&#30784;&#26469;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#65292;&#21253;&#25324;&#23567;&#35268;&#27169;&#33021;&#21147;&#21333;&#20803;&#27979;&#35797;&#65292;&#39044;&#27979;&#23610;&#24230;&#35268;&#24459;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#21512;&#25104;&#30340;&#20196;&#29260;&#25805;&#20316;&#20219;&#21153;&#65292;&#22914;&#21387;&#32553;&#21644;&#22238;&#24518;&#65292;&#26088;&#22312;&#25506;&#32034;&#33021;&#21147;&#65292;&#25105;&#20204;&#35782;&#21035;&#24182;&#27979;&#35797;&#30001;&#21508;&#31181;&#35745;&#31639;&#22522;&#20803;&#26500;&#24314;&#30340;&#26032;&#22411;&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35745;&#31639;&#20248;&#21270;&#21644;&#19968;&#39033;&#26032;&#30340;&#29366;&#24577;&#26368;&#20248;&#21270;&#23610;&#24230;&#20998;&#26512;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;70M&#21040;7B&#21442;&#25968;&#20043;&#38388;&#30340;500&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#23545;&#32467;&#26524;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;MAD&#21512;&#25104;&#19982;&#35745;&#31639;&#26368;&#20248;&#22256;&#24785;&#24230;&#30456;&#20851;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#26032;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17844v1 Announce Type: new  Abstract: The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectur
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#21512;&#25104;&#30340;HDR&#22270;&#20687;&#25968;&#25454;&#38598; GTA-HDR&#65292;&#20174;GTA-V&#35270;&#39057;&#28216;&#25103;&#20013;&#37319;&#26679;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#25429;&#25417;&#19981;&#21516;&#22330;&#26223;&#26465;&#20214;&#21644;&#22270;&#20687;&#29305;&#24449;&#26041;&#38754;&#30340;&#19981;&#36275;</title><link>https://arxiv.org/abs/2403.17837</link><description>&lt;p&gt;
GTA-HDR&#65306;&#29992;&#20110;HDR&#22270;&#20687;&#37325;&#24314;&#30340;&#22823;&#35268;&#27169;&#21512;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17837
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#21512;&#25104;&#30340;HDR&#22270;&#20687;&#25968;&#25454;&#38598; GTA-HDR&#65292;&#20174;GTA-V&#35270;&#39057;&#28216;&#25103;&#20013;&#37319;&#26679;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#25429;&#25417;&#19981;&#21516;&#22330;&#26223;&#26465;&#20214;&#21644;&#22270;&#20687;&#29305;&#24449;&#26041;&#38754;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#21160;&#24577;&#33539;&#22260;&#65288;HDR&#65289;&#20869;&#23481;&#65288;&#21363;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20174;&#29616;&#23454;&#22330;&#26223;&#20013;&#25429;&#33719;HDR&#20869;&#23481;&#26159;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#12290;&#22240;&#27492;&#65292;&#20174;&#20854;&#20302;&#21160;&#24577;&#33539;&#22260;&#65288;LDR&#65289;&#23545;&#24212;&#29289;&#20013;&#37325;&#24314;&#35270;&#35273;&#31934;&#30830;&#30340;HDR&#22270;&#20687;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#24341;&#36215;&#20102;&#35270;&#35273;&#30740;&#31350;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#22330;&#26223;&#26465;&#20214;&#65288;&#22914;&#29031;&#26126;&#12289;&#38452;&#24433;&#12289;&#22825;&#27668;&#12289;&#20301;&#32622;&#12289;&#26223;&#35266;&#12289;&#29289;&#20307;&#12289;&#20154;&#31867;&#12289;&#24314;&#31569;&#65289;&#21644;&#21508;&#31181;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#39068;&#33394;&#12289;&#23545;&#27604;&#24230;&#12289;&#39281;&#21644;&#24230;&#12289;&#33394;&#35843;&#12289;&#20142;&#24230;&#12289;&#36752;&#23556;&#24230;&#65289;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GTA-HDR&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;GTA-V&#35270;&#39057;&#28216;&#25103;&#20013;&#37319;&#26679;&#30340;&#22823;&#35268;&#27169;&#36924;&#30495;HDR&#22270;&#20687;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17837v1 Announce Type: cross  Abstract: High Dynamic Range (HDR) content (i.e., images and videos) has a broad range of applications. However, capturing HDR content from real-world scenes is expensive and time- consuming. Therefore, the challenging task of reconstructing visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is gaining attention in the vision research community. A major challenge in this research problem is the lack of datasets, which capture diverse scene conditions (e.g., lighting, shadows, weather, locations, landscapes, objects, humans, buildings) and various image features (e.g., color, contrast, saturation, hue, luminance, brightness, radiance). To address this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic HDR images sampled from the GTA-V video game. We perform thorough evaluation of the proposed dataset, which demonstrates significant qualitative and quantitative improvements of the
&lt;/p&gt;</description></item><item><title>GPFL&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#25237;&#24433;&#30340;&#23458;&#25143;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#26412;&#22320;&#21644;&#20840;&#23616;&#19979;&#38477;&#26041;&#21521;&#26469;&#34913;&#37327;&#23458;&#25143;&#20215;&#20540;&#65292;&#24182;&#37319;&#29992;&#21033;&#29992;-&#25506;&#32034;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;FEMINST&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#36229;&#36807;9%&#30340;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2403.17833</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#25237;&#24433;&#30340;&#23458;&#25143;&#36873;&#25321;&#26694;&#26550;&#29992;&#20110;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17833
&lt;/p&gt;
&lt;p&gt;
GPFL&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#25237;&#24433;&#30340;&#23458;&#25143;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#26412;&#22320;&#21644;&#20840;&#23616;&#19979;&#38477;&#26041;&#21521;&#26469;&#34913;&#37327;&#23458;&#25143;&#20215;&#20540;&#65292;&#24182;&#37319;&#29992;&#21033;&#29992;-&#25506;&#32034;&#26426;&#21046;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;FEMINST&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#36229;&#36807;9%&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#36873;&#25321;&#23545;&#30830;&#23450;&#21442;&#19982;&#32773;&#23458;&#25143;&#20197;&#24179;&#34913;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#25968;&#25454;&#24322;&#26500;&#24615;&#12289;&#35745;&#31639;&#36127;&#25285;&#21644;&#29420;&#31435;&#23458;&#25143;&#22788;&#29702;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPFL&#65292;&#36890;&#36807;&#27604;&#36739;&#26412;&#22320;&#21644;&#20840;&#23616;&#19979;&#38477;&#26041;&#21521;&#26469;&#34913;&#37327;&#23458;&#25143;&#20215;&#20540;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#21033;&#29992;-&#25506;&#32034;&#26426;&#21046;&#26469;&#22686;&#24378;&#24615;&#33021;&#12290;&#23545;FEMINST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPFL&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;FEMINST&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#36229;&#36807;9%&#30340;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;GPFL&#36890;&#36807;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#39044;&#36873;&#21644;&#21442;&#25968;&#37325;&#29992;&#23637;&#31034;&#20986;&#26356;&#30701;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17833v1 Announce Type: new  Abstract: Federated learning client selection is crucial for determining participant clients while balancing model accuracy and communication efficiency. Existing methods have limitations in handling data heterogeneity, computational burdens, and independent client treatment. To address these challenges, we propose GPFL, which measures client value by comparing local and global descent directions. We also employ an Exploit-Explore mechanism to enhance performance. Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times through pre-selection and parameter reuse in federated learning.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#23454;&#26045;&#19981;&#21516;&#30340;&#29615;&#22659;&#35774;&#35745;&#20915;&#31574;&#23545;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26368;&#20248;&#28526;&#27969;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#30340;&#39318;&#35201;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.17831</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#20248;&#28526;&#27969;&#65306;&#29615;&#22659;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning the Optimal Power Flow: Environment Design Matters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17831
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#23454;&#26045;&#19981;&#21516;&#30340;&#29615;&#22659;&#35774;&#35745;&#20915;&#31574;&#23545;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26368;&#20248;&#28526;&#27969;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#30340;&#39318;&#35201;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#26368;&#20248;&#28526;&#27969;&#65288;OPF&#65289;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23558;OPF&#38382;&#39064;&#20316;&#20026;RL&#29615;&#22659;&#30340;&#30830;&#20999;&#24418;&#24335;&#65292;RL-OPF&#25991;&#29486;&#23384;&#22312;&#30528;&#24456;&#22823;&#20998;&#27495;&#12290;&#26412;&#25991;&#25910;&#38598;&#24182;&#23454;&#29616;&#20102;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#12289;&#35266;&#27979;&#31354;&#38388;&#12289;&#22238;&#21512;&#23450;&#20041;&#21644;&#22870;&#21169;&#20989;&#25968;&#36873;&#25321;&#30340;&#25991;&#29486;&#20013;&#21508;&#31181;&#19981;&#21516;&#30340;&#29615;&#22659;&#35774;&#35745;&#20915;&#31574;&#12290;&#22312;&#23454;&#39564;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#29615;&#22659;&#35774;&#35745;&#36873;&#39033;&#23545;RL-OPF&#35757;&#32451;&#24615;&#33021;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#24314;&#35758;&#20851;&#20110;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#30340;&#36873;&#25321;&#12290;&#25152;&#21019;&#24314;&#30340;&#29615;&#22659;&#26694;&#26550;&#26159;&#23436;&#20840;&#24320;&#28304;&#30340;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;RL-OPF&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17831v1 Announce Type: new  Abstract: To solve the optimal power flow (OPF) problem, reinforcement learning (RL) emerges as a promising new approach. However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment. In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice. In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance. Further, we derive some first recommendations regarding the choice of these design decisions. The created environment framework is fully open-source and can serve as a benchmark for future research in the RL-OPF field.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.17827</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21512;&#25104;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17827
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#33258;&#28982;&#30340;3D&#25163;-&#29289;&#20307;&#20132;&#20114;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26399;&#26395;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#29289;&#20307;&#21160;&#20316;&#22312;&#29289;&#29702;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#19978;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffH2O&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#21333;&#25163;&#25110;&#21452;&#25163;&#29289;&#20307;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19977;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#25235;&#21462;&#38454;&#27573;&#21644;&#22522;&#20110;&#25991;&#26412;&#20132;&#20114;&#38454;&#27573;&#65292;&#24182;&#20026;&#27599;&#20010;&#38454;&#27573;&#20351;&#29992;&#21333;&#29420;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#25235;&#21462;&#38454;&#27573;&#20013;&#65292;&#27169;&#22411;&#20165;&#29983;&#25104;&#25163;&#37096;&#21160;&#20316;&#65292;&#32780;&#22312;&#20132;&#20114;&#38454;&#27573;&#20013;&#65292;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#37117;&#34987;&#21512;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#23494;&#32806;&#21512;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17827v1 Announce Type: cross  Abstract: Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes 
&lt;/p&gt;</description></item><item><title>&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#19981;&#20165;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#36824;&#21462;&#20915;&#20110;&#21387;&#32553;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#27169;&#22411;&#21387;&#32553;&#24182;&#19981;&#24635;&#26159;&#20250;&#20351;&#22312;&#23569;&#25968;&#23376;&#32676;&#20307;&#19978;&#30340;&#24615;&#33021;&#21464;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.17811</link><description>&lt;p&gt;
&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23545;&#23376;&#32676;&#20307;&#31283;&#20581;&#24615;&#24433;&#21709;&#36739;&#23567;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Compressed Language Models Less Subgroup Robust?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17811
&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#19981;&#20165;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#36824;&#21462;&#20915;&#20110;&#21387;&#32553;&#26041;&#27861;&#65292;&#21516;&#26102;&#21457;&#29616;&#27169;&#22411;&#21387;&#32553;&#24182;&#19981;&#24635;&#26159;&#20250;&#20351;&#22312;&#23569;&#25968;&#23376;&#32676;&#20307;&#19978;&#30340;&#24615;&#33021;&#21464;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#27169;&#22411;&#21387;&#32553;&#26469;&#21019;&#24314;&#26356;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#30001;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#21644;&#23646;&#24615;&#23450;&#20041;&#30340;&#23569;&#25968;&#23376;&#32676;&#20307;&#30340;&#31283;&#20581;&#24615;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;18&#31181;&#19981;&#21516;&#30340;&#21387;&#32553;&#26041;&#27861;&#21644;&#35774;&#32622;&#23545;BERT&#35821;&#35328;&#27169;&#22411;&#30340;&#23376;&#32676;&#20307;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#24046;&#32676;&#32452;&#30340;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#27169;&#22411;&#22823;&#23567;&#65292;&#36824;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21387;&#32553;&#24182;&#19981;&#24635;&#26159;&#20250;&#20351;&#22312;&#23569;&#25968;&#23376;&#32676;&#20307;&#19978;&#30340;&#24615;&#33021;&#21464;&#24046;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26377;&#21161;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#27169;&#22411;&#21387;&#32553;&#23545;&#23376;&#32676;&#20307;&#31283;&#20581;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17811v1 Announce Type: cross  Abstract: To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of model compression.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#20687;&#32032;&#32423;&#27880;&#37322;&#30340;&#36924;&#30495;&#21512;&#25104;&#26174;&#24494;&#35270;&#39057;&#30340;&#29983;&#29289;&#21307;&#23398;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17808</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#27969;&#22330;&#29983;&#25104;&#24102;&#27880;&#37322;&#30340;&#29983;&#29289;&#21307;&#23398;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17808
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#20687;&#32032;&#32423;&#27880;&#37322;&#30340;&#36924;&#30495;&#21512;&#25104;&#26174;&#24494;&#35270;&#39057;&#30340;&#29983;&#29289;&#21307;&#23398;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#30340;&#20998;&#21106;&#21644;&#36319;&#36394;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#30284;&#30151;&#30740;&#31350;&#12289;&#33647;&#29289;&#24320;&#21457;&#21644;&#21457;&#32946;&#29983;&#29289;&#23398;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#20013;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#29289;&#21307;&#23398;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65288;BVDM&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#21512;&#25104;&#26174;&#24494;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17808v1 Announce Type: cross  Abstract: The segmentation and tracking of living cells play a vital role within the biomedical domain, particularly in cancer research, drug development, and developmental biology. These are usually tedious and time-consuming tasks that are traditionally done by biomedical experts. Recently, to automatize these processes, deep learning based segmentation and tracking methods have been proposed. These methods require large-scale datasets and their full potential is constrained by the scarcity of annotated data in the biomedical imaging domain. To address this limitation, we propose Biomedical Video Diffusion Model (BVDM), capable of generating realistic-looking synthetic microscopy videos. Trained only on a single real video, BVDM can generate videos of arbitrary length with pixel-level annotations that can be used for training data-hungry models. It is composed of a denoising diffusion probabilistic model (DDPM) generating high-fidelity synthet
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;EAP-IG&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;</title><link>https://arxiv.org/abs/2403.17806</link><description>&lt;p&gt;
&#22362;&#20449;&#24544;&#23454;&#65306;&#22312;&#25214;&#21040;&#27169;&#22411;&#26426;&#21046;&#26102;&#36229;&#36234;&#30005;&#36335;&#37325;&#21472;
&lt;/p&gt;
&lt;p&gt;
Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17806
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;EAP-IG&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#24050;&#37319;&#29992;&#30005;&#36335;&#26694;&#26550;&#65292;&#26088;&#22312;&#25214;&#21040;&#35299;&#37322;LM&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#34892;&#20026;&#30340;&#26368;&#23567;&#35745;&#31639;&#23376;&#22270;&#25110;&#30005;&#36335;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#36890;&#36807;&#29420;&#31435;&#23545;&#27599;&#20010;&#36793;&#25191;&#34892;&#22240;&#26524;&#24178;&#39044;&#26469;&#30830;&#23450;&#21738;&#20123;&#36793;&#23646;&#20110;LM&#30340;&#30005;&#36335;&#65292;&#20294;&#36825;&#22312;&#27169;&#22411;&#35268;&#27169;&#36739;&#22823;&#26102;&#25928;&#29575;&#20302;&#19979;&#12290;&#36793;&#32536;&#24402;&#22240;&#20462;&#34917;&#65288;EAP&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36817;&#20284;&#24178;&#39044;&#26041;&#27861;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#20294;&#19981;&#23436;&#32654;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; - &#24102;&#26377;&#38598;&#25104;&#26799;&#24230;&#30340;EAP&#65288;EAP-IG&#65289;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;&#12290;&#22914;&#26524;&#30005;&#36335;&#26159;&#24544;&#23454;&#30340;&#65292;&#21017;&#21487;&#20197;&#21435;&#25481;&#30005;&#36335;&#20043;&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#36793;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#22312;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;&#24544;&#23454;&#24615;&#26159;&#30740;&#31350;&#30005;&#36335;&#32780;&#19981;&#26159;&#23436;&#25972;&#27169;&#22411;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;EAP&#25214;&#21040;&#30340;&#30005;&#36335;&#19981;&#22826;&#24544;&#23454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17806v1 Announce Type: cross  Abstract: Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faith
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MATS-Gym&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;CARLA&#20013;&#35757;&#32451;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#36890;&#22330;&#26223;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#20855;&#26377;&#21487;&#21464;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#20132;&#36890;&#22330;&#26223;&#24182;&#25972;&#21512;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#20132;&#36890;&#22330;&#26223;&#25551;&#36848;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17805</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#39537;&#21160;&#30340;&#35838;&#31243;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MATS-Gym&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;CARLA&#20013;&#35757;&#32451;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#36890;&#22330;&#26223;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#20855;&#26377;&#21487;&#21464;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#20132;&#36890;&#22330;&#26223;&#24182;&#25972;&#21512;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#20132;&#36890;&#22330;&#26223;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#35757;&#32451;&#22330;&#26223;&#30340;&#33258;&#21160;&#21270;&#29983;&#25104;&#22312;&#35768;&#22810;&#22797;&#26434;&#23398;&#20064;&#20219;&#21153;&#20013;&#26159;&#37325;&#35201;&#30340;&#12290;&#29305;&#21035;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#33258;&#20027;&#39550;&#39542;&#65292;&#33258;&#21160;&#29983;&#25104;&#35838;&#31243;&#34987;&#35748;&#20026;&#23545;&#33719;&#24471;&#24378;&#20581;&#21644;&#36890;&#29992;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20805;&#28385;&#25361;&#25112;&#30340;&#20223;&#30495;&#29615;&#22659;&#20013;&#65292;&#20026;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#22810;&#20010;&#24322;&#26500;&#26234;&#33021;&#20307;&#36827;&#34892;&#35774;&#35745;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#32321;&#29712;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MATS-Gym&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#39640;&#20445;&#30495;&#39550;&#39542;&#27169;&#25311;&#22120;CARLA&#20013;&#35757;&#32451;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#36890;&#22330;&#26223;&#26694;&#26550;&#12290;MATS-Gym&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#30340;&#22810;&#26234;&#33021;&#20307;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#37096;&#20998;&#22330;&#26223;&#35268;&#33539;&#29983;&#25104;&#20855;&#26377;&#21487;&#21464;&#26234;&#33021;&#20307;&#25968;&#37327;&#30340;&#20132;&#36890;&#22330;&#26223;&#12290;&#36825;&#31687;&#35770;&#25991;&#23558;&#21508;&#31181;&#29616;&#26377;&#30340;&#20132;&#36890;&#22330;&#26223;&#25551;&#36848;&#26041;&#27861;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#35757;&#32451;&#26694;&#26550;&#20013;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#19982;&#20854;&#20182;&#33258;&#20027;&#39550;&#39542;&#31639;&#27861;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17805v1 Announce Type: cross  Abstract: The automated generation of diverse and complex training scenarios has been an important ingredient in many complex learning tasks. Especially in real-world application domains, such as autonomous driving, auto-curriculum generation is considered vital for obtaining robust and general policies. However, crafting traffic scenarios with multiple, heterogeneous agents is typically considered as a tedious and time-consuming task, especially in more complex simulation environments. In our work, we introduce MATS-Gym, a Multi-Agent Traffic Scenario framework to train agents in CARLA, a high-fidelity driving simulator. MATS-Gym is a multi-agent training framework for autonomous driving that uses partial scenario specifications to generate traffic scenarios with variable numbers of agents. This paper unifies various existing approaches to traffic scenario description into a single training framework and demonstrates how it can be integrated wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23433;&#20840;&#32858;&#21512;&#22312;&#38544;&#31169;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#38754;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26102;&#24182;&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#31169;&#23494;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17775</link><description>&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#22312;&#38754;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26102;&#24182;&#38750;&#31169;&#23494;&#30340;
&lt;/p&gt;
&lt;p&gt;
Secure Aggregation is Not Private Against Membership Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23433;&#20840;&#32858;&#21512;&#22312;&#38544;&#31169;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#38754;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26102;&#24182;&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#31169;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#65288;SecAgg&#65289;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#38544;&#31169;&#22686;&#24378;&#26426;&#21046;&#65292;&#20165;&#20801;&#35768;&#26381;&#21153;&#22120;&#35775;&#38382;&#27169;&#22411;&#26356;&#26032;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#26356;&#26032;&#30340;&#26426;&#23494;&#24615;&#12290;&#23613;&#31649;&#26377;&#20851;SecAgg&#20445;&#25252;&#38544;&#31169;&#33021;&#21147;&#30340;&#24191;&#27867;&#22768;&#26126;&#65292;&#20294;&#32570;&#20047;&#23545;&#20854;&#38544;&#31169;&#24615;&#30340;&#27491;&#24335;&#20998;&#26512;&#65292;&#22240;&#27492;&#36825;&#20123;&#20551;&#35774;&#26159;&#19981;&#21512;&#29702;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;SecAgg&#35270;&#20026;&#27599;&#20010;&#23616;&#37096;&#26356;&#26032;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#26426;&#21046;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;SecAgg&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#25915;&#20987;&#26041;&#24335;&#65292;&#20854;&#20013;&#23545;&#25163;&#26381;&#21153;&#22120;&#35797;&#22270;&#22312;SecAgg&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#19968;&#35757;&#32451;&#36718;&#20013;&#25512;&#26029;&#20986;&#23458;&#25143;&#31471;&#25552;&#20132;&#30340;&#26356;&#26032;&#21521;&#37327;&#26159;&#20004;&#20010;&#21487;&#33021;&#21521;&#37327;&#20013;&#30340;&#21738;&#19968;&#20010;&#12290;&#36890;&#36807;&#36827;&#34892;&#38544;&#31169;&#23457;&#26680;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#25915;&#20987;&#30340;&#25104;&#21151;&#27010;&#29575;&#65292;&#24182;&#37327;&#21270;&#20102;SecAgg&#25552;&#20379;&#30340;LDP&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#25968;&#23383;&#32467;&#26524;&#25581;&#31034;&#20102;&#65292;&#19982;&#26222;&#36941;&#22768;&#26126;&#30456;&#21453;&#65292;SecAgg&#24182;&#27809;&#26377;&#25552;&#20379;&#31169;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17775v1 Announce Type: new  Abstract: Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates. Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified. In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update. We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg. By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg. Our numerical results unveil that, contrary to prevailing claims, SecAgg offer
&lt;/p&gt;</description></item><item><title>&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17768</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#22797;&#26434;&#24615;&#21040;&#20844;&#20247;&#21465;&#20107;&#65306;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17768
&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#65292;&#24039;&#22937;&#22320;&#23558;&#22797;&#26434;&#30340;&#30740;&#31350;&#25991;&#31456;&#32763;&#35793;&#25104;&#19982;&#26356;&#24191;&#27867;&#30340;&#20844;&#20247; resonant &#30340;&#25253;&#36947;&#12290;&#36825;&#31181;&#21465;&#20107;&#30340;&#33258;&#21160;&#29983;&#25104;&#22686;&#24378;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#26009;&#24211;&#26469;&#20419;&#36827;&#36825;&#31181;&#33539;&#24335;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;&#20061;&#20010;&#23398;&#31185;&#39046;&#22495;&#20013;&#23398;&#26415;&#20986;&#29256;&#29289;&#21450;&#20854;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#24179;&#34892;&#32534;&#35793;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#31185;&#23398;&#26032;&#38395;&#21465;&#20107;&#21644;&#23398;&#26415;&#25991;&#31295;&#20043;&#38388;&#30340;&#21487;&#35835;&#24615;&#21644;&#31616;&#27905;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#36807;&#31243;&#21253;&#25324;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#20026;&#26410;&#26469;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17768v1 Announce Type: cross  Abstract: Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#28176;&#36817;&#36125;&#21494;&#26031;&#39118;&#38505;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#19982;&#26368;&#20339;&#31639;&#27861;&#27604;&#36739;&#24471;&#20986;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.17767</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#28176;&#36817;&#36125;&#21494;&#26031;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Bayes risk of semi-supervised learning with uncertain labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17767
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#28176;&#36817;&#36125;&#21494;&#26031;&#39118;&#38505;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#19982;&#26368;&#20339;&#31639;&#27861;&#27604;&#36739;&#24471;&#20986;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#19978;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#35774;&#32622;&#65292;&#20854;&#20013;&#25968;&#25454;&#30340;&#26631;&#31614;&#19981;&#20687;&#36890;&#24120;&#37027;&#26679;&#20005;&#26684;&#65292;&#32780;&#26159;&#24102;&#26377;&#19981;&#30830;&#23450;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35745;&#31639;&#35813;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#39118;&#38505;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#35813;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#39118;&#38505;&#19982;&#30446;&#21069;&#24050;&#30693;&#30340;&#26368;&#20339;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#36825;&#31181;&#27604;&#36739;&#26368;&#32456;&#20026;&#35813;&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17767v1 Announce Type: cross  Abstract: This article considers a semi-supervised classification setting on a Gaussian mixture model, where the data is not labeled strictly as usual, but instead with uncertain labels. Our main aim is to compute the Bayes risk for this model. We compare the behavior of the Bayes risk and the best known algorithm for this model. This comparison eventually gives new insights over the algorithm.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Noise2Noise4Mars&#65288;N2N4M&#65289;&#27169;&#22411;&#29992;&#20110;&#21435;&#38500;CRISM&#22270;&#20687;&#22122;&#22768;&#65292;&#26080;&#38656;&#38646;&#22122;&#22768;&#30446;&#26631;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#21644;CRISM&#22270;&#20687;&#19978;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17757</link><description>&lt;p&gt;
CRISM&#39640;&#20809;&#35889;&#25968;&#25454;&#30340;Noise2Noise&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Noise2Noise Denoising of CRISM Hyperspectral Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17757
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Noise2Noise4Mars&#65288;N2N4M&#65289;&#27169;&#22411;&#29992;&#20110;&#21435;&#38500;CRISM&#22270;&#20687;&#22122;&#22768;&#65292;&#26080;&#38656;&#38646;&#22122;&#22768;&#30446;&#26631;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#21644;CRISM&#22270;&#20687;&#19978;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26102;&#38388;&#25512;&#31227;&#65292;Compact Reconnaissance Imaging Spectrometer for Mars&#65288;CRISM&#65289;&#33719;&#21462;&#30340;&#39640;&#20809;&#35889;&#25968;&#25454;&#24050;&#32463;&#23548;&#33268;&#20102;&#28779;&#26143;&#34920;&#38754;&#30719;&#29289;&#23398;&#26144;&#23556;&#30340;&#26080;&#19982;&#20262;&#27604;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26550;&#26500;Noise2Noise4Mars&#65288;N2N4M&#65289;&#65292;&#29992;&#20110;&#21435;&#38500;CRISM&#22270;&#20687;&#20013;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#33258;&#25105;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#38646;&#22122;&#22768;&#30446;&#26631;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#22312;&#34892;&#26143;&#31185;&#23398;&#24212;&#29992;&#20013;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#21512;&#25104;&#22122;&#22768;&#25968;&#25454;&#21644;CRISM&#22270;&#20687;&#19978;&#30340;&#24378;&#22823;&#24615;&#33021;&#65292;&#20197;&#21450;&#23545;&#19979;&#28216;&#20998;&#31867;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#23545;&#28779;&#26143;&#34920;&#38754;&#20851;&#38190;&#20852;&#36259;&#28857;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#21253;&#25324;&#25552;&#35758;&#30340;&#30528;&#38470;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17757v1 Announce Type: cross  Abstract: Hyperspectral data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the surface mineralogy of Mars. Due to sensor degradation over time, a significant portion of the recently acquired data is considered unusable. Here a new data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to remove noise from CRISM images. Our model is self-supervised and does not require zero-noise target data, making it well suited for use in Planetary Science applications where high quality labelled data is scarce. We demonstrate its strong performance on synthetic-noise data and CRISM images, and its impact on downstream classification performance, outperforming benchmark methods on most metrics. This allows for detailed analysis for critical sites of interest on the Martian surface, including proposed lander sites.
&lt;/p&gt;</description></item><item><title>CCDSReFormer&#27169;&#22411;&#24341;&#20837;&#20102;&#19977;&#31181;&#21019;&#26032;&#27169;&#22359;&#26469;&#25552;&#39640;&#20132;&#36890;&#27969;&#39044;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21253;&#25324;&#22686;&#24378;&#25972;&#27969;&#31354;&#38388;&#33258;&#27880;&#24847;&#21147;&#12289;&#22686;&#24378;&#25972;&#27969;&#24310;&#36831;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#21644;&#22686;&#24378;&#25972;&#27969;&#26102;&#38388;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#23454;&#29616;&#31232;&#30095;&#27880;&#24847;&#21147;&#12289;&#21487;&#35299;&#37322;&#30340;&#23616;&#37096;&#20449;&#24687;&#21644;&#34701;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.17753</link><description>&lt;p&gt;
CCDSReFormer&#65306;&#19968;&#31181;&#20132;&#21449;&#21452;&#27969;&#22686;&#24378;&#25972;&#27969;&#21464;&#21387;&#22120;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream Enhanced Rectified Transformer Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17753
&lt;/p&gt;
&lt;p&gt;
CCDSReFormer&#27169;&#22411;&#24341;&#20837;&#20102;&#19977;&#31181;&#21019;&#26032;&#27169;&#22359;&#26469;&#25552;&#39640;&#20132;&#36890;&#27969;&#39044;&#27979;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21253;&#25324;&#22686;&#24378;&#25972;&#27969;&#31354;&#38388;&#33258;&#27880;&#24847;&#21147;&#12289;&#22686;&#24378;&#25972;&#27969;&#24310;&#36831;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#21644;&#22686;&#24378;&#25972;&#27969;&#26102;&#38388;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#23454;&#29616;&#31232;&#30095;&#27880;&#24847;&#21147;&#12289;&#21487;&#35299;&#37322;&#30340;&#23616;&#37096;&#20449;&#24687;&#21644;&#34701;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#32780;&#26377;&#25928;&#30340;&#20132;&#36890;&#39044;&#27979;&#23545;&#20110;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#22478;&#24066;&#20132;&#36890;&#35268;&#21010;&#21644;&#31649;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26102;&#31354;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#23613;&#31649;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#22312;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12289;&#20559;&#22909;&#20840;&#23616;&#32780;&#38750;&#23616;&#37096;&#20449;&#24687;&#12289;&#20197;&#21450;&#20998;&#24320;&#22788;&#29702;&#31354;&#38388;&#21644;&#26102;&#38388;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23545;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20132;&#21449;&#21452;&#27969;&#22686;&#24378;&#25972;&#27969;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;CCDSReFormer&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#20010;&#21019;&#26032;&#27169;&#22359;&#65306;&#22686;&#24378;&#25972;&#27969;&#31354;&#38388;&#33258;&#27880;&#24847;&#21147;&#65288;ReSSA&#65289;&#12289;&#22686;&#24378;&#25972;&#27969;&#24310;&#36831;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;ReDASA&#65289;&#21644;&#22686;&#24378;&#25972;&#27969;&#26102;&#38388;&#33258;&#27880;&#24847;&#21147;&#65288;ReTSA&#65289;&#12290;&#36825;&#20123;&#27169;&#22359;&#26088;&#22312;&#36890;&#36807;&#31232;&#30095;&#27880;&#24847;&#21147;&#38477;&#20302;&#35745;&#31639;&#38656;&#27714;&#65292;&#19987;&#27880;&#20110;&#23616;&#37096;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20132;&#36890;&#21160;&#24577;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#23398;&#20064;&#26041;&#27861;&#21512;&#24182;&#31354;&#38388;&#21644;&#26102;&#38388;&#35265;&#35299;&#12290;&#23545;&#20845;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17753v1 Announce Type: new  Abstract: Accurate, and effective traffic forecasting is vital for smart traffic systems, crucial in urban traffic planning and management. Current Spatio-Temporal Transformer models, despite their prediction capabilities, struggle with balancing computational efficiency and accuracy, favoring global over local information, and handling spatial and temporal data separately, limiting insight into complex interactions. We introduce the Criss-Crossed Dual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes three innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA), Enhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified Temporal Self-attention (ReTSA). These modules aim to lower computational needs via sparse attention, focus on local information for better traffic dynamics understanding, and merge spatial and temporal insights through a unique learning method. Extensive tests on six real
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAREMed&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;-&#24494;&#35843;&#23398;&#20064;&#33539;&#24335;&#22686;&#24378;&#32597;&#35265;&#30142;&#30149;&#30340;&#33647;&#29289;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19987;&#38376;&#30340;&#33647;&#29289;&#38656;&#27714;&#21644;&#20020;&#24202;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;</title><link>https://arxiv.org/abs/2403.17745</link><description>&lt;p&gt;
&#19981;&#35753;&#20219;&#20309;&#24739;&#32773;&#25481;&#38431;&#65306;&#22686;&#24378;&#32597;&#35265;&#30149;&#24739;&#32773;&#30340;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17745
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAREMed&#30340;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;-&#24494;&#35843;&#23398;&#20064;&#33539;&#24335;&#22686;&#24378;&#32597;&#35265;&#30142;&#30149;&#30340;&#33647;&#29289;&#25512;&#33616;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19987;&#38376;&#30340;&#33647;&#29289;&#38656;&#27714;&#21644;&#20020;&#24202;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#31995;&#32479;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#20379;&#23450;&#21046;&#21644;&#26377;&#25928;&#30340;&#33647;&#29289;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#30456;&#36739;&#20110;&#24739;&#26377;&#24120;&#35265;&#30142;&#30149;&#30340;&#24739;&#32773;&#65292;&#23545;&#20110;&#24739;&#26377;&#32597;&#35265;&#30149;&#30151;&#30340;&#24739;&#32773;&#65292;&#25512;&#33616;&#24448;&#24448;&#26356;&#20934;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Robust and Accurate REcommendations for Medication&#65288;RAREMed&#65289;&#30340;&#21019;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;-&#24494;&#35843;&#23398;&#20064;&#33539;&#24335;&#26469;&#22686;&#24378;&#32597;&#35265;&#30142;&#30149;&#30340;&#20934;&#30830;&#24615;&#12290;RAREMed&#37319;&#29992;&#20855;&#26377;&#32479;&#19968;&#36755;&#20837;&#24207;&#21015;&#26041;&#27861;&#30340;Transformer&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#30142;&#30149;&#21644;&#31243;&#24207;&#20195;&#30721;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#23427;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21363;Sequence Matching Prediction&#65288;SMP&#65289;&#21644;Self Reconstruction&#65288;SR&#65289;&#65292;&#26469;&#23398;&#20064;&#19987;&#38376;&#30340;&#33647;&#29289;&#38656;&#27714;&#21644;&#20020;&#24202;&#20195;&#30721;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17745v1 Announce Type: new  Abstract: Medication recommendation systems have gained significant attention in healthcare as a means of providing tailored and effective drug combinations based on patients' clinical information. However, existing approaches often suffer from fairness issues, as recommendations tend to be more accurate for patients with common diseases compared to those with rare conditions. In this paper, we propose a novel model called Robust and Accurate REcommendations for Medication (RAREMed), which leverages the pretrain-finetune learning paradigm to enhance accuracy for rare diseases. RAREMed employs a transformer encoder with a unified input sequence approach to capture complex relationships among disease and procedure codes. Additionally, it introduces two self-supervised pre-training tasks, namely Sequence Matching Prediction (SMP) and Self Reconstruction (SR), to learn specialized medication needs and interrelations among clinical codes. Experimental 
&lt;/p&gt;</description></item><item><title>EulerFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#36716;&#25442;&#22120;&#21464;&#20307;&#65292;&#32479;&#19968;&#20102;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.17729</link><description>&lt;p&gt;
EulerFormer&#65306;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17729
&lt;/p&gt;
&lt;p&gt;
EulerFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#36716;&#25442;&#22120;&#21464;&#20307;&#65292;&#32479;&#19968;&#20102;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25429;&#25417;&#29992;&#25143;&#20559;&#22909;&#65292;&#36716;&#25442;&#22120;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#27169;&#39034;&#24207;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#12290;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#26680;&#24515;&#22312;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#35745;&#31639;&#24207;&#21015;&#20013;&#30340;&#25104;&#23545;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#30001;&#20110;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;&#29305;&#24615;&#65292;&#20301;&#32622;&#32534;&#30721;&#29992;&#20110;&#22686;&#24378;&#20196;&#29260;&#34920;&#31034;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#22312;&#36825;&#31181;&#35774;&#23450;&#19979;&#65292;&#25104;&#23545;&#27880;&#24847;&#21147;&#20998;&#25968;&#21487;&#20197;&#36890;&#36807;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#20004;&#32773;&#34893;&#29983;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#32463;&#24120;&#20197;&#19981;&#21516;&#26041;&#24335;&#24314;&#27169;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24046;&#24322;&#27979;&#37327;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#24207;&#21015;&#24314;&#27169;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EulerFormer&#30340;&#20855;&#26377;&#22797;&#26434;&#21521;&#37327;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#36716;&#25442;&#22120;&#21464;&#20307;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#34920;&#36848;&#35821;&#20041;&#24046;&#24322;&#21644;&#20301;&#32622;&#24046;&#24322;&#12290; EulerFormer&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#25216;&#26415;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17729v1 Announce Type: cross  Abstract: To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. Fi
&lt;/p&gt;</description></item><item><title>&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36328;&#36234;PDEs&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#29992;&#28508;&#22312;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.17728</link><description>&lt;p&gt;
&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#26159;PDE&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders are PDE Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17728
&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36328;&#36234;PDEs&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#29992;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27714;&#35299;&#22120;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23454;&#29992;&#24615;&#30446;&#21069;&#21463;&#21040;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290; PDE&#22312;&#24191;&#27867;&#30340;&#23610;&#24230;&#19978;&#28436;&#21464;&#24182;&#23637;&#31034;&#20986;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#65307;&#39044;&#27979;&#36825;&#20123;&#29616;&#35937;&#23558;&#38656;&#35201;&#23398;&#20064;&#36328;&#36234;&#21508;&#31181;&#36755;&#20837;&#30340;&#34920;&#31034;&#65292;&#36825;&#20123;&#36755;&#20837;&#21487;&#33021;&#28085;&#30422;&#19981;&#21516;&#30340;&#31995;&#25968;&#12289;&#20960;&#20309;&#22270;&#24418;&#25110;&#26041;&#31243;&#12290;&#20316;&#20026;&#36890;&#21521;&#21487;&#27867;&#21270;PDE&#24314;&#27169;&#30340;&#19968;&#27493;&#65292;&#25105;&#20204;&#20026;PDEs&#35843;&#25972;&#20102;&#25513;&#30721;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36328;&#36234;PDEs&#65292;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#23398;&#20064;&#26377;&#29992;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#25513;&#30721;&#39044;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#27714;&#35299;&#22120;&#23545;&#26410;&#35265;&#26041;&#31243;&#30340;&#31995;&#25968;&#22238;&#24402;&#21644;&#26102;&#38388;&#27493;&#39588;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25513;&#30721;&#39044;&#35757;&#32451;&#33021;&#25104;&#20026;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#12289;&#26410;&#26631;&#35760;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#35268;&#27169;&#21270;&#30340;&#28508;&#22312;&#29289;&#29702;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17728v1 Announce Type: new  Abstract: Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#65292;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#25552;&#21462;&#23494;&#38598;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;Triplet SSM&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.17701</link><description>&lt;p&gt;
&#26059;&#36716;&#25195;&#25551;&#65306;&#24102;&#26377;&#19977;&#20803;SSM&#27169;&#22359;&#30340;UNet-like Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#65292;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#25552;&#21462;&#23494;&#38598;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;Triplet SSM&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#21344;&#25454;&#37325;&#35201;&#20301;&#32622;&#12290;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#30001;&#20110;&#26377;&#38480;&#24863;&#21463;&#37326;&#25110;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#65292;&#29305;&#21035;&#26159;Mamba&#21450;&#20854;&#21464;&#20307;&#65292;&#22312;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21487;&#33021;&#19981;&#22815;&#26377;&#25928;&#65292;&#20445;&#30041;&#20102;&#19968;&#20123;&#20887;&#20313;&#32467;&#26500;&#65292;&#30041;&#19979;&#20102;&#21442;&#25968;&#20943;&#23569;&#30340;&#31354;&#38388;&#12290;&#21463;&#20808;&#21069;&#30340;&#31354;&#38388;&#21644;&#36890;&#36947;&#27880;&#24847;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#26469;&#25552;&#21462;&#23494;&#38598;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#21516;&#26102;&#21033;&#29992;Triplet SSM&#26469;&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;ISIC17&#12289;ISIC18&#12289;CVC-300&#12289;CVC-ClinicDB&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17701v1 Announce Type: cross  Abstract: Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain. Traditional convolutional neural networks (CNNs) and Transformer models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity. Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision. However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction. Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet. The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MEP&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#26680;&#20989;&#25968;&#29983;&#25104;&#20559;&#24046;&#26469;&#35299;&#20915;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#38271;&#24230;&#22806;&#25512;&#26102;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17698</link><description>&lt;p&gt;
MEP: &#22810;&#26680;&#23398;&#20064;&#22686;&#24378;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#38271;&#24230;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MEP&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#26680;&#20989;&#25968;&#29983;&#25104;&#20559;&#24046;&#26469;&#35299;&#20915;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#38271;&#24230;&#22806;&#25512;&#26102;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#39044;&#27979;&#30340;&#24207;&#21015;&#38271;&#24230;&#36229;&#36807;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#38271;&#24230;&#26102;&#65292;&#21464;&#21387;&#22120;&#30340;&#25512;&#29702;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#12290;&#29616;&#26377;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;ALiBi&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#20165;&#36890;&#36807;&#23454;&#29616;&#21333;&#20010;&#26680;&#20989;&#25968;&#26469;&#35299;&#20915;&#38271;&#24230;&#22806;&#25512;&#25361;&#25112;&#65292;&#36825;&#20250;&#26681;&#25454;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#20026;&#27599;&#20010;&#21518;Softmax&#27880;&#24847;&#21147;&#20998;&#25968;&#24341;&#20837;&#24658;&#23450;&#20559;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#26410;&#25506;&#35752;&#25110;&#20351;&#29992;&#22810;&#20010;&#26680;&#20989;&#25968;&#26469;&#24212;&#23545;&#22806;&#25512;&#25361;&#25112;&#12290;&#20511;&#37492;ALiBi&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;MEP&#65292;&#23427;&#37319;&#29992;&#21152;&#26435;&#24179;&#22343;&#26469;&#32467;&#21512;&#19981;&#21516;&#30340;&#26680;&#20989;&#25968;&#65288;&#22914;&#25351;&#25968;&#26680;&#21644;&#39640;&#26031;&#26680;&#65289;&#20135;&#29983;&#19968;&#20010;&#24212;&#29992;&#20110;&#21518;Softmax&#27880;&#24847;&#21147;&#20998;&#25968;&#30340;&#20559;&#24046;&#12290;&#26368;&#21021;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21508;&#31181;&#26680;&#20989;&#25968;&#26500;&#24314;&#22810;&#20010;&#26680;&#20989;&#25968;&#12290;&#27599;&#20010;&#26680;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17698v1 Announce Type: cross  Abstract: When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kern
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#20102;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#38750;&#23618;&#27425;Mamba&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#36830;&#32493;2D&#25195;&#25551;&#36807;&#31243;&#21644;&#26041;&#21521;&#24863;&#30693;&#26356;&#26032;&#65292;&#25552;&#39640;&#20102;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23398;&#20064;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.17695</link><description>&lt;p&gt;
PlainMamba&#65306;&#25913;&#36827;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#38750;&#23618;&#27425;Mamba
&lt;/p&gt;
&lt;p&gt;
PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17695
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20102;&#35270;&#35273;&#35782;&#21035;&#20013;&#30340;&#38750;&#23618;&#27425;Mamba&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#36830;&#32493;2D&#25195;&#25551;&#36807;&#31243;&#21644;&#26041;&#21521;&#24863;&#30693;&#26356;&#26032;&#65292;&#25552;&#39640;&#20102;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23398;&#20064;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;PlainMamba&#65306;&#19968;&#31181;&#31616;&#21333;&#30340;&#38750;&#23618;&#27425;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#65292;&#26088;&#22312;&#29992;&#20110;&#19968;&#33324;&#30340;&#35270;&#35273;&#35782;&#21035;&#12290;&#26368;&#36817;&#30340;Mamba&#27169;&#22411;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#39034;&#24207;&#25968;&#25454;&#19978;SSM&#21487;&#20197;&#19982;&#20854;&#20182;&#26550;&#26500;&#31454;&#20105;&#28608;&#28872;&#65292;&#24182;&#24050;&#21021;&#27493;&#23581;&#35797;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;Mamba&#30340;&#36873;&#25321;&#24615;&#25195;&#25551;&#36807;&#31243;&#20197;&#36866;&#24212;&#35270;&#35273;&#39046;&#22495;&#65292;&#36890;&#36807;&#65288;i&#65289;&#36890;&#36807;&#30830;&#20445;&#22312;&#25195;&#25551;&#24207;&#21015;&#20013;&#20196;&#29260;&#30456;&#37051;&#26469;&#25913;&#21892;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#36830;&#32493;2D&#25195;&#25551;&#36807;&#31243;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#21551;&#29992;&#27169;&#22411;&#21306;&#20998;&#20196;&#29260;&#30340;&#31354;&#38388;&#20851;&#31995;&#30340;&#26041;&#21521;&#24863;&#30693;&#26356;&#26032;&#65292;&#36890;&#36807;&#32534;&#30721;&#26041;&#21521;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#35774;&#35745;&#26131;&#20110;&#20351;&#29992;&#21644;&#26131;&#20110;&#25193;&#23637;&#65292;&#30001;&#22534;&#21472;&#30456;&#21516;&#30340;PlainMamba&#22359;&#24418;&#25104;&#65292;&#32467;&#26524;&#26159;&#22987;&#32456;&#20855;&#26377;&#24658;&#23450;&#23485;&#24230;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17695v1 Announce Type: cross  Abstract: We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition. The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images. In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information. Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers. The architecture is further simplified by removing the 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#27969;&#24418;&#24341;&#23548;&#30340;Lyapunov&#25511;&#21046;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#25509;&#36817;&#39044;&#23450;&#27969;&#24418;&#30340;&#28176;&#36817;&#31283;&#23450;&#21521;&#37327;&#22330;&#26469;&#29983;&#25104;&#31283;&#23450;&#25511;&#21046;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#22312;&#24555;&#36895;&#38646;-shot&#25511;&#21046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.17692</link><description>&lt;p&gt;
&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27969;&#24418;&#24341;&#23548;Lyapunov&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Manifold-Guided Lyapunov Control with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17692
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#27969;&#24418;&#24341;&#23548;&#30340;Lyapunov&#25511;&#21046;&#65292;&#36890;&#36807;&#35782;&#21035;&#26368;&#25509;&#36817;&#39044;&#23450;&#27969;&#24418;&#30340;&#28176;&#36817;&#31283;&#23450;&#21521;&#37327;&#22330;&#26469;&#29983;&#25104;&#31283;&#23450;&#25511;&#21046;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#22312;&#24555;&#36895;&#38646;-shot&#25511;&#21046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20026;&#19968;&#22823;&#31867;&#21160;&#24577;&#31995;&#32479;&#29983;&#25104;&#31283;&#23450;&#25511;&#21046;&#22120;&#12290;&#26680;&#24515;&#30446;&#26631;&#26159;&#36890;&#36807;&#35782;&#21035;&#19982;&#39044;&#23450;&#27969;&#24418;&#30456;&#23545;&#26368;&#36817;&#30340;&#28176;&#36817;&#31283;&#23450;&#21521;&#37327;&#22330;&#26469;&#24320;&#21457;&#31283;&#23450;&#25511;&#21046;&#20989;&#25968;&#65292;&#24182;&#26681;&#25454;&#36825;&#19968;&#21457;&#29616;&#35843;&#25972;&#25511;&#21046;&#20989;&#25968;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#22312;&#28176;&#36817;&#31283;&#23450;&#21521;&#37327;&#22330;&#21450;&#20854;&#23545;&#24212;&#30340;Lyapunov&#20989;&#25968;&#20043;&#38388;&#36827;&#34892;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#24555;&#36895;&#22320;&#23454;&#29616;&#23545;&#20808;&#21069;&#26410;&#35265;&#31995;&#32479;&#30340;&#31283;&#23450;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#24555;&#36895;&#38646;-shot&#25511;&#21046;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17692v1 Announce Type: cross  Abstract: This paper presents a novel approach to generating stabilizing controllers for a large class of dynamical systems using diffusion models. The core objective is to develop stabilizing control functions by identifying the closest asymptotically stable vector field relative to a predetermined manifold and adjusting the control function based on this finding. To achieve this, we employ a diffusion model trained on pairs consisting of asymptotically stable vector fields and their corresponding Lyapunov functions. Our numerical results demonstrate that this pre-trained model can achieve stabilization over previously unseen systems efficiently and rapidly, showcasing the potential of our approach in fast zero-shot control and generalizability.
&lt;/p&gt;</description></item><item><title>ABLQ&#26426;&#21046;&#22312;&#19981;&#21516;&#25209;&#22788;&#29702;&#37319;&#26679;&#19979;&#30340;&#38544;&#31169;&#20445;&#35777;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#65292;DP-SGD&#30340;&#23454;&#38469;&#23454;&#29616;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#27927;&#29260;&#30340;&#26041;&#27861;&#65292;&#20294;&#26356;&#21487;&#38752;&#30340;&#38544;&#31169;&#20998;&#26512;&#21364;&#26469;&#33258;&#20110;&#22522;&#20110;&#27850;&#26494;&#23376;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17673</link><description>&lt;p&gt;
DP-SGD&#30340;&#38544;&#31169;&#24615;&#26377;&#22810;&#31169;&#23494;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Private is DP-SGD?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17673
&lt;/p&gt;
&lt;p&gt;
ABLQ&#26426;&#21046;&#22312;&#19981;&#21516;&#25209;&#22788;&#29702;&#37319;&#26679;&#19979;&#30340;&#38544;&#31169;&#20445;&#35777;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#65292;DP-SGD&#30340;&#23454;&#38469;&#23454;&#29616;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#27927;&#29260;&#30340;&#26041;&#27861;&#65292;&#20294;&#26356;&#21487;&#38752;&#30340;&#38544;&#31169;&#20998;&#26512;&#21364;&#26469;&#33258;&#20110;&#22522;&#20110;&#27850;&#26494;&#23376;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25209;&#22788;&#29702;&#37319;&#26679;&#19979;&#65292;&#33258;&#36866;&#24212;&#25209;&#37327;&#32447;&#24615;&#26597;&#35810;&#65288;ABLQ&#65289;&#26426;&#21046;&#30340;&#38544;&#31169;&#20445;&#35777;&#20043;&#38388;&#23384;&#22312;&#30528;&#23454;&#36136;&#24615;&#24046;&#36317;&#65306;&#65288;i&#65289;&#27927;&#29260;&#65292;&#21644;&#65288;ii&#65289;&#27850;&#26494;&#23376;&#37319;&#26679;&#65307;&#20856;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20998;&#26512;&#36890;&#36807;&#23558;&#20854;&#35299;&#37322;&#20026;ABLQ&#30340;&#21518;&#22788;&#29702;&#26469;&#36827;&#34892;&#12290;&#34429;&#28982;&#22522;&#20110;&#27927;&#29260;&#30340;DP-SGD&#22312;&#23454;&#38469;&#23454;&#29616;&#20013;&#26356;&#24120;&#29992;&#65292;&#20294;&#23427;&#22312;&#38544;&#31169;&#20998;&#26512;&#19978;&#26082;&#19981;&#26131;&#20110;&#35299;&#26512;&#20063;&#19981;&#26131;&#20110;&#25968;&#20540;&#35745;&#31639;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#27850;&#26494;&#23376;&#37319;&#26679;&#30340;DP-SGD&#38590;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#20855;&#26377;&#33391;&#22909;&#29702;&#35299;&#30340;&#38544;&#31169;&#20998;&#26512;&#65292;&#26377;&#22810;&#20010;&#24320;&#28304;&#30340;&#25968;&#20540;&#32039;&#23494;&#30340;&#38544;&#31169;&#36134;&#25143;&#21487;&#29992;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#20570;&#27861;&#65292;&#21363;&#20351;&#29992;&#22522;&#20110;&#27927;&#29260;&#30340;DP-SGD&#65292;&#20294;&#20351;&#29992;&#30456;&#24212;&#27850;&#26494;&#23376;&#37319;&#26679;&#29256;&#26412;&#30340;&#38544;&#31169;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38544;&#31169;&#20998;&#26512;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17673v1 Announce Type: new  Abstract: We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis. On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy anal
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;CANOS&#65292;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#39044;&#27979;&#25509;&#36817;&#26368;&#20248;&#35299;&#65288;&#22312;1%&#20869;&#65289;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#21487;&#34892;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17660</link><description>&lt;p&gt;
CANOS&#65306;&#19968;&#31181;&#24555;&#36895;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;AC-OPF&#27714;&#35299;&#22120;&#65292;&#23545;N-1&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1 Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17660
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;CANOS&#65292;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#39044;&#27979;&#25509;&#36817;&#26368;&#20248;&#35299;&#65288;&#22312;1%&#20869;&#65289;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#21487;&#34892;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#21151;&#29575;&#27969;&#65288;OPF&#65289;&#28041;&#21450;&#19968;&#31995;&#21015;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#39640;&#25928;&#19988;&#23433;&#20840;&#22320;&#36816;&#34892;&#30005;&#21147;&#31995;&#32479;&#12290;&#22312;&#26368;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#65292;OPF&#30830;&#23450;&#38656;&#35201;&#21457;&#30005;&#22810;&#23569;&#30005;&#21147;&#65292;&#20197;&#22312;&#28385;&#36275;&#30005;&#21147;&#38656;&#27714;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#25104;&#26412;&#65292;&#24182;&#28385;&#36275;&#29289;&#29702;&#21644;&#36816;&#34892;&#32422;&#26463;&#12290;&#21363;&#20351;&#22312;&#26368;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#35299;&#20915;&#31934;&#30830;&#38382;&#39064;&#30340;&#36895;&#24230;&#19982;&#29616;&#20195;&#27714;&#35299;&#22120;&#36807;&#24930;&#65292;&#30005;&#21147;&#32593;&#26684;&#36816;&#33829;&#21830;&#20351;&#29992;AC-OPF&#38382;&#39064;&#30340;&#36817;&#20284;&#35299;&#12290;&#36825;&#20123;&#36817;&#20284;&#35299;&#20026;&#20102;&#36895;&#24230;&#32780;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#21644;&#36816;&#34892;&#21487;&#34892;&#24615;&#12290;&#36825;&#31181;&#26435;&#34913;&#23548;&#33268;&#20102;&#26114;&#36149;&#30340;&#8220;&#22686;&#20540;&#20184;&#27454;&#8221;&#21644;&#30899;&#25490;&#25918;&#30340;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#22411;&#30005;&#21147;&#32593;&#26684;&#26469;&#35828;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#65288;CANOS&#65289;&#65292;&#20197;&#39044;&#27979;&#25509;&#36817;&#26368;&#20248;&#35299;&#65288;&#30495;&#23454;AC-OPF&#25104;&#26412;&#30340;1%&#20869;&#65289;&#32780;&#19981;&#29306;&#29298;&#36895;&#24230;&#65288;&#36816;&#34892;&#26102;&#38388;&#20165;&#20026;33-65&#27627;&#31186;&#65289;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;CANOS&#21487;&#25193;&#23637;&#21040;&#36924;&#30495;&#30340;&#32593;&#26684;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17660v1 Announce Type: new  Abstract: Optimal Power Flow (OPF) refers to a wide range of related optimization problems with the goal of operating power systems efficiently and securely. In the simplest setting, OPF determines how much power to generate in order to minimize costs while meeting demand for power and satisfying physical and operational constraints. In even the simplest case, power grid operators use approximations of the AC-OPF problem because solving the exact problem is prohibitively slow with state-of-the-art solvers. These approximations sacrifice accuracy and operational feasibility in favor of speed. This trade-off leads to costly "uplift payments" and increased carbon emissions, especially for large power grids. In the present work, we train a deep learning system (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF cost) without compromising speed (running in as little as 33--65 ms). Importantly, CANOS scales to realistic grid sizes wi
&lt;/p&gt;</description></item><item><title>SGHormer&#26159;&#19968;&#31181;&#30001;&#33033;&#20914;&#39537;&#21160;&#30340;&#33410;&#33021;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#23558;&#20840;&#31934;&#24230;&#23884;&#20837;&#36716;&#25442;&#20026;&#31232;&#30095;&#21644;&#20108;&#20540;&#21270;&#33033;&#20914;&#20197;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#22270;&#21464;&#25442;&#22120;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17656</link><description>&lt;p&gt;
SGHormer&#65306;&#19968;&#31181;&#30001;&#33033;&#20914;&#39537;&#21160;&#30340;&#33410;&#33021;&#22270;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
SGHormer: An Energy-Saving Graph Transformer Driven by Spikes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17656
&lt;/p&gt;
&lt;p&gt;
SGHormer&#26159;&#19968;&#31181;&#30001;&#33033;&#20914;&#39537;&#21160;&#30340;&#33410;&#33021;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#23558;&#20840;&#31934;&#24230;&#23884;&#20837;&#36716;&#25442;&#20026;&#31232;&#30095;&#21644;&#20108;&#20540;&#21270;&#33033;&#20914;&#20197;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#39640;&#20102;&#22270;&#21464;&#25442;&#22120;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#22270;&#21464;&#25442;&#22120;&#65288;GTs&#65289;&#22312;&#21508;&#31181;&#22270;&#20219;&#21153;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;GTs&#20986;&#33394;&#24615;&#33021;&#32972;&#21518;&#30340;&#20195;&#20215;&#26159;&#26356;&#39640;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#35745;&#31639;&#24320;&#38144;&#12290;&#20256;&#32479;&#21464;&#25442;&#22120;&#20013;&#27880;&#24847;&#21147;&#35745;&#31639;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#21644;&#20108;&#27425;&#22797;&#26434;&#24230;&#20005;&#37325;&#24433;&#21709;&#20854;&#22312;&#22823;&#35268;&#27169;&#22270;&#25968;&#25454;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#22312;&#31616;&#21270;&#22359;&#20043;&#38388;&#30340;&#32452;&#21512;&#25110;&#27880;&#24847;&#21147;&#23398;&#20064;&#33539;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#20197;&#25552;&#39640;GTs&#30340;&#25928;&#29575;&#65292;&#20294;&#22312;&#26500;&#24314;GT&#26694;&#26550;&#26102;&#24456;&#23569;&#32771;&#34385;&#28304;&#33258;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#32467;&#26500;&#30340;&#19968;&#31995;&#21015;&#33410;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#22270;&#21464;&#25442;&#22120;&#65288;SGHormer&#65289;&#12290;&#23427;&#23558;&#20840;&#31934;&#24230;&#23884;&#20837;&#36716;&#25442;&#20026;&#31232;&#30095;&#21644;&#20108;&#20540;&#21270;&#33033;&#20914;&#20197;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;SGHormer&#20013;&#30340;&#33033;&#20914;&#22270;&#33258;&#27880;&#24847;&#21147;&#21644;&#33033;&#20914;&#20462;&#27491;&#22359;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17656v1 Announce Type: cross  Abstract: Graph Transformers (GTs) with powerful representation learning ability make a huge success in wide range of graph tasks. However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead. The complex structure and quadratic complexity during attention calculation in vanilla transformer seriously hinder its scalability on the large-scale graph data. Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs' efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework. To this end, we propose a new spiking-based graph transformer (SGHormer). It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs. The spiking graph self-attention and spiking rectify blocks in SGHorm
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#29615;&#22659;&#38543;&#26426;&#24615;&#65292;&#22312;&#39118;&#38505;&#25935;&#24863;&#21644;&#35268;&#36991;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.17646</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Distributional Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17646
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#29615;&#22659;&#38543;&#26426;&#24615;&#65292;&#22312;&#39118;&#38505;&#25935;&#24863;&#21644;&#35268;&#36991;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65292;&#22240;&#20854;&#20165;&#20381;&#36182;&#20110;&#35266;&#27979;&#25968;&#25454;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#20013;&#24515;&#20851;&#27880;&#28857;&#26159;&#36890;&#36807;&#37327;&#21270;&#19982;&#21508;&#31181;&#34892;&#21160;&#21644;&#29615;&#22659;&#38543;&#26426;&#24615;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30830;&#20445;&#25152;&#23398;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#20027;&#35201;&#24378;&#35843;&#36890;&#36807;&#23398;&#20064;&#39118;&#38505;&#35268;&#36991;&#31574;&#30053;&#26469;&#32531;&#35299;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#29615;&#22659;&#38543;&#26426;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21516;&#26102;&#22788;&#29702;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#29615;&#22659;&#38543;&#26426;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#39118;&#38505;&#35268;&#36991;&#31574;&#30053;&#24182;&#34920;&#24449;&#25240;&#29616;&#32047;&#31215;&#22870;&#21169;&#30340;&#25972;&#20010;&#20998;&#24067;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#22823;&#21270;&#32047;&#31215;&#25240;&#29616;&#22238;&#25253;&#30340;&#26399;&#26395;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#39118;&#38505;&#25935;&#24863;&#21644;&#39118;&#38505;&#35268;&#36991;&#35774;&#32622;&#19979;&#30340;&#20840;&#38754;&#23454;&#39564;&#24471;&#21040;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17646v1 Announce Type: new  Abstract: Offline reinforcement learning (RL) presents distinct challenges as it relies solely on observational data. A central concern in this context is ensuring the safety of the learned policy by quantifying uncertainties associated with various actions and environmental stochasticity. Traditional approaches primarily emphasize mitigating epistemic uncertainty by learning risk-averse policies, often overlooking environmental stochasticity. In this study, we propose an uncertainty-aware distributional offline RL method to simultaneously address both epistemic uncertainty and environmental stochasticity. We propose a model-free offline RL algorithm capable of learning risk-averse policies and characterizing the entire distribution of discounted cumulative rewards, as opposed to merely maximizing the expected value of accumulated discounted returns. Our method is rigorously evaluated through comprehensive experiments in both risk-sensitive and ri
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.17637</link><description>&lt;p&gt;
PeersimGym&#65306;&#29992;&#20110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17637
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21368;&#36733;&#23545;&#20110;&#22312;&#35832;&#22914;&#29289;&#32852;&#32593;&#20043;&#31867;&#30340;&#32593;&#32476;&#20013;&#24179;&#34913;&#35774;&#22791;&#30340;&#35745;&#31639;&#36127;&#36733;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38754;&#20020;&#30528;&#35832;&#22914;&#22312;&#20005;&#26684;&#30340;&#36890;&#20449;&#21644;&#23384;&#20648;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#24310;&#36831;&#21644;&#33021;&#28304;&#20351;&#29992;&#31561;&#37325;&#35201;&#20248;&#21270;&#25361;&#25112;&#12290;&#20256;&#32479;&#20248;&#21270;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65307;&#21551;&#21457;&#24335;&#26041;&#27861;&#32570;&#20047;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#20801;&#35768;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#23398;&#20064;&#26368;&#20339;&#21368;&#36733;&#31574;&#30053;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;RL &#30340;&#21151;&#25928;&#21462;&#20915;&#20110;&#23545;&#20016;&#23500;&#25968;&#25454;&#38598;&#21644;&#23450;&#21046;&#30340;&#29616;&#23454;&#35757;&#32451;&#29615;&#22659;&#30340;&#35775;&#38382;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; PeersimGym&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;PeersimGym &#25903;&#25345;&#21508;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#35745;&#31639;&#32422;&#26463;&#65292;&#24182;&#25972;&#21512;&#20102;&#19968;&#31181;"PettingZo"&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#37197;&#32622;&#20223;&#30495;&#21442;&#25968;&#21644;&#30417;&#25511;&#20223;&#30495;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17637v1 Announce Type: cross  Abstract: Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \textit{PettingZo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39034;&#24207;&#20915;&#31574;&#24314;&#27169;&#20026;&#25512;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36974;&#32617;&#37197;&#32622;&#26469;&#37325;&#26032;&#35299;&#37322;RLRS&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.17634</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#36866;&#24212;&#36974;&#32617;&#30340;&#20445;&#30041;&#20915;&#31574;&#21464;&#21387;&#22120;&#29992;&#20110;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RL&#25512;&#33616;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39034;&#24207;&#20915;&#31574;&#24314;&#27169;&#20026;&#25512;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36974;&#32617;&#37197;&#32622;&#26469;&#37325;&#26032;&#35299;&#37322;RLRS&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#65288;RLRS&#65289;&#22312;&#19968;&#31995;&#21015;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20174;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#21040;&#27969;&#23186;&#20307;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#21046;&#23450;&#22870;&#21169;&#20989;&#25968;&#21644;&#21033;&#29992;RL&#26694;&#26550;&#20013;&#30340;&#22823;&#22411;&#29616;&#26377;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#31163;&#32447;RLRS&#30340;&#25216;&#26415;&#36827;&#27493;&#20026;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#22312;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#26102;&#21487;&#33021;&#20250;&#24341;&#20837;&#19982;&#35745;&#31639;&#36164;&#28304;&#21644;&#35757;&#32451;&#25104;&#26412;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20027;&#27969;&#26041;&#27861;&#20351;&#29992;&#22266;&#23450;&#38271;&#24230;&#30340;&#36755;&#20837;&#36712;&#36857;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#33719;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;&#21916;&#22909;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;RLRS&#26041;&#27861;&#26469;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#39034;&#24207;&#20915;&#31574;&#24314;&#27169;&#20026;&#25512;&#29702;&#20219;&#21153;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#36974;&#32617;&#37197;&#32622;&#26469;&#37325;&#26032;&#35299;&#37322;RLRS&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17634v1 Announce Type: cross  Abstract: Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#24037;&#20855;&#22312;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#33021;&#32791;&#24314;&#27169;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;</title><link>https://arxiv.org/abs/2403.17632</link><description>&lt;p&gt;
&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#33021;&#32791;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#24037;&#20855;&#22312;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#33021;&#32791;&#24314;&#27169;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#25317;&#22581;&#21644;&#29615;&#22659;&#24694;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#26085;&#30410;&#21152;&#21095;&#65292;&#20984;&#26174;&#20102;&#22312;&#22478;&#24066;&#31354;&#38388;&#25512;&#34892;E-Mobility&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;E-&#28369;&#26495;&#36710;&#21644;E-&#33258;&#34892;&#36710;&#31561;&#24494;&#22411;E-Mobility&#24037;&#20855;&#22312;&#36825;&#19968;&#36716;&#21464;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#22478;&#24066;&#36890;&#21220;&#32773;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#30340;&#33021;&#32791;&#27169;&#24335;&#26159;&#24433;&#21709;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#23545;&#20110;&#20986;&#34892;&#35268;&#21010;&#20197;&#21450;&#22686;&#24378;&#29992;&#25143;&#22312;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#26102;&#30340;&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#38024;&#23545;&#29305;&#23450;&#31227;&#21160;&#24037;&#20855;&#21644;&#26465;&#20214;&#23450;&#21046;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#24443;&#24213;&#27169;&#22411;&#35780;&#20272;&#21644;&#39564;&#35777;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29233;&#23572;&#20848;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#33021;&#32791;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17632v1 Announce Type: new  Abstract: The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for ene
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#23384;&#22312;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#30456;&#20851;&#30340;&#20559;&#35265;&#65292;&#21435;&#38500;&#36825;&#20123;&#20559;&#35265;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;JPEG&#21387;&#32553;&#30340;&#31283;&#20581;&#24615;&#24182;&#26174;&#33879;&#25913;&#21464;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17608</link><description>&lt;p&gt;
&#20266;&#36896;&#36824;&#26159;JPEG&#65311;&#25581;&#31034;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#24120;&#35265;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17608
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#23384;&#22312;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#30456;&#20851;&#30340;&#20559;&#35265;&#65292;&#21435;&#38500;&#36825;&#20123;&#20559;&#35265;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;JPEG&#21387;&#32553;&#30340;&#31283;&#20581;&#24615;&#24182;&#26174;&#33879;&#25913;&#21464;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#20984;&#26174;&#20102;&#26816;&#27979;&#20154;&#36896;&#20869;&#23481;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#36825;&#26159;&#25171;&#20987;&#24191;&#27867;&#25805;&#32437;&#21644;&#35823;&#23548;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26816;&#27979;&#22120;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#25968;&#25454;&#38598;&#19981;&#32463;&#24847;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#20559;&#35265;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26816;&#27979;&#22120;&#30340;&#25928;&#26524;&#21644;&#35780;&#20272;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#35768;&#22810;&#29992;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#26377;&#20851;&#30340;&#20559;&#35265;&#12290;&#20351;&#29992;GenImage&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#26816;&#27979;&#22120;&#30830;&#23454;&#20174;&#36825;&#20123;&#19981;&#21463;&#27426;&#36814;&#30340;&#22240;&#32032;&#20013;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#21435;&#38500;&#36825;&#20123;&#21629;&#21517;&#20559;&#35265;&#20250;&#26174;&#33879;&#22686;&#21152;&#38024;&#23545;JPEG&#21387;&#32553;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#26174;&#33879;&#25913;&#21464;&#35780;&#20272;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;ResNet50&#21644;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17608v1 Announce Type: cross  Abstract: The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and S
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17601</link><description>&lt;p&gt;
LASIL&#65306;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#38271;&#26399;&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17601
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#22312;&#20132;&#36890;&#24037;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#21333;&#20010;&#36710;&#36742;&#34892;&#20026;&#21644;&#25972;&#20307;&#20132;&#36890;&#27969;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#19968;&#20010;&#30495;&#23454;&#30340;&#27169;&#25311;&#22120;&#65292;&#31934;&#30830;&#22797;&#21046;&#21508;&#31181;&#20132;&#36890;&#26465;&#20214;&#19979;&#30340;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#20381;&#36182;&#21551;&#21457;&#24335;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#24448;&#24448;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20132;&#36890;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#32780;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#30340;&#27169;&#25311;&#12290;&#30001;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#31283;&#23450;&#30340;&#38271;&#26399;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#19987;&#23478;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#22686;&#24378;&#29366;&#24577;&#24847;&#35782;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17601v1 Announce Type: new  Abstract: Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#33391;&#24615;&#36807;&#25311;&#21512;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#20102;&#24658;&#23450;&#30340;&#36229;&#20986;&#20998;&#24067;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.17592</link><description>&lt;p&gt;
&#23545;&#36229;&#21442;&#25968;&#21270;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
On the Benefits of Over-parameterization for Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17592
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#33391;&#24615;&#36807;&#25311;&#21512;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#20102;&#24658;&#23450;&#30340;&#36229;&#20986;&#20998;&#24067;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#22522;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#24456;&#23481;&#26131;&#34987;&#36829;&#21453;&#65292;&#23548;&#33268;&#20102;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#38382;&#39064;&#12290;&#29702;&#35299;&#29616;&#20195;&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#24179;&#20961;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#34892;&#20026;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#30446;&#21069;&#23545;&#20854;&#22312;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#26159;&#19981;&#36275;&#30340;&#12290;&#29616;&#26377;&#30340;&#29702;&#35770;&#24037;&#20316;&#24120;&#24120;&#20026;OOD&#22330;&#26223;&#20013;&#30340;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#25552;&#20379;&#26080;&#24847;&#20041;&#30340;&#32467;&#26524;&#65292;&#29978;&#33267;&#19982;&#23454;&#35777;&#32467;&#26524;&#30456;&#30683;&#30462;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27491;&#22312;&#30740;&#31350;&#22312;&#19968;&#33324;&#33391;&#24615;&#36807;&#25311;&#21512;&#26465;&#20214;&#19979;&#65292;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#22312;OOD&#27867;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#19978;&#65292;&#24182;&#30740;&#31350;&#38750;&#24179;&#20961;&#33258;&#28982;&#20998;&#24067;&#20559;&#31227;&#65292;&#20854;&#20013;&#33391;&#24615;&#36807;&#25311;&#21512;&#20272;&#35745;&#22120;&#23637;&#31034;&#20986;&#24658;&#23450;&#30340;&#36807;&#22823;OOD&#25439;&#22833;&#65292;&#23613;&#31649;&#36798;&#21040;&#20102;&#38646;&#36807;&#22823;i
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17592v1 Announce Type: new  Abstract: In recent years, machine learning models have achieved success based on the independently and identically distributed assumption. However, this assumption can be easily violated in real-world applications, leading to the Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized DNNs behave under non-trivial natural distributional shifts is essential, as current theoretical understanding is insufficient. Existing theoretical works often provide meaningless results for over-parameterized models in OOD scenarios or even contradict empirical findings. To this end, we are investigating the performance of the over-parameterized model in terms of OOD generalization under the general benign overfitting conditions. Our analysis focuses on a random feature model and examines non-trivial natural distributional shifts, where the benign overfitting estimators demonstrate a constant excess OOD loss, despite achieving zero excess i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#23384;&#20648;&#32593;&#32476;&#30340;&#22810;&#21151;&#33021;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22312;&#38646;&#27425;&#36866;&#24212;&#12289;&#23569;&#27425;&#36866;&#24212;&#21644;&#26080;&#38656;&#35757;&#32451;&#30340;&#23569;&#27425;&#36866;&#24212;&#19977;&#31181;&#35774;&#32622;&#19979;&#39640;&#25928;&#36816;&#34892;</title><link>https://arxiv.org/abs/2403.17589</link><description>&lt;p&gt;
&#21452;&#23384;&#20648;&#32593;&#32476;&#65306;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#23384;&#20648;&#32593;&#32476;&#30340;&#22810;&#21151;&#33021;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22312;&#38646;&#27425;&#36866;&#24212;&#12289;&#23569;&#27425;&#36866;&#24212;&#21644;&#26080;&#38656;&#35757;&#32451;&#30340;&#23569;&#27425;&#36866;&#24212;&#19977;&#31181;&#35774;&#32622;&#19979;&#39640;&#25928;&#36816;&#34892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20687;CLIP&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;&#20309;&#23558;&#23427;&#20204;&#35843;&#25972;&#21040;&#21508;&#31181;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#24050;&#32463;&#24341;&#36215;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#37325;&#35270;&#12290;&#35813;&#36866;&#24212;&#31574;&#30053;&#36890;&#24120;&#21487;&#20197;&#24402;&#31867;&#20026;&#19977;&#31181;&#33539;&#24335;&#65306;&#38646;&#27425;&#36866;&#24212;&#12289;&#23569;&#27425;&#36866;&#24212;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#23569;&#27425;&#36866;&#24212;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#29305;&#23450;&#35774;&#32622;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#21482;&#33021;&#28385;&#36275;&#20854;&#20013;&#19968;&#31181;&#25110;&#20004;&#31181;&#33539;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#36825;&#19977;&#31181;&#35774;&#32622;&#19979;&#36816;&#34892;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#23384;&#20648;&#32593;&#32476;&#65292;&#21253;&#25324;&#21160;&#24577;&#21644;&#38745;&#24577;&#35760;&#24518;&#32452;&#20214;&#12290;&#38745;&#24577;&#35760;&#24518;&#32531;&#23384;&#35757;&#32451;&#25968;&#25454;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#23569;&#27425;&#36866;&#24212;&#65292;&#32780;&#21160;&#24577;&#35760;&#24518;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#22312;&#32447;&#20445;&#23384;&#21382;&#21490;&#27979;&#35797;&#29305;&#24449;&#65292;&#20801;&#35768;&#25506;&#32034;&#36229;&#20986;&#35770;&#25991;&#20013;&#24050;&#35757;&#32451;&#25968;&#25454;&#30340;&#39069;&#22806;&#25968;&#25454;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17589v1 Announce Type: cross  Abstract: With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the
&lt;/p&gt;</description></item><item><title>Forest-ORE&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#35268;&#21017;&#38598;&#20351;&#38543;&#26426;&#26862;&#26519;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20840;&#23616;&#21644;&#23616;&#37096;&#35299;&#37322;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24433;&#21709;&#21487;&#35299;&#37322;&#35268;&#21017;&#38598;&#36873;&#25321;&#30340;&#22810;&#20010;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.17588</link><description>&lt;p&gt;
Forest-ORE: &#25366;&#25496;&#26368;&#20339;&#35268;&#21017;&#38598;&#20197;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17588
&lt;/p&gt;
&lt;p&gt;
Forest-ORE&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#35268;&#21017;&#38598;&#20351;&#38543;&#26426;&#26862;&#26519;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20840;&#23616;&#21644;&#23616;&#37096;&#35299;&#37322;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24433;&#21709;&#21487;&#35299;&#37322;&#35268;&#21017;&#38598;&#36873;&#25321;&#30340;&#22810;&#20010;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17588v1&#23459;&#24067;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#20197;&#20854;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#38395;&#21517;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#12290; &#30001;&#20110;&#26377;&#25968;&#30334;&#20010;&#28145;&#23618;&#20915;&#31574;&#26641;&#65292;&#23427;&#20063;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#40657;&#30418;&#12290; &#36825;&#31181;&#32570;&#20047;&#35299;&#37322;&#24615;&#21487;&#20197;&#25104;&#20026;RF&#27169;&#22411;&#22312;&#20960;&#31181;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#34987;&#25509;&#21463;&#30340;&#19968;&#20010;&#30495;&#27491;&#32570;&#28857;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#24433;&#21709;&#20010;&#20154;&#29983;&#27963;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#23433;&#20840;&#21644;&#27861;&#24459;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;Forest-ORE&#65292;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#30340;&#35268;&#21017;&#38598;&#65288;ORE&#65289;&#20351;RF&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#29992;&#20110;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#12290; &#19981;&#21516;&#20110;&#20854;&#20182;&#26088;&#22312;&#35299;&#37322;RF&#27169;&#22411;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#20960;&#20010;&#24433;&#21709;&#36873;&#25321;&#21487;&#35299;&#37322;&#35268;&#21017;&#38598;&#30340;&#21442;&#25968;&#12290; &#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#39044;&#27979;&#24615;&#33021;&#32622;&#20110;&#21487;&#35299;&#37322;&#24615;&#35206;&#30422;&#33539;&#22260;&#20043;&#19978;&#65292;&#24182;&#19988;&#19981;&#25552;&#20379;&#26377;&#20851;&#29616;&#26377;&#35268;&#21017;&#20043;&#38388;&#30340;&#37325;&#21472;&#25110;&#20132;&#20114;&#20316;&#29992;&#30340;&#20449;&#24687;&#12290; Forest-ORE&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;prog
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17588v1 Announce Type: new  Abstract: Random Forest (RF) is well-known as an efficient ensemble learning method in terms of predictive performance. It is also considered a Black Box because of its hundreds of deep decision trees. This lack of interpretability can be a real drawback for acceptance of RF models in several real-world applications, especially those affecting one's lives, such as in healthcare, security, and law. In this work, we present Forest-ORE, a method that makes RF interpretable via an optimized rule ensemble (ORE) for local and global interpretation. Unlike other rule-based approaches aiming at interpreting the RF model, this method simultaneously considers several parameters that influence the choice of an interpretable rule ensemble. Existing methods often prioritize predictive performance over interpretability coverage and do not provide information about existing overlaps or interactions between rules. Forest-ORE uses a mixed-integer optimization prog
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#23545;&#35805;&#26641;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35757;&#32451;&#20986;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20195;&#29702;&#36798;&#21040;&#19982;&#22312;&#20154;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17582</link><description>&lt;p&gt;
&#26397;&#30528;&#38646;&#25968;&#25454;&#12289;&#21487;&#25511;&#12289;&#33258;&#36866;&#24212;&#23545;&#35805;&#31995;&#32479;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards a Zero-Data, Controllable, Adaptive Dialog System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#23545;&#35805;&#26641;&#29983;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#35757;&#32451;&#20986;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20195;&#29702;&#36798;&#21040;&#19982;&#22312;&#20154;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#26641;&#25628;&#32034;&#65288;V&#228;th&#31561;&#65292;2023&#24180;&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#23545;&#35805;&#31995;&#32479;&#25511;&#21046;&#26041;&#27861;&#65292;&#20854;&#20013;&#39046;&#22495;&#19987;&#23478;&#36890;&#36807;&#23545;&#35805;&#26641;&#22609;&#36896;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#20195;&#29702;&#23398;&#20250;&#26377;&#25928;&#22320;&#27983;&#35272;&#36825;&#26869;&#26641;&#65292;&#21516;&#26102;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#65292;&#20363;&#22914;&#39046;&#22495;&#29087;&#24713;&#24230;&#12290;&#28982;&#32780;&#65292;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#38459;&#30861;&#20102;&#22312;&#26032;&#39046;&#22495;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30452;&#25509;&#20174;&#23545;&#35805;&#26641;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25913;&#36827;&#20102;&#21407;&#22987;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#20195;&#29702;&#21487;&#20197;&#23454;&#29616;&#19982;&#22312;&#20154;&#31867;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#23545;&#35805;&#25104;&#21151;&#29575;&#65292;&#26080;&#35770;&#26159;&#20351;&#29992;&#21830;&#19994;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#36824;&#26159;&#20351;&#29992;&#36739;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;GPU&#19978;&#36816;&#34892;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25910;&#38598;&#21644;&#27979;&#35797;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65306;ONBOARD&#65292;&#19968;&#20010;&#24110;&#21161;&#22806;&#22269;&#23621;&#27665;&#25644;&#36801;&#30340;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17582v1 Announce Type: cross  Abstract: Conversational Tree Search (V\"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32852;&#37030;&#31169;&#26377;&#26412;&#22320;&#35757;&#32451;&#31639;&#27861;&#65288;Fed-PLT&#65289;&#65292;&#36890;&#36807;&#20801;&#35768;&#37096;&#20998;&#21442;&#19982;&#21644;&#26412;&#22320;&#35757;&#32451;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36890;&#20449;&#36718;&#27425;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#26469;&#22686;&#24378;&#38544;&#31169;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17572</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Privacy in Federated Learning through Local Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17572
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32852;&#37030;&#31169;&#26377;&#26412;&#22320;&#35757;&#32451;&#31639;&#27861;&#65288;Fed-PLT&#65289;&#65292;&#36890;&#36807;&#20801;&#35768;&#37096;&#20998;&#21442;&#19982;&#21644;&#26412;&#22320;&#35757;&#32451;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36890;&#20449;&#36718;&#27425;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#26412;&#22320;&#35757;&#32451;&#26469;&#22686;&#24378;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#32852;&#37030;&#31169;&#26377;&#26412;&#22320;&#35757;&#32451;&#31639;&#27861;&#65288;Fed-PLT&#65289;&#65292;&#20197;&#20811;&#26381;&#65288;i&#65289;&#26114;&#36149;&#30340;&#36890;&#20449;&#21644;&#65288;ii&#65289;&#38544;&#31169;&#20445;&#25252;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20801;&#35768;&#37096;&#20998;&#21442;&#19982;&#21644;&#26412;&#22320;&#35757;&#32451;&#26469;&#35299;&#20915;&#65288;i&#65289;&#65292;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#20013;&#22830;&#21327;&#35843;&#21592;&#21644;&#35745;&#31639;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#36718;&#27425;&#12290;&#31639;&#27861;&#22312;&#26412;&#22320;&#35757;&#32451;&#30340;&#20351;&#29992;&#19978;&#36798;&#21040;&#20102;&#30446;&#21069;&#25216;&#26415;&#27700;&#24179;&#65292;&#21487;&#20197;&#35777;&#26126;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#20195;&#29702;&#21487;&#20197;&#28789;&#27963;&#36873;&#25321;&#21508;&#31181;&#26412;&#22320;&#35757;&#32451;&#27714;&#35299;&#22120;&#65292;&#22914;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#21644;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#35757;&#32451;&#26469;&#22686;&#24378;&#38544;&#31169;&#24615;&#65292;&#35299;&#20915;&#20102;&#28857;&#65288;ii&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#24046;&#20998;&#38544;&#31169;&#30028;&#38480;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#23545;&#26412;&#22320;&#35757;&#32451;&#32426;&#20803;&#25968;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17572v1 Announce Type: new  Abstract: In this paper we propose the federated private local training algorithm (Fed-PLT) for federated learning, to overcome the challenges of (i) expensive communications and (ii) privacy preservation. We address (i) by allowing for both partial participation and local training, which significantly reduce the number of communication rounds between the central coordinator and computing agents. The algorithm matches the state of the art in the sense that the use of local training demonstrably does not impact accuracy. Additionally, agents have the flexibility to choose from various local training solvers, such as (stochastic) gradient descent and accelerated gradient descent. Further, we investigate how employing local training can enhance privacy, addressing point (ii). In particular, we derive differential privacy bounds and highlight their dependence on the number of local training epochs. We assess the effectiveness of the proposed algorithm
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#21457;&#23637;</title><link>https://arxiv.org/abs/2403.17561</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21450;&#20854;&#26368;&#26032;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning and State-of-the-arts Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17561
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;, &#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#23618;&#20114;&#36830;&#21333;&#20803;&#65288;&#31070;&#32463;&#20803;&#65289;&#20174;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#21644;&#34920;&#31034;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#21463;&#21040;&#36825;&#31181;&#23398;&#20064;&#33021;&#21147;&#30340;&#36171;&#33021;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26159;&#35768;&#22810;&#31361;&#30772;&#24615;&#25216;&#26415;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#39537;&#21160;&#21147;&#12290;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#29616;&#23454;&#38382;&#39064;&#30340;&#21160;&#24577;&#24615;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31867;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26368;&#26032;&#21457;&#23637;&#30340;&#35206;&#30422;&#38754;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#36825;&#20123;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17561v1 Announce Type: new  Abstract: Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DeepMIF&#65292;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31995;&#32479;&#38598;&#25104;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#22312;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#20013;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#65292;&#36991;&#20813;&#20102;LiDAR&#27979;&#37327;&#30340;&#22024;&#26434;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17550</link><description>&lt;p&gt;
DeepMIF: &#29992;&#20110;&#22823;&#35268;&#27169;LiDAR 3D&#22320;&#22270;&#32472;&#21046;&#30340;&#28145;&#24230;&#21333;&#35843;&#38544;&#24335;&#22330;
&lt;/p&gt;
&lt;p&gt;
DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17550
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DeepMIF&#65292;&#36890;&#36807;&#35774;&#35745;&#23398;&#20064;&#31995;&#32479;&#38598;&#25104;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#22312;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#20013;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#65292;&#36991;&#20813;&#20102;LiDAR&#27979;&#37327;&#30340;&#22024;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#20195;&#33719;&#21462;&#35774;&#22791;&#22914;LiDAR&#20256;&#24863;&#22120;&#65292;&#22312;&#24863;&#30693;&#30495;&#23454;&#22823;&#35268;&#27169;&#23460;&#22806;3D&#29615;&#22659;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#25104;&#31264;&#23494;&#12289;&#23436;&#25972;&#30340;3D&#22330;&#26223;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#21644;&#21487;&#20248;&#21270;&#29305;&#24449;&#32593;&#26684;&#65292;&#20197;&#36924;&#36817;3D&#22330;&#26223;&#30340;&#34920;&#38754;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#27839;&#21407;&#22987;LiDAR&#20809;&#32447;&#25311;&#21512;&#26679;&#26412;&#20250;&#23548;&#33268;&#30001;&#20110;&#31232;&#30095;&#12289;&#20114;&#30456;&#30683;&#30462;&#30340;LiDAR&#27979;&#37327;&#30340;&#29305;&#24615;&#32780;&#20135;&#29983;&#22024;&#26434;&#30340;3D&#32472;&#22270;&#32467;&#26524;&#12290;&#30456;&#21453;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20877;&#31934;&#30830;&#25311;&#21512;LiDAR&#25968;&#25454;&#65292;&#32780;&#26159;&#35753;&#32593;&#32476;&#20248;&#21270;&#22312;3D&#31354;&#38388;&#20013;&#23450;&#20041;&#30340;&#38750;&#24230;&#37327;&#21333;&#35843;&#38544;&#24335;&#22330;&#12290;&#20026;&#36866;&#24212;&#25105;&#20204;&#30340;&#22330;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#31995;&#32479;&#65292;&#38598;&#25104;&#20102;&#19968;&#20010;&#21333;&#35843;&#24615;&#25439;&#22833;&#65292;&#20351;&#24471;&#33021;&#22815;&#20248;&#21270;&#31070;&#32463;&#21333;&#35843;&#22330;&#24182;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;3D&#22320;&#22270;&#32472;&#21046;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17550v1 Announce Type: cross  Abstract: Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm ac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20215;&#20540;&#24046;&#24322;&#21644;&#29366;&#24577;&#35745;&#25968;&#65292;&#21033;&#29992;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#30450;&#30446;&#20999;&#25442;&#26426;&#21046;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.17542</link><description>&lt;p&gt;
VDSC&#65306;&#21033;&#29992;&#20215;&#20540;&#24046;&#24322;&#21644;&#29366;&#24577;&#35745;&#25968;&#22686;&#24378;&#25506;&#32034;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17542
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20215;&#20540;&#24046;&#24322;&#21644;&#29366;&#24577;&#35745;&#25968;&#65292;&#21033;&#29992;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#30450;&#30446;&#20999;&#25442;&#26426;&#21046;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#20110;&#8220;&#25506;&#32034;&#22810;&#23569;&#8221;&#21644;&#8220;&#22914;&#20309;&#25506;&#32034;&#8221;&#38382;&#39064;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#20294;&#23545;&#20110;&#8220;&#20309;&#26102;&#8221;&#25506;&#32034;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26356;&#22797;&#26434;&#30340;&#25506;&#32034;&#31574;&#30053;&#21487;&#20197;&#22312;&#29305;&#23450;&#30340;&#12289;&#36890;&#24120;&#31232;&#30095;&#30340;&#22870;&#21169;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#22914;$\epsilon$-&#36138;&#24515;&#65292;&#22312;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#20013;&#32487;&#32493;&#34920;&#29616;&#20248;&#24322;&#12290;&#36825;&#20123;&#31616;&#21333;&#31574;&#30053;&#30340;&#21560;&#24341;&#21147;&#22312;&#20110;&#23427;&#20204;&#30340;&#26131;&#23454;&#29616;&#24615;&#21644;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#26222;&#36941;&#36866;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#32570;&#28857;&#22312;&#20110;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#19968;&#31181;&#30450;&#30446;&#30340;&#20999;&#25442;&#26426;&#21046;&#65292;&#23436;&#20840;&#24573;&#30053;&#20102;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#26469;&#20915;&#23450;&#8220;&#20309;&#26102;&#8221;&#36827;&#34892;&#25506;&#32034;&#65292;&#20174;&#32780;&#35299;&#20915;&#30450;&#30446;&#20999;&#25442;&#26426;&#21046;&#30340;&#32570;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#31283;&#24577;&#65288;VDSC&#65289;&#25552;&#20986;&#20102;&#20215;&#20540;&#24046;&#24322;&#21644;&#29366;&#24577;&#35745;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17542v1 Announce Type: cross  Abstract: Despite the considerable attention given to the questions of \textit{how much} and \textit{how to} explore in deep reinforcement learning, the investigation into \textit{when} to explore remains relatively less researched. While more sophisticated exploration strategies can excel in specific, often sparse reward environments, existing simpler approaches, such as $\epsilon$-greedy, persist in outperforming them across a broader spectrum of domains. The appeal of these simpler strategies lies in their ease of implementation and generality across a wide range of domains. The downside is that these methods are essentially a blind switching mechanism, which completely disregards the agent's internal state. In this paper, we propose to leverage the agent's internal state to decide \textit{when} to explore, addressing the shortcomings of blind switching mechanisms. We present Value Discrepancy and State Counts through homeostasis (VDSC), a no
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;BVR&#20307;&#32946;&#39302;&#65292;&#29992;&#20110;&#25506;&#31350;&#36229;&#35270;&#36317;&#31354;&#20013;&#25112;&#26007;&#39046;&#22495;&#30340;&#28508;&#22312;&#25112;&#26415;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#24320;&#28304;&#39134;&#34892;&#21160;&#21147;&#23398;&#27169;&#25311;&#22120;JSBSim&#30340;&#39640;&#20445;&#30495;&#24230;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2403.17533</link><description>&lt;p&gt;
BVR&#20307;&#32946;&#39302;&#65306;&#19968;&#31181;&#36229;&#35270;&#36317;&#31354;&#25112;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range Air Combat
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17533
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;BVR&#20307;&#32946;&#39302;&#65292;&#29992;&#20110;&#25506;&#31350;&#36229;&#35270;&#36317;&#31354;&#20013;&#25112;&#26007;&#39046;&#22495;&#30340;&#28508;&#22312;&#25112;&#26415;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#24320;&#28304;&#39134;&#34892;&#21160;&#21147;&#23398;&#27169;&#25311;&#22120;JSBSim&#30340;&#39640;&#20445;&#30495;&#24230;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#26032;&#30340;&#31354;&#25112;&#25112;&#26415;&#21644;&#21457;&#29616;&#26032;&#30340;&#26426;&#21160;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#39134;&#34892;&#21592;&#30340;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#31181;&#19981;&#21516;&#30340;&#20316;&#25112;&#22330;&#26223;&#65292;&#30456;&#21516;&#30340;&#31574;&#30053;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#35774;&#22791;&#24615;&#33021;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#20250;&#26497;&#22823;&#22320;&#25913;&#21464;&#31354;&#20013;&#25112;&#26007;&#32467;&#26524;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#20197;&#24110;&#21161;&#35843;&#26597;&#36229;&#35270;&#36317;&#65288;BVR&#65289;&#31354;&#20013;&#25112;&#26007;&#39046;&#22495;&#28508;&#22312;&#30340;&#31354;&#25112;&#25112;&#26415;&#65306;BVR&#20307;&#32946;&#39302;&#12290;&#36825;&#31181;&#31354;&#20013;&#25112;&#26007;&#26159;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36828;&#31243;&#23548;&#24377;&#36890;&#24120;&#26159;&#31354;&#20013;&#25112;&#26007;&#20013;&#39318;&#20808;&#34987;&#20351;&#29992;&#30340;&#27494;&#22120;&#12290;&#19968;&#20123;&#29616;&#26377;&#29615;&#22659;&#25552;&#20379;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#65292;&#20294;&#35201;&#20040;&#19981;&#26159;&#24320;&#28304;&#30340;&#65292;&#35201;&#20040;&#27809;&#26377;&#36866;&#24212;&#20110;BVR&#31354;&#25112;&#39046;&#22495;&#12290;&#20854;&#20182;&#29615;&#22659;&#26159;&#24320;&#28304;&#30340;&#65292;&#20294;&#20351;&#29992;&#19981;&#22826;&#20934;&#30830;&#30340;&#27169;&#25311;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#24320;&#28304;&#39134;&#34892;&#21160;&#21147;&#23398;&#27169;&#25311;&#22120;JSBSim&#30340;&#39640;&#20445;&#30495;&#24230;&#29615;&#22659;&#65292;&#24182;&#36866;&#24212;&#20110;BVR&#31354;&#25112;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17533v1 Announce Type: new  Abstract: Creating new air combat tactics and discovering novel maneuvers can require numerous hours of expert pilots' time. Additionally, for each different combat scenario, the same strategies may not work since small changes in equipment performance may drastically change the air combat outcome. For this reason, we created a reinforcement learning environment to help investigate potential air combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR Gym. This type of air combat is important since long-range missiles are often the first weapon to be used in aerial combat. Some existing environments provide high-fidelity simulations but are either not open source or are not adapted to the BVR air combat domain. Other environments are open source but use less accurate simulation models. Our work provides a high-fidelity environment based on the open-source flight dynamics simulator JSBSim and is adapted to the BVR air combat dom
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Fisher-Rao&#33539;&#25968;&#27491;&#21017;&#21270;&#65292;&#26412;&#30740;&#31350;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26631;&#20934;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#22797;&#26434;&#24615;&#35282;&#24230;&#23545;&#27492;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.17520</link><description>&lt;p&gt;
&#36890;&#36807;Fisher-Rao&#33539;&#25968;&#27491;&#21017;&#21270;&#25552;&#21319;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Boosting Adversarial Training via Fisher-Rao Norm-based Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17520
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Fisher-Rao&#33539;&#25968;&#27491;&#21017;&#21270;&#65292;&#26412;&#30740;&#31350;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26631;&#20934;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#22797;&#26434;&#24615;&#35282;&#24230;&#23545;&#27492;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#20013;&#20943;&#36731;&#26631;&#20934;&#27867;&#21270;&#24615;&#33021;&#30340;&#19979;&#38477;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#35270;&#35282;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;Fisher-Rao&#33539;&#25968;&#65292;&#19968;&#20010;&#22312;&#27169;&#22411;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#20960;&#20309;&#19981;&#21464;&#24230;&#37327;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;ReLU&#28608;&#27963;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;Cross-Entropy Loss-based Rademacher&#22797;&#26434;&#24230;&#30340;&#38750;&#24179;&#20961;&#30028;&#38480;&#12290;&#28982;&#21518;&#25105;&#20204;&#25512;&#24191;&#20102;&#19968;&#20010;&#19982;&#27169;&#22411;&#23485;&#24230;&#21464;&#21270;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26435;&#34913;&#22240;&#32032;&#25935;&#24863;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#30456;&#20851;&#21464;&#37327;&#12290;&#27492;&#22806;&#65292;&#22823;&#37327;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#27492;&#21464;&#37327;&#19982;&#23545;&#25239;&#35757;&#32451;&#21644;&#26631;&#20934;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;Cross-Entropy loss&#30340;&#27867;&#21270;&#24046;&#36317;&#39640;&#24230;&#30456;&#20851;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#30340;&#21021;&#22987;&#21644;&#26368;&#32456;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17520v1 Announce Type: new  Abstract: Adversarial training is extensively utilized to improve the adversarial robustness of deep neural networks. Yet, mitigating the degradation of standard generalization performance in adversarial-trained models remains an open problem. This paper attempts to resolve this issue through the lens of model complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant metric for model complexity, to establish the non-trivial bounds of the Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer Perceptron. Then we generalize a complexity-related variable, which is sensitive to the changes in model width and the trade-off factors in adversarial training. Moreover, intensive empirical evidence validates that this variable highly correlates with the generalization gap of Cross-Entropy loss between adversarial-trained and standard-trained models, especially during the initial and final phases of the training p
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#30340;&#39044;&#27979;&#20998;&#20139;&#26041;&#38754;&#30340;&#26032;&#39062;&#20043;&#22788;&#65292;&#24182;&#20171;&#32461;&#21644;&#31361;&#20986;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#20849;&#20139;&#21512;&#21516;&#12290;</title><link>https://arxiv.org/abs/2403.17515</link><description>&lt;p&gt;
&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#30340;&#39044;&#27979;&#20998;&#20139;
&lt;/p&gt;
&lt;p&gt;
Prediction-sharing During Training and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17515
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#30340;&#39044;&#27979;&#20998;&#20139;&#26041;&#38754;&#30340;&#26032;&#39062;&#20043;&#22788;&#65292;&#24182;&#20171;&#32461;&#21644;&#31361;&#20986;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#20849;&#20139;&#21512;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#23478;&#20844;&#21496;&#21442;&#19982;&#31454;&#20105;&#24615;&#39044;&#27979;&#20219;&#21153;&#12290;&#27599;&#23478;&#20844;&#21496;&#26377;&#20004;&#20010;&#25968;&#25454;&#26469;&#28304; -- &#26377;&#26631;&#31614;&#30340;&#21382;&#21490;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#30340;&#25512;&#29702;&#26102;&#38388;&#25968;&#25454; -- &#24182;&#19988;&#20351;&#29992;&#21069;&#32773;&#21046;&#23450;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#29992;&#21518;&#32773;&#23545;&#26032;&#23454;&#20363;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20844;&#21496;&#20043;&#38388;&#30340;&#25968;&#25454;&#20849;&#20139;&#21512;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#20171;&#32461;&#21644;&#31361;&#20986;&#20165;&#20998;&#20139;&#39044;&#27979;&#27169;&#22411;&#30340;&#21512;&#21516;&#12289;&#20165;&#20998;&#20139;&#25512;&#29702;&#26102;&#38388;&#39044;&#27979;&#30340;&#21512;&#21516;&#20197;&#21450;&#20998;&#20139;&#20004;&#32773;&#30340;&#21512;&#21516;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20998;&#20026;&#19977;&#20010;&#23618;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20415;&#20110;&#36827;&#34892;&#30740;&#31350;&#30340;&#19968;&#33324;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#28966;&#28857;&#32553;&#23567;&#21040;&#36825;&#20010;&#26694;&#26550;&#20869;&#30340;&#20004;&#20010;&#33258;&#28982;&#35774;&#32622;&#65306;(i) &#27599;&#23478;&#20844;&#21496;&#30340;&#39044;&#27979;&#27169;&#22411;&#20934;&#30830;&#24230;&#26159;&#20849;&#30693;&#30340;&#65292;&#20294;&#21508;&#33258;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26410;&#30693;&#65307;(ii) &#23384;&#22312;&#20004;&#20010;&#20851;&#20110;&#26368;&#20248;&#39044;&#27979;&#22120;&#30340;&#20551;&#35774;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17515v1 Announce Type: cross  Abstract: Two firms are engaged in a competitive prediction task. Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;EL-MLFFs&#65292;&#21033;&#29992;&#22534;&#21472;&#26041;&#27861;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;MLFFs&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17507</link><description>&lt;p&gt;
EL-MLFFs&#65306;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#30340;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17507
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;EL-MLFFs&#65292;&#21033;&#29992;&#22534;&#21472;&#26041;&#27861;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;MLFFs&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#65288;MLFFs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24357;&#21512;&#37327;&#23376;&#21147;&#23398;&#26041;&#27861;&#30340;&#31934;&#30830;&#24615;&#21644;&#32463;&#20856;&#21147;&#22330;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;MLFF&#27169;&#22411;&#30340;&#20016;&#23500;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#21407;&#23376;&#21147;&#30340;&#25361;&#25112;&#32473;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#26694;&#26550;EL-MLFFs&#65292;&#21033;&#29992;&#22534;&#21472;&#26041;&#27861;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;MLFFs&#30340;&#39044;&#27979;&#65292;&#22686;&#24378;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#26500;&#24314;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#24182;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#20803;&#27169;&#22411;&#65292;EL-MLFFs&#26377;&#25928;&#22320;&#25429;&#25417;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#24182;&#25913;&#36827;&#21147;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#30002;&#28919;&#20998;&#23376;&#21644;&#21560;&#38468;&#22312;Cu&#65288;100&#65289;&#34920;&#38754;&#19978;&#30340;&#30002;&#37255;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EL-MLFFs&#30456;&#23545;&#20110;&#21333;&#20010;MLFF&#26174;&#33879;&#25552;&#39640;&#20102;&#21147;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17507v1 Announce Type: new  Abstract: Machine learning force fields (MLFFs) have emerged as a promising approach to bridge the accuracy of quantum mechanical methods and the efficiency of classical force fields. However, the abundance of MLFF models and the challenge of accurately predicting atomic forces pose significant obstacles in their practical application. In this paper, we propose a novel ensemble learning framework, EL-MLFFs, which leverages the stacking method to integrate predictions from diverse MLFFs and enhance force prediction accuracy. By constructing a graph representation of molecular structures and employing a graph neural network (GNN) as the meta-model, EL-MLFFs effectively captures atomic interactions and refines force predictions. We evaluate our approach on two distinct datasets: methane molecules and methanol adsorbed on a Cu(100) surface. The results demonstrate that EL-MLFFs significantly improves force prediction accuracy compared to individual ML
&lt;/p&gt;</description></item><item><title>DS-AL&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;&#20998;&#26512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#27969;&#21644;&#34917;&#20607;&#27969;&#30456;&#32467;&#21512;&#65292;&#37325;&#26032;&#23450;&#20041;CIL&#38382;&#39064;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26080;&#26679;&#26412;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17503</link><description>&lt;p&gt;
DS-AL&#65306;&#19968;&#31181;&#38754;&#21521;&#26080;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#21452;&#27969;&#20998;&#26512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17503
&lt;/p&gt;
&lt;p&gt;
DS-AL&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;&#20998;&#26512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#27969;&#21644;&#34917;&#20607;&#27969;&#30456;&#32467;&#21512;&#65292;&#37325;&#26032;&#23450;&#20041;CIL&#38382;&#39064;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26080;&#26679;&#26412;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#26679;&#26412;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;(CIL)&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#36981;&#24490;&#27492;&#32422;&#26463;&#26465;&#20214;&#30340;&#26041;&#27861;&#24448;&#24448;&#27604;&#20445;&#30041;&#23545;&#36807;&#21435;&#26679;&#26412;&#35775;&#38382;&#26435;&#30340;&#22238;&#25918;&#25216;&#26415;&#26356;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#26080;&#26679;&#26412;CIL&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;&#20998;&#26512;&#23398;&#20064;(DS-AL)&#26041;&#27861;&#12290;DS-AL&#21253;&#21547;&#19968;&#20010;&#20027;&#27969;&#25552;&#20379;&#20998;&#26512;&#65288;&#21363;&#38381;&#24335;&#65289;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#19968;&#20010;&#25913;&#21892;&#30001;&#20110;&#37319;&#29992;&#32447;&#24615;&#26144;&#23556;&#32780;&#22266;&#26377;&#30340;&#27424;&#25311;&#21512;&#38480;&#21046;&#30340;&#34917;&#20607;&#27969;&#12290;&#20027;&#27969;&#23558;CIL&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#36830;&#25509;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;(C-RLS)&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;CIL&#21450;&#20854;&#32852;&#21512;&#23398;&#20064;&#23545;&#24212;&#20219;&#21153;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#34917;&#20607;&#27969;&#30001;&#21452;&#28608;&#27963;&#34917;&#20607;(DAC)&#27169;&#22359;&#25511;&#21046;&#12290;&#35813;&#27169;&#22359;&#20351;&#29992;&#19981;&#21516;&#20110;&#20027;&#27969;&#30340;&#28608;&#27963;&#20989;&#25968;&#37325;&#26032;&#28608;&#27963;&#23884;&#20837;&#65292;&#24182;&#23547;&#25214;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17503v1 Announce Type: new  Abstract: Class-incremental learning (CIL) under an exemplar-free constraint has presented a significant challenge. Existing methods adhering to this constraint are prone to catastrophic forgetting, far more so than replay-based techniques that retain access to past samples. In this paper, to solve the exemplar-free CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The DS-AL contains a main stream offering an analytical (i.e., closed-form) linear solution, and a compensation stream improving the inherent under-fitting limitation due to adopting linear mapping. The main stream redefines the CIL problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an equivalence between the CIL and its joint-learning counterpart. The compensation stream is governed by a Dual-Activation Compensation (DAC) module. This module re-activates the embedding with a different activation function from the main stream one, and seek
&lt;/p&gt;</description></item><item><title>VGAE&#26694;&#26550;&#22312;&#24402;&#32435;&#23398;&#20064;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.17500</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#24402;&#32435;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#21322;&#30417;&#30563;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Graph Auto-Encoder Based Inductive Learning Method for Semi-Supervised Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17500
&lt;/p&gt;
&lt;p&gt;
VGAE&#26694;&#26550;&#22312;&#24402;&#32435;&#23398;&#20064;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#26159;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30740;&#31350;&#38382;&#39064;&#65292;&#20854;&#20013;&#24402;&#32435;&#23398;&#20064;&#38382;&#39064;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#27010;&#25324;&#21040;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#24402;&#32435;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#23436;&#20840;&#30417;&#30563;&#30340;&#35757;&#32451;&#35774;&#32622;&#19979;&#20005;&#37325;&#20381;&#36182;&#20110;&#26631;&#35760;&#33410;&#28857;&#12290;&#19982;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VGAEs&#65289;&#34987;&#35748;&#20026;&#26356;&#20855;&#19968;&#33324;&#24615;&#65292;&#33021;&#22815;&#25429;&#25417;&#22270;&#30340;&#20869;&#37096;&#32467;&#26500;&#20449;&#24687;&#65292;&#29420;&#31435;&#20110;&#33410;&#28857;&#26631;&#31614;&#65292;&#24182;&#22312;&#22810;&#20010;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#20173;&#28982;&#32570;&#20047;&#20851;&#20110;&#21033;&#29992;VGAE&#26694;&#26550;&#36827;&#34892;&#24402;&#32435;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#30001;&#20110;&#22312;&#30417;&#30563;&#26041;&#24335;&#19979;&#35757;&#32451;&#27169;&#22411;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17500v1 Announce Type: new  Abstract: Graph representation learning is a fundamental research issue in various domains of applications, of which the inductive learning problem is particularly challenging as it requires models to generalize to unseen graph structures during inference. In recent years, graph neural networks (GNNs) have emerged as powerful graph models for inductive learning tasks such as node classification, whereas they typically heavily rely on the annotated nodes under a fully supervised training setting. Compared with the GNN-based methods, variational graph auto-encoders (VGAEs) are known to be more generalizable to capture the internal structural information of graphs independent of node labels and have achieved prominent performance on multiple unsupervised learning tasks. However, so far there is still a lack of work focusing on leveraging the VGAE framework for inductive learning, due to the difficulties in training the model in a supervised manner an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#22312;&#32447;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#35843;&#33410;&#27963;&#21160;&#26381;&#21153;&#22120;&#25968;&#37327;&#26368;&#23567;&#21270;&#20316;&#19994;&#24310;&#36831;&#65292;&#24341;&#20837;&#20102;&#20999;&#25442;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#31454;&#20105;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17480</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#23384;&#21644;&#20999;&#25442;&#25104;&#26412;&#30340;&#22312;&#32447;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#23481;&#37327;&#35843;&#37197;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Capacity Provisioning Motivated Online Non-Convex Optimization Problem with Memory and Switching Cost
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17480
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#22312;&#32447;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#35843;&#33410;&#27963;&#21160;&#26381;&#21153;&#22120;&#25968;&#37327;&#26368;&#23567;&#21270;&#20316;&#19994;&#24310;&#36831;&#65292;&#24341;&#20837;&#20102;&#20999;&#25442;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#31454;&#20105;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#20102;&#19968;&#31181;&#22312;&#32447;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#35843;&#33410;&#27963;&#21160;&#26381;&#21153;&#22120;&#30340;&#25968;&#37327;&#26469;&#26368;&#23567;&#21270;&#19968;&#32452;&#20316;&#19994;&#30340;&#27969;&#37327;&#26102;&#38388;&#65288;&#24635;&#24310;&#36831;&#65289;&#65292;&#20294;&#22312;&#26102;&#38388;&#21464;&#21270;&#26102;&#25913;&#21464;&#27963;&#21160;&#26381;&#21153;&#22120;&#25968;&#37327;&#20250;&#20135;&#29983;&#20999;&#25442;&#25104;&#26412;&#12290;&#27599;&#20010;&#20316;&#19994;&#22312;&#20219;&#20309;&#26102;&#38388;&#20869;&#26368;&#22810;&#21487;&#20197;&#30001;&#19968;&#20010;&#22266;&#23450;&#36895;&#24230;&#30340;&#26381;&#21153;&#22120;&#22788;&#29702;&#12290;&#19982;&#36890;&#24120;&#20855;&#26377;&#20999;&#25442;&#25104;&#26412;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#38382;&#39064;&#30456;&#27604;&#65292;&#25152;&#32771;&#34385;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;&#38750;&#20984;&#30340;&#65292;&#24182;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#65292;&#23427;&#21462;&#20915;&#20110;&#25152;&#26377;&#36807;&#21435;&#30340;&#20915;&#31574;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24403;&#21069;&#30340;&#20915;&#31574;&#12290;&#32771;&#34385;&#20102;&#26368;&#22351;&#24773;&#20917;&#21644;&#38543;&#26426;&#36755;&#20837;&#65307;&#23545;&#20110;&#36825;&#20004;&#31181;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#31454;&#20105;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17480v1 Announce Type: cross  Abstract: An online non-convex optimization problem is considered where the goal is to minimize the flow time (total delay) of a set of jobs by modulating the number of active servers, but with a switching cost associated with changing the number of active servers over time. Each job can be processed by at most one fixed speed server at any time. Compared to the usual online convex optimization (OCO) problem with switching cost, the objective function considered is non-convex and more importantly, at each time, it depends on all past decisions and not just the present one. Both worst-case and stochastic inputs are considered; for both cases, competitive algorithms are derived.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#38656;&#27714;&#24322;&#21619;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#25490;&#21517;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#21487;&#27979;&#35797;&#24615;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#34913;&#37327;&#21644;&#37327;&#21270;&#38656;&#27714;&#30340;&#21487;&#27979;&#35797;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17479</link><description>&lt;p&gt;
&#22522;&#20110;&#38656;&#27714;&#24322;&#21619;&#30340;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#21487;&#27979;&#35797;&#24615;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Natural Language Requirements Testability Measurement Based on Requirement Smells
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#38656;&#27714;&#24322;&#21619;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#25490;&#21517;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#21487;&#27979;&#35797;&#24615;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#34913;&#37327;&#21644;&#37327;&#21270;&#38656;&#27714;&#30340;&#21487;&#27979;&#35797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#27714;&#26500;&#25104;&#20102;&#23450;&#20041;&#36719;&#20214;&#31995;&#32479;&#20041;&#21153;&#21644;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;&#21487;&#27979;&#35797;&#30340;&#38656;&#27714;&#26377;&#21161;&#20110;&#38450;&#27490;&#22833;&#36133;&#65292;&#38477;&#20302;&#32500;&#25252;&#25104;&#26412;&#65292;&#24182;&#31616;&#21270;&#39564;&#25910;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#34913;&#37327;&#21644;&#37327;&#21270;&#38656;&#27714;&#21487;&#27979;&#35797;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#23578;&#26410;&#25552;&#20986;&#22522;&#20110;&#38656;&#27714;&#24322;&#21619;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#34913;&#37327;&#38656;&#27714;&#30340;&#21487;&#27979;&#35797;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#27169;&#22411;&#65292;&#20197;&#35780;&#20272;&#21644;&#25490;&#21517;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#30340;&#21487;&#27979;&#35797;&#24615;&#65292;&#22522;&#20110;&#19968;&#20010;&#24191;&#27867;&#30340;&#20061;&#20010;&#38656;&#27714;&#24322;&#21619;&#38598;&#21512;&#65292;&#33258;&#21160;&#26816;&#27979;&#65292;&#24182;&#26681;&#25454;&#38656;&#27714;&#38271;&#24230;&#21644;&#20854;&#24212;&#29992;&#39046;&#22495;&#26469;&#30830;&#23450;&#39564;&#25910;&#27979;&#35797;&#24037;&#20316;&#30340;&#21162;&#21147;&#12290;&#22823;&#22810;&#25968;&#24322;&#21619;&#28304;&#20110;&#19981;&#21487;&#25968;&#30340;&#24418;&#23481;&#35789;&#65292;&#19978;&#19979;&#25991;&#25935;&#24863;&#21644;&#27169;&#31946;&#35789;&#12290;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#23383;&#20856;&#26469;&#26816;&#27979;&#36825;&#20123;&#35789;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31070;&#32463;&#35789;&#23884;&#20837;&#25216;&#26415;&#26469;&#29983;&#25104;&#36825;&#26679;&#19968;&#20010;&#23383;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17479v1 Announce Type: cross  Abstract: Requirements form the basis for defining software systems' obligations and tasks. Testable requirements help prevent failures, reduce maintenance costs, and make it easier to perform acceptance tests. However, despite the importance of measuring and quantifying requirements testability, no automatic approach for measuring requirements testability has been proposed based on the requirements smells, which are at odds with the requirements testability. This paper presents a mathematical model to evaluate and rank the natural language requirements testability based on an extensive set of nine requirements smells, detected automatically, and acceptance test efforts determined by requirement length and its application domain. Most of the smells stem from uncountable adjectives, context-sensitive, and ambiguous words. A comprehensive dictionary is required to detect such words. We offer a neural word-embedding technique to generate such a dic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#31070;&#32463;&#20869;&#26680;(UNK)&#65292;&#21487;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#23398;&#20064;&#27493;&#39588;&#19979;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;NTK&#30340;&#34892;&#20026;&#65292;&#24403;&#23398;&#20064;&#27493;&#39588;&#36924;&#36817;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;NNGP&#12290;</title><link>https://arxiv.org/abs/2403.17467</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
A Unified Kernel for Neural Network Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#31070;&#32463;&#20869;&#26680;(UNK)&#65292;&#21487;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#23398;&#20064;&#27493;&#39588;&#19979;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;NTK&#30340;&#34892;&#20026;&#65292;&#24403;&#23398;&#20064;&#27493;&#39588;&#36924;&#36817;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;NNGP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20869;&#26680;&#23398;&#20064;&#20043;&#38388;&#30340;&#21306;&#21035;&#21644;&#32852;&#31995;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#22312;&#36830;&#25509;&#26080;&#38480;&#23485;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#26031;&#36807;&#31243;&#26041;&#38754;&#21462;&#24471;&#20102;&#29702;&#35770;&#19978;&#30340;&#36827;&#23637;&#12290;&#20986;&#29616;&#20102;&#20004;&#31181;&#20027;&#27969;&#26041;&#27861;&#65306;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;(NNGP)&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#12290;&#21069;&#32773;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20195;&#34920;&#20102;&#38646;&#38454;&#26680;&#65292;&#32780;&#21518;&#32773;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20999;&#21521;&#31354;&#38388;&#65292;&#26159;&#31532;&#19968;&#38454;&#26680;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32479;&#19968;&#31070;&#32463;&#20869;&#26680;(UNK)&#65292;&#35813;&#20869;&#26680;&#34920;&#24449;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#26799;&#24230;&#19979;&#38477;&#21644;&#21442;&#25968;&#21021;&#22987;&#21270;&#20013;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25152;&#25552;&#20986;&#30340;UNK&#20869;&#26680;&#20445;&#25345;&#20102;NNGP&#21644;NTK&#30340;&#26497;&#38480;&#29305;&#24615;&#65292;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;NTK&#30340;&#34892;&#20026;&#65292;&#20294;&#26377;&#26377;&#38480;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#24182;&#19988;&#24403;&#23398;&#20064;&#27493;&#39588;&#25509;&#36817;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;NNGP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#23545;UNK&#20869;&#26680;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17467v1 Announce Type: cross  Abstract: Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning. Recent advancements have made theoretical progress in connecting infinite-wide neural networks and Gaussian processes. Two predominant approaches have emerged: the Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel. In this paper, we present the Unified Neural Kernel (UNK), which characterizes the learning dynamics of neural networks with gradient descents and parameter initialization. The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity. Besides, we also theoreticall
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#19981;&#21516;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21457;&#29616;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#21462;&#20915;&#20110;&#22806;&#37096;&#21464;&#37327;&#65292;&#22914;&#25915;&#20987;&#31867;&#22411;&#12289;&#22797;&#26434;&#24615;&#21644;&#32593;&#32476;&#29615;&#22659;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#24182;&#38750;&#22987;&#32456;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.17458</link><description>&lt;p&gt;
&#26399;&#26395;&#19982;&#29616;&#23454;&#65306;&#23454;&#36341;&#20013;&#35780;&#20272;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17458
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#19981;&#21516;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#21457;&#29616;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#21462;&#20915;&#20110;&#22806;&#37096;&#21464;&#37327;&#65292;&#22914;&#25915;&#20987;&#31867;&#22411;&#12289;&#22797;&#26434;&#24615;&#21644;&#32593;&#32476;&#29615;&#22659;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#24182;&#38750;&#22987;&#32456;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#26368;&#36817;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23458;&#35266;&#27604;&#36739;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#26681;&#25454;&#20854;&#38656;&#27714;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#27809;&#26377;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#26368;&#22909;&#30340;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#22806;&#37096;&#21464;&#37327;&#65292;&#22914;&#25915;&#20987;&#31867;&#22411;&#12289;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#38598;&#20013;&#30340;&#32593;&#32476;&#29615;&#22659;&#12290;&#20363;&#22914;&#65292;BoT_IoT&#21644;Stratosphere IoT&#25968;&#25454;&#38598;&#37117;&#25429;&#33719;&#20102;&#19982;&#29289;&#32852;&#32593;&#30456;&#20851;&#30340;&#25915;&#20987;&#65292;&#20294;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;BoT_IoT&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;&#22312;&#20351;&#29992;Stratosphere IoT&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#26102;HELAD&#34920;&#29616;&#26368;&#20339;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#39640;&#30340;&#24179;&#22343;F1&#20998;&#25968;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20351;&#29992;&#25991;&#29486;&#21644;&#39033;&#30446;&#23384;&#20648;&#24211;&#20013;&#30340;IDS&#30340;&#22256;&#38590;&#65292;&#36825;&#20351;&#24471;&#23601;IDS&#36873;&#25321;&#24471;&#20986;&#26126;&#30830;&#32467;&#35770;&#21464;&#24471;&#22797;&#26434;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17458v1 Announce Type: cross  Abstract: Our paper provides empirical comparisons between recent IDSs to provide an objective comparison between them to help users choose the most appropriate solution based on their requirements. Our results show that no one solution is the best, but is dependent on external variables such as the types of attacks, complexity, and network environment in the dataset. For example, BoT_IoT and Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural network performed the best when tested using the BoT_IoT dataset while HELAD performed the best when tested using the Stratosphere IoT dataset. So although we found that a deep neural network solution had the highest average F1 scores on tested datasets, it is not always the best-performing one. We further discuss difficulties in using IDS from literature and project repositories, which complicated drawing definitive conclusions regarding IDS selection.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17456</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Cost-Constrained Behaviors in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#22797;&#26434;&#30340;&#35745;&#21010;&#21644;&#35843;&#24230;&#38382;&#39064;&#19968;&#30452;&#36890;&#36807;&#21508;&#31181;&#20248;&#21270;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#30340;&#27169;&#20223;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#27169;&#22411;&#25110;&#30452;&#25509;&#34892;&#20026;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26080;&#38480;&#21046;&#35774;&#32622;&#19979;&#30340;&#27169;&#20223;&#65288;&#20363;&#22914;&#65292;&#36710;&#36742;&#28040;&#32791;&#30340;&#29123;&#27833;&#37327;&#27809;&#26377;&#38480;&#21046;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#19987;&#23478;&#30340;&#34892;&#20026;&#19981;&#20165;&#21463;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#30340;&#24433;&#21709;&#65292;&#36824;&#21463;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#33258;&#21160;&#39550;&#39542;&#36865;&#36135;&#36710;&#30340;&#20915;&#31574;&#19981;&#20165;&#21462;&#20915;&#20110;&#36335;&#24452;&#20559;&#22909;/&#22870;&#21169;&#65288;&#26681;&#25454;&#36807;&#21435;&#30340;&#38656;&#27714;&#25968;&#25454;&#65289;&#65292;&#36824;&#21462;&#20915;&#20110;&#36710;&#36742;&#20869;&#30340;&#29123;&#27833;&#21644;&#36865;&#36798;&#26102;&#38388;&#31561;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21387;&#32553;&#38142;&#8221;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#21270;&#12289;&#21098;&#26525;&#12289;&#25552;&#21069;&#36864;&#20986;&#21644;&#30693;&#35782;&#33976;&#39311;&#31561;&#24120;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2403.17447</link><description>&lt;p&gt;
&#21387;&#32553;&#38142;&#65306;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#32452;&#21512;&#21387;&#32553;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21387;&#32553;&#38142;&#8221;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#21270;&#12289;&#21098;&#26525;&#12289;&#25552;&#21069;&#36864;&#20986;&#21644;&#30693;&#35782;&#33976;&#39311;&#31561;&#24120;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27969;&#34892;&#65292;&#20294;&#23427;&#20204;&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#26041;&#38754;&#30340;&#23494;&#38598;&#24615;&#32473;&#36164;&#28304;&#26377;&#38480;&#30340;&#35745;&#31639;&#31995;&#32479;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#23454;&#26102;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#20943;&#36731;&#36127;&#25285;&#65292;&#27169;&#22411;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#35768;&#22810;&#26041;&#27861;&#65292;&#22914;&#37327;&#21270;&#12289;&#21098;&#26525;&#12289;&#25552;&#21069;&#36864;&#20986;&#21644;&#30693;&#35782;&#33976;&#39311;&#24050;&#32463;&#35777;&#26126;&#20102;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#20013;&#20887;&#20313;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#21487;&#20197;&#26126;&#26174;&#30475;&#20986;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#21033;&#29992;&#20102;&#20854;&#29420;&#29305;&#30340;&#29305;&#24615;&#26469;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#24403;&#23427;&#20204;&#32467;&#21512;&#22312;&#19968;&#36215;&#26102;&#20063;&#21487;&#20197;&#23637;&#29616;&#20986;&#20114;&#34917;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#20174;&#20114;&#34917;&#29305;&#24615;&#20013;&#33719;&#30410;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21387;&#32553;&#38142;&#65292;&#23427;&#22312;&#32452;&#21512;&#24207;&#21015;&#19978;&#25805;&#20316;&#65292;&#24212;&#29992;&#36825;&#20123;&#24120;&#35265;&#25216;&#26415;&#26469;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17447v1 Announce Type: new  Abstract: Convolutional neural networks (CNNs) have achieved significant popularity, but their computational and memory intensity poses challenges for resource-constrained computing systems, particularly with the prerequisite of real-time performance. To release this burden, model compression has become an important research focus. Many approaches like quantization, pruning, early exit, and knowledge distillation have demonstrated the effect of reducing redundancy in neural networks. Upon closer examination, it becomes apparent that each approach capitalizes on its unique features to compress the neural network, and they can also exhibit complementary behavior when combined. To explore the interactions and reap the benefits from the complementary features, we propose the Chain of Compression, which works on the combinational sequence to apply these common techniques to compress the neural network. Validated on the image-based regression and classi
&lt;/p&gt;</description></item><item><title>&#23558;&#31616;&#21333;&#30340;&#25351;&#25968;&#24179;&#28369;&#27861;&#19982;MLP&#32467;&#21512;&#65292;&#36890;&#36807;&#22686;&#21152;&#21442;&#25968;&#21644;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#22797;&#26434;S4&#27169;&#22411;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;</title><link>https://arxiv.org/abs/2403.17445</link><description>&lt;p&gt;
&#23558;&#25351;&#25968;&#24179;&#28369;&#27861;&#34701;&#20837;MLP&#65306;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17445
&lt;/p&gt;
&lt;p&gt;
&#23558;&#31616;&#21333;&#30340;&#25351;&#25968;&#24179;&#28369;&#27861;&#19982;MLP&#32467;&#21512;&#65292;&#36890;&#36807;&#22686;&#21152;&#21442;&#25968;&#21644;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#22797;&#26434;S4&#27169;&#22411;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#25968;&#25454;&#20013;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#26159;&#24207;&#21015;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26368;&#36817;&#21457;&#23637;&#30340;&#27169;&#22411;&#8220;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#8221;&#65288;S4&#65289;&#22312;&#24314;&#27169;&#38271;&#26399;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;S4&#30340;&#25104;&#21151;&#26159;&#22240;&#20026;&#20854;&#22797;&#26434;&#30340;&#21442;&#25968;&#21270;&#21644;HiPPO&#21021;&#22987;&#21270;&#36824;&#26159;&#20165;&#20165;&#30001;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#35752;&#28145;&#24230;SSMs&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#20174;&#31616;&#21333;&#30340;SSM&#25351;&#25968;&#24179;&#28369;&#65288;ETS&#65289;&#24320;&#22987;&#65292;&#24182;&#36890;&#36807;&#30452;&#25509;&#23558;&#20854;&#34701;&#20837;&#36880;&#20803;&#32032;MLP&#25552;&#20986;&#20102;&#19968;&#20010;&#21472;&#21152;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#23383;&#27573;&#26469;&#25193;&#20805;&#31616;&#21333;&#30340;ETS&#20197;&#20943;&#23569;&#24402;&#32435;&#20559;&#24046;&#12290;&#23613;&#31649;&#22312;&#36880;&#20803;&#32032;MLP&#30340;&#21442;&#25968;&#22686;&#21152;&#19981;&#21040;1%&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;LRA&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#19982;S4&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17445v1 Announce Type: cross  Abstract: Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.
&lt;/p&gt;</description></item><item><title>&#22312;ALICE&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31890;&#23376;&#35782;&#21035;&#65292;&#21253;&#25324;&#29305;&#24449;&#38598;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#22312;&#19981;&#23436;&#25972;&#25968;&#25454;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;ML&#39033;&#30446;&#19982;ALICE&#20998;&#26512;&#36719;&#20214;&#38598;&#25104;&#65292;&#35752;&#35770;&#20102;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.17436</link><description>&lt;p&gt;
&#22312;ALICE&#23454;&#39564;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#36827;&#34892;&#31890;&#23376;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Particle identification with machine learning from incomplete data in the ALICE experiment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17436
&lt;/p&gt;
&lt;p&gt;
&#22312;ALICE&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31890;&#23376;&#35782;&#21035;&#65292;&#21253;&#25324;&#29305;&#24449;&#38598;&#23884;&#20837;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#22312;&#19981;&#23436;&#25972;&#25968;&#25454;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;ML&#39033;&#30446;&#19982;ALICE&#20998;&#26512;&#36719;&#20214;&#38598;&#25104;&#65292;&#35752;&#35770;&#20102;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LHC&#30340;ALICE&#23454;&#39564;&#27979;&#37327;&#22312;&#36229;&#30456;&#23545;&#35770;&#37325;&#31163;&#23376;&#23545;&#25758;&#20013;&#24418;&#25104;&#30340;&#24378;&#30456;&#20114;&#20316;&#29992;&#29289;&#36136;&#30340;&#24615;&#36136;&#12290;&#36825;&#20123;&#30740;&#31350;&#38656;&#35201;&#20934;&#30830;&#30340;&#31890;&#23376;&#35782;&#21035;(PID)&#12290;ALICE&#36890;&#36807;&#20960;&#20010;&#25506;&#27979;&#22120;&#20026;&#21160;&#37327;&#20174;&#32422;100 MeV/c&#21040;20 GeV/c&#30340;&#31890;&#23376;&#25552;&#20379;PID&#20449;&#24687;&#12290;&#20256;&#32479;&#19978;&#65292;&#31890;&#23376;&#26159;&#36890;&#36807;&#30697;&#24418;&#20999;&#21106;&#36827;&#34892;&#36873;&#25321;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;(ML)&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;(NN)&#20316;&#20026;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29305;&#24449;&#38598;&#23884;&#20837;&#21644;&#20851;&#27880;&#25193;&#23637;&#20102;&#31890;&#23376;&#20998;&#31867;&#22120;&#65292;&#20197;&#20415;&#23545;&#21253;&#21547;&#19981;&#23436;&#25972;&#26679;&#26412;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;ML&#39033;&#30446;&#19982;ALICE&#20998;&#26512;&#36719;&#20214;&#30340;&#38598;&#25104;&#65292;&#24182;&#35752;&#35770;&#20102;&#22495;&#33258;&#36866;&#24212;&#65292;&#36825;&#26159;&#23558;&#30693;&#35782;&#20174;&#27169;&#25311;&#25968;&#25454;&#36716;&#31227;&#21040;&#23454;&#38469;&#23454;&#39564;&#25968;&#25454;&#25152;&#38656;&#30340;ML&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17436v1 Announce Type: cross  Abstract: The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39640;&#24230;&#25511;&#21046;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;EREN&#65288;&#36890;&#36807;&#38405;&#35835;&#31508;&#35760;&#26469;&#32534;&#36753;&#27169;&#22411;&#65289;&#65292;&#20197;&#25913;&#21892;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17431</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#19988;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Robust and Scalable Model Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17431
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39640;&#24230;&#25511;&#21046;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;EREN&#65288;&#36890;&#36807;&#38405;&#35835;&#31508;&#35760;&#26469;&#32534;&#36753;&#27169;&#22411;&#65289;&#65292;&#20197;&#25913;&#21892;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#20351;&#29992;&#21442;&#25968;&#21270;&#30693;&#35782;&#36827;&#34892;&#39044;&#27979;--&#21363;&#32534;&#30721;&#22312;&#27169;&#22411;&#26435;&#37325;&#20013;&#30340;&#30693;&#35782;--&#25110;&#32773;&#26159;&#19978;&#19979;&#25991;&#30693;&#35782;--&#21363;&#21576;&#29616;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#30693;&#35782;&#12290;&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#65292;&#19968;&#20010;&#29702;&#24819;&#30340;&#34892;&#20026;&#26159;&#24403;LLMs&#22312;&#21442;&#25968;&#21270;&#30693;&#35782;&#19982;&#19978;&#19979;&#25991;&#30693;&#35782;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#22312;&#19978;&#19979;&#25991;&#26080;&#20851;&#26102;&#22238;&#36864;&#21040;&#20351;&#29992;&#20182;&#20204;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#36825;&#20351;&#24471;&#36890;&#36807;&#19978;&#19979;&#25991;&#32534;&#36753;&#26469;&#26356;&#26032;&#21644;&#32416;&#27491;&#27169;&#22411;&#30340;&#30693;&#35782;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#20542;&#21521;&#20110;&#24573;&#35270;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#26102;&#26080;&#27861;&#21487;&#38752;&#22320;&#22238;&#36864;&#21040;&#21442;&#25968;&#21270;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;LLMs&#21487;&#20197;&#34987;&#19978;&#19979;&#25991;&#30693;&#35782;&#39640;&#24230;&#25511;&#21046;&#65292;&#24182;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;EREN&#65288;&#36890;&#36807;&#38405;&#35835;&#31508;&#35760;&#26469;&#32534;&#36753;&#27169;&#22411;&#65289;&#26469;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17431v1 Announce Type: new  Abstract: Large language models (LLMs) can make predictions using parametric knowledge--knowledge encoded in the model weights--or contextual knowledge--knowledge presented in the context. In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model's knowledge by in-context editing instead of retraining. Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalabil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Masked Multi-Domain Network&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#31867;&#22411;&#21644;&#22810;&#22330;&#26223;&#36716;&#21270;&#29575;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20415;&#21033;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17425</link><description>&lt;p&gt;
Masked Multi-Domain Network: &#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#31867;&#22411;&#21644;&#22810;&#22330;&#26223;&#36716;&#21270;&#29575;&#39044;&#27979;&#30340;&#22810;&#22495;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Masked Multi-Domain Network&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#36827;&#34892;&#22810;&#31867;&#22411;&#21644;&#22810;&#22330;&#26223;&#36716;&#21270;&#29575;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20415;&#21033;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24191;&#21578;&#31995;&#32479;&#20013;&#65292;&#36716;&#21270;&#20855;&#26377;&#19981;&#21516;&#30340;&#31867;&#22411;&#65292;&#24191;&#21578;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#23637;&#31034;&#22330;&#26223;&#20013;&#23637;&#31034;&#65292;&#36825;&#20004;&#32773;&#37117;&#26497;&#22823;&#22320;&#24433;&#21709;&#23454;&#38469;&#30340;&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#12290;&#36825;&#23548;&#33268;&#20102;&#22810;&#31867;&#22411;&#21644;&#22810;&#22330;&#26223;CVR&#39044;&#27979;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29702;&#24819;&#27169;&#22411;&#24212;&#28385;&#36275;&#20197;&#19979;&#35201;&#27714;&#65306;1&#65289;&#20934;&#30830;&#24615;&#65306;&#27169;&#22411;&#24212;&#38024;&#23545;&#20219;&#20309;&#36716;&#21270;&#31867;&#22411;&#22312;&#20219;&#20309;&#23637;&#31034;&#22330;&#26223;&#19978;&#23454;&#29616;&#31934;&#32454;&#30340;&#20934;&#30830;&#24615;&#12290;2&#65289;&#21487;&#25193;&#23637;&#24615;&#65306;&#27169;&#22411;&#21442;&#25968;&#22823;&#23567;&#24212;&#35813;&#26159;&#21487;&#25215;&#21463;&#30340;&#12290;3&#65289;&#20415;&#21033;&#24615;&#65306;&#27169;&#22411;&#19981;&#24212;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#20998;&#21306;&#12289;&#23376;&#38598;&#22788;&#29702;&#21644;&#21333;&#29420;&#23384;&#20648;&#12290;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;&#20363;&#22914;&#65292;&#20026;&#27599;&#20010;&#65288;&#36716;&#21270;&#31867;&#22411;&#65292;&#23637;&#31034;&#22330;&#26223;&#65289;&#23545;&#26500;&#24314;&#21333;&#29420;&#30340;&#27169;&#22411;&#26082;&#19981;&#20855;&#22791;&#21487;&#25193;&#23637;&#24615;&#20063;&#19981;&#20415;&#20110;&#25805;&#20316;&#12290;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#65292;&#21253;&#25324;&#36716;&#21270;&#31867;&#22411;&#21644;&#23637;&#31034;&#22330;&#26223;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17425v1 Announce Type: cross  Abstract: In real-world advertising systems, conversions have different types in nature and ads can be shown in different display scenarios, both of which highly impact the actual conversion rate (CVR). This results in the multi-type and multi-scenario CVR prediction problem. A desired model for this problem should satisfy the following requirements: 1) Accuracy: the model should achieve fine-grained accuracy with respect to any conversion type in any display scenario. 2) Scalability: the model parameter size should be affordable. 3) Convenience: the model should not require a large amount of effort in data partitioning, subset processing and separate storage. Existing approaches cannot simultaneously satisfy these requirements. For example, building a separate model for each (conversion type, display scenario) pair is neither scalable nor convenient. Building a unified model trained on all the data with conversion type and display scenario incl
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#22914;Deep Sets&#21644;Transformers&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#30340;&#36827;&#23637;</title><link>https://arxiv.org/abs/2403.17410</link><description>&lt;p&gt;
&#35770;&#25490;&#21015;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On permutation-invariant neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17410
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22914;Deep Sets&#21644;Transformers&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#22312;&#20551;&#35774;&#36755;&#20837;&#25968;&#25454;&#36981;&#24490;&#22522;&#20110;&#21521;&#37327;&#30340;&#26684;&#24335;&#30340;&#21069;&#25552;&#19979;&#35774;&#35745;&#65292;&#30528;&#37325;&#20110;&#22522;&#20110;&#21521;&#37327;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38656;&#27714;&#28041;&#21450;&#22522;&#20110;&#38598;&#21512;&#30340;&#20219;&#21153;&#30340;&#22686;&#38271;&#65292;&#30740;&#31350;&#30028;&#23545;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#20852;&#36259;&#21457;&#29983;&#20102;&#33539;&#24335;&#36716;&#21464;&#12290;&#36817;&#24180;&#26469;&#65292;Deep Sets&#21644;Transformers&#31561;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#20986;&#29616;&#22312;&#22788;&#29702;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#26550;&#26500;&#19987;&#38376;&#35774;&#35745;&#20026;&#33258;&#28982;&#23481;&#32435;&#38598;&#21512;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#22788;&#29702;&#38598;&#21512;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#22823;&#37327;&#33268;&#21147;&#20110;&#25506;&#32034;&#21644;&#21033;&#29992;&#36825;&#20123;&#26550;&#26500;&#33021;&#21147;&#30340;&#30740;&#31350;&#21162;&#21147;&#65292;&#20197;&#36924;&#36817;&#38598;&#21512;&#20989;&#25968;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#26088;&#22312;&#27010;&#36848;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17410v1 Announce Type: cross  Abstract: Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17407</link><description>&lt;p&gt;
&#20351;&#29992;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;
&lt;/p&gt;
&lt;p&gt;
Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23391;&#21152;&#25289;&#25991;&#26412;&#19982;&#22320;&#26041;&#26041;&#35328;&#36716;&#24405;&#20026;&#22269;&#38469;&#38899;&#26631;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#22320;&#21306;&#26041;&#35328;&#20449;&#24687;&#65292;&#20197;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26412;&#21040;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#30340;&#20934;&#30830;&#36716;&#24405;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#38899;&#38901;&#23398;&#21644;&#35821;&#22659;&#30456;&#20851;&#30340;&#38899;&#21464;&#12290;&#23545;&#20110;&#21306;&#22495;&#23391;&#21152;&#25289;&#26041;&#35328;&#26469;&#35828;&#65292;&#30001;&#20110;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26041;&#35328;&#30340;&#26631;&#20934;&#25340;&#20889;&#32422;&#23450;&#12289;&#24403;&#22320;&#21644;&#22806;&#35821;&#22312;&#36825;&#20123;&#22320;&#21306;&#20013;&#27969;&#34892;&#30340;&#35789;&#27719;&#20197;&#21450;&#19981;&#21516;&#22320;&#21306;&#20043;&#38388;&#30340;&#38899;&#38901;&#22810;&#26679;&#24615;&#65292;&#36825;&#19968;&#25361;&#25112;&#29978;&#33267;&#26356;&#20026;&#20005;&#23803;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#35206;&#30422;&#23391;&#21152;&#25289;&#22269;&#20845;&#20010;&#22320;&#21306;&#30340;&#26032;&#25968;&#25454;&#38598;&#19978;&#24341;&#20837;&#8220;&#21306;&#22495;&#25351;&#23548;&#26631;&#35760;&#8221;&#65288;DGT&#65289;&#25216;&#26415;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#29983;&#25104;IPA&#36716;&#24405;&#20043;&#21069;&#21521;&#27169;&#22411;&#25552;&#20379;&#26377;&#20851;&#36755;&#20837;&#25991;&#26412;&#30340;&#21306;&#22495;&#26041;&#35328;&#25110;&#8220;&#22320;&#21306;&#8221;&#30340;&#26126;&#30830;&#20449;&#24687;&#12290;&#36825;&#36890;&#36807;&#22312;&#36755;&#20837;&#24207;&#21015;&#21069;&#28155;&#21152;&#19968;&#20010;&#22320;&#21306;&#26631;&#35760;&#26469;&#23454;&#29616;&#65292;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#29702;&#35299;&#19982;&#27599;&#20010;&#22320;&#21306;&#30456;&#20851;&#30340;&#29420;&#29305;&#38899;&#38901;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22312;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#25506;&#32034;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.17404</link><description>&lt;p&gt;
&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#30340;&#27867;&#21270;&#35823;&#24046;&#20998;&#26512;: &#19968;&#39033;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalization Error Analysis for Sparse Mixture-of-Experts: A Preliminary Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22312;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#25506;&#32034;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixture-of-Experts (MoE)&#20195;&#34920;&#20102;&#19968;&#31181;&#25972;&#21512;&#39044;&#27979;&#26469;&#33258;&#20960;&#20010;&#19987;&#38376;&#23376;&#27169;&#22411;&#65288;&#31216;&#20026;&#19987;&#23478;&#65289;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#34701;&#21512;&#26159;&#36890;&#36807;&#19968;&#20010;&#36335;&#30001;&#26426;&#21046;&#23454;&#29616;&#30340;&#65292;&#26681;&#25454;&#36755;&#20837;&#25968;&#25454;&#21160;&#24577;&#20998;&#37197;&#26435;&#37325;&#32473;&#27599;&#20010;&#19987;&#23478;&#30340;&#36129;&#29486;&#12290;&#20256;&#32479;&#30340;MoE&#26426;&#21046;&#36873;&#25321;&#25152;&#26377;&#21487;&#29992;&#30340;&#19987;&#23478;&#65292;&#24102;&#26469;&#20102;&#21487;&#35266;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#30456;&#21453;&#65292;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#65288;Sparse MoE&#65289;&#21482;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#65292;&#29978;&#33267;&#21482;&#26377;&#19968;&#20010;&#19987;&#23478;&#65292;&#26174;&#30528;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#65292;&#21516;&#26102;&#22312;&#32463;&#39564;&#19978;&#20445;&#30041;&#65292;&#26377;&#26102;&#29978;&#33267;&#22686;&#24378;&#24615;&#33021;&#12290;&#23613;&#31649;MoE&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#36825;&#20123;&#20248;&#28857;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#31232;&#30095;MoE&#22312;&#21508;&#31181;&#20851;&#38190;&#22240;&#32032;&#26041;&#38754;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17404v1 Announce Type: new  Abstract: Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates predictions from several specialized sub-models (referred to as experts). This fusion is accomplished through a router mechanism, dynamically assigning weights to each expert's contribution based on the input data. Conventional MoE mechanisms select all available experts, incurring substantial computational costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages only a limited number, or even just one expert, significantly reducing computation overhead while empirically preserving, and sometimes even enhancing, performance. Despite its wide-ranging applications and these advantageous characteristics, MoE's theoretical underpinnings have remained elusive. In this paper, we embark on an exploration of Sparse MoE's generalization error concerning various critical factors. Specifically, we investigate the impact of the number of data samples, 
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;&#39537;&#21160;&#30740;&#31350;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#21487;&#20197;&#19982;&#26041;&#27861;&#39537;&#21160;&#30740;&#31350;&#26377;&#30410;&#22320;&#21327;&#21516;&#65292;&#20294;&#30446;&#21069;&#23457;&#26597;&#12289;&#25307;&#32856;&#21644;&#25945;&#23398;&#23454;&#36341;&#24448;&#24448;&#38459;&#30861;&#20102;&#36825;&#31181;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2403.17381</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#39537;&#21160;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
Application-Driven Innovation in Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17381
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#39537;&#21160;&#30740;&#31350;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#21487;&#20197;&#19982;&#26041;&#27861;&#39537;&#21160;&#30740;&#31350;&#26377;&#30410;&#22320;&#21327;&#21516;&#65292;&#20294;&#30446;&#21069;&#23457;&#26597;&#12289;&#25307;&#32856;&#21644;&#25945;&#23398;&#23454;&#36341;&#24448;&#24448;&#38459;&#30861;&#20102;&#36825;&#31181;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#21463;&#29305;&#23450;&#29616;&#23454;&#25361;&#25112;&#21551;&#21457;&#30340;&#21019;&#26032;&#31639;&#27861;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#36825;&#26679;&#30340;&#24037;&#20316;&#19981;&#20165;&#22312;&#24212;&#29992;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20063;&#22312;&#26426;&#22120;&#23398;&#20064;&#26412;&#36523;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24212;&#29992;&#39537;&#21160;&#30740;&#31350;&#30340;&#33539;&#24335;&#65292;&#23558;&#20854;&#19982;&#26356;&#26631;&#20934;&#30340;&#26041;&#27861;&#39537;&#21160;&#30740;&#31350;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#24212;&#29992;&#39537;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#21487;&#20197;&#19982;&#26041;&#27861;&#39537;&#21160;&#24037;&#20316;&#26377;&#30410;&#22320;&#21327;&#21516;&#12290;&#23613;&#31649;&#20855;&#26377;&#36825;&#20123;&#22909;&#22788;&#65292;&#25105;&#20204;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23457;&#26597;&#12289;&#25307;&#32856;&#21644;&#25945;&#23398;&#23454;&#36341;&#24448;&#24448;&#38459;&#30861;&#20102;&#24212;&#29992;&#39537;&#21160;&#21019;&#26032;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#22914;&#20309;&#25913;&#36827;&#36825;&#20123;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17381v1 Announce Type: cross  Abstract: As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#38899;&#39057;&#30340;&#24773;&#24863;&#20998;&#26512;&#22312;&#38899;&#20048;&#20013;&#30340;&#36816;&#29992;&#65292;&#36890;&#36807;&#39044;&#27979;&#38899;&#20048;&#29255;&#27573;&#38543;&#26102;&#38388;&#30340;&#24773;&#24863;&#21464;&#21270;&#20197;&#21450;&#30830;&#23450;&#38899;&#20048;&#26102;&#38388;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#24773;&#24863;&#20540;&#26469;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#12290;</title><link>https://arxiv.org/abs/2403.17379</link><description>&lt;p&gt;
&#25506;&#32034;&#21644;&#24212;&#29992;&#22522;&#20110;&#38899;&#39057;&#30340;&#24773;&#24863;&#20998;&#26512;&#22312;&#38899;&#20048;&#20013;
&lt;/p&gt;
&lt;p&gt;
Exploring and Applying Audio-Based Sentiment Analysis in Music
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#38899;&#39057;&#30340;&#24773;&#24863;&#20998;&#26512;&#22312;&#38899;&#20048;&#20013;&#30340;&#36816;&#29992;&#65292;&#36890;&#36807;&#39044;&#27979;&#38899;&#20048;&#29255;&#27573;&#38543;&#26102;&#38388;&#30340;&#24773;&#24863;&#21464;&#21270;&#20197;&#21450;&#30830;&#23450;&#38899;&#20048;&#26102;&#38388;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#24773;&#24863;&#20540;&#26469;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#26159;&#25991;&#26412;&#22788;&#29702;&#20013;&#19981;&#26029;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#28041;&#21450;&#23545;&#25991;&#26412;&#30340;&#24847;&#35265;&#12289;&#24773;&#24863;&#21644;&#20027;&#35266;&#24615;&#30340;&#35745;&#31639;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#24819;&#27861;&#19981;&#20165;&#38480;&#20110;&#25991;&#26412;&#21644;&#35821;&#38899;&#65292;&#20107;&#23454;&#19978;&#65292;&#23427;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#24418;&#24335;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#31867;&#22312;&#38899;&#20048;&#20013;&#34920;&#36798;&#33258;&#24049;&#30340;&#28145;&#24230;&#19981;&#22914;&#22312;&#25991;&#26412;&#20013;&#12290;&#35745;&#31639;&#27169;&#22411;&#35299;&#37322;&#38899;&#20048;&#24773;&#24863;&#30340;&#33021;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#21487;&#33021;&#23545;&#27835;&#30103;&#21644;&#38899;&#20048;&#25773;&#25918;&#31561;&#26041;&#38754;&#20135;&#29983;&#24433;&#21709;&#21644;&#29992;&#36884;&#12290;&#26412;&#25991;&#28041;&#21450;&#20004;&#20010;&#29420;&#31435;&#20219;&#21153;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;(1)&#39044;&#27979;&#38899;&#20048;&#29255;&#27573;&#38543;&#26102;&#38388;&#30340;&#24773;&#24863;&#65292;&#20197;&#21450;(2)&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30830;&#23450;&#38899;&#20048;&#21518;&#30340;&#19979;&#19968;&#20010;&#24773;&#24863;&#20540;&#65292;&#20197;&#30830;&#20445;&#26080;&#32541;&#36807;&#28193;&#12290;&#21033;&#29992;&#21253;&#21547;&#20174;Free Music Archive&#20013;&#36873;&#20013;&#24182;&#29992;Russel&#30340;af&#22278;&#29615;&#27169;&#22411;&#25253;&#21578;&#30340;&#24841;&#24742;&#21644;&#28608;&#27963;&#27700;&#24179;&#27880;&#37322;&#30340;&#27468;&#26354;&#29255;&#27573;&#30340;Emotions in Music&#25968;&#25454;&#24211;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17379v1 Announce Type: cross  Abstract: Sentiment analysis is a continuously explored area of text processing that deals with the computational analysis of opinions, sentiments, and subjectivity of text. However, this idea is not limited to text and speech, in fact, it could be applied to other modalities. In reality, humans do not express themselves in text as deeply as they do in music. The ability of a computational model to interpret musical emotions is largely unexplored and could have implications and uses in therapy and musical queuing. In this paper, two individual tasks are addressed. This study seeks to (1) predict the emotion of a musical clip over time and (2) determine the next emotion value after the music in a time series to ensure seamless transitions. Utilizing data from the Emotions in Music Database, which contains clips of songs selected from the Free Music Archive annotated with levels of valence and arousal as reported on Russel's circumplex model of af
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#30340;&#26032;&#22411;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955; U-Net &#20013;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#20174;&#32780;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35774;&#32622;&#19979;&#25913;&#21892;&#25193;&#25955;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.17377</link><description>&lt;p&gt;
&#20855;&#26377;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#33258;&#30699;&#27491;&#25193;&#25955;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17377
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#30340;&#26032;&#22411;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955; U-Net &#20013;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#20174;&#32780;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35774;&#32622;&#19979;&#25913;&#21892;&#25193;&#25955;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20294;&#20854;&#36136;&#37327;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#27604;&#22914;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CG&#65289;&#21644;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CFG&#65289;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#25110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#22914;&#22270;&#20687;&#24674;&#22797;&#20013;&#26080;&#27861;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#31216;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#65292;&#23427;&#25913;&#36827;&#20102;&#25193;&#25955;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#19981;&#31649;&#26159;&#22312;&#26080;&#26465;&#20214;&#36824;&#26159;&#26377;&#26465;&#20214;&#30340;&#35774;&#32622;&#20013;&#65292;&#37117;&#33021;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#25110;&#25972;&#21512;&#22806;&#37096;&#27169;&#22359;&#12290;PAG &#26088;&#22312;&#36890;&#36807;&#25972;&#20010;&#21435;&#22122;&#36807;&#31243;&#36880;&#27493;&#22686;&#24378;&#26679;&#26412;&#30340;&#32467;&#26500;&#12290;&#23427;&#28041;&#21450;&#36890;&#36807;&#29992;&#24658;&#31561;&#30697;&#38453;&#26367;&#25442;&#25193;&#25955; U-Net &#20013;&#36873;&#25321;&#30340;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#32771;&#34385;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17377v1 Announce Type: cross  Abstract: Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#21160;&#25968;&#25454;&#24341;&#25806;&#65288;AIDE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#38382;&#39064;&#12289;&#39640;&#25928;&#31579;&#36873;&#25968;&#25454;&#12289;&#33258;&#21160;&#26631;&#27880;&#25913;&#36827;&#27169;&#22411;&#12289;&#29983;&#25104;&#22810;&#26679;&#21270;&#22330;&#26223;&#39564;&#35777;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17373</link><description>&lt;p&gt;
AIDE&#65306;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30446;&#26631;&#26816;&#27979;&#30340;&#33258;&#21160;&#25968;&#25454;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17373
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#21160;&#25968;&#25454;&#24341;&#25806;&#65288;AIDE&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35782;&#21035;&#38382;&#39064;&#12289;&#39640;&#25928;&#31579;&#36873;&#25968;&#25454;&#12289;&#33258;&#21160;&#26631;&#27880;&#25913;&#36827;&#27169;&#22411;&#12289;&#29983;&#25104;&#22810;&#26679;&#21270;&#22330;&#26223;&#39564;&#35777;&#27169;&#22411;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17373v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#31995;&#32479;&#20381;&#36182;&#31283;&#20581;&#30340;&#24863;&#30693;&#27169;&#22411;&#20316;&#20026;&#23433;&#20840;&#20445;&#38556;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#22312;&#36947;&#36335;&#19978;&#36935;&#21040;&#30340;&#29289;&#20307;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#65292;&#32597;&#35265;&#25110;&#26410;&#35265;&#31867;&#21035;&#23545;&#37096;&#32626;&#30340;&#24863;&#30693;&#27169;&#22411;&#26500;&#25104;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#36890;&#36807;&#26114;&#36149;&#30340;&#36807;&#31243;&#19981;&#26029;&#22320;&#31579;&#36873;&#21644;&#26631;&#27880;&#25968;&#25454;&#65292;&#38656;&#35201;&#20154;&#21147;&#30340;&#24040;&#22823;&#25237;&#20837;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#35774;&#35745;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#24341;&#25806;&#65288;AIDE&#65289;&#65292;&#33258;&#21160;&#35782;&#21035;&#38382;&#39064;&#65292;&#39640;&#25928;&#31579;&#36873;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#21160;&#26631;&#27880;&#25913;&#36827;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#22330;&#26223;&#39564;&#35777;&#27169;&#22411;&#12290;&#36825;&#19968;&#36807;&#31243;&#21487;&#20197;&#36845;&#20195;&#36827;&#34892;&#65292;&#20801;&#35768;&#27169;&#22411;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#25918;&#19990;&#30028;&#26816;&#27979;&#22522;&#20934;&#65292;&#20197;&#20840;&#38754;&#35780;&#20272;&#21508;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;AV&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#22312;&#38477;&#20302;&#26041;&#38754;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17373v1 Announce Type: cross  Abstract: Autonomous vehicle (AV) systems rely on robust perception models as a cornerstone of safety assurance. However, objects encountered on the road exhibit a long-tailed distribution, with rare or unseen categories posing challenges to a deployed perception model. This necessitates an expensive process of continuously curating and annotating data with significant human effort. We propose to leverage recent advances in vision-language and large language models to design an Automatic Data Engine (AIDE) that automatically identifies issues, efficiently curates data, improves the model through auto-labeling, and verifies the model through generation of diverse scenarios. This process operates iteratively, allowing for continuous self-improvement of the model. We further establish a benchmark for open-world detection on AV datasets to comprehensively evaluate various learning paradigms, demonstrating our method's superior performance at a reduc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#26367;&#20195;LQR&#25104;&#26412;&#65292;&#21487;&#26377;&#25928;&#35843;&#25972;&#21040;&#26032;&#23454;&#29616;&#30340;&#20803;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#20102;&#25214;&#21040;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17364</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;LQR&#20803;&#31574;&#30053;&#20272;&#35745;&#30340;Moreau&#21253;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Moreau Envelope Approach for LQR Meta-Policy Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#26367;&#20195;LQR&#25104;&#26412;&#65292;&#21487;&#26377;&#25928;&#35843;&#25972;&#21040;&#26032;&#23454;&#29616;&#30340;&#20803;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#20102;&#25214;&#21040;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#26102;&#19981;&#21464;&#31163;&#25955;&#26102;&#38388;&#19981;&#30830;&#23450;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#31574;&#30053;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#26367;&#20195;LQR&#25104;&#26412;&#65292;&#30001;&#19981;&#30830;&#23450;&#31995;&#32479;&#30340;&#26377;&#38480;&#23454;&#29616;&#26500;&#24314;&#65292;&#20197;&#23450;&#20041;&#19968;&#20010;&#23545;&#26032;&#23454;&#29616;&#26377;&#25928;&#35843;&#25972;&#30340;&#20803;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#25214;&#21040;&#20803;LQR&#25104;&#26412;&#20989;&#25968;&#30340;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26032;&#23454;&#29616;&#30340;&#32447;&#24615;&#31995;&#32479;&#19978;&#32988;&#36807;&#20102;&#25511;&#21046;&#22120;&#30340;&#26420;&#32032;&#24179;&#22343;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17364v1 Announce Type: cross  Abstract: We study the problem of policy estimation for the Linear Quadratic Regulator (LQR) in discrete-time linear time-invariant uncertain dynamical systems. We propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of realizations of the uncertain system, to define a meta-policy efficiently adjustable to new realizations. Moreover, we design an algorithm to find an approximate first-order stationary point of the meta-LQR cost function. Numerical results show that the proposed approach outperforms naive averaging of controllers on new realizations of the linear system. We also provide empirical evidence that our method has better sample complexity than Model-Agnostic Meta-Learning (MAML) approaches.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21644;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#26102;&#38388;-&#21152;&#36895;&#24230;&#26368;&#20248;&#36712;&#36857;&#35268;&#21010;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#36712;&#36857;&#35268;&#21010;&#26102;&#38388;&#21644;&#32553;&#23567;&#20248;&#21270;&#24046;&#36317;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;</title><link>https://arxiv.org/abs/2403.17353</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#22810;&#30446;&#26631;&#36712;&#36857;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Trajectory Planning with Dual-Encoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17353
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21644;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#20248;&#21270;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#26102;&#38388;-&#21152;&#36895;&#24230;&#26368;&#20248;&#36712;&#36857;&#35268;&#21010;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#36712;&#36857;&#35268;&#21010;&#26102;&#38388;&#21644;&#32553;&#23567;&#20248;&#21270;&#24046;&#36317;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;-&#21152;&#36895;&#24230;&#26368;&#20248;&#36712;&#36857;&#35268;&#21010;&#23545;&#20110;&#25552;&#21319;&#26426;&#22120;&#20154;&#33218;&#22312;&#21160;&#24577;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#23548;&#33268;&#29983;&#25104;&#20248;&#21270;&#36712;&#36857;&#30340;&#26174;&#33879;&#24310;&#36831;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#26469;&#21152;&#36895;&#26102;&#38388;-&#21152;&#36895;&#24230;&#26368;&#20248;&#36712;&#36857;&#35268;&#21010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#24314;&#31435;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#27493;&#36712;&#36857;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#23545;&#35813;&#36712;&#36857;&#36827;&#34892;&#32454;&#21270;&#65292;&#20197;&#25552;&#39640;&#20854;&#26368;&#20248;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#36712;&#36857;&#35268;&#21010;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#36798;79.72\%&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32553;&#23567;&#20102;&#19982;&#30446;&#26631;&#20989;&#25968;&#20540;&#30456;&#20851;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#26368;&#39640;&#38477;&#20302;&#20102;29.9\%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17353v1 Announce Type: cross  Abstract: Time-jerk optimal trajectory planning is crucial in advancing robotic arms' performance in dynamic tasks. Traditional methods rely on solving complex nonlinear programming problems, bringing significant delays in generating optimized trajectories. In this paper, we propose a two-stage approach to accelerate time-jerk optimal trajectory planning. Firstly, we introduce a dual-encoder based transformer model to establish a good preliminary trajectory. This trajectory is subsequently refined through sequential quadratic programming to improve its optimality and robustness. Our approach outperforms the state-of-the-art by up to 79.72\% in reducing trajectory planning time. Compared with existing methods, our method shrinks the optimality gap with the objective function value decreasing by up to 29.9\%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HiGNN&#65292;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#26597;&#33410;&#28857;&#30340;&#37051;&#23621;&#20998;&#24067;&#26469;&#26377;&#25928;&#21033;&#29992;&#24322;&#36136;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17351</link><description>&lt;p&gt;
&#20174;&#24322;&#36136;&#24615;&#23398;&#20064;&#65306;&#24322;&#36136;&#20449;&#24687;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learn from Heterophily: Heterophilous Information-enhanced Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HiGNN&#65292;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#26597;&#33410;&#28857;&#30340;&#37051;&#23621;&#20998;&#24067;&#26469;&#26377;&#25928;&#21033;&#29992;&#24322;&#36136;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#29615;&#22659;&#20013;&#65292;Graph Neural Networks (GNNs)&#36890;&#24120;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#19981;&#21516;&#26631;&#31614;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#22522;&#20110;&#35821;&#20041;&#21547;&#20041;&#30456;&#36830;&#12290;&#30446;&#21069;&#20851;&#20110;&#22270;&#24322;&#36136;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32858;&#21512;&#26657;&#20934;&#25110;&#37051;&#23621;&#25193;&#23637;&#19978;&#65292;&#36890;&#36807;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#25110;&#32467;&#26500;&#20449;&#24687;&#26469;&#25913;&#21892;GNN&#34920;&#31034;&#20197;&#35299;&#20915;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#24322;&#36136;&#24615;&#20013;&#20869;&#22312;&#30340;&#23453;&#36149;&#35821;&#20041;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#35843;&#26597;&#22270;&#20013;&#27599;&#20010;&#21333;&#29420;&#33410;&#28857;&#30340;&#37051;&#23621;&#20998;&#24067;&#26469;&#26377;&#25928;&#22320;&#21033;&#29992;&#22312;&#22270;&#23398;&#20064;&#20013;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#35770;&#35777;&#20102;&#36825;&#19968;&#29702;&#24565;&#22312;&#22686;&#24378;&#22270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#27492;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;HiGNN&#65292;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#26032;&#22270;&#32467;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#33410;&#28857;&#20998;&#24067;&#25972;&#21512;&#24322;&#36136;&#20449;&#24687;&#26469;&#22686;&#24378;&#22270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17351v1 Announce Type: new  Abstract: Under circumstances of heterophily, where nodes with different labels tend to be connected based on semantic meanings, Graph Neural Networks (GNNs) often exhibit suboptimal performance. Current studies on graph heterophily mainly focus on aggregation calibration or neighbor extension and address the heterophily issue by utilizing node features or structural information to improve GNN representations. In this paper, we propose and demonstrate that the valuable semantic information inherent in heterophily can be utilized effectively in graph learning by investigating the distribution of neighbors for each individual node within the graph. The theoretical analysis is carried out to demonstrate the efficacy of the idea in enhancing graph learning. Based on this analysis, we propose HiGNN, an innovative approach that constructs an additional new graph structure, that integrates heterophilous information by leveraging node distribution to enha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17343</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#30340;&#20813;&#36153;&#21161;&#25512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models are Free Boosters for Biomedical Imaging Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26159;&#20256;&#32479;&#19978;&#32570;&#20047;&#35821;&#35328;&#25110;&#25991;&#26412;&#25968;&#25454;&#30340;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#19981;&#21516;&#20110;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#25552;&#21462;&#30340;&#20923;&#32467;&#21464;&#21387;&#22120;&#22359;&#20316;&#20026;&#21019;&#26032;&#30340;&#32534;&#30721;&#22120;&#23618;&#65292;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#12290;&#36825;&#31181;&#31574;&#30053;&#19982;&#36890;&#24120;&#20381;&#36182;&#20110;&#35821;&#35328;&#39537;&#21160;&#25552;&#31034;&#21644;&#36755;&#20837;&#30340;&#26631;&#20934;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#26694;&#26550;&#26377;&#30528;&#26174;&#33879;&#30340;&#19981;&#21516;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;LLMs&#33021;&#22815;&#25552;&#21319;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;2D&#21644;3D&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#65292;&#20805;&#24403;&#21363;&#25554;&#21363;&#29992;&#30340;&#21161;&#25512;&#22120;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;M&#30340;&#24191;&#27867;&#12289;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17343v1 Announce Type: cross  Abstract: In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in M
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#36861;&#27714;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#35299;&#20915;&#20559;&#35265;&#38382;&#39064;&#65292;&#30830;&#20445;&#27169;&#22411;&#19981;&#20250;&#26377;&#24847;&#25110;&#26080;&#24847;&#22320;&#23545;&#26576;&#20123;&#32676;&#20307;&#20135;&#29983;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.17333</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#36861;&#27714;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Pursuit of Fairness in Artificial Intelligence Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17333
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20013;&#30340;&#20844;&#24179;&#36861;&#27714;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#35299;&#20915;&#20559;&#35265;&#38382;&#39064;&#65292;&#30830;&#20445;&#27169;&#22411;&#19981;&#20250;&#26377;&#24847;&#25110;&#26080;&#24847;&#22320;&#23545;&#26576;&#20123;&#32676;&#20307;&#20135;&#29983;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#12289;&#25945;&#32946;&#21644;&#23601;&#19994;&#31561;&#26041;&#26041;&#38754;&#38754;&#12290;&#30001;&#20110;&#23427;&#20204;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#25935;&#24863;&#29615;&#22659;&#65292;&#24182;&#20570;&#20986;&#21487;&#33021;&#25913;&#21464;&#20154;&#29983;&#30340;&#20915;&#31574;&#65292;&#28508;&#22312;&#30340;&#20559;&#35265;&#32467;&#26524;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#24320;&#21457;&#20154;&#21592;&#24212;&#30830;&#20445;&#36825;&#31867;&#27169;&#22411;&#19981;&#20250;&#34920;&#29616;&#20986;&#20219;&#20309;&#24847;&#22806;&#30340;&#27495;&#35270;&#34892;&#20026;&#65292;&#27604;&#22914;&#20559;&#29233;&#26576;&#20123;&#24615;&#21035;&#12289;&#31181;&#26063;&#25110;&#27531;&#30142;&#20154;&#22763;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26222;&#36941;&#20256;&#25773;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#23545;&#19981;&#20844;&#24179;&#27169;&#22411;&#36234;&#26469;&#36234;&#26377;&#24847;&#35782;&#65292;&#24182;&#33268;&#21147;&#20110;&#20943;&#23569;&#20854;&#20013;&#30340;&#20559;&#35265;&#12290;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#37325;&#35201;&#30740;&#31350;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#19981;&#20250;&#26377;&#24847;&#25110;&#26080;&#24847;&#22320;&#24310;&#32493;&#20559;&#35265;&#12290;&#36825;&#39033;&#35843;&#26597;&#27010;&#36848;&#20102;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#21069;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20844;&#24179;&#24615;&#19981;&#21516;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17333v1 Announce Type: new  Abstract: Artificial Intelligence (AI) models are now being utilized in all facets of our lives such as healthcare, education and employment. Since they are used in numerous sensitive environments and make decisions that can be life altering, potential biased outcomes are a pressing matter. Developers should ensure that such models don't manifest any unexpected discriminatory practices like partiality for certain genders, ethnicities or disabled people. With the ubiquitous dissemination of AI systems, researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them. Significant research has been conducted in addressing such issues to ensure models don't intentionally or unintentionally perpetuate bias. This survey offers a synopsis of the different ways researchers have promoted fairness in AI systems. We explore the different definitions of fairness existing in the current literature. We create a compr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#20171;&#32461;&#20102;DeepKKT&#26465;&#20214;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#31867;&#20284;&#65292;&#20026;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.17329</link><description>&lt;p&gt;
&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Deep Support Vectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#20171;&#32461;&#20102;DeepKKT&#26465;&#20214;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#31867;&#20284;&#65292;&#20026;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#36890;&#24120;&#34987;&#24402;&#22240;&#20110;&#20854;&#19982;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#31561;&#20215;&#24615;&#65292;&#20294;&#36825;&#31181;&#20851;&#31995;&#30340;&#23454;&#38469;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#22312;&#36825;&#19968;&#39046;&#22495;&#24320;&#23637;&#20102;&#19968;&#39033;&#25506;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeepKKT&#26465;&#20214;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#28145;&#24230;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#20256;&#32479;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#26465;&#20214;&#30340;&#35843;&#25972;&#29256;&#26412;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#30340;&#20999;&#23454;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#65292;&#31867;&#20284;&#20110;SVM&#20013;&#30340;&#36807;&#31243;&#12290;&#20195;&#30721;&#23558;&#20250;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17329v1 Announce Type: cross  Abstract: While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ALISA&#65292;&#19968;&#31181;&#36890;&#36807;&#31232;&#30095;&#24863;&#30693;KV&#32531;&#23384;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#31995;&#32479;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.17312</link><description>&lt;p&gt;
ALISA: &#36890;&#36807;&#31232;&#30095;&#24863;&#30693;KV&#32531;&#23384;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ALISA&#65292;&#19968;&#31181;&#36890;&#36807;&#31232;&#30095;&#24863;&#30693;KV&#32531;&#23384;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#31995;&#32479;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#26174;&#33879;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#65292;&#24182;&#19988;&#22312;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26041;&#38754;&#20855;&#26377;&#22522;&#30784;&#24615;&#20316;&#29992;&#65292;&#22914;LLaMA&#21644;OPT&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;LLMs&#22312;&#23454;&#38469;&#25512;&#29702;&#20013;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#65292;&#28041;&#21450;&#35745;&#31639;&#21644;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#12290;&#30001;&#20110;LLM&#25512;&#29702;&#20855;&#26377;&#33258;&#22238;&#24402;&#29305;&#24615;&#65292;Transformer&#20013;&#30340;&#27880;&#24847;&#23618;&#30340;KV&#32531;&#23384;&#21487;&#20197;&#36890;&#36807;&#23558;&#20108;&#27425;&#22797;&#26434;&#24230;&#35745;&#31639;&#26367;&#25442;&#20026;&#32447;&#24615;&#22797;&#26434;&#24230;&#20869;&#23384;&#35775;&#38382;&#65292;&#20174;&#32780;&#26377;&#25928;&#21152;&#36895;LLM&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23545;&#22788;&#29702;&#26356;&#38271;&#24207;&#21015;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#22686;&#21152;&#20869;&#23384;&#12290;&#36825;&#31181;&#24320;&#38144;&#23548;&#33268;&#30001;&#20110;I/O&#29942;&#39048;&#21644;&#29978;&#33267;&#26159;&#20869;&#23384;&#19981;&#36275;&#38169;&#35823;&#32780;&#23548;&#33268;&#21534;&#21520;&#37327;&#38477;&#20302;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#31995;&#32479;&#19978;&#65292;&#22914;&#21333;&#20010;&#36890;&#29992;GPU&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17312v1 Announce Type: new  Abstract: The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-desi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#25991;&#26723;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#21644;&#20004;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22343;&#33021;&#20135;&#29983;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17308</link><description>&lt;p&gt;
&#31070;&#32463;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#65306;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Neural Multimodal Topic Modeling: A Comprehensive Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#25991;&#26723;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#21644;&#20004;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#22343;&#33021;&#20135;&#29983;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#22320;&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#25214;&#21040;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#22914;&#22270;&#29255;&#21644;&#25991;&#26412;&#65289;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#29255;&#30340;&#25991;&#26723;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#30340;&#31995;&#32479;&#24615;&#21644;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#35299;&#20915;&#26041;&#26696;&#21644;&#20004;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#20016;&#23500;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20004;&#20010;&#27169;&#22411;&#37117;&#33021;&#20135;&#29983;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26041;&#27861;&#20248;&#20110;&#21478;&#19968;&#20010;&#26041;&#27861;&#30340;&#31243;&#24230;&#21462;&#20915;&#20110;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65292;&#36825;&#34920;&#26126;&#26410;&#26469;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#31616;&#27905;&#30340;&#20154;&#24037;&#35780;&#20272;&#19982;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#25152;&#30830;&#23450;&#30340;&#32467;&#26524;&#19968;&#33268;&#12290;&#36825;&#31181;&#19968;&#33268;&#19981;&#20165;&#21152;&#24378;&#20102;&#25105;&#20204;&#25351;&#26631;&#30340;&#21487;&#20449;&#24230;&#65292;&#20063;&#31361;&#26174;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17308v1 Announce Type: cross  Abstract: Neural topic models can successfully find coherent and diverse topics in textual data. However, they are limited in dealing with multimodal datasets (e.g., images and text). This paper presents the first systematic and comprehensive evaluation of multimodal topic modeling of documents containing both text and images. In the process, we propose two novel topic modeling solutions and two novel evaluation metrics. Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse topics. Nevertheless, the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future. Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics. This alignment not only reinforces the credibility of our metrics but also highlights the po
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#36923;&#36753;&#22238;&#24402;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26032;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#65292;&#36890;&#36807;&#37319;&#29992;&#31192;&#23494;&#20849;&#20139;&#26597;&#25214;&#34920;&#35745;&#31639;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#25506;&#32034;&#20102;&#25918;&#26494;&#23433;&#20840;&#25514;&#26045;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17296</link><description>&lt;p&gt;
Hawk: &#20351;&#29992;&#23433;&#20840;&#26597;&#25214;&#34920;&#35745;&#31639;&#30340;&#20934;&#30830;&#24555;&#36895;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#36923;&#36753;&#22238;&#24402;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26032;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#65292;&#36890;&#36807;&#37319;&#29992;&#31192;&#23494;&#20849;&#20139;&#26597;&#25214;&#34920;&#35745;&#31639;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#25506;&#32034;&#20102;&#25918;&#26494;&#23433;&#20840;&#25514;&#26045;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#30452;&#25509;&#25968;&#25454;&#20849;&#20139;&#30340;&#24773;&#20917;&#19979;&#23545;&#26469;&#33258;&#22810;&#20010;&#23454;&#20307;&#30340;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#35299;&#38145;&#21463;&#21040;&#19994;&#21153;&#12289;&#27861;&#24459;&#25110;&#20262;&#29702;&#32422;&#26463;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#65292;&#29992;&#20110;&#36923;&#36753;&#22238;&#24402;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#37319;&#29992;&#25968;&#25454;&#25152;&#26377;&#32773;&#22312;&#20004;&#20010;&#26381;&#21153;&#22120;&#20043;&#38388;&#36827;&#34892;&#25968;&#25454;&#31192;&#23494;&#20849;&#20139;&#65292;&#36825;&#20004;&#20010;&#26381;&#21153;&#22120;&#23545;&#32852;&#21512;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#20302;&#25928;&#21644;&#19981;&#20934;&#30830;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;Yao&#30340;&#28151;&#28102;&#30005;&#36335;&#26469;&#35745;&#31639;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31192;&#23494;&#20849;&#20139;&#26597;&#25214;&#34920;&#35745;&#31639;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#65292;&#26082;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21448;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#38500;&#20102;&#24341;&#20837;&#26080;&#27844;&#28431;&#25216;&#26415;&#65292;&#25105;&#20204;&#36824;&#24320;&#23637;&#20102;&#23545;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30340;&#25918;&#26494;&#23433;&#20840;&#25514;&#26045;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17296v1 Announce Type: cross  Abstract: Training machine learning models on data from multiple entities without direct data sharing can unlock applications otherwise hindered by business, legal, or ethical constraints. In this work, we design and implement new privacy-preserving machine learning protocols for logistic regression and neural network models. We adopt a two-server model where data owners secret-share their data between two servers that train and evaluate the model on the joint data. A significant source of inefficiency and inaccuracy in existing methods arises from using Yao's garbled circuits to compute non-linear activation functions. We propose new methods for computing non-linear functions based on secret-shared lookup tables, offering both computational efficiency and improved accuracy.   Beyond introducing leakage-free techniques, we initiate the exploration of relaxed security measures for privacy-preserving machine learning. Instead of claiming that the 
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#35780;&#20272;&#30740;&#31350;&#25581;&#31034;&#20986;&#65292;&#27809;&#26377;&#19968;&#31181;&#31639;&#27861;&#21487;&#20197;&#22312;&#25152;&#26377;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#34429;&#28982;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#21364;&#20276;&#38543;&#30528;&#26356;&#39640;&#30340;&#35745;&#31639;&#25110;&#36890;&#20449;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2403.17287</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#37117;&#19968;&#35270;&#21516;&#20161;&#65306;&#19968;&#39033;&#24615;&#33021;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17287
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#35780;&#20272;&#30740;&#31350;&#25581;&#31034;&#20986;&#65292;&#27809;&#26377;&#19968;&#31181;&#31639;&#27861;&#21487;&#20197;&#22312;&#25152;&#26377;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#34429;&#28982;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#21364;&#20276;&#38543;&#30528;&#26356;&#39640;&#30340;&#35745;&#31639;&#25110;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;&#32852;&#37030;&#23398;&#20064;(FL)&#20316;&#20026;&#19968;&#31181;&#20174;&#20998;&#25955;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30340;&#23454;&#38469;&#26041;&#27861;&#23835;&#36215;&#12290;FL&#30340;&#27969;&#34892;&#23548;&#33268;&#20102;&#20247;&#22810;FL&#31639;&#27861;&#21644;&#26426;&#21046;&#30340;&#21457;&#23637;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#36825;&#20123;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#19978;&#65292;&#20294;&#23545;&#20854;&#20182;&#26041;&#38754;&#65292;&#22914;&#35745;&#31639;&#24320;&#38144;&#65292;&#24615;&#33021;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#31561;&#30340;&#29702;&#35299;&#21364;&#24456;&#23569;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#27454;&#21517;&#20026;Flame&#30340;&#24320;&#28304;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23545;&#20960;&#31181;&#20256;&#32479;FL&#31639;&#27861;&#65288;FedAvg&#65292;FedProx&#65292;FedYogi&#65292;FedAdam&#65292;SCAFFOLD&#21644;FedDyn&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#27979;&#37327;&#30740;&#31350;&#34920;&#26126;&#65292;&#27809;&#26377;&#19968;&#31181;&#31639;&#27861;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#26368;&#22909;&#12290;&#19968;&#20123;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#26159;&#65306;&#65288;1&#65289;&#34429;&#28982;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#20250;&#24102;&#26469;&#26356;&#39640;&#30340;&#35745;&#31639;&#24320;&#38144;&#65288;FedDyn&#65289;&#25110;&#36890;&#20449;&#24320;&#38144;&#65288;SCAFFOLD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17287v1 Announce Type: new  Abstract: Federated Learning (FL) emerged as a practical approach to training a model from decentralized data. The proliferation of FL led to the development of numerous FL algorithms and mechanisms. Many prior efforts have given their primary focus on accuracy of those approaches, but there exists little understanding of other aspects such as computational overheads, performance and training stability, etc. To bridge this gap, we conduct extensive performance evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi, FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning framework called Flame. Our comprehensive measurement study reveals that no single algorithm works best across different performance metrics. A few key observations are: (1) While some state-of-the-art algorithms achieve higher accuracy than others, they incur either higher computation overheads (FedDyn) or communication overheads (SCAFFOLD).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#8220;&#24369;&#20449;&#21495;&#20998;&#26512;&#8221;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24448;&#36820;&#35774;&#35745;&#23545;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22823;&#37096;&#20998;&#22870;&#21169;&#35823;&#24046;&#20026;&#27491;&#30456;&#20851;&#26102;&#65292;&#24448;&#36820;&#35774;&#35745;&#27604;&#27599;&#26085;&#20999;&#25442;&#31574;&#30053;&#26356;&#26377;&#25928;&#65292;&#22686;&#21152;&#25919;&#31574;&#20999;&#25442;&#39057;&#29575;&#21487;&#20197;&#38477;&#20302;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.17285</link><description>&lt;p&gt;
&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24448;&#36820;&#35774;&#35745;&#36827;&#34892;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Switchback Designs in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#8220;&#24369;&#20449;&#21495;&#20998;&#26512;&#8221;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24448;&#36820;&#35774;&#35745;&#23545;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22823;&#37096;&#20998;&#22870;&#21169;&#35823;&#24046;&#20026;&#27491;&#30456;&#20851;&#26102;&#65292;&#24448;&#36820;&#35774;&#35745;&#27604;&#27599;&#26085;&#20999;&#25442;&#31574;&#30053;&#26356;&#26377;&#25928;&#65292;&#22686;&#21152;&#25919;&#31574;&#20999;&#25442;&#39057;&#29575;&#21487;&#20197;&#38477;&#20302;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;A/B&#27979;&#35797;&#20013;&#24448;&#36820;&#35774;&#35745;&#30340;&#35814;&#32454;&#30740;&#31350;&#65292;&#36825;&#20123;&#35774;&#35745;&#38543;&#26102;&#38388;&#22312;&#22522;&#20934;&#21644;&#26032;&#31574;&#30053;&#20043;&#38388;&#20132;&#26367;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#35774;&#35745;&#23545;&#20854;&#20135;&#29983;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20272;&#35745;&#22120;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;&#24369;&#20449;&#21495;&#20998;&#26512;&#8221;&#26694;&#26550;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#36825;&#20123;ATE&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#22312;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#29615;&#22659;&#20013;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(i) &#24403;&#22823;&#37096;&#20998;&#22870;&#21169;&#35823;&#24046;&#21576;&#27491;&#30456;&#20851;&#26102;&#65292;&#24448;&#36820;&#35774;&#35745;&#27604;&#27599;&#26085;&#20999;&#25442;&#31574;&#30053;&#30340;&#20132;&#26367;&#35774;&#35745;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#22686;&#21152;&#25919;&#31574;&#20999;&#25442;&#30340;&#39057;&#29575;&#24448;&#24448;&#20250;&#38477;&#20302;ATE&#20272;&#35745;&#22120;&#30340;MSE&#12290;(ii) &#28982;&#32780;&#65292;&#24403;&#35823;&#24046;&#19981;&#30456;&#20851;&#26102;&#65292;&#25152;&#26377;&#36825;&#20123;&#35774;&#35745;&#21464;&#24471;&#28176;&#36817;&#31561;&#25928;&#12290;(iii) &#22312;&#22823;&#22810;&#25968;&#35823;&#24046;&#20026;&#36127;&#30456;&#20851;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17285v1 Announce Type: cross  Abstract: This paper offers a detailed investigation of switchback designs in A/B testing, which alternate between baseline and new policies over time. Our aim is to thoroughly evaluate the effects of these designs on the accuracy of their resulting average treatment effect (ATE) estimators. We propose a novel "weak signal analysis" framework, which substantially simplifies the calculations of the mean squared errors (MSEs) of these ATEs in Markov decision process environments. Our findings suggest that (i) when the majority of reward errors are positively correlated, the switchback design is more efficient than the alternating-day design which switches policies in a daily basis. Additionally, increasing the frequency of policy switches tends to reduce the MSE of the ATE estimator. (ii) When the errors are uncorrelated, however, all these designs become asymptotically equivalent. (iii) In cases where the majority of errors are negative correlate
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#21644;&#35838;&#31243;&#23398;&#20064;&#65292;&#26412;&#30740;&#31350;&#22312;&#19977;&#25351;&#26426;&#26800;&#33218;&#25805;&#32437;&#20219;&#21153;&#20013;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#30340;&#30446;&#26631;</title><link>https://arxiv.org/abs/2403.17266</link><description>&lt;p&gt;
&#25506;&#32034;CausalWorld&#65306;&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#21644;&#35838;&#31243;&#23398;&#20064;&#22686;&#24378;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17266
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#36716;&#31227;&#21644;&#35838;&#31243;&#23398;&#20064;&#65292;&#26412;&#30740;&#31350;&#22312;&#19977;&#25351;&#26426;&#26800;&#33218;&#25805;&#32437;&#20219;&#21153;&#20013;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#30340;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#19977;&#25351;&#26426;&#26800;&#33218;&#25805;&#32437;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#25351;&#38388;&#30340;&#22797;&#26434;&#36816;&#21160;&#21644;&#21327;&#35843;&#12290;&#36890;&#36807;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#26234;&#33021;&#20307;&#26469;&#33719;&#24471;&#29087;&#32451;&#25805;&#32437;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#20026;&#20102;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#20013;&#37319;&#29992;&#20102;&#20004;&#31181;&#30693;&#35782;&#36716;&#31227;&#31574;&#30053;&#65292;&#21363;&#24494;&#35843;&#21644;&#35838;&#31243;&#23398;&#20064;&#12290;&#24494;&#35843;&#20351;&#26234;&#33021;&#20307;&#21487;&#20197;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#24182;&#23558;&#20854;&#35843;&#25972;&#21040;&#26032;&#20219;&#21153;&#20013;&#12290;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#22810;&#31181;&#21464;&#20307;&#65292;&#22914;&#27169;&#22411;&#36716;&#31227;&#12289;&#31574;&#30053;&#36716;&#31227;&#21644;&#36328;&#20219;&#21153;&#36716;&#31227;&#12290;&#20026;&#20102;&#28040;&#38500;&#39044;&#35757;&#32451;&#30340;&#38656;&#27714;&#65292;&#35838;&#31243;&#23398;&#20064;&#23558;&#39640;&#32423;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#31616;&#21333;&#12289;&#28176;&#36827;&#30340;&#38454;&#27573;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#12290;&#21457;&#29616;&#23398;&#20064;&#38454;&#27573;&#30340;&#25968;&#37327;&#12289;&#23376;&#20219;&#21153;&#30340;&#32972;&#26223;&#20197;&#21450;&#36716;&#25442;&#26102;&#26426;&#26159;&#20851;&#38190;&#30340;&#35774;&#35745;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17266v1 Announce Type: cross  Abstract: This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers. By employing reinforcement learning, we train an agent to acquire the necessary skills for proficient manipulation. To enhance the efficiency and effectiveness of the learning process, two knowledge transfer strategies, fine-tuning and curriculum learning, were utilized within the soft actor-critic architecture. Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to new tasks. Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated. To eliminate the need for pretraining, curriculum learning decomposes the advanced task into simpler, progressive stages, mirroring how humans learn. The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameter
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#25193;&#25955;&#36127;&#37319;&#26679;&#30340;&#26032;&#31574;&#30053;&#65292;&#33021;&#22815;&#20174;&#28508;&#22312;&#31354;&#38388;&#20013;&#20197;&#22810;&#20010;&#28789;&#27963;&#21644;&#21487;&#25511;&#30340;&#8220;&#38590;&#24230;&#8221;&#32423;&#21035;&#29983;&#25104;&#36127;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.17259</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#25193;&#25955;&#36127;&#37319;&#26679;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Negative Sampling on Graphs for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17259
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#25193;&#25955;&#36127;&#37319;&#26679;&#30340;&#26032;&#31574;&#30053;&#65292;&#33021;&#22815;&#20174;&#28508;&#22312;&#31354;&#38388;&#20013;&#20197;&#22810;&#20010;&#28789;&#27963;&#21644;&#21487;&#25511;&#30340;&#8220;&#38590;&#24230;&#8221;&#32423;&#21035;&#29983;&#25104;&#36127;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#30784;&#20219;&#21153;&#65292;&#22312;&#32593;&#32476;&#19978;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#22914;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#12290;&#29616;&#20195;&#22270;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#23545;&#27604;&#26041;&#27861;&#26469;&#23398;&#20064;&#31283;&#20581;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20854;&#20013;&#36127;&#37319;&#26679;&#33267;&#20851;&#37325;&#35201;&#12290;&#20856;&#22411;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#26088;&#22312;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#21551;&#21457;&#24335;&#25110;&#33258;&#21160;&#23545;&#25239;&#26041;&#27861;&#26816;&#32034;&#22256;&#38590;&#31034;&#20363;&#65292;&#36825;&#21487;&#33021;&#19981;&#22815;&#28789;&#27963;&#25110;&#38590;&#20197;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#22312;&#38142;&#25509;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20174;&#22270;&#30340;&#29616;&#26377;&#23376;&#32467;&#26500;&#20013;&#23545;&#36127;&#33410;&#28857;&#36827;&#34892;&#25277;&#26679;&#65292;&#38169;&#36807;&#20102;&#28508;&#22312;&#26356;&#20248;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26679;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#32423;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#33021;&#22815;&#20174;&#28508;&#22312;&#31354;&#38388;&#20013;&#20197;&#28789;&#27963;&#19988;&#21487;&#25511;&#21046;&#30340;&#8220;&#38590;&#24230;&#8221;&#32423;&#21035;&#29983;&#25104;&#36127;&#33410;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#26377;&#26465;&#20214;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17259v1 Announce Type: new  Abstract: Link prediction is a fundamental task for graph analysis with important applications on the Web, such as social network analysis and recommendation systems, etc. Modern graph link prediction methods often employ a contrastive approach to learn robust node representations, where negative sampling is pivotal. Typical negative sampling methods aim to retrieve hard examples based on either predefined heuristics or automatic adversarial approaches, which might be inflexible or difficult to control. Furthermore, in the context of link prediction, most previous methods sample negative nodes from existing substructures of the graph, missing out on potentially more optimal samples in the latent space. To address these issues, we investigate a novel strategy of multi-level negative sampling that enables negative node generation with flexible and controllable ``hardness'' levels from the latent space. Our method, called Conditional Diffusion-based 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21046;&#36896;&#26381;&#21153;&#33021;&#21147;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#22270;&#33410;&#28857;&#37051;&#22495;&#20449;&#24687;&#21644;&#23545;&#22270;&#25968;&#25454;&#36827;&#34892;&#36807;&#37319;&#26679;&#26469;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#65292;&#22686;&#24378;&#21046;&#36896;&#33021;&#21147;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17239</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21046;&#36896;&#26381;&#21153;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Manufacturing Service Capability Prediction with Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17239
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21046;&#36896;&#26381;&#21153;&#33021;&#21147;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#21512;&#22270;&#33410;&#28857;&#37051;&#22495;&#20449;&#24687;&#21644;&#23545;&#22270;&#25968;&#25454;&#36827;&#34892;&#36807;&#37319;&#26679;&#26469;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#65292;&#22686;&#24378;&#21046;&#36896;&#33021;&#21147;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#29615;&#22659;&#20013;&#65292;&#20174;&#21046;&#36896;&#21830;&#36523;&#19978;&#35782;&#21035;&#21046;&#36896;&#33021;&#21147;&#30340;&#20027;&#35201;&#26041;&#27861;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#20851;&#38190;&#35789;&#21305;&#37197;&#21644;&#35821;&#20041;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#22312;&#24573;&#35270;&#23453;&#36149;&#30340;&#38544;&#34255;&#20449;&#24687;&#25110;&#35823;&#35299;&#20851;&#38190;&#25968;&#25454;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#23545;&#21046;&#36896;&#21830;&#33021;&#21147;&#30340;&#35782;&#21035;&#19981;&#23436;&#25972;&#12290;&#36825;&#20984;&#26174;&#20102;&#36843;&#20999;&#38656;&#35201;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#26469;&#25552;&#39640;&#21046;&#36896;&#33021;&#21147;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21046;&#36896;&#26381;&#21153;&#33021;&#21147;&#35782;&#21035;&#26041;&#27861;&#65292;&#21487;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#24212;&#29992;&#12290;&#20026;&#20102;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#20174;&#22270;&#33410;&#28857;&#37051;&#22495;&#32858;&#21512;&#20449;&#24687;&#20197;&#21450;&#23545;&#22270;&#25968;&#25454;&#36827;&#34892;&#36807;&#37319;&#26679;&#65292;&#35813;&#26041;&#27861;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#23454;&#36341;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17239v1 Announce Type: new  Abstract: In the current landscape, the predominant methods for identifying manufacturing capabilities from manufacturers rely heavily on keyword matching and semantic matching. However, these methods often fall short by either overlooking valuable hidden information or misinterpreting critical data. Consequently, such approaches result in an incomplete identification of manufacturers' capabilities. This underscores the pressing need for data-driven solutions to enhance the accuracy and completeness of manufacturing capability identification. To address the need, this study proposes a Graph Neural Network-based method for manufacturing service capability identification over a knowledge graph. To enhance the identification performance, this work introduces a novel approach that involves aggregating information from the graph nodes' neighborhoods as well as oversampling the graph data, which can be effectively applied across a wide range of practica
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#23558;&#36712;&#36857;&#25968;&#25454;&#20998;&#35299;&#20026;&#26102;&#38388;&#21644;&#35821;&#35328;&#25551;&#36848;&#30340;&#23376;&#20219;&#21153;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26102;&#38388;&#30456;&#20284;&#24615;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#20004;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.17238</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#21644;&#35821;&#20041;&#35780;&#20272;&#25351;&#26631;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#23376;&#20219;&#21153;&#20107;&#21518;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17238
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#25552;&#31034;&#31574;&#30053;&#23558;&#36712;&#36857;&#25968;&#25454;&#20998;&#35299;&#20026;&#26102;&#38388;&#21644;&#35821;&#35328;&#25551;&#36848;&#30340;&#23376;&#20219;&#21153;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26102;&#38388;&#30456;&#20284;&#24615;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#20004;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#20219;&#21153;&#21644;&#36816;&#21160;&#35268;&#21010;&#65288;TAMP&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#24102;&#26377;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#35328;&#30417;&#30563;&#26426;&#22120;&#20154;&#36712;&#36857;&#36827;&#34892;&#25511;&#21046;&#31574;&#30053;&#35757;&#32451;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20195;&#29702;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#23545;&#23558;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#19968;&#33324;&#29992;&#20363;&#26500;&#25104;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#23558;&#36712;&#36857;&#25968;&#25454;&#20998;&#35299;&#20026;&#22522;&#20110;&#26102;&#38388;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#25551;&#36848;&#24615;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#26500;&#25104;&#23436;&#25972;&#36712;&#36857;&#30340;&#24213;&#23618;&#23376;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#20110;&#26102;&#38388;&#21644;&#35821;&#35328;&#30340;&#25551;&#36848;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#30340;&#33258;&#21160;&#26631;&#35760;&#26694;&#26550;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861; SIMILARITY &#26469;&#29983;&#25104;&#20004;&#31181;&#26032;&#39062;&#30340;&#25351;&#26631;&#65292;&#21363;&#26102;&#38388;&#30456;&#20284;&#24615;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17238v1 Announce Type: cross  Abstract: Recent works in Task and Motion Planning (TAMP) show that training control policies on language-supervised robot trajectories with quality labeled data markedly improves agent task success rates. However, the scarcity of such data presents a significant hurdle to extending these methods to general use cases. To address this concern, we present an automated framework to decompose trajectory data into temporally bounded and natural language-based descriptive sub-tasks by leveraging recent prompting strategies for Foundation Models (FMs) including both Large Language Models (LLMs) and Vision Language Models (VLMs). Our framework provides both time-based and language-based descriptions for lower-level sub-tasks that comprise full trajectories. To rigorously evaluate the quality of our automatic labeling framework, we contribute an algorithm SIMILARITY to produce two novel metrics, temporal similarity and semantic similarity. The metrics me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#20943;&#36731;&#37327;&#21270;&#24433;&#21709;&#30340;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#37327;&#21270;&#25972;&#27969;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#39044;&#27979;&#26410;&#32463;&#37327;&#21270;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.17236</link><description>&lt;p&gt;
&#20855;&#26377;&#37327;&#21270;&#25972;&#27969;&#22120;&#30340;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Neural Image Compression with Quantization Rectifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#20943;&#36731;&#37327;&#21270;&#24433;&#21709;&#30340;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#37327;&#21270;&#25972;&#27969;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#39044;&#27979;&#26410;&#32463;&#37327;&#21270;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#22312;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#24341;&#20837;&#35823;&#24046;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#21387;&#32553;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#35299;&#20915;&#20102;&#37327;&#21270;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#35757;&#32451;-&#27979;&#35797;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20294;&#23545;&#22270;&#20687;&#29305;&#24449;&#34920;&#36798;&#30340;&#37327;&#21270;&#38543;&#26426;&#24433;&#21709;&#20173;&#26410;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21387;&#32553;&#37327;&#21270;&#25972;&#27969;&#22120;&#65288;QR&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#20943;&#36731;&#37327;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20174;&#37327;&#21270;&#30340;&#29305;&#24449;&#20013;&#39044;&#27979;&#26410;&#32463;&#37327;&#21270;&#30340;&#29305;&#24449;&#65292;&#20445;&#25345;&#29305;&#24449;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#20687;&#37325;&#24314;&#36136;&#37327;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36719;-&#39044;&#27979;&#35757;&#32451;&#25216;&#26415;&#65292;&#23558;QR&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#20013;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23558;QR&#38598;&#25104;&#21040;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17236v1 Announce Type: new  Abstract: Neural image compression has been shown to outperform traditional image codecs in terms of rate-distortion performance. However, quantization introduces errors in the compression process, which can degrade the quality of the compressed image. Existing approaches address the train-test mismatch problem incurred during quantization, the random impact of quantization on the expressiveness of image features is still unsolved. This paper presents a novel quantization rectifier (QR) method for image compression that leverages image feature correlation to mitigate the impact of quantization. Our method designs a neural network architecture that predicts unquantized features from the quantized ones, preserving feature expressiveness for better image reconstruction quality. We develop a soft-to-predictive training technique to integrate QR into existing neural image codecs. In evaluation, we integrate QR into state-of-the-art neural image codecs 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#24341;&#20837;&#37319;&#26679;&#36807;&#31243;&#20013;&#65292;&#21152;&#36895;&#23398;&#20064;&#24182;&#38477;&#20302;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#20272;&#35745;&#21160;&#21147;&#23398;&#12290;</title><link>https://arxiv.org/abs/2403.17233</link><description>&lt;p&gt;
&#20351;&#29992;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#36827;&#34892;&#21160;&#21147;&#23398;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17233
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#24341;&#20837;&#37319;&#26679;&#36807;&#31243;&#20013;&#65292;&#21152;&#36895;&#23398;&#20064;&#24182;&#38477;&#20302;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#20272;&#35745;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20391;&#38754;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#26126;&#30830;&#32435;&#20837;&#37319;&#26679;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#24341;&#23548;&#25506;&#32034;&#36208;&#21521;&#34920;&#29616;&#20986;&#35266;&#23519;&#25968;&#25454;&#19982;&#20174;&#20391;&#38754;&#20449;&#24687;&#27966;&#29983;&#30340;&#21160;&#21147;&#23398;&#19981;&#23436;&#32654;&#20808;&#39564;&#27169;&#22411;&#20043;&#38388;&#39640;&#32463;&#39564;&#24615;&#19981;&#19968;&#33268;&#24615;&#30340;&#21306;&#22495;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#25506;&#32034;&#39640;&#19981;&#19968;&#33268;&#24615;&#21306;&#22495;&#24182;&#21152;&#36895;&#23398;&#20064;&#65292;&#21516;&#26102;&#38477;&#20302;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#20026;&#26368;&#22823;&#39044;&#27979;&#26041;&#24046;&#25552;&#20379;&#26174;&#24335;&#25910;&#25947;&#36895;&#29575;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#30340;&#22522;&#30784;&#21160;&#21147;&#23398;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#27424;&#39537;&#21160;&#30340;&#25670;&#31995;&#32479;&#21644;&#21322;&#29454;&#35961;MuJoCo&#29615;&#22659;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17233v1 Announce Type: cross  Abstract: We present an active learning algorithm for learning dynamics that leverages side information by explicitly incorporating prior domain knowledge into the sampling process. Our proposed algorithm guides the exploration toward regions that demonstrate high empirical discrepancy between the observed data and an imperfect prior model of the dynamics derived from side information. Through numerical experiments, we demonstrate that this strategy explores regions of high discrepancy and accelerates learning while simultaneously reducing model uncertainty. We rigorously prove that our active learning algorithm yields a consistent estimate of the underlying dynamics by providing an explicit rate of convergence for the maximum predictive variance. We demonstrate the efficacy of our approach on an under-actuated pendulum system and on the half-cheetah MuJoCo environment.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Dyna-LfLH&#65292;&#36890;&#36807;&#23398;&#20064;&#24187;&#35273;&#20013;&#30340;&#21160;&#24577;&#29615;&#22659;&#65292;&#23433;&#20840;&#22320;&#23398;&#20064;&#22320;&#38754;&#26426;&#22120;&#20154;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#28789;&#27963;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.17231</link><description>&lt;p&gt;
Dyna-LfLH:&#20174;&#23398;&#21040;&#30340;&#24187;&#35273;&#20013;&#23398;&#20250;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23398;&#20064;&#28789;&#27963;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Dyna-LfLH&#65292;&#36890;&#36807;&#23398;&#20064;&#24187;&#35273;&#20013;&#30340;&#21160;&#24577;&#29615;&#22659;&#65292;&#23433;&#20840;&#22320;&#23398;&#20064;&#22320;&#38754;&#26426;&#22120;&#20154;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#28789;&#27963;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23433;&#20840;&#22320;&#23398;&#20064;&#22320;&#38754;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#65292;&#20197;&#22312;&#23494;&#38598;&#19988;&#21160;&#24577;&#30340;&#38556;&#30861;&#29289;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;&#38024;&#23545;&#39640;&#24230;&#28151;&#20081;&#12289;&#24555;&#36895;&#31227;&#21160;&#12289;&#38590;&#20197;&#39044;&#27979;&#30340;&#38556;&#30861;&#29289;&#65292;&#20256;&#32479;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#21487;&#33021;&#26080;&#27861;&#36319;&#19978;&#26377;&#38480;&#30340;&#26426;&#36733;&#35745;&#31639;&#12290;&#23545;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#35268;&#21010;&#22120;&#65292;&#24456;&#38590;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#28436;&#31034;&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#21516;&#26102;&#24378;&#21270;&#23398;&#20064;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#30001;&#20110;&#39640;&#30896;&#25758;&#27010;&#29575;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#23433;&#20840;&#26377;&#25928;&#22320;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;LfH&#26041;&#27861;&#22522;&#20110;&#36807;&#21435;&#25104;&#21151;&#30340;&#23548;&#33322;&#32463;&#39564;&#22312;&#30456;&#23545;&#31616;&#21333;&#25110;&#23436;&#20840;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#32508;&#21512;&#22256;&#38590;&#30340;&#23548;&#33322;&#29615;&#22659;&#65292;&#20294;&#36951;&#25022;&#30340;&#26159;&#26080;&#27861;&#35299;&#20915;&#21160;&#24577;&#38556;&#30861;&#29289;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;Dyna-LfLH&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#20998;&#24067;&#21644;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17231v1 Announce Type: cross  Abstract: This paper presents a self-supervised learning method to safely learn a motion planner for ground robots to navigate environments with dense and dynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict obstacles, classical motion planners may not be able to keep up with limited onboard computation. For learning-based planners, high-quality demonstrations are difficult to acquire for imitation learning while reinforcement learning becomes inefficient due to the high probability of collision during exploration. To safely and efficiently provide training data, the Learning from Hallucination (LfH) approaches synthesize difficult navigation environments based on past successful navigation experiences in relatively easy or completely open ones, but unfortunately cannot address dynamic obstacles. In our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and learn a novel latent distribution and sample
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;&#35299;&#37322;&#26041;&#27861;&#26469;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#35745;&#31639;&#35299;&#37322;&#20998;&#24067;&#30340;&#21464;&#24322;&#31995;&#25968;&#65292;&#35780;&#20272;&#20102;&#35299;&#37322;&#30340;&#32622;&#20449;&#24230;&#24182;&#30830;&#23450;Guided Backpropagation&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17224</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Gradient-based Explanations in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;&#35299;&#37322;&#26041;&#27861;&#26469;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#35745;&#31639;&#35299;&#37322;&#20998;&#24067;&#30340;&#21464;&#24322;&#31995;&#25968;&#65292;&#35780;&#20272;&#20102;&#35299;&#37322;&#30340;&#32622;&#20449;&#24230;&#24182;&#30830;&#23450;Guided Backpropagation&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#26041;&#27861;&#26377;&#21161;&#20110;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#21407;&#22240;&#12290;&#36825;&#20123;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#21442;&#19982;&#27169;&#22411;&#35843;&#35797;&#12289;&#24615;&#33021;&#20248;&#21270;&#65292;&#24182;&#33719;&#24471;&#23545;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#30340;&#27934;&#35265;&#12290;&#37492;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#20851;&#38190;&#24212;&#29992;&#65292;&#34913;&#37327;&#36825;&#20123;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21644;&#35299;&#37322;&#26041;&#27861;&#26469;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#27969;&#31243;&#20026;CIFAR-10&#12289;FER+&#21644;California Housing&#25968;&#25454;&#38598;&#29983;&#25104;&#35299;&#37322;&#20998;&#24067;&#12290;&#36890;&#36807;&#35745;&#31639;&#36825;&#20123;&#20998;&#24067;&#30340;&#21464;&#24322;&#31995;&#25968;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35299;&#37322;&#30340;&#32622;&#20449;&#24230;&#65292;&#24182;&#30830;&#23450;&#20351;&#29992;&#24341;&#23548;&#21453;&#21521;&#20256;&#25773;&#29983;&#25104;&#30340;&#35299;&#37322;&#19982;&#20302;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#20462;&#25913;&#30340;&#20687;&#32032;&#25554;&#20837;/&#21024;&#38500;&#24230;&#37327;&#26469;&#35780;&#20215;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17224v1 Announce Type: cross  Abstract: Explanation methods help understand the reasons for a model's prediction. These methods are increasingly involved in model debugging, performance optimization, and gaining insights into the workings of a model. With such critical applications of these methods, it is imperative to measure the uncertainty associated with the explanations generated by these methods. In this paper, we propose a pipeline to ascertain the explanation uncertainty of neural networks by combining uncertainty estimation methods and explanation methods. We use this pipeline to produce explanation distributions for the CIFAR-10, FER+, and California Housing datasets. By computing the coefficient of variation of these distributions, we evaluate the confidence in the explanation and determine that the explanations generated using Guided Backpropagation have low uncertainty associated with them. Additionally, we compute modified pixel insertion/deletion metrics to ev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#22810;&#26631;&#31614;&#29289;&#20307;&#31867;&#21035;&#20013;&#22522;&#30784;&#29289;&#20307;&#30340;&#20849;&#23384;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#20849;&#23384;&#30697;&#38453;&#20998;&#26512;&#29983;&#25104;&#39057;&#32321;&#27169;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#26410;&#26631;&#35760;&#29289;&#20307;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.17223</link><description>&lt;p&gt;
&#29289;&#20307;&#26816;&#27979;&#21644;&#35782;&#21035;&#30340;&#20849;&#23384;&#65292;&#20197;&#21450;&#26410;&#26631;&#35760;&#29289;&#20307;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Co-Occurring of Object Detection and Identification towards unlabeled object discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17223
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#22810;&#26631;&#31614;&#29289;&#20307;&#31867;&#21035;&#20013;&#22522;&#30784;&#29289;&#20307;&#30340;&#20849;&#23384;&#29289;&#20307;&#65292;&#24182;&#36890;&#36807;&#20849;&#23384;&#30697;&#38453;&#20998;&#26512;&#29983;&#25104;&#39057;&#32321;&#27169;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#26410;&#26631;&#35760;&#29289;&#20307;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22810;&#26631;&#31614;&#29289;&#20307;&#31867;&#21035;&#20013;&#19982;&#22522;&#30784;&#29289;&#20307;&#20849;&#23384;&#30340;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#22312;&#25552;&#20986;&#27169;&#22411;&#30340;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#36793;&#30028;&#26694;&#21450;&#20854;&#23545;&#24212;&#30340;&#26631;&#31614;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#36827;&#34892;&#20849;&#23384;&#30697;&#38453;&#20998;&#26512;&#12290;&#22312;&#20849;&#23384;&#30697;&#38453;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#26631;&#31614;&#30340;&#26368;&#22823;&#20986;&#29616;&#27425;&#25968;&#35774;&#23450;&#22522;&#26412;&#31867;&#65292;&#24182;&#26500;&#24314;&#20851;&#32852;&#35268;&#21017;&#24182;&#29983;&#25104;&#39057;&#32321;&#27169;&#24335;&#12290;&#36825;&#20123;&#39057;&#32321;&#27169;&#24335;&#23558;&#26174;&#31034;&#22522;&#26412;&#31867;&#21450;&#20854;&#23545;&#24212;&#30340;&#20849;&#23384;&#31867;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;Pascal VOC&#21644;MS-COCO&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17223v1 Announce Type: cross  Abstract: In this paper, we propose a novel deep learning based approach for identifying co-occurring objects in conjunction with base objects in multilabel object categories. Nowadays, with the advancement in computer vision based techniques we need to know about co-occurring objects with respect to base object for various purposes. The pipeline of the proposed work is composed of two stages: in the first stage of the proposed model we detect all the bounding boxes present in the image and their corresponding labels, then in the second stage we perform co-occurrence matrix analysis. In co-occurrence matrix analysis, we set base classes based on the maximum occurrences of the labels and build association rules and generate frequent patterns. These frequent patterns will show base classes and their corresponding co-occurring classes. We performed our experiments on two publicly available datasets: Pascal VOC and MS-COCO. The experimental results 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21313;&#19968;&#31181;&#39046;&#20808;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#20026;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#26497;&#38480;&#25552;&#20379;&#20102;&#37325;&#35201;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.17218</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#33021;&#21147;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#21313;&#19968;&#31181;&#39046;&#20808;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28431;&#27934;&#26816;&#27979;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#20026;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#26497;&#38480;&#25552;&#20379;&#20102;&#37325;&#35201;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#20854;&#20182;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28431;&#27934;&#26816;&#27979;&#23545;&#20110;&#32500;&#25252;&#36719;&#20214;&#31995;&#32479;&#30340;&#23433;&#20840;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#31934;&#30830;&#30340;&#28431;&#27934;&#26816;&#27979;&#38656;&#35201;&#23545;&#20195;&#30721;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#26497;&#38480;&#30340;&#33391;&#22909;&#26696;&#20363;&#30740;&#31350;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#21033;&#29992;&#36890;&#29992;&#25552;&#31034;&#25216;&#26415;&#23558;LLMs&#24212;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;&#65292;&#20294;&#23427;&#20204;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#30340;&#23436;&#25972;&#33021;&#21147;&#20197;&#21450;&#22312;&#35299;&#37322;&#30830;&#23450;&#30340;&#28431;&#27934;&#26102;&#25152;&#29359;&#30340;&#38169;&#35823;&#31867;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21313;&#19968;&#31181;&#39046;&#20808;&#30340;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#22788;&#20110;&#26368;&#21069;&#27839;&#19988;&#36890;&#24120;&#29992;&#20316;&#32534;&#30721;&#21161;&#25163;&#30340;LLMs&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25628;&#32034;&#20102;&#25928;&#26524;&#26368;&#20339;&#30340;&#25552;&#31034;&#65292;&#32467;&#21512;&#20102;&#35832;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38142;&#24335;&#23398;&#20064;&#31561;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17218v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential for code generation and other software engineering tasks. Vulnerability detection is of crucial importance to maintaining the security, integrity, and trustworthiness of software systems. Precise vulnerability detection requires reasoning about the code, making it a good case study for exploring the limits of LLMs' reasoning capabilities. Although recent work has applied LLMs to vulnerability detection using generic prompting techniques, their full capabilities for this task and the types of errors they make when explaining identified vulnerabilities remain unclear.   In this paper, we surveyed eleven LLMs that are state-of-the-art in code generation and commonly used as coding assistants, and evaluated their capabilities for vulnerability detection. We systematically searched for the best-performing prompts, incorporating techniques such as in-context learning and chain-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#65292;&#21487;&#20197;&#24555;&#36895;&#27979;&#35797;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.17212</link><description>&lt;p&gt;
&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Sanity Checks for Explanation Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#65292;&#21487;&#20197;&#24555;&#36895;&#27979;&#35797;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#26041;&#27861;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#21487;&#33021;&#38590;&#20197;&#35299;&#37322;&#25110;&#20986;&#29616;&#38169;&#35823;&#12290; &#23558;&#35299;&#37322;&#26041;&#27861;&#19982;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#30456;&#32467;&#21512;&#20250;&#20135;&#29983;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#12290; &#35780;&#20272;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#26159;&#22256;&#38590;&#30340;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#65292;&#20854;&#20013;&#38024;&#23545;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#35299;&#37322;&#23450;&#20041;&#20102;&#26435;&#37325;&#21644;&#25968;&#25454;&#38543;&#26426;&#21270;&#27979;&#35797;&#65292;&#20801;&#35768;&#23545;&#19981;&#30830;&#23450;&#24615;&#21644;&#35299;&#37322;&#26041;&#27861;&#30340;&#32452;&#21512;&#36827;&#34892;&#24555;&#36895;&#27979;&#35797;&#12290; &#25105;&#20204;&#22312;CIFAR10&#21644;&#21152;&#21033;&#31119;&#23612;&#20122;&#25151;&#23627;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#27880;&#24847;&#21040;Ensemble&#20284;&#20046;&#22312;Guided Backpropagation&#65292;Integrated Gradients&#21644;LIME&#35299;&#37322;&#19978;&#19968;&#33268;&#36890;&#36807;&#20102;&#36825;&#20004;&#39033;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17212v1 Announce Type: cross  Abstract: Explanations for machine learning models can be hard to interpret or be wrong. Combining an explanation method with an uncertainty estimation method produces explanation uncertainty. Evaluating explanation uncertainty is difficult. In this paper we propose sanity checks for uncertainty explanation methods, where a weight and data randomization tests are defined for explanations with uncertainty, allowing for quick tests to combinations of uncertainty and explanation methods. We experimentally show the validity and effectiveness of these tests on the CIFAR10 and California Housing datasets, noting that Ensembles seem to consistently pass both tests with Guided Backpropagation, Integrated Gradients, and LIME explanations.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.17210</link><description>&lt;p&gt;
CADGL: &#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17210
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDIs&#65289;&#30340;&#30740;&#31350;&#26159;&#33647;&#29289;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20803;&#32032;&#12290;DDIs&#21457;&#29983;&#22312;&#19968;&#20010;&#33647;&#29289;&#30340;&#24615;&#36136;&#21463;&#20854;&#20182;&#33647;&#29289;&#21253;&#21547;&#30340;&#24433;&#21709;&#26102;&#12290;&#26816;&#27979;&#26377;&#21033;&#30340;DDIs&#26377;&#21487;&#33021;&#20026;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#21019;&#26032;&#33647;&#29289;&#30340;&#21019;&#36896;&#21644;&#25512;&#36827;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#12289;&#31283;&#20581;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#29616;&#23454;&#24212;&#29992;&#21487;&#33021;&#24615;&#26041;&#38754;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;CADGL&#30340;&#26032;&#39062;&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22522;&#20110;&#23450;&#21046;&#30340;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;VGAE&#65289;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#19978;&#19979;&#25991;&#39044;&#22788;&#29702;&#22120;&#20174;&#20004;&#20010;&#19981;&#21516;&#35270;&#35282;&#65306;&#23616;&#37096;&#37051;&#22495;&#21644;&#20998;&#23376;&#19978;&#19979;&#25991;&#65292;&#22312;&#24322;&#36136;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#25429;&#33719;&#20851;&#38190;&#30340;&#32467;&#26500;&#21644;&#29983;&#29702;&#21270;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#25991;&#31456;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#30693;&#35782;&#40511;&#27807;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#24191;&#27867;&#35835;&#32773;&#32676;&#20307;&#30340;&#20449;&#21495;&#22788;&#29702;&#22522;&#30784;&#25945;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.17181</link><description>&lt;p&gt;
&#20449;&#21495;&#22788;&#29702;&#19982;&#26426;&#22120;&#23398;&#20064;&#20132;&#38598;&#30740;&#31350;&#65306;&#19968;&#20010;&#20197;&#29992;&#20363;&#39537;&#21160;&#30340;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Intersection of Signal Processing and Machine Learning: A Use Case-Driven Analysis Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#25991;&#31456;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#30693;&#35782;&#40511;&#27807;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#24191;&#27867;&#35835;&#32773;&#32676;&#20307;&#30340;&#20449;&#21495;&#22788;&#29702;&#22522;&#30784;&#25945;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20256;&#24863;&#12289;&#27979;&#37327;&#21644;&#35745;&#31639;&#25216;&#26415;&#30340;&#36827;&#27493;&#26174;&#33879;&#25193;&#23637;&#20102;&#22522;&#20110;&#20449;&#21495;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#21033;&#29992;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26469;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#36825;&#31181;&#34701;&#21512;&#20195;&#34920;&#20102;&#20449;&#21495;&#31995;&#32479;&#28436;&#21464;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#28857;&#65292;&#31361;&#26174;&#20102;&#24357;&#21512;&#36825;&#20004;&#20010;&#36328;&#23398;&#31185;&#39046;&#22495;&#20043;&#38388;&#29616;&#26377;&#30693;&#35782;&#24046;&#36317;&#30340;&#24517;&#35201;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#25991;&#29486;&#20013;&#26377;&#35768;&#22810;&#23581;&#35797;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#38480;&#20110;&#29305;&#23450;&#24212;&#29992;&#65292;&#24182;&#19988;&#20027;&#35201;&#20391;&#37325;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#36890;&#24120;&#20551;&#35774;&#35835;&#32773;&#22312;&#20449;&#21495;&#22788;&#29702;&#26041;&#38754;&#26377;&#24191;&#27867;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#31181;&#20551;&#35774;&#20026;&#24191;&#27867;&#35835;&#32773;&#32676;&#20307;&#25552;&#20986;&#20102;&#26174;&#33879;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#37319;&#21462;&#20102;&#32508;&#21512;&#35770;&#25991;&#30340;&#26041;&#24335;&#12290;&#23427;&#20174;&#20449;&#21495;&#22788;&#29702;&#22522;&#30784;&#30340;&#35814;&#32454;&#25945;&#31243;&#24320;&#22987;&#65292;&#20026;&#35835;&#32773;&#25552;&#20379;&#24517;&#35201;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17181v1 Announce Type: cross  Abstract: Recent advancements in sensing, measurement, and computing technologies have significantly expanded the potential for signal-based applications, leveraging the synergy between signal processing and Machine Learning (ML) to improve both performance and reliability. This fusion represents a critical point in the evolution of signal-based systems, highlighting the need to bridge the existing knowledge gap between these two interdisciplinary fields. Despite many attempts in the existing literature to bridge this gap, most are limited to specific applications and focus mainly on feature extraction, often assuming extensive prior knowledge in signal processing. This assumption creates a significant obstacle for a wide range of readers. To address these challenges, this paper takes an integrated article approach. It begins with a detailed tutorial on the fundamentals of signal processing, providing the reader with the necessary background kno
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33041;&#21330;&#20013;&#20998;&#21106;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#38656;&#35201;&#39640;&#32423;&#21035;&#35774;&#35745;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17177</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#33041;&#21330;&#20013;&#20998;&#21106;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#33041;&#21330;&#20013;&#20998;&#21106;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#38656;&#35201;&#39640;&#32423;&#21035;&#35774;&#35745;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#21330;&#20013;&#20998;&#21106;&#22312;&#33041;&#21330;&#20013;&#24739;&#32773;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#25552;&#20379;&#21463;&#24433;&#21709;&#33041;&#21306;&#22495;&#30340;&#31354;&#38388;&#20449;&#24687;&#21644;&#21463;&#25439;&#31243;&#24230;&#12290;&#20934;&#30830;&#20998;&#21106;&#33041;&#21330;&#20013;&#30149;&#21464;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#25163;&#24037;&#25216;&#26415;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#26368;&#36817;&#65292;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#22411;&#24050;&#34987;&#24341;&#20837;&#29992;&#20110;&#19968;&#33324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23637;&#31034;&#20986;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#26102;&#36229;&#36234;&#35768;&#22810;&#26368;&#20808;&#36827;&#32593;&#32476;&#30340;&#26377;&#21069;&#26223;&#32467;&#26524;&#12290;&#38543;&#30528;&#35270;&#35273;Transformer&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#22522;&#20110;&#23427;&#20204;&#24341;&#20837;&#20102;&#20960;&#31181;&#27169;&#22411;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#26088;&#22312;&#35774;&#35745;&#22522;&#20110;&#20256;&#32479;&#21367;&#31215;&#23618;&#26469;&#25552;&#21462;&#20687;Transformer&#36825;&#26679;&#30340;&#38271;&#31243;&#20381;&#36182;&#30340;&#26356;&#22909;&#27169;&#22359;&#12290;&#26159;&#21542;&#23545;&#25152;&#26377;&#20998;&#21106;&#26696;&#20363;&#37117;&#38656;&#35201;&#36825;&#26679;&#39640;&#32423;&#21035;&#30340;&#35774;&#35745;&#26469;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#31572;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17177v1 Announce Type: cross  Abstract: Stroke segmentation plays a crucial role in the diagnosis and treatment of stroke patients by providing spatial information about affected brain regions and the extent of damage. Segmenting stroke lesions accurately is a challenging task, given that conventional manual techniques are time consuming and prone to errors. Recently, advanced deep models have been introduced for general medical image segmentation, demonstrating promising results that surpass many state of the art networks when evaluated on specific datasets. With the advent of the vision Transformers, several models have been introduced based on them, while others have aimed to design better modules based on traditional convolutional layers to extract long-range dependencies like Transformers. The question of whether such high-level designs are necessary for all segmentation cases to achieve the best results remains unanswered. In this study, we selected four types of deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#23398;&#20064;&#20013;&#30340;&#20449;&#24565;&#26679;&#26412;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#36890;&#20449;&#21644;&#36164;&#28304;&#38480;&#21046;&#30340;&#26032;&#22411;&#23398;&#20064;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.17174</link><description>&lt;p&gt;
&#20449;&#24565;&#26679;&#26412;&#23601;&#26159;&#31038;&#20250;&#23398;&#20064;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Belief Samples Are All You Need For Social Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20250;&#23398;&#20064;&#20013;&#30340;&#20449;&#24565;&#26679;&#26412;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#36890;&#20449;&#21644;&#36164;&#28304;&#38480;&#21046;&#30340;&#26032;&#22411;&#23398;&#20064;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#31038;&#20250;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#32676;&#23884;&#20837;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20195;&#29702;&#23545;&#23398;&#20064;&#19990;&#30028;&#30340;&#28508;&#22312;&#29366;&#24577;&#24863;&#20852;&#36259;&#12290;&#20195;&#29702;&#20154;&#20855;&#26377;&#19981;&#23436;&#25972;&#12289;&#22024;&#26434;&#21644;&#24322;&#36136;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#20026;&#20182;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#19990;&#30028;&#28508;&#22312;&#29366;&#24577;&#30340;&#37325;&#22797;&#31169;&#20154;&#35266;&#23519;&#12290;&#20195;&#29702;&#20154;&#21487;&#20197;&#36890;&#36807;&#37319;&#21462;&#23545;&#21516;&#34892;&#21487;&#35266;&#23519;&#30340;&#34892;&#21160;&#26469;&#19982;&#21516;&#34892;&#20998;&#20139;&#20182;&#20204;&#30340;&#23398;&#20064;&#32463;&#39564;&#65292;&#36825;&#20123;&#34892;&#21160;&#30340;&#20215;&#20540;&#26469;&#33258;&#19968;&#20010;&#26377;&#38480;&#21487;&#34892;&#29366;&#24577;&#38598;&#12290;&#36825;&#20123;&#34892;&#21160;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#20195;&#29702;&#20154;&#21487;&#33021;&#24418;&#25104;&#21644;&#26356;&#26032;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#29366;&#24577;&#30340;&#20449;&#24565;&#30340;&#26679;&#26412;&#12290;&#20998;&#20139;&#26679;&#26412;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#20449;&#24565;&#65292;&#26159;&#21463;&#21040;&#20195;&#29702;&#20154;&#29305;&#21035;&#26159;&#22312;&#22823;&#22411;&#20154;&#32676;&#20013;&#21487;&#29992;&#30340;&#36890;&#20449;&#12289;&#35748;&#30693;&#21644;&#20449;&#24687;&#22788;&#29702;&#36164;&#28304;&#26377;&#38480;&#30340;&#21551;&#21457;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;(Salhab&#31561;&#20154;)&#25552;&#20986;&#20102;&#36825;&#26679;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#26524;&#21482;&#20801;&#35768;&#20195;&#29702;&#20154;&#27807;&#36890;&#65292;&#37027;&#20040;&#26159;&#21542;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#27010;&#29575;&#20026;&#19968;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17174v1 Announce Type: new  Abstract: In this paper, we consider the problem of social learning, where a group of agents embedded in a social network are interested in learning an underlying state of the world. Agents have incomplete, noisy, and heterogeneous sources of information, providing them with recurring private observations of the underlying state of the world. Agents can share their learning experience with their peers by taking actions observable to them, with values from a finite feasible set of states. Actions can be interpreted as samples from the beliefs which agents may form and update on what the true state of the world is. Sharing samples, in place of full beliefs, is motivated by the limited communication, cognitive, and information-processing resources available to agents especially in large populations. Previous work (Salhab et al.) poses the question as to whether learning with probability one is still achievable if agents are only allowed to communicat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#20026;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#25171;&#24320;&#20102;&#19968;&#26465;&#26032;&#36884;&#24452;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#22810;&#26679;&#29305;&#24449;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#65292;&#21487;&#20197;&#20248;&#21270;&#26230;&#20307;&#32467;&#26500;&#31283;&#23450;&#24615;&#20197;&#21450;&#20854;&#20182;&#30446;&#26631;&#22914;&#30913;&#24615;&#25110;&#28909;&#30005;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17164</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36136;&#37327;&#22810;&#26679;&#24615;&#29992;&#20110;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Quality-Diversity for Crystal Structure Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#20026;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#25171;&#24320;&#20102;&#19968;&#26465;&#26032;&#36884;&#24452;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#22810;&#26679;&#29305;&#24449;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#65292;&#21487;&#20197;&#20248;&#21270;&#26230;&#20307;&#32467;&#26500;&#31283;&#23450;&#24615;&#20197;&#21450;&#20854;&#20182;&#30446;&#26631;&#22914;&#30913;&#24615;&#25110;&#28909;&#30005;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#32467;&#26500;&#22312;&#20174;&#30005;&#27744;&#21040;&#22826;&#38451;&#33021;&#30005;&#27744;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#38024;&#23545;&#20854;&#21407;&#23376;&#37197;&#32622;&#39044;&#27979;&#24615;&#33021;&#24050;&#32463;&#26377;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#20391;&#37325;&#20110;&#35782;&#21035;&#33021;&#37327;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#22788;&#30340;&#26368;&#31283;&#23450;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#24573;&#30053;&#20102;&#37027;&#20123;&#21487;&#33021;&#20301;&#20110;&#30456;&#37051;&#23616;&#37096;&#26497;&#23567;&#20540;&#22788;&#12289;&#20855;&#26377;&#19981;&#21516;&#26448;&#26009;&#29305;&#24615;&#65288;&#22914;&#30005;&#23548;&#29575;&#25110;&#25239;&#21464;&#24418;&#24615;&#65289;&#30340;&#20854;&#20182;&#26377;&#36259;&#26448;&#26009;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#20026;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#65292;&#22240;&#20026;&#23427;&#26088;&#22312;&#25214;&#21040;&#20855;&#26377;&#22810;&#26679;&#29305;&#24449;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#26230;&#20307;&#32467;&#26500;&#31283;&#23450;&#24615;&#20197;&#21450;&#20854;&#20182;&#30446;&#26631;&#65288;&#22914;&#30913;&#24615;&#25110;&#28909;&#30005;&#25928;&#29575;&#65289;&#20063;&#21487;&#33021;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17164v1 Announce Type: cross  Abstract: Crystal structures are indispensable across various domains, from batteries to solar cells, and extensive research has been dedicated to predicting their properties based on their atomic configurations. However, prevailing Crystal Structure Prediction methods focus on identifying the most stable solutions that lie at the global minimum of the energy function. This approach overlooks other potentially interesting materials that lie in neighbouring local minima and have different material properties such as conductivity or resistance to deformation. By contrast, Quality-Diversity algorithms provide a promising avenue for Crystal Structure Prediction as they aim to find a collection of high-performing solutions that have diverse characteristics. However, it may also be valuable to optimise for the stability of crystal structures alongside other objectives such as magnetism or thermoelectric efficiency. Therefore, in this work, we harness 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#21270;TSP&#22270;&#34920;&#31034;&#21644;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#20351;&#32534;&#30721;&#22120;&#38598;&#20013;&#20110;TSP&#23454;&#20363;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#21516;&#26102;&#20801;&#35768;&#20449;&#24687;&#22312;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#33258;&#30001;&#27969;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.17159</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810; - &#20851;&#20110;&#31232;&#30095;&#21270;&#22312;Transformers&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;TSP&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Less Is More - On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17159
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#21270;TSP&#22270;&#34920;&#31034;&#21644;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#20351;&#32534;&#30721;&#22120;&#38598;&#20013;&#20110;TSP&#23454;&#20363;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#21516;&#26102;&#20801;&#35768;&#20449;&#24687;&#22312;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#33258;&#30001;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#36817;&#30740;&#31350;&#22788;&#29702;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#31561;&#36335;&#30001;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;transformer&#25110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#35768;&#22810;&#30740;&#31350;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#32534;&#30721;&#22120;&#65292;&#20801;&#35768;&#23427;&#20204;&#22312;&#25972;&#20010;TSP&#23454;&#20363;&#19978;&#32858;&#21512;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#32534;&#30721;&#22120;&#20165;&#20851;&#27880;TSP&#23454;&#20363;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20026;&#20256;&#36882;&#32473;GNN&#30340;TSP&#22270;&#34920;&#31034;&#25552;&#20986;&#20102;&#22270;&#31232;&#30095;&#21270;&#65292;&#24182;&#20026;&#20256;&#36882;&#32473;transformers&#30340;TSP&#23454;&#20363;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#23631;&#34109;&#65292;&#20854;&#20013;mask&#23545;&#24212;&#20110;&#31232;&#30095;TSP&#22270;&#34920;&#31034;&#30340;&#37051;&#25509;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#31232;&#30095;&#21270;&#32423;&#21035;&#30340;&#38598;&#21512;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#19987;&#27880;&#20110;&#26368;&#26377;&#21069;&#36884;&#30340;&#37096;&#20998;&#65292;&#21516;&#26102;&#36824;&#20801;&#35768;TSP&#23454;&#20363;&#30340;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17159v1 Announce Type: cross  Abstract: Most of the recent studies tackling routing problems like the Traveling Salesman Problem (TSP) with machine learning use a transformer or Graph Neural Network (GNN) based encoder architecture. However, many of them apply these encoders naively by allowing them to aggregate information over the whole TSP instances. We, on the other hand, propose a data preprocessing method that allows the encoders to focus on the most relevant parts of the TSP instances only. In particular, we propose graph sparsification for TSP graph representations passed to GNNs and attention masking for TSP instances passed to transformers where the masks correspond to the adjacency matrices of the sparse TSP graph representations. Furthermore, we propose ensembles of different sparsification levels allowing models to focus on the most promising parts while also allowing information flow between all nodes of a TSP instance. In the experimental studies, we show that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#26368;&#22823;&#30340;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21457;&#24067;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.17143</link><description>&lt;p&gt;
&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#29992;&#20110;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#65306;&#36866;&#24212;&#26032;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#26368;&#22823;&#30340;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21457;&#24067;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#20851;&#31995;&#25277;&#21462;&#23545;&#20110;&#22312;&#25968;&#23383;&#20154;&#25991;&#23398;&#21644;&#30456;&#20851;&#23398;&#31185;&#32972;&#26223;&#19979;&#25552;&#21462;&#21644;&#29702;&#35299;&#20256;&#35760;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#31038;&#21306;&#23545;&#26500;&#24314;&#33021;&#22815;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#20851;&#31995;&#30340;&#25968;&#25454;&#38598;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#65292;&#32780;&#19988;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#26412;&#25991;&#24212;&#29992;&#20102;&#24341;&#23548;&#24335;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#36229;&#36807;80,000&#20010;&#23454;&#20363;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#26159;&#26368;&#22823;&#30340;&#24503;&#35821;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;2000&#20010;&#23454;&#20363;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#19982;&#21033;&#29992;&#24341;&#23548;&#24335;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#32534;&#21046;&#30340;&#25968;&#25454;&#38598;&#19968;&#36215;&#21457;&#24067;&#12290;&#25105;&#20204;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17143v1 Announce Type: new  Abstract: Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as 
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#36275;&#22815;&#20809;&#28369;&#30340;&#20989;&#25968;&#65292;&#26412;&#25991;&#35777;&#26126;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;ReLU&#32593;&#32476;&#21487;&#20197;&#22312;&#39640;&#27010;&#29575;&#19979;&#23454;&#29616;$O(m^{-1/2})$&#30340;$L_{\infty}$&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.17142</link><description>&lt;p&gt;
&#21033;&#29992;&#38543;&#26426;&#27973;&#23618;ReLU&#32593;&#32476;&#26469;&#36827;&#34892;&#36924;&#36817;&#21450;&#20854;&#22312;&#27169;&#22411;&#21442;&#32771;&#33258;&#36866;&#24212;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17142
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36275;&#22815;&#20809;&#28369;&#30340;&#20989;&#25968;&#65292;&#26412;&#25991;&#35777;&#26126;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;ReLU&#32593;&#32476;&#21487;&#20197;&#22312;&#39640;&#27010;&#29575;&#19979;&#23454;&#29616;$O(m^{-1/2})$&#30340;$L_{\infty}$&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24120;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#20197;&#21450;&#30456;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#32467;&#26500;&#26159;&#20351;&#29992;&#20855;&#26377;&#21333;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;&#21363;&#27973;&#23618;&#32593;&#32476;&#65289;&#65292;&#20854;&#20013;&#26435;&#37325;&#21644;&#20559;&#32622;&#25552;&#21069;&#22266;&#23450;&#65292;&#21482;&#26377;&#36755;&#20986;&#23618;&#34987;&#35757;&#32451;&#12290;&#23613;&#31649;&#32463;&#20856;&#32467;&#26524;&#34920;&#26126;&#65292;&#23384;&#22312;&#36825;&#31181;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#26377;&#30028;&#21306;&#22495;&#19978;&#30340;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#26159;&#38750;&#26500;&#36896;&#24615;&#30340;&#65292;&#23454;&#38469;&#20351;&#29992;&#30340;&#32593;&#32476;&#27809;&#26377;&#36924;&#36817;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#30340;&#36924;&#36817;&#24615;&#36136;&#26159;&#20551;&#35774;&#30340;&#65292;&#32780;&#19981;&#26159;&#34987;&#35777;&#26126;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23637;&#31034;&#23545;&#20110;&#36275;&#22815;&#20809;&#28369;&#30340;&#20989;&#25968;&#65292;&#20855;&#26377;&#38543;&#26426;&#29983;&#25104;&#30340;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;ReLU&#32593;&#32476;&#21487;&#20197;&#22312;&#39640;&#27010;&#29575;&#19979;&#23454;&#29616;$O(m^{-1/2})$&#30340;$L_{\infty}$&#35823;&#24046;&#65292;&#20854;&#20013;$m$&#26159;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17142v1 Announce Type: cross  Abstract: Neural networks are regularly employed in adaptive control of nonlinear systems and related methods o reinforcement learning. A common architecture uses a neural network with a single hidden layer (i.e. a shallow network), in which the weights and biases are fixed in advance and only the output layer is trained. While classical results show that there exist neural networks of this type that can approximate arbitrary continuous functions over bounded regions, they are non-constructive, and the networks used in practice have no approximation guarantees. Thus, the approximation properties required for control with neural networks are assumed, rather than proved. In this paper, we aim to fill this gap by showing that for sufficiently smooth functions, ReLU networks with randomly generated weights and biases achieve $L_{\infty}$ error of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It suffices to generate the wei
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20998;&#31867;&#22120;&#22312;&#19981;&#21516;&#30142;&#30149;&#38388;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#22312;&#24191;&#27867;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#38750;&#30284;&#30151;&#35797;&#39564;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.17135</link><description>&lt;p&gt;
&#25506;&#32034;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20998;&#31867;&#22120;&#22312;&#30142;&#30149;&#38388;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers Across Diseases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20998;&#31867;&#22120;&#22312;&#19981;&#21516;&#30142;&#30149;&#38388;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21457;&#29616;&#22312;&#24191;&#27867;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#38750;&#30284;&#30151;&#35797;&#39564;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#23545;&#20110;&#21307;&#23398;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#22686;&#24378;&#20854;&#25104;&#21151;&#65292;&#22312;&#25307;&#21215;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#36164;&#26684;&#20998;&#31867;&#22312;&#24191;&#27867;&#20020;&#24202;&#35797;&#39564;&#33539;&#22260;&#20869;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20174;&#38454;&#27573;3&#30284;&#30151;&#35797;&#39564;&#24320;&#22987;&#65292;&#26631;&#35760;&#26377;&#19971;&#31181;&#36164;&#26684;&#25490;&#38500;&#26465;&#20214;&#65292;&#28982;&#21518;&#30830;&#23450;&#27169;&#22411;&#22312;&#38750;&#30284;&#30151;&#21644;&#38750;&#38454;&#27573;3&#35797;&#39564;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#20026;&#35780;&#20272;&#27492;&#38382;&#39064;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#20116;&#31181;&#35797;&#39564;&#31867;&#22411;&#30340;&#36164;&#26684;&#26631;&#20934;&#25968;&#25454;&#65306;&#65288;1&#65289;&#20854;&#20182;&#38454;&#27573;3&#30284;&#30151;&#35797;&#39564;&#65292;&#65288;2&#65289;&#30284;&#30151;&#38454;&#27573;1&#21644;2&#35797;&#39564;&#65292;&#65288;3&#65289;&#24515;&#33039;&#30149;&#35797;&#39564;&#65292;&#65288;4&#65289;2&#22411;&#31958;&#23615;&#30149;&#35797;&#39564;&#21644;&#65288;5&#65289;&#20219;&#20309;&#30142;&#30149;&#30340;&#35266;&#23519;&#24615;&#35797;&#39564;&#65292;&#36328;&#19971;&#31181;&#25490;&#38500;&#31867;&#22411;&#20849;&#28085;&#30422;&#20102;2,490&#20010;&#24050;&#26631;&#27880;&#30340;&#36164;&#26684;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#24191;&#27867;&#30284;&#30151;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#38750;&#30284;&#30151;&#35797;&#39564;&#20013;&#24120;&#35265;&#30340;&#26631;&#20934;&#65292;&#22914;&#33258;&#36523;&#20813;&#30123;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#35832;&#22914;&#32570;&#20047;&#26631;&#20934;&#31561;&#26631;&#20934;&#26102;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17135v1 Announce Type: new  Abstract: Clinical trials are pivotal in medical research, and NLP can enhance their success, with application in recruitment. This study aims to evaluate the generalizability of eligibility classification across a broad spectrum of clinical trials. Starting with phase 3 cancer trials, annotated with seven eligibility exclusions, then to determine how well models can generalize to non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility criteria data for five types of trials: (1) additional phase 3 cancer trials, (2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes trials, and (5) observational trials for any disease, comprising 2,490 annotated eligibility criteria across seven exclusion types. Our results show that models trained on the extensive cancer dataset can effectively handle criteria commonly found in non-cancer trials, such as autoimmune diseases. However, they struggle with criteria disp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#20808;&#21069;&#22312;&#23569;&#20110;&#19968;&#27425;&#23398;&#20064;&#20013;&#25552;&#20986;&#30340;&#31616;&#21333;&#33976;&#39311;&#25216;&#26415;&#22312;&#21407;&#22411;&#36719;&#26631;&#31614;&#33976;&#39311;&#20013;&#30340;&#28508;&#21147;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#25972;&#21512;&#20248;&#21270;&#27493;&#39588;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17130</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#21407;&#22411;&#30340;&#36719;&#26631;&#31614;&#25968;&#25454;&#33976;&#39311;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#31867;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#20808;&#21069;&#22312;&#23569;&#20110;&#19968;&#27425;&#23398;&#20064;&#20013;&#25552;&#20986;&#30340;&#31616;&#21333;&#33976;&#39311;&#25216;&#26415;&#22312;&#21407;&#22411;&#36719;&#26631;&#31614;&#33976;&#39311;&#20013;&#30340;&#28508;&#21147;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#25972;&#21512;&#20248;&#21270;&#27493;&#39588;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26088;&#22312;&#36890;&#36807;&#23569;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#39033;&#21512;&#25104;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24403;&#36825;&#20123;&#25968;&#25454;&#34987;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#33021;&#22815;&#37325;&#29616;&#25110;&#36924;&#36817;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#23601;&#22909;&#20687;&#23427;&#26159;&#22312;&#25972;&#20010;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#33976;&#39311;&#26041;&#27861;&#36890;&#24120;&#19982;&#29305;&#23450;&#30340;ML&#31639;&#27861;&#26377;&#20851;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#25991;&#29486;&#20027;&#35201;&#28041;&#21450;&#22312;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32972;&#26223;&#19979;&#23545;&#22823;&#37327;&#22270;&#20687;&#30340;&#33976;&#39311;&#65292;&#20294;&#34920;&#26684;&#25968;&#25454;&#30340;&#33976;&#39311;&#20195;&#34920;&#24615;&#36739;&#20302;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#29702;&#35770;&#35270;&#35282;&#19978;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#23569;&#20110;&#19968;&#27425;&#23398;&#20064;&#20013;&#25552;&#20986;&#30340;&#31616;&#21333;&#33976;&#39311;&#25216;&#26415;&#22312;&#21407;&#22411;&#36719;&#26631;&#31614;&#33976;&#39311;&#20013;&#30340;&#28508;&#21147;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#22312;&#33976;&#39311;&#36807;&#31243;&#20013;&#25972;&#21512;&#20248;&#21270;&#27493;&#39588;&#65292;&#25512;&#21160;&#22522;&#20110;&#21407;&#22411;&#30340;&#36719;&#26631;&#31614;&#33976;&#39311;&#22312;&#20998;&#31867;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;&#35813;&#20998;&#26512;&#26159;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17130v1 Announce Type: cross  Abstract: Dataset distillation aims at synthesizing a dataset by a small number of artificially generated data items, which, when used as training data, reproduce or approximate a machine learning (ML) model as if it were trained on the entire original dataset. Consequently, data distillation methods are usually tied to a specific ML algorithm. While recent literature deals mainly with distillation of large collections of images in the context of neural network models, tabular data distillation is much less represented and mainly focused on a theoretical perspective. The current paper explores the potential of a simple distillation technique previously proposed in the context of Less-than-one shot learning. The main goal is to push further the performance of prototype-based soft-labels distillation in terms of classification accuracy, by integrating optimization steps in the distillation process. The analysis is performed on real-world data sets
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#25351;&#23548;&#22810;&#27493;&#28436;&#31034;&#20013;&#38544;&#21547;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#25628;&#32034;&#65292;&#20197;&#21450;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#28436;&#31034;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#12290;</title><link>https://arxiv.org/abs/2403.17124</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#35745;&#21010;&#22522;&#20110;&#28436;&#31034;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#36827;&#34892;&#33853;&#23454;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Plans in Demonstrations Through Counterfactual Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17124
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;LLMs&#26469;&#25351;&#23548;&#22810;&#27493;&#28436;&#31034;&#20013;&#38544;&#21547;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#32422;&#26463;&#30340;&#25628;&#32034;&#65292;&#20197;&#21450;&#36890;&#36807;&#21453;&#20107;&#23454;&#24178;&#25200;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#28436;&#31034;&#29366;&#24577;&#31354;&#38388;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#25512;&#29702;&#22522;&#20110;&#29289;&#29702;&#39046;&#22495;&#33853;&#23454;&#22312;&#20307;&#29616;&#26234;&#33021;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#19987;&#27880;&#20110;&#30452;&#25509;&#21033;&#29992;LLMs&#22312;&#31526;&#21495;&#31354;&#38388;&#20869;&#35268;&#21010;&#65292;&#36825;&#39033;&#24037;&#20316;&#20351;&#29992;LLMs&#25351;&#23548;&#20219;&#21153;&#32467;&#26500;&#30340;&#25628;&#32034;&#65292;&#38544;&#21547;&#22312;&#22810;&#27493;&#28436;&#31034;&#20013;&#30340;&#32422;&#26463;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#25805;&#32437;&#35268;&#21010;&#25991;&#29486;&#20013;&#30340;&#27169;&#24335;&#26063;&#30340;&#27010;&#24565;&#65292;&#23427;&#25353;&#29031;&#29305;&#23450;&#36816;&#21160;&#32422;&#26463;&#23558;&#26426;&#22120;&#20154;&#37197;&#32622;&#20998;&#32452;&#65292;&#20316;&#20026;LLM&#39640;&#32423;&#35821;&#35328;&#34920;&#31034;&#21644;&#26426;&#22120;&#20154;&#20302;&#32423;&#29289;&#29702;&#36712;&#36857;&#20043;&#38388;&#30340;&#25277;&#35937;&#23618;&#12290;&#36890;&#36807;&#29992;&#21512;&#25104;&#24178;&#25200;&#37325;&#26032;&#25773;&#25918;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#35206;&#30422;&#28436;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#24182;&#39069;&#22806;&#29983;&#25104;&#25104;&#21151;&#25191;&#34892;&#20197;&#21450;&#26410;&#23436;&#25104;&#20219;&#21153;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#23398;&#20064;&#26694;&#26550;&#35757;&#32451;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17124v1 Announce Type: cross  Abstract: Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural networ
&lt;/p&gt;</description></item><item><title>&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#38556;&#65292;&#24182;&#23637;&#31034;&#20102;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#30456;&#36739;&#20110;&#20840;&#25209;&#27425;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17105</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Langevin Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#38556;&#65292;&#24182;&#23637;&#31034;&#20102;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#30456;&#36739;&#20110;&#20840;&#25209;&#27425;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#8221;&#26159;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#27861;&#24459;&#25152;&#30830;&#20445;&#30340;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26426;&#22120;&#21453;&#36951;&#24536;&#26088;&#22312;&#39640;&#25928;&#22320;&#28040;&#38500;&#24050;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#19978;&#26576;&#20123;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#36817;&#20284;&#20110;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24102;&#26377;&#38544;&#31169;&#20445;&#38556;&#30340;&#22122;&#22768;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#21453;&#36951;&#24536;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20984;&#24615;&#20551;&#35774;&#19979;&#30340;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20840;&#25209;&#27425;&#23545;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#22312;&#38544;&#31169;&#22797;&#26434;&#24230;&#26435;&#34913;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21453;&#36951;&#24536;&#26041;&#27861;&#20855;&#26377;&#35832;&#22810;&#31639;&#27861;&#20248;&#21183;&#65292;&#21253;&#25324;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#30340;&#22797;&#26434;&#24230;&#33410;&#30465;&#65292;&#20197;&#21450;&#25903;&#25345;&#39034;&#24207;&#21644;&#25209;&#37327;&#21453;&#36951;&#24536;&#12290;&#20026;&#20102;&#26816;&#39564;&#25105;&#20204;&#26041;&#27861;&#30340;&#38544;&#31169;-&#25928;&#29992;-&#22797;&#26434;&#24230;&#26435;&#34913;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17105v1 Announce Type: new  Abstract: ``The right to be forgotten'' ensured by laws for user data privacy becomes increasingly important. Machine unlearning aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch. This work proposes stochastic gradient Langevin unlearning, the first unlearning framework based on noisy stochastic gradient descent (SGD) with privacy guarantees for approximate unlearning problems under convexity assumption. Our results show that mini-batch gradient updates provide a superior privacy-complexity trade-off compared to the full-batch counterpart. There are numerous algorithmic benefits of our unlearning approach, including complexity saving compared to retraining, and supporting sequential and batch unlearning. To examine the privacy-utility-complexity trade-off of our method, we conduct experiments on benchmark datasets compared 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;SynFog&#21512;&#25104;&#38654;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#31471;&#21040;&#31471;&#20223;&#30495;&#31649;&#36947;&#29983;&#25104;&#36924;&#30495;&#30340;&#38654;&#22270;&#20687;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#30495;&#23454;&#19990;&#30028;&#21435;&#38654;&#25216;&#26415;&#30740;&#31350;&#25552;&#20379;&#20102;&#20808;&#36827;&#24037;&#20855;</title><link>https://arxiv.org/abs/2403.17094</link><description>&lt;p&gt;
SynFog: &#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#25104;&#20687;&#27169;&#25311;&#30340;&#36924;&#30495;&#21512;&#25104;&#38654;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25512;&#36827;&#33258;&#21160;&#39550;&#39542;&#20013;&#30495;&#23454;&#19990;&#30028;&#21435;&#38654;&#25216;&#26415;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17094
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;SynFog&#21512;&#25104;&#38654;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#31471;&#21040;&#31471;&#20223;&#30495;&#31649;&#36947;&#29983;&#25104;&#36924;&#30495;&#30340;&#38654;&#22270;&#20687;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#30495;&#23454;&#19990;&#30028;&#21435;&#38654;&#25216;&#26415;&#30740;&#31350;&#25552;&#20379;&#20102;&#20808;&#36827;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25512;&#36827;&#22522;&#20110;&#23398;&#20064;&#30340;&#21435;&#38654;&#31639;&#27861;&#30740;&#31350;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21512;&#25104;&#38654;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22823;&#27668;&#25955;&#23556;&#27169;&#22411;&#65288;ASM&#65289;&#25110;&#23454;&#26102;&#28210;&#26579;&#24341;&#25806;&#21019;&#24314;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#36890;&#24120;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#27169;&#25311;&#23454;&#38469;&#25104;&#20687;&#36807;&#31243;&#30340;&#36924;&#30495;&#38654;&#22270;&#20687;&#65292;&#36825;&#31181;&#38480;&#21046;&#22952;&#30861;&#20102;&#27169;&#22411;&#20174;&#21512;&#25104;&#25968;&#25454;&#21040;&#30495;&#23454;&#25968;&#25454;&#30340;&#26377;&#25928;&#27867;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#20223;&#30495;&#31649;&#36947;&#65292;&#26088;&#22312;&#29983;&#25104;&#36924;&#30495;&#30340;&#38654;&#22270;&#20687;&#12290;&#35813;&#31649;&#36947;&#20840;&#38754;&#32771;&#34385;&#25972;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#38654;&#26223;&#25104;&#20687;&#36807;&#31243;&#65292;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#25429;&#33719;&#26041;&#27861;&#23494;&#20999;&#30456;&#20851;&#12290;&#22522;&#20110;&#35813;&#31649;&#36947;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SynFog&#30340;&#26032;&#21512;&#25104;&#38654;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#22825;&#31354;&#20809;&#21644;&#20027;&#21160;&#29031;&#26126;&#26465;&#20214;&#65292;&#20197;&#21450;&#19977;&#20010;&#38654;&#27987;&#24230;&#32423;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;SynFog&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17094v1 Announce Type: cross  Abstract: To advance research in learning-based defogging algorithms, various synthetic fog datasets have been developed. However, existing datasets created using the Atmospheric Scattering Model (ASM) or real-time rendering engines often struggle to produce photo-realistic foggy images that accurately mimic the actual imaging process. This limitation hinders the effective generalization of models from synthetic to real data. In this paper, we introduce an end-to-end simulation pipeline designed to generate photo-realistic foggy images. This pipeline comprehensively considers the entire physically-based foggy scene imaging process, closely aligning with real-world image capture methods. Based on this pipeline, we present a new synthetic fog dataset named SynFog, which features both sky light and active lighting conditions, as well as three levels of fog density. Experimental results demonstrate that models trained on SynFog exhibit superior perf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#65292;&#22312;&#26080;&#20154;&#26426;&#23433;&#20840;&#39046;&#22495;&#23454;&#26045;&#38646;&#20449;&#20219;&#26550;&#26500;&#20197;&#25552;&#39640;&#23433;&#20840;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;84.59\%&#30340;&#26080;&#20154;&#26426;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17093</link><description>&lt;p&gt;
&#36890;&#36807;&#38646;&#20449;&#20219;&#26550;&#26500;&#22686;&#24378;&#26080;&#20154;&#26426;&#23433;&#20840;&#65306;&#39640;&#32423;&#28145;&#24230;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep Learning and Explainable AI Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20998;&#26512;&#65292;&#22312;&#26080;&#20154;&#26426;&#23433;&#20840;&#39046;&#22495;&#23454;&#26045;&#38646;&#20449;&#20219;&#26550;&#26500;&#20197;&#25552;&#39640;&#23433;&#20840;&#24615;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;84.59\%&#30340;&#26080;&#20154;&#26426;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#20154;&#26426;&#39046;&#22495;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#20013;&#65292;&#20445;&#35777;&#23433;&#20840;&#25514;&#26045;&#20855;&#26377;&#24377;&#24615;&#21644;&#28165;&#26224;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#23454;&#26045;&#38646;&#20449;&#20219;&#26550;&#26500;&#65288;ZTA&#65289;&#20197;&#22686;&#24378;&#26080;&#20154;&#26426;&#23433;&#20840;&#30340;&#24517;&#35201;&#24615;&#65292;&#22240;&#27492;&#25670;&#33073;&#20102;&#21487;&#33021;&#26292;&#38706;&#28431;&#27934;&#30340;&#20256;&#32479;&#22806;&#22260;&#38450;&#24481;&#12290;&#38646;&#20449;&#20219;&#26550;&#26500;&#65288;ZTA&#65289;&#33539;&#24335;&#35201;&#27714;&#23545;&#25152;&#26377;&#32593;&#32476;&#23454;&#20307;&#21644;&#36890;&#20449;&#36827;&#34892;&#20005;&#26684;&#21644;&#25345;&#32493;&#30340;&#35748;&#35777;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#21644;&#35782;&#21035;&#26080;&#20154;&#26426;&#30340;&#20934;&#30830;&#29575;&#20026;84.59\%&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20869;&#21033;&#29992;&#23556;&#39057;&#20449;&#21495;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#12290;&#31934;&#30830;&#30340;&#35782;&#21035;&#22312;&#38646;&#20449;&#20219;&#26550;&#26500;&#65288;ZTA&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20915;&#23450;&#20102;&#32593;&#32476;&#35775;&#38382;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24037;&#20855;&#65292;&#22914;SHapley Additive exPla
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17093v1 Announce Type: new  Abstract: In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs), the utmost importance lies in guaranteeing resilient and lucid security measures. This study highlights the necessity of implementing a Zero Trust Architecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs), hence departing from conventional perimeter defences that may expose vulnerabilities. The Zero Trust Architecture (ZTA) paradigm requires a rigorous and continuous process of authenticating all network entities and communications. The accuracy of our methodology in detecting and identifying unmanned aerial vehicles (UAVs) is 84.59\%. This is achieved by utilizing Radio Frequency (RF) signals within a Deep Learning framework, a unique method. Precise identification is crucial in Zero Trust Architecture (ZTA), as it determines network access. In addition, the use of eXplainable Artificial Intelligence (XAI) tools such as SHapley Additive exPla
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#33073;&#26426;&#31574;&#30053;&#35780;&#20272;&#20219;&#21153;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#21463;&#32858;&#21512;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#27987;&#32553;&#31995;&#25968;&#25511;&#21046;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;MDP&#20013;&#30340;&#31995;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.17091</link><description>&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#65306;&#29366;&#24577;&#32858;&#21512;&#21644;&#36712;&#36857;&#25968;&#25454;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17091
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20110;&#33073;&#26426;&#31574;&#30053;&#35780;&#20272;&#20219;&#21153;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#21463;&#32858;&#21512;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#27987;&#32553;&#31995;&#25968;&#25511;&#21046;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;MDP&#20013;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20855;&#26377;&#20215;&#20540;&#20989;&#25968;&#21487;&#23454;&#29616;&#24615;&#20294;&#19981;&#20855;&#26377;&#36125;&#23572;&#26364;&#23436;&#22791;&#24615;&#30340;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#33073;&#26426;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21463;&#32858;&#21512;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#27987;&#32553;&#31995;&#25968;&#25511;&#21046;&#30340;&#21457;&#29616;&#65292;&#20197;&#21450;&#25552;&#20379;&#20102;&#20165;&#20855;&#26377;&#20215;&#20540;&#20989;&#25968;&#21487;&#23454;&#29616;&#24615;&#30340;&#33073;&#26426;&#31574;&#30053;&#35780;&#20272;&#30340;&#30456;&#24403;&#23436;&#25972;&#30340;&#22270;&#26223;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26377;&#19977;&#20010;&#65306;1&#65289;&#33073;&#26426;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30001;&#32858;&#21512;&#30340;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#38598;&#20013;&#31995;&#25968;&#20915;&#23450;&#65292;&#36825;&#20010;&#31995;&#25968;&#30001;&#20989;&#25968;&#31867;&#21644;&#33073;&#26426;&#25968;&#25454;&#20998;&#24067;&#20849;&#21516;&#30830;&#23450;&#65292;&#32780;&#19981;&#26159;&#21407;&#22987;MDP&#20013;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17091v1 Announce Type: cross  Abstract: We revisit the problem of offline reinforcement learning with value function realizability but without Bellman completeness. Previous work by Xie and Jiang (2021) and Foster et al. (2022) left open the question whether a bounded concentrability coefficient along with trajectory-based offline data admits a polynomial sample complexity. In this work, we provide a negative answer to this question for the task of offline policy evaluation. In addition to addressing this question, we provide a rather complete picture for offline policy evaluation with only value function realizability. Our primary findings are threefold: 1) The sample complexity of offline policy evaluation is governed by the concentrability coefficient in an aggregated Markov Transition Model jointly determined by the function class and the offline data distribution, rather than that in the original MDP. This unifies and generalizes the ideas of Xie and Jiang (2021) and Fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#25968;&#25454;&#38598;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#25439;&#22833;&#20540;&#30340;&#36873;&#25321;&#65292;&#23558;&#35757;&#32451;&#38598;&#32553;&#20943;&#33267;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17083</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20462;&#21098;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study in Dataset Pruning for Image Super-Resolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#25968;&#25454;&#38598;&#35757;&#32451;&#36164;&#28304;&#38656;&#27714;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#25439;&#22833;&#20540;&#30340;&#36873;&#25321;&#65292;&#23558;&#35757;&#32451;&#38598;&#32553;&#20943;&#33267;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#20013;&#65292;&#20381;&#36182;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#23613;&#31649;&#25552;&#20379;&#20016;&#23500;&#30340;&#35757;&#32451;&#32032;&#26448;&#65292;&#20294;&#20063;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#38598;&#20462;&#21098;&#20316;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#38598;&#32553;&#20943;&#21040;&#22522;&#20110;&#20854;&#25439;&#22833;&#20540;&#32780;&#36873;&#25321;&#30340;&#19968;&#32452;&#26680;&#24515;&#35757;&#32451;&#26679;&#26412;&#12290;&#36890;&#36807;&#20165;&#23558;&#35757;&#32451;&#37325;&#28857;&#25918;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;50%&#19978;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#25439;&#22833;&#20540;&#26368;&#39640;&#30340;&#26679;&#26412;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#25110;&#29978;&#33267;&#36229;&#36807;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#32467;&#26524;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#20855;&#26377;&#26368;&#39640;&#25439;&#22833;&#20540;&#30340;&#21069;5&#65285;&#26679;&#26412;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25490;&#38500;&#36825;&#20123;&#26679;&#26412;&#24182;&#35843;&#25972;&#36873;&#25321;&#20197;&#20559;&#22909;&#26356;&#23481;&#26131;&#30340;&#26679;&#26412;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#35757;&#32451;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17083v1 Announce Type: cross  Abstract: In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword. While offering rich training material, they also demand substantial computational and storage resources. In this work, we analyze dataset pruning as a solution to these challenges. We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model. By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset. Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process. Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes. Our work opens new p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26159;&#19968;&#39033;&#31995;&#32479;&#24615;&#26144;&#23556;&#30740;&#31350;&#65292;&#26088;&#22312;&#20840;&#38754;&#22238;&#39038;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21306;&#22359;&#38142;&#25968;&#25454;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#65292;&#26377;&#21161;&#20110;&#30830;&#23450;&#26410;&#26469;&#30740;&#31350;&#20013;&#24212;&#35813;&#37325;&#28857;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.17081</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#26144;&#23556;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Learning on Blockchain Data: A Systematic Mapping Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26159;&#19968;&#39033;&#31995;&#32479;&#24615;&#26144;&#23556;&#30740;&#31350;&#65292;&#26088;&#22312;&#20840;&#38754;&#22238;&#39038;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21306;&#22359;&#38142;&#25968;&#25454;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#65292;&#26377;&#21161;&#20110;&#30830;&#23450;&#26410;&#26469;&#30740;&#31350;&#20013;&#24212;&#35813;&#37325;&#28857;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17081v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#32972;&#26223;&#65306;&#21306;&#22359;&#38142;&#25216;&#26415;&#22312;&#25991;&#29486;&#21644;&#23454;&#36341;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21306;&#22359;&#38142;&#25216;&#26415;&#20135;&#29983;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#22240;&#27492;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24863;&#20852;&#36259;&#30340;&#35805;&#39064;&#12290; &#30446;&#26631;&#65306;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20840;&#38754;&#22238;&#39038;&#24212;&#29992;&#20110;&#21306;&#22359;&#38142;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#31995;&#32479;&#22320;&#35782;&#21035;&#12289;&#20998;&#26512;&#21644;&#20998;&#31867;&#24212;&#29992;&#20110;&#21306;&#22359;&#38142;&#25968;&#25454;&#30340;&#25991;&#29486;&#12290; &#36825;&#23558;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#26410;&#26469;&#30740;&#31350;&#20013;&#24212;&#35813;&#25237;&#20837;&#26356;&#22810;&#21162;&#21147;&#30340;&#39046;&#22495;&#12290; &#26041;&#27861;&#65306;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#24615;&#26144;&#23556;&#30740;&#31350;&#20197;&#35782;&#21035;&#30456;&#20851;&#25991;&#29486;&#12290;&#26368;&#32456;&#65292;&#36873;&#25321;&#20102;159&#31687;&#25991;&#31456;&#65292;&#24182;&#26681;&#25454;&#21508;&#31181;&#32500;&#24230;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20855;&#20307;&#21253;&#25324;&#39046;&#22495;&#29992;&#20363;&#12289;&#21306;&#22359;&#38142;&#12289;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290; &#32467;&#26524;&#65306;&#22823;&#22810;&#25968;&#35770;&#25991;&#65288;49.7%&#65289;&#23646;&#20110;&#24322;&#24120;&#29992;&#20363;&#12290; &#27604;&#29305;&#24065;&#65288;47.2%
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17081v1 Announce Type: cross  Abstract: Context: Blockchain technology has drawn growing attention in the literature and in practice. Blockchain technology generates considerable amounts of data and has thus been a topic of interest for Machine Learning (ML).   Objective: The objective of this paper is to provide a comprehensive review of the state of the art on machine learning applied to blockchain data. This work aims to systematically identify, analyze, and classify the literature on ML applied to blockchain data. This will allow us to discover the fields where more effort should be placed in future research.   Method: A systematic mapping study has been conducted to identify the relevant literature. Ultimately, 159 articles were selected and classified according to various dimensions, specifically, the domain use case, the blockchain, the data, and the machine learning models.   Results: The majority of the papers (49.7%) fall within the Anomaly use case. Bitcoin (47.2%
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35782;&#21035;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23545;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.17064</link><description>&lt;p&gt;
&#22312;T2I&#27169;&#22411;&#20013;&#36890;&#36807;&#35782;&#21035;&#35821;&#20041;&#26041;&#21521;&#23454;&#29616;&#36830;&#32493;&#12289;&#20027;&#39064;&#29305;&#23450;&#30340;&#23646;&#24615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17064
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23545;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#38480;&#21046;&#65288;&#20363;&#22914;&#8220;&#20154;&#8221;&#21644;&#8220;&#32769;&#24180;&#20154;&#8221;&#20043;&#38388;&#19981;&#23384;&#22312;&#36830;&#32493;&#30340;&#20013;&#38388;&#25551;&#36848;&#30340;&#38598;&#21512;&#65289;&#65292;&#23454;&#29616;&#23545;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#25110;&#29983;&#25104;&#36807;&#31243;&#20197;&#23454;&#29616;&#36825;&#31181;&#25511;&#21046;&#65292;&#20294;&#19981;&#38656;&#35201;&#22266;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#21551;&#29992;&#20840;&#23616;&#32454;&#31890;&#24230;&#23646;&#24615;&#34920;&#36798;&#25511;&#21046;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#31895;&#31890;&#24230;&#23646;&#24615;&#34920;&#36798;&#25511;&#21046;&#65292;&#32780;&#19981;&#33021;&#21516;&#26102;&#20860;&#39038;&#20004;&#32773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#29992;&#30340;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#23384;&#22312;&#21487;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#30340;&#26041;&#21521;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17064v1 Announce Type: cross  Abstract: In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient op
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#38750;&#32447;&#24615;&#36870;&#30340;&#34920;&#36798;&#25968;&#25454;&#20808;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.17042</link><description>&lt;p&gt;
&#21487;&#35777;&#23454;&#40065;&#26834;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#21518;&#39564;&#37319;&#26679;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17042
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#38750;&#32447;&#24615;&#36870;&#30340;&#34920;&#36798;&#25968;&#25454;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#65292;&#30446;&#26631;&#26159;&#20174;&#24050;&#30693;&#25551;&#36848;&#26576;&#31181;&#24863;&#30693;&#25110;&#25104;&#20687;&#27169;&#24335;&#30340;&#24050;&#30693;&#21069;&#21521;&#27169;&#22411;&#25910;&#38598;&#30340;&#23569;&#37327;&#27979;&#37327;&#20013;&#25512;&#26029;&#26410;&#30693;&#22270;&#20687;&#12290;&#30001;&#20110;&#36164;&#28304;&#38480;&#21046;&#65292;&#36825;&#20010;&#20219;&#21153;&#36890;&#24120;&#38750;&#24120;&#19981;&#36866;&#21512;&#65292;&#36825;&#23601;&#38656;&#35201;&#37319;&#32435;&#34920;&#36798;&#20016;&#23500;&#30340;&#20808;&#39564;&#20449;&#24687;&#26469;&#35268;&#33539;&#35299;&#31354;&#38388;&#12290;&#30001;&#20110;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#25104;&#21151;&#65292;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#37325;&#24314;&#20013;&#19968;&#20010;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#34920;&#36798;&#20808;&#39564;&#30340;&#20505;&#36873;&#32773;&#12290;&#20026;&#20102;&#19968;&#27425;&#24615;&#23481;&#32435;&#22810;&#26679;&#30340;&#20219;&#21153;&#65292;&#24320;&#21457;&#23558;&#22270;&#20687;&#20808;&#39564;&#20998;&#24067;&#30340;&#26080;&#26465;&#20214;&#35780;&#20998;&#20989;&#25968;&#19982;&#28789;&#27963;&#30340;&#21069;&#21521;&#27169;&#22411;&#36873;&#25321;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#12289;&#19968;&#33268;&#21644;&#40065;&#26834;&#31639;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#38750;&#32447;&#24615;&#36870;&#30340;&#34920;&#36798;&#25968;&#25454;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17042v1 Announce Type: cross  Abstract: In a great number of tasks in science and engineering, the goal is to infer an unknown image from a small number of measurements collected from a known forward model describing certain sensing or imaging modality. Due to resource constraints, this task is often extremely ill-posed, which necessitates the adoption of expressive prior information to regularize the solution space. Score-based diffusion models, due to its impressive empirical success, have emerged as an appealing candidate of an expressive prior in image reconstruction. In order to accommodate diverse tasks at once, it is of great interest to develop efficient, consistent and robust algorithms that incorporate {\em unconditional} score functions of an image prior distribution in conjunction with flexible choices of forward models.   This work develops an algorithmic framework for employing score-based diffusion models as an expressive data prior in general nonlinear invers
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17040</link><description>&lt;p&gt;
&#29992;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#23545;&#31038;&#20132;&#32593;&#32476;&#12289;&#21270;&#21512;&#29289;&#21644;&#29983;&#29289;&#31995;&#32479;&#31561;&#22797;&#26434;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;&#26368;&#36817;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#20316;&#20026;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22270;&#23398;&#20064;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#32780;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#32534;&#30721;&#21644;&#22788;&#29702;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#19982;SNNs&#32467;&#21512;&#20197;&#25913;&#21892;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;SNN&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#22270;&#20013;&#37325;&#35201;&#30340;&#33410;&#28857;&#21644;&#30456;&#24212;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#19982;&#29616;&#26377;&#22270;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17040v1 Announce Type: new  Abstract: Graph representation learning has become a crucial task in machine learning and data mining due to its potential for modeling complex structures such as social networks, chemical compounds, and biological systems. Spiking neural networks (SNNs) have recently emerged as a promising alternative to traditional neural networks for graph learning tasks, benefiting from their ability to efficiently encode and process temporal and spatial information. In this paper, we propose a novel approach that integrates attention mechanisms with SNNs to improve graph representation learning. Specifically, we introduce an attention mechanism for SNN that can selectively focus on important nodes and corresponding features in a graph during the learning process. We evaluate our proposed method on several benchmark datasets and show that it achieves comparable performance compared to existing graph learning techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;Convolutional Autoencoder-Reservoir Computing-Normalizing Flow&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#38477;&#38454;&#24314;&#27169;&#65292;&#33021;&#22815;&#39640;&#25928;&#25551;&#36848;&#33258;&#28982;&#29616;&#35937;&#30340;&#20851;&#38190;&#21160;&#24577;&#21644;&#32479;&#35745;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.17032</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#38543;&#26426;&#21442;&#25968;&#38477;&#38454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Stochastic parameter reduced-order model based on hybrid machine learning approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;Convolutional Autoencoder-Reservoir Computing-Normalizing Flow&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#38477;&#38454;&#24314;&#27169;&#65292;&#33021;&#22815;&#39640;&#25928;&#25551;&#36848;&#33258;&#28982;&#29616;&#35937;&#30340;&#20851;&#38190;&#21160;&#24577;&#21644;&#32479;&#35745;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#33258;&#28982;&#29616;&#35937;&#20013;&#22797;&#26434;&#31995;&#32479;&#30340;&#36866;&#24403;&#25968;&#23398;&#27169;&#22411;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#21152;&#28145;&#25105;&#20204;&#23545;&#33258;&#28982;&#30340;&#29702;&#35299;&#65292;&#32780;&#19988;&#21487;&#20197;&#29992;&#20110;&#29366;&#24577;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#29616;&#35937;&#30340;&#26497;&#31471;&#22797;&#26434;&#24615;&#20351;&#24471;&#24320;&#21457;&#20840;&#38454;&#27169;&#22411;&#65288;FOMs&#65289;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30740;&#31350;&#35768;&#22810;&#24863;&#20852;&#36259;&#30340;&#37327;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36866;&#24403;&#30340;&#38477;&#38454;&#27169;&#22411;&#65288;ROMs&#65289;&#22240;&#20854;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#25551;&#36848;&#33258;&#28982;&#29616;&#35937;&#20851;&#38190;&#21160;&#24577;&#21644;&#32479;&#35745;&#29305;&#24449;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#20197;&#31896;&#24615;Burgers&#26041;&#31243;&#20026;&#20363;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;-&#20648;&#22791;&#35745;&#31639;-&#27491;&#21017;&#21270;&#27969;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20013;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#24314;&#31435;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#65292;&#32780;&#20648;&#22791;&#35745;&#31639;-&#27491;&#21017;&#21270;&#27969;&#26694;&#26550;&#21017;&#29992;&#20110;&#34920;&#24449;&#28508;&#22312;&#29366;&#24577;&#21464;&#37327;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17032v1 Announce Type: new  Abstract: Establishing appropriate mathematical models for complex systems in natural phenomena not only helps deepen our understanding of nature but can also be used for state estimation and prediction. However, the extreme complexity of natural phenomena makes it extremely challenging to develop full-order models (FOMs) and apply them to studying many quantities of interest. In contrast, appropriate reduced-order models (ROMs) are favored due to their high computational efficiency and ability to describe the key dynamics and statistical characteristics of natural phenomena. Taking the viscous Burgers equation as an example, this paper constructs a Convolutional Autoencoder-Reservoir Computing-Normalizing Flow algorithm framework, where the Convolutional Autoencoder is used to construct latent space representations, and the Reservoir Computing-Normalizing Flow framework is used to characterize the evolution of latent state variables. In this way,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22797;&#21046;&#20102;OpenAI&#22312;TL;DR&#24635;&#32467;&#20013;&#25253;&#36947;&#30340;RLHF&#24378;&#21270;&#23398;&#20064;&#35268;&#27169;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20986;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#22823;&#65292;RLHF&#35757;&#32451;&#30340;Pythia&#27169;&#22411;&#22312;&#21709;&#24212;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2403.17031</link><description>&lt;p&gt;
&#24102;&#26377;PPO&#30340;RLHF&#30340;N+&#23454;&#29616;&#32454;&#33410;&#65306;TL;DR&#24635;&#32467;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17031
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22797;&#21046;&#20102;OpenAI&#22312;TL;DR&#24635;&#32467;&#20013;&#25253;&#36947;&#30340;RLHF&#24378;&#21270;&#23398;&#20064;&#35268;&#27169;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20986;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#22823;&#65292;RLHF&#35757;&#32451;&#30340;Pythia&#27169;&#22411;&#22312;&#21709;&#24212;&#36136;&#37327;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#22797;&#21046;&#20102;OpenAI&#22312;TL;DR&#24635;&#32467;&#24037;&#20316;&#20013;&#25253;&#21578;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#35268;&#27169;&#34892;&#20026;&#12290;&#25105;&#20204;&#20174;&#22836;&#24320;&#22987;&#21019;&#24314;&#20102;&#19968;&#20010;RLHF&#27969;&#27700;&#32447;&#65292;&#21015;&#20030;&#20102;&#36229;&#36807;20&#20010;&#20851;&#38190;&#30340;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#22312;&#22797;&#21046;&#36807;&#31243;&#20013;&#20998;&#20139;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;RLHF Pythia&#27169;&#22411;&#34920;&#29616;&#20986;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#22686;&#38271;&#32780;&#22686;&#21152;&#30340;&#21709;&#24212;&#36136;&#37327;&#30340;&#26174;&#33879;&#25552;&#39640;&#65292;&#25105;&#20204;&#30340;28&#20159;&#12289;69&#20159;&#27169;&#22411;&#20248;&#20110;OpenAI&#21457;&#24067;&#30340;13&#20159;&#26816;&#26597;&#28857;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#26816;&#26597;&#28857;&#21644;&#20195;&#30721;&#65292;&#20197;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#24182;&#21152;&#24555;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17031v1 Announce Type: new  Abstract: This work is the first to openly reproduce the Reinforcement Learning from Human Feedback (RLHF) scaling behaviors reported in OpenAI's seminal TL;DR summarization work. We create an RLHF pipeline from scratch, enumerate over 20 key implementation details, and share key insights during the reproduction. Our RLHF-trained Pythia models demonstrate significant gains in response quality that scale with model size, with our 2.8B, 6.9B models outperforming OpenAI's released 1.3B checkpoint. We publicly release the trained model checkpoints and code to facilitate further research and accelerate progress in the field (\url{https://github.com/vwxyzjn/summarize_from_feedback_details}).
&lt;/p&gt;</description></item><item><title>HEAL-ViT&#26159;&#19987;&#20026;&#20013;&#31243;&#22825;&#27668;&#39044;&#25253;&#35774;&#35745;&#30340;&#22522;&#20110;&#29699;&#24418;&#32593;&#26684;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#22312;&#30697;&#24418;&#26629;&#26684;&#19978;&#22788;&#29702;&#29699;&#24418;&#22825;&#27668;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#22833;&#30495;&#38382;&#39064;&#65292;&#23558;&#32463;&#32428;&#24230;&#26684;&#32593;&#26144;&#23556;&#21040;&#29699;&#24418;&#32593;&#26684;&#65292;&#25552;&#39640;&#20102;&#23545;&#26497;&#22320;&#38468;&#36817;&#25968;&#25454;&#30340;&#24314;&#27169;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17016</link><description>&lt;p&gt;
&#22522;&#20110;&#29699;&#24418;&#32593;&#26684;&#30340;HEAL-ViT&#65306;&#29992;&#20110;&#20013;&#31243;&#22825;&#27668;&#39044;&#25253;&#30340;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
HEAL-ViT: Vision Transformers on a spherical mesh for medium-range weather forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17016
&lt;/p&gt;
&lt;p&gt;
HEAL-ViT&#26159;&#19987;&#20026;&#20013;&#31243;&#22825;&#27668;&#39044;&#25253;&#35774;&#35745;&#30340;&#22522;&#20110;&#29699;&#24418;&#32593;&#26684;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#22312;&#30697;&#24418;&#26629;&#26684;&#19978;&#22788;&#29702;&#29699;&#24418;&#22825;&#27668;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#22833;&#30495;&#38382;&#39064;&#65292;&#23558;&#32463;&#32428;&#24230;&#26684;&#32593;&#26144;&#23556;&#21040;&#29699;&#24418;&#32593;&#26684;&#65292;&#25552;&#39640;&#20102;&#23545;&#26497;&#22320;&#38468;&#36817;&#25968;&#25454;&#30340;&#24314;&#27169;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#21644;&#25216;&#26415;&#22312;&#20135;&#29983;&#29087;&#32451;&#30340;&#20013;&#31243;&#22825;&#27668;&#39044;&#25253;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;&#22914;Pangu-Weather&#12289;FuXi&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#23558;&#22825;&#27668;&#25968;&#25454;&#35270;&#20026;&#30697;&#24418;&#26629;&#26684;&#19978;&#30340;&#22810;&#36890;&#36947;&#22270;&#20687;&#65292;&#20960;&#20046;&#21487;&#20197;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#12290;&#28982;&#32780;&#65292;&#30697;&#24418;&#26629;&#26684;&#36866;&#29992;&#20110;2D&#22270;&#20687;&#65292;&#22825;&#27668;&#25968;&#25454;&#22312;&#26412;&#36136;&#19978;&#26159;&#29699;&#24418;&#30340;&#65292;&#22240;&#27492;&#22312;&#30697;&#24418;&#26629;&#26684;&#19978;&#22312;&#26497;&#22320;&#21306;&#22495;&#20986;&#29616;&#20005;&#37325;&#22833;&#30495;&#65292;&#23548;&#33268;&#29992;&#20110;&#23545;&#26497;&#22320;&#38468;&#36817;&#25968;&#25454;&#24314;&#27169;&#30340;&#35745;&#31639;&#19981;&#25104;&#27604;&#20363;&#12290;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65288;&#22914;GraphCast&#65289;&#19981;&#21463;&#27492;&#38382;&#39064;&#22256;&#25200;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#32463;&#32428;&#24230;&#26684;&#32593;&#26144;&#23556;&#21040;&#29699;&#24418;&#32593;&#26684;&#65292;&#20294;&#36890;&#24120;&#26356;&#21344;&#20869;&#23384;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#23613;&#31649;&#22312;&#31354;&#38388;&#19978;&#26159;&#22343;&#21248;&#30340;&#65292;&#29699;&#24418;&#32593;&#26684;&#24182;&#19981;&#23481;&#26131;&#34987;Transformer&#27169;&#22411;&#38544;&#24335;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17016v1 Announce Type: cross  Abstract: In recent years, a variety of ML architectures and techniques have seen success in producing skillful medium range weather forecasts. In particular, Vision Transformer (ViT)-based models (e.g. Pangu-Weather, FuXi) have shown strong performance, working nearly "out-of-the-box" by treating weather data as a multi-channel image on a rectilinear grid. While a rectilinear grid is appropriate for 2D images, weather data is inherently spherical and thus heavily distorted at the poles on a rectilinear grid, leading to disproportionate compute being used to model data near the poles. Graph-based methods (e.g. GraphCast) do not suffer from this problem, as they map the longitude-latitude grid to a spherical mesh, but are generally more memory intensive and tend to need more compute resources for training and inference. While spatially homogeneous, the spherical mesh does not lend itself readily to be modeled by ViT-based models that implicitly r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#39640;&#20809;&#35889;&#25968;&#25454;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31995;&#21015;&#36866;&#29992;&#20110;&#22686;&#24378;&#39640;&#20809;&#35889;&#25968;&#25454;&#30340;&#21464;&#25442;&#65292;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#22238;&#24402;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17014</link><description>&lt;p&gt;
&#23545;&#39640;&#20809;&#35889;&#25968;&#25454;&#36827;&#34892;&#22238;&#24402;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning for Regression on Hyperspectral Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#39640;&#20809;&#35889;&#25968;&#25454;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#31995;&#21015;&#36866;&#29992;&#20110;&#22686;&#24378;&#39640;&#20809;&#35889;&#25968;&#25454;&#30340;&#21464;&#25442;&#65292;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#22238;&#24402;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#21644;&#29305;&#21035;&#26159;&#39640;&#20809;&#35889;&#25968;&#25454;&#24212;&#29992;&#30340;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#39640;&#20809;&#35889;&#25968;&#25454;&#22238;&#24402;&#20219;&#21153;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#36866;&#29992;&#20110;&#22686;&#24378;&#39640;&#20809;&#35889;&#25968;&#25454;&#30340;&#21464;&#25442;&#65292;&#24182;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22238;&#24402;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21644;&#21464;&#25442;&#26174;&#33879;&#25552;&#39640;&#20102;&#22238;&#24402;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17014v1 Announce Type: cross  Abstract: Contrastive learning has demonstrated great effectiveness in representation learning especially for image classification tasks. However, there is still a shortage in the studies targeting regression tasks, and more specifically applications on hyperspectral data. In this paper, we propose a contrastive learning framework for the regression tasks for hyperspectral data. To this end, we provide a collection of transformations relevant for augmenting hyperspectral data, and investigate contrastive learning for regression. Experiments on synthetic and real hyperspectral datasets show that the proposed framework and transformations significantly improve the performance of regression models, achieving better scores than other state-of-the-art transformations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26102;&#38388;&#31354;&#38388;&#29468;&#24819;&#65288;TSC&#65289;&#30340;&#29468;&#24819;&#65292;&#24378;&#35843;&#35270;&#39057;&#20449;&#21495;&#30340;&#26102;&#38388;&#34920;&#31034;&#20013;&#25658;&#24102;&#37325;&#35201;&#20449;&#24687;&#20869;&#23481;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;VMM&#65289;&#26469;&#39564;&#35777;&#36825;&#19968;&#29468;&#24819;&#12290;</title><link>https://arxiv.org/abs/2403.17013</link><description>&lt;p&gt;
&#36890;&#36807;&#24310;&#36831;&#29615;&#36335;&#20648;&#30041;&#31070;&#32463;&#32593;&#32476;&#23545;&#20107;&#20214;&#30456;&#26426;&#25968;&#25454;&#36827;&#34892;&#26102;&#38388;&#31354;&#38388;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Temporal-Spatial Processing of Event Camera Data via Delay-Loop Reservoir Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17013
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26102;&#38388;&#31354;&#38388;&#29468;&#24819;&#65288;TSC&#65289;&#30340;&#29468;&#24819;&#65292;&#24378;&#35843;&#35270;&#39057;&#20449;&#21495;&#30340;&#26102;&#38388;&#34920;&#31034;&#20013;&#25658;&#24102;&#37325;&#35201;&#20449;&#24687;&#20869;&#23481;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;VMM&#65289;&#26469;&#39564;&#35777;&#36825;&#19968;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#22788;&#29702;&#30340;&#26102;&#38388;&#31354;&#38388;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#20107;&#20214;&#30456;&#26426;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26102;&#38388;&#31354;&#38388;&#29468;&#24819;&#65288;TSC&#65289;&#30340;&#29468;&#24819;&#65292;&#21463;&#21040;&#25105;&#20204;&#20043;&#21069;&#20351;&#29992;&#24310;&#36831;&#29615;&#36335;&#20648;&#30041;&#65288;DLR&#65289;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35270;&#39057;&#22788;&#29702;&#30340;&#30740;&#31350;&#30340;&#21551;&#21457;&#12290;TSC&#20551;&#35774;&#35270;&#39057;&#20449;&#21495;&#30340;&#26102;&#38388;&#34920;&#31034;&#20013;&#25658;&#24102;&#30528;&#37325;&#35201;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20250;&#20174;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#37327;&#36827;&#34892;&#20998;&#21035;&#20248;&#21270;&#20197;&#36827;&#34892;&#26234;&#33021;&#22788;&#29702;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#39564;&#35777;&#25110;&#21542;&#23450;TSC&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23558;&#35270;&#39057;&#25286;&#20998;&#20026;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#37327;&#65292;&#24182;&#20272;&#35745;&#36825;&#20123;&#20998;&#37327;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#12290;&#30001;&#20110;&#35745;&#31639;&#35270;&#39057;&#20114;&#20449;&#24687;&#22797;&#26434;&#19988;&#32791;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#20114;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17013v1 Announce Type: cross  Abstract: This paper describes a temporal-spatial model for video processing with special applications to processing event camera videos. We propose to study a conjecture motivated by our previous study of video processing with delay loop reservoir (DLR) neural network, which we call Temporal-Spatial Conjecture (TSC). The TSC postulates that there is significant information content carried in the temporal representation of a video signal and that machine learning algorithms would benefit from separate optimization of the spatial and temporal components for intelligent processing. To verify or refute the TSC, we propose a Visual Markov Model (VMM) which decompose the video into spatial and temporal components and estimate the mutual information (MI) of these components. Since computation of video mutual information is complex and time consuming, we use a Mutual Information Neural Network to estimate the bounds of the mutual information. Our resul
&lt;/p&gt;</description></item><item><title>SUDO&#26694;&#26550;&#20801;&#35768;&#22312;&#32570;&#20047;&#30495;&#23454;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#20026;&#23454;&#38469;&#25968;&#25454;&#28857;&#20998;&#37197;&#20020;&#26102;&#26631;&#31614;&#24182;&#30452;&#25509;&#20351;&#29992;&#23427;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.17011</link><description>&lt;p&gt;
SUDO&#65306;&#19968;&#31181;&#26080;&#38656;&#30495;&#23454;&#26631;&#27880;&#35780;&#20272;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SUDO: a framework for evaluating clinical artificial intelligence systems without ground-truth annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17011
&lt;/p&gt;
&lt;p&gt;
SUDO&#26694;&#26550;&#20801;&#35768;&#22312;&#32570;&#20047;&#30495;&#23454;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;AI&#31995;&#32479;&#65292;&#36890;&#36807;&#20026;&#23454;&#38469;&#25968;&#25454;&#28857;&#20998;&#37197;&#20020;&#26102;&#26631;&#31614;&#24182;&#30452;&#25509;&#20351;&#29992;&#23427;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#36890;&#24120;&#22312;&#19968;&#20010;&#26410;&#26333;&#20809;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39564;&#35777;&#65288;&#20363;&#22914;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31995;&#32479;&#30340;&#19981;&#21516;&#21307;&#38498;&#30340;&#25968;&#25454;&#65289;&#12290;&#36825;&#31181;&#35780;&#20272;&#36807;&#31243;&#26088;&#22312;&#27169;&#25311;&#23558;AI&#31995;&#32479;&#37096;&#32626;&#22312;&#26410;&#34987;&#31995;&#32479;&#35265;&#36807;&#20294;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#39044;&#35745;&#20250;&#36935;&#21040;&#30340;&#25968;&#25454;&#19978;&#65307;&#28982;&#32780;&#65292;&#24403;&#23454;&#38469;&#25968;&#25454;&#19982;&#26410;&#26333;&#20809;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#26102;&#65292;&#21363;&#20998;&#24067;&#36716;&#31227;&#29616;&#35937;&#65292;&#24182;&#19988;&#32570;&#20047;&#30495;&#23454;&#26631;&#27880;&#26102;&#65292;&#19981;&#28165;&#26970;&#22522;&#20110;AI&#30340;&#21457;&#29616;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#33021;&#21542;&#21463;&#20449;&#20219;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;SUDO&#65292;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#26080;&#38656;&#30495;&#23454;&#26631;&#27880;&#30340;AI&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;SUDO&#20026;&#23454;&#38469;&#25968;&#25454;&#28857;&#20998;&#37197;&#20020;&#26102;&#26631;&#31614;&#65292;&#24182;&#30452;&#25509;&#20351;&#29992;&#36825;&#20123;&#26631;&#31614;&#35757;&#32451;&#19981;&#21516;&#27169;&#22411;&#65292;&#34920;&#29616;&#26368;&#20248;&#30340;&#27169;&#22411;&#34920;&#26126;&#26368;&#21487;&#33021;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17011v1 Announce Type: cross  Abstract: A clinical artificial intelligence (AI) system is often validated on a held-out set of data which it has not been exposed to before (e.g., data from a different hospital with a distinct electronic health record system). This evaluation process is meant to mimic the deployment of an AI system on data in the wild; those which are currently unseen by the system yet are expected to be encountered in a clinical setting. However, when data in the wild differ from the held-out set of data, a phenomenon referred to as distribution shift, and lack ground-truth annotations, it becomes unclear the extent to which AI-based findings can be trusted on data in the wild. Here, we introduce SUDO, a framework for evaluating AI systems without ground-truth annotations. SUDO assigns temporary labels to data points in the wild and directly uses them to train distinct models, with the highest performing model indicative of the most likely label. Through exp
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>DISL&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;514,506&#20010;&#37096;&#32626;&#22312;&#20197;&#22826;&#22346;&#20027;&#32593;&#19978;&#30340;&#29420;&#29305;Solidity&#25991;&#20214;&#65292;&#25104;&#20026;&#20102;&#24320;&#21457;&#26234;&#33021;&#21512;&#32422;&#35774;&#35745;&#24037;&#20855;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.16861</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;Solidity&#26234;&#33021;&#21512;&#32422;&#25968;&#25454;&#38598;&#25512;&#21160;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16861
&lt;/p&gt;
&lt;p&gt;
DISL&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;514,506&#20010;&#37096;&#32626;&#22312;&#20197;&#22826;&#22346;&#20027;&#32593;&#19978;&#30340;&#29420;&#29305;Solidity&#25991;&#20214;&#65292;&#25104;&#20026;&#20102;&#24320;&#21457;&#26234;&#33021;&#21512;&#32422;&#35774;&#35745;&#24037;&#20855;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DISL&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;514,506&#20010;&#37096;&#32626;&#22312;&#20197;&#22826;&#22346;&#20027;&#32593;&#19978;&#30340;&#29420;&#29305;Solidity&#25991;&#20214;&#65292;&#28385;&#36275;&#20102;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#26234;&#33021;&#21512;&#32422;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;DISL&#25104;&#20026;&#20102;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21644;&#20026;&#26234;&#33021;&#21512;&#32422;&#35774;&#35745;&#30340;&#36719;&#20214;&#24037;&#31243;&#24037;&#20855;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#25910;&#38598;&#25130;&#33267;2024&#24180;1&#26376;15&#26085;&#22312;Etherscan&#19978;&#39564;&#35777;&#30340;&#27599;&#20010;&#26234;&#33021;&#21512;&#32422;&#65292;DISL&#22312;&#35268;&#27169;&#21644;&#26102;&#25928;&#24615;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16861v1 Announce Type: cross  Abstract: The DISL dataset features a collection of $514,506$ unique Solidity files that have been deployed to Ethereum mainnet. It caters to the need for a large and diverse dataset of real-world smart contracts. DISL serves as a resource for developing machine learning systems and for benchmarking software engineering tools designed for smart contracts. By aggregating every verified smart contract from Etherscan up to January 15, 2024, DISL surpasses existing datasets in size and recency.
&lt;/p&gt;</description></item><item><title>DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2403.16451</link><description>&lt;p&gt;
DeepMachining: &#38115;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#22312;&#32447;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeepMachining: Online Prediction of Machining Errors of Lathe Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16451
&lt;/p&gt;
&lt;p&gt;
DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;DeepMachining&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#21152;&#24037;&#35823;&#24046;&#12290;&#25105;&#20204;&#22522;&#20110;&#24037;&#21378;&#30340;&#21046;&#36896;&#25968;&#25454;&#26500;&#24314;&#24182;&#35780;&#20272;&#20102;DeepMachining&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29305;&#23450;&#36710;&#24202;&#26426;&#24202;&#25805;&#20316;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21152;&#24037;&#29366;&#24577;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#29305;&#23450;&#21152;&#24037;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepMachining&#22312;&#28041;&#21450;&#19981;&#21516;&#24037;&#20214;&#21644;&#20992;&#20855;&#30340;&#22810;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#39318;&#25209;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#30446;&#26631;&#22359;&#24494;&#35843;&#65292;&#26681;&#25454;&#25968;&#25454;&#28418;&#31227;&#31867;&#22411;&#24494;&#35843;&#19981;&#21516;&#27169;&#22359;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21644;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2403.15905</link><description>&lt;p&gt;
&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#37327;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#30446;&#26631;&#22359;&#24494;&#35843;&#65292;&#26681;&#25454;&#25968;&#25454;&#28418;&#31227;&#31867;&#22411;&#24494;&#35843;&#19981;&#21516;&#27169;&#22359;&#20197;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21644;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20010;&#24615;&#21270;&#20197;&#35299;&#20915;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#22312;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20391;&#37325;&#20110;&#24494;&#35843;&#23436;&#25972;&#22522;&#30784;&#27169;&#22411;&#25110;&#20854;&#26368;&#21518;&#20960;&#23618;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#33021;&#28304;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#20302;&#33021;&#32791;&#33258;&#36866;&#24212;&#20010;&#24615;&#21270;&#26694;&#26550;&#8212;&#8212;&#30446;&#26631;&#22359;&#24494;&#35843;&#65288;TBFT&#65289;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#28418;&#31227;&#21644;&#20010;&#24615;&#21270;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#36755;&#20837;&#32423;&#21035;&#12289;&#29305;&#24449;&#32423;&#21035;&#21644;&#36755;&#20986;&#32423;&#21035;&#12290;&#38024;&#23545;&#27599;&#31181;&#31867;&#22411;&#65292;&#25105;&#20204;&#24494;&#35843;&#19981;&#21516;&#27169;&#22411;&#22359;&#20197;&#23454;&#29616;&#22312;&#38477;&#20302;&#33021;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36755;&#20837;&#32423;&#12289;&#29305;&#24449;&#32423;&#21644;&#36755;&#20986;&#32423;&#23545;&#24212;&#20110;&#24494;&#35843;&#27169;&#22411;&#30340;&#21069;&#31471;&#12289;&#20013;&#27573;&#21644;&#21518;&#31471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15905v1 Announce Type: new  Abstract: The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications. Presently, most approaches focus on fine-tuning either the full base model or its last few layers to adapt to new data, while often neglecting energy costs. However, various types of data drift exist, and fine-tuning the full base model or the last few layers may not result in optimal performance in certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices. We categorize data drift and personalization into three types: input-level, feature-level, and output-level. For each type, we fine-tune different blocks of the model to achieve optimal performance with reduced energy costs. Specifically, input-, feature-, and output-level correspond to fine-tuning the front, middle, and rear blocks of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;NaNa&#21644;MiGu&#20004;&#31181;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#34507;&#30333;&#36136;&#30340;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.14736</link><description>&lt;p&gt;
NaNa&#21644;MiGu&#65306;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#34507;&#30333;&#36136;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14736
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;NaNa&#21644;MiGu&#20004;&#31181;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#34507;&#30333;&#36136;&#30340;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#65292;&#29992;&#20110;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#26159;&#21160;&#24577;&#21464;&#21270;&#30340;&#65292;&#36825;&#23558;&#20915;&#23450;&#34507;&#30333;&#36136;&#30340;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;ProNet&#65292;&#20165;&#35775;&#38382;&#26377;&#38480;&#30340;&#26500;&#35937;&#29305;&#24449;&#21644;&#34507;&#30333;&#36136;&#20391;&#38142;&#29305;&#24449;&#65292;&#23548;&#33268;&#39044;&#27979;&#20013;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#19981;&#20999;&#23454;&#38469;&#21644;&#34507;&#30333;&#36136;&#31867;&#21035;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;NaNa&#21644;MiGu&#65292;&#23558;&#34507;&#30333;&#36136;&#20027;&#38142;&#21270;&#23398;&#21644;&#20391;&#38142;&#29983;&#29289;&#29289;&#29702;&#20449;&#24687;&#32435;&#20837;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#21644;&#20849;&#23884;&#27531;&#24046;&#23398;&#20064;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#34507;&#30333;&#36136;&#30340;&#20998;&#23376;&#29983;&#29289;&#29289;&#29702;&#12289;&#20108;&#32423;&#32467;&#26500;&#12289;&#21270;&#23398;&#38190;&#21644;&#31163;&#23376;&#29305;&#24449;&#26469;&#20419;&#36827;&#34507;&#30333;&#36136;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14736v1 Announce Type: cross  Abstract: Protein classification tasks are essential in drug discovery. Real-world protein structures are dynamic, which will determine the properties of proteins. However, the existing machine learning methods, like ProNet (Wang et al., 2022a), only access limited conformational characteristics and protein side-chain features, leading to impractical protein structure and inaccuracy of protein classes in their predictions. In this paper, we propose novel semantic data augmentation methods, Novel Augmentation of New Node Attributes (NaNa), and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate backbone chemical and side-chain biophysical information into protein classification tasks and a co-embedding residual learning framework. Specifically, we leverage molecular biophysical, secondary structure, chemical bonds, and ionic features of proteins to facilitate protein classification tasks. Furthermore, our semantic augmentation me
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#25991;&#26412;&#21644;&#38899;&#39057;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30456;&#31561;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.14438</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Approach to Device-Directed Speech Detection with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14438
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35774;&#22791;&#23450;&#21521;&#35821;&#38899;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#25991;&#26412;&#21644;&#38899;&#39057;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30456;&#31561;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#21161;&#25163;&#30340;&#20132;&#20114;&#36890;&#24120;&#20174;&#39044;&#23450;&#20041;&#35302;&#21457;&#30701;&#35821;&#24320;&#22987;&#65292;&#28982;&#21518;&#26159;&#29992;&#25143;&#21629;&#20196;&#12290;&#20026;&#20102;&#20351;&#19982;&#21161;&#25163;&#30340;&#20132;&#20114;&#26356;&#30452;&#35266;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#25918;&#24323;&#29992;&#25143;&#24517;&#39035;&#29992;&#35302;&#21457;&#30701;&#35821;&#24320;&#22987;&#27599;&#20010;&#21629;&#20196;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20010;&#20219;&#21153;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#20174;&#38899;&#39057;&#27874;&#24418;&#20013;&#33719;&#24471;&#30340;&#22768;&#23398;&#20449;&#24687;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#35299;&#30721;&#22120;&#36755;&#20986;&#65292;&#20363;&#22914;1-best&#20551;&#35774;&#65292;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#31995;&#32479;&#65292;&#23558;&#22768;&#23398;&#21644;&#35789;&#27719;&#29305;&#24449;&#20197;&#21450;ASR&#35299;&#30721;&#22120;&#20449;&#21495;&#32467;&#21512;&#22312;LLM&#20013;&#12290;&#20351;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30456;&#23545;&#20110;&#20165;&#25991;&#26412;&#21644;&#20165;&#38899;&#39057;&#27169;&#22411;&#25552;&#39640;&#20102;&#30456;&#31561;&#38169;&#35823;&#29575;&#39640;&#36798;39%&#21644;61%&#12290;&#22686;&#21152;LLM&#30340;&#22823;&#23567;&#24182;&#36890;&#36807;&#20302;&#31209;&#35843;&#25972;&#36827;&#34892;&#35757;&#32451;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#30456;&#23545;EER&#20540;&#30340;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14438v1 Announce Type: new  Abstract: Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13374</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#33258;&#36866;&#24212;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13374
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;Robust Average Gradient Algorithm&#65288;RAGA&#65289;&#65292;&#26412;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;RAGA&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22788;&#29702;&#20102;&#22312;&#23384;&#22312;&#24694;&#24847;&#25308;&#21344;&#24237;&#25915;&#20987;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#24179;&#22343;&#26799;&#24230;&#31639;&#27861;&#65288;RAGA&#65289;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20960;&#20309;&#20013;&#20301;&#25968;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#21487;&#20197;&#33258;&#30001;&#36873;&#25321;&#26412;&#22320;&#26356;&#26032;&#30340;&#36718;&#25968;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24377;&#24615;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#25110;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25910;&#25947;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#24378;&#20984;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#22312;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#20998;&#26512;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21482;&#35201;&#24694;&#24847;&#29992;&#25143;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23567;&#20110;&#19968;&#21322;&#65292;RAGA&#23601;&#21487;&#20197;&#20197;$\mathcal{O}({1}/{T^{2/3- \delta}})$&#30340;&#36895;&#24230;&#23454;&#29616;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#65292;&#20854;&#20013;$T$&#20026;&#36845;&#20195;&#27425;&#25968;&#65292;$\delta \in (0, 2/3)$&#65292;&#23545;&#20110;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#21017;&#21576;&#32447;&#24615;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#31283;&#23450;&#28857;&#25110;&#20840;&#23616;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim
&lt;/p&gt;</description></item><item><title>GeRM&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#32467;&#26500;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25968;&#25454;&#21033;&#29992;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24615;&#33021;&#38382;&#39064;&#21644;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#30340;&#24773;&#20917;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13358</link><description>&lt;p&gt;
GeRM&#65306;&#19968;&#31181;&#29992;&#20110;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#28151;&#21512;&#19987;&#23478;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13358
&lt;/p&gt;
&lt;p&gt;
GeRM&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#32467;&#26500;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25968;&#25454;&#21033;&#29992;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24615;&#33021;&#38382;&#39064;&#21644;&#25968;&#25454;&#25910;&#38598;&#22256;&#38590;&#30340;&#24773;&#20917;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#23398;&#20064;&#22312;&#35299;&#20915;&#22810;&#26679;&#21270;&#21644;&#22797;&#26434;&#24773;&#26223;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#21463;&#21040;&#24615;&#33021;&#38382;&#39064;&#21644;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#38598;&#22256;&#38590;&#30340;&#38459;&#30861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeRM&#65288;&#36890;&#29992;&#26426;&#22120;&#20154;&#27169;&#22411;&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#25968;&#25454;&#21033;&#29992;&#31574;&#30053;&#65292;&#20174;&#28436;&#31034;&#21644;&#27425;&#20248;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#36229;&#36234;&#20154;&#31867;&#28436;&#31034;&#30340;&#23616;&#38480;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;VLA&#32593;&#32476;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#24182;&#36755;&#20986;&#21160;&#20316;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#28151;&#21512;&#32467;&#26500;&#65292;GeRM&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#25972;&#20307;&#27169;&#22411;&#23481;&#37327;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;RL&#21442;&#25968;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#22312;&#25511;&#21046;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#23454;GeRM&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;, &#32780;&#19988;&#36824;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13358v1 Announce Type: cross  Abstract: Multi-task robot learning holds significant importance in tackling diverse and complex scenarios. However, current approaches are hindered by performance issues and difficulties in collecting training datasets. In this paper, we propose GeRM (Generalist Robotic Model). We utilize offline reinforcement learning to optimize data utilization strategies to learn from both demonstrations and sub-optimal data, thus surpassing the limitations of human demonstrations. Thereafter, we employ a transformer-based VLA network to process multi-modal inputs and output actions. By introducing the Mixture-of-Experts structure, GeRM allows faster inference speed with higher whole model capacity, and thus resolves the issue of limited RL parameters, enhancing model performance in multi-task learning while controlling computational costs. Through a series of experiments, we demonstrate that GeRM outperforms other methods across all tasks, while also valid
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12151</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#20869;&#23481;&#34701;&#20837;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#22686;&#24378;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12151
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;&#32467;&#21512;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#26377;&#21161;&#20110;&#35299;&#20915;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294;&#29983;&#25104;&#36825;&#31181;&#30693;&#35782;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36890;&#36807;&#35821;&#20041;&#23884;&#20837;&#29983;&#25104;&#21644;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#23558;LLM&#38598;&#25104;&#21040;&#19968;&#20010;&#27969;&#31243;&#20013;&#65292;&#35813;&#27969;&#31243;&#22312;&#35270;&#35273;&#22522;&#30784;&#38646;&#26679;&#26412;&#23545;&#35937;&#29366;&#24577;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#21521;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#24443;&#24213;&#30740;&#31350;&#20102;LLM&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#23884;&#20837;&#19982;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20511;&#37492;&#36825;&#19968;&#28040;&#34701;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#23545;&#31454;&#20105;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20174;&#32780;&#31361;&#20986;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.11996</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#30693;&#35782;&#25552;&#21462;&#12289;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#26234;&#33021;&#22270;&#25512;&#29702;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11996
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22270;&#31639;&#27861;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25581;&#31034;&#35770;&#25991;&#20043;&#38388;&#30340;&#28145;&#20837;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#26448;&#26009;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#25105;&#20204;&#23558;&#19968;&#32452;&#28041;&#21450;&#29983;&#29289;&#26448;&#26009;&#39046;&#22495;&#30340;1,000&#31687;&#31185;&#23398;&#35770;&#25991;&#36716;&#21270;&#20026;&#35814;&#32454;&#30340;&#26412;&#20307;&#30693;&#35782;&#22270;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#26080;&#26631;&#24230;&#29305;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24615;&#21644;&#20171;&#25968;&#20013;&#24515;&#24615;&#30340;&#32452;&#21512;&#25490;&#21517;&#65292;&#25506;&#27979;&#19981;&#21516;&#27010;&#24565;&#20043;&#38388;&#30340;&#22270;&#36941;&#21382;&#36335;&#24452;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#20837;&#30340;&#36328;&#23398;&#31185;&#20851;&#31995;&#65292;&#21487;&#29992;&#20110;&#22238;&#31572;&#26597;&#35810;&#65292;&#35782;&#21035;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#21069;&#25152;&#26410;&#35265;&#30340;&#26448;&#26009;&#35774;&#35745;&#21450;&#20854;&#34892;&#20026;&#12290;&#19968;&#39033;&#27604;&#36739;&#25581;&#31034;&#20102;&#29983;&#29289;&#26448;&#26009;&#21644;&#36125;&#22810;&#33452;&#31532;&#20061;&#20132;&#21709;&#26354;&#20043;&#38388;&#30340;&#35814;&#32454;&#32467;&#26500;&#30456;&#20284;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#36890;&#36807;&#21516;&#26500;&#26144;&#23556;&#20849;&#20139;&#22797;&#26434;&#24615;&#27169;&#24335;&#12290;&#35813;&#31639;&#27861;&#36827;&#19968;&#27493;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20998;&#32423;&#33740;&#19997;&#20307;&#30340;&#22797;&#21512;&#26448;&#26009;&#65292;&#23558;&#22270;&#37319;&#26679;&#30340;&#32852;&#21512;&#21512;&#25104;&#21407;&#29702;&#19982;&#24247;&#23450;&#26031;&#22522;&#12298;&#31532;&#19971;&#32452;&#25104;&#12299;&#20013;&#25552;&#21462;&#30340;&#21407;&#21017;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.10949</link><description>&lt;p&gt;
SelfIE&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SelfIE: Self-Interpretation of Large Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SelfIE&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#35299;&#37322;&#20854;&#23884;&#20837;&#65292;&#25581;&#31034;&#20869;&#37096;&#25512;&#29702;&#65292;&#21253;&#25324;&#36947;&#24503;&#20915;&#31574;&#12289;&#25552;&#31034;&#27880;&#20837;&#21644;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#33719;&#24471;&#31572;&#26696;&#65311;&#35299;&#37322;&#21644;&#25511;&#21046;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;&#21487;&#38752;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#26410;&#26469;&#27169;&#22411;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SelfIE&#65288;&#23884;&#20837;&#30340;&#33258;&#25105;&#35299;&#37322;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;LLMs&#21709;&#24212;&#20851;&#20110;&#32473;&#23450;&#27573;&#33853;&#30340;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#33258;&#24049;&#30340;&#23884;&#20837;&#12290;SelfIE&#33021;&#22815;&#35299;&#37322;&#38544;&#34255;&#23884;&#20837;&#20013;&#30340;&#24320;&#25918;&#19990;&#30028;&#27010;&#24565;&#65292;&#22312;&#26696;&#20363;&#20013;&#25581;&#31034;LLM&#30340;&#20869;&#37096;&#25512;&#29702;&#65292;&#22914;&#20570;&#20986;&#36947;&#24503;&#20915;&#31574;&#12289;&#20869;&#21270;&#25552;&#31034;&#27880;&#20837;&#21644;&#22238;&#24819;&#26377;&#23475;&#30693;&#35782;&#12290;SelfIE&#23545;&#38544;&#34255;&#23884;&#20837;&#30340;&#25991;&#26412;&#25551;&#36848;&#20063;&#24320;&#36767;&#20102;&#25511;&#21046;LLM&#25512;&#29702;&#30340;&#26032;&#36884;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#30563;&#25511;&#21046;&#65292;&#23427;&#20801;&#35768;&#32534;&#36753;&#24320;&#25918;&#24335;&#27010;&#24565;&#65292;&#32780;&#21482;&#38656;&#35201;&#35745;&#31639;&#21333;&#20010;&#23618;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#23558;RLHF&#25193;&#23637;&#21040;&#38544;&#34255;&#30340;&#23884;&#20837;&#65292;&#24182;&#25552;&#20986;&#20102;&#24378;&#21270;&#25511;&#21046;&#26469;&#28040;&#38500;&#26377;&#23475;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.09857</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09857
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#27169;&#22411;&#26088;&#22312;&#22312;&#20445;&#30041;&#26087;&#31867;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#36880;&#27493;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#31232;&#32570;&#26679;&#26412;&#12290;&#29616;&#26377;&#30340;FSCIL&#26041;&#27861;&#36890;&#24120;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#24494;&#35843;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38459;&#30861;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#28508;&#21147;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#22522;&#20110;&#25552;&#31034;&#30340;CIL&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#29992;&#36275;&#22815;&#30340;&#25968;&#25454;&#35757;&#32451;&#25552;&#31034;&#26469;&#20943;&#36731;&#36951;&#24536;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#65288;ASP&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;ASP&#36890;&#36807;&#20174;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;ASP&#20013;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#25552;&#20379;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;&#24635;&#20043;&#65292;ASP&#38450;&#27490;&#20102;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#30340;&#36807;&#25311;&#21512;&#65292;&#24182;&#19981;&#38656;&#35201;&#22312;&#23569;&#26679;&#26412;&#22686;&#37327;&#20219;&#21153;&#20013;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09857v1 Announce Type: cross  Abstract: Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#32570;&#22833;&#27169;&#24577;&#21644;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.09428</link><description>&lt;p&gt;
&#19982;&#37051;&#23621;&#20511;&#23453;&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35299;&#20915;&#32570;&#22833;&#27169;&#24577;&#21644;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#32570;&#22833;&#27169;&#24577;&#21644;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#22914;&#21307;&#30103;&#20445;&#20581;&#12290;&#26412;&#25991;&#23558;&#29616;&#26377;&#20851;&#20110;&#32570;&#22833;&#27169;&#24577;&#30340;&#30740;&#31350;&#25299;&#23637;&#21040;&#20302;&#25968;&#25454;&#24773;&#22659;&#65292;&#21363;&#19968;&#20010;&#19979;&#28216;&#20219;&#21153;&#26082;&#23384;&#22312;&#32570;&#22833;&#27169;&#24577;&#21448;&#23384;&#22312;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#37322;&#25918;&#21464;&#21387;&#22120;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#21033;&#29992;&#29616;&#26377;&#30340;&#20840;&#27169;&#24577;&#25968;&#25454;&#30340;&#20215;&#20540;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#25361;&#25112;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09428v1 Announce Type: new  Abstract: Multimodal machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare. This paper extends the current research into missing modalities to the low-data regime, i.e., a downstream task has both missing modalities and limited sample size issues. This problem setting is particularly challenging and also practical as it is often expensive to get full-modality data and sufficient annotated training samples. We propose to use retrieval-augmented in-context learning to address these two crucial issues by unleashing the potential of a transformer's in-context learning ability. Diverging from existing methods, which primarily belong to the parametric paradigm and often require sufficient training samples, our work exploits the value of the available full-modality data, offering a novel perspective on resolving the challenge. The proposed data-dependent framework exhibits a high
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20840;&#38754;&#35780;&#20272;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#21457;&#29616;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#65289;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02232</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#23545;Mal-API-2019&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02232
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20840;&#38754;&#35780;&#20272;&#20102;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#65292;&#21457;&#29616;&#38598;&#25104;&#26041;&#27861;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#65289;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#25506;&#35752;&#65292;&#37325;&#28857;&#35780;&#20272;&#20102;&#20351;&#29992;Mal-API-2019&#25968;&#25454;&#38598;&#30340;&#21508;&#31181;&#20998;&#31867;&#27169;&#22411;&#12290;&#26088;&#22312;&#36890;&#36807;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#32531;&#35299;&#23041;&#32961;&#26469;&#25512;&#36827;&#32593;&#32476;&#23433;&#20840;&#33021;&#21147;&#12290;&#30740;&#31350;&#25506;&#35752;&#20102;&#38598;&#25104;&#21644;&#38750;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#12289;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#12290;&#29305;&#21035;&#24378;&#35843;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;TF-IDF&#34920;&#31034;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#38598;&#25104;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#22312;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#38480;&#21046;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#19981;&#26029;&#36866;&#24212;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02232v1 Announce Type: cross  Abstract: This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset. The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively. Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance. Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection. The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#21644;LSTM&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;HOGAB&#27169;&#22411;&#65292;&#23545;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22810;&#22270;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00033</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#38454;&#27880;&#24847;&#21147;&#22823;&#33041;&#32593;&#32476;&#20998;&#26512;&#22823;&#40635;&#20351;&#29992;&#32773;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00033
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#21644;LSTM&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;HOGAB&#27169;&#22411;&#65292;&#23545;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22810;&#22270;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#40635;&#30340;&#25345;&#32493;&#20351;&#29992;&#26126;&#26174;&#24433;&#21709;&#20154;&#20204;&#30340;&#29983;&#27963;&#21644;&#20581;&#24247;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;HOGAB&#65288;High-Order Attention Graph Attention&#31070;&#32463;&#32593;&#32476;&#65289;&#27169;&#22411;&#65292;&#20197;&#20998;&#26512;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#23616;&#37096;&#24322;&#24120;&#33041;&#27963;&#21160;&#12290;HOGAB&#23558;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#19982;LSTM&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25429;&#25417;&#22823;&#40635;&#29992;&#25143;fMRI&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#23545;&#37051;&#22495;&#33410;&#28857;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#22686;&#24378;&#38271;&#26399;&#22823;&#40635;&#29992;&#25143;&#30340;&#31038;&#21306;&#32858;&#31867;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#22810;&#22270;&#20998;&#31867;&#20013;&#23454;&#29616;&#20102;85.1%&#30340;AUC&#21644;80.7%&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#32447;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;HODAB&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00033v1 Announce Type: cross  Abstract: The sustained use of marijuana significantly impacts the lives and health of people. In this study, we propose an interpretable novel framework called the HOGAB (High-Order Attention Graph Attention Neural Networks) model to analyze local abnormal brain activity in chronic marijuana users in two datasets. The HOGAB integrates dynamic intrinsic functional networks with LSTM technology to capture temporal patterns in fMRI time series of marijuana users. Moreover, we use the high-order attention module in neighborhood nodes for information fusion and message passing, enhancing community clustering analysis for long-term marijuana users. Furthermore, we improve the overall learning ability of the model by incorporating attention mechanisms, achieving an AUC of 85.1% and an accuracy of 80.7% in multigraph classification. In addition, we compare linear machine learning methods and evaluate the effectiveness of our proposed HODAB model. Speci
&lt;/p&gt;</description></item><item><title>&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#30340;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#24335;&#25191;&#34892;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16731</link><description>&lt;p&gt;
&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerating Graph Neural Networks on Real Processing-In-Memory Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16731
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#30340;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#24335;&#25191;&#34892;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25191;&#34892;&#28041;&#21450;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#65292;&#21518;&#32773;&#22312;&#24635;&#26102;&#38388;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#21463;&#25968;&#25454;&#22312;&#20869;&#23384;&#21644;&#22788;&#29702;&#22120;&#20043;&#38388;&#31227;&#21160;&#30340;&#20005;&#37325;&#29942;&#39048;&#25152;&#38480;&#21046;&#12290;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#22312;&#20869;&#23384;&#38453;&#21015;&#38468;&#36817;&#25110;&#20869;&#37096;&#25918;&#32622;&#31616;&#21333;&#22788;&#29702;&#22120;&#26469;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PyGim&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;PIM&#31995;&#32479;&#19978;&#21152;&#36895;GNNs&#12290;&#25105;&#20204;&#20026;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#23450;&#21046;&#30340;GNN&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#25552;&#20986;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#24182;&#20026;&#23427;&#20204;&#24320;&#21457;&#20102;&#26041;&#20415;&#30340;Python API&#12290;&#25105;&#20204;&#25552;&#20379;&#28151;&#21512;&#24335;GNN&#25191;&#34892;&#65292;&#20854;&#20013;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#20998;&#21035;&#22312;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#21644;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#31995;&#32479;&#20013;&#25191;&#34892;&#65292;&#20197;&#21305;&#37197;&#23427;&#20204;&#30340;&#31639;&#27861;&#29305;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16731v2 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data. Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML framework that accelerates GNNs on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them. We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively, to match their algorithmic nature. We extensively evaluate 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#26694;&#26550;LocalHealth&#65292;&#29992;&#20110;&#39044;&#27979;&#24403;&#22320;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#12290;&#36890;&#36807;&#19982;GPT3.5&#32467;&#21512;&#20351;&#29992;&#65292;&#35813;&#26694;&#26550;&#22312;MH&#30417;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13452</link><description>&lt;p&gt;
&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#26694;&#26550;&#65306;&#20174;&#24403;&#22320;&#25512;&#25991;&#21040;&#24403;&#22320;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#26694;&#26550;LocalHealth&#65292;&#29992;&#20110;&#39044;&#27979;&#24403;&#22320;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#12290;&#36890;&#36807;&#19982;GPT3.5&#32467;&#21512;&#20351;&#29992;&#65292;&#35813;&#26694;&#26550;&#22312;MH&#30417;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;Twitter&#25968;&#25454;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20379;&#20102;&#23427;&#22312;&#24320;&#21457;&#34917;&#20805;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#30417;&#27979;&#20844;&#20849;&#20581;&#24247;&#65292;&#37325;&#28857;&#20851;&#27880;&#31934;&#31070;&#20581;&#24247;&#65288;MH&#65289;&#32467;&#26524;&#12290;&#25105;&#20204;&#20551;&#35774;&#24403;&#22320;&#21457;&#24067;&#30340;&#25512;&#25991;&#21487;&#20197;&#34920;&#26126;&#24403;&#22320;&#30340;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#65292;&#24182;&#25910;&#38598;&#20102;&#26469;&#33258;&#32654;&#22269;765&#20010;&#22320;&#21306;&#65288;&#20154;&#21475;&#26222;&#26597;&#20998;&#32452;&#65289;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#22320;&#21306;&#30340;&#36825;&#20123;&#25512;&#25991;&#19982;&#30142;&#30149;&#25511;&#21046;&#20013;&#24515;&#65288;CDC&#65289;&#25253;&#21578;&#30340;&#30456;&#24212;MH&#32467;&#26524;&#37197;&#23545;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;LocalTweets&#12290;&#20511;&#21161;LocalTweets&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Twitter&#30340;MH&#30417;&#27979;&#31995;&#32479;&#30340;&#39318;&#20010;&#20154;&#21475;&#32423;&#35780;&#20272;&#20219;&#21153;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#26377;&#25928;&#30340;&#26041;&#27861;LocalHealth&#65292;&#29992;&#20110;&#26681;&#25454;LocalTweets&#39044;&#27979;MH&#32467;&#26524;&#12290;&#24403;&#19982;GPT3.5&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;LocalHealth&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;F1&#20540;&#21644;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#36798;&#21040;0.7429&#21644;79.78\%&#65292;F1&#20540;&#25552;&#39640;&#20102;59\%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13452v1 Announce Type: cross  Abstract: Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\% improvement in F
&lt;/p&gt;</description></item><item><title>&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.11800</link><description>&lt;p&gt;
&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65306;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#30340;&#26377;&#38480;&#26102;&#38388;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11800
&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#20445;&#35777;&#20102;&#27599;&#27425;&#36845;&#20195;&#24555;&#36895;&#25910;&#25947;&#21040;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#65292;&#30028;&#38480;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;&#21644;&#28151;&#21512;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22823;&#35268;&#27169;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#20855;&#26377;&#24310;&#36831;&#26356;&#26032;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#26041;&#26696;&#30340;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;&#34429;&#28982;&#24310;&#36831;&#30340;&#24433;&#21709;&#22312;&#20248;&#21270;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#19982;&#24213;&#23618;&#39532;&#23572;&#31185;&#22827;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#20197;&#22609;&#36896;SA&#30340;&#26377;&#38480;&#26102;&#38388;&#24615;&#33021;&#30340;&#26041;&#24335;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#35777;&#26126;&#22312;&#26102;&#38388;&#21464;&#21270;&#26377;&#30028;&#24310;&#36831;&#19979;&#65292;&#24310;&#36831;&#30340;SA&#26356;&#26032;&#35268;&#21017;&#30830;&#20445;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21040;SA&#36816;&#31639;&#31526;&#22266;&#23450;&#28857;&#21608;&#22260;&#30340;&#29699;&#20307;&#20855;&#26377;&#25351;&#25968;&#24555;&#36895;&#30340;&#36895;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#20381;&#36182;&#20110;&#26368;&#22823;&#24310;&#36831;$\tau_{max}$&#21644;&#28151;&#21512;&#26102;&#38388;$\tau_{mix}$&#26041;&#38754;&#26159;\emph{&#32039;&#33268;&#30340;}&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#32039;&#23494;&#30028;&#38480;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24402;&#32435;&#35777;&#26126;&#25216;&#26415;&#65292;&#19982;&#21508;&#31181;&#29616;&#26377;&#24310;&#36831;&#20248;&#21270;&#20998;&#26512;&#19981;&#21516;&#65292;&#23427;&#20381;&#36182;&#20110;&#24314;&#31435;&#26410;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#30340;&#40614;&#20811;&#39118;&#25968;&#25454;&#20013;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#30340;&#37325;&#24314;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#39318;&#27425;&#20351;&#29992;&#20102;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.04866</link><description>&lt;p&gt;
&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#35268;&#21017;&#20998;&#24067;&#30340;&#40614;&#20811;&#39118;&#37325;&#24314;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Room transfer function reconstruction using complex-valued neural networks and irregularly distributed microphones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#30340;&#40614;&#20811;&#39118;&#25968;&#25454;&#20013;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#30340;&#37325;&#24314;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#39318;&#27425;&#20351;&#29992;&#20102;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#24314;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#29992;&#20110;&#35745;&#31639;&#25151;&#38388;&#20869;&#30340;&#22797;&#26434;&#22768;&#22330;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#40614;&#20811;&#39118;&#65292;&#36825;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26368;&#36817;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#20174;&#25151;&#38388;&#20869;&#38646;&#25955;&#28857;&#27979;&#37327;&#24471;&#21040;&#30340;&#26377;&#38480;&#30340;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#26469;&#37325;&#24314;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#22312;&#31532;&#19968;&#20010;&#22768;&#23398;&#20849;&#25391;&#39057;&#29575;&#33539;&#22260;&#20869;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#35268;&#21017;&#20998;&#24067;&#30340;&#40614;&#20811;&#39118;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#20351;&#29992;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#25151;&#38388;&#20256;&#36882;&#20989;&#25968;&#12290;&#20026;&#20102;&#20998;&#26512;&#23558;&#22797;&#20540;&#20248;&#21270;&#24212;&#29992;&#20110;&#25152;&#32771;&#34385;&#20219;&#21153;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#19982;&#26368;&#20808;&#36827;&#30340;&#23454;&#20540;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21644;&#22522;&#20110;&#26680;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing the room transfer functions needed to calculate the complex sound field in a room has several important real-world applications. However, an unpractical number of microphones is often required. Recently, in addition to classical signal processing methods, deep learning techniques have been applied to reconstruct the room transfer function starting from a very limited set of room transfer functions measured at scattered points in the room. In this study, we employ complex-valued neural networks to estimate room transfer functions in the frequency range of the first room resonances, using a few irregularly distributed microphones. To the best of our knowledge, this is the first time complex-valued neural networks are used to estimate room transfer functions. To analyze the benefits of applying complex-valued optimization to the considered task, we compare the proposed technique with a state-of-the-art real-valued neural network method and a state-of-the-art kernel-based si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#26684;&#29983;&#25104;&#33521;&#35821;&#12289;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21644;SVA&#26029;&#35328;&#65292;&#24182;&#25104;&#21151;&#20943;&#23569;&#20102;&#26029;&#35328;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.00093</link><description>&lt;p&gt;
ChIRAAG: &#36890;&#36807;ChatGPT&#29983;&#25104;&#24555;&#36895;&#21644;&#33258;&#21160;&#26029;&#35328;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#26684;&#29983;&#25104;&#33521;&#35821;&#12289;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21644;SVA&#26029;&#35328;&#65292;&#24182;&#25104;&#21151;&#20943;&#23569;&#20102;&#26029;&#35328;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
System Verilog Assertion (SVA)&#30340;&#24418;&#24335;&#21270;&#26159;Formal Property Verification (FPV)&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20294;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#19978;&#65292;SVA&#30340;&#24418;&#24335;&#21270;&#38656;&#35201;&#32463;&#39564;&#20016;&#23500;&#30340;&#19987;&#23478;&#35299;&#37322;&#35268;&#26684;&#12290;&#36825;&#26159;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#26029;&#35328;&#29983;&#25104;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#26684;&#20013;&#29983;&#25104;&#33521;&#35821;&#12289;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21644;SVA&#30340;&#26029;&#35328;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;OpenAI GPT4&#30340;&#33258;&#23450;&#20041;LLM&#29992;&#20110;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#27979;&#35797;&#24179;&#21488;&#26469;&#39564;&#35777;LLM&#29983;&#25104;&#30340;&#26029;&#35328;&#12290;&#21482;&#26377;43%&#30340;LLM&#29983;&#25104;&#30340;&#21407;&#22987;&#26029;&#35328;&#23384;&#22312;&#38169;&#35823;&#65292;&#21253;&#25324;&#35821;&#27861;&#21644;&#36923;&#36753;&#38169;&#35823;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;&#27979;&#35797;&#26696;&#20363;&#22833;&#36133;&#20013;&#24471;&#20986;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#36845;&#20195;&#22320;&#20419;&#20351;LLM&#65292;&#35813;&#27969;&#27700;&#32447;&#22312;&#26368;&#22810;&#20061;&#27425;&#25552;&#31034;&#36845;&#20195;&#21518;&#21487;&#20197;&#29983;&#25104;&#27491;&#30830;&#30340;SVA&#12290;
&lt;/p&gt;
&lt;p&gt;
System Verilog Assertion (SVA) formulation, a critical yet complex task, is a pre-requisite in the Formal Property Verification (FPV) process. Traditionally, SVA formulation involves expert-driven interpretation of specifications. This is time consuming and prone to human error. However, recent advances in Large Language Models (LLM), LLM-informed automatic assertion generation is gaining interest. We designed a novel LLM-based pipeline to generate assertions in English Language, Linear Temporal Logic, and SVA from natural language specifications. We developed a custom LLM-based on OpenAI GPT4 for our experiments. Furthermore, we developed testbenches to verify/validate the LLM-generated assertions. Only 43% of LLM-generated raw assertions had errors, including syntax and logical errors. By iteratively prompting the LLMs using carefully crafted prompts derived from test case failures, the pipeline could generate correct SVAs after a maximum of nine iterations of prompting. Our results 
&lt;/p&gt;</description></item><item><title>&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#30456;&#20284;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#36890;&#20449;&#25104;&#26412;&#29942;&#39048;&#12290;</title><link>https://arxiv.org/abs/2401.07809</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#26368;&#20339;&#25968;&#25454;&#20998;&#35010;
&lt;/p&gt;
&lt;p&gt;
Optimal Data Splitting in Distributed Optimization for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07809
&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#30456;&#20284;&#24615;&#30340;&#31639;&#27861;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#36890;&#20449;&#25104;&#26412;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#36817;&#26469;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19982;&#38750;&#20998;&#24067;&#24335;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#25152;&#38656;&#26102;&#38388;&#36739;&#30701;&#31561;&#35768;&#22810;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20998;&#24067;&#24335;&#26041;&#27861;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#29942;&#39048; - &#36890;&#20449;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#26412;&#22320;&#25968;&#25454;&#30456;&#20284;&#24615;&#12290;&#29305;&#21035;&#26159;&#26377;&#19968;&#31181;&#31639;&#27861;&#21487;&#20197;&#35777;&#26126;&#22320;&#21033;&#29992;&#30456;&#20284;&#24615;&#23646;&#24615;&#12290;&#20294;&#26159;&#65292;&#36825;&#19968;&#32467;&#26524;&#20197;&#21450;&#20854;&#20182;&#20316;&#21697;&#30340;&#32467;&#26524;&#37117;&#26159;&#36890;&#36807;&#19987;&#27880;&#20110;&#36890;&#20449;&#26126;&#26174;&#27604;&#26412;&#22320;&#35745;&#31639;&#26356;&#26114;&#36149;&#36825;&#19968;&#20107;&#23454;&#32780;&#35299;&#20915;&#20102;&#36890;&#20449;&#29942;&#39048;&#65292;&#24182;&#26410;&#32771;&#34385;&#21040;&#32593;&#32476;&#35774;&#22791;&#30340;&#21508;&#31181;&#23481;&#37327;&#20197;&#21450;&#36890;&#20449;&#26102;&#38388;&#21644;&#26412;&#22320;&#35745;&#31639;&#36153;&#29992;&#20043;&#38388;&#30340;&#19981;&#21516;&#20851;&#31995;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#19968;&#35774;&#32622;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07809v2 Announce Type: replace-cross  Abstract: The distributed optimization problem has become increasingly relevant recently. It has a lot of advantages such as processing a large amount of data in less time compared to non-distributed methods. However, most distributed approaches suffer from a significant bottleneck - the cost of communications. Therefore, a large amount of research has recently been directed at solving this problem. One such approach uses local data similarity. In particular, there exists an algorithm provably optimally exploiting the similarity property. But this result, as well as results from other works solve the communication bottleneck by focusing only on the fact that communication is significantly more expensive than local computing and does not take into account the various capacities of network devices and the different relationship between communication time and local computing expenses. We consider this setup and the objective of this study i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;-&#24182;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#35774;&#32622;&#20013;&#23545;&#28608;&#27963;&#21644;&#26799;&#24230;&#21516;&#26102;&#36827;&#34892;&#21387;&#32553;&#23545;&#25910;&#25947;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26799;&#24230;&#38656;&#35201;&#27604;&#28608;&#27963;&#26356;&#36731;&#24494;&#30340;&#21387;&#32553;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.07788</link><description>&lt;p&gt;
&#27169;&#22411;&#24182;&#34892;&#35757;&#32451;&#20013;&#30340;&#28608;&#27963;&#21644;&#26799;&#24230;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Activations and Gradients Compression for Model-Parallel Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;-&#24182;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#35774;&#32622;&#20013;&#23545;&#28608;&#27963;&#21644;&#26799;&#24230;&#21516;&#26102;&#36827;&#34892;&#21387;&#32553;&#23545;&#25910;&#25947;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26799;&#24230;&#38656;&#35201;&#27604;&#28608;&#27963;&#26356;&#36731;&#24494;&#30340;&#21387;&#32553;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24222;&#22823;&#30340;&#35745;&#31639;&#26426;&#38598;&#32676;&#12290;&#22312;&#27169;&#22411;&#24182;&#34892;&#35757;&#32451;&#20013;&#65292;&#24403;&#27169;&#22411;&#26550;&#26500;&#22312;&#24037;&#20316;&#32773;&#20043;&#38388;&#34987;&#39034;&#24207;&#20998;&#21106;&#26102;&#65292;&#25104;&#20026;&#35757;&#32451;&#29616;&#20195;&#27169;&#22411;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#20449;&#24687;&#21387;&#32553;&#21487;&#20197;&#24212;&#29992;&#20110;&#20943;&#23569;&#24037;&#20316;&#32773;&#30340;&#36890;&#20449;&#26102;&#38388;&#65292;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#36890;&#24120;&#26159;&#19968;&#20010;&#29942;&#39048;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;&#24182;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#35774;&#32622;&#20013;&#21516;&#26102;&#21387;&#32553;&#28608;&#27963;&#21644;&#26799;&#24230;&#23545;&#25910;&#25947;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35832;&#22914;&#37327;&#21270;&#21644;TopK&#21387;&#32553;&#31561;&#21387;&#32553;&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20102;&#35823;&#24046;&#34917;&#20607;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;TopK&#19982;AQ-SGD&#27599;&#25209;&#27425;&#35823;&#24046;&#21453;&#39304;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26799;&#24230;&#38656;&#35201;&#27604;&#28608;&#27963;&#26356;&#36731;&#24494;&#30340;&#21387;&#32553;&#29575;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;$K=10\%$&#26159;&#26368;&#20302;&#30340;TopK&#21387;&#32553;&#32423;&#21035;&#65292;&#36825;&#19981;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07788v2 Announce Type: replace  Abstract: Large neural networks require enormous computational clusters of machines. Model-parallel training, when the model architecture is partitioned sequentially between workers, is a popular approach for training modern models. Information compression can be applied to decrease workers communication time, as it is often a bottleneck in such systems. This work explores how simultaneous compression of activations and gradients in model-parallel distributed training setup affects convergence. We analyze compression methods such as quantization and TopK compression, and also experiment with error compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error feedback approach. We conduct experiments on image classification and language model fine-tuning tasks. Our findings demonstrate that gradients require milder compression rates than activations. We observe that $K=10\%$ is the lowest TopK compression level, which does not h
&lt;/p&gt;</description></item><item><title>AI&#21644;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#22312;&#30740;&#31350;&#21457;&#29616;&#21644;&#24635;&#32467;&#26041;&#38754;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#21253;&#25324;&#33021;&#22815;&#26356;&#24555;&#22320;&#25214;&#21040;&#30456;&#20851;&#25991;&#29486;&#21644;&#29992;&#31616;&#27905;&#35821;&#35328;&#24635;&#32467;&#30740;&#31350;&#25991;&#31456;&#30340;&#35201;&#28857;&#12290;</title><link>https://arxiv.org/abs/2401.06795</link><description>&lt;p&gt;
AI&#21644;&#29983;&#25104;&#24335;AI&#29992;&#20110;&#30740;&#31350;&#21457;&#29616;&#19982;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
AI and Generative AI for Research Discovery and Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06795
&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#22312;&#30740;&#31350;&#21457;&#29616;&#21644;&#24635;&#32467;&#26041;&#38754;&#26377;&#37325;&#22823;&#24433;&#21709;&#65292;&#21253;&#25324;&#33021;&#22815;&#26356;&#24555;&#22320;&#25214;&#21040;&#30456;&#20851;&#25991;&#29486;&#21644;&#29992;&#31616;&#27905;&#35821;&#35328;&#24635;&#32467;&#30740;&#31350;&#25991;&#31456;&#30340;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#21644;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#21253;&#25324;&#20381;&#36182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22914;ChatGPT&#65292;&#20170;&#24180;&#36805;&#36895;&#23835;&#36215;&#65292;&#20026;&#22686;&#21152;&#24037;&#20316;&#25928;&#29575;&#21644;&#25913;&#21892;&#29983;&#27963;&#21019;&#36896;&#20102;&#38590;&#20197;&#32622;&#20449;&#30340;&#26426;&#20250;&#12290;&#32479;&#35745;&#23398;&#23478;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#24050;&#32463;&#24320;&#22987;&#20197;&#22810;&#31181;&#26041;&#24335;&#20307;&#39564;&#21040;&#36825;&#20123;&#24037;&#20855;&#30340;&#22909;&#22788;&#65292;&#27604;&#22914;&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#32534;&#31243;&#20195;&#30721;&#20197;&#20998;&#26512;&#25968;&#25454;&#25110;&#25311;&#21512;&#32479;&#35745;&#27169;&#22411;&#12290;&#36825;&#20123;&#24037;&#20855;&#21487;&#20197;&#22312;&#30740;&#31350;&#21457;&#29616;&#21644;&#24635;&#32467;&#26041;&#38754;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#20043;&#19968;&#12290;&#27491;&#22312;&#24320;&#21457;&#29420;&#31435;&#24037;&#20855;&#21644;&#25554;&#20214;&#32473;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#27604;2023&#24180;&#20043;&#21069;&#30340;&#25628;&#32034;&#24037;&#20855;&#26356;&#24555;&#22320;&#25214;&#21040;&#30456;&#20851;&#25991;&#29486;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#24050;&#32463;&#21457;&#23637;&#21040;&#21487;&#20197;&#29992;&#31616;&#27905;&#30340;&#35821;&#35328;&#24635;&#32467;&#21644;&#25552;&#21462;&#30740;&#31350;&#25991;&#31456;&#30340;&#35201;&#28857;&#30340;&#31243;&#24230;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;LLMs&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#29992;&#20110;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06795v2 Announce Type: replace-cross  Abstract: AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simula
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20316;&#20026;&#20266;&#20108;&#32500;&#65288;P2D&#65289;&#30005;&#27744;&#27169;&#22411;&#26657;&#20934;&#30340;&#20195;&#29702;&#65292;&#36827;&#34892;&#20102;&#21442;&#25968;&#25512;&#26029;&#30740;&#31350;&#65292;&#21487;&#20197;&#20943;&#23569;Bayesian&#26657;&#20934;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2312.17336</link><description>&lt;p&gt;
Li-ion&#30005;&#27744;&#27169;&#22411;&#30340;PINN&#20195;&#29702;&#29992;&#20110;&#21442;&#25968;&#25512;&#26029;&#12290;&#31532;II&#37096;&#20998;&#65306;&#27491;&#21017;&#21270;&#21644;&#20266;&#20108;&#32500;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PINN surrogate of Li-ion battery models for parameter inference. Part II: Regularization and application of the pseudo-2D model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17336
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20316;&#20026;&#20266;&#20108;&#32500;&#65288;P2D&#65289;&#30005;&#27744;&#27169;&#22411;&#26657;&#20934;&#30340;&#20195;&#29702;&#65292;&#36827;&#34892;&#20102;&#21442;&#25968;&#25512;&#26029;&#30740;&#31350;&#65292;&#21487;&#20197;&#20943;&#23569;Bayesian&#26657;&#20934;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bayesian&#21442;&#25968;&#25512;&#26029;&#23545;&#25913;&#36827;&#38146;&#31163;&#23376;&#30005;&#27744;&#35786;&#26029;&#26377;&#29992;&#65292;&#24182;&#26377;&#21161;&#20110;&#21046;&#23450;&#30005;&#27744;&#32769;&#21270;&#27169;&#22411;&#12290;&#20026;&#20102;&#38477;&#20302;Bayesian&#26657;&#20934;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21487;&#20197;&#29992;&#26356;&#24555;&#30340;&#20195;&#29702;&#26367;&#25442;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#12290;&#23558;&#19968;&#20010;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#24320;&#21457;&#20026;&#20266;&#20108;&#32500;&#65288;P2D&#65289;&#30005;&#27744;&#27169;&#22411;&#26657;&#20934;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17336v2 Announce Type: replace  Abstract: Bayesian parameter inference is useful to improve Li-ion battery diagnostics and can help formulate battery aging models. However, it is computationally intensive and cannot be easily repeated for multiple cycles, multiple operating conditions, or multiple replicate cells. To reduce the computational cost of Bayesian calibration, numerical solvers for physics-based models can be replaced with faster surrogates. A physics-informed neural network (PINN) is developed as a surrogate for the pseudo-2D (P2D) battery model calibration. For the P2D surrogate, additional training regularization was needed as compared to the PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and P2D surrogate models are exercised for parameter inference and compared to data obtained from a direct numerical solution of the governing equations. A parameter inference study highlights the ability to use these PINNs to calibrate scaling paramet
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;PINN&#20195;&#29702;&#27169;&#22411;&#26367;&#20195;&#22522;&#20110;&#29289;&#29702;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#65292;&#24110;&#21161;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#35786;&#26029;&#30005;&#27744;&#20869;&#37096;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2312.17329</link><description>&lt;p&gt;
&#22522;&#20110; PINN &#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#21442;&#25968;&#25512;&#26029;&#20195;&#29702;&#27169;&#22411;&#12290;&#31532;&#19968;&#37096;&#20998;&#65306;&#23454;&#29616;&#19982;&#22810;&#20445;&#30495;&#24230;&#31561;&#32423;&#32467;&#26500;&#29992;&#20110;&#21333;&#31890;&#23376;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PINN surrogate of Li-ion battery models for parameter inference. Part I: Implementation and multi-fidelity hierarchies for the single-particle model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17329
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;PINN&#20195;&#29702;&#27169;&#22411;&#26367;&#20195;&#22522;&#20110;&#29289;&#29702;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#65292;&#24110;&#21161;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#35786;&#26029;&#30005;&#27744;&#20869;&#37096;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#21644;&#20248;&#21270;&#33021;&#37327;&#23384;&#20648;&#38656;&#27714;&#38656;&#35201;&#32771;&#34385;&#38146;&#31163;&#23376;&#30005;&#27744;&#32769;&#21270;&#21160;&#24577;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#25216;&#26415;&#20934;&#30830;&#24555;&#36895;&#22320;&#35786;&#26029;&#30005;&#27744;&#20869;&#37096;&#29366;&#24577;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#65288;&#22914;&#21333;&#31890;&#23376;&#27169;&#22411;&#65288;SPM&#65289;&#21644;&#20266;&#20108;&#32500;&#65288;P2D&#65289;&#27169;&#22411;&#65289;&#26367;&#20195;&#65292;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20195;&#29702;&#26469;&#20943;&#23569;&#30830;&#23450;&#30005;&#27744;&#20869;&#37096;&#29366;&#24577;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#19968;&#39033;&#20004;&#37096;&#20998;&#31995;&#21015;&#30340;&#31532;&#19968;&#37096;&#20998;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#21442;&#25968;&#25512;&#26029;&#65288;&#21363;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#65289;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#30340;PINN&#20195;&#29702;&#12290;&#22312;&#36825;&#31532;&#19968;&#37096;&#20998;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;SPM&#30340;PINN&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22810;&#20445;&#30495;&#24230;&#20998;&#23618;&#35757;&#32451;&#65292;&#20854;&#20013;&#26377;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17329v2 Announce Type: replace  Abstract: To plan and optimize energy storage demands that account for Li-ion battery aging dynamics, techniques need to be developed to diagnose battery internal states accurately and rapidly. This study seeks to reduce the computational resources needed to determine a battery's internal states by replacing physics-based Li-ion battery models -- such as the single-particle model (SPM) and the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN) surrogate. The surrogate model makes high-throughput techniques, such as Bayesian calibration, tractable to determine battery internal parameters from voltage responses. This manuscript is the first of a two-part series that introduces PINN surrogates of Li-ion battery models for parameter inference (i.e., state-of-health diagnostics). In this first part, a method is presented for constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical training, where several neural ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#22788;&#29702;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.12467</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#25509;&#35302;&#32593;&#26684;&#21464;&#25442;&#22120;&#23398;&#20064;&#28789;&#27963;&#36523;&#20307;&#30896;&#25758;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#22788;&#29702;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#22522;&#20110;&#32593;&#26684;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#24050;&#34987;&#25552;&#20986;&#29992;&#26469;&#24314;&#27169;&#22797;&#26434;&#30340;&#39640;&#32500;&#29289;&#29702;&#31995;&#32479;&#12290;&#19982;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#23601;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#27714;&#35299;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#30340;&#26159;&#23427;&#20204;&#26159;&#21542;&#26377;&#25928;&#22320;&#24212;&#23545;&#28789;&#27963;&#20307;&#21160;&#21147;&#23398;&#30340;&#25361;&#25112;&#65292;&#21363;&#30636;&#26102;&#30896;&#25758;&#21457;&#29983;&#22312;&#26497;&#30701;&#26102;&#38388;&#20869;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23618;&#32593;&#26684;&#32467;&#26500;&#30340;Hierarchical Contact Mesh Transformer&#65288;HCMT&#65289;&#65292;&#33021;&#22815;&#23398;&#20064;&#36523;&#20307;&#31354;&#38388;&#20301;&#32622;&#20043;&#38388;&#65288;&#30001;&#30896;&#25758;&#24341;&#36215;&#30340;&#65289;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;--&#22312;&#26356;&#39640;&#32423;&#21035;&#32593;&#26684;&#20013;&#30340;&#20004;&#20010;&#25509;&#36817;&#20301;&#32622;&#23545;&#24212;&#20110;&#36523;&#20307;&#20013;&#30340;&#20004;&#20010;&#36828;&#36317;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12467v2 Announce Type: replace-cross  Abstract: Recently, many mesh-based graph neural network (GNN) models have been proposed for modeling complex high-dimensional physical systems. Remarkable achievements have been made in significantly reducing the solving time compared to traditional numerical solvers. These methods are typically designed to i) reduce the computational cost in solving physical dynamics and/or ii) propose techniques to enhance the solution accuracy in fluid and rigid body dynamics. However, it remains under-explored whether they are effective in addressing the challenges of flexible body dynamics, where instantaneous collisions occur within a very short timeframe. In this paper, we present Hierarchical Contact Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn long-range dependencies (occurred by collisions) among spatially distant positions of a body -- two close positions in a higher-level mesh corresponds to two distant posi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#65292;&#36991;&#20813;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#32780;&#31215;&#32047;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2312.08533</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#24341;&#23548;&#30340;&#36712;&#36857;&#25193;&#25955;&#23454;&#29616;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
World Models via Policy-Guided Trajectory Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08533
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#65292;&#36991;&#20813;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#22686;&#38271;&#32780;&#31215;&#32047;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#26159;&#24320;&#21457;&#26234;&#33021;agent&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36890;&#36807;&#39044;&#27979;&#19968;&#31995;&#21015;&#34892;&#21160;&#30340;&#32467;&#26524;&#65292;&#19990;&#30028;&#27169;&#22411;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#22312;&#8220;&#24819;&#35937;&#20013;&#8221;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#20248;&#21270;&#31574;&#30053;&#65292;&#21363;&#36890;&#36807;&#22312;&#32447;&#31574;&#30053;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#23454;&#29616;&#12290;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#26159;&#33258;&#22238;&#24402;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340;&#21516;&#26102;&#20174;&#31574;&#30053;&#20013;&#37319;&#26679;&#19979;&#19968;&#20010;&#34892;&#21160;&#12290;&#38543;&#30528;&#36712;&#36857;&#38271;&#24230;&#30340;&#22686;&#38271;&#65292;&#39044;&#27979;&#35823;&#24046;&#24517;&#28982;&#20250;&#32047;&#31215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19990;&#30028;&#24314;&#27169;&#26041;&#27861;&#65292;&#19981;&#26159;&#33258;&#22238;&#24402;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#29983;&#25104;&#25972;&#20010;&#22312;&#32447;&#31574;&#30053;&#36712;&#36857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Policy-Guided Trajectory Diffusion (PolyGRAD)&#65292;&#21033;&#29992;&#20102;&#38500;&#20102;&#31574;&#30053;&#30340;&#21160;&#20316;&#20998;&#24067;&#26799;&#24230;&#20043;&#22806;&#30340;&#19968;&#20010;&#21435;&#22122;&#27169;&#22411;&#65292;&#23558;&#26368;&#21021;&#38543;&#26426;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#36712;&#36857;&#25193;&#25955;&#25104;&#19968;&#20010;&#22312;&#32447;&#21512;&#25104;&#36712;&#36857;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;PolyGRAD&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08533v3 Announce Type: replace-cross  Abstract: World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD,
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#20154;&#31867;&#21644;&#38750;&#20154;&#31867;&#30340;&#27010;&#24565;&#65292;&#20294;&#24182;&#19981;&#26159;&#29992;&#21333;&#20010;&#21333;&#20803;&#26469;&#34920;&#31034;&#36825;&#20123;&#27010;&#24565;</title><link>https://arxiv.org/abs/2312.05337</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#27010;&#24565;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Nets and the Representation of Human Concepts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05337
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#20154;&#31867;&#21644;&#38750;&#20154;&#31867;&#30340;&#27010;&#24565;&#65292;&#20294;&#24182;&#19981;&#26159;&#29992;&#21333;&#20010;&#21333;&#20803;&#26469;&#34920;&#31034;&#36825;&#20123;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#21040;&#24213;&#23398;&#20064;&#21040;&#20102;&#20160;&#20040;&#21602;&#65311;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31038;&#21306;&#35748;&#20026;&#65292;&#20026;&#20102;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24517;&#39035;&#21457;&#23637;&#25277;&#35937;&#30340;&#20154;&#31867;&#27010;&#24565;&#12290;&#19968;&#20123;&#20154;&#29978;&#33267;&#35748;&#20026;&#36825;&#20123;&#27010;&#24565;&#23384;&#20648;&#22312;&#32593;&#32476;&#30340;&#21333;&#20010;&#21333;&#20803;&#20013;&#12290;&#26681;&#25454;&#24403;&#21069;&#30340;&#30740;&#31350;&#65292;&#25105;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#25903;&#25745;&#36825;&#19968;&#35828;&#27861;&#30340;&#20551;&#35774;&#12290;&#25105;&#24471;&#20986;&#32467;&#35770;&#65292;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#30830;&#33021;&#22815;&#25191;&#34892;&#22797;&#26434;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#20026;&#27492;&#23427;&#20204;&#21487;&#33021;&#23398;&#20064;&#20102;&#20154;&#31867;&#21644;&#38750;&#20154;&#31867;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#35777;&#25454;&#34920;&#26126;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24182;&#27809;&#26377;&#29992;&#21333;&#20010;&#21333;&#20803;&#26469;&#34920;&#31034;&#36825;&#20123;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05337v2 Announce Type: replace-cross  Abstract: What do artificial neural networks (ANNs) learn? The machine learning (ML) community shares the narrative that ANNs must develop abstract human concepts to perform complex tasks. Some go even further and believe that these concepts are stored in individual units of the network. Based on current research, I systematically investigate the assumptions underlying this narrative. I conclude that ANNs are indeed capable of performing complex prediction tasks, and that they may learn human and non-human concepts to do so. However, evidence indicates that ANNs do not represent these concepts in individual units.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31169;&#26377;&#35757;&#32451;&#21644;&#25512;&#29702;&#26694;&#26550;Delta&#65292;&#36890;&#36807;&#20004;&#20010;&#19981;&#23545;&#31216;&#25968;&#25454;&#27969;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#27604;&#36739;&#27169;&#22411;&#24615;&#33021;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>https://arxiv.org/abs/2312.05264</link><description>&lt;p&gt;
&#25152;&#26377;&#30340;&#27827;&#27969;&#37117;&#27719;&#32858;&#21040;&#22823;&#28023;&#65306;&#20855;&#26377;&#19981;&#23545;&#31216;&#27969;&#37327;&#30340;&#31169;&#26377;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
All Rivers Run to the Sea: Private Learning with Asymmetric Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05264
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31169;&#26377;&#35757;&#32451;&#21644;&#25512;&#29702;&#26694;&#26550;Delta&#65292;&#36890;&#36807;&#20004;&#20010;&#19981;&#23545;&#31216;&#25968;&#25454;&#27969;&#23454;&#29616;&#20102;&#20855;&#26377;&#21487;&#27604;&#36739;&#27169;&#22411;&#24615;&#33021;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#22312;&#20113;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#24179;&#21488;&#20013;&#22791;&#21463;&#20851;&#27880;&#65292;&#24403;&#25935;&#24863;&#25968;&#25454;&#26292;&#38706;&#32473;&#26381;&#21153;&#25552;&#20379;&#21830;&#26102;&#12290;&#20026;&#20102;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#24615;&#33021;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31169;&#26377;&#35757;&#32451;&#21644;&#25512;&#29702;&#26694;&#26550;Delta&#65292;&#20855;&#26377;&#19982;&#38750;&#31169;&#26377;&#38598;&#20013;&#35757;&#32451;&#30456;&#24403;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;Delta&#20855;&#26377;&#20004;&#20010;&#19981;&#23545;&#31216;&#30340;&#25968;&#25454;&#27969;&#65306;&#20027;&#35201;&#30340;&#20449;&#24687;&#25935;&#24863;&#27969;&#21644;&#27531;&#24046;&#27969;&#12290;&#20027;&#35201;&#37096;&#20998;&#27969;&#20837;&#19968;&#20010;&#23567;&#27169;&#22411;&#65292;&#32780;&#27531;&#20313;&#37096;&#20998;&#21017;&#34987;&#36716;&#31227;&#21040;&#19968;&#20010;&#22823;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Delta&#23558;&#20449;&#24687;&#25935;&#24863;&#34920;&#31034;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#20013;&#65292;&#21516;&#26102;&#23558;&#20449;&#24687;&#19981;&#25935;&#24863;&#37096;&#20998;&#25512;&#20837;&#39640;&#32500;&#27531;&#24046;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05264v2 Announce Type: replace-cross  Abstract: Data privacy is of great concern in cloud machine-learning service platforms, when sensitive data are exposed to service providers. While private computing environments (e.g., secure enclaves), and cryptographic approaches (e.g., homomorphic encryption) provide strong privacy protection, their computing performance still falls short compared to cloud GPUs. To achieve privacy protection with high computing performance, we propose Delta, a new private training and inference framework, with comparable model performance as non-private centralized training. Delta features two asymmetric data flows: the main information-sensitive flow and the residual flow. The main part flows into a small model while the residuals are offloaded to a large model. Specifically, Delta embeds the information-sensitive representations into a low-dimensional space while pushing the information-insensitive part into high-dimension residuals. To ensure priv
&lt;/p&gt;</description></item><item><title>DreamComposer&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#20837;&#22810;&#35270;&#35282;&#26465;&#20214;&#22686;&#24378;&#29616;&#26377;&#35270;&#35282;&#24863;&#30693;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#21487;&#25511;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.03611</link><description>&lt;p&gt;
DreamComposer: &#36890;&#36807;&#22810;&#35270;&#35282;&#26465;&#20214;&#23454;&#29616;&#21487;&#25511;&#30340;3D&#23545;&#35937;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DreamComposer: Controllable 3D Object Generation via Multi-View Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03611
&lt;/p&gt;
&lt;p&gt;
DreamComposer&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#20837;&#22810;&#35270;&#35282;&#26465;&#20214;&#22686;&#24378;&#29616;&#26377;&#35270;&#35282;&#24863;&#30693;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#21487;&#25511;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;2D&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#33021;&#22815;&#20174;&#21333;&#20010;in-the-wild&#22270;&#20687;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26469;&#33258;&#22810;&#20010;&#35270;&#35282;&#30340;&#20449;&#24687;&#65292;&#36825;&#20123;&#30740;&#31350;&#22312;&#29983;&#25104;&#21487;&#25511;&#30340;&#26032;&#35270;&#22270;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DreamComposer&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#27880;&#20837;&#22810;&#35270;&#35282;&#26465;&#20214;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#35270;&#35282;&#24863;&#30693;&#25193;&#25955;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DreamComposer&#39318;&#20808;&#20351;&#29992;&#35270;&#35282;&#24863;&#30693;&#30340;3D&#25552;&#21319;&#27169;&#22359;&#20174;&#22810;&#20010;&#35270;&#35282;&#33719;&#21462;&#23545;&#35937;&#30340;3D&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#22810;&#35270;&#35282;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#20174;3D&#34920;&#31034;&#20013;&#28210;&#26579;&#30446;&#26631;&#35270;&#22270;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#20174;&#22810;&#35270;&#35282;&#36755;&#20837;&#20013;&#25552;&#21462;&#30340;&#30446;&#26631;&#35270;&#22270;&#29305;&#24449;&#34987;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DreamComposer&#19982;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#20860;&#23481;&#65292;&#29992;&#20110;&#38646;-shot&#26032;&#35270;&#22270;sy
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03611v2 Announce Type: replace-cross  Abstract: Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view sy
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEEL&#30340;&#26032;&#22411;&#12289;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#37051;&#25509;&#30697;&#38453;&#22823;&#23567;&#21644;&#35789;&#27719;&#37327;&#65292;&#21516;&#26102;&#36890;&#36807;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#23454;&#29616;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#38024;&#23545;&#23646;&#24615;&#22270;&#35774;&#35745;&#20102;&#26032;&#30340;&#25193;&#23637;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.02230</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Simple and Scalable Representation for Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEEL&#30340;&#26032;&#22411;&#12289;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#37051;&#25509;&#30697;&#38453;&#22823;&#23567;&#21644;&#35789;&#27719;&#37327;&#65292;&#21516;&#26102;&#36890;&#36807;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#23454;&#29616;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#38024;&#23545;&#23646;&#24615;&#22270;&#35774;&#35745;&#20102;&#26032;&#30340;&#25193;&#23637;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#29983;&#25104;&#20135;&#29983;&#20102;&#27987;&#21402;&#20852;&#36259;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#20851;&#38190;&#24212;&#29992;&#20215;&#20540;&#30340;&#22522;&#26412;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#22914;&#20998;&#23376;&#35774;&#35745;&#21644;&#31038;&#21306;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#29983;&#25104;&#22823;&#35268;&#27169;&#22270;&#26102;&#36935;&#21040;&#20102;&#37325;&#22823;&#38480;&#21046;&#12290;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#38656;&#35201;&#36755;&#20986;&#38543;&#30528;&#33410;&#28857;&#25968;&#37327;&#21576;&#20108;&#27425;&#22686;&#38271;&#30340;&#23436;&#25972;&#37051;&#25509;&#30697;&#38453;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#34920;&#31034;&#65292;&#21517;&#20026;&#38388;&#38553;&#32534;&#30721;&#36793;&#21015;&#34920;&#65288;GEEL&#65289;&#65292;&#20854;&#34920;&#31034;&#22823;&#23567;&#36739;&#23567;&#19988;&#19982;&#36793;&#25968;&#37327;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;GEEL&#36890;&#36807;&#32467;&#21512;&#38388;&#38553;&#32534;&#30721;&#21644;&#24102;&#23485;&#38480;&#21046;&#26041;&#26696;&#26174;&#33879;&#20943;&#23569;&#20102;&#35789;&#27719;&#37327;&#12290;&#36890;&#36807;&#21152;&#20837;&#33410;&#28857;&#20301;&#32622;&#32534;&#30721;&#65292;GEEL&#21487;&#20197;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;GEEL&#25193;&#23637;&#21040;&#22788;&#29702;&#23646;&#24615;&#22270;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02230v2 Announce Type: replace-cross  Abstract: Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list (GEEL) that has a small representation size that aligns with the number of edges. In addition, GEEL significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. GEEL can be autoregressively generated with the incorporation of node positional encoding, and we further extend GEEL to deal with attributed graphs by designing a new grammar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.01201</link><description>&lt;p&gt;
PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PAC Privacy Preserving Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01201
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#27491;&#22312;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#26377;&#21487;&#33021;&#29983;&#25104;&#26082;&#20855;&#26377;&#39640;&#38544;&#31169;&#24615;&#21448;&#20855;&#26377;&#33391;&#22909;&#35270;&#35273;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#30830;&#20445;&#22312;&#31169;&#26377;&#21270;&#29305;&#23450;&#25968;&#25454;&#23646;&#24615;&#26102;&#30340;&#24378;&#22823;&#20445;&#25252;&#65292;&#24403;&#21069;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#38754;&#32463;&#24120;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21407;&#29702;&#24182;&#30830;&#20445;&#8220;&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#65288;PAC&#65289;&#8221;&#38544;&#31169;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;Langevin&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#27169;&#22411;&#38544;&#31169;&#24615;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#36825;&#20010;&#26032;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;&#30697;&#38453;&#35745;&#31639;&#25903;&#25345;PAC&#30028;&#38480;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#38544;&#31169;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01201v2 Announce Type: replace-cross  Abstract: Data privacy protection is garnering increased attention among researchers. Diffusion models (DMs), particularly with strict differential privacy, can potentially produce images with both high privacy and visual quality. However, challenges arise such as in ensuring robust protection in privatizing specific data attributes, areas where current models often fall short. To address these challenges, we introduce the PAC Privacy Preserving Diffusion Model, a model leverages diffusion principles and ensure Probably Approximately Correct (PAC) privacy. We enhance privacy protection by integrating a private classifier guidance into the Langevin Sampling Process. Additionally, recognizing the gap in measuring the privacy of models, we have developed a novel metric to gauge privacy levels. Our model, assessed with this new metric and supported by Gaussian matrix computations for the PAC bound, has shown superior performance in privacy p
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#20687;&#32032;&#32622;&#25442;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#31070;&#32463;&#22330;&#35757;&#32451;&#65292;&#36890;&#36807;&#28040;&#38500;&#26131;&#21305;&#37197;&#27169;&#24335;&#26469;&#20419;&#36827;&#26089;&#26399;&#38454;&#27573;&#30340;&#20248;&#21270;</title><link>https://arxiv.org/abs/2311.17094</link><description>&lt;p&gt;
&#23547;&#25214;&#21152;&#36895;&#31070;&#32463;&#22330;&#35757;&#32451;&#30340;&#25968;&#25454;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In Search of a Data Transformation That Accelerates Neural Field Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17094
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20687;&#32032;&#32622;&#25442;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#31070;&#32463;&#22330;&#35757;&#32451;&#65292;&#36890;&#36807;&#28040;&#38500;&#26131;&#21305;&#37197;&#27169;&#24335;&#26469;&#20419;&#36827;&#26089;&#26399;&#38454;&#27573;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22330;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#25968;&#25454;&#34920;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#32473;&#23450;&#20449;&#21495;&#12290;&#38459;&#30861;&#20854;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#20010;&#20851;&#38190;&#38556;&#30861;&#26159;&#29983;&#25104;&#31070;&#32463;&#22330;&#30340;&#32534;&#30721;&#36895;&#24230;&#38656;&#35201;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#24230;&#25311;&#21512;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;SGD&#27493;&#39588;&#25165;&#33021;&#36798;&#21040;&#25152;&#38656;&#30340;&#20445;&#30495;&#24230;&#27700;&#24179;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#36716;&#25442;&#23545;&#31070;&#32463;&#22330;&#35757;&#32451;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#23545;&#20687;&#32032;&#20301;&#32622;&#36827;&#34892;&#38543;&#26426;&#25490;&#21015;&#22914;&#20309;&#24433;&#21709;SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#26426;&#25490;&#21015;&#20687;&#32032;&#20301;&#32622;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#36890;&#36807;PSNR&#26354;&#32447;&#12289;&#25439;&#22833;&#22320;&#24418;&#21644;&#38169;&#35823;&#27169;&#24335;&#26469;&#23457;&#35270;&#31070;&#32463;&#22330;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#26426;&#20687;&#32032;&#32622;&#25442;&#28040;&#38500;&#20102;&#26131;&#21305;&#37197;&#30340;&#27169;&#24335;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#26089;&#26399;&#38454;&#27573;&#36827;&#34892;&#31616;&#21333;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17094v2 Announce Type: replace  Abstract: Neural field is an emerging paradigm in data representation that trains a neural network to approximate the given signal. A key obstacle that prevents its widespread adoption is the encoding speed-generating neural fields requires an overfitting of a neural network, which can take a significant number of SGD steps to reach the desired fidelity level. In this paper, we delve into the impacts of data transformations on the speed of neural field training, specifically focusing on how permuting pixel locations affect the convergence speed of SGD. Counterintuitively, we find that randomly permuting the pixel locations can considerably accelerate the training. To explain this phenomenon, we examine the neural field training through the lens of PSNR curves, loss landscapes, and error patterns. Our analyses suggest that the random pixel permutations remove the easy-to-fit patterns, which facilitate easy optimization in the early stage but hi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.15964</link><description>&lt;p&gt;
&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Pre-training for Localized Instruction Generation of Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35270;&#39057;&#23637;&#31034;&#20102;&#35832;&#22914;&#39135;&#35889;&#20934;&#22791;&#31561;&#20219;&#21153;&#30340;&#36880;&#27493;&#28436;&#31034;&#12290;&#29702;&#35299;&#27492;&#31867;&#35270;&#39057;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;&#27493;&#39588;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#24182;&#29983;&#25104;&#25991;&#23383;&#35828;&#26126;&#12290;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#24182;&#32534;&#20889;&#35828;&#26126;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#38480;&#21046;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#24182;&#38459;&#30861;&#20102;&#26377;&#25928;&#23398;&#20064;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#30340;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#36716;&#24405;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#21592;&#32534;&#20889;&#30340;&#35828;&#26126;&#30456;&#27604;&#23384;&#22312;&#39118;&#26684;&#21464;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;Sieve-&amp;-Swap&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#21644;&#20351;&#29992;&#25991;&#26412;&#39135;&#35889;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#33258;&#21160;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#25991;&#23383;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&amp;-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#26041;&#38754;&#20173;&#19981;&#28165;&#26970;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#26550;&#26500;&#22312;&#22522;&#20110;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#26102;&#21487;&#33021;&#23548;&#33268;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#35814;&#32454;&#30740;&#31350;&#20102;&#38544;&#21547;&#20559;&#24046;&#30340;&#25968;&#37327;&#21270;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2311.15404</link><description>&lt;p&gt;
&#23558;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Applying statistical learning theory to deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15404
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#29702;&#35770;&#26041;&#38754;&#20173;&#19981;&#28165;&#26970;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#26550;&#26500;&#22312;&#22522;&#20110;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#26102;&#21487;&#33021;&#23548;&#33268;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#35814;&#32454;&#30740;&#31350;&#20102;&#38544;&#21547;&#20559;&#24046;&#30340;&#25968;&#37327;&#21270;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35299;&#30417;&#30563;&#23398;&#20064;&#30340;&#22362;&#23454;&#26694;&#26550;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#30340;&#35768;&#22810;&#29702;&#35770;&#26041;&#38754;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#19981;&#21516;&#30340;&#26550;&#26500;&#22914;&#20309;&#23548;&#33268;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#20123;&#35762;&#24231;&#30340;&#30446;&#26631;&#26159;&#20174;&#23398;&#20064;&#29702;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#26102;&#20986;&#29616;&#30340;&#19968;&#20123;&#20027;&#35201;&#38382;&#39064;&#30340;&#27010;&#35272;&#12290;&#22312;&#31616;&#35201;&#22238;&#39038;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#21644;&#38543;&#26426;&#20248;&#21270;&#20043;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#32972;&#26223;&#19979;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#19968;&#33324;&#24615;&#25551;&#36848;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#22914;&#20309;&#22312;&#32473;&#23450;&#23398;&#20064;&#38382;&#39064;&#30340;&#21442;&#25968;&#31354;&#38388;&#21644;&#30456;&#24212;&#30340;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#26469;&#22238;&#31227;&#21160;&#65292;&#20197;&#21450;&#23398;&#20064;&#38382;&#39064;&#30340;&#20960;&#20309;&#24615;&#36136;&#22914;&#20309;&#21487;&#20197;&#29992;&#24230;&#37327;&#24352;&#37327;&#34920;&#31034;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23545;&#25968;&#37327;&#21270;&#38544;&#21547;&#20559;&#24046;&#30340;&#20855;&#20307;&#30740;&#31350;&#36827;&#34892;&#20102;&#35814;&#32454;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15404v2 Announce Type: replace  Abstract: Although statistical learning theory provides a robust framework to understand supervised learning, many theoretical aspects of deep learning remain unclear, in particular how different architectures may lead to inductive bias when trained using gradient based methods. The goal of these lectures is to provide an overview of some of the main questions that arise when attempting to understand deep learning from a learning theory perspective. After a brief reminder on statistical learning theory and stochastic optimization, we discuss implicit bias in the context of benign overfitting. We then move to a general description of the mirror descent algorithm, showing how we may go back and forth between a parameter space and the corresponding function space for a given learning problem, as well as how the geometry of the learning problem may be represented by a metric tensor. Building on this framework, we provide a detailed study of the im
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#25345;&#32493;&#29305;&#24449;&#21521;&#37327;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26041;&#27861;&#26469;&#36319;&#36394;&#38669;&#22855;&#25289;&#26222;&#25289;&#26031;&#30340;&#20010;&#20307;&#35856;&#27874;&#12289;&#26059;&#24230;&#21644;&#26799;&#24230;&#29305;&#24449;&#21521;&#37327;/&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.14427</link><description>&lt;p&gt;
&#35299;&#24320;&#38669;&#22855;&#25289;&#26222;&#25289;&#26031;&#30340;&#35889;&#29305;&#24615;&#65306;&#24182;&#38750;&#25152;&#26377;&#23567;&#29305;&#24449;&#20540;&#37117;&#30456;&#31561;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Spectral Properties of the Hodge Laplacian: Not All Small Eigenvalues Are Equal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14427
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#25345;&#32493;&#29305;&#24449;&#21521;&#37327;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#19968;&#31181;&#26041;&#27861;&#26469;&#36319;&#36394;&#38669;&#22855;&#25289;&#26222;&#25289;&#26031;&#30340;&#20010;&#20307;&#35856;&#27874;&#12289;&#26059;&#24230;&#21644;&#26799;&#24230;&#29305;&#24449;&#21521;&#37327;/&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#20016;&#23500;&#35889;&#20449;&#24687;&#23545;&#22270;&#35770;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#22914;&#22270;&#20998;&#31867;&#12289;&#32858;&#31867;&#25110;&#29305;&#24449;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#38669;&#22855;&#25289;&#26222;&#25289;&#26031;&#20316;&#20026;&#26222;&#36890;&#25289;&#26222;&#25289;&#26031;&#22312;&#39640;&#38454;&#22270;&#27169;&#22411;&#65288;&#22914;&#21333;&#32431;&#21644;&#32990;&#22797;&#24418;&#65289;&#20013;&#30340;&#25512;&#24191;&#24341;&#36215;&#20102;&#27880;&#24847;&#12290;&#31867;&#20284;&#20110;&#20256;&#32479;&#22270;&#25289;&#26222;&#25289;&#26031;&#30340;&#20998;&#26512;&#65292;&#35768;&#22810;&#20316;&#32773;&#20998;&#26512;&#38669;&#22855;&#25289;&#26222;&#25289;&#26031;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#65292;&#36825;&#20123;&#29305;&#24449;&#20540;&#19982;&#37325;&#35201;&#30340;&#25299;&#25169;&#29305;&#24615;&#22914;&#21516;&#35843;&#26377;&#20851;&#12290;&#28982;&#32780;&#65292;&#38669;&#22855;&#25289;&#26222;&#25289;&#26031;&#30340;&#23567;&#29305;&#24449;&#20540;&#21487;&#33021;&#25658;&#24102;&#19981;&#21516;&#20449;&#24687;&#65292;&#21462;&#20915;&#20110;&#23427;&#20204;&#26159;&#21542;&#19982;&#26059;&#24230;&#25110;&#26799;&#24230;&#29305;&#24449;&#27169;&#24335;&#30456;&#20851;&#32852;&#65292;&#22240;&#27492;&#21487;&#33021;&#19981;&#21487;&#27604;&#36739;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25345;&#32493;&#29305;&#24449;&#21521;&#37327;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#36319;&#36394;&#20010;&#20307;&#35856;&#27874;&#12289;&#26059;&#24230;&#21644;&#26799;&#24230;&#29305;&#24449;&#21521;&#37327;/&#20540;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14427v2 Announce Type: replace-cross  Abstract: The rich spectral information of the graph Laplacian has been instrumental in graph theory, machine learning, and graph signal processing for applications such as graph classification, clustering, or eigenmode analysis. Recently, the Hodge Laplacian has come into focus as a generalisation of the ordinary Laplacian for higher-order graph models such as simplicial and cellular complexes. Akin to the traditional analysis of graph Laplacians, many authors analyse the smallest eigenvalues of the Hodge Laplacian, which are connected to important topological properties such as homology. However, small eigenvalues of the Hodge Laplacian can carry different information depending on whether they are related to curl or gradient eigenmodes, and thus may not be comparable. We therefore introduce the notion of persistent eigenvector similarity and provide a method to track individual harmonic, curl, and gradient eigenvectors/-values through 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#65292;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#20195;&#26367;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#12289;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#35299;&#30721;&#22120;&#35299;&#20915;&#20102;&#24615;&#33021;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.12086</link><description>&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26159;&#40065;&#26834;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders Are Robust Neural Architecture Search Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12086
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#65292;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#20195;&#26367;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#12289;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#35299;&#30721;&#22120;&#35299;&#20915;&#20102;&#24615;&#33021;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30446;&#21069;&#20005;&#37325;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#30340;&#26032;&#22411;NAS&#26694;&#26550;&#65292;&#23427;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#28040;&#38500;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#23558;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#26367;&#25442;&#20026;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#33021;&#22815;&#22312;&#19981;&#25439;&#23475;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#40065;&#26834;&#22320;&#21457;&#29616;&#32593;&#32476;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#35299;&#30721;&#22120;&#35299;&#20915;&#20102;&#22312;&#26080;&#30417;&#30563;&#33539;&#24335;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#26041;&#27861;&#36935;&#21040;&#30340;&#24615;&#33021;&#23849;&#28291;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#25628;&#32034;&#31354;&#38388;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20026;&#20854;&#32988;&#36807;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12086v2 Announce Type: replace  Abstract: Neural Architecture Search (NAS) currently relies heavily on labeled data, which is both expensive and time-consuming to acquire. In this paper, we propose a novel NAS framework based on Masked Autoencoders (MAE) that eliminates the need for labeled data during the search process. By replacing the supervised learning objective with an image reconstruction task, our approach enables the robust discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) method in the unsupervised paradigm by introducing a multi-scale decoder. Through extensive experiments conducted on various search spaces and datasets, we demonstrate the effectiveness and robustness of the proposed method, providing empirical evidence of its superiority over baseline approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21019;&#26032;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#23548;&#33268;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#20002;&#22833;&#21644;&#22270;&#24418;&#32467;&#26500;&#19981;&#19968;&#33268;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.08744</link><description>&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Signal Diffusion Model for Collaborative Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21019;&#26032;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#23548;&#33268;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#20002;&#22833;&#21644;&#22270;&#24418;&#32467;&#26500;&#19981;&#19968;&#33268;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#33539;&#24335;&#26159;&#22522;&#20110;&#21382;&#21490;&#35266;&#23519;&#37325;&#24314;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#12290;&#36825;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#26368;&#36817;&#21457;&#23637;&#30340;&#25193;&#25955;&#27169;&#22411;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#32570;&#20047;&#23545;&#38544;&#24335;&#21453;&#39304;&#25968;&#25454;&#24314;&#27169;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#26631;&#20934;&#25193;&#25955;&#36807;&#31243;&#30340;&#21508;&#21521;&#21516;&#24615;&#29305;&#24615;&#26410;&#33021;&#32771;&#34385;&#29289;&#21697;&#20043;&#38388;&#30340;&#24322;&#36136;&#20381;&#36182;&#20851;&#31995;&#65292;&#23548;&#33268;&#19982;&#20132;&#20114;&#31354;&#38388;&#30340;&#22270;&#24418;&#32467;&#26500;&#19981;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#38543;&#26426;&#22122;&#22768;&#30772;&#22351;&#20102;&#20132;&#20114;&#21521;&#37327;&#20013;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#23548;&#33268;&#21453;&#21521;&#37325;&#24314;&#22256;&#38590;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#25913;&#36827;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;&#30340;&#22270;&#20449;&#21495;&#25193;&#25955;&#27169;&#22411;&#65288;&#31216;&#20026;GiffCF&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08744v2 Announce Type: replace-cross  Abstract: Collaborative filtering is a critical technique in recommender systems. Among various methods, an increasingly popular paradigm is to reconstruct user-item interactions based on the historical observations. This can be viewed as a conditional generative task, where recently developed diffusion model demonstrates great potential. However, existing studies on diffusion models lack effective solutions for modeling implicit feedback data. Particularly, the isotropic nature of the standard diffusion process fails to account for the heterogeneous dependencies among items, leading to a misalignment with the graphical structure of the interaction space. Meanwhile, random noise destroying personalized information in interaction vectors, causing difficulty in reverse reconstruction. In this paper, we make novel adaptions of diffusion model and propose Graph Signal Diffusion Model for Collaborative Filtering (named GiffCF). To better repr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#26377;&#21521;&#22270;&#19978;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#31163;&#25955;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#38142;&#36335;&#19978;&#21452;&#38543;&#26426;&#26435;&#37325;&#35774;&#35745;&#30340;&#38656;&#35201;&#65292;&#24182;&#20026;&#20998;&#24067;&#24335;&#20248;&#21270;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#26377;&#21521;&#22270;&#19978;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2311.07939</link><description>&lt;p&gt;
&#21160;&#24577;&#26377;&#21521;&#22270;&#19978;&#30340;&#31163;&#25955;&#20998;&#24067;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Discretized Distributed Optimization over Dynamic Digraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07939
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#26377;&#21521;&#22270;&#19978;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#31163;&#25955;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#38142;&#36335;&#19978;&#21452;&#38543;&#26426;&#26435;&#37325;&#35774;&#35745;&#30340;&#38656;&#35201;&#65292;&#24182;&#20026;&#20998;&#24067;&#24335;&#20248;&#21270;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#26377;&#21521;&#22270;&#19978;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21160;&#24577;&#26377;&#21521;&#22270;&#65288;digraphs&#65289;&#19978;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#65292;&#20855;&#26377;&#20998;&#24067;&#24335;&#23398;&#20064;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#20248;&#21270;&#31639;&#27861;&#36866;&#29992;&#20110;&#22312;&#20999;&#25442;&#25299;&#25169;&#30340;&#24191;&#20041;&#24378;&#36830;&#25509;&#21160;&#24577;&#32593;&#32476;&#19978;&#65292;&#20363;&#22914;&#31227;&#21160;&#22810;Agent&#31995;&#32479;&#21644;&#30001;&#20110;&#38142;&#36335;&#22833;&#36133;&#32780;&#19981;&#31283;&#23450;&#30340;&#32593;&#32476;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#26041;&#21521;&#30456;&#27604;&#65292;&#22312;&#38142;&#36335;&#19978;&#27809;&#26377;&#24517;&#35201;&#36827;&#34892;&#21452;&#38543;&#26426;&#26435;&#37325;&#35774;&#35745;&#12290;&#29616;&#26377;&#25991;&#29486;&#22823;&#22810;&#38656;&#35201;&#38142;&#36335;&#26435;&#37325;&#26159;&#38543;&#26426;&#30340;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#26435;&#37325;&#35774;&#35745;&#31639;&#27861;&#65292;&#22312;&#21021;&#22987;&#21270;&#21644;&#32593;&#32476;&#25299;&#25169;&#21464;&#21270;&#26102;&#37117;&#38656;&#35201;&#36825;&#20123;&#31639;&#27861;&#12290;&#26412;&#25991;&#28040;&#38500;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#38656;&#27714;&#65292;&#20026;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26377;&#21521;&#22270;&#19978;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#38138;&#24179;&#36947;&#36335;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#28176;&#36817;&#24615;&#26799;&#24230;&#36319;&#36394;&#27493;&#38271;&#21644;&#31163;&#25955;&#26102;&#38388;&#27493;&#38271;&#30340;&#25910;&#25947;&#30028;&#65292;&#24182;&#20351;&#29992;&#20849;&#35782;&#31639;&#27861;&#35777;&#26126;&#20102;&#21160;&#24577;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07939v2 Announce Type: replace-cross  Abstract: We consider a discrete-time model of continuous-time distributed optimization over dynamic directed-graphs (digraphs) with applications to distributed learning. Our optimization algorithm works over general strongly connected dynamic networks under switching topologies, e.g., in mobile multi-agent systems and volatile networks due to link failures. Compared to many existing lines of work, there is no need for bi-stochastic weight designs on the links. The existing literature mostly needs the link weights to be stochastic using specific weight-design algorithms needed both at the initialization and at all times when the topology of the network changes. This paper eliminates the need for such algorithms and paves the way for distributed optimization over time-varying digraphs. We derive the bound on the gradient-tracking step-size and discrete time-step for convergence and prove dynamic stability using arguments from consensus al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BrainRGIN&#24314;&#27169;&#26550;&#26500;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26234;&#21147;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#24182;&#32467;&#21512;&#20102;&#32858;&#31867;&#23884;&#20837;&#12289;&#22270;&#21516;&#26500;&#32593;&#32476;&#12289;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.03520</link><description>&lt;p&gt;
&#22823;&#33041;&#32593;&#32476;&#19982;&#26234;&#21147;&#65306;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BrainRGIN&#24314;&#27169;&#26550;&#26500;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26234;&#21147;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#24182;&#32467;&#21512;&#20102;&#32858;&#31867;&#23884;&#20837;&#12289;&#22270;&#21516;&#26500;&#32593;&#32476;&#12289;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;rsfMRI&#65289;&#26159;&#19968;&#31181;&#30740;&#31350;&#22823;&#33041;&#21151;&#33021;&#21644;&#35748;&#30693;&#36807;&#31243;&#20851;&#31995;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#33719;&#22823;&#33041;&#30340;&#21151;&#33021;&#32452;&#32455;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#25110;&#21050;&#28608;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;BrainRGIN&#30340;&#26032;&#39062;&#24314;&#27169;&#26550;&#26500;&#65292;&#21033;&#29992;rsfMRI&#25512;&#23548;&#30340;&#38745;&#24577;&#21151;&#33021;&#32593;&#32476;&#36830;&#25509;&#30697;&#38453;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26234;&#21147;&#65288;&#27969;&#20307;&#12289;&#26230;&#20307;&#21644;&#24635;&#20307;&#26234;&#21147;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#23558;&#32858;&#31867;&#23884;&#20837;&#21644;&#22270;&#21516;&#26500;&#32593;&#32476;&#32435;&#20837;&#21040;&#22270;&#21367;&#31215;&#23618;&#20013;&#65292;&#20197;&#21453;&#26144;&#22823;&#33041;&#23376;&#32593;&#32476;&#32452;&#32455;&#30340;&#24615;&#36136;&#21644;&#39640;&#25928;&#32593;&#32476;&#34920;&#36798;&#65292;&#20877;&#36741;&#20197;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03520v2 Announce Type: replace-cross  Abstract: Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful tool for investigating the relationship between brain function and cognitive processes as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this paper, we present a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on rsfMRI derived static functional network connectivity matrices. Extending from the existing graph convolution networks, our approach incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer to reflect the nature of the brain sub-network organization and efficient network expression, in combination with TopK pooling and attention-based readout functions. We evaluated our proposed architecture on a large dataset, specifically the A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#31283;&#23450;&#32447;&#24615;&#23376;&#31354;&#38388;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;SIMBa&#65292;&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#22343;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2311.03197</link><description>&lt;p&gt;
&#31283;&#23450;&#32447;&#24615;&#23376;&#31354;&#38388;&#35782;&#21035;&#65306;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stable Linear Subspace Identification: A Machine Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#31283;&#23450;&#32447;&#24615;&#23376;&#31354;&#38388;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;SIMBa&#65292;&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#22343;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#65288;SI&#65289;&#22312;&#21382;&#21490;&#19978;&#26159;&#29420;&#31435;&#21457;&#23637;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25104;&#29087;&#30340;ML&#24037;&#20855; - &#29305;&#21035;&#26159;&#33258;&#21160;&#24494;&#20998;&#26694;&#26550; - &#24341;&#20837;SIMBa&#65292;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#30340;&#31163;&#25955;&#32447;&#24615;&#22810;&#27493;&#39044;&#27979;&#29366;&#24577;&#31354;&#38388;SI&#26041;&#27861;&#30340;&#31995;&#21015;&#12290;SIMBa&#20381;&#36182;&#20110;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#30340;&#33258;&#30001;&#21442;&#25968;&#21270;&#21490;&#40667;&#23572;&#30697;&#38453;&#65292;&#20197;&#30830;&#20445;&#25152;&#35782;&#21035;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SIMBa&#22914;&#20309;&#26222;&#36941;&#20248;&#20110;&#20256;&#32479;&#30340;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;SI&#26041;&#27861;&#65292;&#26377;&#26102;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#23613;&#31649;&#20184;&#20986;&#26356;&#39640;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#19982;&#20854;&#20182;&#20855;&#26377;&#31283;&#23450;&#24615;&#20445;&#35777;&#30340;SI&#26041;&#27861;&#30456;&#27604;&#23588;&#20026;&#26174;&#33879;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#22686;&#30410;&#36890;&#24120;&#22823;&#20110;25%&#65292;&#26263;&#31034;&#30528;SIMBa&#21516;&#26102;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25311;&#21512;&#24615;&#33021;&#24182;&#24378;&#21046;&#31283;&#23450;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03197v4 Announce Type: replace-cross  Abstract: Machine Learning (ML) and linear System Identification (SI) have been historically developed independently. In this paper, we leverage well-established ML tools - especially the automatic differentiation framework - to introduce SIMBa, a family of discrete linear multi-step-ahead state-space SI methods using backpropagation. SIMBa relies on a novel Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure the stability of the identified model.   We show how SIMBa generally outperforms traditional linear state-space SI methods, and sometimes significantly, although at the price of a higher computational burden. This performance gap is particularly remarkable compared to other SI methods with stability guarantees, where the gain is frequently above 25% in our investigations, hinting at SIMBa's ability to simultaneously achieve state-of-the-art fitting performance and enforce stability. Interestingly, these o
&lt;/p&gt;</description></item><item><title>&#40654;&#26364;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;Fisher&#24230;&#37327;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36924;&#36817;&#26063;&#65292;&#35299;&#20915;&#20102;&#22312;&#26080;&#38480;&#25968;&#25454;&#26497;&#38480;&#19979;&#20808;&#21069;&#26041;&#27861;&#24230;&#37327;&#36873;&#25321;&#19981;&#24403;&#23548;&#33268;&#36924;&#36817;&#36807;&#20110;&#29421;&#31364;&#21644;&#26377;&#20559;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.02766</link><description>&lt;p&gt;
&#20855;&#26377;Fisher&#24230;&#37327;&#30340;&#40654;&#26364;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Riemannian Laplace Approximation with the Fisher Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02766
&lt;/p&gt;
&lt;p&gt;
&#40654;&#26364;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;Fisher&#24230;&#37327;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#36924;&#36817;&#26063;&#65292;&#35299;&#20915;&#20102;&#22312;&#26080;&#38480;&#25968;&#25454;&#26497;&#38480;&#19979;&#20808;&#21069;&#26041;&#27861;&#24230;&#37327;&#36873;&#25321;&#19981;&#24403;&#23548;&#33268;&#36924;&#36817;&#36807;&#20110;&#29421;&#31364;&#21644;&#26377;&#20559;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Laplace&#26041;&#27861;&#29992;&#39640;&#26031;&#20998;&#24067;&#22312;&#20854;&#27169;&#24335;&#22788;&#23545;&#30446;&#26631;&#23494;&#24230;&#36827;&#34892;&#36817;&#20284;&#12290;&#22522;&#20110;Bernstein-von Mises&#23450;&#29702;&#65292;&#23427;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#26159;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#28176;&#36817;&#20934;&#30830;&#30340;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#30446;&#26631;&#21644;&#26377;&#38480;&#25968;&#25454;&#21518;&#39564;&#65292;&#23427;&#24448;&#24448;&#26159;&#19968;&#31181;&#36807;&#20110;&#31895;&#31961;&#30340;&#36817;&#20284;&#12290;&#26368;&#36817;&#23545;Laplace&#36924;&#36817;&#30340;&#19968;&#33324;&#21270;&#26159;&#26681;&#25454;&#36873;&#25321;&#30340;&#40654;&#26364;&#20960;&#20309;&#23545;&#39640;&#26031;&#36817;&#20284;&#36827;&#34892;&#36716;&#25442;&#65292;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#36817;&#20284;&#26063;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#26412;&#25991;&#25152;&#31034;&#65292;&#20854;&#24615;&#36136;&#20005;&#37325;&#20381;&#36182;&#20110;&#25152;&#36873;&#25321;&#30340;&#24230;&#37327;&#65292;&#23454;&#38469;&#19978;&#65292;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#37319;&#29992;&#30340;&#24230;&#37327;&#23548;&#33268;&#30340;&#36924;&#36817;&#21363;&#20351;&#22312;&#26080;&#38480;&#25968;&#25454;&#37327;&#30340;&#26497;&#38480;&#19979;&#20063;&#36807;&#20110;&#29421;&#31364;&#19988;&#23384;&#22312;&#20559;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#19968;&#27493;&#21457;&#23637;&#36924;&#36817;&#26063;&#65292;&#25512;&#23548;&#20986;&#20004;&#31181;&#22312;&#26080;&#38480;&#25968;&#25454;&#26497;&#38480;&#19979;&#31934;&#30830;&#30340;&#26367;&#20195;&#21464;&#31181;&#65292;&#25193;&#23637;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02766v3 Announce Type: replace  Abstract: Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the
&lt;/p&gt;</description></item><item><title>&#35813;&#31639;&#27861;&#38024;&#23545;&#38750;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#20351;&#29992;&#20102;&#19981;&#31934;&#30830;&#30340;&#26799;&#24230;&#21644;Hessian&#20449;&#24687;&#65292;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#36817;&#20284;&#30340;&#36127;&#26354;&#29575;&#26041;&#21521;&#27493;&#36827;&#65292;&#23454;&#29616;&#36817;&#20284;&#20108;&#38454;&#26368;&#20248;&#24615;&#65292;&#24182;&#22312;&#26799;&#24230;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2310.18841</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#19981;&#31934;&#30830;&#35780;&#20272;&#21644;&#22797;&#26434;&#24615;&#20445;&#35777;&#30340;&#38750;&#20984;&#26368;&#23567;&#21270;&#38543;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A randomized algorithm for nonconvex minimization with inexact evaluations and complexity guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18841
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31639;&#27861;&#38024;&#23545;&#38750;&#20984;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#20351;&#29992;&#20102;&#19981;&#31934;&#30830;&#30340;&#26799;&#24230;&#21644;Hessian&#20449;&#24687;&#65292;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#36817;&#20284;&#30340;&#36127;&#26354;&#29575;&#26041;&#21521;&#27493;&#36827;&#65292;&#23454;&#29616;&#36817;&#20284;&#20108;&#38454;&#26368;&#20248;&#24615;&#65292;&#24182;&#22312;&#26799;&#24230;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#23545;&#26799;&#24230;&#21644;Hessian&#65288;&#19981;&#20551;&#35774;&#23545;&#20989;&#25968;&#20540;&#26377;&#35775;&#38382;&#26435;&#38480;&#65289;&#30340;&#19981;&#31934;&#30830;Oracle&#35775;&#38382;&#26469;&#26368;&#23567;&#21270;&#24179;&#28369;&#38750;&#20984;&#20989;&#25968;&#65292;&#20197;&#23454;&#29616;&#36817;&#20284;&#20108;&#38454;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#29305;&#28857;&#26159;&#65292;&#22914;&#26524;&#36873;&#25321;&#19968;&#20010;&#36817;&#20284;&#30340;&#36127;&#26354;&#29575;&#26041;&#21521;&#20316;&#20026;&#27493;&#39588;&#65292;&#25105;&#20204;&#20197;&#30456;&#31561;&#27010;&#29575;&#36873;&#25321;&#20854;&#26041;&#21521;&#20026;&#27491;&#25110;&#36127;&#12290;&#25105;&#20204;&#20801;&#35768;&#26799;&#24230;&#22312;&#30456;&#23545;&#24847;&#20041;&#19978;&#19981;&#31934;&#30830;&#65292;&#24182;&#25918;&#26494;&#19981;&#31934;&#30830;&#24230;&#38408;&#20540;&#22312;&#19968;&#38454;&#21644;&#20108;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#20043;&#38388;&#30340;&#32806;&#21512;&#12290;&#25105;&#20204;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21253;&#25324;&#22522;&#20110;&#38797;&#28857;&#20998;&#26512;&#30340;&#26399;&#26395;&#19978;&#30028;&#21644;&#22522;&#20110;&#27987;&#24230;&#19981;&#31561;&#24335;&#30340;&#39640;&#27010;&#29575;&#19978;&#30028;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#27604;&#29616;&#26377;&#20316;&#21697;&#26356;&#22909;&#30340;&#26799;&#24230;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18841v2 Announce Type: replace-cross  Abstract: We consider minimization of a smooth nonconvex function with inexact oracle access to gradient and Hessian (without assuming access to the function value) to achieve approximate second-order optimality. A novel feature of our method is that if an approximate direction of negative curvature is chosen as the step, we choose its sense to be positive or negative with equal probability. We allow gradients to be inexact in a relative sense and relax the coupling between inexactness thresholds for the first- and second-order optimality conditions. Our convergence analysis includes both an expectation bound based on martingale analysis and a high-probability bound based on concentration inequalities. We apply our algorithm to empirical risk minimization problems and obtain improved gradient sample complexity over existing works.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SD4Match&#65292;&#22312;&#35821;&#20041;&#21305;&#37197;&#20013;&#23398;&#20064;&#25552;&#31034;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#21644;&#26465;&#20214;&#25552;&#31034;&#27169;&#22359;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#20934;&#30830;&#24615;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2310.17569</link><description>&lt;p&gt;
&#23398;&#20064;&#25552;&#31034;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#21305;&#37197;&#30340;SD4Match
&lt;/p&gt;
&lt;p&gt;
SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.17569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SD4Match&#65292;&#22312;&#35821;&#20041;&#21305;&#37197;&#20013;&#23398;&#20064;&#25552;&#31034;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#21644;&#26465;&#20214;&#25552;&#31034;&#27169;&#22359;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#20934;&#30830;&#24615;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#22270;&#20687;&#23545;&#20043;&#38388;&#21305;&#37197;&#35821;&#20041;&#30456;&#20284;&#20851;&#38190;&#28857;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#20013;UNet&#30340;&#20013;&#38388;&#36755;&#20986;&#21487;&#20197;&#20316;&#20026;&#36825;&#31181;&#21305;&#37197;&#20219;&#21153;&#30340;&#31283;&#20581;&#22270;&#20687;&#29305;&#24449;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#37319;&#29992;&#22522;&#26412;&#30340;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;&#65292;&#21487;&#20197;&#21457;&#25381;&#31283;&#23450;&#25193;&#25955;&#30340;&#22266;&#26377;&#28508;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#25552;&#31034;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#23558;&#25552;&#31034;&#26465;&#20214;&#35774;&#32622;&#20026;&#36755;&#20837;&#22270;&#20687;&#23545;&#30340;&#23616;&#37096;&#32454;&#33410;&#65292;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;SD4Match&#65292;&#21363;&#31283;&#23450;&#25193;&#25955;&#29992;&#20110;&#35821;&#20041;&#21305;&#37197;&#12290;&#23545;PF-Pascal&#12289;PF-Willow&#21644;SPair-71k&#25968;&#25454;&#38598;&#19978;&#23545;SD4Match&#30340;&#20840;&#38754;&#35780;&#20272;&#26174;&#31034;&#65292;&#23427;&#22312;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#22343;&#35774;&#31435;&#20102;&#26032;&#30340;&#20934;&#30830;&#24615;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.17569v2 Announce Type: replace-cross  Abstract: In this paper, we address the challenge of matching semantically similar keypoints across image pairs. Existing research indicates that the intermediate output of the UNet within the Stable Diffusion (SD) can serve as robust image feature maps for such a matching task. We demonstrate that by employing a basic prompt tuning technique, the inherent potential of Stable Diffusion can be harnessed, resulting in a significant enhancement in accuracy over previous approaches. We further introduce a novel conditional prompting module that conditions the prompt on the local details of the input image pairs, leading to a further improvement in performance. We designate our approach as SD4Match, short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets new benchmarks in accuracy across all these datasets. Particularly, SD4Match outperforms 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.02129</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Pitfalls of Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#24320;&#21457;&#32534;&#36753;LLMs&#20869;&#22312;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#19968;&#20010;&#38452;&#20113;&#24748;&#22312;&#22836;&#39030;&#19978; - &#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#35302;&#21457;&#34676;&#34678;&#25928;&#24212;&#65311;&#22240;&#20026;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#24341;&#20837;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#19982;LLMs&#30693;&#35782;&#32534;&#36753;&#30456;&#20851;&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#30693;&#35782;&#20914;&#31361;&#65306;&#32534;&#36753;&#36923;&#36753;&#20914;&#31361;&#30340;&#20107;&#23454;&#32452;&#21487;&#33021;&#20250;&#25918;&#22823;LLMs&#22266;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615; - &#36825;&#26159;&#20197;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#65288;2&#65289;&#30693;&#35782;&#25197;&#26354;&#65306;&#20026;&#20102;&#32534;&#36753;&#20107;&#23454;&#30693;&#35782;&#32780;&#26356;&#25913;&#21442;&#25968;&#21487;&#33021;&#20250;&#19981;&#21487;&#36870;&#22320;&#25197;&#26354;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2309.13339</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340; remarkable generalizability&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#32463;&#24120;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#24314;&#31435;&#36830;&#36143;&#30340;&#24605;&#32500;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26410;&#21463;&#36923;&#36753;&#21407;&#21017;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#31995;&#32479;&#22320;&#39564;&#35777;&#21644;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;Troika&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#31435;&#19977;&#20010;&#35782;&#21035;&#20998;&#25903;&#20849;&#21516;&#23545;&#29366;&#24577;&#12289;&#23545;&#35937;&#21644;&#32452;&#21512;&#36827;&#34892;&#24314;&#27169;&#65292;&#22312;&#23545;&#40784;&#20998;&#25903;&#29305;&#23450;&#25552;&#31034;&#34920;&#31034;&#21644;&#20998;&#35299;&#30340;&#35270;&#35273;&#29305;&#24449;&#30340;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;Cross-Modal Traction&#27169;&#22359;&#26469;&#26657;&#20934;&#22810;&#27169;&#24577;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2303.15230</link><description>&lt;p&gt;
Troika: &#22810;&#36335;&#24452;&#36328;&#27169;&#24577;&#25302;&#26355;&#23545;&#20110;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.15230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;Troika&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#31435;&#19977;&#20010;&#35782;&#21035;&#20998;&#25903;&#20849;&#21516;&#23545;&#29366;&#24577;&#12289;&#23545;&#35937;&#21644;&#32452;&#21512;&#36827;&#34892;&#24314;&#27169;&#65292;&#22312;&#23545;&#40784;&#20998;&#25903;&#29305;&#23450;&#25552;&#31034;&#34920;&#31034;&#21644;&#20998;&#35299;&#30340;&#35270;&#35273;&#29305;&#24449;&#30340;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;Cross-Modal Traction&#27169;&#22359;&#26469;&#26657;&#20934;&#22810;&#27169;&#24577;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32452;&#21512;&#24335;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;CZSL&#65289;&#26041;&#27861;&#36890;&#36807;&#20165;&#20026;&#32452;&#21512;&#29366;&#24577;-&#23545;&#35937;&#23545;&#26500;&#24314;&#21487;&#35757;&#32451;&#25552;&#31034;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23398;&#20064;&#24050;&#35265;&#32452;&#21512;&#30340;&#32852;&#21512;&#34920;&#31034;&#65292;&#32780;&#24573;&#30053;&#20102;&#23545;&#29366;&#24577;&#21644;&#23545;&#35937;&#30340;&#26174;&#24335;&#24314;&#27169;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#21033;&#29992;&#21644;&#23545;&#26410;&#35265;&#32452;&#21512;&#30340;&#27867;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#35299;&#20915;&#26041;&#26696;&#30340;&#26222;&#36866;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;CZSL&#27169;&#22411;&#24314;&#31435;&#19977;&#20010;&#35782;&#21035;&#20998;&#25903;&#65288;&#21363;Multi-Path&#65289;&#20197;&#20849;&#21516;&#24314;&#27169;&#29366;&#24577;&#12289;&#23545;&#35937;&#21644;&#32452;&#21512;&#30340;&#26032;&#33539;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;Troika&#26159;&#25105;&#20204;&#30340;&#23454;&#29616;&#65292;&#23427;&#23558;&#20998;&#25903;&#29305;&#23450;&#30340;&#25552;&#31034;&#34920;&#31034;&#19982;&#20998;&#35299;&#30340;&#35270;&#35273;&#29305;&#24449;&#23545;&#40784;&#12290;&#20026;&#20102;&#26657;&#20934;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;Cross-Modal Traction&#27169;&#22359;&#26469;&#23558;&#25552;&#31034;&#31227;&#21160;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.15230v2 Announce Type: replace-cross  Abstract: Recent compositional zero-shot learning (CZSL) methods adapt pre-trained vision-language models (VLMs) by constructing trainable prompts only for composed state-object pairs. Relying on learning the joint representation of seen compositions, these methods ignore the explicit modeling of the state and object, thus limiting the exploitation of pre-trained knowledge and generalization to unseen compositions. With a particular focus on the universality of the solution, in this work, we propose a novel paradigm for CZSL models that establishes three identification branches (i.e., Multi-Path) to jointly model the state, object, and composition. The presented Troika is our implementation that aligns the branch-specific prompt representations with decomposed visual features. To calibrate the bias between semantically similar multi-modal representations, we further devise a Cross-Modal Traction module into Troika that shifts the prompt 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#20363;&#8220;&#26799;&#24230;&#25552;&#21319;&#8221;&#65288;GB&#65289;&#65292;&#20351;&#29992;&#24207;&#21015;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20248;&#36234;&#32467;&#26524;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#38145;&#20102;&#22312;PINNs&#20013;&#24212;&#29992;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2302.13143</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#23398;&#20064;&#65306;&#26799;&#24230;&#25552;&#21319;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble learning for Physics Informed Neural Networks: a Gradient Boosting approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13143
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#20363;&#8220;&#26799;&#24230;&#25552;&#21319;&#8221;&#65288;GB&#65289;&#65292;&#20351;&#29992;&#24207;&#21015;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20248;&#36234;&#32467;&#26524;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#35299;&#38145;&#20102;&#22312;PINNs&#20013;&#24212;&#29992;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;PINNs&#22312;&#27169;&#25311;&#22810;&#23610;&#24230;&#21644;&#22855;&#24322;&#25668;&#21160;&#38382;&#39064;&#26041;&#38754;&#24182;&#19981;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26799;&#24230;&#25552;&#21319;&#8221;&#65288;GB&#65289;&#30340;&#26032;&#35757;&#32451;&#33539;&#20363;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#26159;&#30452;&#25509;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32473;&#23450;PDE&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26159;&#37319;&#29992;&#19968;&#31995;&#21015;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#35299;&#20915;&#20256;&#32479;PINNs&#38590;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#19982;&#26377;&#38480;&#20803;&#26041;&#27861;&#21644;PINNs&#30340;&#27604;&#36739;&#65292;&#36827;&#19968;&#27493;&#35299;&#38145;&#20102;&#22312;PINNs&#20013;&#24212;&#29992;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#30340;&#26426;&#20250;&#65292;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.13143v2 Announce Type: replace  Abstract: While the popularity of physics-informed neural networks (PINNs) is steadily rising, to this date, PINNs have not been successful in simulating multi-scale and singular perturbation problems. In this work, we present a new training paradigm referred to as "gradient boosting" (GB), which significantly enhances the performance of physics informed neural networks (PINNs). Rather than learning the solution of a given PDE using a single neural network directly, our algorithm employs a sequence of neural networks to achieve a superior outcome. This approach allows us to solve problems presenting great challenges for traditional PINNs. Our numerical experiments demonstrate the effectiveness of our algorithm through various benchmarks, including comparisons with finite element methods and PINNs. Furthermore, this work also unlocks the door to employing ensemble learning techniques in PINNs, providing opportunities for further improvement in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#38544;&#24335;&#23618;&#29702;&#35770;&#65292;$\Psi$-GNN&#27169;&#22411;&#20102;&#19968;&#20010;&#8220;&#26080;&#38480;&#8221;&#28145;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32463;&#39564;&#35843;&#25972;&#25152;&#38656;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#27425;&#20197;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2302.10891</link><description>&lt;p&gt;
&#19968;&#31181;&#38544;&#24335;GNN&#27714;&#35299;&#22120;&#29992;&#20110;&#31867;&#27850;&#26494;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
An Implicit GNN Solver for Poisson-like problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.10891
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38544;&#24335;&#23618;&#29702;&#35770;&#65292;$\Psi$-GNN&#27169;&#22411;&#20102;&#19968;&#20010;&#8220;&#26080;&#38480;&#8221;&#28145;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32463;&#39564;&#35843;&#25972;&#25152;&#38656;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#27425;&#20197;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\Psi$-GNN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#26222;&#36941;&#27850;&#26494;PDE&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38544;&#24335;&#23618;&#29702;&#35770;&#65292;$\Psi$-GNN&#24314;&#27169;&#20102;&#19968;&#20010;&#8220;&#26080;&#38480;&#8221;&#28145;&#30340;&#32593;&#32476;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32463;&#39564;&#35843;&#25972;&#25152;&#38656;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#27425;&#20197;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#21407;&#22987;&#26550;&#26500;&#26126;&#30830;&#32771;&#34385;&#20102;&#36793;&#30028;&#26465;&#20214;&#65292;&#36825;&#26159;&#29289;&#29702;&#24212;&#29992;&#30340;&#20851;&#38190;&#21069;&#25552;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#20219;&#20309;&#26368;&#21021;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; $\Psi$-GNN&#20351;&#29992;&#8220;&#29289;&#29702;&#20449;&#24687;&#8221;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#35757;&#32451;&#36807;&#31243;&#30001;&#35774;&#35745;&#31283;&#23450;&#65292;&#24182;&#23545;&#20854;&#21021;&#22987;&#21270;&#19981;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#24182;&#19988;&#20854;&#26580;&#38887;&#24615;&#21644;&#27867;&#21270;&#25928;&#29575;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65306;&#30456;&#21516;&#30340;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22788;&#29702;&#21508;&#31181;&#23610;&#23544;&#21644;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#30340;&#38750;&#32467;&#26500;&#21270;&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.10891v3 Announce Type: replace-cross  Abstract: This paper presents $\Psi$-GNN, a novel Graph Neural Network (GNN) approach for solving the ubiquitous Poisson PDE problems with mixed boundary conditions. By leveraging the Implicit Layer Theory, $\Psi$-GNN models an "infinitely" deep network, thus avoiding the empirical tuning of the number of required Message Passing layers to attain the solution. Its original architecture explicitly takes into account the boundary conditions, a critical prerequisite for physical applications, and is able to adapt to any initially provided solution. $\Psi$-GNN is trained using a "physics-informed" loss, and the training process is stable by design, and insensitive to its initialization. Furthermore, the consistency of the approach is theoretically proven, and its flexibility and generalization efficiency are experimentally demonstrated: the same learned model can accurately handle unstructured meshes of various sizes, as well as different bo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2302.03788</link><description>&lt;p&gt;
&#38754;&#21521;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22240;&#26524;&#35770;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Toward a Theory of Causation for Interpreting Neural Code Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models of Code&#65292;&#25110;&#32773;&#31216;&#20026;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#65288;NCMs&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#20174;&#30740;&#31350;&#21407;&#22411;&#21457;&#23637;&#20026;&#21830;&#19994;&#24320;&#21457;&#32773;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36890;&#24120;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#21482;&#33021;&#25581;&#31034;&#23427;&#20204;&#30495;&#23454;&#24615;&#33021;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;NCMs&#30340;&#24615;&#33021;&#20284;&#20046;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#30446;&#21069;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#20173;&#26377;&#24456;&#22810;&#26410;&#30693;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;NCMs&#65292;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;$do_{code}$&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#20197;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;&#34429;&#28982;$do_{code}$&#30340;&#29702;&#35770;&#22522;&#30784;&#21487;&#25193;&#23637;&#21040;&#25506;&#32034;&#19981;&#21516;&#30340;&#27169;&#22411;&#23646;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#23454;&#20363;&#65292;&#26088;&#22312;&#20943;&#23569;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;18&#20010;&#20195;&#34920;&#24615;&#30340;&#36807;&#21435;&#30740;&#31350;&#65292;&#24182;&#22312;&#21253;&#21547;124,000&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#24179;&#34913;&#12289;&#30456;&#20851;&#21644;&#26368;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#26032;&#23454;&#39564;&#65292;&#21457;&#29616;&#20165;&#36890;&#36807;&#38745;&#24577;&#20998;&#26512;&#25552;&#21462;&#30340;&#29305;&#24449;&#23601;&#33021;&#23454;&#29616;&#39640;&#36798;96.8%&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2301.12778</link><description>&lt;p&gt;
&#25506;&#31350;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#37325;&#35201;&#24615;&#65306;&#19968;&#39033;&#23454;&#26045;&#35843;&#26597;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23454;&#39564;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Investigating Feature and Model Importance in Android Malware Detection: An Implemented Survey and Experimental Comparison of ML-Based Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.12778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;18&#20010;&#20195;&#34920;&#24615;&#30340;&#36807;&#21435;&#30740;&#31350;&#65292;&#24182;&#22312;&#21253;&#21547;124,000&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#24179;&#34913;&#12289;&#30456;&#20851;&#21644;&#26368;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#26032;&#23454;&#39564;&#65292;&#21457;&#29616;&#20165;&#36890;&#36807;&#38745;&#24577;&#20998;&#26512;&#25552;&#21462;&#30340;&#29305;&#24449;&#23601;&#33021;&#23454;&#29616;&#39640;&#36798;96.8%&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Android&#30340;&#26222;&#21450;&#24847;&#21619;&#30528;&#23427;&#25104;&#20026;&#24694;&#24847;&#36719;&#20214;&#30340;&#24120;&#35265;&#30446;&#26631;&#12290;&#22810;&#24180;&#26469;&#65292;&#21508;&#31181;&#30740;&#31350;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#24694;&#24847;&#36719;&#20214;&#21644;&#33391;&#24615;&#24212;&#29992;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25805;&#20316;&#31995;&#32479;&#30340;&#28436;&#36827;&#65292;&#24694;&#24847;&#36719;&#20214;&#20063;&#22312;&#19981;&#26029;&#21457;&#23637;&#65292;&#23545;&#20808;&#21069;&#30740;&#31350;&#30340;&#21457;&#29616;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#20854;&#20013;&#35768;&#22810;&#25253;&#21578;&#31216;&#20351;&#29992;&#23567;&#22411;&#12289;&#36807;&#26102;&#19988;&#32463;&#24120;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#33719;&#24471;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23454;&#29616;&#20102;18&#39033;&#20855;&#20195;&#34920;&#24615;&#30340;&#36807;&#21435;&#30740;&#31350;&#24182;&#20351;&#29992;&#21253;&#25324;124,000&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#24179;&#34913;&#12289;&#30456;&#20851;&#19988;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#23545;&#23427;&#20204;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#26032;&#30340;&#23454;&#39564;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#21033;&#29992;&#30740;&#31350;&#32467;&#26524;&#30830;&#23450;&#22312;&#24403;&#20195;&#29615;&#22659;&#20013;&#29992;&#20110;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26368;&#26377;&#25928;&#29305;&#24449;&#21644;&#27169;&#22411;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#38745;&#24577;&#20998;&#26512;&#25552;&#21462;&#30340;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#36798;96.8%&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.12778v2 Announce Type: replace  Abstract: The popularity of Android means it is a common target for malware. Over the years, various studies have found that machine learning models can effectively discriminate malware from benign applications. However, as the operating system evolves, so does malware, bringing into question the findings of these previous studies, many of which report very high accuracies using small, outdated, and often imbalanced datasets. In this paper, we reimplement 18 representative past works and reevaluate them using a balanced, relevant, and up-to-date dataset comprising 124,000 applications. We also carry out new experiments designed to fill holes in existing knowledge, and use our findings to identify the most effective features and models to use for Android malware detection within a contemporary environment. We show that high detection accuracies (up to 96.8%) can be achieved using features extracted through static analysis alone, yielding a mode
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#19977;&#31639;&#23376;ADMM&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36793;&#32536;&#26381;&#21153;&#22120;&#20016;&#23500;&#25968;&#25454;&#30340;&#20248;&#21183;&#65292;&#19982;&#20165;&#20351;&#29992;&#29992;&#25143;&#25968;&#25454;&#30456;&#27604;&#26377;&#26356;&#22823;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2211.04152</link><description>&lt;p&gt;
&#20351;&#29992;&#19977;&#31639;&#23376;ADMM&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Using Three-Operator ADMM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.04152
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#19977;&#31639;&#23376;ADMM&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#36793;&#32536;&#26381;&#21153;&#22120;&#20016;&#23500;&#25968;&#25454;&#30340;&#20248;&#21183;&#65292;&#19982;&#20165;&#20351;&#29992;&#29992;&#25143;&#25968;&#25454;&#30456;&#27604;&#26377;&#26356;&#22823;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#36991;&#20813;&#20102;&#22312;&#29992;&#25143;&#31471;&#29983;&#25104;&#25968;&#25454;&#30340;&#20256;&#36755;&#12290;&#23613;&#31649;&#25968;&#25454;&#19981;&#34987;&#20256;&#36755;&#65292;&#36793;&#32536;&#35774;&#22791;&#20173;&#28982;&#24517;&#39035;&#22788;&#29702;&#26377;&#38480;&#30340;&#36890;&#20449;&#24102;&#23485;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#30001;&#20110;&#29992;&#25143;&#35774;&#22791;&#30340;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#24341;&#36215;&#30340;&#8220;straggler&#8221;&#25928;&#24212;&#12290;FedADMM&#26159;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#30340;&#19968;&#20010;&#33879;&#21517;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#32463;&#20856;&#30340;&#21452;&#31639;&#23376;&#20849;&#35782;&#20132;&#26367;&#26041;&#21521;&#20056;&#25968;&#27861;&#65288;ADMM&#65289;&#12290;FL&#31639;&#27861;&#30340;&#19968;&#20010;&#26222;&#36941;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#20165;&#20351;&#29992;&#29992;&#25143;&#31471;&#25968;&#25454;&#32780;&#19981;&#20351;&#29992;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#36793;&#32536;&#23398;&#20064;&#20013;&#65292;&#26381;&#21153;&#22120;&#39044;&#35745;&#20250;&#38752;&#36817;&#22522;&#31449;&#24182;&#30452;&#25509;&#35775;&#38382;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21033;&#29992;&#36793;&#32536;&#26381;&#21153;&#22120;&#19978;&#30340;&#20016;&#23500;&#25968;&#25454;&#27604;&#20165;&#21033;&#29992;&#29992;&#25143;&#25968;&#25454;&#26356;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.04152v3 Announce Type: replace  Abstract: Federated learning (FL) has emerged as an instance of distributed machine learning paradigm that avoids the transmission of data generated on the users' side. Although data are not transmitted, edge devices have to deal with limited communication bandwidths, data heterogeneity, and straggler effects due to the limited computational resources of users' devices. A prominent approach to overcome such difficulties is FedADMM, which is based on the classical two-operator consensus alternating direction method of multipliers (ADMM). The common assumption of FL algorithms, including FedADMM, is that they learn a global model using data only on the users' side and not on the edge server. However, in edge learning, the server is expected to be near the base station and have direct access to rich datasets. In this paper, we argue that leveraging the rich data on the edge server is much more beneficial than utilizing only user datasets. Specifi
&lt;/p&gt;</description></item><item><title>&#24046;&#20998;&#31169;&#26377;&#22810;&#21464;&#37327;&#20013;&#20301;&#25968;&#30340;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#20445;&#35777;&#20026;&#24120;&#29992;&#28145;&#24230;&#20989;&#25968;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#37325;&#23614;&#20301;&#32622;&#20272;&#35745;&#30340;&#25104;&#26412;&#36229;&#36807;&#20102;&#38544;&#31169;&#20445;&#25252;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2210.06459</link><description>&lt;p&gt;
&#24046;&#20998;&#31169;&#26377;&#22810;&#21464;&#37327;&#20013;&#20301;&#25968;
&lt;/p&gt;
&lt;p&gt;
Differentially private multivariate medians
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06459
&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#31169;&#26377;&#22810;&#21464;&#37327;&#20013;&#20301;&#25968;&#30340;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#20445;&#35777;&#20026;&#24120;&#29992;&#28145;&#24230;&#20989;&#25968;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#37325;&#23614;&#20301;&#32622;&#20272;&#35745;&#30340;&#25104;&#26412;&#36229;&#36807;&#20102;&#38544;&#31169;&#20445;&#25252;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#38656;&#35201;&#28385;&#36275;&#20005;&#26684;&#38544;&#31169;&#20445;&#35777;&#30340;&#32479;&#35745;&#24037;&#20855;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#23545;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;&#19982;&#24046;&#20998;&#38544;&#31169;&#26377;&#20851;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20351;&#29992;&#22810;&#20803;&#20013;&#20301;&#25968;&#36827;&#34892;&#24046;&#20998;&#31169;&#26377;&#21644;&#40065;&#26834;&#30340;&#22810;&#20803;&#20301;&#32622;&#20272;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#20026;&#24046;&#20998;&#31169;&#26377;&#22810;&#20803;&#28145;&#24230;&#20013;&#20301;&#25968;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#22522;&#26412;&#19978;&#26159;&#23574;&#38160;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#28085;&#30422;&#20102;&#24120;&#29992;&#30340;&#28145;&#24230;&#20989;&#25968;&#65292;&#22914;&#21322;&#24179;&#38754;&#65288;&#25110;Tukey&#65289;&#28145;&#24230;&#65292;&#31354;&#38388;&#28145;&#24230;&#21644;&#38598;&#25104;&#21452;&#28145;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26607;&#35199;&#36793;&#38469;&#19979;&#65292;&#37325;&#23614;&#20301;&#32622;&#20272;&#35745;&#30340;&#20195;&#20215;&#36229;&#36807;&#20102;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;&#25105;&#20204;&#22312;&#39640;&#36798;d = 100&#30340;&#32500;&#24230;&#19978;&#20351;&#29992;&#39640;&#26031;&#27745;&#26579;&#27169;&#22411;&#36827;&#34892;&#20102;&#25968;&#20540;&#28436;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#31169;&#26377;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20316;&#20026;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#21103;&#20135;&#21697;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06459v2 Announce Type: replace-cross  Abstract: Statistical tools which satisfy rigorous privacy guarantees are necessary for modern data analysis. It is well-known that robustness against contamination is linked to differential privacy. Despite this fact, using multivariate medians for differentially private and robust multivariate location estimation has not been systematically studied. We develop novel finite-sample performance guarantees for differentially private multivariate depth-based medians, which are essentially sharp. Our results cover commonly used depth functions, such as the halfspace (or Tukey) depth, spatial depth, and the integrated dual depth. We show that under Cauchy marginals, the cost of heavy-tailed location estimation outweighs the cost of privacy. We demonstrate our results numerically using a Gaussian contamination model in dimensions up to d = 100, and compare them to a state-of-the-art private mean estimation algorithm. As a by-product of our inv
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;P2ANet&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#20050;&#20051;&#29699;&#27604;&#36187;&#24191;&#25773;&#35270;&#39057;&#20013;&#36827;&#34892;&#31264;&#23494;&#21160;&#20316;&#26816;&#27979;&#65292;&#21253;&#21547;2,721&#20010;&#35270;&#39057;&#21098;&#36753;&#65307;&#36890;&#36807;&#19982;&#19987;&#19994;&#20154;&#21592;&#21512;&#20316;&#33719;&#24471;&#20102;&#32454;&#31890;&#24230;&#30340;&#21160;&#20316;&#26631;&#31614;&#65292;&#22312;14&#20010;&#31867;&#21035;&#20013;&#28085;&#30422;&#20102;&#27599;&#20010;&#20050;&#20051;&#29699;&#21160;&#20316;&#65292;&#25552;&#20986;&#20102;&#21160;&#20316;&#23450;&#20301;&#21644;&#21160;&#20316;&#35782;&#21035;&#20004;&#31181;&#38382;&#39064;&#65292;&#24182;&#23545;&#22810;&#31181;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2207.12730</link><description>&lt;p&gt;
P2ANet&#65306;&#29992;&#20110;&#20174;&#20050;&#20051;&#29699;&#27604;&#36187;&#24191;&#25773;&#35270;&#39057;&#20013;&#36827;&#34892;&#31264;&#23494;&#21160;&#20316;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
P2ANet: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.12730
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;P2ANet&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#20050;&#20051;&#29699;&#27604;&#36187;&#24191;&#25773;&#35270;&#39057;&#20013;&#36827;&#34892;&#31264;&#23494;&#21160;&#20316;&#26816;&#27979;&#65292;&#21253;&#21547;2,721&#20010;&#35270;&#39057;&#21098;&#36753;&#65307;&#36890;&#36807;&#19982;&#19987;&#19994;&#20154;&#21592;&#21512;&#20316;&#33719;&#24471;&#20102;&#32454;&#31890;&#24230;&#30340;&#21160;&#20316;&#26631;&#31614;&#65292;&#22312;14&#20010;&#31867;&#21035;&#20013;&#28085;&#30422;&#20102;&#27599;&#20010;&#20050;&#20051;&#29699;&#21160;&#20316;&#65292;&#25552;&#20986;&#20102;&#21160;&#20316;&#23450;&#20301;&#21644;&#21160;&#20316;&#35782;&#21035;&#20004;&#31181;&#38382;&#39064;&#65292;&#24182;&#23545;&#22810;&#31181;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35270;&#39057;&#20998;&#26512;&#65292;&#22914;&#35270;&#39057;&#20998;&#31867;&#21644;&#21160;&#20316;&#26816;&#27979;&#65292;&#20294;&#22312;&#24555;&#36895;&#31227;&#21160;&#20307;&#32946;&#35270;&#39057;&#20013;&#36827;&#34892;&#31264;&#23494;&#21160;&#20316;&#26816;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#21478;&#19968;&#20010;&#20307;&#32946;&#35270;&#39057;&#22522;&#20934;\TheName{}&#65292;&#29992;&#20110;\emph{\underline{P}}ing \emph{\underline{P}}ong-\emph{\underline{A}}ction&#26816;&#27979;&#65292;&#21253;&#25324;&#20174;&#19990;&#30028;&#20050;&#20051;&#29699;&#38182;&#26631;&#36187;&#21644;&#22885;&#26519;&#21305;&#20811;&#36816;&#21160;&#20250;&#30340;&#19987;&#19994;&#20050;&#20051;&#29699;&#27604;&#36187;&#24191;&#25773;&#35270;&#39057;&#20013;&#25910;&#38598;&#30340;2,721&#20010;&#35270;&#39057;&#21098;&#36753;&#12290;&#25105;&#20204;&#19982;&#20050;&#20051;&#29699;&#19987;&#19994;&#20154;&#21592;&#21644;&#35009;&#21028;&#21512;&#20316;&#65292;&#20351;&#29992;&#19987;&#38376;&#35774;&#35745;&#30340;&#27880;&#37322;&#24037;&#20855;&#31665;&#33719;&#24471;&#20102;&#25968;&#25454;&#38598;&#20013;&#20986;&#29616;&#30340;&#27599;&#20010;&#20050;&#20051;&#29699;&#21160;&#20316;&#30340;&#32454;&#31890;&#24230;&#21160;&#20316;&#26631;&#31614;&#65288;14&#20010;&#31867;&#65289;&#65292;&#24182;&#21046;&#23450;&#20102;&#20004;&#32452;&#21160;&#20316;&#26816;&#27979;&#38382;&#39064;--\emph{&#21160;&#20316;&#23450;&#20301;}&#21644;\emph{&#21160;&#20316;&#35782;&#21035;}&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35768;&#22810;&#24120;&#35265;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65288;&#20363;&#22914;TSM&#65292;TSN&#65292;Video SwinTr&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.12730v2 Announce Type: replace-cross  Abstract: While deep learning has been widely used for video analytics, such as video classification and action detection, dense action detection with fast-moving subjects from sports videos is still challenging. In this work, we release yet another sports video benchmark \TheName{} for \emph{\underline{P}}ing \emph{\underline{P}}ong-\emph{\underline{A}}ction detection, which consists of 2,721 video clips collected from the broadcasting videos of professional table tennis matches in World Table Tennis Championships and Olympiads. We work with a crew of table tennis professionals and referees on a specially designed annotation toolbox to obtain fine-grained action labels (in 14 classes) for every ping-pong action that appeared in the dataset, and formulate two sets of action detection problems -- \emph{action localization} and \emph{action recognition}. We evaluate a number of commonly-seen action recognition (e.g., TSM, TSN, Video SwinTr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#24863;&#30693;&#22240;&#26524;FL&#31639;&#27861;&#65288;FedCau&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#26080;&#32447;&#32593;&#32476;&#19978;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#39640;&#25928;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#36845;&#20195;&#32456;&#27490;&#26041;&#27861;&#26435;&#34913;&#20102;&#35757;&#32451;&#24615;&#33021;&#21644;&#32593;&#32476;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2204.07773</link><description>&lt;p&gt;
FedCau&#65306;&#19968;&#31181;&#29992;&#20110;&#36890;&#20449;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#20027;&#21160;&#20572;&#27490;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
FedCau: A Proactive Stop Policy for Communication and Computation Efficient Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.07773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#24863;&#30693;&#22240;&#26524;FL&#31639;&#27861;&#65288;FedCau&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#26080;&#32447;&#32593;&#32476;&#19978;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#36890;&#20449;&#21644;&#35745;&#31639;&#39640;&#25928;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#36845;&#20195;&#32456;&#27490;&#26041;&#27861;&#26435;&#34913;&#20102;&#35757;&#32451;&#24615;&#33021;&#21644;&#32593;&#32476;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#32447;&#35774;&#22791;&#32452;&#25104;&#30340;&#26080;&#32447;&#32593;&#32476;&#19978;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#20998;&#24067;&#24335;&#35757;&#32451;&#31639;&#27861;&#30340;&#36890;&#20449;&#36845;&#20195;&#21487;&#33021;&#21463;&#21040;&#35774;&#22791;&#32972;&#26223;&#27969;&#37327;&#12289;&#25968;&#25454;&#21253;&#20002;&#22833;&#12289;&#25317;&#22622;&#25110;&#24310;&#36831;&#31561;&#24433;&#21709;&#32780;&#22823;&#24133;&#24694;&#21270;&#29978;&#33267;&#34987;&#38459;&#26029;&#12290;&#25105;&#20204;&#23558;&#36890;&#20449;-&#35745;&#31639;&#24433;&#21709;&#25277;&#35937;&#20026;&#8220;&#36845;&#20195;&#25104;&#26412;&#8221;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#25104;&#26412;&#24863;&#30693;&#22240;&#26524;FL&#31639;&#27861;&#65288;FedCau&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#32456;&#27490;&#26041;&#27861;&#65292;&#26435;&#34913;&#20102;&#35757;&#32451;&#24615;&#33021;&#21644;&#32593;&#32476;&#25104;&#26412;&#12290;&#25105;&#20204;&#22312;&#23458;&#25143;&#31471;&#20351;&#29992;&#20998;&#27133;ALOHA&#12289;&#24102;&#20914;&#31361;&#36991;&#20813;&#30340;&#36733;&#27874;&#30417;&#21548;&#22810;&#36335;&#35775;&#38382;&#65288;CSMA/CA&#65289;&#21644;&#27491;&#20132;&#39057;&#20998;&#22810;&#22336;&#65288;OFDMA&#65289;&#21327;&#35758;&#26102;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#32473;&#23450;&#24635;&#25104;&#26412;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#24615;&#33021;&#20250;&#38543;&#30528;&#32972;&#26223;&#36890;&#20449;&#27969;&#37327;&#25110;&#29305;&#24449;&#30340;&#32500;&#24230;&#22686;&#21152;&#32780;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.07773v2 Announce Type: replace  Abstract: This paper investigates efficient distributed training of a Federated Learning~(FL) model over a wireless network of wireless devices. The communication iterations of the distributed training algorithm may be substantially deteriorated or even blocked by the effects of the devices' background traffic, packet losses, congestion, or latency. We abstract the communication-computation impacts as an `iteration cost' and propose a cost-aware causal FL algorithm~(FedCau) to tackle this problem. We propose an iteration-termination method that trade-offs the training performance and networking costs. We apply our approach when clients use the slotted-ALOHA, the carrier-sense multiple access with collision avoidance~(CSMA/CA), and the orthogonal frequency-division multiple access~(OFDMA) protocols. We show that, given a total cost budget, the training performance degrades as either the background communication traffic or the dimension of the t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;$S^{4}\textit{Crowd}$&#65292;&#21487;&#20197;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#20154;&#32676;&#35745;&#25968;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#25439;&#22833;&#21644;&#22522;&#20110;&#20154;&#32676;&#21464;&#21270;&#30340;&#36882;&#24402;&#21333;&#20803;&#65292;&#20197;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2108.13969</link><description>&lt;p&gt;
&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#20154;&#32676;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Crowd Counting from Unlabeled Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.13969
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;$S^{4}\textit{Crowd}$&#65292;&#21487;&#20197;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#20154;&#32676;&#35745;&#25968;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#25439;&#22833;&#21644;&#22522;&#20110;&#20154;&#32676;&#21464;&#21270;&#30340;&#36882;&#24402;&#21333;&#20803;&#65292;&#20197;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#24182;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20154;&#32676;&#34892;&#20026;&#20998;&#26512;&#21487;&#20197;&#26377;&#25928;&#24110;&#21161;&#26085;&#24120;&#20132;&#36890;&#32479;&#35745;&#21644;&#35268;&#21010;&#65292;&#20174;&#32780;&#20419;&#36827;&#26234;&#24935;&#22478;&#24066;&#24314;&#35774;&#12290; &#20154;&#32676;&#35745;&#25968;&#20316;&#20026;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#20851;&#38190;&#65292;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#20294;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#20154;&#32676;&#26631;&#27880;&#12290; &#20026;&#20102;&#20943;&#36731;&#22312;&#23454;&#38469;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; $S^{4}\textit{Crowd}$&#65292;&#21487;&#20197;&#21033;&#29992;&#26080;&#26631;&#31614;/&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#31283;&#20581;&#30340;&#20154;&#32676;&#35745;&#25968;&#12290; &#22312;&#26080;&#30417;&#30563;&#36890;&#36947;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;\textit{&#33258;&#30417;&#30563;&#25439;&#22833;}&#26469;&#27169;&#25311;&#20154;&#32676;&#21464;&#21270;&#65292;&#22914;&#23610;&#24230;&#65292;&#29031;&#26126;&#65292;&#22522;&#20110;&#27492;&#29983;&#25104;&#24182;&#36880;&#28176;&#23436;&#21892;&#20102;&#30417;&#30563;&#20449;&#24687;&#20266;&#26631;&#31614;&#12290; &#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#32676;&#20026;&#39537;&#21160;&#30340;&#24490;&#29615;&#21333;&#20803;\textit{Gated-Crowd-Recurrent-Unit&#65288;GCRU&#65289;}&#65292;&#21487;&#20197;&#20445;&#30041;&#37492;&#21035;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.13969v3 Announce Type: replace-cross  Abstract: Automatic Crowd behavior analysis can be applied to effectively help the daily transportation statistics and planning, which helps the smart city construction. As one of the most important keys, crowd counting has drawn increasing attention. Recent works achieved promising performance but relied on the supervised paradigm with expensive crowd annotations. To alleviate the annotation cost in real-world transportation scenarios, in this work we proposed a semi-supervised learning framework $S^{4}\textit{Crowd}$, which can leverage both unlabeled/labeled data for robust crowd counting. In the unsupervised pathway, two \textit{self-supervised losses} were proposed to simulate the crowd variations such as scale, illumination, based on which supervised information pseudo labels were generated and gradually refined. We also proposed a crowd-driven recurrent unit \textit{Gated-Crowd-Recurrent-Unit (GCRU)}, which can preserve discrimina
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#20855;&#26377;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#26679;&#26412;&#39640;&#25928;&#24046;&#20998;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#26080;&#38656;&#23545;&#21327;&#26041;&#24046;&#36827;&#34892;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2106.13329</link><description>&lt;p&gt;
&#26080;&#38656;&#31169;&#26377;&#21327;&#26041;&#24046;&#20272;&#35745;&#30340;&#21327;&#26041;&#24046;&#24863;&#30693;&#31169;&#26377;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Covariance-Aware Private Mean Estimation Without Private Covariance Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.13329
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#20855;&#26377;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#26679;&#26412;&#39640;&#25928;&#24046;&#20998;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#26080;&#38656;&#23545;&#21327;&#26041;&#24046;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#20855;&#26377;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;$d$&#32500;(&#23376;)&#39640;&#26031;&#20998;&#24067;&#30340;&#26679;&#26412;&#39640;&#25928;&#24046;&#20998;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#22120;&#12290;&#31616;&#21333;&#26469;&#35828;&#65292;&#23545;&#20110;&#20855;&#26377;&#22343;&#20540;$\mu$&#21644;&#21327;&#26041;&#24046;$\Sigma$&#30340;&#36825;&#31181;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#32473;&#20986;$\tilde\mu$&#65292;&#20351;&#24471;$\|\tilde\mu - \mu\|_{\Sigma} \leq \alpha$&#65292;&#20854;&#20013;$\|\cdot\|_{\Sigma}$&#26159;&#39532;&#27663;&#36317;&#31163;&#12290;&#25152;&#26377;&#20808;&#21069;&#20855;&#26377;&#30456;&#21516;&#20445;&#35777;&#30340;&#20272;&#35745;&#22120;&#35201;&#20040;&#38656;&#35201;&#23545;&#21327;&#26041;&#24046;&#30697;&#38453;&#26377;&#24378;&#22823;&#30340;&#20808;&#39564;&#30028;&#38480;&#65292;&#35201;&#20040;&#38656;&#35201;$\Omega(d^{3/2})$&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#27599;&#20010;&#20272;&#35745;&#22120;&#37117;&#22522;&#20110;&#19968;&#31181;&#31616;&#21333;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#35774;&#35745;&#24046;&#20998;&#31169;&#26377;&#26426;&#21046;&#65292;&#20294;&#36890;&#36807;&#26032;&#39062;&#30340;&#25216;&#26415;&#27493;&#39588;&#20351;&#20272;&#35745;&#22120;&#20855;&#26377;&#38544;&#31169;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2106.13329v3 Announce Type: replace  Abstract: We present two sample-efficient differentially private mean estimators for $d$-dimensional (sub)Gaussian distributions with unknown covariance. Informally, given $n \gtrsim d/\alpha^2$ samples from such a distribution with mean $\mu$ and covariance $\Sigma$, our estimators output $\tilde\mu$ such that $\| \tilde\mu - \mu \|_{\Sigma} \leq \alpha$, where $\| \cdot \|_{\Sigma}$ is the Mahalanobis distance. All previous estimators with the same guarantee either require strong a priori bounds on the covariance matrix or require $\Omega(d^{3/2})$ samples.   Each of our estimators is based on a simple, general approach to designing differentially private mechanisms, but with novel technical steps to make the estimator private and sample-efficient. Our first estimator samples a point with approximately maximum Tukey depth using the exponential mechanism, but restricted to the set of points of large Tukey depth. Its accuracy guarantees hold e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#31995;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#21457;&#29616;&#30340;&#20808;&#36827;&#36125;&#21494;&#26031;&#31232;&#30095;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38408;&#20540;&#36125;&#21494;&#26031;&#32452;Lasso&#22238;&#24402;&#21644;Gibbs&#37319;&#26679;&#22120;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#21644;&#20943;&#36731;&#20102;&#35745;&#31639;&#36127;&#25285;</title><link>https://arxiv.org/abs/2102.01432</link><description>&lt;p&gt;
&#24102;&#21487;&#21464;&#31995;&#25968;&#30340;&#36125;&#21494;&#26031;&#25968;&#25454;&#39537;&#21160;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Bayesian data-driven discovery of partial differential equations with variable coefficients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2102.01432
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#31995;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#21457;&#29616;&#30340;&#20808;&#36827;&#36125;&#21494;&#26031;&#31232;&#30095;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38408;&#20540;&#36125;&#21494;&#26031;&#32452;Lasso&#22238;&#24402;&#21644;Gibbs&#37319;&#26679;&#22120;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#21644;&#20943;&#36731;&#20102;&#35745;&#31639;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#21457;&#29616;&#26159;&#24212;&#29992;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;PDEs&#21457;&#29616;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#25152;&#21457;&#29616;&#26041;&#31243;&#23545;&#22122;&#22768;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#27169;&#22411;&#36873;&#25321;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#21487;&#21464;&#31995;&#25968;&#30340;PDE&#21457;&#29616;&#65292;&#23588;&#20854;&#26159;&#24403;&#31995;&#25968;&#22312;&#31354;&#38388;&#25110;&#26102;&#38388;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#26102;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20855;&#26377;&#23574;&#23792;&#21644;&#24179;&#26495;&#20808;&#39564;&#30340;&#38408;&#20540;&#36125;&#21494;&#26031;&#32452;Lasso&#22238;&#24402;&#65288;tBGL-SS&#65289;&#65292;&#24182;&#21033;&#29992;Gibbs&#37319;&#26679;&#22120;&#26469;&#23545;PDE&#31995;&#25968;&#36827;&#34892;&#36125;&#21494;&#26031;&#21518;&#39564;&#20272;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22686;&#24378;&#20102;&#28857;&#20272;&#35745;&#30340;&#31283;&#20581;&#24615;&#24182;&#20855;&#26377;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#32780;&#19988;&#36890;&#36807;&#23558;&#31995;&#25968;&#38408;&#20540;&#38598;&#25104;&#20026;&#36817;&#20284;MCMC&#26041;&#27861;&#65292;&#20943;&#36731;&#20102;&#26469;&#33258;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2102.01432v2 Announce Type: replace-cross  Abstract: The discovery of Partial Differential Equations (PDEs) is an essential task for applied science and engineering. However, data-driven discovery of PDEs is generally challenging, primarily stemming from the sensitivity of the discovered equation to noise and the complexities of model selection. In this work, we propose an advanced Bayesian sparse learning algorithm for PDE discovery with variable coefficients, predominantly when the coefficients are spatially or temporally dependent. Specifically, we apply threshold Bayesian group Lasso regression with a spike-and-slab prior (tBGL-SS) and leverage a Gibbs sampler for Bayesian posterior estimation of PDE coefficients. This approach not only enhances the robustness of point estimation with valid uncertainty quantification but also relaxes the computational burden from Bayesian inference through the integration of coefficient thresholds as an approximate MCMC method. Moreover, from
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23436;&#20840;&#29420;&#31435;&#36890;&#20449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#35777;&#26126;&#29420;&#31435;&#26234;&#33021;&#20307;&#20173;&#21487;&#20197;&#23398;&#20064;&#36890;&#20449;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#36890;&#20449;&#22312;&#19981;&#21516;&#32593;&#32476;&#23481;&#37327;&#19979;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.15059</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23436;&#20840;&#29420;&#31435;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Fully Independent Communication in Multi-Agent Reinforcement Learning. (arXiv:2401.15059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23436;&#20840;&#29420;&#31435;&#36890;&#20449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#35777;&#26126;&#29420;&#31435;&#26234;&#33021;&#20307;&#20173;&#21487;&#20197;&#23398;&#20064;&#36890;&#20449;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#36890;&#20449;&#22312;&#19981;&#21516;&#32593;&#32476;&#23481;&#37327;&#19979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#39046;&#22495;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#19987;&#27880;&#20110;&#30740;&#31350;MARL&#20013;&#30340;&#36890;&#20449;&#26041;&#27861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#36890;&#20449;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20173;&#28982;&#36807;&#20110;&#22797;&#26434;&#65292;&#19981;&#23481;&#26131;&#36801;&#31227;&#21040;&#26356;&#23454;&#38469;&#30340;&#24773;&#22659;&#20013;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#20351;&#29992;&#20102;&#33879;&#21517;&#30340;&#21442;&#25968;&#20849;&#20139;&#25216;&#24039;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;MARL&#20013;&#19981;&#20849;&#20139;&#21442;&#25968;&#30340;&#29420;&#31435;&#23398;&#20064;&#32773;&#22914;&#20309;&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#35774;&#32622;&#21487;&#33021;&#20250;&#24102;&#26469;&#19968;&#20123;&#38382;&#39064;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#26696;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#38754;&#20020;&#25361;&#25112;&#65292;&#29420;&#31435;&#30340;&#26234;&#33021;&#20307;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#36890;&#20449;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#30740;&#31350;MARL&#20013;&#30340;&#36890;&#20449;&#22914;&#20309;&#21463;&#21040;&#19981;&#21516;&#32593;&#32476;&#23481;&#37327;&#30340;&#24433;&#21709;&#65292;&#26080;&#35770;&#26159;&#20849;&#20139;&#21442;&#25968;&#36824;&#26159;&#19981;&#20849;&#20139;&#21442;&#25968;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#20449;&#30340;&#33021;&#21147;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research within the field of multi-agent systems. Several recent works have focused specifically on the study of communication approaches in MARL. While multiple communication methods have been proposed, these might still be too complex and not easily transferable to more practical contexts. One of the reasons for that is due to the use of the famous parameter sharing trick. In this paper, we investigate how independent learners in MARL that do not share parameters can communicate. We demonstrate that this setting might incur into some problems, to which we propose a new learning scheme as a solution. Our results show that, despite the challenges, independent agents can still learn communication strategies following our method. Additionally, we use this method to investigate how communication in MARL is affected by different network capacities, both for sharing and not sharing parameters. We observe that communication 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12533</link><description>&lt;p&gt;
&#26377;&#25928;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;k&#20013;&#24515;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24515;&#20026;&#22522;&#30784;&#30340;&#32858;&#31867;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#37117;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36755;&#20837;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;k&#20013;&#24515;&#32858;&#31867;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#32972;&#26223;&#30693;&#35782;&#24314;&#27169;&#20026;&#24517;&#36830;&#65288;ML&#65289;&#21644;&#19981;&#36830;&#65288;CL&#65289;&#32422;&#26463;&#38598;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21253;&#25324;k&#20013;&#24515;&#22312;&#20869;&#30340;&#32858;&#31867;&#38382;&#39064;&#26412;&#36136;&#19978;&#37117;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#21463;&#32422;&#26463;&#21464;&#20307;&#34987;&#35748;&#20026;&#21463;&#21040;&#26356;&#20005;&#37325;&#30340;&#36817;&#20284;&#21644;&#35745;&#31639;&#38556;&#30861;&#30340;&#38480;&#21046;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#21453;&#25903;&#37197;&#38598;&#65292;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#25972;&#25968;&#24179;&#38754;&#21644;LP&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#32422;&#26463;k&#20013;&#24515;&#30340;&#39640;&#25928;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#31454;&#20105;&#22522;&#20934;&#31639;&#27861;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#36817;&#20284;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#29109;&#26368;&#22823;&#21270;&#30340;&#26041;&#24335;&#22312;&#27169;&#25311;&#35757;&#32451;&#20013;&#35843;&#25972;&#21160;&#21147;&#23398;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#65292;&#26080;&#38656;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#33021;&#22815;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#21644;&#20195;&#29702;&#30340;&#39640;&#25104;&#21151;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.01885</link><description>&lt;p&gt;
&#36890;&#36807;&#29109;&#26368;&#22823;&#21270;&#36827;&#34892;&#39046;&#22495;&#38543;&#26426;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Randomization via Entropy Maximization. (arXiv:2311.01885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#29109;&#26368;&#22823;&#21270;&#30340;&#26041;&#24335;&#22312;&#27169;&#25311;&#35757;&#32451;&#20013;&#35843;&#25972;&#21160;&#21147;&#23398;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#65292;&#26080;&#38656;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#33021;&#22815;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#21644;&#20195;&#29702;&#30340;&#39640;&#25104;&#21151;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#25913;&#21464;&#27169;&#25311;&#20013;&#30340;&#21160;&#21147;&#23398;&#21442;&#25968;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20811;&#26381;&#29616;&#23454;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#38543;&#26426;&#21270;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#21160;&#21147;&#23398;&#21442;&#25968;&#30340;&#25277;&#26679;&#20998;&#24067;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#39640;&#21464;&#24322;&#23545;&#20110;&#35268;&#33539;&#20195;&#29702;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36807;&#24230;&#38543;&#26426;&#21270;&#20250;&#23548;&#33268;&#36807;&#20110;&#20445;&#23432;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#27169;&#25311;&#21040;&#30495;&#23454;&#30340;&#36716;&#31227;&#65292;&#21363;&#22312;&#27169;&#25311;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#21160;&#35843;&#25972;&#21160;&#21147;&#23398;&#20998;&#24067;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36807;&#29109;&#26368;&#22823;&#21270;&#23454;&#29616;&#39046;&#22495;&#38543;&#26426;&#21270;&#30340;&#26041;&#27861;&#65288;DORAEMON&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#38480;&#20248;&#21270;&#38382;&#39064;&#65292;&#30452;&#25509;&#26368;&#22823;&#21270;&#35757;&#32451;&#20998;&#24067;&#30340;&#29109;&#21516;&#26102;&#20445;&#30041;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;DORAEMON&#38543;&#30528;&#24403;&#21069;&#31574;&#30053;&#25104;&#21151;&#27010;&#29575;&#36275;&#22815;&#39640;&#65292;&#36880;&#28176;&#22686;&#21152;&#26679;&#26412;&#21160;&#21147;&#23398;&#21442;&#25968;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. In this paper, we propose a novel approach to address sim-to-real transfer, which automatically shapes dynamics distributions during training in simulation without requiring real-world data. We introduce DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. In achieving this, DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high. We empiri
&lt;/p&gt;</description></item><item><title>PPI++&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#39044;&#27979;&#39537;&#21160;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#39044;&#27979;&#36136;&#37327;&#26469;&#25913;&#21892;&#32463;&#20856;&#21306;&#38388;&#30340;&#35745;&#31639;&#32622;&#20449;&#21306;&#38388;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#32479;&#35745;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.01453</link><description>&lt;p&gt;
PPI++:&#39640;&#25928;&#30340;&#39044;&#27979;&#39537;&#21160;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PPI++: Efficient Prediction-Powered Inference. (arXiv:2311.01453v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01453
&lt;/p&gt;
&lt;p&gt;
PPI++&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#39044;&#27979;&#39537;&#21160;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#39044;&#27979;&#36136;&#37327;&#26469;&#25913;&#21892;&#32463;&#20856;&#21306;&#38388;&#30340;&#35745;&#31639;&#32622;&#20449;&#21306;&#38388;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#32479;&#35745;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PPI++&#65306;&#19968;&#31181;&#22522;&#20110;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#36890;&#24120;&#27604;&#36739;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#25968;&#25454;&#38598;&#30340;&#35745;&#31639;&#36731;&#37327;&#32423;&#30340;&#20272;&#35745;&#21644;&#25512;&#29702;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#21487;&#29992;&#39044;&#27979;&#30340;&#36136;&#37327;&#65292;&#20135;&#29983;&#26131;&#20110;&#35745;&#31639;&#30340;&#32622;&#20449;&#21306;&#38388; - &#23545;&#20110;&#20219;&#24847;&#32500;&#24230;&#30340;&#21442;&#25968; - &#24635;&#26159;&#33021;&#22815;&#22312;&#21482;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#32463;&#20856;&#21306;&#38388;&#12290;PPI++&#22522;&#20110;&#39044;&#27979;&#39537;&#21160;&#25512;&#29702;&#65288;PPI&#65289;&#65292;&#38024;&#23545;&#30456;&#21516;&#30340;&#38382;&#39064;&#22330;&#26223;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#12290;&#30495;&#23454;&#21644;&#21512;&#25104;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#25913;&#36827;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present PPI++: a computationally lightweight methodology for estimation and inference based on a small labeled dataset and a typically much larger dataset of machine-learning predictions. The methods automatically adapt to the quality of available predictions, yielding easy-to-compute confidence sets -for parameters of any dimensionality -- that always improve on classical intervals using only the labeled data. PPI++ builds on prediction-powered inference (PPI), which targets the same problem setting, improving its computational and statistical efficiency. Real and synthetic experiments demonstrate the benefits of the proposed adaptations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#27969;&#21305;&#37197;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;</title><link>http://arxiv.org/abs/2310.16338</link><description>&lt;p&gt;
&#24102;&#26377;&#27969;&#21305;&#37197;&#30340;&#35821;&#38899;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-training for Speech with Flow Matching. (arXiv:2310.16338v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#20351;&#29992;&#27969;&#21305;&#37197;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36827;&#34892;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#38656;&#35201;&#20272;&#35745;&#21644;&#25277;&#26679;&#25968;&#25454;&#20998;&#24067;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#21512;&#25104;&#25968;&#25454;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#35821;&#38899;&#39046;&#22495;&#65292;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#21644;&#31070;&#32463;&#22768;&#30721;&#22120;&#26159;&#29983;&#25104;&#27169;&#22411;&#25104;&#21151;&#24212;&#29992;&#30340;&#20856;&#22411;&#20363;&#23376;&#12290;&#23613;&#31649;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#35821;&#38899;&#30340;&#19981;&#21516;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20294;&#36824;&#27809;&#26377;&#19968;&#20010;&#36890;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#21333;&#19968;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#24182;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36808;&#20986;&#20102;&#36825;&#20010;&#26041;&#21521;&#30340;&#19968;&#27493;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#27969;&#21305;&#37197;&#21644;&#33945;&#29256;&#26465;&#20214;&#22312;60k&#23567;&#26102;&#30340;&#26410;&#36716;&#24405;&#35821;&#38899;&#19978;&#39044;&#35757;&#32451;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechFlow&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22312;&#35821;&#38899;&#22686;&#24378;&#12289;&#20998;&#31163;&#21644;&#21512;&#25104;&#26041;&#38754;&#36798;&#21040;&#25110;&#36229;&#36807;&#29616;&#26377;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.15694</link><description>&lt;p&gt;
COPF: &#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15694
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#25216;&#26415;&#26159;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;RLHF&#30340;LM&#22312;&#24341;&#20837;&#26032;&#30340;&#26597;&#35810;&#25110;&#21453;&#39304;&#26102;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20154;&#31867;&#20559;&#22909;&#22312;&#19981;&#21516;&#39046;&#22495;&#25110;&#20219;&#21153;&#20043;&#38388;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#30001;&#20110;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#37325;&#26032;&#35757;&#32451;LM&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#23454;&#38469;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#65288;COPF&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#27861;&#20272;&#35745;&#19968;&#31995;&#21015;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#36890;&#36807;&#20989;&#25968;&#27491;&#21017;&#21270;&#19981;&#26029;&#25311;&#21512;&#31574;&#30053;&#24207;&#21015;&#12290;COPF&#21253;&#21547;&#19968;&#20010;&#21333;&#19968;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23398;&#20064;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20803;&#23398;&#20064;&#22522;&#20934;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#25110;&#19982;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2310.10971</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Meta-Learning. (arXiv:2310.10971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23398;&#20064;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20803;&#23398;&#20064;&#22522;&#20934;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#25110;&#19982;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26080;&#38656;&#24494;&#35843;&#23601;&#33021;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#25512;&#29702;&#36807;&#31243;&#20013;&#26816;&#27979;&#26032;&#23545;&#35937;&#30340;&#35270;&#35273;&#27169;&#22411;&#23578;&#26410;&#33021;&#22815;&#22797;&#21046;&#36825;&#31181;&#33021;&#21147;&#65292;&#32780;&#26159;&#34920;&#29616;&#31967;&#31957;&#25110;&#38656;&#35201;&#23545;&#31867;&#20284;&#23545;&#35937;&#36827;&#34892;&#20803;&#35757;&#32451;&#21644;/&#25110;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23398;&#20064;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#32780;&#26080;&#38656;&#24494;&#35843;&#26469;&#27169;&#20223;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19968;&#20010;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#31867;&#20284;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#23558;&#20803;&#23398;&#20064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22312;&#24050;&#30693;&#26631;&#31614;&#30340;&#25968;&#25454;&#28857;&#21644;&#26410;&#30693;&#26631;&#31614;&#30340;&#27979;&#35797;&#25968;&#25454;&#28857;&#19978;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#22312;11&#20010;&#20803;&#23398;&#20064;&#22522;&#20934;&#20013;&#30340;8&#20010;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861; - &#26080;&#38656;&#20803;&#35757;&#32451;&#25110;&#24494;&#35843; - &#36229;&#36807;&#25110;&#19982;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#32463;&#36807;&#20803;&#35757;&#32451;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;P&gt;M&gt;F&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or matches the state-of-the-art algorithm, P&gt;M&gt;F, which is meta-trained on these benchmarks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#21457;&#20986;&#22768;&#38899;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#22522;&#20110;&#20381;&#24651;&#29702;&#35770;&#30340;&#33258;&#25105;&#20381;&#24651;&#25216;&#26415;&#12290;&#36890;&#36807;&#20351;&#29992;&#35268;&#21017;&#21644;&#20998;&#31867;&#27169;&#22359;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#36755;&#20837;&#24182;&#25512;&#33616;&#36866;&#24403;&#30340;&#33258;&#25105;&#20381;&#24651;&#32451;&#20064;&#12290;&#35813;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20934;&#30830;&#29575;&#36229;&#36807;92%&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22359;&#65292;&#20197;&#35782;&#21035;&#29992;&#25143;&#24773;&#24863;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#22312;&#21518;&#30123;&#24773;&#26102;&#20195;&#25552;&#20379;&#25968;&#23383;&#24515;&#29702;&#30103;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.09362</link><description>&lt;p&gt;
&#20174;&#35789;&#35821;&#21644;&#32451;&#20064;&#21040;&#20581;&#24247;&#65306;&#29992;&#20110;&#33258;&#25105;&#20381;&#24651;&#25216;&#26415;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique. (arXiv:2310.09362v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#21457;&#20986;&#22768;&#38899;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#29992;&#20110;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#22522;&#20110;&#20381;&#24651;&#29702;&#35770;&#30340;&#33258;&#25105;&#20381;&#24651;&#25216;&#26415;&#12290;&#36890;&#36807;&#20351;&#29992;&#35268;&#21017;&#21644;&#20998;&#31867;&#27169;&#22359;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#29702;&#35299;&#29992;&#25143;&#36755;&#20837;&#24182;&#25512;&#33616;&#36866;&#24403;&#30340;&#33258;&#25105;&#20381;&#24651;&#32451;&#20064;&#12290;&#35813;&#30740;&#31350;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20934;&#30830;&#29575;&#36229;&#36807;92%&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22359;&#65292;&#20197;&#35782;&#21035;&#29992;&#25143;&#24773;&#24863;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#22312;&#21518;&#30123;&#24773;&#26102;&#20195;&#25552;&#20379;&#25968;&#23383;&#24515;&#29702;&#30103;&#27861;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21518;&#30123;&#24773;&#26102;&#20195;&#65292;&#31038;&#20132;&#23396;&#31435;&#21644;&#25233;&#37057;&#28966;&#34385;&#30151;&#30340;&#24739;&#30149;&#29575;&#25856;&#21319;&#30340;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#25968;&#23383;&#24515;&#29702;&#30103;&#27861;&#30340;&#23545;&#35805;&#20195;&#29702;&#30456;&#23545;&#20110;&#20256;&#32479;&#30103;&#27861;&#20250;&#21457;&#25381;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#21457;&#20986;&#22768;&#38899;&#30340;&#27874;&#26031;&#35821;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#25351;&#23548;&#29992;&#25143;&#36827;&#34892;&#33258;&#25105;&#20381;&#24651;(Self-Attachment, SAT)&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20381;&#24651;&#29702;&#35770;&#30340;&#26032;&#22411;&#12289;&#33258;&#25105;&#31649;&#29702;&#12289;&#20840;&#38754;&#30340;&#24515;&#29702;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20351;&#29992;&#19968;&#31995;&#21015;&#22522;&#20110;&#35268;&#21017;&#21644;&#20998;&#31867;&#30340;&#27169;&#22359;&#26469;&#29702;&#35299;&#29992;&#25143;&#22312;&#23545;&#35805;&#20013;&#30340;&#36755;&#20837;&#65292;&#24182;&#30456;&#24212;&#22320;&#23548;&#33322;&#23545;&#35805;&#27969;&#31243;&#22270;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#24773;&#24863;&#21644;&#24515;&#29702;&#29366;&#24577;&#25512;&#33616;&#36866;&#24403;&#30340;SAT&#32451;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#36229;&#36807;6,000&#27425;&#35805;&#35821;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22359;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#24773;&#24863;&#20998;&#20026;12&#20010;&#31867;&#21035;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;92%&#12290;&#20026;&#20102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26032;&#39062;&#21644;&#21560;&#24341;&#21147;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#26159;&#20174;&#22823;&#37327;&#35805;&#35821;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the wake of the post-pandemic era, marked by social isolation and surging rates of depression and anxiety, conversational agents based on digital psychotherapy can play an influential role compared to traditional therapy sessions. In this work, we develop a voice-capable chatbot in Farsi to guide users through Self-Attachment (SAT), a novel, self-administered, holistic psychological technique based on attachment theory. Our chatbot uses a dynamic array of rule-based and classification-based modules to comprehend user input throughout the conversation and navigates a dialogue flowchart accordingly, recommending appropriate SAT exercises that depend on the user's emotional and mental state. In particular, we collect a dataset of over 6,000 utterances and develop a novel sentiment-analysis module that classifies user sentiment into 12 classes, with accuracy above 92%. To keep the conversation novel and engaging, the chatbot's responses are retrieved from a large dataset of utterances c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Entropy-MCMC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#30340;&#24341;&#23548;&#21464;&#37327;&#26469;&#22312;&#24179;&#22374;&#30406;&#22320;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05401</link><description>&lt;p&gt;
Entropy-MCMC: &#36731;&#26494;&#20174;&#24179;&#22374;&#30406;&#22320;&#36827;&#34892;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Entropy-MCMC: Sampling from Flat Basins with Ease. (arXiv:2310.05401v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Entropy-MCMC&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#36741;&#21161;&#30340;&#24341;&#23548;&#21464;&#37327;&#26469;&#22312;&#24179;&#22374;&#30406;&#22320;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20381;&#36182;&#20110;&#23545;&#21518;&#39564;&#20998;&#24067;&#30340;&#36136;&#37327;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#20998;&#24067;&#22312;&#24615;&#36136;&#19978;&#26159;&#39640;&#24230;&#22810;&#27169;&#24577;&#30340;&#65292;&#23616;&#37096;&#27169;&#24335;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#65292;&#20174;&#21407;&#22987;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#19968;&#20123;&#26679;&#26412;&#21487;&#33021;&#20250;&#38519;&#20837;&#8220;&#22351;&#8221;&#27169;&#24335;&#24182;&#20986;&#29616;&#36807;&#25311;&#21512;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#20302;&#27867;&#21270;&#35823;&#24046;&#30340;&#8220;&#22909;&#8221;&#27169;&#24335;&#36890;&#24120;&#23384;&#22312;&#20110;&#33021;&#37327;&#26223;&#35266;&#30340;&#24179;&#22374;&#30406;&#22320;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20559;&#32622;&#37319;&#26679;&#26397;&#21521;&#36825;&#20123;&#24179;&#22374;&#21306;&#22495;&#30340;&#21518;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#24341;&#23548;&#21464;&#37327;&#65292;&#20854;&#31283;&#24577;&#20998;&#24067;&#31867;&#20284;&#20110;&#24179;&#28369;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#27809;&#26377;&#23574;&#38160;&#30340;&#27169;&#24577;&#65292;&#20197;&#24341;&#23548;MCMC&#37319;&#26679;&#22120;&#22312;&#24179;&#22374;&#30340;&#30406;&#22320;&#20013;&#37319;&#26679;&#12290;&#36890;&#36807;&#23558;&#27492;&#24341;&#23548;&#21464;&#37327;&#19982;&#27169;&#22411;&#21442;&#25968;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#19979;&#23454;&#29616;&#39640;&#25928;&#37319;&#26679;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20803;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#22278;&#38181;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#27714;&#35299;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#26469;&#36741;&#21161;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.02969</link><description>&lt;p&gt;
&#21452;&#22278;&#38181;&#20195;&#29702;&#29992;&#20110;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Dual Conic Proxies for AC Optimal Power Flow. (arXiv:2310.02969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#22278;&#38181;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#27714;&#35299;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#26469;&#36741;&#21161;&#35757;&#32451;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#27969;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#65288;AC-OPF&#65289;&#20248;&#21270;&#20195;&#29702;&#30340;&#21457;&#23637;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#34429;&#28982;&#22312;&#39044;&#27979;&#39640;&#36136;&#37327;&#21407;&#22987;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26080;&#27861;&#20026;AC-OPF&#25552;&#20379;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;AC-OPF&#30340;&#19968;&#20010;&#20984;&#26494;&#24347;&#30340;&#20248;&#21270;&#20195;&#29702;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;AC-OPF&#30340;&#20108;&#38454;&#22278;&#38181;&#65288;SOC&#65289;&#26494;&#24347;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#20598;&#26550;&#26500;&#65292;&#23884;&#20837;&#20102;&#19968;&#20010;&#24555;&#36895;&#12289;&#21487;&#24494;&#20998;&#30340;&#65288;&#23545;&#20598;&#65289;&#21487;&#34892;&#24615;&#24674;&#22797;&#65292;&#20174;&#32780;&#25552;&#20379;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#26032;&#26550;&#26500;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#30456;&#32467;&#21512;&#65292;&#20943;&#36731;&#20102;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#38656;&#27714;&#12290;&#23545;&#20013;&#31561;&#21644;&#22823;&#35268;&#27169;&#30005;&#21147;&#32593;&#32476;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been significant interest in the development of machine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF). Although significant progress has been achieved in predicting high-quality primal solutions, no existing learning-based approach can provide valid dual bounds for AC-OPF. This paper addresses this gap by training optimization proxies for a convex relaxation of AC-OPF. Namely, the paper considers a second-order cone (SOC) relaxation of ACOPF, and proposes a novel dual architecture that embeds a fast, differentiable (dual) feasibility recovery, thus providing valid dual bounds. The paper combines this new architecture with a self-supervised learning scheme, which alleviates the need for costly training data generation. Extensive numerical experiments on medium- and large-scale power grids demonstrate the efficiency and scalability of the proposed methodology.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#35856;&#27874;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#38556;&#30861;&#20989;&#25968;&#65288;harmonic CLBF&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#32422;&#26463;&#25511;&#21046;&#38382;&#39064;&#20013;&#35299;&#20915;&#36991;&#38556;&#35201;&#27714;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#19982;&#35856;&#27874;CLBF&#26368;&#38497;&#19979;&#38477;&#26041;&#21521;&#30340;&#20869;&#31215;&#26469;&#36873;&#25321;&#25511;&#21046;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#36827;&#20837;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#39118;&#38505;&#24182;&#25552;&#39640;&#36827;&#20837;&#30446;&#26631;&#21306;&#22495;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02869</link><description>&lt;p&gt;
&#20351;&#29992;&#35856;&#27874;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#38556;&#30861;&#20989;&#25968;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#19982;&#36991;&#38556;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Harmonic Control Lyapunov Barrier Functions for Constrained Optimal Control with Reach-Avoid Specifications. (arXiv:2310.02869v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#35856;&#27874;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#38556;&#30861;&#20989;&#25968;&#65288;harmonic CLBF&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#32422;&#26463;&#25511;&#21046;&#38382;&#39064;&#20013;&#35299;&#20915;&#36991;&#38556;&#35201;&#27714;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#19982;&#35856;&#27874;CLBF&#26368;&#38497;&#19979;&#38477;&#26041;&#21521;&#30340;&#20869;&#31215;&#26469;&#36873;&#25321;&#25511;&#21046;&#36755;&#20837;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#36827;&#20837;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#39118;&#38505;&#24182;&#25552;&#39640;&#36827;&#20837;&#30446;&#26631;&#21306;&#22495;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35856;&#27874;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#38556;&#30861;&#20989;&#25968;&#65288;harmonic CLBF&#65289;&#65292;&#23427;&#26377;&#21161;&#20110;&#35299;&#20915;&#35832;&#22914;&#36991;&#38556;&#38382;&#39064;&#31561;&#30340;&#32422;&#26463;&#25511;&#21046;&#38382;&#39064;&#12290;&#35856;&#27874;CLBF&#21033;&#29992;&#35856;&#27874;&#20989;&#25968;&#28385;&#36275;&#30340;&#26368;&#22823;&#20540;&#21407;&#29702;&#26469;&#32534;&#30721;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#38556;&#30861;&#20989;&#25968;&#30340;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#23454;&#39564;&#24320;&#22987;&#26102;&#21021;&#22987;&#21270;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#26679;&#26412;&#36712;&#36857;&#36827;&#34892;&#35757;&#32451;&#12290;&#25511;&#21046;&#36755;&#20837;&#34987;&#36873;&#25321;&#20026;&#26368;&#22823;&#21270;&#31995;&#32479;&#21160;&#21147;&#23398;&#19982;&#35856;&#27874;CLBF&#26368;&#38497;&#19979;&#38477;&#26041;&#21521;&#30340;&#20869;&#31215;&#12290;&#25968;&#20540;&#32467;&#26524;&#22312;&#19981;&#21516;&#30340;&#36991;&#38556;&#29615;&#22659;&#19979;&#23637;&#31034;&#20102;&#22235;&#20010;&#19981;&#21516;&#31995;&#32479;&#30340;&#24773;&#20917;&#12290;&#35856;&#27874;CLBF&#26174;&#31034;&#20986;&#36827;&#20837;&#19981;&#23433;&#20840;&#21306;&#22495;&#30340;&#39118;&#38505;&#26174;&#33879;&#38477;&#20302;&#65292;&#36827;&#20837;&#30446;&#26631;&#21306;&#22495;&#30340;&#27010;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces harmonic control Lyapunov barrier functions (harmonic CLBF) that aid in constrained control problems such as reach-avoid problems. Harmonic CLBFs exploit the maximum principle that harmonic functions satisfy to encode the properties of control Lyapunov barrier functions (CLBFs). As a result, they can be initiated at the start of an experiment rather than trained based on sample trajectories. The control inputs are selected to maximize the inner product of the system dynamics with the steepest descent direction of the harmonic CLBF. Numerical results are presented with four different systems under different reach-avoid environments. Harmonic CLBFs show a significantly low risk of entering unsafe regions and a high probability of entering the goal region.
&lt;/p&gt;</description></item><item><title>PR-MPNNs&#36890;&#36807;&#27010;&#29575;&#37325;&#36830;&#23398;&#20064;&#21152;&#20837;&#30456;&#20851;&#36793;&#65292;&#24182;&#30465;&#30053;&#23545;&#39044;&#27979;&#20219;&#21153;&#27809;&#26377;&#24110;&#21161;&#30340;&#36793;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02156</link><description>&lt;p&gt;
&#27010;&#29575;&#37325;&#36830;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Probabilistically Rewired Message-Passing Neural Networks. (arXiv:2310.02156v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02156
&lt;/p&gt;
&lt;p&gt;
PR-MPNNs&#36890;&#36807;&#27010;&#29575;&#37325;&#36830;&#23398;&#20064;&#21152;&#20837;&#30456;&#20851;&#36793;&#65292;&#24182;&#30465;&#30053;&#23545;&#39044;&#27979;&#20219;&#21153;&#27809;&#26377;&#24110;&#21161;&#30340;&#36793;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#20316;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#36755;&#20837;&#30340;&#24378;&#22823;&#24037;&#20855;&#32780;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22266;&#23450;&#30340;&#36755;&#20837;&#22270;&#32467;&#26500;&#19978;&#25805;&#20316;&#65292;&#24573;&#30053;&#20102;&#28508;&#22312;&#30340;&#22122;&#22768;&#21644;&#32570;&#22833;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#23616;&#37096;&#32858;&#21512;&#26426;&#21046;&#21487;&#33021;&#23548;&#33268;&#38382;&#39064;&#65292;&#22914;&#36807;&#24230;&#21387;&#32553;&#21644;&#22312;&#25429;&#25417;&#30456;&#20851;&#22270;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#25512;&#26029;&#19982;&#32473;&#23450;&#39044;&#27979;&#20219;&#21153;&#30456;&#20851;&#30340;&#22270;&#32467;&#26500;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#31934;&#30830;&#21644;&#21487;&#24494;&#20998;&#30340;k-&#23376;&#38598;&#37319;&#26679;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#27010;&#29575;&#37325;&#36830;&#30340;MPNN (PR-MPNN)&#65292;&#23427;&#20204;&#23398;&#20064;&#22312;&#30465;&#30053;&#23545;&#39044;&#27979;&#20219;&#21153;&#27809;&#26377;&#24110;&#21161;&#30340;&#36793;&#30340;&#21516;&#26102;&#28155;&#21152;&#30456;&#20851;&#30340;&#36793;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#39318;&#27425;&#25506;&#32034;&#20102;PR-MPNN&#22914;&#20309;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#25105;&#20204;&#30830;&#23450;&#20102;&#30830;&#20999;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message-passing graph neural networks (MPNNs) emerged as powerful tools for processing graph-structured input. However, they operate on a fixed input graph structure, ignoring potential noise and missing information. Furthermore, their local aggregation mechanism can lead to problems such as over-squashing and limited expressive power in capturing relevant graph structures. Existing solutions to these challenges have primarily relied on heuristic methods, often disregarding the underlying data distribution. Hence, devising principled approaches for learning to infer graph structures relevant to the given prediction task remains an open challenge. In this work, leveraging recent progress in exact and differentiable $k$-subset sampling, we devise probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges while omitting less beneficial ones. For the first time, our theoretical analysis explores how PR-MPNNs enhance expressive power, and we identify precise conditions un
&lt;/p&gt;</description></item><item><title>Node-Aligned Graph-to-Graph (NAG2G)&#26159;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#26080;&#27169;&#26495;&#27169;&#22411;&#65292;&#21033;&#29992;2D&#20998;&#23376;&#22270;&#21644;3D&#26500;&#35937;&#20449;&#24687;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20998;&#23376;&#30340;&#25299;&#25169;&#20449;&#24687;&#21644;&#23545;&#40784;&#21407;&#23376;&#65292;&#25552;&#39640;&#21333;&#27493;&#21453;&#21512;&#25104;&#39044;&#27979;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15798</link><description>&lt;p&gt;
Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction. (arXiv:2309.15798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction. (arXiv:2309.15798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15798
&lt;/p&gt;
&lt;p&gt;
Node-Aligned Graph-to-Graph (NAG2G)&#26159;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#26080;&#27169;&#26495;&#27169;&#22411;&#65292;&#21033;&#29992;2D&#20998;&#23376;&#22270;&#21644;3D&#26500;&#35937;&#20449;&#24687;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20998;&#23376;&#30340;&#25299;&#25169;&#20449;&#24687;&#21644;&#23545;&#40784;&#21407;&#23376;&#65292;&#25552;&#39640;&#21333;&#27493;&#21453;&#21512;&#25104;&#39044;&#27979;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#27493;&#21453;&#21512;&#25104;&#26159;&#26377;&#26426;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#35782;&#21035;&#20986;&#21512;&#25104;&#29305;&#23450;&#21270;&#21512;&#29289;&#25152;&#38656;&#30340;&#21453;&#24212;&#29289;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#36741;&#21161;&#21512;&#25104;&#35268;&#21010;&#30340;&#20986;&#29616;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20851;&#27880;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#26080;&#27169;&#26495;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;transformer&#32467;&#26500;&#65292;&#24182;&#23558;&#20998;&#23376;&#34920;&#31034;&#20026;ID&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#22312;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#30340;&#24191;&#27867;&#25299;&#25169;&#20449;&#24687;&#21644;&#22312;&#29983;&#25104;&#29289;&#21644;&#21453;&#24212;&#29289;&#20043;&#38388;&#23545;&#40784;&#21407;&#23376;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#23548;&#33268;&#32467;&#26524;&#19981;&#22914;&#21322;&#27169;&#26495;&#27169;&#22411;&#31454;&#20105;&#21147;&#24378;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;Node-Aligned Graph-to-Graph (NAG2G)&#65292;&#20063;&#26159;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#26080;&#27169;&#26495;&#27169;&#22411;&#65292;&#20294;&#26159;&#21033;&#29992;&#20102;2D&#20998;&#23376;&#22270;&#21644;3D&#26500;&#35937;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;n
&lt;/p&gt;
&lt;p&gt;
Single-step retrosynthesis is a crucial task in organic chemistry and drug design, requiring the identification of required reactants to synthesize a specific compound. with the advent of computer-aided synthesis planning, there is growing interest in using machine-learning techniques to facilitate the process. Existing template-free machine learning-based models typically utilize transformer structures and represent molecules as ID sequences. However, these methods often face challenges in fully leveraging the extensive topological information of the molecule and aligning atoms between the production and reactants, leading to results that are not as competitive as those of semi-template models. Our proposed method, Node-Aligned Graph-to-Graph (NAG2G), also serves as a transformer-based template-free model but utilizes 2D molecular graphs and 3D conformation information. Furthermore, our approach simplifies the incorporation of production-reactant atom mapping alignment by leveraging n
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#26377;&#20851;&#30340;&#36951;&#20256;&#21464;&#24322;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#39564;&#35774;&#35745;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#36951;&#20256;&#21644;&#36801;&#31227;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#39118;&#38505;&#20272;&#35745;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10837</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#20026;&#21644;&#36951;&#20256;&#29305;&#24449;&#25972;&#21512;&#25913;&#36827;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#39118;&#38505;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Improving Opioid Use Disorder Risk Modelling through Behavioral and Genetic Feature Integration. (arXiv:2309.10837v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10837
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#26377;&#20851;&#30340;&#36951;&#20256;&#21464;&#24322;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#39564;&#35774;&#35745;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#36951;&#20256;&#21644;&#36801;&#31227;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#39118;&#38505;&#20272;&#35745;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#29255;&#31867;&#33647;&#29289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24613;&#24615;&#21644;&#24930;&#24615;&#30140;&#30171;&#27490;&#30171;&#33647;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#30270;&#39118;&#38505;&#65292;&#23548;&#33268;&#32654;&#22269;&#27599;&#24180;&#25968;&#30334;&#19975;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#65288;OUD&#65289;&#30149;&#20363;&#21644;&#25968;&#19975;&#20154;&#30340;&#36807;&#26089;&#27515;&#20129;&#12290;&#22312;&#22788;&#26041;&#20043;&#21069;&#20272;&#35745;OUD&#39118;&#38505;&#21487;&#20197;&#25913;&#36827;&#27835;&#30103;&#26041;&#26696;&#30340;&#25928;&#26524;&#12289;&#30417;&#27979;&#35745;&#21010;&#21644;&#24178;&#39044;&#31574;&#30053;&#65292;&#20294;&#39118;&#38505;&#20272;&#35745;&#36890;&#24120;&#22522;&#20110;&#33258;&#25105;&#25253;&#21578;&#30340;&#25968;&#25454;&#25110;&#35843;&#26597;&#38382;&#21367;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#23454;&#39564;&#35774;&#35745;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#23558;&#19982;OUD&#30456;&#20851;&#30340;&#36951;&#20256;&#21464;&#24322;&#19982;&#20174;GPS&#21644;Wi-Fi&#26102;&#31354;&#22352;&#26631;&#20013;&#25552;&#21462;&#30340;&#34892;&#20026;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#35780;&#20272;OUD&#39118;&#38505;&#12290;&#30001;&#20110;OUD&#36801;&#31227;&#21644;&#36951;&#20256;&#25968;&#25454;&#19981;&#23384;&#22312;&#20110;&#21516;&#19968;&#20010;&#32676;&#20307;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31639;&#27861;&#26469;(1)&#26681;&#25454;&#32463;&#39564;&#20998;&#24067;&#29983;&#25104;&#36801;&#31227;&#29305;&#24449;&#21644;(2)&#21512;&#25104;&#36801;&#31227;&#21644;&#36951;&#20256;&#26679;&#26412;&#65292;&#20551;&#35774;&#23384;&#22312;&#26576;&#31181;&#20849;&#30149;&#21644;&#30456;&#23545;&#39118;&#38505;&#27700;&#24179;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25972;&#21512;&#36951;&#20256;&#21644;&#36801;&#31227;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#39118;&#38505;&#20272;&#35745;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioids are an effective analgesic for acute and chronic pain, but also carry a considerable risk of addiction leading to millions of opioid use disorder (OUD) cases and tens of thousands of premature deaths in the United States yearly. Estimating OUD risk prior to prescription could improve the efficacy of treatment regimens, monitoring programs, and intervention strategies, but risk estimation is typically based on self-reported data or questionnaires. We develop an experimental design and computational methods that combines genetic variants associated with OUD with behavioral features extracted from GPS and Wi-Fi spatiotemporal coordinates to assess OUD risk. Since both OUD mobility and genetic data do not exist for the same cohort, we develop algorithms to (1) generate mobility features from empirical distributions and (2) synthesize mobility and genetic samples assuming a level of comorbidity and relative risks. We show that integrating genetic and mobility modalities improves ris
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.10275</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#20026;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31354;&#20013;&#32676;&#20307;&#12289;&#33258;&#21160;&#21270;&#20179;&#20648;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#21035;&#65306;&#38598;&#20013;&#24335;&#35268;&#21010;&#21644;&#20998;&#25955;&#24335;&#35268;&#21010;&#12290;&#38598;&#20013;&#24335;&#35268;&#21010;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#22240;&#27492;&#22312;&#22823;&#22411;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#19981;&#20855;&#22791;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20998;&#25955;&#24335;&#35268;&#21010;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#26102;&#36335;&#24452;&#35268;&#21010;&#65292;&#23637;&#31034;&#20102;&#38544;&#24335;&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#23427;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#19988;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRAMP&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#24335;&#35838;&#31243;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#27604;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#36825;&#19968;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.00736</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#20013;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Prediction Error Estimation in Random Forests. (arXiv:2309.00736v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37327;&#21270;&#35780;&#20272;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#27604;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#24182;&#19988;&#36825;&#19968;&#32467;&#26524;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#37327;&#35780;&#20272;&#20102;&#20998;&#31867;&#38543;&#26426;&#26862;&#26519;&#30340;&#35823;&#24046;&#20272;&#35745;&#12290;&#22312;Bates&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24314;&#31435;&#30340;&#21021;&#27493;&#29702;&#35770;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#20174;&#29702;&#35770;&#21644;&#32463;&#39564;&#35282;&#24230;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#24120;&#35265;&#30340;&#21508;&#31181;&#35823;&#24046;&#20272;&#35745;&#26041;&#27861;&#22312;&#30495;&#23454;&#35823;&#24046;&#29575;&#21644;&#26399;&#26395;&#35823;&#24046;&#29575;&#26041;&#38754;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20998;&#31867;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#35823;&#24046;&#20272;&#35745;&#24179;&#22343;&#26356;&#25509;&#36817;&#30495;&#23454;&#35823;&#24046;&#29575;&#65292;&#32780;&#19981;&#26159;&#24179;&#22343;&#39044;&#27979;&#35823;&#24046;&#12290;&#19982;Bates&#31561;&#20154;&#65288;2023&#24180;&#65289;&#23545;&#36923;&#36753;&#22238;&#24402;&#30340;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#36825;&#20010;&#32467;&#26524;&#36866;&#29992;&#20110;&#20132;&#21449;&#39564;&#35777;&#12289;&#33258;&#20030;&#21644;&#25968;&#25454;&#21010;&#20998;&#31561;&#19981;&#21516;&#30340;&#35823;&#24046;&#20272;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, error estimates of classification Random Forests are quantitatively assessed. Based on the initial theoretical framework built by Bates et al. (2023), the true error rate and expected error rate are theoretically and empirically investigated in the context of a variety of error estimation methods common to Random Forests. We show that in the classification case, Random Forests' estimates of prediction error is closer on average to the true error rate instead of the average prediction error. This is opposite the findings of Bates et al. (2023) which were given for logistic regression. We further show that this result holds across different error estimation strategies such as cross-validation, bagging, and data splitting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#26725;&#27169;&#22411;&#30340;&#21453;&#21512;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#31163;&#25955;&#20998;&#24067;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#36817;&#20284;&#65292;&#30452;&#25509;&#29983;&#25104;&#21487;&#33021;&#30340;&#21069;&#20307;&#20998;&#23376;&#65292;&#20026;&#21453;&#21512;&#25104;&#35268;&#21010;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.16212</link><description>&lt;p&gt;
RetroBridge: &#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#26725;&#27169;&#22411;&#36827;&#34892;&#21453;&#21512;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
RetroBridge: Modeling Retrosynthesis with Markov Bridges. (arXiv:2308.16212v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#26725;&#27169;&#22411;&#30340;&#21453;&#21512;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#31163;&#25955;&#20998;&#24067;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#36817;&#20284;&#65292;&#30452;&#25509;&#29983;&#25104;&#21487;&#33021;&#30340;&#21069;&#20307;&#20998;&#23376;&#65292;&#20026;&#21453;&#21512;&#25104;&#35268;&#21010;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21512;&#25104;&#35268;&#21010;&#26159;&#21270;&#23398;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#25361;&#25112;&#65292;&#26088;&#22312;&#20174;&#24066;&#21806;&#36215;&#22987;&#26448;&#26009;&#35774;&#35745;&#21453;&#24212;&#36335;&#24452;&#21040;&#30446;&#26631;&#20998;&#23376;&#12290;&#22810;&#27493;&#21453;&#21512;&#25104;&#35268;&#21010;&#20013;&#30340;&#27599;&#19968;&#27493;&#37117;&#38656;&#35201;&#20934;&#30830;&#39044;&#27979;&#32473;&#23450;&#30446;&#26631;&#20998;&#23376;&#30340;&#21487;&#33021;&#21069;&#20307;&#20998;&#23376;&#65292;&#24182;&#32473;&#20986;&#32622;&#20449;&#24230;&#20272;&#35745;&#20197;&#25351;&#23548;&#21551;&#21457;&#24335;&#25628;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#21333;&#27493;&#21453;&#21512;&#25104;&#35268;&#21010;&#24314;&#27169;&#20026;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39532;&#23572;&#21487;&#22827;&#26725;&#27169;&#22411;&#65292;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#36817;&#20284;&#20004;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#31163;&#25955;&#20998;&#24067;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;&#32806;&#21512;&#25968;&#25454;&#28857;&#36827;&#34892;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#26725;&#30340;&#27010;&#24565;&#65292;&#21363;&#20197;&#20854;&#31471;&#28857;&#20026;&#21021;&#22987;&#28857;&#22266;&#23450;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#19982;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#39532;&#23572;&#21487;&#22827;&#26725;&#27169;&#22411;&#19981;&#38656;&#35201;&#20316;&#20026;&#37319;&#26679;&#20195;&#29702;&#30340;&#21487;&#36861;&#36394;&#22122;&#22768;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20351;&#29992;&#36755;&#20837;&#20135;&#29289;&#20998;&#23376;&#20316;&#20026;&#26469;&#33258;&#38590;&#20197;&#22788;&#29702;&#30340;&#20808;&#39564;&#20998;&#24067;&#30340;&#26679;&#26412;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis planning is a fundamental challenge in chemistry which aims at designing reaction pathways from commercially available starting materials to a target molecule. Each step in multi-step retrosynthesis planning requires accurate prediction of possible precursor molecules given the target molecule and confidence estimates to guide heuristic search algorithms. We model single-step retrosynthesis planning as a distribution learning problem in a discrete state space. First, we introduce the Markov Bridge Model, a generative framework aimed to approximate the dependency between two intractable discrete distributions accessible via a finite sample of coupled data points. Our framework is based on the concept of a Markov bridge, a Markov process pinned at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does not need a tractable noise distribution as a sampling proxy and directly operates on the input product molecules as samples from the intractable prior di
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12243</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#19981;&#21516;&#30340;&#20914;&#31361;&#20248;&#21270;&#20934;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#30340;&#20027;&#20219;&#21153;&#65288;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65289;&#65292;&#20063;&#21487;&#20197;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#21644;&#27425;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#25439;&#22833;&#26368;&#23567;&#21270;&#19982;&#31232;&#30095;&#24615;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#31616;&#21333;&#22320;&#21152;&#26435;&#20934;&#21017;&#65292;&#20294;&#22312;&#20984;&#35774;&#32622;&#20013;&#25165;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#23545;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26631;&#37327;&#21270;&#25216;&#26415;&#65292;&#31639;&#27861;&#21487;&#20197;&#35782;&#21035;&#21407;&#22987;&#38382;&#39064;&#30340;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#23558;&#20854;&#22797;&#26434;&#24615;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22686;&#24191;Lagrangian&#26041;&#27861;&#26469;&#35299;&#20915;&#31616;&#21270;&#21518;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;Adam&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#22788;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#32463;&#27982;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#21644;&#32447;&#24615;&#25506;&#27979;&#19982;&#24494;&#35843;&#30340;&#38598;&#25104;&#65292;&#26377;&#25928;&#20943;&#36731;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07728</link><description>&lt;p&gt;
&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65306;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#21644;&#32447;&#24615;&#25506;&#27979;&#19982;&#24494;&#35843;&#30340;&#38598;&#25104;&#65292;&#26377;&#25928;&#20943;&#36731;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#24050;&#20855;&#22791;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#21457;&#29983;&#30072;&#21464;&#12290;&#22312;&#36866;&#24212;&#26032;&#30446;&#26631;&#39046;&#22495;&#26102;&#20943;&#36731;&#29305;&#24449;&#30072;&#21464;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#23545;&#22836;&#23618;&#36827;&#34892;&#23545;&#40784;&#22788;&#29702;&#21487;&#20197;&#22788;&#29702;&#29305;&#24449;&#30072;&#21464;&#38382;&#39064;&#21462;&#24471;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25209;&#24402;&#19968;&#21270;&#23618;&#30340;&#22788;&#29702;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39046;&#22495;&#24863;&#30693;&#24494;&#35843;&#65288;DAFT&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#12289;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#25209;&#24402;&#19968;&#21270;&#36716;&#25442;&#26041;&#27861;&#36890;&#36807;&#20943;&#23569;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#20462;&#25913;&#26469;&#26377;&#25928;&#20943;&#36731;&#29305;&#24449;&#30072;&#21464;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integrati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#21152;&#36895;&#36816;&#21160;&#35268;&#21010;&#20248;&#21270;&#36807;&#31243;&#12290;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#24182;&#21487;&#20197;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2308.01557</link><description>&lt;p&gt;
&#36816;&#21160;&#35268;&#21010;&#25193;&#25955;&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#23398;&#20064;&#19982;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#20197;&#21152;&#36895;&#36816;&#21160;&#35268;&#21010;&#20248;&#21270;&#36807;&#31243;&#12290;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#24182;&#21487;&#20197;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36712;&#36857;&#20998;&#24067;&#30340;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#21152;&#24555;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#20248;&#21270;&#12290;&#22312;&#32473;&#23450;&#20808;&#21069;&#25104;&#21151;&#30340;&#35268;&#21010;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#36712;&#36857;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#26032;&#35268;&#21010;&#38382;&#39064;&#30340;&#20808;&#39564;&#30693;&#35782;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#36825;&#31181;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#24341;&#23548;&#30340;&#26041;&#27861;&#12290;&#21487;&#20197;&#36890;&#36807;&#20174;&#20808;&#39564;&#30693;&#35782;&#20013;&#37319;&#26679;&#21021;&#22987;&#21270;&#65292;&#25110;&#32773;&#22312;&#26368;&#22823;&#21518;&#39564;&#20248;&#21270;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#20808;&#39564;&#20998;&#24067;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#21435;&#22122;&#36807;&#31243;&#65292;&#30452;&#25509;&#20174;&#20219;&#21153;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#21518;&#39564;&#36712;&#36857;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#24615;&#65292;&#36825;&#23545;&#20110;&#22823;&#37327;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#38750;&#24120;&#36866;&#29992;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;-&#36816;&#21160;&#35268;&#21010;&#25193;&#25955;&#19982;&#20960;&#31181;&#22522;&#20934;&#26041;&#21457;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several ba
&lt;/p&gt;</description></item><item><title>RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15909</link><description>&lt;p&gt;
RL$^3$:&#36890;&#36807;RL&#20869;&#37096;&#30340;RL$^2$&#25552;&#21319;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15909
&lt;/p&gt;
&lt;p&gt;
RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#26041;&#27861;&#65292;&#22914;RL$^2$&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#38024;&#23545;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#20219;&#21153;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32463;&#39564;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#33324;&#30340;&#24378;&#21270;&#23398;&#20064;&#32452;&#20214;&#65292;&#20363;&#22914;&#20215;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;transformers&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#21464;&#24471;&#31105;&#27490;&#20043;&#21069;&#20063;&#23545;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25512;&#29702;&#30340;&#21382;&#21490;&#38271;&#24230;&#26377;&#23454;&#38469;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#20204;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL$^3$&#65292;&#19968;&#31181;&#32452;&#21512;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#36807;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
&lt;/p&gt;</description></item><item><title>Omega&#26159;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#21382;&#21490;&#26799;&#24230;EMA&#26469;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#24182;&#22312;&#38543;&#26426;&#28216;&#25103;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.07905</link><description>&lt;p&gt;
Omega: &#20048;&#35266;EMA Gradients
&lt;/p&gt;
&lt;p&gt;
Omega: Optimistic EMA Gradients. (arXiv:2306.07905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07905
&lt;/p&gt;
&lt;p&gt;
Omega&#26159;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#21382;&#21490;&#26799;&#24230;EMA&#26469;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#24182;&#22312;&#38543;&#26426;&#28216;&#25103;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GAN&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#36827;&#27493;&#65292;&#38543;&#26426;min-max&#20248;&#21270;&#21463;&#21040;&#20102;&#26426;&#22120;&#23398;&#20064;&#30028;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#30830;&#23450;&#24615;&#29366;&#24577;&#19979;&#30340;&#21338;&#24328;&#20248;&#21270;&#24050;&#32463;&#30456;&#24403;&#22909;&#22320;&#29702;&#35299;&#20102;&#65292;&#20294;&#22312;&#38543;&#26426;&#29366;&#24577;&#19979;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;&#20048;&#35266;&#26799;&#24230;&#36825;&#26679;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#26041;&#27861;&#23545;&#22122;&#22768;&#38750;&#24120;&#25935;&#24863;&#25110;&#32773;&#20250;&#23548;&#33268;&#22833;&#36133;&#12290;&#34429;&#28982;&#23384;&#22312;&#26367;&#20195;&#31574;&#30053;&#65292;&#20294;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Omega&#65292;&#19968;&#31181;&#20855;&#26377;&#31867;&#20284;&#20110;&#20048;&#35266;&#26356;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20854;&#26356;&#26032;&#35268;&#21017;&#20013;&#21512;&#24182;&#21382;&#21490;&#26799;&#24230;&#30340;EMA&#26469;&#20943;&#36731;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#21253;&#21547;&#21160;&#37327;&#30340;&#35813;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#34429;&#28982;&#25105;&#20204;&#27809;&#26377;&#25552;&#20379;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#20294;&#25105;&#20204;&#22312;&#38543;&#26426;&#28216;&#25103;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#24212;&#29992;&#20110;&#32447;&#24615;&#29609;&#23478;&#26102;&#65292;Omega&#20248;&#20110;&#20048;&#35266;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic min-max optimization has gained interest in the machine learning community with the advancements in GANs and adversarial training. Although game optimization is fairly well understood in the deterministic setting, some issues persist in the stochastic regime. Recent work has shown that stochastic gradient descent-ascent methods such as the optimistic gradient are highly sensitive to noise or can fail to converge. Although alternative strategies exist, they can be prohibitively expensive. We introduce Omega, a method with optimistic-like updates that mitigates the impact of noise by incorporating an EMA of historic gradients in its update rule. We also explore a variation of this algorithm that incorporates momentum. Although we do not provide convergence guarantees, our experiments on stochastic games show that Omega outperforms the optimistic gradient method when applied to linear players.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#24182;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.06208</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#24182;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#21487;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#38656;&#35201;&#24040;&#22823;&#30340;&#22788;&#29702;&#33021;&#21147;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;GPU&#21644;TPU&#31561;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#24555;&#36895;&#12289;&#21450;&#26102;&#30340;&#22788;&#29702;&#12290;&#22312;&#27169;&#22411;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#23376;&#20248;&#26144;&#23556;&#21487;&#33021;&#20250;&#23548;&#33268;&#23454;&#26102;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#22833;&#36133;&#65292;&#20174;&#32780;&#23548;&#33268;&#26102;&#38388;&#19981;&#30830;&#23450;&#24615;&#21644;&#38169;&#35823;&#34892;&#20026;&#12290;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#26144;&#23556;&#26159;&#36890;&#36807;&#22810;&#20010;&#36719;&#20214;&#32452;&#20214;&#36827;&#34892;&#30340;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12289;&#32534;&#35793;&#22120;&#12289;&#35774;&#22791;&#24211;&#31561;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#35745;&#31639;&#29615;&#22659;&#12290;&#38543;&#30528;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#21307;&#30103;&#25104;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#22686;&#21152;&#65292;&#35780;&#20272;&#23427;&#20204;&#23545;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12289;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#30828;&#20214;&#35774;&#22791;&#31561;&#21442;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#27491;&#30830;&#24615;&#30340;&#24433;&#21709;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#23545;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#19968;&#32452;&#21442;&#32771;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26356;&#25913;&#36719;&#20214;&#32452;&#20214;&#26469;&#25200;&#21160;&#35745;&#31639;&#29615;&#22659;&#65292;&#29983;&#25104;&#20855;&#26377;&#24050;&#30693;&#39044;&#27979;&#36755;&#20986;&#24046;&#24322;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#27604;&#36739;&#21407;&#22987;&#22270;&#20687;&#21644;&#25200;&#21160;&#22270;&#20687;&#30340;&#39044;&#27979;&#36755;&#20986;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21463;&#21040;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#35797;&#19977;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#26469;&#35777;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#20854;&#22312;&#35745;&#31639;&#29615;&#22659;&#21464;&#21270;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to sub-optimal mapping on hardware accelerators during model deployment, which may lead to timing uncertainty and erroneous behavior. Mapping on hardware accelerators is done through multiple software components like deep learning frameworks, compilers, device libraries, that we refer to as the computational environment. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess their robustness to changes in the computational environment, as the impact of parameters like deep learning frameworks, compiler optimizations, and hardware devices on model performance and correctness is not well understood.  In this paper we present a differential testing framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20013;&#20986;&#29616;&#30340;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#22810;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#36328;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#36716;&#25442;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.06157</link><description>&lt;p&gt;
&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#26694;&#26550;&#36716;&#25442;&#30340;&#25925;&#38556;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Fault Localization for Framework Conversions of Image Recognition Models. (arXiv:2306.06157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20013;&#20986;&#29616;&#30340;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;&#25104;&#21151;&#20462;&#22797;&#22810;&#20010;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#36328;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#36716;&#25442;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#26102;&#65292;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#23558;&#27169;&#22411;&#20174;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#65288;&#20363;&#22914;&#65292;&#20174;TensorFlow&#21040;PyTorch&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#20986;&#38169;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#30446;&#26631;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#30830;&#23450;&#36825;&#31181;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#25105;&#20204;&#23545;&#19977;&#20010;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;DNNs&#65288;MobileNetV2&#12289;ResNet101&#21644;InceptionV3&#65289;&#36827;&#34892;&#20102;&#19981;&#21516;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;PyTorch&#12289;Keras&#12289;TensorFlow&#65288;TF&#65289;&#21644;TFLite&#65289;&#20043;&#38388;&#36827;&#34892;&#20102;&#36716;&#25442;&#65292;&#24182;&#21457;&#29616;&#20102;&#35768;&#22810;&#27169;&#22411;&#23849;&#28291;&#21644;&#36755;&#20986;&#26631;&#31614;&#24046;&#24322;&#39640;&#36798;100&#65285;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23450;&#20301;&#25925;&#38556;&#21644;&#20462;&#22797;&#26377;&#32570;&#38519;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#36716;&#25442;&#65292;&#37325;&#28857;&#25918;&#22312;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#20998;&#26512;&#38454;&#27573;&#65306;1&#65289;&#36716;&#25442;&#24037;&#20855;&#65292;2&#65289;&#27169;&#22411;&#21442;&#25968;&#65292;3&#65289;&#27169;&#22411;&#36229;&#21442;&#25968;&#65292;4&#65289;&#22270;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#38024;&#23545;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#36716;&#25442;&#24037;&#20855;&#30340;&#25512;&#33616;&#12289;&#35843;&#35797;&#25216;&#24039;&#20197;&#21450;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#25104;&#21151;&#20462;&#22797;&#25152;&#26377;&#27979;&#35797;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;MobileNetV2&#65292;ResNet101&#21644;InceptionV3 &#30340;&#26377;&#32570;&#38519;&#30340;&#36716;&#25442;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs used for image recognition (MobileNetV2, ResNet101, and InceptionV3), converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 100%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four primary stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose a number of strategies towards faul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.00038</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;(FedCSD)
&lt;/p&gt;
&lt;p&gt;
FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#23454;&#39564;&#20013;&#20998;&#21035;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#24182;&#19988;&#27604;&#38598;&#20013;&#24335;&#21644;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCSD&#30340;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#35753;&#32452;&#32455;&#21327;&#20316;&#35757;&#32451;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#19977;&#20010;&#23454;&#39564;&#26469;&#25903;&#25345;&#36825;&#20123;&#26029;&#35328;&#65292;&#36825;&#20123;&#23454;&#39564;&#21033;&#29992;&#20102;&#19977;&#20010;&#25163;&#21160;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#26816;&#27979;&#21644;&#30740;&#31350;&#19981;&#21516;&#30340;&#20195;&#30721;&#24322;&#21619;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$K^2$-&#26641;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32039;&#20945;&#29983;&#25104;&#65292;&#24182;&#21516;&#26102;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#36890;&#36807;&#25552;&#20986;&#39034;&#24207;$K^2$-&#26641;&#34920;&#31034;&#21644;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19125</link><description>&lt;p&gt;
&#22522;&#20110;$K^2$-&#26641;&#30340;&#20998;&#32423;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Graph Generation with $K^2$-trees. (arXiv:2305.19125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;$K^2$-&#26641;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32039;&#20945;&#29983;&#25104;&#65292;&#24182;&#21516;&#26102;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#36890;&#36807;&#25552;&#20986;&#39034;&#24207;$K^2$-&#26641;&#34920;&#31034;&#21644;&#24341;&#20837;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26412;&#25991;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30446;&#26631;&#20998;&#24067;&#29983;&#25104;&#22270;&#26159;&#35768;&#22810;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21253;&#25324;&#33647;&#29289;&#21457;&#29616;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26080;&#25439;&#22270;&#21387;&#32553;&#30340;$K^2$-&#26641;&#34920;&#31034;&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;$K^2$-&#26641;&#33021;&#22815;&#22312;&#36827;&#34892;&#32039;&#20945;&#29983;&#25104;&#30340;&#21516;&#26102;&#65292;&#25429;&#33719;&#22270;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;(1)&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#21098;&#26525;&#12289;&#25153;&#24179;&#21270;&#21644;&#35760;&#21495;&#21270;&#36807;&#31243;&#30340;&#39034;&#24207;K2&#26641;&#34920;&#31034;&#21644;(2)&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#19987;&#19994;&#26641;&#24418;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26469;&#29983;&#25104;&#24207;&#21015;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;&#24120;&#35268;&#21644;&#20004;&#20010;&#20998;&#23376;&#22270;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#20197;&#35777;&#23454;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#65292;&#29992;&#20110;&#24494;&#35843;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#21512;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;UCF-101&#21644;MSR-VTT&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10474</link><description>&lt;p&gt;
&#20445;&#30041;&#20320;&#33258;&#24049;&#30340;&#30456;&#20851;&#24615;&#65306;&#29992;&#20110;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models. (arXiv:2305.10474v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#65292;&#29992;&#20110;&#24494;&#35843;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#21512;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;UCF-101&#21644;MSR-VTT&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#21512;&#25104;&#36830;&#32493;&#30340;&#21160;&#30011;&#24103;&#65292;&#26082;&#20855;&#26377;&#20809;&#30495;&#23454;&#24863;&#65292;&#21448;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#24615;&#20173;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#22312;&#21487;&#20197;&#20351;&#29992;&#25104;&#20159;&#32423;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#65292;&#25910;&#38598;&#30456;&#20284;&#35268;&#27169;&#30340;&#35270;&#39057;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#22270;&#20687;&#23545;&#24212;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#35757;&#32451;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#35745;&#31639;&#20195;&#20215;&#26356;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#35270;&#39057;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#35270;&#39057;&#21512;&#25104;&#20219;&#21153;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35270;&#39057;&#25193;&#25955;&#20013;&#22825;&#30495;&#22320;&#23558;&#22270;&#20687;&#22122;&#22768;&#20808;&#39564;&#25193;&#23637;&#20026;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#65292;&#20854;&#22312;&#35270;&#39057;&#25193;&#25955;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#26356;&#22909;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411; Preserve Your Own Correlation (PYoCo) &#22312; UCF-101 &#21644; MSR-VTT &#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#23545;&#35270;&#39057;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and temporally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still challenging. Also, training a video diffusion model is computationally much more expensive than its image counterpart. In this work, we explore finetuning a pretrained image diffusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffusion leads to sub-optimal performance. Our carefully designed video noise prior leads to substantially better performance. Extensive experimental validation shows that our model, Preserve Your Own Correlation (PYoCo), attains SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It also
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20250;&#35805;&#26641;&#25628;&#32034;(CTS)&#65292;&#23427;&#21487;&#20197;&#26550;&#36215;FAQ&#21644;&#23545;&#35805;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#23450;&#20041;&#23545;&#35805;&#26641;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#21482;&#23398;&#20064;&#25552;&#20986;&#23548;&#33322;&#29992;&#25143;&#36798;&#21040;&#30446;&#26631;&#25152;&#38656;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10227</link><description>&lt;p&gt;
&#20250;&#35805;&#26641;&#25628;&#32034;&#65306;&#19968;&#39033;&#26032;&#30340;&#28151;&#21512;&#23545;&#35805;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Conversational Tree Search: A New Hybrid Dialog Task. (arXiv:2303.10227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20250;&#35805;&#26641;&#25628;&#32034;(CTS)&#65292;&#23427;&#21487;&#20197;&#26550;&#36215;FAQ&#21644;&#23545;&#35805;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#23450;&#20041;&#23545;&#35805;&#26641;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#21482;&#23398;&#20064;&#25552;&#20986;&#23548;&#33322;&#29992;&#25143;&#36798;&#21040;&#30446;&#26631;&#25152;&#38656;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25509;&#21475;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#21644;&#26041;&#20415;&#30340;&#26041;&#24335;&#65292;&#35753;&#29992;&#25143;&#33719;&#21462;&#21407;&#26412;&#21487;&#33021;&#38590;&#20197;&#25110;&#19981;&#26041;&#20415;&#33719;&#24471;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30028;&#38754;&#22823;&#20307;&#19978;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;FAQ&#65292;&#29992;&#25143;&#24517;&#39035;&#25552;&#20986;&#26126;&#30830;&#30340;&#38382;&#39064;&#20197;&#26816;&#32034;&#19968;&#33324;&#30340;&#31572;&#26696;&#65307;&#25110;&#32773;&#23545;&#35805;&#65292;&#29992;&#25143;&#24517;&#39035;&#36981;&#24490;&#39044;&#23450;&#20041;&#30340;&#36335;&#24452;&#20294;&#21487;&#33021;&#20250;&#25509;&#25910;&#21040;&#20010;&#24615;&#21270;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#20250;&#35805;&#26641;&#25628;&#32034;(CTS)&#65292;&#23427;&#26550;&#36215;&#20102;&#20449;&#24687;&#26816;&#32034;&#39118;&#26684;FAQ&#21644;&#38754;&#21521;&#20219;&#21153;&#23545;&#35805;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#23450;&#20041;&#23545;&#35805;&#26641;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#35805;&#31574;&#30053;&#65292;&#21482;&#23398;&#20064;&#25552;&#20986;&#23548;&#33322;&#29992;&#25143;&#36798;&#21040;&#30446;&#26631;&#25152;&#38656;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26053;&#34892;&#25253;&#38144;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#22522;&#32447;&#65288;baseline&#65289;&#20197;&#21450;&#19968;&#39033;&#26032;&#39062;&#30340;&#28145;&#24230;&#22686;&#24378;&#23398;&#20064;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26032;&#30340;&#26550;&#26500;&#32508;&#21512;&#20102;FAQ&#21644;&#23545;&#35805;&#30340;&#20248;&#28857;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational interfaces provide a flexible and easy way for users to seek information that may otherwise be difficult or inconvenient to obtain. However, existing interfaces generally fall into one of two categories: FAQs, where users must have a concrete question in order to retrieve a general answer, or dialogs, where users must follow a predefined path but may receive a personalized answer. In this paper, we introduce Conversational Tree Search (CTS) as a new task that bridges the gap between FAQ-style information retrieval and task-oriented dialog, allowing domain-experts to define dialog trees which can then be converted to an efficient dialog policy that learns only to ask the questions necessary to navigate a user to their goal. We collect a dataset for the travel reimbursement domain and demonstrate a baseline as well as a novel deep Reinforcement Learning architecture for this task. Our results show that the new architecture combines the positive aspects of both the FAQ and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#19982;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25512;&#23548;&#20102;&#29992;&#20110;&#25511;&#21046;&#28508;&#22312;SDE&#36793;&#38469;&#23494;&#24230;&#28436;&#21270;&#30340;&#27721;&#23494;&#23572;&#39039;-&#38597;&#21487;&#27604;-&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#24182;&#23558;&#29983;&#25104;&#24314;&#27169;&#34920;&#36848;&#20026;&#23545;&#21512;&#36866;&#24230;&#37327;&#20043;&#38388;Kullback-Leibler&#25955;&#24230;&#30340;&#26368;&#23567;&#21270;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26041;&#27861;&#29992;&#20110;&#37319;&#26679;&#38750;&#24402;&#19968;&#21270;&#23494;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.01364</link><description>&lt;p&gt;
&#23545;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20248;&#25511;&#21046;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An optimal control perspective on diffusion-based generative modeling. (arXiv:2211.01364v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#19982;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25512;&#23548;&#20102;&#29992;&#20110;&#25511;&#21046;&#28508;&#22312;SDE&#36793;&#38469;&#23494;&#24230;&#28436;&#21270;&#30340;&#27721;&#23494;&#23572;&#39039;-&#38597;&#21487;&#27604;-&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#24182;&#23558;&#29983;&#25104;&#24314;&#27169;&#34920;&#36848;&#20026;&#23545;&#21512;&#36866;&#24230;&#37327;&#20043;&#38388;Kullback-Leibler&#25955;&#24230;&#30340;&#26368;&#23567;&#21270;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26041;&#27861;&#29992;&#20110;&#37319;&#26679;&#38750;&#24402;&#19968;&#21270;&#23494;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#19982;&#22522;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20363;&#22914;&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#27721;&#23494;&#23572;&#39039;-&#38597;&#21487;&#27604;-&#36125;&#23572;&#26364;&#26041;&#31243;&#65292;&#29992;&#20110;&#25511;&#21046;&#28508;&#22312;&#30340;SDE&#36793;&#38469;&#23494;&#24230;&#30340;&#28436;&#21270;&#12290;&#36825;&#20010;&#35270;&#35282;&#20801;&#35768;&#23558;&#26368;&#20248;&#25511;&#21046;&#29702;&#35770;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20013;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35777;&#25454;&#19979;&#30028;&#26159;&#25511;&#21046;&#29702;&#35770;&#20013;&#24191;&#20026;&#20154;&#30693;&#30340;&#39564;&#35777;&#23450;&#29702;&#30340;&#30452;&#25509;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24314;&#27169;&#34920;&#36848;&#20026;&#36335;&#24452;&#31354;&#38388;&#20013;&#21512;&#36866;&#24230;&#37327;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#26368;&#23567;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#38750;&#24402;&#19968;&#21270;&#23494;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#26032;&#22411;&#25193;&#25955;&#26041;&#27861;&#65292;&#36825;&#22312;&#32479;&#35745;&#23398;&#21644;&#35745;&#31639;&#31185;&#23398;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26102;&#24207;&#21453;&#21521;&#25193;&#25955;&#37319;&#26679;&#22120;&#65288;DIS&#65289;&#21487;&#20197;&#32988;&#36807;&#20854;&#20182;&#22522;&#20110;&#25193;&#25955;&#30340;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a connection between stochastic optimal control and generative models based on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic models. In particular, we derive a Hamilton-Jacobi-Bellman equation that governs the evolution of the log-densities of the underlying SDE marginals. This perspective allows to transfer methods from optimal control theory to generative modeling. First, we show that the evidence lower bound is a direct consequence of the well-known verification theorem from control theory. Further, we can formulate diffusion-based generative modeling as a minimization of the Kullback-Leibler divergence between suitable measures in path space. Finally, we develop a novel diffusion-based method for sampling from unnormalized densities -- a problem frequently occurring in statistics and computational sciences. We demonstrate that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling approache
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35266;&#28857;&#24066;&#22330;&#27169;&#22411;&#65288;OMM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#31215;&#26497;&#24178;&#39044;&#26469;&#36943;&#21046;&#26497;&#21491;&#27966;&#35266;&#28857;&#30340;&#20256;&#25773;&#12290;&#36825;&#20010;&#27169;&#22411;&#23558;&#35266;&#28857;&#30340;&#20851;&#27880;&#24066;&#22330;&#35268;&#27169;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#31454;&#20105;&#65292;&#26088;&#22312;&#35780;&#20272;&#31215;&#26497;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.06620</link><description>&lt;p&gt;
&#35266;&#28857;&#24066;&#22330;&#27169;&#22411;&#65306;&#21033;&#29992;&#31215;&#26497;&#24178;&#39044;&#26469;&#36943;&#21046;&#26497;&#21491;&#27966;&#35266;&#28857;&#30340;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions. (arXiv:2208.06620v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35266;&#28857;&#24066;&#22330;&#27169;&#22411;&#65288;OMM&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#31215;&#26497;&#24178;&#39044;&#26469;&#36943;&#21046;&#26497;&#21491;&#27966;&#35266;&#28857;&#30340;&#20256;&#25773;&#12290;&#36825;&#20010;&#27169;&#22411;&#23558;&#35266;&#28857;&#30340;&#20851;&#27880;&#24066;&#22330;&#35268;&#27169;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#20102;&#35266;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#31454;&#20105;&#65292;&#26088;&#22312;&#35780;&#20272;&#31215;&#26497;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#26497;&#31471;&#20027;&#20041;&#20855;&#26377;&#20005;&#37325;&#30340;&#31038;&#20250;&#21518;&#26524;&#65292;&#21253;&#25324;&#23558;&#20167;&#24680;&#35328;&#35770;&#21512;&#29702;&#21270;&#12289;&#29992;&#25143;&#30340;&#28608;&#36827;&#21270;&#20197;&#21450;&#31038;&#20250;&#20998;&#35010;&#30340;&#21152;&#21095;&#12290;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#32531;&#35299;&#31574;&#30053;&#26469;&#24212;&#23545;&#36825;&#20123;&#21518;&#26524;&#12290;&#20854;&#20013;&#19968;&#31181;&#31574;&#30053;&#20351;&#29992;&#31215;&#26497;&#24178;&#39044;&#65306;&#25511;&#21046;&#20449;&#21495;&#26469;&#22686;&#21152;&#23545;&#35266;&#28857;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#27880;&#65292;&#20197;&#25552;&#21319;&#26576;&#20123;&#35266;&#28857;&#12290;&#20026;&#20102;&#35780;&#20272;&#31215;&#26497;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35266;&#28857;&#24066;&#22330;&#27169;&#22411;&#65288;OMM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#35266;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#31215;&#26497;&#24178;&#39044;&#20316;&#29992;&#30340;&#20004;&#23618;&#22312;&#32447;&#35266;&#28857;&#29983;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#35266;&#28857;&#30340;&#20851;&#27880;&#24066;&#22330;&#35268;&#27169;&#20351;&#29992;&#22810;&#20803;&#31163;&#25955;&#26102;&#38388;Hawkes&#36807;&#31243;&#22312;&#31532;&#19968;&#23618;&#36827;&#34892;&#24314;&#27169;&#65307;&#22312;&#31532;&#20108;&#23618;&#20013;&#65292;&#35266;&#28857;&#22312;&#26377;&#38480;&#30340;&#20851;&#27880;&#24230;&#19979;&#21512;&#20316;&#21644;&#31454;&#20105;&#20197;&#33719;&#24471;&#24066;&#22330;&#20221;&#39069;&#65292;&#20351;&#29992;&#24066;&#22330;&#20221;&#39069;&#21560;&#24341;&#27169;&#22411;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20272;&#35745;&#26041;&#26696;&#30340;&#25910;&#25947;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;OMM
&lt;/p&gt;
&lt;p&gt;
Online extremism has severe societal consequences, including normalizing hate speech, user radicalization, and increased social divisions. Various mitigation strategies have been explored to address these consequences. One such strategy uses positive interventions: controlled signals that add attention to the opinion ecosystem to boost certain opinions. To evaluate the effectiveness of positive interventions, we introduce the Opinion Market Model (OMM), a two-tier online opinion ecosystem model that considers both inter-opinion interactions and the role of positive interventions. The size of the opinion attention market is modeled in the first tier using the multivariate discrete-time Hawkes process; in the second tier, opinions cooperate and compete for market share, given limited attention using the market share attraction model. We demonstrate the convergence of our proposed estimation scheme on a synthetic dataset. Next, we test OMM on two learning tasks, applying to two real-world
&lt;/p&gt;</description></item><item><title>Han&#23618;&#26159;&#19968;&#31181;&#26799;&#24230;&#31283;&#23450;&#12289;&#21442;&#25968;&#26356;&#23569;&#30340;&#31070;&#32463;&#23618;&#32467;&#26500;&#65292;&#21487;&#20197;&#26367;&#25442;&#20840;&#36830;&#25509;&#23618;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2106.04088</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#32423;&#19988;&#26799;&#24230;&#31283;&#23450;&#30340;&#31070;&#32463;&#23618;
&lt;/p&gt;
&lt;p&gt;
A Lightweight and Gradient-Stable Nerual Layer. (arXiv:2106.04088v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.04088
&lt;/p&gt;
&lt;p&gt;
Han&#23618;&#26159;&#19968;&#31181;&#26799;&#24230;&#31283;&#23450;&#12289;&#21442;&#25968;&#26356;&#23569;&#30340;&#31070;&#32463;&#23618;&#32467;&#26500;&#65292;&#21487;&#20197;&#26367;&#25442;&#20840;&#36830;&#25509;&#23618;&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Householder&#26435;&#37325;&#21644;&#32477;&#23545;&#20540;&#28608;&#27963;&#30340;&#31070;&#32463;&#23618;&#32467;&#26500;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;Householder-absolute&#31070;&#32463;&#23618;&#25110;&#31616;&#31216;Han&#23618;&#12290;&#19982;&#20855;&#26377;$d$&#20010;&#31070;&#32463;&#20803;&#21644;$d$&#20010;&#36755;&#20986;&#30340;&#20840;&#36830;&#25509;&#23618;&#30456;&#27604;&#65292;Han&#23618;&#23558;&#21442;&#25968;&#25968;&#37327;&#21644;&#30456;&#24212;&#30340;&#22797;&#26434;&#24230;&#20174;$O&#65288;d ^ 2&#65289;$&#38477;&#20302;&#21040;$O&#65288;d&#65289;$&#12290;Han&#23618;&#32467;&#26500;&#20445;&#35777;&#20102;&#20004;&#20010;&#29702;&#24819;&#23646;&#24615;&#65306;&#65288;1&#65289;&#26799;&#24230;&#31283;&#23450;&#24615;&#65288;&#19981;&#20250;&#20986;&#29616;&#26799;&#24230;&#28040;&#22833;&#25110;&#26799;&#24230;&#29190;&#28856;&#65289;&#65292;&#20197;&#21450;&#65288;2&#65289;1-Lipschitz&#36830;&#32493;&#24615;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#31574;&#30053;&#22320;&#20351;&#29992;Han&#23618;&#26367;&#25442;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#65292;&#20174;&#32780;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#29978;&#33267;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;Han&#23618;&#32467;&#26500;&#22312;&#19968;&#20123;&#23567;&#22411;&#21270;&#30340;&#27169;&#22411;&#19978;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#35752;&#35770;&#20854;&#24403;&#21069;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a neural-layer architecture based on Householder weighting and absolute-value activating, hence called Householder-absolute neural layer or simply Han-layer. Compared to a fully-connected layer with $d$-neurons and $d$ outputs, a Han-layer reduces the number of parameters and the corresponding complexity from $O(d^2)$ to $O(d)$. The Han-layer structure guarantees two desirable properties: (1) gradient stability (free of vanishing or exploding gradient), and (2) 1-Lipschitz continuity. Extensive numerical experiments show that one can strategically use Han-layers to replace fully-connected (FC) layers, reducing the number of model parameters while maintaining or even improving the generalization performance. We will showcase the capabilities of the Han-layer architecture on a few small stylized models, and also discuss its current limitations.
&lt;/p&gt;</description></item></channel></rss>