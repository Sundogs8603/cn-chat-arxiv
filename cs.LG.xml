<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16184</link><description>&lt;p&gt;
&#20851;&#20110;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#35821;&#20041;&#23398;&#65306;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Semantics of LM Latent Space: A Vocabulary-defined Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#25913;&#36827;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20998;&#26512;&#24448;&#24448;&#22312;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#23545;LM&#35821;&#20041;&#30340;&#20998;&#31163;&#27934;&#23519;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#24573;&#35270;&#20102;LM&#36866;&#24212;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#21709;&#24212;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#65292;&#23427;&#22312;LM&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20132;&#32455;&#20998;&#26512;&#65292;&#21033;&#29992;LM&#35789;&#27719;&#26469;&#33719;&#24471;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#65292;&#24378;&#35843;&#21487;&#24494;&#20998;&#24615;&#21644;&#23616;&#37096;&#31561;&#36317;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#36827;&#34892;&#35821;&#20041;&#26657;&#20934;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25991;&#26412;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#31639;&#23376;&#23398;&#20064;&#25361;&#25112;&#30340;&#26032;&#22411;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#21333;&#20010;&#31070;&#32463;&#31639;&#23376;&#22788;&#29702;&#22810;&#31639;&#23376;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#24179;&#22343;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.02892</link><description>&lt;p&gt;
MODNO: &#20855;&#26377;&#20998;&#24067;&#24335;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MODNO: Multi Operator Learning With Distributed Neural Operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#31639;&#23376;&#23398;&#20064;&#25361;&#25112;&#30340;&#26032;&#22411;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;&#21333;&#20010;&#31070;&#32463;&#31639;&#23376;&#22788;&#29702;&#22810;&#31639;&#23376;&#23398;&#20064;&#38382;&#39064;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#24179;&#22343;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#30740;&#31350;&#28041;&#21450;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#31639;&#23376;&#12290;&#20256;&#32479;&#19978;&#65292;&#37325;&#28857;&#25918;&#22312;&#21333;&#31639;&#23376;&#23398;&#20064;&#65288;SOL&#65289;&#19978;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#36805;&#36895;&#23558;&#20854;&#25193;&#23637;&#21040;&#21253;&#21547;&#20351;&#29992;&#20855;&#26377;&#25968;&#30334;&#19975;&#25110;&#25968;&#21313;&#20159;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#36924;&#36817;&#22810;&#31639;&#23376;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#22810;&#31639;&#23376;&#23398;&#20064;&#65288;MOL&#65289;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#21333;&#20010;&#31070;&#32463;&#31639;&#23376;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#31639;&#23376;&#23398;&#20064;&#25361;&#25112;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#24179;&#22343;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#20284;Chen-Chen&#22411;&#31070;&#32463;&#31639;&#23376;&#65292;&#22914;&#28145;&#31639;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;DON&#65289;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#29420;&#31435;&#23398;&#20064;&#27599;&#20010;&#31639;&#23376;&#30340;&#36755;&#20986;&#22522;&#20989;&#25968;&#65292;&#20351;&#29992;&#20854;&#19987;&#29992;&#25968;&#25454;&#65292;&#21516;&#26102;&#38598;&#20013;&#23398;&#20064;&#36755;&#20837;fu&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02892v1 Announce Type: new  Abstract: The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input fu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21040;&#26368;&#21518;&#23618;&#30340;&#28608;&#27963;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#20351;&#29992;HCR&#30028;&#38480;&#21487;&#37327;&#21270;&#20445;&#25252;&#26426;&#23494;&#24615;&#30340;&#21487;&#20449;&#24230;</title><link>https://arxiv.org/abs/2404.02866</link><description>&lt;p&gt;
&#36890;&#36807;Hammersley-Chapman-Robbins&#30028;&#38480;&#20445;&#35777;&#26426;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02866
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#21040;&#26368;&#21518;&#23618;&#30340;&#28608;&#27963;&#26469;&#20445;&#25252;&#38544;&#31169;&#65292;&#20351;&#29992;HCR&#30028;&#38480;&#21487;&#37327;&#21270;&#20445;&#25252;&#26426;&#23494;&#24615;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#36807;&#31243;&#20013;&#36890;&#36807;&#21521;&#26368;&#21518;&#20960;&#23618;&#30340;&#28608;&#27963;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#25252;&#38544;&#31169;&#26159;&#21487;&#33021;&#30340;&#12290;&#36825;&#20123;&#23618;&#20013;&#30340;&#28608;&#27963;&#34987;&#31216;&#20026;&#8220;&#29305;&#24449;&#8221;&#65288;&#23569;&#35265;&#30340;&#31216;&#20026;&#8220;&#23884;&#20837;&#8221;&#25110;&#8220;&#29305;&#24449;&#23884;&#20837;&#8221;&#65289;&#12290;&#28155;&#21152;&#30340;&#22122;&#22768;&#26377;&#21161;&#20110;&#38450;&#27490;&#20174;&#22024;&#26434;&#30340;&#29305;&#24449;&#20013;&#37325;&#24314;&#36755;&#20837;&#12290;&#36890;&#36807;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#26080;&#20559;&#20272;&#35745;&#37327;&#30340;&#26041;&#24046;&#36827;&#34892;&#19979;&#38480;&#20272;&#35745;&#65292;&#37327;&#21270;&#20102;&#30001;&#27492;&#28155;&#21152;&#30340;&#22122;&#22768;&#20135;&#29983;&#30340;&#26426;&#23494;&#24615;&#12290;&#32463;&#20856;&#19981;&#31561;&#24335;Hammersley&#21644;Chapman&#20197;&#21450;Robbins&#25552;&#20379;&#20415;&#21033;&#30340;&#12289;&#21487;&#35745;&#31639;&#30340;&#30028;&#38480;-- HCR&#30028;&#38480;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#20110;&#21253;&#21547;10&#20010;&#31867;&#21035;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#8220;MNIST&#8221;&#21644;&#8220;CIFAR-10&#8221;&#65292;HCR&#30028;&#38480;&#22312;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;HCR&#30028;&#38480;&#20284;&#20046;&#21333;&#29420;&#26080;&#27861;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02866v1 Announce Type: new  Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as "features" (or, less commonly, as "embeddings" or "feature embeddings"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, "MNIST" and "CIFAR-10," which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guar
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#30005;&#21160;&#36710;&#36742;&#30340;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#35299;&#20915;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2404.02448</link><description>&lt;p&gt;
&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#29992;&#20110;&#24212;&#24613;&#20379;&#30005;&#65306;&#38754;&#21521;&#30005;&#20449;&#22522;&#31449;&#25937;&#21161;
&lt;/p&gt;
&lt;p&gt;
Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#30005;&#21160;&#36710;&#36742;&#30340;&#36335;&#24452;&#38382;&#39064;&#65292;&#36890;&#36807;&#32452;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#35299;&#20915;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#23478;&#30005;&#20449;&#25552;&#20379;&#21830;&#65292;&#25105;&#20204;&#20844;&#21496;&#26377;&#19968;&#20010;&#20851;&#38190;&#20351;&#21629;&#65292;&#21363;&#22312;&#20572;&#30005;&#26399;&#38388;&#20445;&#25345;&#30005;&#20449;&#26381;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#20351;&#21629;&#65292;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#32500;&#25345;&#30005;&#20449;&#22522;&#31449;&#30340;&#30005;&#21147;&#12290;&#26412;&#25991;&#32771;&#34385;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#30005;&#21160;&#36710;&#36742; (EVs) &#30452;&#25509;&#21069;&#24448;&#20854;&#20301;&#32622;&#20026;&#22522;&#31449;&#25552;&#20379;&#30005;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#23567;&#21270;&#25152;&#26377;&#30005;&#21160;&#36710;&#36742;&#30340;&#24635;&#34892;&#39542;&#36317;&#31163;&#21644;&#25925;&#38556;&#22522;&#31449;&#25968;&#37327;&#30340;EV&#36335;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#36335;&#24452;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#26032;&#22411;&#30340;&#30005;&#21160;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064; (EVRP) &#21464;&#20307;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#36710;&#36742;&#36873;&#25321;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#33410;&#28857;&#36873;&#25321;&#22120;&#30456;&#32467;&#21512;&#30340;&#27714;&#35299;&#22120;&#12290;&#36710;&#36742;&#36873;&#25321;&#22120;&#30340;&#35268;&#21017;&#30830;&#20445;&#20102;&#25152;&#36873;EV&#24320;&#22987;&#31227;&#21160;&#26102;&#30340;&#30830;&#20999;&#29615;&#22659;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;RL&#27169;&#22411;&#30340;&#33410;&#28857;&#36873;&#25321;&#23454;&#29616;&#20102;&#24555;&#36895;&#36335;&#24452;&#29983;&#25104;&#65292;&#22312;&#32039;&#24613;&#24773;&#20917;&#19979;&#23588;&#20026;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#19978;&#23545;&#25105;&#20204;&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02448v1 Announce Type: cross  Abstract: As a telecom provider, our company has a critical mission to maintain telecom services even during power outages. To accomplish the mission, it is essential to maintain the power of the telecom base stations. Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations. The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations. In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning (RL)-based node selector. The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move. In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies. We evaluate our solver on bot
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#21033;&#29992;&#24739;&#32773;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35328;&#35828;&#35805;&#32773;&#39564;&#35777;&#65292;&#20197;&#24212;&#23545;&#36523;&#20221;&#39564;&#35777;&#21644;&#25490;&#38500;&#37325;&#22797;&#20837;&#32452;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.01981</link><description>&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#20013;&#30340;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#35828;&#35805;&#32773;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01981
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#21033;&#29992;&#24739;&#32773;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35328;&#35828;&#35805;&#32773;&#39564;&#35777;&#65292;&#20197;&#24212;&#23545;&#36523;&#20221;&#39564;&#35777;&#21644;&#25490;&#38500;&#37325;&#22797;&#20837;&#32452;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20020;&#24202;&#35797;&#39564;&#28041;&#21450;&#22823;&#37327;&#20020;&#24202;&#21307;&#29983;&#12289;&#24739;&#32773;&#21644;&#25968;&#25454;&#25910;&#38598;&#29615;&#22659;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#65292;&#24739;&#32773;&#26681;&#25454;&#20854;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#26816;&#27979;&#21644;&#30417;&#27979;&#35748;&#30693;&#21644;&#24515;&#29702;&#20581;&#24247;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#36825;&#20123;&#35821;&#38899;&#24405;&#38899;&#26469;&#39564;&#35777;&#24050;&#20837;&#32452;&#24739;&#32773;&#30340;&#36523;&#20221;&#65292;&#24182;&#35782;&#21035;&#21644;&#25490;&#38500;&#35797;&#22270;&#22810;&#27425;&#20837;&#32452;&#21516;&#19968;&#35797;&#39564;&#30340;&#20010;&#20307;&#12290;&#30001;&#20110;&#20020;&#24202;&#30740;&#31350;&#32463;&#24120;&#36328;&#36234;&#19981;&#21516;&#22269;&#23478;&#36827;&#34892;&#65292;&#22240;&#27492;&#24517;&#39035;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#25191;&#34892;&#35828;&#35805;&#32773;&#39564;&#35777;&#30340;&#31995;&#32479;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#24320;&#21457;&#24037;&#20316;&#12290;&#25105;&#20204;&#36890;&#36807;&#25307;&#21215;&#21644;&#27979;&#35797;&#35762;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#20025;&#40614;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#35821;&#38899;&#21463;&#25439;&#24739;&#32773;&#26469;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;TitaNet&#12289;ECAPA-TDNN&#21644;SpeakerNet&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#27979;&#35797;&#30340;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01981v1 Announce Type: new  Abstract: Due to the substantial number of clinicians, patients, and data collection environments involved in clinical trials, gathering data of superior quality poses a significant challenge. In clinical trials, patients are assessed based on their speech data to detect and monitor cognitive and mental health disorders. We propose using these speech recordings to verify the identities of enrolled patients and identify and exclude the individuals who try to enroll multiple times in the same trial. Since clinical studies are often conducted across different countries, creating a system that can perform speaker verification in diverse languages without additional development effort is imperative. We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models by enrolling and testing with speech-impaired patients speaking English, German, Danish, Spanish, and Arabic languages. Our results demonstrate that tested models can effectively generalize 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;Bi-LORA&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;VLMs&#21644;LORA&#35843;&#25972;&#25216;&#26415;&#65292;&#23558;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#36716;&#21270;&#20026;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#35265;&#36807;&#30340;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.01959</link><description>&lt;p&gt;
Bi-LORA&#65306;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#35270;&#35273;-&#35821;&#35328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bi-LORA: A Vision-Language Approach for Synthetic Image Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01959
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;Bi-LORA&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;VLMs&#21644;LORA&#35843;&#25972;&#25216;&#26415;&#65292;&#23558;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#36716;&#21270;&#20026;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#23545;&#26410;&#35265;&#36807;&#30340;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#20687;&#21512;&#25104;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#65292;&#24050;&#32463;&#24320;&#21551;&#20102;&#19968;&#20010;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#22270;&#20687;&#30340;&#26102;&#20195;&#12290;&#23613;&#31649;&#36825;&#31181;&#25216;&#26415;&#36827;&#27493;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#23545;&#20110;&#21306;&#20998;&#30495;&#23454;&#22270;&#20687;&#21644;&#21512;&#25104;&#22270;&#20687;&#20043;&#38388;&#28508;&#22312;&#38590;&#24230;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#20511;&#37492;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#24378;&#22823;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#32467;&#21512;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#38646;&#27425;&#23398;&#20064;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Bi-LORA&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;VLMs&#65292;&#32467;&#21512;&#20302;&#31209;&#36866;&#24212;&#65288;LORA&#65289;&#35843;&#25972;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#27169;&#22411;&#29983;&#25104;&#22270;&#20687;&#30340;&#21512;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#27010;&#24565;&#36716;&#21464;&#22312;&#20110;&#23558;&#20108;&#20998;&#31867;&#37325;&#26032;&#26500;&#24314;&#20026;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#21033;&#29992;&#20102;&#23574;&#31471;VLM&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01959v1 Announce Type: cross  Abstract: Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#21160;&#20316;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Gromov-Wasserstein&#38382;&#39064;&#20013;&#32534;&#30721;&#26102;&#38388;&#19968;&#33268;&#24615;&#20808;&#39564;&#26469;&#23454;&#29616;&#20174;&#35270;&#39057;&#24103;&#21644;&#21160;&#20316;&#31867;&#21035;&#20043;&#38388;&#30340;&#22122;&#22768;&#25104;&#26412;&#20013;&#35299;&#30721;&#26102;&#38388;&#19968;&#33268;&#30340;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2404.01518</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#21160;&#20316;&#20998;&#21106;&#30340;&#20020;&#26102;&#19968;&#33268;&#19981;&#24179;&#34913;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01518
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#21160;&#20316;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Gromov-Wasserstein&#38382;&#39064;&#20013;&#32534;&#30721;&#26102;&#38388;&#19968;&#33268;&#24615;&#20808;&#39564;&#26469;&#23454;&#29616;&#20174;&#35270;&#39057;&#24103;&#21644;&#21160;&#20316;&#31867;&#21035;&#20043;&#38388;&#30340;&#22122;&#22768;&#25104;&#26412;&#20013;&#35299;&#30721;&#26102;&#38388;&#19968;&#33268;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#26102;&#38388;&#26410;&#20462;&#21098;&#35270;&#39057;&#30340;&#21160;&#20316;&#20998;&#21106;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#35299;&#20915;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#19968;&#33268;&#24615;&#20808;&#39564;&#32534;&#30721;&#21040;Gromov-Wasserstein&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#35270;&#39057;&#24103;&#21644;&#21160;&#20316;&#31867;&#21035;&#20043;&#38388;&#30340;&#22122;&#22768;&#20851;&#32852;/&#21305;&#37197;&#25104;&#26412;&#30697;&#38453;&#20013;&#35299;&#30721;&#20986;&#19968;&#20010;&#26102;&#38388;&#19968;&#33268;&#30340;&#20998;&#21106;&#12290;&#19982;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#30693;&#36947;&#35270;&#39057;&#30340;&#21160;&#20316;&#39034;&#24207;&#26469;&#23454;&#29616;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#65288;&#34701;&#21512;&#65289;Gromov-Wasserstein&#38382;&#39064;&#21487;&#20197;&#22312;GPU&#19978;&#20351;&#29992;&#20960;&#27425;&#25237;&#24433;&#38236;&#19979;&#38477;&#36845;&#20195;&#39640;&#25928;&#27714;&#35299;&#12290;&#25105;&#20204;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#33258;&#35757;&#32451;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;Breakfast&#12289;50-Salads&#12289;YouTube Instructions&#21644;Desktop Assembly&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#20998;&#21106;&#26041;&#27861;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01518v1 Announce Type: cross  Abstract: We propose a novel approach to the action segmentation task for long, untrimmed videos, based on solving an optimal transport problem. By encoding a temporal consistency prior into a Gromov-Wasserstein problem, we are able to decode a temporally consistent segmentation from a noisy affinity/matching cost matrix between video frames and action classes. Unlike previous approaches, our method does not require knowing the action order for a video to attain temporal consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can be efficiently solved on GPUs using a few iterations of projected mirror descent. We demonstrate the effectiveness of our method in an unsupervised learning setting, where our method is used to generate pseudo-labels for self-training. We evaluate our segmentation approach and unsupervised learning pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly datasets, yielding state
&lt;/p&gt;</description></item><item><title>Metarobotics&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#26080;&#32447;&#36890;&#20449;&#12289;&#22810;&#24863;&#23448;&#27785;&#28024;&#21644;&#38598;&#20307;&#26234;&#33021;&#65292;&#20026;&#36828;&#31243;&#26426;&#22120;&#20154;&#24212;&#29992;&#25552;&#20379;&#26222;&#36941;&#12289;&#27969;&#21160;&#21644;&#38750;&#20405;&#20837;&#24335;&#30340;&#35775;&#38382;&#21644;&#20114;&#21160;&#65292;&#26377;&#26395;&#20026;&#24037;&#19994;&#21644;&#31038;&#20250;&#24102;&#26469;&#35832;&#22810;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2404.00797</link><description>&lt;p&gt;
&#38754;&#21521;&#24037;&#19994;&#21644;&#31038;&#20250;&#30340;&#20803;&#26426;&#22120;&#20154;&#65306;&#24895;&#26223;&#12289;&#25216;&#26415;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Metarobotics for Industry and Society: Vision, Technologies, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00797
&lt;/p&gt;
&lt;p&gt;
Metarobotics&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#26080;&#32447;&#36890;&#20449;&#12289;&#22810;&#24863;&#23448;&#27785;&#28024;&#21644;&#38598;&#20307;&#26234;&#33021;&#65292;&#20026;&#36828;&#31243;&#26426;&#22120;&#20154;&#24212;&#29992;&#25552;&#20379;&#26222;&#36941;&#12289;&#27969;&#21160;&#21644;&#38750;&#20405;&#20837;&#24335;&#30340;&#35775;&#38382;&#21644;&#20114;&#21160;&#65292;&#26377;&#26395;&#20026;&#24037;&#19994;&#21644;&#31038;&#20250;&#24102;&#26469;&#35832;&#22810;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metarobotics&#26088;&#22312;&#23558;&#19979;&#19968;&#20195;&#26080;&#32447;&#36890;&#20449;&#65292;&#22810;&#24863;&#23448;&#27785;&#28024;&#21644;&#38598;&#20307;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#23545;&#36828;&#31243;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#26222;&#36941;&#12289;&#27969;&#21160;&#21644;&#38750;&#20405;&#20837;&#24335;&#35775;&#38382;&#21644;&#20114;&#21160;&#12290;&#24037;&#19994;&#21644;&#31038;&#20250;&#26377;&#26395;&#20174;&#36825;&#20123;&#21151;&#33021;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;Metarobotics&#22312;&#31038;&#20250;&#12289;&#24037;&#19994;&#21644;&#20004;&#32773;&#20043;&#38388;&#30340;&#30446;&#26631;&#12290;&#23427;&#30830;&#23450;&#24182;&#35843;&#26597;&#20102;&#21487;&#33021;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26550;&#26500;&#26469;&#25512;&#36827;Metarobotics&#20851;&#38190;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00797v1 Announce Type: cross  Abstract: Metarobotics aims to combine next generation wireless communication, multi-sense immersion, and collective intelligence to provide a pervasive, itinerant, and non-invasive access and interaction with distant robotized applications. Industry and society are expected to benefit from these functionalities. For instance, robot programmers will no longer travel worldwide to plan and test robot motions, even collaboratively. Instead, they will have a personalized access to robots and their environments from anywhere, thus spending more time with family and friends. Students enrolled in robotics courses will be taught under authentic industrial conditions in real-time. This paper describes objectives of Metarobotics in society, industry, and in-between. It identifies and surveys technologies likely to enable their completion and provides an architecture to put forward the interplay of key components of Metarobotics. Potentials for self-determ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00521</link><description>&lt;p&gt;
CHAIN&#65306;&#36890;&#36807;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#22686;&#24378;&#25968;&#25454;&#39640;&#25928;GANs&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26174;&#30528;&#25512;&#21160;&#20102;&#22270;&#20687;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;GANs&#32463;&#24120;&#38754;&#20020;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#35782;&#21035;Batch Normalization&#65288;BN&#65289;&#20013;&#30340;&#20851;&#38190;&#32570;&#38519;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65306;&#22312;&#20013;&#24515;&#21270;&#21644;&#32553;&#25918;&#27493;&#39588;&#20013;&#26799;&#24230;&#29190;&#28856;&#30340;&#20542;&#21521;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CHAIN&#65288;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#65289;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#20013;&#24515;&#21270;&#27493;&#39588;&#26367;&#25442;&#20026;&#38646;&#22343;&#20540;&#27491;&#21017;&#21270;&#65292;&#24182;&#22312;&#32553;&#25918;&#27493;&#39588;&#20013;&#38598;&#25104;&#20102;Lipschitz&#36830;&#32493;&#24615;&#32422;&#26463;&#12290;CHAIN&#36890;&#36807;&#33258;&#36866;&#24212;&#25554;&#20540;&#24402;&#19968;&#21270;&#21644;&#38750;&#24402;&#19968;&#21270;&#29305;&#24449;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;GANs&#30340;&#35757;&#32451;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00521v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;</title><link>https://arxiv.org/abs/2404.00482</link><description>&lt;p&gt;
&#29992;&#20110;&#26031;&#25289;&#22827;&#35821;&#30340;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Named Entity Corpus for Slavic Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00482
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#65288;&#20445;&#21152;&#21033;&#20122;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#12289;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#12289;&#20420;&#35821;&#21644;&#20044;&#20811;&#20848;&#35821;&#65289;&#21629;&#21517;&#23454;&#20307;&#30340;&#35821;&#26009;&#24211;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;2017-2023&#24180;&#38388;&#26031;&#25289;&#22827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#35752;&#20250;&#30340;&#19968;&#31995;&#21015;&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;5017&#20221;&#28085;&#30422;&#19971;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#25991;&#26723;&#26631;&#26377;&#20116;&#31867;&#21629;&#21517;&#23454;&#20307;&#65292;&#27599;&#20010;&#23454;&#20307;&#30001;&#31867;&#21035;&#12289;&#24341;&#29992;&#35789;&#21644;&#21807;&#19968;&#36328;&#35821;&#35328;&#26631;&#35782;&#31526;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#35843;&#25972;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998; - &#21333;&#20010;&#20027;&#39064;&#21010;&#20998;&#21644;&#36328;&#20027;&#39064;&#21010;&#20998;&#12290;&#23545;&#20110;&#27599;&#20010;&#21010;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#32622;&#20102;&#22522;&#20934;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;XLM-RoBERTa-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#25552;&#21450;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;mT5-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;</title><link>https://arxiv.org/abs/2404.00076</link><description>&lt;p&gt;
&#20351;&#29992;&#20498;&#32622;&#26631;&#31614;&#30340;&#21518;&#38376;&#26041;&#27861;&#65306;&#33039;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22768;&#38899;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#32463;&#24120;&#20351;&#29992;&#20844;&#20849;&#25110;&#31532;&#19977;&#26041;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#36825;&#20351;&#24471;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#28508;&#22312;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#12290;&#22312;&#36825;&#31181;&#25915;&#20987;&#31867;&#22411;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#27602;&#21270;&#25968;&#25454;&#26469;&#35757;&#32451;DNN&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#21478;&#19968;&#31181;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#38750;&#24120;&#30456;&#20851;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#31867;&#22411;&#26159;&#26631;&#31614;&#32763;&#36716;&#65292;&#25915;&#20987;&#32773;&#22312;&#20854;&#20013;&#25805;&#32437;&#25968;&#25454;&#23376;&#38598;&#30340;&#26631;&#31614;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#21363;&#20351;&#26159;&#33021;&#21147;&#26377;&#38480;&#30340;&#25915;&#20987;&#32773;&#65292;&#36825;&#20123;&#25915;&#20987;&#20063;&#21487;&#33021;&#26497;&#22823;&#22320;&#38477;&#20302;&#31995;&#32479;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#65292;&#8220;&#26631;&#31614;&#23545;&#26631;&#31614;&#8221;&#65292;&#22312;&#19982;&#30446;&#26631;&#31867;&#21035;&#30456;&#20851;&#30340;&#36873;&#23450;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65288;&#25293;&#25163;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
&lt;/p&gt;</description></item><item><title>FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.20261</link><description>&lt;p&gt;
FABind+: &#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#22686;&#24378;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20261
&lt;/p&gt;
&lt;p&gt;
FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#25216;&#26415;&#20381;&#36182;&#20110;&#21463;&#29289;&#29702;&#21407;&#29702;&#25903;&#37197;&#30340;&#24191;&#27867;&#37319;&#26679;&#21644;&#27169;&#25311;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21069;&#26223;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#22686;&#38271;&#12290;&#24314;&#31435;&#22312;FABind&#30340;&#22522;&#30784;&#24037;&#20316;&#20043;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind+&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22823;&#25552;&#21319;&#20854;&#21069;&#36523;&#24615;&#33021;&#30340;&#22686;&#24378;&#29256;&#12290;&#25105;&#20204;&#30830;&#23450;&#21475;&#34955;&#39044;&#27979;&#26159;&#20998;&#23376;&#23545;&#25509;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#33879;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23545;&#25509;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#23545;&#25509;&#27169;&#22359;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#22686;&#24378;&#20854;&#23039;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#20256;&#32479;&#37319;&#26679;/&#29983;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.20208</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#39044;&#27979;&#34920;&#26684;&#20219;&#21153;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#65292;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#32570;&#22833;&#20540;&#22635;&#20805;&#31561;&#39044;&#27979;&#20219;&#21153;&#26159;&#19982;&#34920;&#26684;&#25968;&#25454;&#30456;&#20851;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#20219;&#21153;&#12290;&#23613;&#31649;LLMs&#25797;&#38271;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25910;&#38598;&#24102;&#26377;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;Llama-2&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;-shot&#39044;&#27979;&#12289;&#23569;-shot&#39044;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
&lt;/p&gt;</description></item><item><title>TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.20150</link><description>&lt;p&gt;
TFB&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#20840;&#38754;&#19988;&#20844;&#24179;&#30340;&#22522;&#20934;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20150
&lt;/p&gt;
&lt;p&gt;
TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20250;&#22312;&#32463;&#27982;&#12289;&#20132;&#36890;&#12289;&#20581;&#24247;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#39046;&#22495;&#20013;&#20135;&#29983;&#65292;&#23545;&#26410;&#26469;&#25968;&#20540;&#30340;&#39044;&#27979;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#35768;&#22810;&#39044;&#27979;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#20026;&#20102;&#30830;&#20445;&#36827;&#23637;&#65292;&#26377;&#24517;&#35201;&#33021;&#22815;&#20197;&#20840;&#38754;&#19988;&#21487;&#38752;&#30340;&#26041;&#24335;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TFB&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#12290;TFB&#36890;&#36807;&#35299;&#20915;&#19982;&#25968;&#25454;&#38598;&#12289;&#27604;&#36739;&#26041;&#27861;&#21644;&#35780;&#20272;&#31649;&#36947;&#30456;&#20851;&#30340;&#32570;&#28857;&#65292;&#25512;&#21160;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#65306;1&#65289;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#65292;2&#65289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;3&#65289;&#19981;&#19968;&#33268;&#21644;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#39046;&#22495;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#26469;&#33258;10&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65306;&#20132;&#36890;&#12289;&#30005;&#21147;&#12289;&#33021;&#28304;&#12289;&#29615;&#22659;&#12289;&#33258;&#28982;&#12289;&#32463;&#27982;&#12289;&#32929;&#31080;&#24066;&#22330;&#12289;&#38134;&#34892;&#12289;&#20581;&#24247;&#21644;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
&lt;/p&gt;</description></item><item><title>FairRAG&#26694;&#26550;&#36890;&#36807;&#22312;&#22806;&#37096;&#22270;&#20687;&#25968;&#25454;&#24211;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#20687;&#26469;&#25552;&#39640;&#20154;&#31867;&#29983;&#25104;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#24212;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21435;&#20559;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#29983;&#25104;&#36807;&#31243;&#25552;&#20379;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.19964</link><description>&lt;p&gt;
FairRAG: &#20844;&#24179;&#20154;&#31867;&#29983;&#25104;&#30340;&#20844;&#24179;&#26816;&#32034;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
FairRAG: Fair Human Generation via Fair Retrieval Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19964
&lt;/p&gt;
&lt;p&gt;
FairRAG&#26694;&#26550;&#36890;&#36807;&#22312;&#22806;&#37096;&#22270;&#20687;&#25968;&#25454;&#24211;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#20687;&#26469;&#25552;&#39640;&#20154;&#31867;&#29983;&#25104;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#24212;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#21435;&#20559;&#31574;&#30053;&#65292;&#20174;&#32780;&#20026;&#29983;&#25104;&#36807;&#31243;&#25552;&#20379;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21453;&#26144;&#29978;&#33267;&#25918;&#22823;&#20102;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#26681;&#28145;&#33922;&#22266;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#23545;&#20154;&#31867;&#22270;&#20687;&#29983;&#25104;&#23588;&#20026;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#27169;&#22411;&#20559;&#21521;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#32452;&#12290;&#29616;&#26377;&#30340;&#32416;&#27491;&#27492;&#38382;&#39064;&#30340;&#23581;&#35797;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#22266;&#26377;&#38480;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#26410;&#33021;&#22312;&#26681;&#26412;&#19978;&#25913;&#21892;&#20154;&#21475;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20844;&#24179;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;FairRAG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#26469;&#33258;&#22806;&#37096;&#22270;&#20687;&#25968;&#25454;&#24211;&#30340;&#21442;&#32771;&#22270;&#20687;&#19978;&#36827;&#34892;&#26465;&#20214;&#21270;&#26469;&#25552;&#39640;&#20154;&#31867;&#29983;&#25104;&#20013;&#30340;&#20844;&#24179;&#24615;&#12290;FairRAG&#36890;&#36807;&#19968;&#20010;&#36731;&#37327;&#32423;&#32447;&#24615;&#27169;&#22359;&#23454;&#29616;&#26465;&#20214;&#21270;&#65292;&#23558;&#21442;&#32771;&#22270;&#20687;&#25237;&#23556;&#21040;&#25991;&#26412;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22686;&#24378;&#20844;&#24179;&#24615;&#65292;FairRAG&#24212;&#29992;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#20379;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#22270;&#20687;&#12290;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19964v1 Announce Type: cross  Abstract: Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18742</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Learning Dynamics of Alignment with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#27169;&#22411;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#34429;&#28982;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29702;&#35770;&#19978;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#27491;&#24335;&#23637;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#29616;&#35937;&#65292;&#21363;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24403;&#20195;LLMs&#21644;&#23545;&#40784;&#20219;&#21153;&#19978;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24378;&#21270;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23545;&#40784;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#20813;&#36131;&#22768;&#26126;&#65306;&#26412;&#25991;&#21253;&#21547;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#30340;&#22810;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#21387;&#32553;&#29575;&#19982;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21462;&#24471;&#20102;&#25968;&#25454;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17886</link><description>&lt;p&gt;
&#21387;&#32553;&#22810;&#20219;&#21153;&#23884;&#20837;&#29992;&#20110;&#22320;&#29699;&#35266;&#27979;&#20013;&#25968;&#25454;&#39640;&#25928;&#19979;&#28216;&#35757;&#32451;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17886
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#30340;&#22810;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#21387;&#32553;&#29575;&#19982;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21462;&#24471;&#20102;&#25968;&#25454;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#20013;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#23384;&#20648;&#24211;&#22686;&#38271;&#65292;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#36716;&#31227;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#65292;&#28040;&#32791;&#20102;&#22823;&#37327;&#36164;&#28304;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#65288;NEC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#23545;&#25968;&#25454;&#20351;&#29992;&#32773;&#20256;&#36755;&#21387;&#32553;&#30340;&#23884;&#20837;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#31070;&#32463;&#21387;&#32553;&#26469;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#65292;&#29983;&#25104;&#22810;&#20219;&#21153;&#23884;&#20837;&#65292;&#21516;&#26102;&#22312;&#21387;&#32553;&#29575;&#21644;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#20165;&#38024;&#23545;FM&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;10%&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#36827;&#34892;&#30701;&#26102;&#38388;&#35757;&#32451;&#65288;&#39044;&#35757;&#32451;&#36845;&#20195;&#30340;1%&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;EO&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;NEC&#65306;&#22330;&#26223;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;&#19982;&#23558;&#20256;&#32479;&#21387;&#32553;&#24212;&#29992;&#20110;&#21407;&#22987;&#25968;&#25454;&#30456;&#27604;&#65292;NEC&#22312;&#20943;&#23569;&#25968;&#25454;&#37327;&#26041;&#38754;&#21487;&#23454;&#29616;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#20102;75%&#21040;90%&#30340;&#25968;&#25454;&#37327;&#12290;&#21363;&#20351;&#22312;99.7%&#30340;&#21387;&#32553;&#19979;&#65292;&#22312;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#19978;&#24615;&#33021;&#20165;&#19979;&#38477;&#20102;5%&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;NEC&#26159;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17886v1 Announce Type: new  Abstract: As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17601</link><description>&lt;p&gt;
LASIL&#65306;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#38271;&#26399;&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17601
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#22312;&#20132;&#36890;&#24037;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#21333;&#20010;&#36710;&#36742;&#34892;&#20026;&#21644;&#25972;&#20307;&#20132;&#36890;&#27969;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#19968;&#20010;&#30495;&#23454;&#30340;&#27169;&#25311;&#22120;&#65292;&#31934;&#30830;&#22797;&#21046;&#21508;&#31181;&#20132;&#36890;&#26465;&#20214;&#19979;&#30340;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#20381;&#36182;&#21551;&#21457;&#24335;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#24448;&#24448;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20132;&#36890;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#32780;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#30340;&#27169;&#25311;&#12290;&#30001;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#31283;&#23450;&#30340;&#38271;&#26399;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#19987;&#23478;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#22686;&#24378;&#29366;&#24577;&#24847;&#35782;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17601v1 Announce Type: new  Abstract: Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.15933</link><description>&lt;p&gt;
&#29702;&#35299;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#20013;&#30340;&#22495;&#22823;&#23567;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Domain-Size Generalization in Markov Logic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#65288;MLNs&#65289;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#20851;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#22810;&#20010;&#30740;&#31350;&#27880;&#24847;&#21040;&#65292;&#22312;&#32473;&#23450;&#22495;&#19978;&#23398;&#20064;&#30340;MLNs&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#22495;&#19978;&#27867;&#21270;&#24456;&#24046;&#12290;&#36825;&#31181;&#34892;&#20026;&#28304;&#20110;MLN&#22312;&#19981;&#21516;&#22495;&#22823;&#23567;&#19978;&#20351;&#29992;&#26102;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#23558;&#20854;&#38480;&#21046;&#22312;MLN&#21442;&#25968;&#30340;&#26041;&#24046;&#33539;&#22260;&#20869;&#12290;&#21442;&#25968;&#26041;&#24046;&#36824;&#38480;&#21046;&#20102;&#20174;&#19981;&#21516;&#22495;&#22823;&#23567;&#20013;&#21462;&#20986;&#30340;MLN&#36793;&#32536;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#23637;&#31034;&#65292;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#65292;&#23545;&#24212;&#20110;&#22495;&#22823;&#23567;&#27867;&#21270;&#30340;&#20004;&#20010;&#33258;&#28982;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#25351;&#25968;&#38543;&#26426;&#22270;&#21644;&#20854;&#20182;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;&#30340;&#20851;&#31995;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24050;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#20250;&#20943;&#23569;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15933v1 Announce Type: new  Abstract: We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the varia
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Transformer&#32593;&#32476;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#20855;&#26377;&#26368;&#20339;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15417</link><description>&lt;p&gt;
&#21033;&#29992;Transformer&#25216;&#26415;&#22686;&#24378;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Automatic Modulation Recognition for IoT Applications Using Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15417
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#32593;&#32476;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#26041;&#27861;&#65292;&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#20855;&#26377;&#26368;&#20339;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;(AMR)&#23545;&#20110;&#30830;&#23450;&#20256;&#20837;&#20449;&#21495;&#30340;&#35843;&#21046;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#32467;&#21512;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#22788;&#29702;&#21644;&#26368;&#23567;&#36164;&#28304;&#20351;&#29992;&#65292;&#36825;&#23545;&#29289;&#32852;&#32593;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#32593;&#32476;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#27169;&#22411;&#22823;&#23567;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15417v1 Announce Type: cross  Abstract: Automatic modulation recognition (AMR) is critical for determining the modulation type of incoming signals. Integrating advanced deep learning approaches enables rapid processing and minimal resource usage, essential for IoT applications. We have introduced a novel method using Transformer networks for efficient AMR, designed specifically to address the constraints on model size prevalent in IoT environments. Our extensive experiments reveal that our proposed method outperformed advanced deep learning techniques, achieving the highest recognition accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26680;&#26041;&#27861;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24179;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#27604;&#20219;&#20309;&#26680;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#20250;&#33719;&#24471;&#19982;&#30446;&#26631;&#20989;&#25968;&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26680;&#12290;</title><link>https://arxiv.org/abs/2403.14917</link><description>&lt;p&gt;
&#20174;&#26680;&#26041;&#27861;&#30340;&#35282;&#24230;&#23545;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24179;&#22343;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26680;&#26041;&#27861;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24179;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#27604;&#20219;&#20309;&#26680;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#20250;&#33719;&#24471;&#19982;&#30446;&#26631;&#20989;&#25968;&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26680;&#26041;&#27861;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#24179;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#32858;&#28966;&#20110;&#31532;&#19968;&#23618;&#35825;&#23548;&#30340;&#26680;&#30340;&#21160;&#24577;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#26497;&#38480;&#65292;&#20854;&#20013;&#31532;&#20108;&#23618;&#27604;&#31532;&#19968;&#23618;&#31227;&#21160;&#24471;&#24555;&#24471;&#22810;&#12290;&#22312;&#36825;&#20010;&#26497;&#38480;&#19979;&#65292;&#23398;&#20064;&#38382;&#39064;&#34987;&#31616;&#21270;&#20026;&#22312;&#20869;&#22312;&#26680;&#19978;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#22343;&#22330; Langevin &#21160;&#21147;&#23398;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#26102;&#38388;&#21644;&#31890;&#23376;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#27604;&#20219;&#20309;&#26680;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#22810;&#20010;&#20877;&#29616;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#24182;&#38598;&#65292;&#24182;&#19988;&#31070;&#32463;&#32593;&#32476;&#20250;&#33719;&#24471;&#19982;&#30446;&#26631;&#20989;&#25968;&#23545;&#40784;&#30340;&#25968;&#25454;&#30456;&#20851;&#26680;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#30340;&#26631;&#31614;&#22122;&#22768;&#36807;&#31243;&#65292;&#24182;&#23637;&#31034;&#33258;&#30001;&#24230;&#20986;&#29616;&#20316;&#20026;&#19968;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14917v1 Announce Type: new  Abstract: In this paper, we study the feature learning ability of two-layer neural networks in the mean-field regime through the lens of kernel methods. To focus on the dynamics of the kernel induced by the first layer, we utilize a two-timescale limit, where the second layer moves much faster than the first layer. In this limit, the learning problem is reduced to the minimization problem over the intrinsic kernel. Then, we show the global convergence of the mean-field Langevin dynamics and derive time and particle discretization error. We also demonstrate that two-layer neural networks can learn a union of multiple reproducing kernel Hilbert spaces more efficiently than any kernel methods, and neural networks acquire data-dependent kernel which aligns with the target function. In addition, we develop a label noise procedure, which converges to the global optimum and show that the degrees of freedom appears as an implicit regularization.
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.13808</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Pretraining Data Diversity for Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13808
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#26159;&#21807;&#19968;&#26679;&#26412;&#25968;&#37327;&#65292;&#22312;&#22266;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;SSL&#24615;&#33021;&#65292;&#23613;&#31649;&#21482;&#26377;&#24403;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#24456;&#23567;&#30340;&#26102;&#20505;&#25165;&#26159;&#22914;&#27492;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#36890;&#36807;&#32593;&#32476;&#29228;&#34411;&#25110;&#25193;&#25955;&#29983;&#25104;&#30340;&#25968;&#25454;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#24322;&#24120;&#22823;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20998;&#24067;&#36716;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#19971;&#31181;SSL&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#35832;&#22914;ImageNet&#21644;YFCC100M&#31561;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#36229;&#36807;200&#20010;GPU&#22825;&#12290;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#23558;&#22312;https://github.com/hammoudhasan/DiversitySSL &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13808v1 Announce Type: cross  Abstract: We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL .
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#65292;&#36890;&#36807;&#21327;&#20316;&#23398;&#20064;&#22810;&#20010;&#23398;&#20064;&#32773;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13245</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#19982;&#38646;&#27425;&#36890;&#29992;&#21270;
&lt;/p&gt;
&lt;p&gt;
Federated reinforcement learning for robot motion planning with zero-shot generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#65292;&#36890;&#36807;&#21327;&#20316;&#23398;&#20064;&#22810;&#20010;&#23398;&#20064;&#32773;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#38646;&#27425;&#36890;&#29992;&#21270;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#36827;&#34892;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#37096;&#32626;&#23398;&#20064;&#31574;&#30053;&#21040;&#26032;&#29615;&#22659;&#26102;&#19981;&#38656;&#35201;&#25968;&#25454;&#25910;&#38598;&#21644;&#31574;&#30053;&#35843;&#25972;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#23398;&#20064;&#32773;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#65288;&#20113;&#31471;&#65289;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#32780;&#19981;&#20998;&#20139;&#21407;&#22987;&#25968;&#25454;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#27599;&#20010;&#23398;&#20064;&#32773;&#23558;&#20854;&#26412;&#22320;&#25511;&#21046;&#31574;&#30053;&#21644;&#30456;&#24212;&#30340;&#20272;&#35745;&#24402;&#19968;&#21270;&#21040;&#36798;&#26102;&#38388;&#19978;&#20256;&#33267;&#20113;&#31471;&#65292;&#28982;&#21518;&#20113;&#31471;&#22312;&#23398;&#20064;&#32773;&#38388;&#35745;&#31639;&#20840;&#23616;&#26368;&#20248;&#24182;&#23558;&#26368;&#20248;&#31574;&#30053;&#24191;&#25773;&#32473;&#23398;&#20064;&#32773;&#12290;&#27599;&#20010;&#23398;&#20064;&#32773;&#28982;&#21518;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20174;&#20854;&#26412;&#22320;&#25511;&#21046;&#31574;&#30053;&#21644;&#20113;&#31471;&#20013;&#36873;&#25321;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#21040;&#36798;&#26102;&#38388;&#21644;&#23433;&#20840;&#24615;&#30340;&#38646;&#27425;&#36890;&#29992;&#21270;&#20445;&#35777;&#12290;&#23545;&#20110;&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#65292;&#20960;&#20046;&#19968;&#33268;&#24615;&#65292;Pare&#30340;&#29702;&#35770;&#20445;&#35777;//}
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13245v1 Announce Type: cross  Abstract: This paper considers the problem of learning a control policy for robot motion planning with zero-shot generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments. We develop a federated reinforcement learning framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data. In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners. Each learner then selects between its local control policy and that from the Cloud for next iteration. The proposed framework leverages on the derived zero-shot generalization guarantees on arrival time and safety. Theoretical guarantees on almost-sure convergence, almost consensus, Pare
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;</title><link>https://arxiv.org/abs/2403.13041</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#31169;&#23494;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Provable Privacy with Non-Private Pre-Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20998;&#26512;&#24046;&#20998;&#31169;&#23494;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#25968;&#25454;&#30456;&#20851;&#30340;&#39044;&#22788;&#29702;&#30340;&#28508;&#22312;&#38544;&#31169;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#26032;&#30340;&#25216;&#26415;&#27010;&#24565;&#24314;&#31435;&#20102;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;&#65306;&#19968;&#31181;&#31216;&#20026;&#24179;&#28369;DP&#30340;DP&#21464;&#20307;&#20197;&#21450;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#26041;&#27861;&#65292;&#22312;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#25552;&#21319;&#65292;&#20197;&#21450;&#35757;&#32451;&#26102;&#38388;&#21644;&#25512;&#29702;&#26102;&#38388;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2403.09680</link><description>&lt;p&gt;
&#39044;&#25490;&#24207;Tsetlin&#26426;&#22120;&#65288;&#22522;&#22240;K-Medoid&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09680
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#26041;&#27861;&#65292;&#22312;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#31934;&#24230;&#25552;&#21319;&#65292;&#20197;&#21450;&#35757;&#32451;&#26102;&#38388;&#21644;&#25512;&#29702;&#26102;&#38388;&#22823;&#24133;&#24230;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Tsetlin Machines&#36827;&#34892;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#25490;&#24207;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#24555;&#36895;&#36951;&#20256;&#31639;&#27861;&#20174;&#25968;&#25454;&#38598;&#20013;&#30830;&#23450;N&#20010;&#25968;&#25454;&#28857;&#65292;&#20197;&#35299;&#20915;&#26368;&#22823;&#31163;&#25955;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34987;&#29992;&#20316;&#36816;&#34892;K-Medoid&#32858;&#31867;&#31639;&#27861;&#30340;&#21021;&#22987;&#25918;&#32622;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#24555;&#36895;&#36951;&#20256;&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#27721;&#26126;&#36317;&#31163;&#26469;&#23545;&#40784;N&#20010;&#29420;&#31435;&#30340;Tsetlin Machines&#12290;&#23545;&#20110;MNIST&#32423;&#21035;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#32467;&#26524;&#26174;&#31034;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;&#39640;&#36798;10&#65285;&#65292;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;&#20102;&#32422;383&#20493;&#65292;&#25512;&#29702;&#26102;&#38388;&#20943;&#23569;&#20102;&#32422;86&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09680v1 Announce Type: cross  Abstract: This paper proposes a machine learning pre-sort stage to traditional supervised learning using Tsetlin Machines. Initially, N data-points are identified from the dataset using an expedited genetic algorithm to solve the maximum dispersion problem. These are then used as the initial placement to run the K-Medoid clustering algorithm. Finally, an expedited genetic algorithm is used to align N independent Tsetlin Machines by maximising hamming distance. For MNIST level classification problems, results demonstrate up to 10% improvement in accuracy, approx. 383X reduction in training time and approx. 86X reduction in inference time.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.09516</link><description>&lt;p&gt;
&#21033;&#29992;&#20856;&#22411;&#34920;&#31034;&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#32780;&#19981;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09516
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#36890;&#24120;&#38656;&#35201;&#35782;&#21035;&#19982;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30456;&#20851;&#32852;&#30340;&#31038;&#20250;&#32676;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DAFair&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#38382;&#39064;&#12290;&#19982;&#20381;&#36182;&#26174;&#24335;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#27492;&#31867;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20154;&#21475;&#32479;&#35745;&#20856;&#22411;&#25991;&#26412;&#65292;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#20943;&#36731;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#21644;&#20004;&#20010;&#27169;&#22411;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#19981;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#27880;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20248;&#20110;&#24120;&#35265;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Keyformer&#8221;&#30340;&#21019;&#26032;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#26631;&#35760;&#26469;&#20943;&#23569;KV&#32531;&#23384;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09054</link><description>&lt;p&gt;
Keyformer&#65306;&#36890;&#36807;&#20851;&#38190;&#26631;&#35760;&#36873;&#25321;&#20943;&#23569;KV&#32531;&#23384;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#29983;&#25104;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Keyformer&#8221;&#30340;&#21019;&#26032;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#26631;&#35760;&#26469;&#20943;&#23569;KV&#32531;&#23384;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#22522;&#30784;&#26550;&#26500;&#12290;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25512;&#26029;&#36807;&#31243;&#28041;&#21450;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;&#25552;&#31034;&#22788;&#29702;&#21644;&#26631;&#35760;&#29983;&#25104;&#12290;&#26631;&#35760;&#29983;&#25104;&#65292;&#26500;&#25104;&#20102;&#22823;&#37096;&#20998;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20027;&#35201;&#28041;&#21450;&#21521;&#37327;-&#30697;&#38453;&#20056;&#27861;&#21644;&#19982;&#38190;-&#20540;(KV)&#32531;&#23384;&#20132;&#20114;&#12290;&#30001;&#20110;&#20174;&#23384;&#20648;&#31995;&#32479;&#20256;&#36755;&#26435;&#37325;&#21644;KV&#32531;&#23384;&#20540;&#21040;&#35745;&#31639;&#21333;&#20803;&#30340;&#24320;&#38144;&#65292;&#36825;&#19968;&#38454;&#27573;&#21463;&#21040;&#20869;&#23384;&#24102;&#23485;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#20869;&#23384;&#29942;&#39048;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#21644;&#22823;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#24212;&#29992;&#20013;&#23588;&#20026;&#31361;&#20986;&#65292;&#36825;&#20004;&#32773;&#23545;LLMs&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;  &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#8220;Keyformer&#8221;&#65292;&#20197;&#32531;&#35299;&#19982;KV&#32531;&#23384;&#22823;&#23567;&#21644;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;Keyformer&#21033;&#29992;&#20102;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#22823;&#32422;90
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces "Keyformer", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.04810</link><description>&lt;p&gt;
&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Restricted Bayesian Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20316;&#20026;&#40657;&#30418;&#27169;&#22411;&#30340;&#36816;&#34892;&#26041;&#24335;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#12289;&#36807;&#25311;&#21512;&#12289;&#27424;&#25311;&#21512;&#12289;&#26799;&#24230;&#28040;&#22833;&#31561;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#31283;&#20581;&#30340;&#25910;&#25947;&#20540;&#65292;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#23588;&#20854;&#26159;&#24403;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#30340;&#20984;&#24615;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04810v1 Announce Type: cross  Abstract: Modern deep learning tools are remarkably effective in addressing intricate problems. However, their operation as black-box models introduces increased uncertainty in predictions. Additionally, they contend with various challenges, including the need for substantial storage space in large networks, issues of overfitting, underfitting, vanishing gradients, and more. This study explores the concept of Bayesian Neural Networks, presenting a novel architecture designed to significantly alleviate the storage space complexity of a network. Furthermore, we introduce an algorithm adept at efficiently handling uncertainties, ensuring robust convergence values without becoming trapped in local optima, particularly when the objective function lacks perfect convexity.
&lt;/p&gt;</description></item><item><title>3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#32467;&#21512;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23398;&#20064;&#22797;&#26434;&#25216;&#33021;&#25152;&#38656;&#22823;&#37327;&#20154;&#31867;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03954</link><description>&lt;p&gt;
3D&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
3D Diffusion Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03954
&lt;/p&gt;
&lt;p&gt;
3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#32467;&#21512;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23398;&#20064;&#22797;&#26434;&#25216;&#33021;&#25152;&#38656;&#22823;&#37327;&#20154;&#31867;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20026;&#25945;&#25480;&#26426;&#22120;&#20154;&#28789;&#24039;&#25216;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#24335;&#65307;&#28982;&#32780;&#65292;&#23398;&#20064;&#22797;&#26434;&#32780;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#25216;&#33021;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#28436;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#31574;&#30053;&#65288;DP3&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;3D&#35270;&#35273;&#34920;&#31034;&#30340;&#24378;&#22823;&#24615;&#34701;&#20837;&#21040;&#25193;&#25955;&#31574;&#30053;&#20013;&#30340;&#26032;&#39062;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#25193;&#25955;&#31574;&#30053;&#26159;&#19968;&#31867;&#26377;&#26465;&#20214;&#30340;&#21160;&#20316;&#29983;&#25104;&#27169;&#22411;&#12290;DP3&#30340;&#26680;&#24515;&#35774;&#35745;&#26159;&#21033;&#29992;&#19968;&#20010;&#32039;&#20945;&#30340;3D&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#26159;&#20174;&#31232;&#30095;&#28857;&#20113;&#20013;&#25552;&#21462;&#20986;&#26469;&#30340;&#65292;&#20351;&#29992;&#39640;&#25928;&#30340;&#28857;&#32534;&#30721;&#22120;&#12290;&#22312;&#25105;&#20204;&#28085;&#30422;&#20102;72&#20010;&#20223;&#30495;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;DP3&#20165;&#38656;&#35201;10&#20010;&#28436;&#31034;&#23601;&#21487;&#20197;&#25104;&#21151;&#22788;&#29702;&#22823;&#22810;&#25968;&#20219;&#21153;&#65292;&#24182;&#19988;&#27604;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;55.3%&#12290;&#22312;4&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;DP3&#34920;&#29616;&#20986;&#20102;&#39640;&#25104;&#21151;&#29575;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#27599;&#39033;&#20219;&#21153;&#20165;&#38656;40&#27425;&#28436;&#31034;&#21363;&#21487;&#25104;&#21151;&#29575;&#20026;85%&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03954v1 Announce Type: cross  Abstract: Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOGA&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#30005;&#36335;&#20013;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#65292;&#36890;&#36807;&#36339;&#25968;&#29305;&#24449;&#21644;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#30005;&#36335;&#32467;&#26500;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.01317</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#38754;&#21521;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#23398;&#20064;&#30340;&#36339;&#25968;&#22270;&#27880;&#24847;&#21147;&#22312;&#30005;&#36335;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOGA&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#30005;&#36335;&#20013;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#65292;&#36890;&#36807;&#36339;&#25968;&#29305;&#24449;&#21644;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#30005;&#36335;&#32467;&#26500;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#20219;&#21153;&#20013;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#26041;&#38754;&#21464;&#24471;&#27969;&#34892;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;&#22823;&#22270;&#26102;&#65292;&#23427;&#20204;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#24182;&#19988;&#23545;&#26032;&#35774;&#35745;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#36825;&#20123;&#38480;&#21046;&#20351;&#23427;&#20204;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22797;&#26434;&#30005;&#36335;&#38382;&#39064;&#26102;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HOGA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20197;&#21487;&#25193;&#23637;&#21644;&#36890;&#29992;&#30340;&#26041;&#24335;&#23398;&#20064;&#30005;&#36335;&#34920;&#31034;&#12290;HOGA&#39318;&#20808;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#38024;&#23545;&#27599;&#20010;&#33410;&#28857;&#35745;&#31639;&#36339;&#25968;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#36339;&#25968;&#29305;&#24449;&#20165;&#29992;&#20110;&#36890;&#36807;&#38376;&#25511;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#29983;&#25104;&#33410;&#28857;&#34920;&#31034;&#65292;&#35813;&#27169;&#22359;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#19981;&#21516;&#36339;&#25968;&#20043;&#38388;&#30340;&#37325;&#35201;&#29305;&#24449;&#65292;&#32780;&#19981;&#28041;&#21450;&#22270;&#25299;&#25169;&#12290;&#22240;&#27492;&#65292;HOGA&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30005;&#36335;&#20043;&#38388;&#30340;&#21508;&#31181;&#32467;&#26500;&#65292;&#24182;&#21487;&#20197;&#20197;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01317v1 Announce Type: new  Abstract: While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the e
&lt;/p&gt;</description></item><item><title>Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01232</link><description>&lt;p&gt;
Polynormer: &#22810;&#39033;&#24335;&#34920;&#36798;&#30340;&#32447;&#24615;&#26102;&#38388;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Polynormer: Polynomial-Expressive Graph Transformer in Linear Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01232
&lt;/p&gt;
&lt;p&gt;
Polynormer&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#65292;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24179;&#34913;&#20102;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#36716;&#25442;&#22120;&#65288;GTs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26550;&#26500;&#65292;&#29702;&#35770;&#19978;&#23427;&#27604;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26356;&#20855;&#34920;&#29616;&#21147;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;GT&#27169;&#22411;&#33267;&#23569;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#22240;&#27492;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#22411;&#22270;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#32447;&#24615;GTs&#65292;&#20294;&#23427;&#20204;&#22312;&#20960;&#20010;&#28909;&#38376;&#22270;&#25968;&#25454;&#38598;&#19978;&#20173;&#33853;&#21518;&#20110;GNN&#23545;&#24212;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#23454;&#38469;&#34920;&#29616;&#21147;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#20102;&#24179;&#34913;GTs&#30340;&#34920;&#29616;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Polynormer&#65292;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#22810;&#39033;&#24335;&#34920;&#36798;GT&#27169;&#22411;&#12290;Polynormer&#26500;&#24314;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#36755;&#20837;&#29305;&#24449;&#19978;&#23398;&#20064;&#39640;&#27425;&#22810;&#39033;&#24335;&#12290;&#20026;&#20102;&#20351;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#20998;&#24320;&#38598;&#25104;&#65292;&#20174;&#32780;&#20135;&#29983;&#26412;&#22320;&#21644;&#20840;&#23616;&#31561;&#21464;&#20851;&#27880;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;Polynormer&#37319;&#29992;&#20102;&#32447;&#24615;&#30340;&#23616;&#37096;&#21040;&#20840;&#23616;&#20851;&#27880;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#23545;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#20013;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00284</link><description>&lt;p&gt;
&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
A Survey of Route Recommendations: Methods, Applications, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00284
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#32508;&#36848;&#23545;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#20013;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#23637;&#31034;&#20102;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#65292;&#38543;&#30528;&#20808;&#36827;&#30340;&#20449;&#24687;&#25216;&#26415;&#37096;&#32626;&#22312;&#25972;&#20010;&#22478;&#24066;&#65292;&#22823;&#37327;&#25968;&#25454;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#27491;&#22312;&#20351;&#29616;&#20195;&#22478;&#24066;&#21457;&#23637;&#26234;&#33021;&#21270;&#12290;&#20316;&#20026;&#26234;&#33021;&#20132;&#36890;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36335;&#32447;&#25512;&#33616;&#21450;&#20854;&#24212;&#29992;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#30452;&#25509;&#24433;&#21709;&#24066;&#27665;&#30340;&#20986;&#34892;&#20064;&#24815;&#12290;&#22522;&#20110;&#22823;&#25968;&#25454;&#65288;&#21487;&#33021;&#26159;&#22810;&#27169;&#24335;&#65289;&#24320;&#21457;&#26234;&#33021;&#39640;&#25928;&#30340;&#20986;&#34892;&#36335;&#32447;&#24050;&#25104;&#20026;&#36335;&#32447;&#25512;&#33616;&#30740;&#31350;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#23545;&#22522;&#20110;&#22478;&#24066;&#35745;&#31639;&#30340;&#36335;&#32447;&#25512;&#33616;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#23427;&#20998;&#20026;&#20197;&#19979;&#19977;&#20010;&#37096;&#20998;&#65306;1&#65289;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#23545;&#22823;&#37327;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35752;&#35770;&#23427;&#20204;&#30340;&#21382;&#21490;&#20851;&#31995;&#24182;&#25581;&#31034;&#26368;&#26032;&#36827;&#23637;&#12290;2&#65289;&#24212;&#29992;&#26041;&#38754;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#37327;&#19982;&#22478;&#24066;&#35745;&#31639;&#22330;&#26223;&#20013;&#36335;&#32447;&#25512;&#33616;&#30456;&#20851;&#30340;&#26032;&#24212;&#29992;&#12290;3&#65289;&#25105;&#20204;&#36842;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00284v1 Announce Type: new  Abstract: Nowadays, with advanced information technologies deployed citywide, large data volumes and powerful computational resources are intelligentizing modern city development. As an important part of intelligent transportation, route recommendation and its applications are widely used, directly influencing citizens` travel habits. Developing smart and efficient travel routes based on big data (possibly multi-modal) has become a central challenge in route recommendation research. Our survey offers a comprehensive review of route recommendation work based on urban computing. It is organized by the following three parts: 1) Methodology-wise. We categorize a large volume of traditional machine learning and modern deep learning methods. Also, we discuss their historical relations and reveal the edge-cutting progress. 2) Application\-wise. We present numerous novel applications related to route commendation within urban computing scenarios. 3) We di
&lt;/p&gt;</description></item><item><title>DS-Agent&#26159;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#20013;&#28789;&#27963;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#25345;&#32493;&#25913;&#21892;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.17453</link><description>&lt;p&gt;
DS-Agent&#65306;&#36890;&#36807;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26696;&#20363;&#25512;&#29702;&#33021;&#21147;&#23454;&#29616;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17453
&lt;/p&gt;
&lt;p&gt;
DS-Agent&#26159;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#20013;&#28789;&#27963;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#25345;&#32493;&#25913;&#21892;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#20197;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#29702;&#35299;&#20219;&#21153;&#35201;&#27714;&#65292;&#28982;&#21518;&#26500;&#24314;&#21644;&#35757;&#32451;&#26368;&#21512;&#36866;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;LLM&#20195;&#29702;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#29983;&#25104;&#19981;&#21512;&#29702;&#30340;&#23454;&#39564;&#35745;&#21010;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DS-Agent&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#30340;&#26032;&#39062;&#33258;&#21160;&#21270;&#26694;&#26550;&#12290;&#22312;&#24320;&#21457;&#38454;&#27573;&#65292;DS-Agent&#36981;&#24490;CBR&#26694;&#26550;&#26469;&#26500;&#24314;&#33258;&#21160;&#36845;&#20195;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#28789;&#27963;&#21033;&#29992;&#26469;&#33258;Kaggle&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#20419;&#36827;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;DS-Agent&#23454;&#29616;&#20102;&#19968;&#20010;&#20302;&#36164;&#28304;&#37096;&#32626;&#38454;&#27573;&#65292;&#37319;&#29992;&#31616;&#21270;&#30340;CBR&#33539;&#20363;&#26469;&#36866;&#24212;&#24320;&#21457;&#38454;&#27573;&#25104;&#21151;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#36827;&#34892;&#30452;&#25509;&#20195;&#30721;&#29983;&#25104;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17453v1 Announce Type: new  Abstract: In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12289;&#31163;&#31574;&#30053;&#23398;&#20064;&#21644;&#33258;&#20030;&#30340;&#8220;&#33268;&#21629;&#19977;&#36830;&#8221;&#22330;&#26223;&#20013;&#30340;&#22810;&#27493;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#37319;&#26679;&#26102;&#38388;&#36328;&#24230; n &#36275;&#22815;&#22823;&#26102;&#36825;&#20123;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#26377;&#24847;&#20041;&#30340;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.15781</link><description>&lt;p&gt;
&#23545;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#31163;&#31574;&#30053;&#22810;&#27493;TD&#23398;&#20064;&#31639;&#27861;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15781
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12289;&#31163;&#31574;&#30053;&#23398;&#20064;&#21644;&#33258;&#20030;&#30340;&#8220;&#33268;&#21629;&#19977;&#36830;&#8221;&#22330;&#26223;&#20013;&#30340;&#22810;&#27493;TD&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#37319;&#26679;&#26102;&#38388;&#36328;&#24230; n &#36275;&#22815;&#22823;&#26102;&#36825;&#20123;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#26377;&#24847;&#20041;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#12289;&#31163;&#31574;&#30053;&#23398;&#20064;&#21644;&#33258;&#20030;&#30340;&#8220;&#33268;&#21629;&#19977;&#36830;&#8221;&#22330;&#26223;&#20013;&#30340;&#22810;&#27493;TD&#23398;&#20064;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#37319;&#26679;&#26102;&#38388;&#36328;&#24230; n &#36275;&#22815;&#22823;&#26102;&#65292;n&#27493;TD&#23398;&#20064;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#35299;&#12290;&#35813;&#35770;&#25991;&#20998;&#20026;&#20004;&#37096;&#20998;&#12290;&#31532;&#19968;&#37096;&#20998;&#20840;&#38754;&#30740;&#31350;&#20102;&#27169;&#22411;&#22522;&#30784;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#22522;&#26412;&#24615;&#36136;&#65292;&#21253;&#25324;&#25237;&#24433;&#20540;&#36845;&#20195;&#12289;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21644;&#25511;&#21046;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#34987;&#35270;&#20026;&#21407;&#22411;&#30830;&#23450;&#24615;&#31639;&#27861;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#26080;&#27169;&#22411;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403; n &#36275;&#22815;&#22823;&#26102;&#65292;&#36825;&#20123;&#31639;&#27861;&#20250;&#25910;&#25947;&#21040;&#26377;&#24847;&#20041;&#30340;&#35299;&#12290;&#26681;&#25454;&#36825;&#20123;&#21457;&#29616;&#65292;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;n&#27493;TD&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15781v1 Announce Type: cross  Abstract: This paper analyzes multi-step TD-learning algorithms within the `deadly triad' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that n-step TD-learning algorithms converge to a solution as the sampling horizon n increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, and the control theoretic approach, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when n is sufficiently large. Based on these findings, two n-step TD-learning algorithms are proposed and analyzed, whi
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#24191;&#21578;&#31454;&#26631;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20256;&#32479;RL&#31639;&#27861;&#22312;&#22312;&#32447;&#29615;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.15102</link><description>&lt;p&gt;
&#36712;&#36857;&#24335;&#36845;&#20195;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#31454;&#26631;
&lt;/p&gt;
&lt;p&gt;
Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15102
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24191;&#21578;&#31454;&#26631;&#20013;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20256;&#32479;RL&#31639;&#27861;&#22312;&#22312;&#32447;&#29615;&#22659;&#19979;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#24191;&#21578;&#20013;&#65292;&#24191;&#21578;&#20027;&#21442;&#19982;&#24191;&#21578;&#31454;&#25293;&#20197;&#33719;&#21462;&#24191;&#21578;&#26426;&#20250;&#65292;&#36890;&#24120;&#26159;&#36890;&#36807;&#38656;&#27714;&#26041;&#24179;&#21488;(DSPs)&#25552;&#20379;&#30340;&#33258;&#21160;&#31454;&#26631;&#24037;&#20855;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#31454;&#26631;&#31639;&#27861;&#36890;&#24120;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;RL&#30340;&#33258;&#21160;&#31454;&#26631;&#31574;&#30053;&#26159;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#37096;&#32626;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#21487;&#20197;&#24182;&#34892;&#37096;&#32626;&#22810;&#20010;&#33258;&#21160;&#31454;&#26631;&#20195;&#29702;&#20197;&#25910;&#38598;&#22823;&#37327;&#20132;&#20114;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#21033;&#29992;&#31163;&#32447;RL&#31639;&#27861;&#35757;&#32451;&#26032;&#31574;&#30053;&#12290;&#35757;&#32451;&#21518;&#30340;&#31574;&#30053;&#38543;&#21518;&#21487;&#20197;&#37096;&#32626;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#36845;&#20195;&#35757;&#32451;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#36845;&#20195;&#31163;&#32447;RL&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31181;&#36845;&#20195;&#31163;&#32447;RL&#26694;&#26550;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#20854;&#26681;&#28304;&#22312;&#20110;&#30001;&#20110;&#20869;&#22312;&#21407;&#22240;&#32780;&#23548;&#33268;&#30340;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#20302;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15102v1 Announce Type: cross  Abstract: In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ reinforcement learning (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in simulation, leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inhe
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2402.10100</link><description>&lt;p&gt;
&#35843;&#35856;&#65306;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#24494;&#35843;&#20043;&#21069;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#23545;&#20020;&#24202;&#25968;&#25454;&#30340;&#24433;&#21709;&#36739;&#22909;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#21487;&#20197;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#38480;&#21046;&#26465;&#20214;&#26159;&#20197;&#21453;&#26144;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#25910;&#38598;&#30340;&#23567;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21253;&#25324;DenseNet&#21644;ConvNeXt&#22312;&#20869;&#30340;CNN&#27169;&#22411;&#65292;&#20197;&#21450;ViT&#12289;SWIN&#21644;AST&#31561;&#36716;&#25442;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35832;&#22914;YAMNet&#21644;VGGish&#30340;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#20020;&#24202;&#25968;&#25454;&#19978;&#24494;&#35843;&#20043;&#21069;&#65292;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#20174;&#21330;&#20013;&#24739;&#32773;&#20013;&#26032;&#25910;&#38598;&#20102;&#20004;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#24739;&#32773;&#38899;&#39057;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#21457;&#29616;&#22522;&#20110;&#23427;&#20204;&#20174;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;RGB&#21644;&#28784;&#24230;&#35889;&#22270;&#36716;&#25442;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CNN&#27169;&#22411;&#22312;&#23567;&#25968;&#25454;&#38598;&#29615;&#22659;&#20013;&#21487;&#20197;&#19982;&#36716;&#25442;&#27169;&#22411;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#65292;&#20854;&#20013;DenseNet-Contrastive&#21644;AST&#27169;&#22411;&#34920;&#29616;&#31361;&#20986;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#36741;&#21161;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#38382;&#39064;&#65292;&#20998;&#21035;&#20272;&#35745;&#30452;&#25509;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#36947;&#12289;&#21453;&#23556;&#36890;&#20449;&#20449;&#36947;&#21644;&#21453;&#23556;&#24863;&#30693;&#20449;&#36947;&#65292;&#20197;&#24212;&#23545;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#30340;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#19981;&#36275;&#21644;&#24863;&#30693;&#19982;&#36890;&#20449;&#20449;&#21495;&#20043;&#38388;&#30340;&#20114;&#30456;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2402.09441</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#36741;&#21161;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#30340;&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep-Learning Channel Estimation for IRS-Assisted Integrated Sensing and Communication System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#36741;&#21161;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#32806;&#38382;&#39064;&#65292;&#20998;&#21035;&#20272;&#35745;&#30452;&#25509;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#36947;&#12289;&#21453;&#23556;&#36890;&#20449;&#20449;&#36947;&#21644;&#21453;&#23556;&#24863;&#30693;&#20449;&#36947;&#65292;&#20197;&#24212;&#23545;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#30340;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#19981;&#36275;&#21644;&#24863;&#30693;&#19982;&#36890;&#20449;&#20449;&#21495;&#20043;&#38388;&#30340;&#20114;&#30456;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20851;&#27880;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#36741;&#21161;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;&#30001;&#20110;&#34987;&#21160;&#26234;&#33021;&#21453;&#23556;&#34920;&#38754;&#32570;&#20047;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#31995;&#32479;&#20013;&#23384;&#22312;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#21495;&#20043;&#38388;&#30340;&#20114;&#30456;&#24178;&#25200;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#31532;&#19968;&#38454;&#27573;&#20272;&#35745;&#30452;&#25509;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#36947;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#20272;&#35745;&#21453;&#23556;&#36890;&#20449;&#20449;&#36947;&#65292;&#22312;&#31532;&#19977;&#38454;&#27573;&#20272;&#35745;&#21453;&#23556;&#24863;&#30693;&#20449;&#36947;&#12290;&#25152;&#25552;&#20986;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#19981;&#21516;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#36827;&#34892;&#20449;&#36947;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09441v1 Announce Type: cross  Abstract: Integrated sensing and communication (ISAC), and intelligent reflecting surface (IRS) are envisioned as revolutionary technologies to enhance spectral and energy efficiencies for next wireless system generations. For the first time, this paper focuses on the channel estimation problem in an IRS-assisted ISAC system. This problem is challenging due to the lack of signal processing capacity in passive IRS, as well as the presence of mutual interference between sensing and communication (SAC) signals in ISAC systems. A three-stage approach is proposed to decouple the estimation problem into sub-ones, including the estimation of the direct SAC channels in the first stage, reflected communication channel in the second stage, and reflected sensing channel in the third stage. The proposed three-stage approach is based on a deep-learning framework, which involves two different convolutional neural network (CNN) architectures to estimate the ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26497;&#38480;&#23398;&#20064;&#26426;&#30340;&#26234;&#33021;&#21453;&#23556;&#38754;&#36741;&#21161;&#22810;&#29992;&#25143;ISAC&#31995;&#32479;&#30340;&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20272;&#35745;&#38382;&#39064;&#20998;&#35299;&#25104;&#23376;&#38382;&#39064;&#26469;&#35299;&#20915;&#20102;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#21495;&#24178;&#25200;&#20197;&#21450;&#34987;&#21160;&#24335;IRS&#32570;&#20047;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#20302;&#25104;&#26412;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23545;SAC&#20449;&#36947;&#21644;&#19979;&#34892;&#36890;&#20449;&#20449;&#36947;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.09440</link><description>&lt;p&gt;
&#22522;&#20110;&#26497;&#38480;&#23398;&#20064;&#26426;&#30340;&#26234;&#33021;&#21453;&#23556;&#38754;&#36741;&#21161;&#22810;&#29992;&#25143;ISAC&#31995;&#32479;&#30340;&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Extreme Learning Machine-based Channel Estimation in IRS-Assisted Multi-User ISAC System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26497;&#38480;&#23398;&#20064;&#26426;&#30340;&#26234;&#33021;&#21453;&#23556;&#38754;&#36741;&#21161;&#22810;&#29992;&#25143;ISAC&#31995;&#32479;&#30340;&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20272;&#35745;&#38382;&#39064;&#20998;&#35299;&#25104;&#23376;&#38382;&#39064;&#26469;&#35299;&#20915;&#20102;&#24863;&#30693;&#21644;&#36890;&#20449;&#20449;&#21495;&#24178;&#25200;&#20197;&#21450;&#34987;&#21160;&#24335;IRS&#32570;&#20047;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#20302;&#25104;&#26412;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#23545;SAC&#20449;&#36947;&#21644;&#19979;&#34892;&#36890;&#20449;&#20449;&#36947;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26234;&#33021;&#21453;&#23556;&#38754;&#65288;IRS&#65289;&#36741;&#21161;&#30340;&#22810;&#29992;&#25143;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#24050;&#32463;&#34987;&#30740;&#31350;&#20197;&#25552;&#20379;&#39640;&#39057;&#35889;&#21644;&#33021;&#37327;&#26377;&#25928;&#24615;&#20256;&#36755;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;IRS&#36741;&#21161;&#30340;&#22810;&#29992;&#25143;ISAC&#31995;&#32479;&#12290;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#65292;&#20272;&#35745;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;SAC&#65289;&#20449;&#21495;&#30456;&#20114;&#24178;&#25200;&#65292;&#34987;&#21160;&#24335;&#30340;IRS&#32570;&#20047;&#20449;&#21495;&#22788;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#23558;&#25972;&#20307;&#20272;&#35745;&#38382;&#39064;&#36880;&#27493;&#36716;&#21270;&#20026;&#23376;&#38382;&#39064;&#65292;&#20381;&#27425;&#21253;&#25324;&#30452;&#25509;&#21644;&#21453;&#23556;&#20449;&#36947;&#30340;&#20272;&#35745;&#12290;&#22522;&#20110;&#27492;&#26041;&#26696;&#65292;ISAC&#22522;&#31449;&#65288;BS&#65289;&#20272;&#35745;&#19982;&#30446;&#26631;&#21644;&#19978;&#34892;&#29992;&#25143;&#30456;&#20851;&#30340;&#25152;&#26377;SAC&#20449;&#36947;&#65292;&#32780;&#27599;&#20010;&#19979;&#34892;&#29992;&#25143;&#21333;&#29420;&#20272;&#35745;&#19979;&#34892;&#36890;&#20449;&#20449;&#36947;&#12290;&#32771;&#34385;&#21040;ISAC&#22522;&#31449;&#21644;&#19979;&#34892;&#29992;&#25143;&#30340;&#20302;&#25104;&#26412;&#38656;&#27714;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09440v1 Announce Type: cross  Abstract: Multi-user integrated sensing and communication (ISAC) assisted by intelligent reflecting surface (IRS) has been recently investigated to provide a high spectral and energy efficiency transmission. This paper proposes a practical channel estimation approach for the first time to an IRS-assisted multiuser ISAC system. The estimation problem in such a system is challenging since the sensing and communication (SAC) signals interfere with each other, and the passive IRS lacks signal processing ability. A two-stage approach is proposed to transfer the overall estimation problem into sub-ones, successively including the direct and reflected channels estimation. Based on this scheme, the ISAC base station (BS) estimates all the SAC channels associated with the target and uplink users, while each downlink user estimates the downlink communication channels individually. Considering a low-cost demand of the ISAC BS and downlink users, the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;IRS&#36741;&#21161;&#30340;ISAC&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09439</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#21453;&#23556;&#24335;&#38754;&#36741;&#21161;ISAC&#31995;&#32479;&#30340;&#20449;&#36947;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep-Learning-Based Channel Estimation for IRS-Assisted ISAC System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#22312;IRS&#36741;&#21161;&#30340;ISAC&#31995;&#32479;&#20013;&#35299;&#20915;&#20102;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32508;&#21512;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#20197;&#21450;&#26234;&#33021;&#21453;&#23556;&#24335;&#38754;&#65288;IRS&#65289;&#34987;&#35270;&#20026;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;IRS&#36741;&#21161;&#30340;ISAC&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#20272;&#35745;&#35813;&#31995;&#32479;&#20013;&#30340;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;S&amp;C&#65289;&#20449;&#36947;&#12290;&#32771;&#34385;&#21040;S&amp;C&#20449;&#36947;&#30340;&#19981;&#21516;&#20256;&#25773;&#29615;&#22659;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26550;&#26500;&#26469;&#23454;&#29616;&#27492;&#26694;&#26550;&#12290;&#31532;&#19968;&#20010;DNN&#34987;&#35774;&#35745;&#22312;ISAC&#22522;&#31449;&#19978;&#29992;&#20110;&#20272;&#35745;&#24863;&#30693;&#20449;&#36947;&#65292;&#32780;&#31532;&#20108;&#20010;DNN&#26550;&#26500;&#21017;&#34987;&#20998;&#37197;&#32473;&#27599;&#20010;&#19979;&#34892;&#29992;&#25143;&#35774;&#22791;&#29992;&#20110;&#20272;&#35745;&#20854;&#36890;&#20449;&#20449;&#36947;&#12290;&#27492;&#22806;&#65292;&#31934;&#24515;&#35774;&#35745;&#20102;&#29992;&#20110;&#35757;&#32451;DNN&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21508;&#31181;&#20449;&#22122;&#27604;&#26465;&#20214;&#19979;&#30340;&#22522;&#20934;&#26041;&#26696;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09439v1 Announce Type: cross  Abstract: Integrated sensing and communication (ISAC) and intelligent reflecting surface (IRS) are viewed as promising technologies for future generations of wireless networks. This paper investigates the channel estimation problem in an IRS-assisted ISAC system. A deep-learning framework is proposed to estimate the sensing and communication (S&amp;C) channels in such a system. Considering different propagation environments of the S&amp;C channels, two deep neural network (DNN) architectures are designed to realize this framework. The first DNN is devised at the ISAC base station for estimating the sensing channel, while the second DNN architecture is assigned to each downlink user equipment to estimate its communication channel. Moreover, the input-output pairs to train the DNNs are carefully designed. Simulation results show the superiority of the proposed estimation approach compared to the benchmark scheme under various signal-to-noise ratio conditi
&lt;/p&gt;</description></item><item><title>EcoVal&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23454;&#29992;&#22320;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.09288</link><description>&lt;p&gt;
EcoVal:&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EcoVal: An Efficient Data Valuation Framework for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09288
&lt;/p&gt;
&lt;p&gt;
EcoVal&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23454;&#29992;&#22320;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20013;&#37327;&#21270;&#25968;&#25454;&#30340;&#20215;&#20540;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#20513;&#35758;&#20013;&#20570;&#20986;&#26356;&#20855;&#25112;&#30053;&#24847;&#20041;&#30340;&#20915;&#31574;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;Shapley&#20540;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#22312;&#35745;&#31639;&#26041;&#38754;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#37325;&#22797;&#35757;&#32451;&#27169;&#22411;&#25165;&#33021;&#33719;&#24471;Shapley&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;EcoVal&#65292;&#20197;&#24555;&#36895;&#23454;&#29992;&#30340;&#26041;&#24335;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#22788;&#29702;&#29420;&#31435;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;&#26159;&#30830;&#23450;&#31867;&#20284;&#30340;&#25968;&#25454;&#28857;&#31751;&#30340;&#20215;&#20540;&#12290;&#36825;&#20010;&#20215;&#20540;&#36827;&#19968;&#27493;&#22312;&#25152;&#26377;&#25104;&#21592;&#31751;&#28857;&#20043;&#38388;&#20256;&#25773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#26469;&#30830;&#23450;&#25972;&#20307;&#25968;&#25454;&#20215;&#20540;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#24314;&#27169;&#20026;&#8220;&#29983;&#20135;&#20989;&#25968;&#8221;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09288v1 Announce Type: new Abstract: Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \textit{production function}, a concept which is po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;3D&#28857;&#20113;&#19978;&#36827;&#34892;&#31232;&#30095;&#21367;&#31215;&#30340;GPU&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28857;&#20113;&#30340;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07710</link><description>&lt;p&gt;
&#22522;&#20110;CUDA&#30340;GPU&#20248;&#21270;&#31232;&#30095;&#21367;&#31215;&#22312;3D&#28857;&#20113;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;3D&#28857;&#20113;&#19978;&#36827;&#34892;&#31232;&#30095;&#21367;&#31215;&#30340;GPU&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28857;&#20113;&#30340;&#31232;&#30095;&#24615;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24212;&#29992;&#26174;&#33879;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#23427;&#20204;&#24050;&#32463;&#25104;&#20026;&#28041;&#21450;&#32467;&#26500;&#21270;&#26684;&#32593;&#25968;&#25454;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#22914;&#22270;&#20687;&#20998;&#26512;&#21644;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LiDAR&#21644;3D&#20256;&#24863;&#22120;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#20351;&#29992;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23545;3D&#28857;&#20113;&#30340;&#20998;&#26512;&#38656;&#27714;&#20063;&#22686;&#21152;&#20102;&#12290;&#21033;&#29992;3D&#28857;&#20113;&#22312;&#21253;&#25324;&#29289;&#20307;&#35782;&#21035;&#21644;&#20998;&#21106;&#22312;&#20869;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19977;&#32500;&#29615;&#22659;&#20013;&#20107;&#29289;&#30340;&#31354;&#38388;&#25551;&#36848;&#12290;&#19982;&#29031;&#29255;&#19981;&#21516;&#65292;&#28857;&#20113;&#20855;&#26377;&#31232;&#30095;&#24615;&#21644;&#32570;&#20047;&#35268;&#21017;&#30340;&#26684;&#32593;&#65292;&#22240;&#27492;&#23384;&#22312;&#30528;&#29420;&#29305;&#30340;&#22788;&#29702;&#21644;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a significant increase in the utilization of deep learning methods, particularly convolutional neural networks (CNNs), which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing. Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds. The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment. In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues.
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06716</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06716
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23427;&#20204;&#25658;&#24102;&#30528;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#27169;&#24335;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#34920;&#31034;&#23398;&#20064;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#20869;&#22312;&#30340;&#21160;&#24577;&#24615;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DGNNs&#23637;&#31034;&#20102;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;DGIB&#65289;&#26694;&#26550;&#26469;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#20511;&#21161;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#21407;&#29702;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#26399;&#26395;&#30340;&#26368;&#20248;&#34920;&#31034;&#24212;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#65288;MSC&#65289;&#26465;&#20214;&#12290;&#20026;&#20102;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#21644;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;DGIB&#36845;&#20195;&#22320;&#24341;&#23548;&#21644;&#25913;&#36827;&#36890;&#36807;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#12290;&#20026;&#20102;&#28385;&#36275;MSC&#26465;&#20214;&#65292;&#25105;&#20204;&#23558;&#25972;&#20307;IB&#30446;&#26631;&#20998;&#35299;&#20026;DGIB$_{MS}$&#21644;DGIB$_C$&#65292;&#20854;&#20013;DGIB$_{MS}$&#36890;&#36947;&#30340;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
&lt;/p&gt;</description></item><item><title>&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;&#26159;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.05964</link><description>&lt;p&gt;
&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey on Transformer Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05964
&lt;/p&gt;
&lt;p&gt;
&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;&#26159;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#20013;&#25198;&#28436;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#26159;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#37492;&#20110;Transformer&#30340;&#29420;&#29305;&#26550;&#26500;&#65292;&#20855;&#26377;&#20132;&#26367;&#30340;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#27169;&#22359;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#21387;&#32553;&#25216;&#26415;&#12290;&#36825;&#20123;&#21387;&#32553;&#26041;&#27861;&#30340;&#25928;&#29575;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#22411;&#27169;&#22411;&#24448;&#24448;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#35843;&#30740;&#25552;&#20379;&#20102;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large models based on the Transformer architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). Model compression methods reduce their memory and computational cost, which is a necessary step to implement the transformer models on practical devices. Given the unique architecture of transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large models on the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design. In each category, we discuss compression methods
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.05713</link><description>&lt;p&gt;
&#26126;&#26126;&#23601;&#22312;&#30524;&#21069;&#65306;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#36827;&#34892;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#30340;&#26631;&#31614;&#27745;&#26579;&#25915;&#20987;&#26469;&#30772;&#22351;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#20154;&#32676;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#21095;&#23545;&#24369;&#21183;&#24739;&#32773;&#32676;&#20307;&#30340;&#20020;&#24202;&#20559;&#35265;&#30340;&#39118;&#38505;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#25991;&#29486;&#20027;&#35201;&#20851;&#27880;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#23637;&#31034;&#30340;&#20559;&#35265;&#30340;&#37327;&#21270;&#65292;&#20294;&#38024;&#23545;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#20197;&#21450;&#20854;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38024;&#23545;&#20154;&#21475;&#32479;&#35745;&#23398;&#26631;&#31614;&#30340;&#27602;&#21270;&#25915;&#20987;&#21487;&#20197;&#21521;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24341;&#20837;&#23545;&#25239;&#24615;&#30340;&#35786;&#26029;&#19981;&#36275;&#20559;&#35265;&#65292;&#24182;&#22312;&#19981;&#24433;&#21709;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#23545;&#34987;&#20302;&#20272;&#32676;&#20307;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#22810;&#20010;&#24615;&#33021;&#25351;&#26631;&#21644;&#20154;&#21475;&#32676;&#20307;&#65288;&#22914;&#24615;&#21035;&#12289;&#24180;&#40836;&#20197;&#21450;&#20854;&#20132;&#21449;&#23376;&#32676;&#65289;&#19978;&#34920;&#26126;&#65292;&#32676;&#20307;&#23545;&#20110;&#19981;&#21487;&#26816;&#27979;&#30340;&#23545;&#25239;&#24615;&#20559;&#35265;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#19982;&#20854;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#24449;&#30452;&#25509;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;</title><link>https://arxiv.org/abs/2402.04854</link><description>&lt;p&gt;
&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#23398;&#26415;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32570;&#20047;&#30740;&#31350;&#22521;&#35757;&#30340;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#30740;&#31350;&#35843;&#26597;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#30740;&#31350;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#24456;&#38590;&#29702;&#35299;&#20182;&#20204;&#30740;&#31350;&#20027;&#39064;&#20869;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#20026;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#25552;&#20379;&#30452;&#35266;&#30340;&#24110;&#21161;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#25552;&#20379;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#24182;&#25512;&#33616;&#30456;&#20851;&#30340;&#23398;&#26415;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#20027;&#35201;&#20381;&#36182;&#20110;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#23383;&#65292;&#24120;&#24120;&#26080;&#27861;&#28165;&#26970;&#22320;&#21576;&#29616;&#22810;&#20010;&#30456;&#20851;&#35770;&#25991;&#20043;&#38388;&#30340;&#36923;&#36753;&#23618;&#27425;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20165;&#20165;&#20381;&#36182;&#20110;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35753;&#30740;&#31350;&#20154;&#21592;&#22256;&#24785;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#29305;&#23450;&#30340;&#25991;&#31456;&#12290;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#20102;&#35299;&#20851;&#20110;&#20182;&#20204;&#24076;&#26395;&#33719;&#24471;&#30340;"&#38382;&#39064;&#35299;&#20915;"&#21644;"&#38382;&#39064;&#21457;&#29616;"&#20043;&#38388;&#30340;&#35265;&#35299;&#36830;&#25509;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25903;&#25345;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#26230;&#20307;&#29983;&#25104;&#20013;&#32771;&#34385;&#20102;&#31354;&#38388;&#32676;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23481;&#26131;&#23454;&#29616;&#30340;&#31561;&#25928;&#24418;&#24335;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;DiffCSP++&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03992</link><description>&lt;p&gt;
&#31354;&#38388;&#32676;&#32422;&#26463;&#30340;&#26230;&#20307;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Space Group Constrained Crystal Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#26230;&#20307;&#29983;&#25104;&#20013;&#32771;&#34385;&#20102;&#31354;&#38388;&#32676;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23481;&#26131;&#23454;&#29616;&#30340;&#31561;&#25928;&#24418;&#24335;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;DiffCSP++&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26230;&#20307;&#26159;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#19994;&#24212;&#29992;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#26230;&#20307;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#21040;&#31354;&#38388;&#32676;&#32422;&#26463;&#65292;&#32780;&#31354;&#38388;&#32676;&#32422;&#26463;&#23545;&#20110;&#25551;&#36848;&#26230;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#35768;&#22810;&#29702;&#24819;&#29305;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#31354;&#38388;&#32676;&#32422;&#26463;&#26159;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31354;&#38388;&#32676;&#32422;&#26463;&#31616;&#21270;&#20026;&#26356;&#23481;&#26131;&#25163;&#24037;&#25918;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#31561;&#25928;&#24418;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#31354;&#38388;&#32676;&#32422;&#26463;&#20998;&#20026;&#20004;&#37096;&#20998;&#65306;&#26230;&#26684;&#30697;&#38453;&#19981;&#21464;&#23545;&#25968;&#31354;&#38388;&#30340;&#22522;&#30784;&#32422;&#26463;&#21644;&#20998;&#25968;&#22352;&#26631;&#30340;Wyckoff&#20301;&#32622;&#32422;&#26463;&#12290;&#22522;&#20110;&#36825;&#20123;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffCSP++&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#23637;&#20102;&#20043;&#21069;&#24037;&#20316;DiffCSP&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#31354;&#38388;&#32676;&#32422;&#26463;&#12290;&#23454;&#39564;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crystals are the foundation of numerous scientific and industrial applications. While various learning-based approaches have been proposed for crystal generation, existing methods seldom consider the space group constraint which is crucial in describing the geometry of crystals and closely relevant to many desirable properties. However, considering space group constraint is challenging owing to its diverse and nontrivial forms. In this paper, we reduce the space group constraint into an equivalent formulation that is more tractable to be handcrafted into the generation process. In particular, we translate the space group constraint into two parts: the basis constraint of the invariant logarithmic space of the lattice matrix and the Wyckoff position constraint of the fractional coordinates. Upon the derived constraints, we then propose DiffCSP++, a novel diffusion model that has enhanced a previous work DiffCSP by further taking space group constraint into account. Experiments on severa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02277</link><description>&lt;p&gt;
&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Bayesian Optimization via Exogenous Distribution Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#23558;&#30446;&#26631;&#21464;&#37327;&#26368;&#22823;&#21270;&#20316;&#20026;&#25805;&#20316;&#30446;&#26631;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CBO&#65289;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#25913;&#21464;&#22240;&#26524;&#32467;&#26500;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#30828;&#24178;&#39044;&#65292;&#35201;&#20040;&#24341;&#20837;&#21160;&#20316;&#33410;&#28857;&#21040;&#20869;&#29983;&#21464;&#37327;&#20013;&#65292;&#20197;&#35843;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#36825;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#36890;&#24120;&#34987;&#24573;&#30053;&#25110;&#36890;&#36807;&#26399;&#26395;&#36827;&#34892;&#36793;&#32536;&#21270;&#12290;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;&#25552;&#39640;&#20102;&#36890;&#24120;&#36890;&#36807;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#35757;&#32451;&#30340;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#22806;&#28304;&#20998;&#24067;&#23558;&#29616;&#26377;&#30340;CBO&#25193;&#23637;&#21040;&#36229;&#20986;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#30340;&#19968;&#33324;&#22240;&#26524;&#26041;&#26696;&#12290;&#24674;&#22797;&#22806;&#28304;&#21464;&#37327;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#22122;&#22768;&#25110;&#26410;&#35266;&#27979;&#21040;&#30340;&#38544;&#34255;&#21464;&#37327;&#20351;&#29992;&#26356;&#28789;&#27963;&#30340;&#20808;&#39564;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;CBO&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PVAW&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#24182;&#22312;&#20998;&#26512;RSV&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27169;&#22411;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.01987</link><description>&lt;p&gt;
&#22312;&#32447;&#36716;&#31227;&#23398;&#20064;&#29992;&#20110;RSV&#30149;&#20363;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Transfer Learning for RSV Case Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01987
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PVAW&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#24182;&#22312;&#20998;&#26512;RSV&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27169;&#22411;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#24207;&#21015;&#27969;&#34892;&#30149;&#23398;&#25968;&#25454;&#26102;&#65292;&#20250;&#38754;&#20020;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#26631;&#27880;&#20449;&#24687;&#21294;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#22810;&#28304;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#39044;&#27979;&#20307;&#31215;&#33258;&#36866;&#24212;&#21152;&#26435;&#65288;PVAW&#65289;&#12290;PVAW&#22312;&#25972;&#21512;&#27169;&#22411;&#20013;&#21019;&#36896;&#24615;&#22320;&#23454;&#29616;&#20102;&#21160;&#24577;&#21152;&#26435;&#26426;&#21046;&#65292;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#28304;&#27169;&#22411;&#21644;&#30446;&#26631;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#21644;&#36129;&#29486;&#24230;&#33258;&#21160;&#35843;&#25972;&#26435;&#37325;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21305;&#20857;&#22561;&#22823;&#23398;&#21307;&#23398;&#20013;&#24515;&#25910;&#38598;&#30340;&#22810;&#20010;&#23395;&#33410;&#30340;&#21628;&#21560;&#36947;&#21512;&#32990;&#30149;&#27602;&#65288;RSV&#65289;&#25968;&#25454;&#19978;&#24212;&#29992;PVAW&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#31361;&#20986;&#20102;&#22312;&#32447;&#36716;&#31227;&#23398;&#20064;&#22312;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning has become a pivotal technique in machine learning, renowned for its effectiveness in various real-world applications. However, a significant challenge arises when applying this approach to sequential epidemiological data, often characterized by a scarcity of labeled information. To address this challenge, we introduce Predictive Volume-Adaptive Weighting (PVAW), a novel online multi-source transfer learning method. PVAW innovatively implements a dynamic weighting mechanism within an ensemble model, allowing for the automatic adjustment of weights based on the relevance and contribution of each source and target model. We demonstrate the effectiveness of PVAW through its application in analyzing Respiratory Syncytial Virus (RSV) data, collected over multiple seasons at the University of Pittsburgh Medical Center. Our method showcases significant improvements in model performance over existing baselines, highlighting the potential of online transfer learning in handlin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17548</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#65306;&#20174;&#39046;&#20808;&#25351;&#26631;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29420;&#31435;&#20110;&#36890;&#36947;&#30340;&#26041;&#27861;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#20294;&#23427;&#20204;&#38169;&#36807;&#20102;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#23616;&#37096;&#24179;&#31283;&#30340;&#39046;&#20808;-&#28382;&#21518;&#20851;&#31995;&#65292;&#21363;&#19968;&#20123;&#28382;&#21518;&#21464;&#37327;&#22312;&#30701;&#26102;&#38388;&#20869;&#21487;&#33021;&#36981;&#24490;&#39046;&#20808;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#31181;&#36890;&#36947;&#30456;&#20851;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#39046;&#20808;&#25351;&#26631;&#25552;&#20379;&#20102;&#20808;&#36827;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#28382;&#21518;&#21464;&#37327;&#30340;&#39044;&#27979;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFT&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#39640;&#25928;&#22320;&#20272;&#35745;&#39046;&#20808;&#25351;&#26631;&#21450;&#20854;&#39046;&#20808;&#27493;&#39588;&#65292;&#28982;&#21518;&#24039;&#22937;&#22320;&#20801;&#35768;&#28382;&#21518;&#21464;&#37327;&#21033;&#29992;&#26469;&#33258;&#39046;&#20808;&#25351;&#26631;&#30340;&#20808;&#36827;&#20449;&#24687;&#12290;LIFT&#20316;&#20026;&#19968;&#20010;&#25554;&#20214;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#21327;&#20316;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LIFT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
&lt;/p&gt;</description></item><item><title>SPO&#31639;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;KL&#25955;&#24230;&#21098;&#20999;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#65292;&#22312;Atari 2600&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12289;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.16025</link><description>&lt;p&gt;
&#31616;&#21333;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simple Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16025
&lt;/p&gt;
&lt;p&gt;
SPO&#31639;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;KL&#25955;&#24230;&#21098;&#20999;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#65292;&#22312;Atari 2600&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#12289;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PPO&#65288;Proximal Policy Optimization&#65289;&#31639;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#34987;&#35748;&#20026;&#26159;TRPO&#65288;Trust Region Policy Optimization&#65289;&#31639;&#27861;&#30340;&#31616;&#21270;&#29256;&#26412;&#12290;&#28982;&#32780;&#65292;PPO&#20013;&#30340;&#27604;&#29575;&#21098;&#20999;&#25805;&#20316;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#22320;&#24378;&#21046;&#25191;&#34892;&#20449;&#20219;&#21306;&#22495;&#32422;&#26463;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21098;&#20999;&#26041;&#27861;&#65292;&#21363;Simple Policy Optimization&#65288;SPO&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#26087;&#31574;&#30053;&#21644;&#24403;&#21069;&#31574;&#30053;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#22312;Atari 2600&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;PPO&#30340;&#20027;&#27969;&#21464;&#20307;&#30456;&#27604;&#65292;SPO&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#26497;&#20302;&#30340;KL&#25955;&#24230;&#21644;&#26356;&#39640;&#30340;&#31574;&#30053;&#29109;&#65292;&#24182;&#19988;&#23545;&#32593;&#32476;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;SPO&#20445;&#25345;&#20102;&#26080;&#32422;&#26463;&#19968;&#38454;&#31639;&#27861;&#30340;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.16025v2 Announce Type: replace  Abstract: PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose Simple Policy Optimization (SPO) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. Extensive experimental results in Atari 2600 environments indicate that, compared to the mainstream variants of PPO, SPO achieves better sample efficiency, extremely low KL divergence, and higher policy entropy, and is robust to the increase in network depth or complexity. More importantly, SPO maintains the simplicity of an unconstrained first-order algorithm. Code is available at https://github.co
&lt;/p&gt;</description></item><item><title>INCPrompt&#37319;&#29992;&#33258;&#36866;&#24212;&#20851;&#38190;&#23398;&#20064;&#32773;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#32467;&#21512;&#36890;&#29992;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#26377;&#25928;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#34920;&#29616;&#20248;&#36234;&#65292;&#23545;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2401.11667</link><description>&lt;p&gt;
INCPrompt&#65306;&#38754;&#21521;&#20219;&#21153;&#30340;&#22686;&#37327;&#25552;&#31034;&#65292;&#26080;&#38656;&#37325;&#22797;&#32451;&#20064;&#30340;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free Class-incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11667
&lt;/p&gt;
&lt;p&gt;
INCPrompt&#37319;&#29992;&#33258;&#36866;&#24212;&#20851;&#38190;&#23398;&#20064;&#32773;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#32467;&#21512;&#36890;&#29992;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#26377;&#25928;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#34920;&#29616;&#20248;&#36234;&#65292;&#23545;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INCPrompt&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290; INCPrompt&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20854;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#20851;&#38190;&#23398;&#20064;&#32773;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290; &#36825;&#31181;&#29420;&#29305;&#32452;&#21512;&#23553;&#35013;&#20102;&#36328;&#20219;&#21153;&#30340;&#36890;&#29992;&#30693;&#35782;&#24182;&#32534;&#30721;&#20102;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290; &#25105;&#20204;&#22312;&#22810;&#20010;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;INCPrompt&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290; &#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#38754;&#21521;&#20219;&#21153;&#30340;&#22686;&#37327;&#25552;&#31034;&#23545;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11667v2 Announce Type: replace  Abstract: This paper introduces INCPrompt, an innovative continual learning solution that effectively addresses catastrophic forgetting. INCPrompt's key innovation lies in its use of adaptive key-learner and task-aware prompts that capture task-relevant information. This unique combination encapsulates general knowledge across tasks and encodes task-specific knowledge. Our comprehensive evaluation across multiple continual learning benchmarks demonstrates INCPrompt's superiority over existing algorithms, showing its effectiveness in mitigating catastrophic forgetting while maintaining high performance. These results highlight the significant impact of task-aware incremental prompting on continual learning performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;AI&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#39044;&#27979;&#20840;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.12462</link><description>&lt;p&gt;
&#23454;&#29616;&#31471;&#21040;&#31471;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards an end-to-end artificial intelligence driven global weather forecasting system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;AI&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#39044;&#27979;&#20840;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#23545;&#31185;&#23398;&#21644;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20110;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;AI&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#20381;&#36182;&#20110;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#31995;&#32479;&#30340;&#20998;&#26512;&#25110;&#20877;&#20998;&#26512;&#20135;&#21697;&#20316;&#20026;&#39044;&#27979;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#21021;&#22987;&#29366;&#24577;&#36890;&#24120;&#30001;&#20256;&#32479;&#25968;&#25454;&#21516;&#21270;&#32452;&#20214;&#29983;&#25104;&#65292;&#36825;&#26159;&#35745;&#31639;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#25968;&#25454;&#21516;&#21270;&#27169;&#22411;&#65292;&#21363;Adas&#65292;&#29992;&#20110;&#20840;&#29699;&#22825;&#27668;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;Adas&#19982;&#20808;&#36827;&#30340;&#22522;&#20110;AI&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65288;&#21363;FengWu&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#22522;&#20110;AI&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65306;FengWu-Adas&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Adas&#33021;&#22815;&#21516;&#21270;&#31232;&#30095;&#30340;&#20840;&#29699;&#35266;&#27979;&#25968;&#25454;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#31283;&#23450;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12462v2 Announce Type: replace-cross  Abstract: The weather forecasting system is important for science and society, and significant achievements have been made in applying artificial intelligence (AI) to medium-range weather forecasting. However, existing AI-based weather forecasting models rely on analysis or reanalysis products from the traditional numerical weather prediction (NWP) systems as initial conditions for making predictions. Initial states are typically generated by traditional data assimilation component, which is computational expensive and time-consuming. Here we present an AI-based data assimilation model, i.e., Adas, for global weather variables. And we combine Adas with the advanced AI-based weather forecasting model (i.e., FengWu) to construct the first end-to-end AI-based global weather forecasting system: FengWu-Adas. We demonstrate that Adas can assimilate sparse global observations to produce high-quality analysis, enabling the system operate stably 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#22810;&#39033;&#24335;&#35745;&#25968;&#25968;&#25454;&#30340;&#20998;&#26512;&#38656;&#27714;&#65292;&#24182;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23436;&#20840;&#33258;&#21160;&#25552;&#21462;&#20986;&#29983;&#29289;&#24847;&#20041;&#30340;&#20803;&#31614;&#21517;&#12290;</title><link>https://arxiv.org/abs/2311.16909</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#20449;&#24565;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multinomial belief networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#22810;&#39033;&#24335;&#35745;&#25968;&#25968;&#25454;&#30340;&#20998;&#26512;&#38656;&#27714;&#65292;&#24182;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23436;&#20840;&#33258;&#21160;&#25552;&#21462;&#20986;&#29983;&#29289;&#24847;&#20041;&#30340;&#20803;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#22312;&#38656;&#35201;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12289;&#22788;&#29702;&#32570;&#22833;&#35266;&#27979;&#12289;&#26679;&#26412;&#31232;&#32570;&#25110;&#25968;&#25454;&#31232;&#30095;&#26102;&#26159;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#20998;&#26512;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#39033;&#24335;&#35745;&#25968;&#25968;&#25454;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#38544;&#34255;&#21333;&#20803;&#22343;&#26381;&#20174;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21033;&#29992;&#19968;&#31995;&#21015;&#22686;&#24191;&#20851;&#31995;&#30340;&#21513;&#24067;&#26031;&#25277;&#26679;&#36807;&#31243;&#65292;&#31867;&#20284;&#20110;&#21608;-&#19995;-&#38472;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#23567;&#35268;&#27169;&#25163;&#20889;&#25968;&#23383;&#21644;&#19968;&#20010;&#22823;&#22411;&#30340;DNA&#31361;&#21464;&#30284;&#30151;&#23454;&#39564;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22914;&#20309;&#33021;&#22815;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#22320;&#25552;&#21462;&#20986;&#29983;&#29289;&#24847;&#20041;&#30340;&#20803;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16909v2 Announce Type: replace-cross  Abstract: A Bayesian approach to machine learning is attractive when we need to quantify uncertainty, deal with missing observations, when samples are scarce, or when the data is sparse. All of these commonly apply when analysing healthcare data. To address these analytical requirements, we propose a deep generative model for multinomial count data where both the weights and hidden units of the network are Dirichlet distributed. A Gibbs sampling procedure is formulated that takes advantage of a series of augmentation relations, analogous to the Zhou--Cong--Chen model. We apply the model on small handwritten digits, and a large experimental dataset of DNA mutations in cancer, and we show how the model is able to extract biologically meaningful meta-signatures in a fully data-driven way.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.13034</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#31232;&#30095;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#25311;&#21512;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#29615;&#22659;&#30340;&#25311;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#25935;&#24863;&#31232;&#30095;&#32534;&#30721;&#65292;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39640;&#25928;&#30340;&#31232;&#30095;&#26356;&#26032;&#65292;&#22312;&#24179;&#34913;&#27169;&#22411;&#23481;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#20248;&#21270;&#25311;&#21512;&#25152;&#26377;&#20808;&#21069;&#32463;&#39564;&#30340;Follow-The-Leader&#65288;FTL&#65289;&#19990;&#30028;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2401.08961</link><description>&lt;p&gt;
&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cascading Reinforcement Learning. (arXiv:2401.08961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32423;&#32852;&#36172;&#21338;&#26426;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#22312;&#32423;&#32852;&#36172;&#21338;&#26426;&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#26102;&#21051;&#65292;&#19968;&#20010;&#20195;&#29702;&#20154;&#20174;&#19968;&#32452;&#20855;&#26377;&#26410;&#30693;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#20013;&#25512;&#33616;&#19968;&#20010;&#26377;&#24207;&#30340;&#39033;&#30446;&#23376;&#38598;&#65288;&#31216;&#20026;&#39033;&#30446;&#21015;&#34920;&#65289;&#12290;&#28982;&#21518;&#65292;&#29992;&#25143;&#26816;&#26597;&#21015;&#34920;&#65292;&#24182;&#28857;&#20987;&#31532;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#39033;&#30446;&#65288;&#22914;&#26524;&#26377;&#30340;&#35805;&#65289;&#65292;&#20043;&#21518;&#65292;&#20195;&#29702;&#25910;&#21040;&#19968;&#20010;&#22870;&#21169;&#12290;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#32423;&#32852;&#36172;&#21338;&#26426;&#25991;&#29486;&#24573;&#30053;&#20102;&#29992;&#25143;&#29366;&#24577;&#65288;&#20363;&#22914;&#21382;&#21490;&#34892;&#20026;&#65289;&#23545;&#25512;&#33616;&#30340;&#24433;&#21709;&#20197;&#21450;&#20250;&#35805;&#36827;&#34892;&#36807;&#31243;&#20013;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;&#21463;&#27492;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with large attraction probabilities but also leading to good successor states. This im
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.03955</link><description>&lt;p&gt;
&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs): &#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#24378;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#30340;&#24555;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015; (TS) &#20013;&#38754;&#20020;&#30528;&#22810;&#26679;&#24615;&#21644;&#20844;&#24320;&#39044;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#21508;&#31181;&#36866;&#24212;&#30340;&#36235;&#21183;&#36880;&#28176;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#65292;&#20986;&#22855;&#22320;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38750;&#24120;&#32531;&#24930;&#19988;&#24222;&#22823;&#65288;&#22823;&#32422;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23618;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTM)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423; TSMixer &#32467;&#26500;&#30340;&#26174;&#33879;&#23567;&#22411;&#27169;&#22411;&#12290;TTM &#26159;&#39318;&#20010;&#25104;&#21151;&#24320;&#21457;&#30340;&#24494;&#22411;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#8804;100&#19975;&#20010;&#21442;&#25968;&#65289;&#65292;&#19987;&#38376;&#22312;&#20844;&#24320;TS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65288;&#20165;&#38656;4-8&#23567;&#26102;&#65289;&#65292;&#20855;&#26377;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#22312;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#22270;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#19978;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2312.10401</link><description>&lt;p&gt;
&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective. (arXiv:2312.10401v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#22312;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#22270;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#19978;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#22270;&#20013;&#25429;&#25417;&#19981;&#21464;&#20449;&#24687;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#25506;&#32034;&#22270;&#30340;&#32467;&#26500;&#29702;&#35770;&#65292;&#20174;&#32780;&#22686;&#21152;&#19981;&#21464;&#20449;&#24687;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22270;&#27169;&#22411;&#26397;&#21521;&#35299;&#37322;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#38169;&#35823;&#23398;&#20064;&#65292;&#22240;&#27492;&#23398;&#20064;&#21040;&#30340;&#22122;&#22768;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#24178;&#25200;&#20102;&#22270;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#25506;&#32034;&#22270;&#30340;&#20869;&#22312;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#22270;&#20013;&#25429;&#25417;&#32500;&#24230;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#19978;&#36848;&#36335;&#24452;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#38416;&#26126;&#32500;&#24230;&#29702;&#35770;&#23545;&#24615;&#33021;&#25913;&#36827;&#30340;&#20869;&#22312;&#26426;&#21046;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning is a general learning paradigm excelling at capturing invariant information from diverse perturbations in graphs. Recent works focus on exploring the structural rationale from graphs, thereby increasing the discriminability of the invariant information. However, such methods may incur in the mis-learning of graph models towards the interpretability of graphs, and thus the learned noisy and task-agnostic information interferes with the prediction of graphs. To this end, with the purpose of exploring the intrinsic rationale of graphs, we accordingly propose to capture the dimensional rationale from graphs, which has not received sufficient attention in the literature. The conducted exploratory experiments attest to the feasibility of the aforementioned roadmap. To elucidate the innate mechanism behind the performance improvement arising from the dimensional rationale, we rethink the dimensional rationale in graph contrastive learning from a causal perspective a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01605</link><description>&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#39044;&#27979;&#30340;&#24544;&#23454;&#21644;&#31283;&#20581;&#30340;&#26412;&#22320;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#24471;&#21040;&#20449;&#20219;&#21644;&#37096;&#32626;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#22797;&#26434;&#65292;&#24182;&#19988;&#32570;&#20047;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#19981;&#33021;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;FRED&#65288;Faithful and Robust Explainer for textual Documents&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24403;&#36825;&#20123;&#35789;&#34987;&#31227;&#38500;&#26102;&#23545;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27491;&#24335;&#30340;&#23450;&#20041;&#21644;&#23545;&#21487;&#35299;&#37322;&#20998;&#31867;&#22120;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#30830;&#31435;&#20102;FRED&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FRED&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is essential for machine learning models to be trusted and deployed in critical domains. However, existing methods for interpreting text models are often complex, lack solid mathematical foundations, and their performance is not guaranteed. In this paper, we propose FRED (Faithful and Robust Explainer for textual Documents), a novel method for interpreting predictions over text. FRED identifies key words in a document that significantly impact the prediction when removed. We establish the reliability of FRED through formal definitions and theoretical analyses on interpretable classifiers. Additionally, our empirical evaluation against state-of-the-art methods demonstrates the effectiveness of FRED in providing insights into text models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#23398;&#20064;&#21464;&#36136;&#26448;&#26009;&#65292;&#23427;&#30001;&#22522;&#20110;&#26230;&#20307;&#31649;&#30340;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#38459;&#24615;&#20803;&#20214;&#26500;&#25104;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#26426;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#38750;&#32447;&#24615;&#20219;&#21153;&#65292;&#24182;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#30340;&#22810;&#20010;&#27169;&#24335;&#65292;&#20026;&#27169;&#25311;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#30828;&#20214;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2311.00537</link><description>&lt;p&gt;
&#27809;&#26377;&#22788;&#29702;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#38750;&#32447;&#24615;&#30005;&#23376;&#21464;&#36136;&#26448;&#26009;&#20013;&#30340;&#28014;&#29616;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Without a Processor: Emergent Learning in a Nonlinear Electronic Metamaterial. (arXiv:2311.00537v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#23398;&#20064;&#21464;&#36136;&#26448;&#26009;&#65292;&#23427;&#30001;&#22522;&#20110;&#26230;&#20307;&#31649;&#30340;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#38459;&#24615;&#20803;&#20214;&#26500;&#25104;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#26426;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#38750;&#32447;&#24615;&#20219;&#21153;&#65292;&#24182;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#30340;&#22810;&#20010;&#27169;&#24335;&#65292;&#20026;&#27169;&#25311;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#30828;&#20214;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#23545;&#22823;&#35268;&#27169;&#38750;&#32447;&#24615;&#32593;&#32476;&#36827;&#34892;&#24494;&#20998;&#65292;&#36825;&#20010;&#36807;&#31243;&#32531;&#24930;&#19988;&#32791;&#33021;&#12290;&#30005;&#23376;&#23398;&#20064;&#21464;&#36136;&#26448;&#26009;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24555;&#36895;&#12289;&#39640;&#25928;&#21644;&#23481;&#38169;&#30340;&#27169;&#25311;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#65292;&#20294;&#29616;&#26377;&#23454;&#29616;&#26159;&#32447;&#24615;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#21151;&#33021;&#12290;&#36825;&#20123;&#31995;&#32479;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#26377;&#24456;&#22823;&#30340;&#21306;&#21035;&#65292;&#22240;&#27492;&#23578;&#26410;&#25506;&#32034;&#23558;&#38750;&#32447;&#24615;&#20803;&#32032;&#32435;&#20837;&#20854;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#23398;&#20064;&#21464;&#36136;&#26448;&#26009;&#8212;&#8212;&#19968;&#31181;&#22522;&#20110;&#26230;&#20307;&#31649;&#30340;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#38459;&#24615;&#20803;&#20214;&#30340;&#27169;&#25311;&#30005;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#26426;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#19981;&#21487;&#23454;&#29616;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#24322;&#25110;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#38750;&#32447;&#24615;&#23398;&#20064;&#21464;&#36136;&#26448;&#26009;&#25353;&#39034;&#24207;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#30340;&#27169;&#24335;&#65288;&#22343;&#20540;&#12289;&#26012;&#29575;&#12289;&#26354;&#29575;&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35889;&#20559;&#24046;&#12290;&#35813;&#30005;&#36335;&#23545;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard deep learning algorithms require differentiating large nonlinear networks, a process that is slow and power-hungry. Electronic learning metamaterials offer potentially fast, efficient, and fault-tolerant hardware for analog machine learning, but existing implementations are linear, severely limiting their capabilities. These systems differ significantly from artificial neural networks as well as the brain, so the feasibility and utility of incorporating nonlinear elements have not been explored. Here we introduce a nonlinear learning metamaterial -- an analog electronic network made of self-adjusting nonlinear resistive elements based on transistors. We demonstrate that the system learns tasks unachievable in linear systems, including XOR and nonlinear regression, without a computer. We find our nonlinear learning metamaterial reduces modes of training error in order (mean, slope, curvature), similar to spectral bias in artificial neural networks. The circuitry is robust to da
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20195;&#29702;&#21482;&#33021;&#36890;&#36807;&#37051;&#23621;&#20132;&#25442;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#19978;&#25805;&#20316;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#36951;&#25022;&#20540;&#26174;&#33879;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#25439;&#22833;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#21487;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2310.17385</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#65306;&#20542;&#21548;&#31038;&#21306;&#30340;&#21927;&#22179;
&lt;/p&gt;
&lt;p&gt;
Multitask Online Learning: Listen to the Neighborhood Buzz. (arXiv:2310.17385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17385
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20195;&#29702;&#21482;&#33021;&#36890;&#36807;&#37051;&#23621;&#20132;&#25442;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#19978;&#25805;&#20316;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#36951;&#25022;&#20540;&#26174;&#33879;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#25439;&#22833;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#21487;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#22312;&#32447;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#20195;&#29702;&#21482;&#33021;&#22312;&#20219;&#24847;&#36890;&#20449;&#32593;&#32476;&#19978;&#19982;&#20854;&#37051;&#23621;&#20132;&#25442;&#20449;&#24687;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#31639;&#27861;$\texttt{MT-CO}_2\texttt{OL}$&#65292;&#20854;&#36951;&#25022;&#20540;&#21462;&#20915;&#20110;&#20219;&#21153;&#30456;&#20284;&#24615;&#21644;&#32593;&#32476;&#32467;&#26500;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;$\texttt{MT-CO}_2\texttt{OL}$&#30340;&#36951;&#25022;&#20540;&#65288;&#24120;&#25968;&#38500;&#22806;&#65289;&#27704;&#36828;&#19981;&#20250;&#27604;&#20195;&#29702;&#19981;&#20849;&#20139;&#20449;&#24687;&#26102;&#33719;&#24471;&#30340;&#19978;&#30028;&#24046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24403;&#30456;&#37051;&#20195;&#29702;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#19978;&#25805;&#20316;&#26102;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#26174;&#33879;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25439;&#22833;&#20989;&#25968;&#20026;&#32447;&#24615;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#38544;&#31169;&#20445;&#25252;&#24615;&#19978;&#20570;&#21040;&#24494;&#19981;&#36275;&#36947;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multitask online learning in a setting where agents can only exchange information with their neighbors on an arbitrary communication network. We introduce $\texttt{MT-CO}_2\texttt{OL}$, a decentralized algorithm for this setting whose regret depends on the interplay between the task similarities and the network structure. Our analysis shows that the regret of $\texttt{MT-CO}_2\texttt{OL}$ is never worse (up to constants) than the bound obtained when agents do not share information. On the other hand, our bounds significantly improve when neighboring agents operate on similar tasks. In addition, we prove that our algorithm can be made differentially private with a negligible impact on the regret when the losses are linear. Finally, we provide experimental support for our theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.12942</link><description>&lt;p&gt;
&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#12290;Siegelmann&#21644;Sontag(1992)&#26366;&#32463;&#23637;&#31034;&#20102;&#20855;&#26377;&#26377;&#29702;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#20197;&#21450;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#30340;RNNs&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#28982;&#32780;&#65292;LMs&#19981;&#20165;&#23450;&#20041;&#20102;&#23383;&#31526;&#20018;&#19978;&#30340;&#21152;&#26435;&#65292;&#36824;&#23450;&#20041;&#20102;(&#38750;&#21152;&#26435;)&#35821;&#35328;&#25104;&#21592;&#20851;&#31995;&#65292;&#23545;RNN LMs&#65288;RLMs&#65289;&#30340;&#35745;&#31639;&#33021;&#21147;&#20998;&#26512;&#24212;&#35813;&#21453;&#26144;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23558;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26377;&#29702;&#26435;&#37325;&#30340;RLM&#21644;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#26469;&#27169;&#25311;&#20219;&#20309;&#27010;&#29575;&#22270;&#28789;&#26426;(PTM)&#12290;&#30001;&#20110;&#22312;&#23454;&#36341;&#20013;&#65292;RLMs&#23454;&#26102;&#24037;&#20316;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#22788;&#29702;&#19968;&#20010;&#31526;&#21495;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#19978;&#36848;&#32467;&#26524;&#20316;&#20026;RLMs&#34920;&#36798;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23637;&#31034;&#22312;&#23454;&#26102;&#35745;&#31639;&#38480;&#21046;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#30830;&#23450;&#24615;&#23454;&#26102;&#26377;&#29702;PTMs&#26469;&#25552;&#20379;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLUs&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#22320;Lipschitz&#24120;&#25968;&#35745;&#31639;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#30830;&#24615;&#39564;&#35777;&#35745;&#31639;&#19978;&#30028;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11104</link><description>&lt;p&gt;
ReLU-FNNs&#30340;&#26412;&#22320;Lipschitz&#24120;&#25968;&#35745;&#31639;&#65306;&#20351;&#29992;&#31934;&#30830;&#24615;&#39564;&#35777;&#35745;&#31639;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification. (arXiv:2310.11104v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;ReLUs&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26412;&#22320;Lipschitz&#24120;&#25968;&#35745;&#31639;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#30830;&#24615;&#39564;&#35777;&#35745;&#31639;&#19978;&#30028;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLUs&#65289;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNNs&#65289;&#30340;&#26412;&#22320;Lipschitz&#24120;&#25968;&#35745;&#31639;&#12290;&#23545;&#20110;&#30446;&#26631;&#36755;&#20837;&#65292;FNN&#30340;&#26412;&#22320;Lipschitz&#24120;&#25968;&#26159;&#34913;&#37327;&#20854;&#21487;&#38752;&#24615;&#30340;&#21512;&#29702;&#25351;&#26631;&#12290;&#36890;&#36807;&#20351;&#29992;&#25429;&#25417;ReLUs&#34892;&#20026;&#30340;&#20056;&#27861;&#22120;&#30340;&#26631;&#20934;&#36807;&#31243;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#26412;&#22320;Lipschitz&#24120;&#25968;&#30340;&#19978;&#30028;&#35745;&#31639;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#65288;SDP&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#20849;&#27491;&#20056;&#27861;&#22120;&#26469;&#20934;&#30830;&#25429;&#25417;ReLU&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#20110;&#19978;&#30028;&#35745;&#31639;&#30340;SDP&#30340;&#23545;&#20598;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24471;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#27979;&#35797;&#26469;&#30830;&#23450;&#35745;&#31639;&#19978;&#30028;&#30340;&#31934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#25968;&#30334;&#20010;ReLU&#30340;&#23454;&#38469;FNNs&#65292;&#36825;&#20123;SDP&#26159;&#26080;&#27861;&#35299;&#20915;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#36896;&#20943;&#24207;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#36755;&#20837;&#36755;&#20986;&#23646;&#24615;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with the computation of the local Lipschitz constant of feedforward neural networks (FNNs) with activation functions being rectified linear units (ReLUs). The local Lipschitz constant of an FNN for a target input is a reasonable measure for its quantitative evaluation of the reliability. By following a standard procedure using multipliers that capture the behavior of ReLUs,we first reduce the upper bound computation problem of the local Lipschitz constant into a semidefinite programming problem (SDP). Here we newly introduce copositive multipliers to capture the ReLU behavior accurately. Then, by considering the dual of the SDP for the upper bound computation, we second derive a viable test to conclude the exactness of the computed upper bound. However, these SDPs are intractable for practical FNNs with hundreds of ReLUs. To address this issue, we further propose a method to construct a reduced order model whose input-output property is identical to the original
&lt;/p&gt;</description></item><item><title>QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08041</link><description>&lt;p&gt;
QLLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#39640;&#25928;&#20302;&#20301;&#23485;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08041
&lt;/p&gt;
&lt;p&gt;
QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#25152;&#38656;&#36164;&#28304;&#36807;&#22823;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;Quantization-Aware Training&#65292;QAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;Post-Training Quantization&#65292;PTQ&#65289;&#25104;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26356;&#23454;&#38469;&#30340;&#26041;&#27861;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#29305;&#23450;&#36890;&#36947;&#20013;&#30340;&#28608;&#27963;&#31163;&#32676;&#20540;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#21518;&#35757;&#32451;&#37327;&#21270;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QLLM&#65292;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;QLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#36890;&#36947;&#25286;&#20998;&#21644;&#36890;&#36947;&#32452;&#35013;&#65292;&#22312;&#20445;&#35777;&#20302;&#20301;&#23485;&#30340;&#24773;&#20917;&#19979;&#23558;&#31163;&#32676;&#36890;&#36947;&#20998;&#35299;&#25104;&#22810;&#20010;&#23376;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#26524;&#22270;&#20808;&#39564;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21644;&#20998;&#35299;&#21160;&#24577;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#33258;&#28982;&#22320;&#35774;&#35745;&#20808;&#39564;&#65292;&#24182;&#19988;&#26681;&#25454;&#20808;&#39564;&#30693;&#35782;&#31243;&#24230;&#36830;&#25509;&#20102;&#36951;&#25022;&#29575;&#19982;&#36125;&#21494;&#26031;&#36951;&#25022;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07518</link><description>&lt;p&gt;
&#21033;&#29992;&#21518;&#39564;&#37319;&#26679;&#21644;&#22240;&#26524;&#22270;&#20808;&#39564;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning. (arXiv:2310.07518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22240;&#26524;&#22270;&#20808;&#39564;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21644;&#20998;&#35299;&#21160;&#24577;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#33258;&#28982;&#22320;&#35774;&#35745;&#20808;&#39564;&#65292;&#24182;&#19988;&#26681;&#25454;&#20808;&#39564;&#30693;&#35782;&#31243;&#24230;&#36830;&#25509;&#20102;&#36951;&#25022;&#29575;&#19982;&#36125;&#21494;&#26031;&#36951;&#25022;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#36716;&#31227;&#21160;&#24577;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#20808;&#39564;&#36890;&#24120;&#34987;&#25351;&#23450;&#20026;&#29615;&#22659;&#21464;&#37327;&#30340;&#65288;&#37096;&#20998;&#65289;&#22240;&#26524;&#22270;&#65292;&#30456;&#27604;&#23454;&#36341;&#20013;&#40635;&#28902;&#30340;&#21442;&#25968;&#20998;&#24067;&#31867;&#21035;&#25351;&#23450;&#26356;&#21152;&#33258;&#28982;&#65292;&#20363;&#22914;&#22312;&#21307;&#30103;&#27835;&#30103;&#30740;&#31350;&#20013;&#21015;&#20986;&#29983;&#29289;&#29305;&#24449;&#20043;&#38388;&#30340;&#24050;&#30693;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#21517;&#20026;C-PSRL&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#23398;&#20064;&#26356;&#39640;&#23618;&#30340;&#23436;&#25972;&#22240;&#26524;&#22270;&#21644;&#26356;&#20302;&#23618;&#23548;&#33268;&#30340;&#20998;&#35299;&#21160;&#24577;&#30340;&#21442;&#25968;&#12290;&#23545;&#20110;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#36125;&#21494;&#26031;&#36951;&#25022;&#30340;&#20998;&#26512;&#65292;&#26126;&#30830;&#22320;&#23558;&#36951;&#25022;&#29575;&#19982;&#20808;&#39564;&#30693;&#35782;&#31243;&#24230;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Posterior sampling allows the exploitation of prior knowledge of the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, a task that can be cumbersome in practice, often resulting in the choice of uninformative priors. In this work, we propose a novel posterior sampling approach in which the prior is given as a (partial) causal graph over the environment's variables. The latter is often more natural to design, such as listing known causal dependencies between biometric features in a medical treatment study. Specifically, we propose a hierarchical Bayesian procedure, called C-PSRL, simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at the lower level. For this procedure, we provide an analysis of its Bayesian regret, which explicitly connects the regret rate with the degree of prior knowledge. Our numerica
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35299;&#20915;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#32479;&#19968;&#35266;&#28857;&#65292;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.06253</link><description>&lt;p&gt;
&#38754;&#21521;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#32479;&#19968;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning. (arXiv:2310.06253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06253
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35299;&#20915;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#32479;&#19968;&#35266;&#28857;&#65292;&#23545;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#29615;&#22659;&#30340;&#26174;&#24335;&#27169;&#22411;&#20351;&#20195;&#29702;&#26356;&#33410;&#32422;&#26679;&#26412;&#12289;&#36866;&#24212;&#24615;&#26356;&#24378;&#21644;&#26356;&#26131;&#35299;&#37322;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;MBRL&#20195;&#29702;&#30340;&#33021;&#21147;&#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#20294;&#22914;&#20309;&#26368;&#22909;&#22320;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;MBRL&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#27169;&#22411;&#20197;&#23545;&#29615;&#22659;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#28982;&#21518;&#20351;&#29992;&#27169;&#22411;&#30830;&#23450;&#26368;&#26377;&#30410;&#30340;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#36890;&#24120;&#19982;&#21160;&#20316;&#36136;&#37327;&#19981;&#30456;&#20851;&#65292;&#23558;&#26681;&#26412;&#21407;&#22240;&#24402;&#32467;&#20026;&#20934;&#30830;&#30340;&#21160;&#24577;&#27169;&#22411;&#23398;&#20064;&#19982;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;&#20043;&#38388;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#12290;&#38543;&#30528;MBRL&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#30340;&#19981;&#26029;&#25104;&#29087;&#65292;&#28044;&#29616;&#20986;&#20102;&#19968;&#20123;&#20114;&#30456;&#20851;&#32852;&#30340;&#35299;&#20915;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#31867;&#21035;&#36827;&#34892;&#20102;&#28145;&#20837;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the \emph{objective mismatch} between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TacoGFN&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#36716;&#25442;&#22120;&#21644;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#23376;&#31354;&#38388;&#25506;&#32034;&#21644;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32467;&#21512;&#25913;&#21892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03223</link><description>&lt;p&gt;
TacoGFN: &#38024;&#23545;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet
&lt;/p&gt;
&lt;p&gt;
TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TacoGFN&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#36716;&#25442;&#22120;&#21644;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#23376;&#31354;&#38388;&#25506;&#32034;&#21644;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32467;&#21512;&#25913;&#21892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#26159;&#23545;&#26377;&#38480;&#25968;&#25454;&#38598;&#20013;&#30340;&#34507;&#30333;&#36136;-&#20998;&#23376;&#20998;&#24067;&#36827;&#34892;&#36817;&#20284;&#65292;&#22240;&#27492;&#22312;&#29983;&#25104;&#30340;&#20998;&#23376;&#20013;&#24456;&#38590;&#23454;&#29616;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#32467;&#21512;&#25913;&#21892;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#23558;&#21475;&#34955;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#23450;&#20041;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;TacoGFN&#65292;&#19968;&#31181;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#32780;&#19981;&#26159;&#36866;&#24212;&#39044;&#20808;&#23384;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#26041;&#27861;&#26469;&#21152;&#24555;&#23545;&#25509;&#24471;&#20998;&#35745;&#31639;&#65292;&#24182;&#25552;&#20986;&#20102;TacoGFN&#26469;&#39640;&#25928;&#22320;&#25506;&#32034;&#20998;&#23376;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#20960;&#36718;&#20027;&#21160;&#23398;&#20064;&#65292;&#20351;&#29992;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#23545;&#29983;&#25104;&#30340;&#26679;&#26412;&#36827;&#34892;&#26597;&#35810;&#65292;&#20197;&#25913;&#21892;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#25506;&#32034;&#26356;&#22810;&#30340;&#20998;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to automate the generation of drug-like compounds conditioned to specific protein pocket targets. Most current methods approximate the protein-molecule distribution of a finite dataset and, therefore struggle to generate molecules with significant binding improvement over the training dataset. We instead frame the pocket-conditioned molecular generation task as an RL problem and develop TacoGFN, a target conditional Generative Flow Network model. Our method is explicitly encouraged to generate molecules with desired properties as opposed to fitting on a pre-existing data distribution. To this end, we develop transformer-based docking score prediction to speed up docking score computation and propose TacoGFN to explore molecule space efficiently. Furthermore, we incorporate several rounds of active learning where generated samples are queried using a docking oracle to improve the docking score prediction. This approach allows us to accurately explore as much of the molecule land
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32791;&#25955;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#23548;&#20986;&#26465;&#20214;&#26469;&#20445;&#35777;&#27169;&#22411;&#30340;&#32791;&#25955;&#24615;&#36136;&#65292;&#24182;&#22312;&#20445;&#25345;&#36924;&#36817;&#33021;&#21147;&#30340;&#21516;&#26102;&#25200;&#21160;&#20559;&#24046;&#12290;&#36890;&#36807;&#29420;&#31435;&#27714;&#35299;&#36825;&#20004;&#20010;&#25200;&#21160;&#38382;&#39064;&#65292;&#24471;&#21040;&#19968;&#20010;&#20445;&#35777;&#20026;&#32791;&#25955;&#30340;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#65292;&#21516;&#26102;&#32039;&#23494;&#36924;&#36817;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16032</link><description>&lt;p&gt;
&#23398;&#20064;&#32791;&#25955;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Dissipative Neural Dynamical Systems. (arXiv:2309.16032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32791;&#25955;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#23548;&#20986;&#26465;&#20214;&#26469;&#20445;&#35777;&#27169;&#22411;&#30340;&#32791;&#25955;&#24615;&#36136;&#65292;&#24182;&#22312;&#20445;&#25345;&#36924;&#36817;&#33021;&#21147;&#30340;&#21516;&#26102;&#25200;&#21160;&#20559;&#24046;&#12290;&#36890;&#36807;&#29420;&#31435;&#27714;&#35299;&#36825;&#20004;&#20010;&#25200;&#21160;&#38382;&#39064;&#65292;&#24471;&#21040;&#19968;&#20010;&#20445;&#35777;&#20026;&#32791;&#25955;&#30340;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#65292;&#21516;&#26102;&#32039;&#23494;&#36924;&#36817;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#19968;&#20010;&#26410;&#30693;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#65292;&#20854;&#24050;&#30693;&#26159;&#32791;&#25955;&#30340;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#65292;&#20197;&#36924;&#36817;&#36825;&#20010;&#31995;&#32479;&#65292;&#24182;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#32791;&#25955;&#24615;&#36136;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#26045;&#21152;&#32791;&#25955;&#24615;&#32422;&#26463;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#27809;&#26377;&#24050;&#30693;&#30340;&#25216;&#26415;&#21487;&#20197;&#35299;&#20915;&#12290;&#26412;&#25991;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#35299;&#20915;&#20102;&#23398;&#20064;&#32791;&#25955;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#32039;&#23494;&#36924;&#36817;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23548;&#20986;&#36275;&#22815;&#30340;&#26465;&#20214;&#26469;&#25200;&#21160;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20197;&#30830;&#20445;&#32791;&#25955;&#24615;&#36136;&#65292;&#24182;&#22312;&#20445;&#25345;&#27169;&#22411;&#19982;&#38750;&#32447;&#24615;&#31995;&#32479;&#36712;&#36857;&#25311;&#21512;&#30340;&#21516;&#26102;&#25200;&#21160;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#25200;&#21160;&#38382;&#39064;&#21487;&#20197;&#29420;&#31435;&#27714;&#35299;&#65292;&#20174;&#32780;&#33719;&#24471;&#19968;&#20010;&#20445;&#35777;&#20026;&#32791;&#25955;&#30340;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#65292;&#21516;&#26102;&#32039;&#23494;&#36924;&#36817;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider an unknown nonlinear dynamical system that is known to be dissipative. The objective of this paper is to learn a neural dynamical model that approximates this system, while preserving the dissipativity property in the model. In general, imposing dissipativity constraints during neural network training is a hard problem for which no known techniques exist. In this work, we address the problem of learning a dissipative neural dynamical system model in two stages. First, we learn an unconstrained neural dynamical model that closely approximates the system dynamics. Next, we derive sufficient conditions to perturb the weights of the neural dynamical model to ensure dissipativity, followed by perturbation of the biases to retain the fit of the model to the trajectories of the nonlinear system. We show that these two perturbation problems can be solved independently to obtain a neural dynamical model that is guaranteed to be dissipative while closely approximating the nonlinear syst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#20013;&#28216;&#25103;&#26234;&#33021;&#20307;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20351;&#35757;&#32451;&#25968;&#25454;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#20998;&#24067;&#65292;&#24182;&#22312;&#22810;&#20010;3D&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24615;&#33021;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12815</link><description>&lt;p&gt;
&#29992;&#25968;&#25454;&#22686;&#24378;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#20013;&#28216;&#25103;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization in Game Agents with Data Augmentation in Imitation Learning. (arXiv:2309.12815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#20013;&#28216;&#25103;&#26234;&#33021;&#20307;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20351;&#35757;&#32451;&#25968;&#25454;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#20998;&#24067;&#65292;&#24182;&#22312;&#22810;&#20010;3D&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24615;&#33021;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#35757;&#32451;&#28216;&#25103;&#26234;&#33021;&#20307;&#21644;&#39640;&#25928;&#28216;&#25103;&#29983;&#25104;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27867;&#21270;&#33021;&#21147;&#8212;&#8212;&#22312;&#30456;&#20851;&#20294;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#30340;&#33021;&#21147;&#8212;&#8212;&#23545;&#20110;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#22411;&#21270;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#31639;&#27861;&#22312;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#37319;&#21462;&#26377;&#24847;&#20041;&#30340;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;&#22312;&#21463;&#30417;&#30563;&#23398;&#20064;&#20013;&#25968;&#25454;&#22686;&#24378;&#30340;&#25104;&#21151;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20351;&#25968;&#25454;&#38598;&#20013;&#30340;&#29366;&#24577;&#21644;&#34892;&#21160;&#20998;&#24067;&#26356;&#22909;&#22320;&#20195;&#34920;&#30495;&#23454;&#30340;&#29366;&#24577;-&#34892;&#21160;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#25968;&#25454;&#22686;&#24378;&#24212;&#29992;&#20110;&#35266;&#23519;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;3D&#29615;&#22659;&#20013;&#23545;&#36825;&#20123;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning is an effective approach for training game-playing agents and, consequently, for efficient game production. However, generalization - the ability to perform well in related but unseen scenarios - is an essential requirement that remains an unsolved challenge for game AI. Generalization is difficult for imitation learning agents because it requires the algorithm to take meaningful actions outside of the training distribution. In this paper we propose a solution to this challenge. Inspired by the success of data augmentation in supervised learning, we augment the training data so the distribution of states and actions in the dataset better represents the real state-action distribution. This study evaluates methods for combining and applying data augmentations to observations, to improve generalization of imitation learning agents. It also provides a performance benchmark of these augmentations across several 3D environments. These results demonstrate that data augmenta
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#20581;&#30340;&#31163;&#32447;&#27169;&#22411;&#23548;&#21521;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#19987;&#23478;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#20027;&#35266;&#27169;&#22411;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#21033;&#29992;&#20808;&#39564;&#20998;&#24067;&#21442;&#25968;&#21270;&#19987;&#23478;&#23545;&#29615;&#22659;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20808;&#39564;&#22320;&#35748;&#20026;&#19987;&#23478;&#23545;&#29615;&#22659;&#20855;&#26377;&#39640;&#24230;&#20934;&#30830;&#30340;&#27169;&#22411;&#26102;&#65292;&#20272;&#35745;&#30340;&#31574;&#30053;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;IRL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08571</link><description>&lt;p&gt;
&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Approach to Robust Inverse Reinforcement Learning. (arXiv:2309.08571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08571
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#31283;&#20581;&#30340;&#31163;&#32447;&#27169;&#22411;&#23548;&#21521;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#19987;&#23478;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#20027;&#35266;&#27169;&#22411;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#21033;&#29992;&#20808;&#39564;&#20998;&#24067;&#21442;&#25968;&#21270;&#19987;&#23478;&#23545;&#29615;&#22659;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20808;&#39564;&#22320;&#35748;&#20026;&#19987;&#23478;&#23545;&#29615;&#22659;&#20855;&#26377;&#39640;&#24230;&#20934;&#30830;&#30340;&#27169;&#22411;&#26102;&#65292;&#20272;&#35745;&#30340;&#31574;&#30053;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;IRL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#31163;&#32447;&#27169;&#22411;&#23548;&#21521;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;(IRL)&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#21516;&#26102;&#20272;&#35745;&#19987;&#23478;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#20027;&#35266;&#27169;&#22411;&#30340;&#29615;&#22659;&#21160;&#24577;&#65292;&#21306;&#21035;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#27169;&#22411;&#23548;&#21521;&#30340;IRL&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31867;&#20808;&#39564;&#20998;&#24067;&#26469;&#21442;&#25968;&#21270;&#19987;&#23478;&#23545;&#29615;&#22659;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#24320;&#21457;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#20272;&#35745;&#19987;&#23478;&#22870;&#21169;&#21644;&#20027;&#35266;&#21160;&#24577;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#24403;&#20808;&#39564;&#22320;&#35748;&#20026;&#19987;&#23478;&#23545;&#29615;&#22659;&#20855;&#26377;&#39640;&#24230;&#20934;&#30830;&#30340;&#27169;&#22411;&#26102;&#65292;&#20272;&#35745;&#30340;&#31574;&#30053;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;MuJoCo&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;IRL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a Bayesian approach to offline model-based inverse reinforcement learning (IRL). The proposed framework differs from existing offline model-based IRL approaches by performing simultaneous estimation of the expert's reward function and subjective model of environment dynamics. We make use of a class of prior distributions which parameterizes how accurate the expert's model of the environment is to develop efficient algorithms to estimate the expert's reward and subjective dynamics in high-dimensional settings. Our analysis reveals a novel insight that the estimated policy exhibits robust performance when the expert is believed (a priori) to have a highly accurate model of the environment. We verify this observation in the MuJoCo environments and show that our algorithms outperform state-of-the-art offline IRL algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;&#20013;&#22797;&#21512;&#23376;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#26500;&#36896;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#65292;&#30740;&#31350;&#20854;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#35777;&#26126;&#20102;&#22797;&#21512;&#23376;&#25910;&#25947;&#26102;&#23545;&#24212;&#30340;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20250;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23637;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.07169</link><description>&lt;p&gt;
&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#39057;&#29575;&#25910;&#25947;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Frequency Convergence of Complexon Shift Operators. (arXiv:2309.07169v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;&#20013;&#22797;&#21512;&#23376;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#26500;&#36896;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#65292;&#30740;&#31350;&#20854;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#35777;&#26126;&#20102;&#22797;&#21512;&#23376;&#25910;&#25947;&#26102;&#23545;&#24212;&#30340;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20250;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23637;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;(TSP)&#21033;&#29992;&#21333;&#32431;&#24418;&#22797;&#21512;&#26469;&#24314;&#27169;&#27604;&#39030;&#28857;&#21644;&#36793;&#26356;&#39640;&#38454;&#30340;&#32467;&#26500;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;TSP&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#22797;&#21512;&#23376;&#30340;&#24191;&#20041;&#39640;&#38454;&#22270;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#22797;&#21512;&#23376;&#30340;&#27010;&#24565;&#65292;&#21363;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#30340;&#26497;&#38480;[1]&#12290;&#21463;&#22270;&#31227;&#20301;&#31639;&#23376;&#30340;&#31215;&#20998;&#31639;&#23376;&#24418;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26681;&#25454;&#22797;&#21512;&#23376;&#30340;&#25152;&#26377;&#21487;&#33021;&#23610;&#23544;&#30340;&#32452;&#20214;&#26500;&#36896;&#20102;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;(CSO)&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;CSO&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#19968;&#31867;&#26032;&#30340;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#19968;&#20010;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#25910;&#25947;&#21040;&#19968;&#20010;&#22797;&#21512;&#23376;&#26102;&#65292;&#30456;&#24212;&#30340;CSO&#30340;&#29305;&#24449;&#20540;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#26263;&#31034;&#20102;&#22312;&#22823;&#22411;&#21333;&#32431;&#24418;&#22797;&#21512;&#25110;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#19978;&#30340;&#23398;&#20064;&#21487;&#36716;&#31227;&#24615;&#65292;&#20174;&#32780;&#25512;&#24191;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological signal processing (TSP) utilizes simplicial complexes to model structures with higher order than vertices and edges. In this paper, we study the transferability of TSP via a generalized higher-order version of graphon, known as complexon. We recall the notion of a complexon as the limit of a simplicial complex sequence [1]. Inspired by the integral operator form of graphon shift operators, we construct a marginal complexon and complexon shift operator (CSO) according to components of all possible dimensions from the complexon. We investigate the CSO's eigenvalues and eigenvectors, and relate them to a new family of weighted adjacency matrices. We prove that when a simplicial complex sequence converges to a complexon, the eigenvalues of the corresponding CSOs converge to that of the limit complexon. These results hint at learning transferability on large simplicial complexes or simplicial complex sequences, which generalize the graphon signal processing framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#26377;&#38480;&#24046;&#20998;&#31163;&#25955;&#21270;&#23478;&#26063;&#65292;&#22312;&#38750;&#22343;&#21248;&#25193;&#25955;&#36807;&#31243;&#20013;&#36890;&#36807;&#23558;&#20108;&#32500;&#25193;&#25955;&#20998;&#35299;&#20026;&#22235;&#20010;&#19968;&#32500;&#25193;&#25955;&#24471;&#20986;&#20102;&#19968;&#20010;3 x 3&#30340;&#27169;&#26495;&#12290;&#35813;&#27169;&#26495;&#31867;&#21253;&#21547;&#19968;&#20010;&#33258;&#30001;&#21442;&#25968;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#29616;&#26377;&#31163;&#25955;&#21270;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#24314;&#31435;&#20102;&#19982;&#27169;&#26495;&#30456;&#23545;&#24212;&#30340;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#19978;&#30028;&#65292;&#24182;&#23558;&#26174;&#24335;&#26041;&#26696;&#36716;&#21270;&#20026;ResNet&#22359;&#12290;</title><link>http://arxiv.org/abs/2309.05575</link><description>&lt;p&gt;
&#19968;&#20010;&#31034;&#20363;&#39064;&#24418;&#30340;&#38750;&#22343;&#21248;&#25193;&#25955;&#27169;&#26495;: &#20174;&#31616;&#21333;&#25512;&#23548;&#21040;&#31283;&#23450;&#24615;&#35780;&#20272;&#20877;&#21040;ResNet&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations. (arXiv:2309.05575v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#26377;&#38480;&#24046;&#20998;&#31163;&#25955;&#21270;&#23478;&#26063;&#65292;&#22312;&#38750;&#22343;&#21248;&#25193;&#25955;&#36807;&#31243;&#20013;&#36890;&#36807;&#23558;&#20108;&#32500;&#25193;&#25955;&#20998;&#35299;&#20026;&#22235;&#20010;&#19968;&#32500;&#25193;&#25955;&#24471;&#20986;&#20102;&#19968;&#20010;3 x 3&#30340;&#27169;&#26495;&#12290;&#35813;&#27169;&#26495;&#31867;&#21253;&#21547;&#19968;&#20010;&#33258;&#30001;&#21442;&#25968;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#29616;&#26377;&#31163;&#25955;&#21270;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#24314;&#31435;&#20102;&#19982;&#27169;&#26495;&#30456;&#23545;&#24212;&#30340;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#19978;&#30028;&#65292;&#24182;&#23558;&#26174;&#24335;&#26041;&#26696;&#36716;&#21270;&#20026;ResNet&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#22343;&#21248;&#25193;&#25955;&#36807;&#31243;&#19982;&#25193;&#25955;&#24352;&#37327;&#22312;&#22270;&#20687;&#20998;&#26512;&#12289;&#29289;&#29702;&#23398;&#21644;&#24037;&#31243;&#23398;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25968;&#20540;&#36817;&#20284;&#23545;&#32791;&#25955;&#24615;&#20266;&#24433;&#21644;&#19982;&#26059;&#36716;&#19981;&#21464;&#24615;&#20559;&#31163;&#26377;&#24378;&#28872;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#26377;&#38480;&#24046;&#20998;&#31163;&#25955;&#21270;&#23478;&#26063;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;3 x 3&#30340;&#27169;&#26495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20108;&#32500;&#38750;&#22343;&#21248;&#25193;&#25955;&#20998;&#35299;&#20026;&#22235;&#20010;&#19968;&#32500;&#25193;&#25955;&#26469;&#25512;&#23548;&#20986;&#23427;&#12290;&#32467;&#26524;&#30340;&#27169;&#26495;&#31867;&#21253;&#21547;&#19968;&#20010;&#33258;&#30001;&#21442;&#25968;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#29616;&#26377;&#31163;&#25955;&#21270;&#12290;&#23427;&#21253;&#25324;Weickert&#31561;&#20154;(2013)&#30340;&#23436;&#25972;&#27169;&#26495;&#23478;&#26063;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#30340;&#20004;&#20010;&#21442;&#25968;&#21253;&#21547;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#27169;&#26495;&#30456;&#23545;&#24212;&#30340;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#19978;&#30028;&#12290;&#36825;&#32473;&#20986;&#20102;&#22312;&#27431;&#20960;&#37324;&#24471;&#33539;&#25968;&#20013;&#20445;&#35777;&#26174;&#24335;&#26041;&#26696;&#31283;&#23450;&#24615;&#30340;&#26102;&#38388;&#27493;&#38271;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#21521;&#20998;&#21106;&#36824;&#20801;&#35768;&#23558;&#26174;&#24335;&#26041;&#26696;&#38750;&#24120;&#33258;&#28982;&#22320;&#36716;&#21270;&#20026;ResNet&#22359;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#24211;&#23454;&#29616;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anisotropic diffusion processes with a diffusion tensor are important in image analysis, physics, and engineering. However, their numerical approximation has a strong impact on dissipative artefacts and deviations from rotation invariance. In this work, we study a large family of finite difference discretisations on a 3 x 3 stencil. We derive it by splitting 2-D anisotropic diffusion into four 1-D diffusions. The resulting stencil class involves one free parameter and covers a wide range of existing discretisations. It comprises the full stencil family of Weickert et al. (2013) and shows that their two parameters contain redundancy. Furthermore, we establish a bound on the spectral norm of the matrix corresponding to the stencil. This gives time step size limits that guarantee stability of an explicit scheme in the Euclidean norm. Our directional splitting also allows a very natural translation of the explicit scheme into ResNet blocks. Employing neural network libraries enables simple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20998;&#21106;&#26041;&#27861;MMSFormer&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MCubeS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.04001</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#29992;&#20110;&#26448;&#26009;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Multimodal Transformer for Material Segmentation. (arXiv:2309.04001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20998;&#21106;&#26041;&#27861;MMSFormer&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#22312;MCubeS&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22810;&#27169;&#24577;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;&#27169;&#24577;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#22235;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#65306;RGB&#12289;&#32447;&#24615;&#20559;&#25391;&#35282;&#65288;AoLP&#65289;&#12289;&#32447;&#24615;&#20559;&#25391;&#24230;&#65288;DoLP&#65289;&#21644;&#36817;&#32418;&#22806;&#65288;NIR&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#27169;&#24577;&#20998;&#21106;&#21464;&#25442;&#22120;&#65288;MMSFormer&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#26448;&#26009;&#20998;&#21106;&#12290;MMSFormer&#22312;&#22810;&#27169;&#24577;&#26448;&#26009;&#20998;&#21106;&#65288;MCubeS&#65289;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;52.05&#65285;&#30340;mIoU&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#30782;&#30707;&#65288;+10.4&#65285;&#65289;&#21644;&#20154;&#31867;&#65288;+9.1&#65285;&#65289;&#31867;&#19978;&#25552;&#20379;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#34701;&#21512;&#22359;&#20013;&#30340;&#19981;&#21516;&#27169;&#22359;&#23545;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20154;&#33080;&#22270;&#20687;&#21464;&#24418;&#21644;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#33080;&#21464;&#24418;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13888</link><description>&lt;p&gt;
&#20154;&#33080;&#22270;&#20687;&#30340;&#31070;&#32463;&#38544;&#24335;&#24418;&#21464;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Morphing of Face Images. (arXiv:2308.13888v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20154;&#33080;&#22270;&#20687;&#21464;&#24418;&#21644;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#33080;&#21464;&#24418;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#21464;&#24418;&#26159;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20855;&#26377;&#20247;&#22810;&#33402;&#26415;&#21644;&#21462;&#35777;&#24212;&#29992;&#12290;&#30001;&#20110;&#23039;&#24577;&#12289;&#20809;&#29031;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30340;&#21464;&#21270;&#65292;&#23427;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#36825;&#20010;&#20219;&#21153;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#30340;&#21464;&#24418;&#21644;&#26080;&#32541;&#36807;&#28193;&#30340;&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#22352;&#26631;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#20154;&#33080;&#22270;&#20687;&#30340;&#36825;&#31181;&#21464;&#24418;&#21644;&#28151;&#21512;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31163;&#25955;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26102;&#38388;&#20381;&#36182;&#30340;&#65292;&#20801;&#35768;&#23545;&#30446;&#26631;&#22270;&#20687;&#36827;&#34892;&#36830;&#32493;&#30340;&#21464;&#24418;&#21644;&#28151;&#21512;&#12290;&#22312;&#21464;&#24418;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#26102;&#38388;&#20381;&#36182;&#21464;&#24418;&#30340;&#30452;&#25509;&#21644;&#36870;&#21464;&#25442;&#12290;&#21069;&#32773;&#36127;&#36131;&#23558;&#30446;&#26631;&#22270;&#20687;&#21464;&#24418;&#20026;&#28304;&#22270;&#20687;&#65292;&#32780;&#21518;&#32773;&#21017;&#29992;&#20110;&#22312;&#30456;&#21453;&#26041;&#21521;&#36827;&#34892;&#21464;&#24418;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#21464;&#24418;&#32593;&#32476;&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face morphing is one of the seminal problems in computer graphics, with numerous artistic and forensic applications. It is notoriously challenging due to pose, lighting, gender, and ethnicity variations. Generally, this task consists of a warping for feature alignment and a blending for a seamless transition between the warped images.  We propose to leverage coordinate-based neural networks to represent such warpings and blendings of face images. During training, we exploit the smoothness and flexibility of such networks, by combining energy functionals employed in classical approaches without discretizations. Additionally, our method is time-dependent, allowing a continuous warping, and blending of the target images.  During warping inference, we need both direct and inverse transformations of the time-dependent warping. The first is responsible for morphing the target image into the source image, while the inverse is used for morphing in the opposite direction. Our neural warping sto
&lt;/p&gt;</description></item><item><title>FE-PINN&#26159;&#19968;&#31181;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#22312;&#20027;&#35757;&#32451;&#20043;&#21069;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#35299;&#20915;&#38382;&#39064;&#30340;&#27169;&#24335;&#12290;&#19982;&#20256;&#32479;PINN&#30456;&#27604;&#65292;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#35757;&#32451;&#21644;&#26356;&#39640;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.08873</link><description>&lt;p&gt;
&#29305;&#24449;&#24378;&#21270;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;FE-PINN&#65289;&#65306;&#22312;&#30446;&#26631;&#20219;&#21153;&#20043;&#21069;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task. (arXiv:2308.08873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08873
&lt;/p&gt;
&lt;p&gt;
FE-PINN&#26159;&#19968;&#31181;&#23398;&#20064;&#24213;&#23618;&#29289;&#29702;&#29305;&#24449;&#30340;&#26694;&#26550;&#65292;&#22312;&#20027;&#35757;&#32451;&#20043;&#21069;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#35299;&#20915;&#38382;&#39064;&#30340;&#27169;&#24335;&#12290;&#19982;&#20256;&#32479;PINN&#30456;&#27604;&#65292;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#35757;&#32451;&#21644;&#26356;&#39640;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#24378;&#21270;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;FE-PINN&#65289;&#30340;&#26032;&#22411;&#26080;&#25968;&#25454;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#20027;&#35757;&#32451;&#24490;&#29615;&#20043;&#21069;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#23398;&#20064;&#20219;&#20309;&#38382;&#39064;&#30340;&#24213;&#23618;&#27169;&#24335;&#12290;&#30001;&#20110;&#23384;&#22312;&#20559;&#24494;&#20998;&#27531;&#24046;&#21644;&#36793;&#30028;&#26465;&#20214;&#22343;&#26041;&#35823;&#24046;&#20004;&#20010;&#39033;&#65292;&#26222;&#36890;PINN&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#24179;&#34913;&#12290;FE-PINN&#36890;&#36807;&#21482;&#38656;&#19968;&#20998;&#38047;&#30340;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#32791;&#26102;&#25968;&#23567;&#26102;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;FE-PINN&#36890;&#36807;&#25191;&#34892;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#23436;&#25104;&#36825;&#20010;&#36807;&#31243;&#12290;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#23398;&#20064;&#26377;&#20851;&#24213;&#23618;&#29289;&#29702;&#30340;&#26377;&#29992;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#20197;&#23436;&#21892;&#35745;&#31639;&#12290;FE-PINN&#24212;&#29992;&#20110;&#19977;&#20010;&#22522;&#20934;&#38382;&#39064;&#65306;&#22278;&#26609;&#20307;&#19978;&#30340;&#27969;&#21160;&#12289;&#20108;&#32500;&#28909;&#20256;&#23548;&#20197;&#21450;&#35745;&#31639;&#20837;&#21475;&#36895;&#24230;&#30340;&#36870;&#38382;&#39064;&#12290;FE-PINN&#21487;&#20197;&#20998;&#21035;&#21152;&#36895;15&#20493;&#12289;2&#20493;&#21644;5&#20493;&#22320;&#35299;&#20915;&#27599;&#20010;&#26696;&#20363;&#12290;&#21478;&#22806;
&lt;/p&gt;
&lt;p&gt;
In this work, a new data-free framework called Feature Enforcing Physics Informed Neural Network (FE-PINN) is introduced. This framework is capable of learning the underlying pattern of any problem with low computational cost before the main training loop. The loss function of vanilla PINN due to the existence of two terms of partial differential residuals and boundary condition mean squared error is imbalanced. FE-PINN solves this challenge with just one minute of training instead of time-consuming hyperparameter tuning for loss function that can take hours. The FE-PINN accomplishes this process by performing a sequence of sub-tasks. The first sub-task learns useful features about the underlying physics. Then, the model trains on the target task to refine the calculations. FE-PINN is applied to three benchmarks, flow over a cylinder, 2D heat conduction, and an inverse problem of calculating inlet velocity. FE-PINN can solve each case with, 15x, 2x, and 5x speed up accordingly. Another
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#21644;&#27604;&#36739;&#20102;&#20808;&#21069;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20844;&#20849;&#26085;&#24535;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.08736</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#26085;&#24535;&#34920;&#31034;&#26041;&#27861;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Log Representation for Log-based Anomaly Detection. (arXiv:2308.08736v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#21644;&#27604;&#36739;&#20102;&#20808;&#21069;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20844;&#20849;&#26085;&#24535;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#26159;&#20154;&#20204;&#20102;&#35299;&#36719;&#20214;&#31995;&#32479;&#36816;&#34892;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#26469;&#28304;&#12290;&#30001;&#20110;&#29616;&#20195;&#36719;&#20214;&#26550;&#26500;&#21644;&#32500;&#25252;&#26041;&#27861;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24037;&#20316;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#12290;&#22312;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26085;&#24535;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#25991;&#26412;&#26085;&#24535;&#25968;&#25454;&#36716;&#25442;&#20026;&#25968;&#23383;&#29305;&#24449;&#21521;&#37327;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#19988;&#24517;&#19981;&#21487;&#23569;&#30340;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#23545;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#65292;&#36825;&#38480;&#21046;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#36873;&#25321;&#20854;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#20013;&#26368;&#20339;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#24182;&#27604;&#36739;&#20102;&#20808;&#21069;&#26085;&#24535;&#20998;&#26512;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20845;&#31181;&#26085;&#24535;&#34920;&#31034;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#19971;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22235;&#31181;&#20844;&#20849;&#26085;&#24535;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logs are an essential source of information for people to understand the running status of a software system. Due to the evolving modern software architecture and maintenance methods, more research efforts have been devoted to automated log analysis. In particular, machine learning (ML) has been widely used in log analysis tasks. In ML-based log analysis tasks, converting textual log data into numerical feature vectors is a critical and indispensable step. However, the impact of using different log representation techniques on the performance of the downstream models is not clear, which limits researchers and practitioners' opportunities of choosing the optimal log representation techniques in their automated log analysis workflows. Therefore, this work investigates and compares the commonly adopted log representation techniques from previous log analysis research. Particularly, we select six log representation techniques and evaluate them with seven ML models and four public log datas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#24615;&#30340;&#20648;&#22791;&#35745;&#31639;&#65292;RNNs&#21487;&#20197;&#36890;&#29992;&#36924;&#36817;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65292;&#36825;&#19968;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2308.02464</link><description>&lt;p&gt;
&#36890;&#36807;RNNs&#23454;&#29616;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#30340;&#36890;&#29992;&#36924;&#36817;&#65306;&#38543;&#26426;&#24615;&#22312;&#20648;&#22791;&#35745;&#31639;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing. (arXiv:2308.02464v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02464
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#24615;&#30340;&#20648;&#22791;&#35745;&#31639;&#65292;RNNs&#21487;&#20197;&#36890;&#29992;&#36924;&#36817;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#65292;&#36825;&#19968;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#20197;&#30456;&#23545;&#28201;&#21644;&#21644;&#26222;&#36866;&#30340;&#26465;&#20214;&#34987;&#35748;&#20026;&#26159;&#21160;&#24577;&#31995;&#32479;&#30340;&#36890;&#29992;&#36817;&#20284;&#22120;&#65292;&#20351;&#20854;&#25104;&#20026;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#30340;&#33391;&#22909;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;RNNs&#36890;&#24120;&#21463;&#21040;&#26631;&#20934;RNN&#35757;&#32451;&#20013;&#26799;&#24230;&#28040;&#22833;&#21644;&#29190;&#28856;&#30340;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#20648;&#22791;&#35745;&#31639;(RC)&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;RNN&#65292;&#20854;&#20013;&#30340;&#24490;&#29615;&#26435;&#37325;&#26159;&#38543;&#26426;&#21270;&#24182;&#30041;&#22312;&#26410;&#32463;&#35757;&#32451;&#29366;&#24577;&#65292;&#23427;&#34987;&#24341;&#20837;&#29992;&#20110;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#22312;&#35832;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26080;&#32447;&#36890;&#20449;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#26679;&#26412;&#26497;&#20854;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#36825;&#31181;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#30340;&#29702;&#35770;&#22522;&#30784;&#24182;&#26410;&#20197;&#30456;&#21516;&#30340;&#36895;&#24230;&#23436;&#20840;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RNNs&#21487;&#20197;&#25552;&#20379;&#32447;&#24615;&#26102;&#19981;&#21464;(LTI)&#31995;&#32479;&#30340;&#36890;&#29992;&#36924;&#36817;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RC&#21487;&#20197;&#23545;&#19968;&#33324;LTI&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#36924;&#36817;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks (RNNs) are known to be universal approximators of dynamic systems under fairly mild and general assumptions, making them good tools to process temporal information. However, RNNs usually suffer from the issues of vanishing and exploding gradients in the standard RNN training. Reservoir computing (RC), a special RNN where the recurrent weights are randomized and left untrained, has been introduced to overcome these issues and has demonstrated superior empirical performance in fields as diverse as natural language processing and wireless communications especially in scenarios where training samples are extremely limited. On the contrary, the theoretical grounding to support this observed performance has not been fully developed at the same pace. In this work, we show that RNNs can provide universal approximation of linear time-invariant (LTI) systems. Specifically, we show that RC can universally approximate a general LTI system. We present a clear signal proces
&lt;/p&gt;</description></item><item><title>PIGEON&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#31934;&#21270;&#65292;&#20197;&#21450;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;StreetCLIP&#12290;</title><link>http://arxiv.org/abs/2307.05845</link><description>&lt;p&gt;
PIGEON: &#39044;&#27979;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05845
&lt;/p&gt;
&lt;p&gt;
PIGEON&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#31934;&#21270;&#65292;&#20197;&#21450;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;StreetCLIP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;PIGEON&#65292;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#22312;&#22806;&#37096;&#22522;&#20934;&#27979;&#35797;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32467;&#21512;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#26631;&#31614;&#24179;&#28369;&#65292;&#23545;&#20855;&#26377;&#22320;&#29702;&#20449;&#24687;&#30340;&#22270;&#20687;&#36827;&#34892;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;ProtoNets&#22312;&#20505;&#36873;&#22320;&#29702;&#21333;&#20803;&#38598;&#21512;&#20013;&#25913;&#36827;&#20301;&#32622;&#39044;&#27979;&#12290;PIGEON&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;&#25968;&#25454;&#30340;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#21019;&#24314;&#21644;&#20998;&#21106;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22320;&#29702;&#21333;&#20803;&#20869;&#37096;&#31934;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;StreetCLIP&#65292;&#20844;&#24320;&#25552;&#20379;&#65292;&#21487;&#29992;&#20110;&#19982;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#22478;&#24066;&#20065;&#26449;&#22330;&#26223;&#29702;&#35299;&#30456;&#20851;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#26412;&#22320;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#26469;&#21152;&#36895;&#24322;&#26500;&#25968;&#25454;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#29616;&#26377;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13263</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#37325;&#25490;&#21152;&#36895;&#24322;&#26500;&#25968;&#25454;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;(arXiv:2306.13263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Synthetic data shuffling accelerates the convergence of federated learning under data heterogeneity. (arXiv:2306.13263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#26412;&#22320;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#26469;&#21152;&#36895;&#24322;&#26500;&#25968;&#25454;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#29616;&#26377;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#24322;&#26500;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23545;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#36827;&#34892;&#27927;&#29260;&#65292;&#20197;&#21516;&#36136;&#21270;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#36829;&#21453;&#25968;&#25454;&#35775;&#38382;&#26435;&#21033;&#65292;&#32780;&#23545;&#20110;&#22312;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#37325;&#25490;&#21487;&#20197;&#21152;&#36895;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#65292;&#30446;&#21069;&#23578;&#26410;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#19982;&#25910;&#25947;&#36895;&#29575;&#21442;&#25968;&#20043;&#38388;&#30340;&#31934;&#30830;&#21487;&#37327;&#21270;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#37325;&#25490;&#21487;&#20197;&#25353;&#30334;&#20998;&#27604;&#24179;&#26041;&#20943;&#23569;&#26799;&#24230;&#24046;&#24322;&#65292;&#20174;&#32780;&#21152;&#36895;&#25910;&#25947;&#12290;&#21463;&#29702;&#35770;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#26412;&#22320;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#26469;&#35299;&#20915;&#25968;&#25454;&#35775;&#38382;&#26435;&#38382;&#39064;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#37325;&#25490;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#29616;&#26377;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In federated learning, data heterogeneity is a critical challenge. A straightforward solution is to shuffle the clients' data to homogenize the distribution. However, this may violate data access rights, and how and when shuffling can accelerate the convergence of a federated optimization algorithm is not theoretically well understood. In this paper, we establish a precise and quantifiable correspondence between data heterogeneity and parameters in the convergence rate when a fraction of data is shuffled across clients. We prove that shuffling can quadratically reduce the gradient dissimilarity with respect to the shuffling percentage, accelerating convergence. Inspired by the theory, we propose a practical approach that addresses the data access rights issue by shuffling locally generated synthetic data. The experimental results show that shuffling synthetic data improves the performance of multiple existing federated learning algorithms by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340; MimiC &#31639;&#27861;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#26356;&#26032;&#35299;&#20915;&#20102;&#32858;&#21512;&#26356;&#26032;&#21644;&#26399;&#26395;&#20013;&#24515;&#26356;&#26032;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.12212</link><description>&lt;p&gt;
MimiC&#65306;&#27169;&#20223;&#20013;&#24515;&#26356;&#26032;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340; MimiC &#31639;&#27861;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#36864;&#20986;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#26356;&#26032;&#35299;&#20915;&#20102;&#32858;&#21512;&#26356;&#26032;&#21644;&#26399;&#26395;&#20013;&#24515;&#26356;&#26032;&#20043;&#38388;&#30340;&#20998;&#27495;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#20219;&#21153;&#20998;&#21457;&#32473;&#23458;&#25143;&#31471;&#65292;&#21482;&#38656;&#35201;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#25910;&#38598;&#27169;&#22411;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#36793;&#32536;&#32593;&#32476;&#20013;&#37096;&#32626;&#26102;&#65292;&#23458;&#25143;&#31471;&#65288;&#22914;&#26234;&#33021;&#25163;&#26426;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65289;&#21487;&#33021;&#20250;&#26080;&#39044;&#35686;&#22320;&#36864;&#20986;&#20219;&#20309;&#19968;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#36825;&#20250;&#38459;&#30861;&#32852;&#37030;&#23398;&#20064;&#36798;&#21040;&#25910;&#25947;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#36825;&#19968;&#20851;&#38190;&#25361;&#25112;&#65292;&#35774;&#35745;&#20986;&#19968;&#31181;&#21517;&#20026; MimiC &#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20013;&#24515;&#26381;&#21153;&#22120;&#20462;&#25913;&#20854;&#26356;&#26032;&#20197;&#27169;&#20223;&#32570;&#22833;&#23458;&#25143;&#31471;&#26356;&#26032;&#65292;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;MimiC &#30456;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modi
&lt;/p&gt;</description></item><item><title>&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10125</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects. (arXiv:2306.10125v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10125
&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#65292;&#36890;&#36807;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;SSL&#26368;&#31361;&#20986;&#30340;&#20248;&#21183;&#26159;&#20943;&#23569;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#19982;&#35768;&#22810;&#20851;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#32508;&#36848;&#30456;&#27604;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;SSL&#30340;&#32508;&#36848;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#24403;&#21069;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#22238;&#39038;&#20102;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#29616;&#26377;&#32508;&#36848;&#65292;&#28982;&#21518;&#36890;&#36807;&#24635;&#32467;&#20174;&#29983;&#25104;&#22411;&#12289;&#23545;&#27604;&#22411;&#21644;&#23545;&#25239;&#22411;&#19977;&#20010;&#35282;&#24230;&#23545;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#26032;&#30340;&#20998;&#31867;&#12290;&#36825;&#20123;&#26041;&#27861;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#21313;&#20010;&#23376;&#31867;&#65292;&#35814;&#32454;&#22238;&#39038;&#21644;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20851;&#38190;&#30452;&#35273;&#12289;&#20027;&#35201;&#26694;&#26550;&#12289;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages an
&lt;/p&gt;</description></item><item><title>DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.09862</link><description>&lt;p&gt;
DoubleAdapt&#65306;&#19968;&#31181;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09862
&lt;/p&gt;
&lt;p&gt;
DoubleAdapt&#26159;&#19968;&#20010;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#26159;&#37327;&#21270;&#25237;&#36164;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#20934;&#30830;&#39044;&#27979;&#20215;&#26684;&#36235;&#21183;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20316;&#20026;&#19968;&#39033;&#22312;&#32447;&#26381;&#21153;&#65292;&#32929;&#31080;&#25968;&#25454;&#38543;&#26102;&#38543;&#22320;&#25345;&#32493;&#21040;&#36798;&#12290;&#20351;&#29992;&#26368;&#26032;&#25968;&#25454;&#23545;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#26356;&#26032;&#26159;&#23454;&#29992;&#32780;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#26032;&#25968;&#25454;&#21487;&#33021;&#25581;&#31034;&#20102;&#26410;&#26469;&#32929;&#31080;&#24066;&#22330;&#20013;&#20250;&#37325;&#22797;&#20986;&#29616;&#30340;&#19968;&#20123;&#26032;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#24067;&#28418;&#31227;&#65288;&#21363;&#27010;&#24565;&#28418;&#31227;&#65289;&#30340;&#25361;&#25112;&#65292;&#32929;&#31080;&#36235;&#21183;&#39044;&#27979;&#30340;&#22686;&#37327;&#23398;&#20064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#38543;&#30528;&#32929;&#31080;&#24066;&#22330;&#21160;&#24577;&#28436;&#21464;&#65292;&#26410;&#26469;&#25968;&#25454;&#30340;&#20998;&#24067;&#21487;&#33021;&#20250;&#19982;&#22686;&#37327;&#25968;&#25454;&#31245;&#24494;&#25110;&#26174;&#30528;&#22320;&#19981;&#21516;&#65292;&#20174;&#32780;&#38459;&#30861;&#22686;&#37327;&#26356;&#26032;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20004;&#20010;&#36866;&#37197;&#22120;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#8212;&#8212;DoubleAdapt&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#25968;&#25454;&#21644;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#20998;&#24067;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#33258;&#21160;&#23398;&#20064;&#22914;&#20309;&#23558;&#32929;&#31080;&#25968;&#25454;&#36866;&#24212;&#21040;&#26412;&#22320;&#24179;&#31283;&#20998;&#24067;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distri
&lt;/p&gt;</description></item><item><title>Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.09750</link><description>&lt;p&gt;
Fedstellar&#65306;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09750
&lt;/p&gt;
&lt;p&gt;
Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2016&#24180;&#65292;&#35895;&#27468;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#36328;&#32852;&#30431;&#21442;&#19982;&#32773;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#34429;&#28982;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#12289;&#21333;&#28857;&#25925;&#38556;&#21644;&#23545;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#31561;&#23616;&#38480;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36890;&#36807;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#21644;&#26368;&#23567;&#21270;&#23545;&#20013;&#22830;&#23454;&#20307;&#30340;&#20381;&#36182;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35757;&#32451;DFL&#27169;&#22411;&#30340;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fedstellar&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#22312;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#19981;&#21516;&#32852;&#30431;&#20013;&#20197;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25506;&#32034;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#20219;&#21153;&#20013;&#65292;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#34920;&#26684;&#31867;&#26816;&#27979;&#12289;&#21015;&#31867;&#22411;&#27880;&#37322;&#21644;&#32852;&#25509;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#26377;&#26395;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#32479;&#19968;&#22312;&#22522;&#30784;&#27169;&#22411;&#19979;&#12290;</title><link>http://arxiv.org/abs/2306.09610</link><description>&lt;p&gt;
CHORUS: &#32479;&#19968;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
CHORUS: Foundation Models for Unified Data Discovery and Exploration. (arXiv:2306.09610v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09610
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25506;&#32034;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#20219;&#21153;&#20013;&#65292;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#34920;&#26684;&#31867;&#26816;&#27979;&#12289;&#21015;&#31867;&#22411;&#27880;&#37322;&#21644;&#32852;&#25509;&#21015;&#39044;&#27979;&#20013;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#26377;&#26395;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#32479;&#19968;&#22312;&#22522;&#30784;&#27169;&#22411;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22522;&#30784;&#27169;&#22411;&#24212;&#29992;&#20110;&#25968;&#25454;&#21457;&#29616;&#21644;&#25506;&#32034;&#20219;&#21153;&#20013;&#12290;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs)&#65292;&#22312;&#21508;&#31181;&#19982;&#20854;&#35757;&#32451;&#26080;&#20851;&#30340;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#25968;&#25454;&#21457;&#29616;&#21644;&#25968;&#25454;&#25506;&#32034;&#39046;&#22495;&#38750;&#24120;&#36866;&#29992;&#12290;&#22312;&#35880;&#24910;&#20351;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#20855;&#26377;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#20248;&#21270;&#34920;&#26684;&#31867;&#26816;&#27979;&#12289;&#21015;&#31867;&#22411;&#27880;&#37322;&#21644;&#32852;&#25509;&#21015;&#39044;&#27979;&#36825;&#19977;&#31181;&#20195;&#34920;&#24615;&#20219;&#21153;&#12290;&#22312;&#36825;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#24120;&#36229;&#36807;&#20154;&#31867;&#19987;&#23478;&#30340;&#20219;&#21153;&#34920;&#29616;&#12290;&#36825;&#34920;&#26126;&#20102;&#23558;&#19981;&#21516;&#30340;&#25968;&#25454;&#31649;&#29702;&#20219;&#21153;&#32479;&#19968;&#22312;&#22522;&#30784;&#27169;&#22411;&#19979;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the application of foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMs) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. This suggests a future direction in which disparate data management tasks can be unified under foundation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.18436</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#23454;&#29616;&#26368;&#20248;K&#22343;&#20540;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;K&#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35299;&#20915;&#38750;&#36127;&#20302;&#31209;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K&#22343;&#20540;&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#27169;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#26494;&#24347;&#26368;&#36817;&#34987;&#25552;&#20986;&#29992;&#20110;&#35299;&#20915;K&#22343;&#20540;&#20248;&#21270;&#38382;&#39064;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#20294;&#23454;&#29616;SDP&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#36825;&#20123;&#20445;&#35777;&#26080;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#34987;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#32570;&#20047;&#22362;&#23454;&#30340;&#32479;&#35745;&#22522;&#30784;&#25110;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;NMF&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#20984;Burer-Monteiro&#20998;&#35299;&#26041;&#27861;&#35299;&#20915;&#21322;&#23450;&#35268;&#21010;&#26494;&#24347;&#30340;K&#22343;&#20540;&#20844;&#24335;&#30340;&#38750;&#36127;&#20302;&#31209;&#38480;&#21046;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;NMF&#31639;&#27861;&#19968;&#26679;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#65292;&#21516;&#26102;&#20063;&#20139;&#26377;&#19982;SDP&#30456;&#21516;&#30340;&#24378;&#22823;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;NMF&#31639;&#27861;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;SDP&#27714;&#35299;&#22120;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
$K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
&lt;/p&gt;</description></item><item><title>&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17328</link><description>&lt;p&gt;
&#12298;Zero-TPrune: &#22522;&#20110;&#39044;&#35757;&#32451;Transformers&#20851;&#27880;&#22270;&#30340;&#38646;&#23556;&#20987;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17328
&lt;/p&gt;
&lt;p&gt;
&#12298;Zero-TPrune&#12299;&#26159;&#19968;&#20010;&#32771;&#34385;&#21040;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20284;&#24615;&#30340;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#36827;&#34892;&#20196;&#29260;&#21098;&#26525;&#65292;&#20197;&#27714;&#35299;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;Transformer&#27169;&#22411;&#21363;&#25554;&#21363;&#29992;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;Transformer&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#27169;&#22411;&#30340;&#20307;&#31215;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#32780;&#25512;&#29702;&#25104;&#26412;&#21017;&#38543;&#36755;&#20837;&#24207;&#21015;&#20013;&#20196;&#29260;&#25968;&#37327;&#30340;&#24179;&#26041;&#25552;&#39640;&#12290;&#20196;&#29260;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#20852;&#35299;&#20915;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#26131;&#20110;&#22312;&#21508;&#31181;Transformer&#25903;&#25345;&#30340;&#27169;&#22411;&#19978;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20196;&#29260;&#21098;&#26525;&#26041;&#27861;&#38656;&#35201;&#22312;&#21098;&#26525;&#21518;&#25110;&#26399;&#38388;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25506;&#35752;&#20102;&#27809;&#26377;&#24494;&#35843;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#39044;&#35757;&#32451;Transformer&#30340;&#21098;&#26525;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Zero-TPrune&#65292;&#36825;&#26159;&#19968;&#31181;&#38646;&#23556;&#20987;&#26041;&#27861;&#65292;&#23427;&#26082;&#32771;&#34385;&#20196;&#29260;&#30340;&#37325;&#35201;&#24615;&#21448;&#32771;&#34385;&#30456;&#20284;&#24615;&#26469;&#25191;&#34892;&#20196;&#29260;&#21098;&#26525;&#12290;Zero-TPrune&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#27880;&#24847;&#22270;&#26469;&#20026;&#20196;&#29260;&#29983;&#25104;&#19968;&#20010;&#37325;&#35201;&#24615;&#25490;&#21517;&#24182;&#31227;&#38500;&#20449;&#24687;&#36739;&#23569;&#30340;&#20196;&#29260;&#12290;&#27880;&#24847;&#30697;&#38453;&#21487;&#29992;&#20110;&#25512;&#26029;&#21363;&#25554;&#21363;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployment of Transformer models on the edge is increasingly challenging due to the exponentially growing model size and inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require a computationally-expensive fine-tuning process after or during pruning, which is not desirable in many cases. Some recent works explore pruning of off-the-shelf pre-trained Transformers without fine-tuning. However, they only take the importance of tokens into consideration. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. Zero-TPrune leverages the attention graph of pre-trained Transformer models to produce an importance rank for tokens and removes the less informative tokens. The attention matrix can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09620</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#35843;&#26597;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#35266;&#28857;&#39044;&#27979;&#20013;&#65292;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#65292;&#22238;&#28335;&#25512;&#29702;&#21644;&#38646;&#27425;&#39044;&#27979;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#23558;&#35843;&#26597;&#38382;&#39064;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#31070;&#32463;&#23884;&#20837;&#24341;&#20837;&#21040;&#35266;&#28857;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;LLMs&#20013;&#12290;&#22312;1972&#24180;&#21040;2021&#24180;&#30340;&#8220;&#24120;&#35268;&#31038;&#20250;&#35843;&#26597;&#8221;&#20013;&#65292;&#25105;&#20204;&#20174;68,846&#21517;&#32654;&#22269;&#20154;&#20013;&#33719;&#24471;&#20102;3,110&#20010;&#20108;&#36827;&#21046;&#35266;&#28857;&#65292;&#22312;Alpaca-7b&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#26524;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65288;AUC=0.87&#65292;&#20844;&#24320;&#35266;&#28857;&#39044;&#27979;&#20026;$\rho$=0.99&#65289;&#21644;&#22238;&#28335;&#25512;&#29702;&#65288;AUC=0.86&#65292;$\rho$=0.98&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26174;&#33879;&#30340;&#39044;&#27979;&#33021;&#21147;&#33021;&#22815;&#20197;&#39640;&#32622;&#20449;&#24230;&#22635;&#34917;&#32570;&#22833;&#30340;&#36235;&#21183;&#65292;&#24182;&#26631;&#26126;&#20844;&#20247;&#24577;&#24230;&#20309;&#26102;&#21457;&#29983;&#21464;&#21270;&#65292;&#22914;&#21516;&#24615;&#23130;&#23035;&#30340;&#33719;&#21462;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#21463;&#21040;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#36895;&#24230;&#21442;&#25968;&#21270;&#21644;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#31561;&#29992;&#20110;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#21644;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.03935</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. (arXiv:2305.03935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;ODEs&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25913;&#36827;&#25216;&#26415;&#65292;&#21253;&#25324;&#36895;&#24230;&#21442;&#25968;&#21270;&#21644;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#31561;&#29992;&#20110;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#21644;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#65288;&#21363;&#25193;&#25955;ODE&#65289;&#26159;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#65288;CNFs&#65289;&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#23427;&#20351;&#24471;&#30830;&#23450;&#24615;&#25512;&#26029;&#21644;&#31934;&#30830;&#20284;&#28982;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;&#25193;&#25955;ODE&#30340;&#20284;&#28982;&#20272;&#35745;&#32467;&#26524;&#20173;&#26377;&#24456;&#22823;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#25913;&#36827;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#35757;&#32451;&#21644;&#35780;&#20272;&#20004;&#20010;&#26041;&#38754;&#65292;&#29992;&#20110;&#25193;&#25955;ODE&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#23545;&#20110;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36895;&#24230;&#21442;&#25968;&#21270;&#65292;&#24182;&#25506;&#32034;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#35823;&#24046;&#26377;&#30028;&#30340;&#39640;&#38454;&#27969;&#21305;&#37197;&#30446;&#26631;&#29992;&#20110;&#24494;&#35843;&#65292;&#20174;&#32780;&#25552;&#39640;ODE&#30340;&#20284;&#28982;&#20272;&#35745;&#24182;&#24179;&#28369;&#20854;&#36712;&#36857;&#12290;&#23545;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#39035;&#35757;&#32451;&#30340;&#25130;&#26029;&#27491;&#24577;&#21435;&#37327;&#21270;&#26041;&#27861;&#26469;&#22635;&#34917;&#35757;&#32451;-&#35780;&#20272;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation (ODE) of diffusion models (i.e., diffusion ODEs) is a particular case of continuous normalizing flows (CNFs), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion ODEs are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion ODEs, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the ODE likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#24314;&#31569;&#24418;&#24335;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#35758;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.00510</link><description>&lt;p&gt;
&#36890;&#21521;&#33258;&#30001;&#35745;&#31639;&#26550;&#26500;: &#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#20803;&#23431;&#23449;&#34394;&#25311;&#24314;&#31569;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse. (arXiv:2305.00510v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#24314;&#31569;&#24418;&#24335;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#24418;&#29366;&#29983;&#25104;&#25216;&#26415;&#27491;&#22312;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24314;&#31569;&#35774;&#35745;&#20004;&#26041;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#32508;&#21512;&#35843;&#26597;&#26088;&#22312;&#35843;&#26597;&#21644;&#27604;&#36739;&#24403;&#21069;&#26368;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGMs&#65289;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#12289;3D&#24863;&#30693;&#22270;&#20687;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;187&#31687;&#25991;&#31456;(&#21344;2018-2022&#24180;&#38388;&#21457;&#34920;&#25991;&#31456;&#30340;80.7%)&#65292;&#20197;&#22238;&#39038;&#22312;&#34394;&#25311;&#29615;&#22659;&#19979;&#24314;&#31569;&#29983;&#25104;&#21487;&#33021;&#24615;&#30340;&#39046;&#22495;&#65292;&#38480;&#20110;&#24314;&#31569;&#24418;&#24335;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24314;&#31569;&#30740;&#31350;&#12289;&#34394;&#25311;&#29615;&#22659;&#21644;&#30456;&#20851;&#25216;&#26415;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#25509;&#30528;&#22238;&#39038;&#20102;&#31163;&#25955;&#20307;&#32032;&#29983;&#25104;&#12289;&#30001;2D&#22270;&#20687;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#20197;&#21450;&#26465;&#20214;&#21442;&#25968;&#30340;&#26368;&#36817;&#36235;&#21183;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;3D&#29983;&#25104;&#21644;&#21442;&#25968;&#21270;&#25511;&#21046;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#27979;&#21253;&#25324;&#29983;&#25104;&#22810;&#26679;&#24615;&#12289;&#26032;&#22411;&#36755;&#20986;&#21644;&#23884;&#20837;&#24335;&#26500;&#24314;&#31561;&#22235;&#20010;&#30740;&#31350;&#35758;&#31243;&#21487;&#33021;&#20250;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D shape generation techniques utilizing deep learning are increasing attention from both computer vision and architectural design. This survey focuses on investigating and comparing the current latest approaches to 3D object generation with deep generative models (DGMs), including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), 3D-aware images, and diffusion models. We discuss 187 articles (80.7% of articles published between 2018-2022) to review the field of generated possibilities of architecture in virtual environments, limited to the architecture form. We provide an overview of architectural research, virtual environment, and related technical approaches, followed by a review of recent trends in discrete voxel generation, 3D models generated from 2D images, and conditional parameters. We highlight under-explored issues in 3D generation and parameterized control that is worth further investigation. Moreover, we speculate that four research agendas including
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#20132;&#26131;&#30340;&#26041;&#26696;&#65292;DeepStock&#36890;&#36807;&#26597;&#30475;&#32929;&#31080;&#20215;&#26684;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;Resnet&#21644;logits&#26469;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#22312;&#26410;&#26469;D&#22825;&#20869;&#26159;&#21542;&#20250;&#21319;&#38477;&#19968;&#23450;&#30334;&#20998;&#27604;&#65292;&#24182;&#22312;&#38889;&#22269;&#21644;&#32654;&#22269;&#24066;&#22330;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;&#30340;&#21033;&#28070;&#12290;</title><link>http://arxiv.org/abs/2304.14870</link><description>&lt;p&gt;
Deep Stock: &#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#20132;&#26131;&#30340;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Deep Stock: training and trading scheme using deep learning. (arXiv:2304.14870v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#21644;&#20132;&#26131;&#30340;&#26041;&#26696;&#65292;DeepStock&#36890;&#36807;&#26597;&#30475;&#32929;&#31080;&#20215;&#26684;&#30340;&#36807;&#21435;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;Resnet&#21644;logits&#26469;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#22312;&#26410;&#26469;D&#22825;&#20869;&#26159;&#21542;&#20250;&#21319;&#38477;&#19968;&#23450;&#30334;&#20998;&#27604;&#65292;&#24182;&#22312;&#38889;&#22269;&#21644;&#32654;&#22269;&#24066;&#22330;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;&#30340;&#21033;&#28070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#25928;&#24066;&#22330;&#20551;&#35828;&#23384;&#22312;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#32929;&#31080;&#24066;&#22330;&#23384;&#22312;&#22833;&#28789;&#29616;&#35937;&#65292;&#23548;&#33268;&#20986;&#29616;&#20102;&#19968;&#20123;&#33021;&#22815;&#33719;&#24471;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;&#30340;&#25216;&#26415;&#65292;&#21363;alpha&#12290;&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#31995;&#32479;&#24615;&#20132;&#26131;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#20998;&#26512;&#21644;&#39044;&#27979;&#24066;&#22330;&#34892;&#20026;&#30340;&#24378;&#22823;&#24037;&#20855;&#24050;&#32463;&#24320;&#22987;&#23853;&#38706;&#22836;&#35282;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#19987;&#19994;&#20132;&#26131;&#21592;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26597;&#30475;&#20808;&#21069;&#30340;600&#22825;&#30340;&#32929;&#31080;&#20215;&#26684;&#65292;&#24182;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#22312;&#25509;&#19979;&#26469;D&#22825;&#20869;&#26159;&#21542;&#20250;&#21319;&#38477;&#19968;&#23450;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31216;&#20026;DeepStock&#65292;&#20351;&#29992;Resnet&#30340;&#36339;&#36291;&#36830;&#25509;&#21644;logits&#26469;&#22686;&#21152;&#27169;&#22411;&#22312;&#20132;&#26131;&#26041;&#26696;&#20013;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#38889;&#22269;&#21644;&#32654;&#22269;&#32929;&#31080;&#24066;&#22330;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#38889;&#22269;&#24066;&#22330;&#19978;&#33719;&#24471;&#20102;N&#65285;&#30340;&#21033;&#28070;&#65292;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;M&#65285;&#65292;&#24182;&#22312;&#32654;&#22269;&#24066;&#22330;&#19978;&#33719;&#24471;&#20102;A&#65285;&#30340;&#21033;&#28070;&#65292;&#36229;&#36807;&#24066;&#22330;&#22238;&#25253;B&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the efficient market hypothesis, many studies suggest the existence of inefficiencies in the stock market, leading to the development of techniques to gain above-market returns, known as alpha. Systematic trading has undergone significant advances in recent decades, with deep learning emerging as a powerful tool for analyzing and predicting market behavior. In this paper, we propose a model inspired by professional traders that look at stock prices of the previous 600 days and predicts whether the stock price rises or falls by a certain percentage within the next D days. Our model, called DeepStock, uses Resnet's skip connections and logits to increase the probability of a model in a trading scheme. We test our model on both the Korean and US stock markets and achieve a profit of N\% on Korea market, which is M\% above the market return, and profit of A\% on US market, which is B\% above the market return.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#34394;&#20551;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24110;&#21161;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#36827;&#34892;&#35760;&#24405;&#28548;&#28165;&#12290;</title><link>http://arxiv.org/abs/2304.07633</link><description>&lt;p&gt;
&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#21270;&#31070;&#32463;&#27169;&#22411;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#22810;&#27169;&#24577;&#35875;&#35328;
&lt;/p&gt;
&lt;p&gt;
Detecting Out-of-Context Multimodal Misinformation with interpretable neural-symbolic model. (arXiv:2304.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#34394;&#20551;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24110;&#21161;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#36827;&#34892;&#35760;&#24405;&#28548;&#28165;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#28436;&#21270;&#25345;&#32493;&#22686;&#38271;&#65292;&#26088;&#22312;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#35875;&#35328;&#25110;&#34394;&#20551;&#26032;&#38395;&#32534;&#36753;&#20027;&#35201;&#20381;&#36182;&#20110;&#29983;&#25104;&#21644;/&#25110;&#20266;&#36896;&#30340;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#35270;&#39057;&#19981;&#21516;&#65292;&#24403;&#21069;&#30340;&#34394;&#20551;&#20449;&#24687;&#21019;&#20316;&#32773;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#19981;&#21305;&#37197;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#65289;&#26469;&#27450;&#39575;&#20844;&#20247;&#21644;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#31181;&#26032;&#22411;&#30340;&#34394;&#20551;&#20449;&#24687;&#19981;&#20165;&#22686;&#21152;&#20102;&#26816;&#27979;&#30340;&#38590;&#24230;&#65292;&#20063;&#22686;&#21152;&#20102;&#28548;&#28165;&#30340;&#38590;&#24230;&#65292;&#22240;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#27169;&#24577;&#37117;&#36275;&#22815;&#25509;&#36817;&#30495;&#23454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#36328;&#27169;&#24577;&#21435;&#19978;&#19979;&#25991;&#26816;&#27979;&#65292;&#21516;&#26102;&#35782;&#21035;&#19981;&#21305;&#37197;&#30340;&#23545;&#21644;&#36328;&#27169;&#24577;&#30683;&#30462;&#65292;&#36825;&#23545;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#30340;&#35760;&#24405;&#28548;&#28165;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#39318;&#20808;&#36890;&#36807;&#25277;&#35937;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22522;&#20110;Abstract M&#36827;&#34892;&#31526;&#21495;&#21270;&#20998;&#35299;&#65292;&#24471;&#21040;&#19968;&#32452;&#20107;&#23454;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the sustained evolution of misinformation that aims at manipulating public opinions. Unlike traditional rumors or fake news editors who mainly rely on generated and/or counterfeited images, text and videos, current misinformation creators now more tend to use out-of-context multimedia contents (e.g. mismatched images and captions) to deceive the public and fake news detection systems. This new type of misinformation increases the difficulty of not only detection but also clarification, because every individual modality is close enough to true information. To address this challenge, in this paper we explore how to achieve interpretable cross-modal de-contextualization detection that simultaneously identifies the mismatched pairs and the cross-modal contradictions, which is helpful for fact-check websites to document clarifications. The proposed model first symbolically disassembles the text-modality information to a set of fact queries based on the Abstract M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06875</link><description>&lt;p&gt;
&#19981;&#38656;&#37325;&#26032;&#25628;&#32034;&#30340;&#30740;&#31350;&#65306;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#21487;&#31934;&#30830;&#39044;&#27979;&#36328;&#23610;&#24230;&#30340;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#65292;&#22240;&#20026;&#23567;&#27169;&#22411;&#30340;&#32467;&#35770;&#19981;&#33021;&#31616;&#21333;&#22320;&#36716;&#31227;&#21040;&#22823;&#27169;&#22411;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#31995;&#32479;&#65292;&#20165;&#22522;&#20110;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#36229;&#21442;&#25968;&#30452;&#25509;&#39044;&#27979;&#22823;&#27169;&#22411;&#30340;&#19968;&#20123;&#25351;&#26631;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32553;&#25918;&#24459;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#65288;muP&#65289;&#20351;&#24471;&#21487;&#20197;&#22312;&#38752;&#36817;&#24120;&#35265;&#25439;&#22833;&#27969;&#22495;&#30340;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25311;&#21512;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25628;&#32034;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22823;&#23610;&#24230;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#65292;&#22312;&#35757;&#32451;&#24320;&#22987;&#20043;&#21069;&#23601;&#21487;&#20197;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20316;&#20026;&#21487;&#38752;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#31532;&#19968;&#27493;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#19981;&#38656;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#23398;&#20064;&#30340;&#27969;&#31243;&#65292;&#20174;&#22810;&#27169;&#24577;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;&#35782;&#21035;&#32954;&#21160;&#33033;&#26964;&#21387;&#21147;&#65288;PAWP&#65289;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#29305;&#24449;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07540</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#39044;&#27979;&#24515;&#33039;MRI&#20013;&#32954;&#21160;&#33033;&#26964;&#21387;&#21147;
&lt;/p&gt;
&lt;p&gt;
Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI. (arXiv:2303.07540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07540
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#23398;&#20064;&#30340;&#27969;&#31243;&#65292;&#20174;&#22810;&#27169;&#24577;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;&#35782;&#21035;&#32954;&#21160;&#33033;&#26964;&#21387;&#21147;&#65288;PAWP&#65289;&#12290;&#36890;&#36807;&#25972;&#21512;&#22810;&#31181;&#29305;&#24449;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34928;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#29983;&#21629;&#23041;&#32961;&#30142;&#30149;&#65292;&#20250;&#23548;&#33268;&#24038;&#24515;&#23460;&#21387;&#21147;&#21319;&#39640;&#12290;&#32954;&#21160;&#33033;&#26964;&#21387;&#21147;&#65288;PAWP&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20195;&#29702;&#26631;&#24535;&#65292;&#34920;&#31034;&#24038;&#24515;&#23460;&#30340;&#39640;&#21387;&#12290;PAWP &#30001;&#21491;&#24515;&#23548;&#31649;&#26816;&#26597;&#65288;RHC&#65289;&#30830;&#23450;&#65292;&#20294;&#23427;&#26159;&#19968;&#31181;&#26377;&#21019;&#24615;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#22320;&#20174;&#22823;&#37327;&#20154;&#32676;&#20013;&#35782;&#21035;&#39640;&#21361;&#24739;&#32773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24352;&#37327;&#23398;&#20064;&#30340;&#27969;&#31243;&#65292;&#20174;&#22810;&#27169;&#24577;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#20013;&#35782;&#21035;PAWP&#12290;&#36825;&#20010;&#27969;&#31243;&#25552;&#21462;&#39640;&#32500;&#25195;&#25551;&#20013;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#20026;&#20102;&#36136;&#37327;&#25511;&#21046;&#65292;&#25105;&#20204;&#37319;&#29992;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20026;&#22522;&#30784;&#30340;&#20998;&#32452;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#36136;&#37327;&#24046;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#29305;&#24449;&#65306;&#24515;&#33039;MRI&#19982;&#30701;&#36724;&#21644;&#22235;&#33108;&#35270;&#22270;&#20197;&#21450;&#30005;&#23376;&#30149;&#21382;&#65292;&#23398;&#20064;&#20114;&#34917;&#20449;&#24687;&#12290;&#36825;&#39033;&#23454;&#39564;&#20998;&#26512;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart failure is a serious and life-threatening condition that can lead to elevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure (PAWP) is an important surrogate marker indicating high pressure in the left ventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an invasive procedure. A non-invasive method is useful in quickly identifying high-risk patients from a large population. In this work, we develop a tensor learning-based pipeline for identifying PAWP from multimodal cardiac Magnetic Resonance Imaging (MRI). This pipeline extracts spatial and temporal features from high-dimensional scans. For quality control, we incorporate an epistemic uncertainty-based binning strategy to identify poor-quality training samples. To improve the performance, we learn complementary information by integrating features from multimodal data: cardiac MRI with short-axis and four-chamber views, and Electronic Health Records. The experimental analysis on a large
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;$\ell_0$&#36867;&#36991;&#12289;&#21518;&#38376;&#21644;&#27745;&#26579;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#65292;FPA&#36895;&#24230;&#26356;&#24555;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#19988;&#33021;&#22815;&#20813;&#36153;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.11628</link><description>&lt;p&gt;
&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#65306;&#19968;&#31181;&#24555;&#36895;&#30340;&#23545;$\ell_0$&#25915;&#20987;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks. (arXiv:2302.11628v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;$\ell_0$&#36867;&#36991;&#12289;&#21518;&#38376;&#21644;&#27745;&#26579;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#65292;FPA&#36895;&#24230;&#26356;&#24555;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#19988;&#33021;&#22815;&#20813;&#36153;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30340;&#25110;$\ell_0$&#23545;&#25239;&#25915;&#20987;&#20250;&#20219;&#24847;&#25200;&#21160;&#26410;&#30693;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;$\ell_0$&#40065;&#26834;&#24615;&#20998;&#26512;&#29305;&#21035;&#36866;&#29992;&#20110;&#24322;&#26500;&#65288;&#34920;&#26684;&#65289;&#25968;&#25454;&#65292;&#20854;&#20013;&#29305;&#24449;&#20855;&#26377;&#19981;&#21516;&#30340;&#31867;&#22411;&#25110;&#23610;&#24230;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;$\ell_0$&#35748;&#35777;&#38450;&#24481;&#22522;&#20110;&#38543;&#26426;&#24179;&#28369;&#65292;&#24182;&#20165;&#36866;&#29992;&#20110;&#36867;&#36991;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#65288;FPA&#65289;--&#19968;&#31181;&#38024;&#23545;$\ell_0$&#36867;&#36991;&#12289;&#21518;&#38376;&#21644;&#27745;&#26579;&#25915;&#20987;&#30340;&#35748;&#35777;&#38450;&#24481;&#12290;FPA&#36890;&#36807;&#38598;&#25104;&#29983;&#25104;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#20854;&#23376;&#27169;&#22411;&#26159;&#22312;&#19981;&#30456;&#20132;&#30340;&#29305;&#24449;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;$\ell_0$&#38450;&#24481;&#30456;&#27604;&#65292;FPA&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#36798;3000&#20493;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#20013;&#20301;&#25968;&#40065;&#26834;&#24615;&#20445;&#35777;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;CIFAR10&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;13&#20687;&#32032;&#65292;MNIST&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;12&#20687;&#32032;&#65292;Weather&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;4&#20010;&#29305;&#24449;&#65292;Ames&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;3&#20010;&#29305;&#24449;&#65289;&#65292;&#36825;&#24847;&#21619;&#30528;FPA&#33021;&#22815;&#20813;&#36153;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse or $\ell_0$ adversarial attacks arbitrarily perturb an unknown subset of the features. $\ell_0$ robustness analysis is particularly well-suited for heterogeneous (tabular) data where features have different types or scales. State-of-the-art $\ell_0$ certified defenses are based on randomized smoothing and apply to evasion attacks only. This paper proposes feature partition aggregation (FPA) -- a certified defense against the union of $\ell_0$ evasion, backdoor, and poisoning attacks. FPA generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Compared to state-of-the-art $\ell_0$ defenses, FPA is up to 3,000${\times}$ faster and provides larger median robustness guarantees (e.g., median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 for MNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaning FPA provides the additional dimensions of robustness essentially for free.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#36830;&#32493;&#36229;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;CDT&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#22312;&#25628;&#32034;&#31354;&#38388;&#20869;&#23398;&#20064;&#26368;&#20248;&#21442;&#25968;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2302.09440</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#36827;&#34892;&#22312;&#32447;&#36830;&#32493;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Continuous Hyperparameter Optimization for Contextual Bandits. (arXiv:2302.09440v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#36830;&#32493;&#36229;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;CDT&#65292;&#33021;&#22815;&#21160;&#24577;&#22320;&#22312;&#25628;&#32034;&#31354;&#38388;&#20869;&#23398;&#20064;&#26368;&#20248;&#21442;&#25968;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38543;&#26426;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#26681;&#25454;&#36807;&#21435;&#30340;&#32463;&#39564;&#20174;&#26102;&#38388;&#30456;&#20851;&#34892;&#21160;&#38598;&#20013;&#20381;&#27425;&#37319;&#21462;&#34892;&#21160;&#65292;&#20197;&#26368;&#23567;&#21270;&#24635;&#21518;&#24724;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19968;&#26679;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20854;&#22810;&#20010;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#29702;&#35770;&#25512;&#23548;&#20986;&#30340;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#23454;&#38469;&#19978;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#19979;&#20351;&#29992;&#31163;&#32447;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#20132;&#21449;&#39564;&#35777;&#65289;&#36873;&#25321;&#36229;&#21442;&#25968;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#20915;&#31574;&#24517;&#39035;&#23454;&#26102;&#36827;&#34892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#36830;&#32493;&#36229;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#65292;&#20197;&#23398;&#20064;&#39134;&#34892;&#20013;&#30340;&#26368;&#20339;&#21442;&#25968;&#37197;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;CDT&#65288;Continuous Dynamic Tuning&#65289;&#30340;&#21452;&#23618;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#23558;&#36229;&#21442;&#25968;&#20248;&#21270;&#24418;&#24335;&#21270;&#20026;&#38750;&#24179;&#31283;&#36830;&#32493;&#27494;&#22120;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#27599;&#20010;&#27494;&#22120;&#20195;&#34920;&#19968;&#31181;&#36229;&#21442;&#25968;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In stochastic contextual bandits, an agent sequentially makes actions from a time-dependent action set based on past experience to minimize the cumulative regret. Like many other machine learning algorithms, the performance of bandits heavily depends on their multiple hyperparameters, and theoretically derived parameter values may lead to unsatisfactory results in practice. Moreover, it is infeasible to use offline tuning methods like cross-validation to choose hyperparameters under the bandit environment, as the decisions should be made in real time. To address this challenge, we propose the first online continuous hyperparameter tuning framework for contextual bandits to learn the optimal parameter configuration within a search space on the fly. Specifically, we use a double-layer bandit framework named CDT (Continuous Dynamic Tuning) and formulate the hyperparameter optimization as a non-stationary continuum-armed bandit, where each arm represents a combination of hyperparameters, a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#27880;&#20876;&#25968;&#25454;&#26469;&#35782;&#21035;&#26032;&#29992;&#25143;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#19988;&#22312;&#20165;&#26377;&#23569;&#37327;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#20063;&#26356;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2302.07517</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#29305;&#23450;&#36816;&#21160;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Extensible Motion-based Identification of XR Users using Non-Specific Motion Data. (arXiv:2302.07517v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07517
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;XR&#29992;&#25143;&#22522;&#20110;&#36816;&#21160;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#27880;&#20876;&#25968;&#25454;&#26469;&#35782;&#21035;&#26032;&#29992;&#25143;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#19988;&#22312;&#20165;&#26377;&#23569;&#37327;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#20063;&#26356;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#24335;&#21644;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#36317;&#31163;&#21644;&#20998;&#31867;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#36890;&#36807;&#29992;&#25143;&#30340;&#36816;&#21160;&#26469;&#35782;&#21035;&#25193;&#23637;&#29616;&#23454;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#8220;&#21322;&#34928;&#26399;&#65306;Alyx&#8221;VR&#28216;&#25103;&#30340;&#29992;&#25143;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#30340;&#22522;&#32447;&#20998;&#31867;&#27169;&#22411;&#20316;&#20026;&#23545;&#27604;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#23884;&#20837;&#24335;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21482;&#20351;&#29992;&#20960;&#20998;&#38047;&#30340;&#27880;&#20876;&#25968;&#25454;&#65292;&#35782;&#21035;&#26032;&#29992;&#25143;&#30340;&#38750;&#29305;&#23450;&#36816;&#21160;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#27880;&#20876;&#26032;&#29992;&#25143;&#65292;&#32780;&#37325;&#26032;&#35757;&#32451;&#22522;&#32447;&#26041;&#27861;&#38656;&#35201;&#33457;&#36153;&#23558;&#36817;&#19968;&#22825;&#30340;&#26102;&#38388;&#65292;&#24403;&#21482;&#26377;&#24456;&#23569;&#30340;&#27880;&#20876;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#21487;&#38752;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#20351;&#29992;&#19981;&#21516;VR&#35774;&#22791;&#35760;&#24405;&#30340;&#26032;&#29992;&#25143;&#25968;&#25454;&#38598;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20026;&#26131;&#20110;&#25193;&#23637;&#30340;XR&#29992;&#25143;&#35782;&#21035;&#31995;&#32479;&#22880;&#23450;&#22522;&#30784;&#65292;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we combine the strengths of distance-based and classification-based approaches for the task of identifying extended reality users by their movements. For this we present an embedding-based approach that leverages deep metric learning. We train the model on a dataset of users playing the VR game ``Half-Life: Alyx'' and conduct multiple experiments and analyses using a state of the art classification-based model as baseline. The results show that the embedding-based method 1) is able to identify new users from non-specific movements using only a few minutes of enrollment data, 2) can enroll new users within seconds, while retraining the baseline approach takes almost a day, 3) is more reliable than the baseline approach when only little enrollment data is available, 4) can be used to identify new users from another dataset recorded with different VR devices.  Altogether, our solution is a foundation for easily extensible XR user identification systems, applicable to a wide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65292;&#35299;&#20026;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#24182;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#34987;&#31216;&#20026;Gibbs&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.06617</link><description>&lt;p&gt;
&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Empirical Risk Minimization with Relative Entropy Regularization. (arXiv:2211.06617v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65292;&#35299;&#20026;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#24182;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#34987;&#31216;&#20026;Gibbs&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20551;&#23450;&#21442;&#32771;&#24230;&#37327;&#20026;sigma&#26377;&#38480;&#27979;&#24230;&#65288;measure&#65289;&#32780;&#38750;&#27010;&#29575;&#27979;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#30456;&#23545;&#29109;&#27491;&#21017;&#21270;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM-RER&#65289;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#20551;&#35774;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;ERM-RER&#38382;&#39064;&#30340;&#27867;&#21270;&#65292;&#20801;&#35768;&#26356;&#22823;&#31243;&#24230;&#22320;&#28789;&#27963;&#22320;&#24182;&#20837;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#36825;&#20123;&#24615;&#36136;&#20013;&#65292;&#22914;&#26524;&#23384;&#22312;ERM-RER&#38382;&#39064;&#30340;&#35299;&#65292;&#21017;&#35813;&#35299;&#26159;&#21807;&#19968;&#30340;&#27010;&#29575;&#27979;&#24230;&#65292;&#36890;&#24120;&#19982;&#21442;&#32771;&#27979;&#24230;&#30456;&#20114;&#32477;&#23545;&#36830;&#32493;&#12290;&#36825;&#26679;&#30340;&#35299;&#23545;&#20110;ERM&#38382;&#39064;&#23637;&#29616;&#20102;&#20960;&#20046;&#27491;&#30830;&#30340;&#20445;&#35777;&#65292;&#32780;&#19981;&#38656;&#20851;&#24515;ERM&#38382;&#39064;&#26159;&#21542;&#26377;&#35299;&#12290;&#24403;&#20174;ERM-RER&#38382;&#39064;&#30340;&#35299;&#25277;&#21462;&#27169;&#22411;&#26102;&#65292;&#22266;&#23450;&#25968;&#25454;&#38598;&#26102;&#65292;&#32463;&#39564;&#39118;&#38505;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#20122;&#39640;&#26031;&#38543;&#26426;&#21464;&#37327;&#12290;ERM-RER&#38382;&#39064;&#30340;&#35299;&#65288;Gibbs&#31639;&#27861;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The empirical risk minimization (ERM) problem with relative entropy regularization (ERM-RER) is investigated under the assumption that the reference measure is a {\sigma}-finite measure, and not necessarily a probability measure. Under this assumption, which leads to a generalization of the ERM-RER problem allowing a larger degree of flexibility for incorporating prior knowledge, numerous relevant properties are stated. Among these properties, the solution to this problem, if it exists, is shown to be a unique probability measure, often mutually absolutely continuous with the reference measure. Such a solution exhibits a probably-approximately-correct guarantee for the ERM problem independently of whether the latter possesses a solution. For a fixed dataset, the empirical risk is shown to be a sub-Gaussian random variable when the models are sampled from the solution to the ERM-RER problem. The generalization capabilities of the solution to the ERM-RER problem (the Gibbs algorithm) are
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#36712;&#36947;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;&#29289;&#29702;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25512;&#23548;&#26410;&#27979;&#37327;&#29289;&#20307;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#36712;&#36947;&#39044;&#27979;&#20934;&#30830;&#24230;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2207.08993</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#36712;&#36947;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning in Orbit Estimation: a Survey. (arXiv:2207.08993v3 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#36712;&#36947;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#35752;&#35770;&#20102;&#24403;&#21069;&#29289;&#29702;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#25512;&#23548;&#26410;&#27979;&#37327;&#29289;&#20307;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#36712;&#36947;&#39044;&#27979;&#20934;&#30830;&#24230;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#19978;&#19990;&#32426;50&#24180;&#20195;&#21457;&#23556;&#20102;&#31532;&#19968;&#39063;&#20154;&#36896;&#21355;&#26143;&#20197;&#26469;&#65292;&#36712;&#36947;&#19978;&#30340;&#31354;&#38388;&#29289;&#20307;&#25968;&#37327;&#25345;&#32493;&#22686;&#21152;&#12290;&#30446;&#21069;&#20272;&#35745;&#22320;&#29699;&#19978;&#26377;&#19968;&#30334;&#19975;&#20010;&#22823;&#23567;&#36229;&#36807;&#19968;&#21400;&#31859;&#30340;&#29289;&#20307;&#27491;&#22312;&#32469;&#22320;&#36816;&#34892;&#65292;&#20854;&#20013;&#21482;&#26377;&#19977;&#19975;&#20010;&#22823;&#23567;&#36229;&#36807;&#21313;&#21400;&#31859;&#30340;&#29289;&#20307;&#34987;&#36319;&#36394;&#12290;&#20026;&#20102;&#36991;&#20813;&#30896;&#25758;&#38142;&#21453;&#24212;&#65292;&#21363;&#22522;&#26031;&#21202;&#30151;&#20505;&#32676;&#30340;&#21457;&#29983;&#65292;&#24517;&#39035;&#20934;&#30830;&#22320;&#36319;&#36394;&#21644;&#39044;&#27979;&#30862;&#29255;&#21644;&#21355;&#26143;&#30340;&#36712;&#36947;&#12290;&#24403;&#21069;&#30340;&#36817;&#20284;&#29289;&#29702;&#26041;&#27861;&#23545;&#20110;&#19971;&#22825;&#30340;&#39044;&#27979;&#35823;&#24046;&#22312;&#20844;&#37324;&#32423;&#21035;&#65292;&#36825;&#22312;&#32771;&#34385;&#36890;&#24120;&#23567;&#20110;&#19968;&#31859;&#30340;&#31354;&#38388;&#30862;&#29255;&#26102;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#36825;&#31181;&#22833;&#36133;&#36890;&#24120;&#26159;&#30001;&#20110;&#22312;&#36712;&#36947;&#36215;&#22987;&#28857;&#38468;&#36817;&#30340;&#31354;&#38388;&#29289;&#20307;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29615;&#22659;&#26465;&#20214;&#65288;&#22914;&#22823;&#27668;&#38459;&#21147;&#65289;&#30340;&#39044;&#27979;&#35823;&#24046;&#20197;&#21450;&#31354;&#38388;&#29289;&#20307;&#30340;&#26410;&#30693;&#29305;&#24449;&#65288;&#22914;&#36136;&#37327;&#25110;&#20960;&#20309;&#24418;&#29366;&#65289;&#25152;&#33268;&#12290;&#25805;&#20316;&#21592;&#21487;&#20197;&#36890;&#36807;&#25512;&#23548;&#26410;&#27979;&#37327;&#29289;&#20307;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#36712;&#36947;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the late 1950s, when the first artificial satellite was launched, the number of Resident Space Objects has steadily increased. It is estimated that around one million objects larger than one cm are currently orbiting the Earth, with only thirty thousand larger than ten cm being tracked. To avert a chain reaction of collisions, known as Kessler Syndrome, it is essential to accurately track and predict debris and satellites' orbits. Current approximate physics-based methods have errors in the order of kilometers for seven-day predictions, which is insufficient when considering space debris, typically with less than one meter. This failure is usually due to uncertainty around the state of the space object at the beginning of the trajectory, forecasting errors in environmental conditions such as atmospheric drag, and unknown characteristics such as the mass or geometry of the space object. Operators can enhance Orbit Prediction accuracy by deriving unmeasured objects' characteristics
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#38754;&#21521;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#21160;&#24577;&#36951;&#25022;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#20013;&#36827;&#19968;&#27493;&#22686;&#24378;&#20445;&#35777;&#65292;&#20855;&#26377;&#20960;&#20309;&#30452;&#35266;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.14368</link><description>&lt;p&gt;
&#38750;&#38745;&#24577;&#36866;&#24212;&#24615;&#65306;&#38754;&#21521;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#38382;&#39064;&#30456;&#20851;&#30340;&#21160;&#24577;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization. (arXiv:2112.14368v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#38754;&#21521;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#21160;&#24577;&#36951;&#25022;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#20013;&#36827;&#19968;&#27493;&#22686;&#24378;&#20445;&#35777;&#65292;&#20855;&#26377;&#20960;&#20309;&#30452;&#35266;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#24182;&#36873;&#25321;&#21160;&#24577;&#36951;&#25022;&#20316;&#20026;&#24615;&#33021;&#24230;&#37327;&#65292;&#23450;&#20041;&#20026;&#22312;&#32447;&#31639;&#27861;&#21644;&#20219;&#20309;&#21487;&#34892;&#27604;&#36739;&#22120;&#24207;&#21015;&#25152;&#32047;&#35745;&#30340;&#25439;&#22833;&#20043;&#38388;&#30340;&#24046;&#20540;&#12290;&#20551;&#35774;$T$&#26159;&#26102;&#38388;&#38271;&#24230;&#65292;$P_T$&#26159;&#23454;&#36136;&#19978;&#21453;&#26144;&#29615;&#22659;&#38750;&#38745;&#24577;&#24615;&#30340;&#36335;&#24452;&#38271;&#24230;&#65292;&#21017;&#26368;&#20808;&#36827;&#30340;&#21160;&#24577;&#36951;&#25022;&#26159;$\mathcal{O}(\sqrt{T(1+P_T)})$&#12290;&#23613;&#31649;&#36825;&#20010;&#30028;&#38480;&#34987;&#35777;&#26126;&#23545;&#20110;&#20984;&#20989;&#25968;&#26159;&#26368;&#23567;&#21270;&#30340;&#65292;&#20294;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#65292;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#20013;&#65292;&#29305;&#21035;&#26159;&#24403;&#22312;&#32447;&#20989;&#25968;&#26159;&#20809;&#28369;&#30340;&#26102;&#20505;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20445;&#35777;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26032;&#39062;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20809;&#28369;&#24615;&#65292;&#24182;&#29992;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#21464;&#21270;&#12289;&#27604;&#36739;&#22120;&#24207;&#21015;&#30340;&#32047;&#35745;&#25439;&#22833;&#21644;&#36825;&#20004;&#20010;&#39033;&#30340;&#26368;&#23567;&#20540;&#20195;&#26367;&#21160;&#24577;&#36951;&#25022;&#20013;&#23545;$T$&#30340;&#20381;&#36182;&#12290;&#36825;&#20123;&#37327;&#34987;&#35777;&#26126;&#20855;&#26377;&#20960;&#20309;&#30452;&#35266;&#24615;&#65292;&#24182;&#19988;&#19982;&#29615;&#22659;&#20013;&#30340;&#38750;&#38745;&#24577;&#29616;&#35937;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate online convex optimization in non-stationary environments and choose the dynamic regret as the performance measure, defined as the difference between cumulative loss incurred by the online algorithm and that of any feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the path length that essentially reflects the non-stationarity of environments, the state-of-the-art dynamic regret is $\mathcal{O}(\sqrt{T(1+P_T)})$. Although this bound is proved to be minimax optimal for convex functions, in this paper, we demonstrate that it is possible to further enhance the guarantee for some easy problem instances, particularly when online functions are smooth. Specifically, we introduce novel online algorithms that can exploit smoothness and replace the dependence on $T$ in dynamic regret with problem-dependent quantities: the variation in gradients of loss functions, the cumulative loss of the comparator sequence, and the minimum of these two terms. These quantitie
&lt;/p&gt;</description></item></channel></rss>