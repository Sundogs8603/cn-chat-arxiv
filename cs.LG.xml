<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;</title><link>https://rss.arxiv.org/abs/2402.00957</link><description>&lt;p&gt;
&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Credal Learning Theory
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#20219;&#23398;&#20064;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#20984;&#38598;&#30340;&#27010;&#29575;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#20449;&#20219;&#38598;&#65292;&#24182;&#25512;&#23548;&#20986;bounds&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#20026;&#20174;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20013;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#39118;&#38505;&#25552;&#20379;&#29702;&#35770;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#21464;&#21270;&#65292;&#23548;&#33268;&#39046;&#22495;&#36866;&#24212;/&#27867;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#8220;&#20449;&#20219;&#8221;&#23398;&#20064;&#29702;&#35770;&#30340;&#22522;&#30784;&#65292;&#20351;&#29992;&#27010;&#29575;&#30340;&#20984;&#38598;&#65288;&#20449;&#20219;&#38598;&#65289;&#26469;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26679;&#30340;&#20449;&#20219;&#38598;&#21487;&#20197;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#23545;&#20110;&#26377;&#38480;&#20551;&#35774;&#31354;&#38388;&#65288;&#26080;&#35770;&#26159;&#21542;&#21487;&#23454;&#29616;&#65289;&#21644;&#26080;&#38480;&#27169;&#22411;&#31354;&#38388;&#65292;&#25512;&#23548;&#20986;&#30028;&#38480;&#65292;&#36825;&#30452;&#25509;&#25512;&#24191;&#20102;&#32463;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learnt from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a `credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not) as well as infinite model spaces, which directly generalize classical results.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#22240;&#26524;&#26410;&#26469;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2404.00462</link><description>&lt;p&gt;
&#20855;&#26377;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#30340;&#26080;&#20154;&#39550;&#39542;&#27773;&#36710;&#38646;&#23556;&#20987;&#23433;&#20840;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00462
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#30452;&#25509;&#39044;&#27979;&#22240;&#26524;&#26410;&#26469;&#29366;&#24577;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27169;&#22411;&#21019;&#36896;&#19968;&#20010;&#20195;&#29702;&#19990;&#30028;&#26469;&#35757;&#32451;&#25511;&#21046;&#22120;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#30340;&#20869;&#37096;&#21160;&#24577;&#27169;&#22411;&#26469;&#39044;&#27979;&#23433;&#20840;&#36829;&#35268;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#32479;&#35745;&#23398;&#20064;&#22914;&#20309;&#35266;&#23519;&#38543;&#30528;&#34892;&#21160;&#32780;&#21464;&#21270;&#65292;&#32570;&#20047;&#23545;&#20195;&#29702;&#21160;&#24577;&#20934;&#30830;&#24615;&#30340;&#31934;&#30830;&#37327;&#21270;&#65292;&#36825;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#26500;&#25104;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#35266;&#23519;&#23884;&#20837;&#21040;&#26377;&#24847;&#20041;&#19988;&#22240;&#26524;&#28508;&#22312;&#30340;&#34920;&#31034;&#20013;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#20195;&#29702;&#21160;&#24577;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#22240;&#26524;&#26410;&#26469;&#29366;&#24577;&#12290;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#36825;&#31181;&#26032;&#39062;&#27169;&#22411;&#22312;&#23433;&#20840;&#39044;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#26631;&#20934;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#19988;&#24615;&#33021;&#19982;&#30417;&#30563;&#23398;&#20064;&#30456;&#24403;&#65292;&#23613;&#31649;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#26356;&#19987;&#19994;&#21644;&#31995;&#32479;&#30456;&#20851;&#30340;&#25351;&#26631;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00462v1 Announce Type: new  Abstract: A world model creates a surrogate world to train a controller and predict safety violations by learning the internal dynamic model of systems. However, the existing world models rely solely on statistical learning of how observations change in response to actions, lacking precise quantification of how accurate the surrogate dynamics are, which poses a significant challenge in safety-critical systems. To address this challenge, we propose foundation world models that embed observations into meaningful and causally latent representations. This enables the surrogate dynamics to directly predict causal future states by leveraging a training-free large language model. In two common benchmarks, this novel model outperforms standard world models in the safety prediction task and has a performance comparable to supervised learning despite not using any data. We evaluate its performance with a more specialized and system-relevant metric by compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#65292;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#25552;&#21462;&#23494;&#38598;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;Triplet SSM&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.17701</link><description>&lt;p&gt;
&#26059;&#36716;&#25195;&#25551;&#65306;&#24102;&#26377;&#19977;&#20803;SSM&#27169;&#22359;&#30340;UNet-like Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#65292;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#25552;&#21462;&#23494;&#38598;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;Triplet SSM&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#21344;&#25454;&#37325;&#35201;&#20301;&#32622;&#12290;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#27169;&#22411;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#30001;&#20110;&#26377;&#38480;&#24863;&#21463;&#37326;&#25110;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#65292;&#29305;&#21035;&#26159;Mamba&#21450;&#20854;&#21464;&#20307;&#65292;&#22312;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21487;&#33021;&#19981;&#22815;&#26377;&#25928;&#65292;&#20445;&#30041;&#20102;&#19968;&#20123;&#20887;&#20313;&#32467;&#26500;&#65292;&#30041;&#19979;&#20102;&#21442;&#25968;&#20943;&#23569;&#30340;&#31354;&#38388;&#12290;&#21463;&#20808;&#21069;&#30340;&#31354;&#38388;&#21644;&#36890;&#36947;&#27880;&#24847;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triplet Mamba-UNet&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#27531;&#20313;VSS&#22359;&#26469;&#25552;&#21462;&#23494;&#38598;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#21516;&#26102;&#21033;&#29992;Triplet SSM&#26469;&#34701;&#21512;&#31354;&#38388;&#21644;&#36890;&#36947;&#32500;&#24230;&#19978;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;ISIC17&#12289;ISIC18&#12289;CVC-300&#12289;CVC-ClinicDB&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17701v1 Announce Type: cross  Abstract: Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain. Traditional convolutional neural networks (CNNs) and Transformer models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity. Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision. However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction. Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet. The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;</title><link>https://arxiv.org/abs/2403.16369</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21464;&#24615;&#23398;&#20064;&#22522;&#20110;&#21160;&#20316;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Action-based Representations Using Invariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#39640;&#32500;&#24230;&#35266;&#27979;&#24517;&#39035;&#33021;&#22815;&#22312;&#35768;&#22810;&#22806;&#28304;&#24615;&#24178;&#25200;&#20013;&#35782;&#21035;&#30456;&#20851;&#29366;&#24577;&#29305;&#24449;&#12290;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21487;&#25511;&#24615;&#30340;&#34920;&#31034;&#36890;&#36807;&#30830;&#23450;&#24433;&#21709;&#20195;&#29702;&#25511;&#21046;&#30340;&#22240;&#32032;&#26469;&#35782;&#21035;&#36825;&#20123;&#29366;&#24577;&#20803;&#32032;&#12290;&#34429;&#28982;&#35832;&#22914;&#36870;&#21160;&#21147;&#23398;&#21644;&#20114;&#20449;&#24687;&#31561;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#26377;&#38480;&#25968;&#37327;&#30340;&#26102;&#38388;&#27493;&#30340;&#21487;&#25511;&#24615;&#65292;&#20294;&#25429;&#33719;&#38271;&#26102;&#38388;&#20803;&#32032;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30701;&#35270;&#30340;&#21487;&#25511;&#24615;&#21487;&#20197;&#25429;&#25417;&#20195;&#29702;&#21363;&#23558;&#25758;&#21521;&#22681;&#22721;&#30340;&#30636;&#38388;&#65292;&#20294;&#19981;&#33021;&#22312;&#20195;&#29702;&#36824;&#26377;&#19968;&#23450;&#36317;&#31163;&#20043;&#26102;&#25429;&#25417;&#22681;&#22721;&#30340;&#25511;&#21046;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#27169;&#25311;&#19981;&#21464;&#37327;&#20551;&#24230;&#37327;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#23398;&#20064;&#20102;&#19968;&#20010;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16369v1 Announce Type: cross  Abstract: Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts dist
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.11894</link><description>&lt;p&gt;
&#20174;&#21487;&#35299;&#37322;&#21040;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#23454;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#21307;&#30103;&#20445;&#20581;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DL&#30340;NLP&#26041;&#27861;&#26085;&#30410;&#22797;&#26434;&#65292;&#38656;&#35201;&#36879;&#26126;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#25110;&#33267;&#23569;&#26159;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#23545;&#21307;&#30103;&#20581;&#24247;NLP&#20013;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;DL&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#33539;&#22260;&#23457;&#26597;&#12290;&#24341;&#20837;&#20102;&#26415;&#35821;&#8220;XIAI&#8221;&#65288;eXplainable&#21644;Interpretable Artificial Intelligence&#65289;&#20197;&#21306;&#20998;XAI&#21644;IAI&#12290;&#26041;&#27861;&#26681;&#25454;&#20854;&#21151;&#33021;&#65288;&#27169;&#22411;&#12289;&#36755;&#20837;&#12289;&#36755;&#20986;&#20026;&#22522;&#30784;&#65289;&#21644;&#33539;&#22260;&#65288;&#23616;&#37096;&#12289;&#20840;&#23616;&#65289;&#36827;&#19968;&#27493;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27880;&#24847;&#26426;&#21046;&#26159;&#26368;&#20027;&#35201;&#30340;&#26032;&#20852;IAI&#12290;&#27492;&#22806;&#65292;IAI&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#23545;&#25239;XAI&#12290;&#30830;&#23450;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;XIAI&#19981;&#25506;&#32034;&#8220;&#20840;&#23616;&#8221;&#24314;&#27169;&#36807;&#31243;&#65292;&#32570;&#20047;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#19988;&#38656;&#35201;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.09891</link><description>&lt;p&gt;
Fisher Mask&#33410;&#28857;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Fisher Mask Nodes for Language Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09891
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;Transformers&#30340;&#26032;&#22411;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#21033;&#29992;Fisher&#20449;&#24687;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#19979;&#28216;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22914;BERT&#21450;&#20854;&#34893;&#29983;&#29289;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26222;&#36941;&#24615;&#20063;&#23548;&#33268;&#20102;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#27169;&#22411;&#30340;&#28608;&#22686;&#12290;&#22312;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#19968;&#39033;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#38598;&#25104;&#12290;&#27169;&#22411;&#21512;&#24182;&#36825;&#19968;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#23558;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#21512;&#24182;&#20026;&#21333;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;Transformers&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20808;&#21069;Fisher&#21152;&#26435;&#24179;&#22343;&#21644;Fisher&#20449;&#24687;&#22312;&#27169;&#22411;&#20462;&#21098;&#20013;&#30340;&#24212;&#29992;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#21033;&#29992;Transformer&#26550;&#26500;&#20869;&#30340;mask&#33410;&#28857;&#30340;Fisher&#20449;&#24687;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#21152;&#26435;&#24179;&#22343;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09891v1 Announce Type: cross  Abstract: Fine-tuning pre-trained models provides significant advantages in downstream performance. The ubiquitous nature of pre-trained models such as BERT and its derivatives in natural language processing has also led to a proliferation of task-specific fine-tuned models. As these models typically only perform one task well, additional training or ensembling is required in multi-task scenarios. The growing field of model merging provides a solution, dealing with the challenge of combining multiple task-specific models into a single multi-task model. In this study, we introduce a novel model merging method for Transformers, combining insights from previous work in Fisher-weighted averaging and the use of Fisher information in model pruning. Utilizing the Fisher information of mask nodes within the Transformer architecture, we devise a computationally efficient weighted-averaging scheme. Our method exhibits a regular and significant performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#26408;&#26143;&#20869;&#37096;&#30913;&#22330;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#28165;&#26224;&#22320;&#35299;&#26512;&#23616;&#37096;&#32467;&#26500;&#24182;&#36991;&#20813;&#28145;&#24230;&#22122;&#22768;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2403.07507</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#26408;&#26143;&#30340;&#30913;&#22330;
&lt;/p&gt;
&lt;p&gt;
Reconstructions of Jupiter's magnetic field using physics informed neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#26408;&#26143;&#20869;&#37096;&#30913;&#22330;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#28165;&#26224;&#22320;&#35299;&#26512;&#23616;&#37096;&#32467;&#26500;&#24182;&#36991;&#20813;&#28145;&#24230;&#22122;&#22768;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#30001;Juno&#20219;&#21153;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#30913;&#27979;&#21487;&#20197;&#29992;&#26469;&#32422;&#26463;&#26408;&#26143;&#30340;&#20869;&#37096;&#12290;&#28982;&#32780;&#65292;&#20551;&#35774;&#30005;&#23548;&#29575;&#20026;&#38646;&#24182;&#20197;&#29699;&#35856;&#20989;&#25968;&#34920;&#31034;&#30340;&#37325;&#24314;&#21521;&#20869;&#32487;&#32493;&#26159;&#21463;&#21040;&#23567;&#23610;&#24230;&#22122;&#22768;&#22686;&#24378;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26408;&#26143;&#20869;&#37096;&#30913;&#22330;&#30340;&#26032;&#37325;&#24314;&#26041;&#27861;&#65292;&#20351;&#29992;Juno&#36712;&#36947;&#30340;&#21069;33&#20010;&#65288;PINN33&#65289;&#25110;&#21069;50&#20010;&#65288;PINN50&#65289;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#26512;&#20986;&#23616;&#37096;&#32467;&#26500;&#65292;&#24182;&#20801;&#35768;&#23384;&#22312;&#24369;&#29615;&#22659;&#30005;&#27969;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#26408;&#26143;&#30913;&#22330;&#30340;&#34920;&#38754;&#21644;&#19978;&#26041;&#30340;&#37325;&#24314;&#37117;&#30456;&#20284;&#65292;&#24182;&#19988;&#19982;Juno&#25968;&#25454;&#25311;&#21512;&#30456;&#20284;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#21463;&#28145;&#24230;&#22122;&#22768;&#24178;&#25200;&#65292;&#22240;&#27492;&#33021;&#22815;&#25552;&#20379;&#26356;&#28165;&#26224;&#30340;&#20869;&#37096;&#32467;&#26500;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07507v1 Announce Type: cross  Abstract: Magnetic sounding using data collected from the Juno mission can be used to provide constraints on Jupiter's interior. However, inwards continuation of reconstructions assuming zero electrical conductivity and a representation in spherical harmonics are limited by the enhancement of noise at small scales. In this paper we describe new reconstructions of Jupiter's internal magnetic field based on physics-informed neural networks and either the first 33 (PINN33) or the first 50 (PINN50) of Juno's orbits. The method can resolve local structures, and allows for weak ambient electrical currents. Compared with other methods, our reconstructions of Jupiter's magnetic field both on and above the surface are similar, and we achieve a similar fit to the Juno data. However, our models are not hampered by noise at depth, and so offer a much clearer picture of the interior structure. We estimate that the dynamo boundary is at a fractional radius of
&lt;/p&gt;</description></item><item><title>Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02545</link><description>&lt;p&gt;
Wukong: &#36808;&#21521;&#22823;&#35268;&#27169;&#25512;&#33616;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Wukong: Towards a Scaling Law for Large-Scale Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02545
&lt;/p&gt;
&lt;p&gt;
Wukong&#36890;&#36807;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#65292;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#26631;&#24230;&#24459;&#65292;&#24182;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#25512;&#33616;&#27169;&#22411;&#24182;&#27809;&#26377;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#35266;&#23519;&#21040;&#30340;&#23450;&#24459;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#21319;&#32423;&#26426;&#21046;&#30340;&#20302;&#25928;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32431;&#22534;&#21472;&#22240;&#23376;&#20998;&#35299;&#26426;&#21644;&#21327;&#21516;&#22686;&#38271;&#31574;&#30053;&#30340;&#26377;&#25928;&#32593;&#32476;&#26550;&#26500;&#65292;&#32479;&#31216;&#20026;Wukong&#65292;&#20197;&#22312;&#25512;&#33616;&#39046;&#22495;&#24314;&#31435;&#19968;&#20010;&#26631;&#24230;&#24459;&#12290;Wukong&#30340;&#29420;&#29305;&#35774;&#35745;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#26356;&#39640;&#26356;&#23485;&#30340;&#23618;&#27425;&#31616;&#21333;&#25429;&#33719;&#21508;&#31181;&#20219;&#24847;&#38454;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;Wukong&#22312;&#36136;&#37327;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20248;&#36234;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;Wuko
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02545v1 Announce Type: cross  Abstract: Scaling laws play an instrumental role in the sustainable improvement in model quality. Unfortunately, recommendation models to date do not exhibit such laws similar to those observed in the domain of large language models, due to the inefficiencies of their upscaling mechanisms. This limitation poses significant challenges in adapting these models to increasingly more complex real-world datasets. In this paper, we propose an effective network architecture based purely on stacked factorization machines, and a synergistic upscaling strategy, collectively dubbed Wukong, to establish a scaling law in the domain of recommendation. Wukong's unique design makes it possible to capture diverse, any-order of interactions simply through taller and wider layers. We conducted extensive evaluations on six public datasets, and our results demonstrate that Wukong consistently outperforms state-of-the-art models quality-wise. Further, we assessed Wuko
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14095</link><description>&lt;p&gt;
&#36328;&#26550;&#26500;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-shot generalization across architectures for visual classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14095
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#22312;&#36328;&#26550;&#26500;&#21644;&#23618;&#38388;&#27867;&#21270;&#21040;&#26410;&#30693;&#31867;&#21035;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#65292;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#19982;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26497;&#31616;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#27867;&#21270;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;&#20174;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;CNNs&#65289;&#21040;transformers&#30340;&#27969;&#34892;&#32593;&#32476;&#22312;&#36890;&#36807;&#23618;&#21644;&#26550;&#26500;&#27867;&#21270;&#21040;&#26410;&#35265;&#31867;&#21035;&#26041;&#38754;&#30340;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#20934;&#30830;&#24615;&#24182;&#19981;&#26159;&#27867;&#21270;&#33021;&#21147;&#30340;&#33391;&#22909;&#39044;&#27979;&#22240;&#23376;&#65292;&#24182;&#19988;&#27867;&#21270;&#33021;&#21147;&#38543;&#30528;&#23618;&#28145;&#24230;&#21576;&#38750;&#21333;&#35843;&#21464;&#21270;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/dyballa/zero-shot-generalization &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14095v1 Announce Type: cross  Abstract: Generalization to unseen data is a key desideratum for deep networks, but its relation to classification accuracy is unclear. Using a minimalist vision dataset and a measure of generalizability, we show that popular networks, from deep convolutional networks (CNNs) to transformers, vary in their power to extrapolate to unseen classes both across layers and across architectures. Accuracy is not a good predictor of generalizability, and generalization varies non-monotonically with layer depth. Code is available at https://github.com/dyballa/zero-shot-generalization.
&lt;/p&gt;</description></item><item><title>rnaglib&#26159;&#19968;&#20010;&#22522;&#20110;3D&#30340;RNA&#21151;&#33021;&#39044;&#27979;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;RNA 3D&#32467;&#26500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09330</link><description>&lt;p&gt;
rnaglib&#20013;&#22522;&#20110;3D&#30340;RNA&#21151;&#33021;&#39044;&#27979;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
3D-based RNA function prediction tools in rnaglib
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09330
&lt;/p&gt;
&lt;p&gt;
rnaglib&#26159;&#19968;&#20010;&#22522;&#20110;3D&#30340;RNA&#21151;&#33021;&#39044;&#27979;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;RNA 3D&#32467;&#26500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;RNA&#30340;&#22797;&#26434;&#32467;&#26500;&#29305;&#24449;&#19982;&#29983;&#29289;&#21151;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#26159;&#36827;&#21270;&#30740;&#31350;&#21644;RNA&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;RNA 3D&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#36866;&#24403;&#30340;&#24314;&#27169;&#36873;&#25321;&#20173;&#28982;&#32791;&#26102;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;RNA 3D&#32467;&#26500;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21151;&#33021;&#39044;&#27979;&#27169;&#22411;&#30340;rnaglib&#30340;&#20351;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09330v1 Announce Type: cross Abstract: Understanding the connection between complex structural features of RNA and biological function is a fundamental challenge in evolutionary studies and in RNA design. However, building datasets of RNA 3D structures and making appropriate modeling choices remains time-consuming and lacks standardization. In this chapter, we describe the use of rnaglib, to train supervised and unsupervised machine learning-based function prediction models on datasets of RNA 3D structures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#35757;&#32451;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2312.03853</link><description>&lt;p&gt;
LLMs&#30340;&#20004;&#38754;&#24615;&#65306;Jekyll&#21338;&#22763;&#19982;Hyde&#20808;&#29983;
&lt;/p&gt;
&lt;p&gt;
Dr. Jekyll and Mr. Hyde: Two Faces of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#35757;&#32451;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20165;&#19968;&#24180;&#21069;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#32467;&#21512;&#20687;&#32842;&#22825;&#26426;&#22120;&#20154;&#21161;&#25163;&#20043;&#31867;&#30340;&#24212;&#29992;&#26102;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#21161;&#25163;&#20135;&#29983;&#19981;&#24403;&#22238;&#24212;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#65288;&#20197;&#21450;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;Bing chat&#65289;&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#36825;&#20123;&#25514;&#26045;&#65292;&#36825;&#20123;&#35282;&#33394;&#19982;&#23427;&#20204;&#26412;&#24212;&#25104;&#20026;&#30340;&#30495;&#23454;&#21161;&#25163;&#30340;&#29305;&#24449;&#30456;&#21453;&#12290;&#25105;&#20204;&#39318;&#20808;&#21019;&#36896;&#20986;&#36825;&#20123;&#20154;&#29289;&#35282;&#33394;&#30340;&#22797;&#26434;&#20256;&#35760;&#65292;&#28982;&#21518;&#22312;&#21516;&#19968;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#26032;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#23545;&#35805;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#39118;&#26684;&#65292;&#20197;&#33719;&#24471;&#21161;&#25163;&#19981;&#34987;&#20801;&#35768;&#25552;&#20379;&#30340;&#22238;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#20154;&#29289;&#35282;&#33394;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#24615;pe
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03853v2 Announce Type: replace-cross  Abstract: Only a year ago, we witnessed a rise in the use of Large Language Models (LLMs), especially when combined with applications like chatbot assistants. Safety mechanisms and specialized training procedures are implemented to prevent improper responses from these assistants. In this work, we bypass these measures for ChatGPT and Bard (and, to some extent, Bing chat) by making them impersonate complex personas with opposite characteristics as those of the truthful assistants they are supposed to be. We start by creating elaborate biographies of these personas, which we then use in a new session with the same chatbots. Our conversation followed a role-play style to get the response the assistant was not allowed to provide. By making use of personas, we show that the response that is prohibited is actually provided, making it possible to obtain unauthorized, illegal, or harmful information. This work shows that by using adversarial pe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#23454;&#29616;&#35299;&#37322;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2311.16834</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#20351;&#29992;&#27880;&#24847;&#21147;&#36827;&#34892;&#35299;&#37322;&#24615;&#21644;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Modular Neural Networks for Time Series Forecasting: Interpretability and Feature Selection using Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#23454;&#29616;&#35299;&#37322;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#27668;&#35937;&#23398;&#21644;&#29983;&#21629;&#31185;&#23398;&#31561;&#39046;&#22495;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#34987;&#25209;&#35780;&#20026;&#8220;&#40657;&#30418;&#8221;&#25110;&#26080;&#27861;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#27169;&#22359;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#26500;&#36896;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#36873;&#25321;&#32452;&#20214;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#24182;&#25233;&#21046;&#22312;&#23398;&#20064;&#26102;&#38388;&#20381;&#36182;&#24615;&#20013;&#20351;&#29992;&#30340;&#20887;&#20313;&#29305;&#24449;&#12290;&#20174;&#36873;&#25321;&#30340;&#29305;&#24449;&#29420;&#31435;&#35757;&#32451;&#27169;&#22359;&#21270;&#28145;&#24230;&#32593;&#32476;&#65292;&#21521;&#29992;&#25143;&#23637;&#31034;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16834v3 Announce Type: replace-cross  Abstract: Multivariate time series have many applications, from healthcare and meteorology to life science. Although deep learning models have shown excellent predictive performance for time series, they have been criticised for being "black-boxes" or non-interpretable. This paper proposes a novel modular neural network model for multivariate time series prediction that is interpretable by construction. A recurrent neural network learns the temporal dependencies in the data while an attention-based feature selection component selects the most relevant features and suppresses redundant features used in the learning of the temporal dependencies. A modular deep network is trained from the selected features independently to show the users how features influence outcomes, making the model interpretable. Experimental results show that this approach can outperform state-of-the-art interpretable Neural Additive Models (NAM) and variations thereo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;InceptionXML&#65292;&#36890;&#36807;&#22312;embedding&#32500;&#24230;&#19978;&#37325;&#26032;&#20998;&#37197;&#21367;&#31215;&#25805;&#20316;&#65292;&#24212;&#23545;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#32570;&#22833;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2109.07319</link><description>&lt;p&gt;
InceptionXML&#65306;&#19968;&#31181;&#24102;&#26377;&#21516;&#27493;&#36127;&#37319;&#26679;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#29992;&#20110;&#30701;&#25991;&#26412;&#26497;&#31471;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.07319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26694;&#26550;InceptionXML&#65292;&#36890;&#36807;&#22312;embedding&#32500;&#24230;&#19978;&#37325;&#26032;&#20998;&#37197;&#21367;&#31215;&#25805;&#20316;&#65292;&#24212;&#23545;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#32570;&#22833;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#25968;&#25454;&#23545;&#22823;&#37327;&#30446;&#26631;&#26631;&#31614;&#36827;&#34892;&#33258;&#21160;&#27880;&#37322;&#65292;&#34987;&#31216;&#20026;&#30701;&#25991;&#26412;&#26497;&#31471;&#20998;&#31867;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#21253;&#25324;&#30456;&#20851;&#25628;&#32034;&#39044;&#27979;&#21644;&#20135;&#21697;&#25512;&#33616;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#26550;&#26500;InceptionXML&#65292;&#20854;&#36731;&#37327;&#20294;&#21151;&#33021;&#24378;&#22823;&#65292;&#24182;&#19988;&#33021;&#22815;&#24212;&#23545;&#25628;&#32034;&#21644;&#25512;&#33616;&#20219;&#21153;&#20013;&#30701;&#25991;&#26412;&#26597;&#35810;&#20013;&#22266;&#26377;&#30340;&#32570;&#20047;&#21333;&#35789;&#39034;&#24207;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#21367;&#31215;&#30340;&#25805;&#20316;&#27839;&#30528;&#23884;&#20837;&#32500;&#24230;&#37325;&#26032;&#26500;&#24314;&#65292;&#32780;&#19981;&#26159;&#20687;&#20256;&#32479;CNNs&#19968;&#26679;&#27839;&#30528;&#21333;&#35789;&#32500;&#24230;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#24212;&#29992;&#21367;&#31215;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#25968;&#30334;&#19975;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;InceptionXML+&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#27493;&#26631;&#31614;&#31579;&#36873;&#22120;&#21644;&#26497;&#31471;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#21160;&#24577;&#30828;&#36127;&#37319;&#26679;&#25216;&#26415;&#22312;&#26631;&#31614;&#31579;&#36873;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.07319v3 Announce Type: replace-cross  Abstract: Automatic annotation of short-text data to a large number of target labels, referred to as Short Text Extreme Classification, has found numerous applications including prediction of related searches and product recommendation tasks. In this paper, we propose a convolutional architecture InceptionXML which is light-weight, yet powerful, and robust to the inherent lack of word-order in short-text queries encountered in search and recommendation tasks. We demonstrate the efficacy of applying convolutions by recasting the operation along the embedding dimension instead of the word dimension as applied in conventional CNNs for text classification. Towards scaling our model to datasets with millions of labels, we also propose InceptionXML+ framework which improves upon the shortcomings of the recently proposed dynamic hard-negative mining technique for label shortlisting by synchronizing the label-shortlister and extreme classifier. 
&lt;/p&gt;</description></item><item><title>LangProp&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36845;&#20195;&#20248;&#21270;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#24615;&#33021;&#21644;&#25429;&#25417;&#24322;&#24120;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#27010;&#24565;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.10314</link><description>&lt;p&gt;
LangProp: &#19968;&#31181;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LangProp: A code optimization framework using Language Models applied to driving. (arXiv:2401.10314v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10314
&lt;/p&gt;
&lt;p&gt;
LangProp&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36845;&#20195;&#20248;&#21270;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#23427;&#36890;&#36807;&#35780;&#20272;&#20195;&#30721;&#24615;&#33021;&#21644;&#25429;&#25417;&#24322;&#24120;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#27010;&#24565;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LangProp&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30417;&#30563;/&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#36845;&#20195;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#34429;&#28982;LLM&#33021;&#22815;&#38646;-shot&#22320;&#29983;&#25104;&#21512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#21021;&#22987;&#20195;&#30721;&#21487;&#33021;&#22312;&#26576;&#20123;&#36793;&#32536;&#24773;&#20917;&#19979;&#22833;&#36133;&#12290;LangProp&#33258;&#21160;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#20195;&#30721;&#24615;&#33021;&#65292;&#24182;&#25429;&#25417;&#20219;&#20309;&#24322;&#24120;&#65292;&#24182;&#23558;&#32467;&#26524;&#21453;&#39304;&#32473;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#20351;LLM&#21487;&#20197;&#36845;&#20195;&#25913;&#36827;&#20854;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#24230;&#37327;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#35757;&#32451;&#33539;&#24335;&#26469;&#36827;&#34892;&#20195;&#30721;&#20248;&#21270;&#36807;&#31243;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20511;&#37492;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#27169;&#20223;&#23398;&#20064;&#12289;DAgger&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;CARLA&#20013;&#33258;&#21160;&#39550;&#39542;&#30340;&#20195;&#30721;&#20248;&#21270;&#30340;&#31532;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;LangProp&#21487;&#20197;&#29983;&#25104;&#21487;&#35299;&#37322;&#21644;&#36879;&#26126;&#30340;&#39550;&#39542;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
LangProp is a framework for iteratively optimizing code generated by large language models (LLMs) in a supervised/reinforcement learning setting. While LLMs can generate sensible solutions zero-shot, the solutions are often sub-optimal. Especially for code generation tasks, it is likely that the initial code will fail on certain edge cases. LangProp automatically evaluates the code performance on a dataset of input-output pairs, as well as catches any exceptions, and feeds the results back to the LLM in the training loop, so that the LLM can iteratively improve the code it generates. By adopting a metricand data-driven training paradigm for this code optimization procedure, one could easily adapt findings from traditional machine learning techniques such as imitation learning, DAgger, and reinforcement learning. We demonstrate the first proof of concept of automated code optimization for autonomous driving in CARLA, showing that LangProp can generate interpretable and transparent dri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20998;&#26512;&#30340;&#27169;&#22411;&#36827;&#34892;&#21051;&#30011;&#12290;</title><link>http://arxiv.org/abs/2401.08788</link><description>&lt;p&gt;
&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Differential Feature Under-reporting on Algorithmic Fairness. (arXiv:2401.08788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20998;&#26512;&#30340;&#27169;&#22411;&#36827;&#34892;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#37096;&#38376;&#30340;&#39044;&#27979;&#39118;&#38505;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#26356;&#23436;&#25972;&#30340;&#34892;&#25919;&#25968;&#25454;&#26469;&#24320;&#21457;&#65292;&#36825;&#20123;&#25968;&#25454;&#23545;&#20110;&#26356;&#22823;&#31243;&#24230;&#20381;&#36182;&#20844;&#20849;&#26381;&#21153;&#30340;&#20122;&#32676;&#20307;&#26356;&#20026;&#23436;&#25972;&#12290;&#20363;&#22914;&#65292;&#22312;&#32654;&#22269;&#65292;&#23545;&#20110;&#30001;&#21307;&#30103;&#34917;&#21161;&#21644;&#21307;&#30103;&#20445;&#38505;&#25903;&#25345;&#30340;&#20010;&#20154;&#65292;&#25919;&#24220;&#26426;&#26500;&#24120;&#24120;&#21487;&#20197;&#33719;&#24471;&#26377;&#20851;&#21307;&#30103;&#20445;&#20581;&#21033;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#23545;&#20110;&#31169;&#20154;&#20445;&#38505;&#30340;&#20154;&#21017;&#27809;&#26377;&#12290;&#23545;&#20844;&#20849;&#37096;&#38376;&#31639;&#27861;&#30340;&#25209;&#35780;&#25351;&#20986;&#65292;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#23548;&#33268;&#31639;&#27861;&#20915;&#31574;&#20013;&#30340;&#19981;&#20844;&#24179;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25968;&#25454;&#20559;&#35265;&#22312;&#25216;&#26415;&#35270;&#35282;&#19979;&#20173;&#28982;&#30740;&#31350;&#19981;&#36275;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#28155;&#21152;&#29305;&#24449;&#22122;&#22768;&#21644;&#26126;&#30830;&#26631;&#35760;&#20026;&#32570;&#22833;&#30340;&#29305;&#24449;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#32570;&#22833;&#25351;&#26631;&#30340;&#25968;&#25454;&#32570;&#22833;&#24773;&#20917;&#65288;&#21363;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#65289;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20998;&#26512;&#30340;&#24046;&#24322;&#29305;&#24449;&#26410;&#25253;&#21578;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#29305;&#24449;&#26410;&#25253;&#21578;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#21051;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive risk models in the public sector are commonly developed using administrative data that is more complete for subpopulations that more greatly rely on public services. In the United States, for instance, information on health care utilization is routinely available to government agencies for individuals supported by Medicaid and Medicare, but not for the privately insured. Critiques of public sector algorithms have identified such differential feature under-reporting as a driver of disparities in algorithmic decision-making. Yet this form of data bias remains understudied from a technical viewpoint. While prior work has examined the fairness impacts of additive feature noise and features that are clearly marked as missing, the setting of data missingness absent indicators (i.e. differential feature under-reporting) has been lacking in research attention. In this work, we present an analytically tractable model of differential feature under-reporting which we then use to charac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#20013;&#38544;&#31169;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20999;&#21106;&#23618;&#23545;&#23458;&#25143;&#31471;&#33021;&#32791;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.09441</link><description>&lt;p&gt;
&#25506;&#32034;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;-&#33021;&#32791;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring the Privacy-Energy Consumption Tradeoff for Split Federated Learning. (arXiv:2311.09441v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#20013;&#38544;&#31169;&#21644;&#33021;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20999;&#21106;&#23618;&#23545;&#23458;&#25143;&#31471;&#33021;&#32791;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#25216;&#26415;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#20998;&#21106;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#23427;&#24378;&#35843;&#20102;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#21019;&#26032;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;SFL&#20013;&#27169;&#22411;&#22312;&#29305;&#23450;&#23618;&#65288;&#31216;&#20026;&#20999;&#21106;&#23618;&#65289;&#19978;&#34987;&#20998;&#21106;&#20026;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#31471;&#27169;&#22411;&#65292;&#36873;&#25321;&#20999;&#21106;&#23618;&#21487;&#33021;&#23545;&#23458;&#25143;&#31471;&#30340;&#33021;&#32791;&#21644;&#38544;&#31169;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#24433;&#21709;&#20102;&#35757;&#32451;&#36127;&#25285;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#30830;&#23450;&#20999;&#21106;&#23618;&#30340;&#35774;&#35745;&#25361;&#25112;&#38750;&#24120;&#22797;&#26434;&#65292;&#20027;&#35201;&#30001;&#20110;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#32593;&#32476;&#33021;&#21147;&#30340;&#22266;&#26377;&#24322;&#36136;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;SFL&#30340;&#36807;&#31243;&#65292;&#24182;&#23545;&#33021;&#32791;&#21644;&#38544;&#31169;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Federated Learning (SFL) has recently emerged as a promising distributed learning technology, leveraging the strengths of both federated learning and split learning. It emphasizes the advantages of rapid convergence while addressing privacy concerns. As a result, this innovation has received significant attention from both industry and academia. However, since the model is split at a specific layer, known as a cut layer, into both client-side and server-side models for the SFL, the choice of the cut layer in SFL can have a substantial impact on the energy consumption of clients and their privacy, as it influences the training burden and the output of the client-side models. Moreover, the design challenge of determining the cut layer is highly intricate, primarily due to the inherent heterogeneity in the computing and networking capabilities of clients. In this article, we provide a comprehensive overview of the SFL process and conduct a thorough analysis of energy consumption and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16960</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Privately Aligning Language Models with Reinforcement Learning. (arXiv:2310.16960v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#21644;&#29992;&#25143;&#37096;&#32626;&#20043;&#38388;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#22521;&#35757;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;(&#22914;ChatGPT)&#30340;&#20027;&#27969;&#31574;&#30053;&#12290;&#26412;&#25991;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;(DP)&#26469;&#30740;&#31350;&#38544;&#31169;&#20445;&#25252;&#30340;LLMs&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#33539;&#24335;&#65306;(i)&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#26041;&#27861;(&#22914;&#31215;&#26497;&#35780;&#20215;&#29983;&#25104;)&#65292;(ii)&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#40784;&#26041;&#27861;(RLHF)(&#22914;&#20197;&#20154;&#31867;&#39318;&#36873;&#26041;&#24335;&#36827;&#34892;&#25688;&#35201;&#29983;&#25104;)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DP&#26694;&#26550;&#26469;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#40784;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#33021;&#22815;&#22312;&#30830;&#20445;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#26377;&#31454;&#20105;&#21147;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2310.11884</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#28608;&#27963;&#21040;&#27010;&#24565;: &#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27010;&#24565;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks. (arXiv:2310.11884v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20013;&#27010;&#24565;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#27010;&#24565;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#21644;&#25512;&#29702;&#20043;&#38388;&#30340;&#33258;&#28982;&#26725;&#26753;&#65306;&#19968;&#26086;&#30830;&#23450;&#20102;&#31070;&#32463;&#23398;&#20064;&#31995;&#32479;&#20351;&#29992;&#30340;&#27010;&#24565;&#65292;&#23601;&#21487;&#20197;&#23558;&#36825;&#20123;&#27010;&#24565;&#19982;&#25512;&#29702;&#31995;&#32479;&#25972;&#21512;&#65292;&#29992;&#20110;&#25512;&#29702;&#25110;&#20351;&#29992;&#25512;&#29702;&#31995;&#32479;&#23545;&#20854;&#36827;&#34892;&#25913;&#36827;&#25110;&#22686;&#24378;&#20197;&#25913;&#21892;&#23398;&#20064;&#31995;&#32479;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19981;&#20165;&#21487;&#20197;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#23558;&#27010;&#24565;&#30693;&#35782;&#25554;&#20837;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#20013;&#12290;&#30001;&#20110;&#25972;&#21512;&#23398;&#20064;&#21644;&#25512;&#29702;&#26159;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#65292;&#25152;&#20197;&#36890;&#36807;&#36825;&#39033;&#35843;&#26597;&#33719;&#24471;&#30340;&#35265;&#35299;&#21487;&#20197;&#25104;&#20026;&#23454;&#29616;&#22522;&#20110;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.
&lt;/p&gt;</description></item><item><title>"BrainSCUBA&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#39044;&#27979;&#26368;&#22823;&#28608;&#27963;&#20010;&#20307;&#24863;&#20852;&#36259;&#20307;&#32032;&#30340;&#22270;&#20687;&#65292;&#36798;&#21040;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#30382;&#23618;&#36873;&#25321;&#24615;&#25551;&#36848;&#12290;"</title><link>http://arxiv.org/abs/2310.04420</link><description>&lt;p&gt;
"BrainSCUBA: &#35270;&#35273;&#30382;&#23618;&#36873;&#25321;&#24615;&#30340;&#32454;&#31890;&#24230;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;"
&lt;/p&gt;
&lt;p&gt;
BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity. (arXiv:2310.04420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04420
&lt;/p&gt;
&lt;p&gt;
"BrainSCUBA&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#39044;&#27979;&#26368;&#22823;&#28608;&#27963;&#20010;&#20307;&#24863;&#20852;&#36259;&#20307;&#32032;&#30340;&#22270;&#20687;&#65292;&#36798;&#21040;&#20102;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#30382;&#23618;&#36873;&#25321;&#24615;&#25551;&#36848;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#29702;&#35299;&#39640;&#32423;&#35270;&#35273;&#30382;&#23618;&#30340;&#21151;&#33021;&#32452;&#32455;&#26159;&#31070;&#32463;&#31185;&#23398;&#30340;&#26680;&#24515;&#20851;&#27880;&#28857;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20027;&#35201;&#20351;&#29992;&#25163;&#21160;&#36873;&#25321;&#30340;&#21050;&#28608;&#26469;&#26144;&#23556;&#31070;&#32463;&#32676;&#20307;&#30340;&#35270;&#35273;&#21644;&#35821;&#20041;&#36873;&#25321;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#35270;&#35273;&#30382;&#23618;&#21151;&#33021;&#30340;&#39044;&#35774;&#20551;&#35774;&#30340;&#32467;&#26524;&#20559;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#39044;&#27979;&#26368;&#22823;&#28608;&#27963;&#20010;&#20307;&#24863;&#20852;&#36259;&#20307;&#32032;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;- &#22522;&#20110;&#23545;&#27604;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23398;&#21040;&#30340;&#20016;&#23500;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#39640;&#38454;&#35270;&#35273;&#21306;&#22495;&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#30340;&#20307;&#32032;&#32423;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#30340;&#22270;&#20687;&#21512;&#25104;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#22270;&#20687;&#22312;&#35821;&#20041;&#19978;&#26159;&#36830;&#36143;&#30340;&#24182;&#19988;&#20855;&#26377;&#39640;&#30340;&#36136;&#37327;&#12290;"
&lt;/p&gt;
&lt;p&gt;
Understanding the functional organization of higher visual cortex is a central focus in neuroscience. Past studies have primarily mapped the visual and semantic selectivity of neural populations using hand-selected stimuli, which may potentially bias results towards pre-existing hypotheses of visual cortex functionality. Moving beyond conventional approaches, we introduce a data-driven method that generates natural language descriptions for images predicted to maximally activate individual voxels of interest. Our method -Semantic Captioning Using Brain Alignments ("BrainSCUBA") -- builds upon the rich embedding space learned by a contrastive vision-language model and utilizes a pre-trained large language model to generate interpretable captions. We validate our method through fine-grained voxel-level captioning across higher-order visual regions. We further perform text-conditioned image synthesis with the captions, and show that our images are semantically coherent and yield high pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#33539;&#24335;&#19982;&#22343;&#22330;&#20998;&#24067;&#34920;&#31034;&#37197;&#23545;&#65292;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#22343;&#22330;&#21338;&#24328;&#21644;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#20174;&#20998;&#24067;&#20013;&#33719;&#21462;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#26080;&#38480;&#26102;&#22495;&#26694;&#26550;&#19979;&#20351;&#29992;&#32447;&#24615;&#20108;&#27425;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.10953</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36830;&#32493;&#31354;&#38388;&#26080;&#38480;&#26102;&#22495;&#22343;&#22330;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Infinite Horizon Mean Field Problems in Continuous Spaces. (arXiv:2309.10953v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#33539;&#24335;&#19982;&#22343;&#22330;&#20998;&#24067;&#34920;&#31034;&#37197;&#23545;&#65292;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#22343;&#22330;&#21338;&#24328;&#21644;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#20174;&#20998;&#24067;&#20013;&#33719;&#21462;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#26080;&#38480;&#26102;&#22495;&#26694;&#26550;&#19979;&#20351;&#29992;&#32447;&#24615;&#20108;&#27425;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#21644;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#21457;&#23637;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;AC&#65289;&#33539;&#24335;&#19982;&#36890;&#36807;&#21442;&#25968;&#21270;&#35780;&#20998;&#20989;&#25968;&#34920;&#31034;&#30340;&#22343;&#22330;&#20998;&#24067;&#37197;&#23545;&#65292;&#21487;&#20197;&#20197;&#22312;&#32447;&#26041;&#24335;&#26377;&#25928;&#22320;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#20174;&#24471;&#21040;&#30340;&#20998;&#24067;&#20013;&#33719;&#24471;&#26679;&#26412;&#12290;AC&#20195;&#29702;&#21644;&#35780;&#20998;&#20989;&#25968;&#25353;&#36845;&#20195;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#65292;&#20197;&#25910;&#25947;&#21040;&#32473;&#23450;&#22343;&#22330;&#38382;&#39064;&#30340;MFG&#24179;&#34913;&#25110;MFC&#26368;&#20248;&#35299;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#23398;&#20064;&#29575;&#30340;&#36873;&#25321;&#12290;&#31639;&#27861;&#30340;&#31616;&#21333;&#20462;&#25913;&#20351;&#25105;&#20204;&#33021;&#22815;&#35299;&#20915;&#28151;&#21512;&#22343;&#22330;&#25511;&#21046;&#21338;&#24328;&#65288;MFCG&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#28176;&#36817;&#26080;&#38480;&#26102;&#22495;&#26694;&#26550;&#20013;&#30340;&#32447;&#24615;&#20108;&#27425;&#22522;&#20934;&#35780;&#20272;&#25105;&#20204;&#30340;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the development and analysis of a reinforcement learning (RL) algorithm designed to solve continuous-space mean field game (MFG) and mean field control (MFC) problems in a unified manner. The proposed approach pairs the actor-critic (AC) paradigm with a representation of the mean field distribution via a parameterized score function, which can be efficiently updated in an online fashion, and uses Langevin dynamics to obtain samples from the resulting distribution. The AC agent and the score function are updated iteratively to converge, either to the MFG equilibrium or the MFC optimum for a given mean field problem, depending on the choice of learning rates. A straightforward modification of the algorithm allows us to solve mixed mean field control games (MFCGs). The performance of our algorithm is evaluated using linear-quadratic benchmarks in the asymptotic infinite horizon framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20195;&#29702;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#12290;</title><link>http://arxiv.org/abs/2309.04504</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22522;&#20110;&#35270;&#35273;&#30340;&#27010;&#24565;&#32452;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositional Learning of Visually-Grounded Concepts Using Reinforcement. (arXiv:2309.04504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20195;&#29702;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#38656;&#35201;&#36890;&#36807;&#25968;&#30334;&#19975;&#20010;&#22238;&#21512;&#30340;&#35757;&#32451;&#25165;&#33021;&#36739;&#22909;&#22320;&#35299;&#20915;&#19982;&#25351;&#20196;&#30456;&#20851;&#30340;&#23548;&#33322;&#20219;&#21153;&#65292;&#24182;&#19988;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#25351;&#20196;&#32452;&#21512;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20799;&#31461;&#21487;&#20197;&#20998;&#35299;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#24182;&#23548;&#33322;&#21040;&#25351;&#23450;&#30340;&#29289;&#20307;&#65292;&#21363;&#20351;&#20182;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#26597;&#35810;&#30340;&#32452;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#20010;3D&#29615;&#22659;&#65292;&#30740;&#31350;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#35752;&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#36827;&#34892;&#32452;&#21512;&#23398;&#20064;&#65292;&#24182;&#19988;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#12289;BERT&#65289;&#22312;&#26356;&#23569;&#30340;&#22238;&#21512;&#20013;&#23398;&#20064;&#21333;&#35789;&#32452;&#21512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#20195;&#29702;&#22312;&#24418;&#29366;&#25110;&#39068;&#33394;&#27010;&#24565;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#65292;&#21487;&#20197;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#25351;&#20196;&#32452;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning agents need to be trained over millions of episodes to decently solve navigation tasks grounded to instructions. Furthermore, their ability to generalize to novel combinations of instructions is unclear. Interestingly however, children can decompose language-based instructions and navigate to the referred object, even if they have not seen the combination of queries prior. Hence, we created three 3D environments to investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task. First, we explore if agents can perform compositional learning, and whether they can leverage on frozen text encoders (e.g. CLIP, BERT) to learn word combinations in fewer episodes. Next, we demonstrate that when agents are pretrained on the shape or color concepts separately, they show a 20 times decrease in training episodes needed to solve unseen combinations of instructions. Lastly, we show tha
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#23618;&#38754;&#19978;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#27745;&#26579;&#27169;&#22411;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#36125;&#21494;&#26031;&#39118;&#38505;&#30340;&#21464;&#21270;&#23637;&#31034;&#20102;&#36825;&#20123;&#27745;&#26579;&#23545;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#21644;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.08643</link><description>&lt;p&gt;
&#19968;&#20010;&#23398;&#20064;&#21463;&#21040;&#27745;&#26579;&#30340;&#36890;&#29992;&#26694;&#26550;&#65306;&#26631;&#31614;&#22122;&#22768;&#12289;&#23646;&#24615;&#22122;&#22768;&#31561;&#31561;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Learning under Corruption: Label Noise, Attribute Noise, and Beyond. (arXiv:2307.08643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#23618;&#38754;&#19978;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#27745;&#26579;&#27169;&#22411;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#36125;&#21494;&#26031;&#39118;&#38505;&#30340;&#21464;&#21270;&#23637;&#31034;&#20102;&#36825;&#20123;&#27745;&#26579;&#23545;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#21644;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#30340;&#27745;&#26579;&#29616;&#35937;&#24456;&#24120;&#35265;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#19981;&#21516;&#30340;&#27745;&#26579;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#20102;&#35299;&#26377;&#38480;&#65292;&#32570;&#20047;&#23545;&#27745;&#26579;&#21450;&#20854;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#26680;&#30340;&#19968;&#33324;&#24615;&#21644;&#35814;&#23613;&#30340;&#26694;&#26550;&#65292;&#22312;&#20998;&#24067;&#23618;&#38754;&#19978;&#27491;&#24335;&#20998;&#26512;&#20102;&#27745;&#26579;&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26631;&#31614;&#21644;&#23646;&#24615;&#19978;&#23384;&#22312;&#30340;&#22797;&#26434;&#32852;&#21512;&#21644;&#20381;&#36182;&#24615;&#27745;&#26579;&#65292;&#36825;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#24456;&#23569;&#35302;&#21450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#36125;&#21494;&#26031;&#39118;&#38505;&#21464;&#21270;&#26469;&#23637;&#31034;&#36825;&#20123;&#27745;&#26579;&#22914;&#20309;&#24433;&#21709;&#26631;&#20934;&#30340;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;&#20110;&#8220;&#26356;&#22797;&#26434;&#8221;&#27745;&#26579;&#23545;&#23398;&#20064;&#38382;&#39064;&#24433;&#21709;&#30340;&#23450;&#24615;&#27934;&#23519;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23450;&#37327;&#27604;&#36739;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#35813;&#26694;&#26550;&#30340;&#24212;&#29992;&#21253;&#25324;&#27745;&#26579;&#26657;&#27491;&#23398;&#20064;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#23376;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Corruption is frequently observed in collected data and has been extensively studied in machine learning under different corruption models. Despite this, there remains a limited understanding of how these models relate such that a unified view of corruptions and their consequences on learning is still lacking. In this work, we formally analyze corruption models at the distribution level through a general, exhaustive framework based on Markov kernels. We highlight the existence of intricate joint and dependent corruptions on both labels and attributes, which are rarely touched by existing research. Further, we show how these corruptions affect standard supervised learning by analyzing the resulting changes in Bayes Risk. Our findings offer qualitative insights into the consequences of "more complex" corruptions on the learning problem, and provide a foundation for future quantitative comparisons. Applications of the framework include corruption-corrected learning, a subcase of which we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25913;&#36827;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02511</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27004;&#23618;&#24179;&#38754;&#31034;&#20363;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Computational Design at the Example of Floor Plans. (arXiv:2307.02511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25913;&#36827;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#29983;&#25104;&#22120;&#22240;&#20854;&#33021;&#22815;&#26681;&#25454;&#31616;&#21333;&#30340;&#25991;&#26412;&#25552;&#31034;&#21019;&#24314;&#22270;&#20687;&#32780;&#21463;&#21040;&#24191;&#27867;&#35752;&#35770;&#12290;&#20294;&#26159;&#65292;&#22312;&#22303;&#26408;&#24037;&#31243;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#38656;&#35201;&#33021;&#22815;&#26681;&#25454;&#32473;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#21019;&#24314;&#29305;&#23450;&#30340;&#24314;&#31569;&#35774;&#35745;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#27004;&#23618;&#24179;&#38754;&#20316;&#20026;&#31034;&#20363;&#65292;&#25506;&#32034;&#22522;&#20110;&#25193;&#25955;&#30340;AI&#29983;&#25104;&#22120;&#22312;&#35745;&#31639;&#35774;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#23427;&#20204;&#30446;&#21069;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#25913;&#36827;&#30340;&#35821;&#20041;&#32534;&#30721;&#30340;&#26032;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#22810;&#27425;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#23558;&#29983;&#25104;&#30340;&#27004;&#23618;&#24179;&#38754;&#30340;&#26377;&#25928;&#24615;&#20174;6%&#25552;&#39640;&#21040;90%&#65292;&#24182;&#25913;&#36827;&#20102;&#19981;&#21516;&#31034;&#20363;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24314;&#31569;&#20449;&#24687;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#38656;&#35201;&#12290;&#36890;&#36807;&#36825;&#20123;&#65292;&#25105;&#20204;&#20026;&#22303;&#26408;&#24037;&#31243;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#26041;&#21521;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI Image generators based on diffusion models are widely discussed recently for their capability to create images from simple text prompts. But, for practical use in civil engineering they need to be able to create specific construction plans for given constraints. Within this paper we explore the capabilities of those diffusion-based AI generators for computational design at the example of floor plans and identify their current limitation. We explain how the diffusion-models work and propose new diffusion models with improved semantic encoding. In several experiments we show that we can improve validity of generated floor plans from 6% to 90% and query performance for different examples. We identify short comings and derive future research challenges of those models and discuss the need to combine diffusion models with building information modelling. With this we provide key insights into the current state and future directions for diffusion models in civil engineering.
&lt;/p&gt;</description></item><item><title>DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.08068</link><description>&lt;p&gt;
DORSal: &#22522;&#20110;&#25193;&#25955;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$. (arXiv:2306.08068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08068
&lt;/p&gt;
&lt;p&gt;
DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#36328;&#22823;&#37327;&#19981;&#21516;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#34920;&#31034;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#21644;&#29289;&#20307;&#30340;&#27867;&#21270;&#65292;&#20165;&#36890;&#36807;&#21333;&#20010;&#25110;&#23569;&#25968;&#22270;&#20687;&#28210;&#26579;&#26032;&#35270;&#22270;&#65292;&#20197;&#21450;&#25903;&#25345;&#32534;&#36753;&#30340;&#21487;&#25511;&#22330;&#26223;&#29983;&#25104;&#29616;&#22312;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#32852;&#21512;&#35757;&#32451;&#22823;&#37327;&#22330;&#26223;&#36890;&#24120;&#20250;&#22312;&#28210;&#26579;&#36136;&#37327;&#19978;&#22949;&#21327;&#65292;&#32780;&#19982;&#21333;&#20010;&#22330;&#26223;&#20248;&#21270;&#27169;&#22411;&#65288;&#22914;NeRF&#65289;&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20351;&#19977;&#32500;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#20855;&#22791;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DORSal&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#35270;&#39057;&#26550;&#26500;&#65292;&#20026;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#22330;&#26223;&#25554;&#27133;&#34920;&#31034;&#30340;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#25552;&#20379;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;&#21512;&#25104;&#22810;&#29289;&#20307;&#22330;&#26223;&#21644;&#29616;&#23454;&#19990;&#30028;&#22823;&#35268;&#27169;&#34903;&#26223;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22330;&#26223;&#26032;&#35270;&#22270;&#65292;&#21516;&#26102;&#25903;&#25345;&#29289;&#20307;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#32441;&#29702;&#21644;&#21453;&#23556;&#31561;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#29616;&#20302;&#36951;&#25022;&#29575;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.07465</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#24179;&#31283;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#40657;&#30418;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning. (arXiv:2306.07465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#29616;&#20302;&#36951;&#25022;&#29575;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#24179;&#31283;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23398;&#20064;&#22343;&#34913;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#21306;&#21035;&#20110;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#28216;&#25103;&#65292;&#20854;&#20013;&#21363;&#20351;&#24453;&#27979;&#35797;&#30340;&#24046;&#36317;&#24456;&#23567;&#65292;&#27979;&#35797;&#19968;&#20010;&#22343;&#34913;&#20063;&#21487;&#33021;&#23548;&#33268;&#22823;&#37327;&#30340;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#38745;&#24577;&#28216;&#25103;&#20013;&#23384;&#22312;&#22810;&#20010;&#26368;&#20248;&#35299;&#65288;&#22343;&#34913;&#65289;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#22914;&#19968;&#33324;&#21644;&#21338;&#24328;&#12289;&#28508;&#22312;&#21338;&#24328;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65292;&#21482;&#35201;&#22312;&#38745;&#24577;&#29615;&#22659;&#19979;&#37197;&#22791;&#36866;&#24403;&#30340;&#23398;&#20064;&#21644;&#27979;&#35797;&#31070;&#35861;&#12290;&#24403;&#38750;&#24179;&#31283;&#31243;&#24230;&#65288;&#36890;&#36807;&#24635;&#21464;&#21270;&#37327; $\Delta$ &#27979;&#37327;&#65289;&#24050;&#30693;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616; $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ &#30340;&#36951;&#25022;&#65292;&#24403; $\Delta$ &#26410;&#30693;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616; $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ &#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. To overcome these obstacles, we propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\widetilde{O}\left(\Delta^{1/4}T^{3/4}\right)$ regret when the degree of nonstationarity, as measured by total variation $\Delta$, is known, and $\widetilde{O}\left(\Delta^{1/5}T^{4/5}\right)$ regret when $\Delta$ is unknown, where $T$ is the number of rounds. Meanwhile, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ECG&#20449;&#21495;&#21512;&#25104;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;DiffECG&#65292;&#33021;&#22815;&#28085;&#30422;&#19977;&#31181;&#24773;&#24418;&#65292;&#24182;&#19988;&#26159;ECG&#21512;&#25104;&#30340;&#31532;&#19968;&#20010;&#24191;&#20041;&#26465;&#20214;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20248;&#20110;&#20854;&#20182;ECG&#29983;&#25104;&#27169;&#22411;&#24182;&#21487;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01875</link><description>&lt;p&gt;
DiffECG&#65306;ECG&#20449;&#21495;&#21512;&#25104;&#30340;&#19968;&#33324;&#21270;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffECG: A Generalized Probabilistic Diffusion Model for ECG Signals Synthesis. (arXiv:2306.01875v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ECG&#20449;&#21495;&#21512;&#25104;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;DiffECG&#65292;&#33021;&#22815;&#28085;&#30422;&#19977;&#31181;&#24773;&#24418;&#65292;&#24182;&#19988;&#26159;ECG&#21512;&#25104;&#30340;&#31532;&#19968;&#20010;&#24191;&#20041;&#26465;&#20214;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20248;&#20110;&#20854;&#20182;ECG&#29983;&#25104;&#27169;&#22411;&#24182;&#21487;&#25552;&#39640;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ECG&#20449;&#21495;&#24515;&#33039;&#30142;&#30149;&#26816;&#27979;&#20013;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25968;&#25454;&#22686;&#24378;&#35299;&#20915;&#26041;&#26696;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;ECG&#21512;&#25104;&#26041;&#27861;,&#35206;&#30422;&#20102;&#19977;&#31181;&#24773;&#24418;&#65306;&#24515;&#36339;&#29983;&#25104;&#12289;&#37096;&#20998;&#20449;&#21495;&#23436;&#25104;&#21644;&#23436;&#25972;&#24515;&#36339;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;ECG&#21512;&#25104;&#30340;&#31532;&#19968;&#20010;&#24191;&#20041;&#26465;&#20214;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#21508;&#31181;ECG&#30456;&#20851;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;ECG&#29983;&#25104;&#27169;&#22411;&#24182;&#21487;&#20197;&#25552;&#39640;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep generative models have gained attention as a promising data augmentation solution for heart disease detection using deep learning approaches applied to ECG signals. In this paper, we introduce a novel approach based on denoising diffusion probabilistic models for ECG synthesis that covers three scenarios: heartbeat generation, partial signal completion, and full heartbeat forecasting. Our approach represents the first generalized conditional approach for ECG synthesis, and our experimental results demonstrate its effectiveness for various ECG-related tasks. Moreover, we show that our approach outperforms other state-of-the-art ECG generative models and can enhance the performance of state-of-the-art classifiers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#39044;&#27979;&#19982;&#27604;&#36739;&#8221;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21464;&#28857;&#30417;&#27979;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#27604;&#29616;&#26377;&#30340;&#22312;&#32447;&#30417;&#27979;&#26041;&#27861;&#26356;&#22909;&#22320;&#25511;&#21046;&#35823;&#25253;&#29575;&#21644;&#22833;&#25511;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;ARIMA&#27169;&#22411;&#21644;LSTM&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06630</link><description>&lt;p&gt;
&#24322;&#36136;&#25968;&#25454;&#30340;&#39044;&#27979;&#24615;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Predictive change point detection for heterogeneous data. (arXiv:2305.06630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#39044;&#27979;&#19982;&#27604;&#36739;&#8221;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21464;&#28857;&#30417;&#27979;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#27604;&#29616;&#26377;&#30340;&#22312;&#32447;&#30417;&#27979;&#26041;&#27861;&#26356;&#22909;&#22320;&#25511;&#21046;&#35823;&#25253;&#29575;&#21644;&#22833;&#25511;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;ARIMA&#27169;&#22411;&#21644;LSTM&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#25512;&#24191;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#39044;&#27979;&#19982;&#27604;&#36739;&#8221;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36741;&#21161;&#30340;&#21464;&#28857;&#26816;&#27979;&#65288;CPD&#65289;&#26694;&#26550;&#65292;&#24182;&#19982;&#20854;&#20182;&#22312;&#32447;CPD&#20363;&#31243;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35823;&#25253;&#29575;&#21644;&#22833;&#25511;&#24179;&#22343;&#36816;&#34892;&#38271;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;&#35813;&#26041;&#27861;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#39044;&#27979;&#27493;&#39588;&#65289;&#20195;&#26367;&#36890;&#24120;&#20351;&#29992;&#30340;&#36235;&#21183;&#20272;&#35745;&#20989;&#25968;&#65288;&#22914;&#28369;&#21160;&#24179;&#22343;&#65289;&#65292;&#24182;&#23558;&#20854;&#39044;&#27979;&#19982;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#65288;&#27604;&#36739;&#27493;&#39588;&#65289;&#65292;&#20174;&#32780;&#25913;&#21892;&#39034;&#24207;&#20998;&#26512;&#20013;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#20363;&#22914;CUSUM&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#36825;&#20123;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
A change point detection (CPD) framework assisted by a predictive machine learning model called ''Predict and Compare'' is introduced and characterised in relation to other state-of-the-art online CPD routines which it outperforms in terms of false positive rate and out-of-control average run length. The method's focus is on improving standard methods from sequential analysis such as the CUSUM rule in terms of these quality measures.  This is achieved by replacing typically used trend estimation functionals such as the running mean with more sophisticated predictive models (Predict step), and comparing their prognosis with actual data (Compare step). The two models used in the Predict step are the ARIMA model and the LSTM recursive neural network. However, the framework is formulated in general terms, so as to allow the use of other prediction or comparison methods than those tested here. The power of the method is demonstrated in a tribological case study in which change points separa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.06058</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#25968;&#32423;&#21035;&#30340;&#23569;&#37327;&#21464;&#20998;&#21442;&#25968;&#30340;&#24352;&#37327;&#32593;&#32476;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compressing neural network by tensor network with exponentially fewer variational parameters. (arXiv:2305.06058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25152;&#21253;&#21547;&#30340;&#24040;&#22823;&#21487;&#21464;&#30340;&#21442;&#25968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36825;&#20123;&#21442;&#25968; encoding &#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#26696;&#28436;&#31034;&#20102;&#20986;&#33394;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20197;&#27973;&#23618;&#24352;&#37327;&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;VGG-16&#20013;&#30340;3&#20010;&#21367;&#31215;&#23618;&#30340;&#22823;&#32422;1000&#19975;&#21442;&#25968;&#34987;&#21387;&#32553;&#21040;&#20855;&#26377;&#20165;632&#20010;&#21442;&#25968;&#30340;TN&#20013;&#65292;&#32780;&#22312;CIFAR-10&#19978;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#20196;&#20154;&#24778;&#21916;&#22320;&#25552;&#39640;&#20102;81.14&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including over-fitting, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN by encoding them to multi-layer tensor networks (TN's) that contain exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, and VGG-16) and datasets (MNIST and CIFAR-10), surpassing the state-of-the-art method based on shallow tensor networks. For instance, about 10 million parameters in the three convolutional layers of VGG-16 are compressed in TN's with just $632$ parameters, while the testing accuracy on CIFAR-10 is surprisingly improved from $81.14
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#65292;&#21152;&#20837;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#20197;&#20445;&#25252;&#38544;&#31169;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.01068</link><description>&lt;p&gt;
Fed-GLOSS-DP: &#21033;&#29992;&#20855;&#26377;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#21512;&#25104;&#38598;&#36827;&#34892;&#32852;&#37030;&#20840;&#23616;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with Record Level Differential Privacy. (arXiv:2302.01068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#65292;&#21152;&#20837;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#20197;&#20445;&#25252;&#38544;&#31169;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Fed-GLOSS-DP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#32447;&#24615;&#36880;&#28857;&#26799;&#24230;&#20998;&#20139;&#26041;&#26696;&#65288;&#22914;FedAvg&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#20844;&#24335;&#21033;&#29992;&#20174;&#23458;&#25143;&#31471;&#25509;&#25910;&#21040;&#30340;&#21512;&#25104;&#26679;&#26412;&#23454;&#29616;&#20102;&#19968;&#31181;&#20840;&#23616;&#20248;&#21270;&#12290;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#20316;&#20026;&#25439;&#22833;&#26367;&#20195;&#29289;&#65292;&#36890;&#36807;&#27169;&#25311;&#26412;&#22320;&#21306;&#22495;&#20869;&#30495;&#23454;&#22270;&#20687;&#30340;&#23454;&#29992;&#24615;&#26469;&#36817;&#20284;&#26412;&#22320;&#25439;&#22833;&#22320;&#24418;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#34913;&#37327;&#26377;&#25928;&#36924;&#36817;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#21453;&#26144;&#20102;&#36817;&#20284;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#26381;&#21153;&#22120;&#21487;&#20197;&#24674;&#22797;&#20840;&#23616;&#25439;&#22833;&#22320;&#24418;&#24182;&#20840;&#38754;&#20248;&#21270;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#21463;&#26085;&#30410;&#20005;&#37325;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#35760;&#24405;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26080;&#32541;&#37197;&#21512;&#65292;&#20026;&#23458;&#25143;&#31471;&#19978;&#30340;&#27599;&#20010;&#25968;&#25454;&#35760;&#24405;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#24191;&#27867;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20844;&#24335;&#22312;&#20855;&#26377;&#39640;&#24230;&#20542;&#26012;&#20998;&#24067;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes Fed-GLOSS-DP, a novel privacy-preserving approach for federated learning. Unlike previous linear point-wise gradient-sharing schemes, such as FedAvg, our formulation enables a type of global optimization by leveraging synthetic samples received from clients. These synthetic samples, serving as loss surrogates, approximate local loss landscapes by simulating the utility of real images within a local region. We additionally introduce an approach to measure effective approximation regions reflecting the quality of the approximation. Therefore, the server can recover the global loss landscape and comprehensively optimize the model. Moreover, motivated by the emerging privacy concerns, we demonstrate that our approach seamlessly works with record-level differential privacy (DP), granting theoretical privacy guarantees for every data record on the clients. Extensive results validate the efficacy of our formulation on various datasets with highly skewed distributions. Our m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00890</link><description>&lt;p&gt;
&#20855;&#26377;&#23436;&#25104;&#21151;&#33021;&#30340;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neural Common Neighbor with Completion for Link Prediction. (arXiv:2302.00890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#27169;&#22411;&#65288;NCN&#65289;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#26469;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#35299;&#20915;&#38142;&#36335;&#19981;&#23436;&#25972;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;vanilla&#20449;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#22312;&#21508;&#31181;&#22270;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#36890;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#21482;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30446;&#26631;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#24182;&#24573;&#30053;&#23427;&#20204;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#33719;&#25104;&#23545;&#20851;&#31995;&#65292;&#19968;&#20123;&#27169;&#22411;&#23558;&#25163;&#21160;&#21151;&#33021;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20013;&#65292;&#24182;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#26469;&#29983;&#25104;&#25104;&#23545;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#20854;&#20182;&#20154;&#30452;&#25509;&#23558;&#25163;&#21160;&#21151;&#33021;&#29992;&#20316;&#25104;&#23545;&#34920;&#31034;&#12290;&#23613;&#31649;&#27492;&#31616;&#21270;&#36991;&#20813;&#20102;&#23558;GNN&#36880;&#20010;&#38142;&#25509;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#38142;&#25509;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#30001;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#21644;&#19981;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#29305;&#24449;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#26377;&#24456;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#31354;&#38388;&#12290;&#20026;&#20102;&#22312;&#20445;&#25345;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#36890;&#29992;&#37051;&#23621;&#65288;NCN&#65289;&#65292;&#23427;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#25104;&#23545;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;NCN&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#38142;&#25509;&#38382;&#39064;&#12290;&#22270;&#30340;&#19981;&#23436;&#25972;&#24615;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#24182;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#26597;&#35810;&#26469;&#26816;&#27979;&#31181;&#26893;&#23376;&#22270;&#23384;&#22312;&#65292;&#30830;&#23450;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#26597;&#35810;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2110.00744</link><description>&lt;p&gt;
&#20351;&#29992;&#26597;&#35810;&#26469;&#26816;&#27979;&#38543;&#26426;&#23376;&#22270;
&lt;/p&gt;
&lt;p&gt;
Random Subgraph Detection Using Queries. (arXiv:2110.00744v4 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#26597;&#35810;&#26469;&#26816;&#27979;&#31181;&#26893;&#23376;&#22270;&#23384;&#22312;&#65292;&#30830;&#23450;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#26597;&#35810;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31181;&#26893;&#30340;&#26368;&#23494;&#23376;&#22270;&#26816;&#27979;&#38382;&#39064;&#26159;&#25351;&#27979;&#35797;&#22312;&#32473;&#23450;&#30340;&#65288;&#38543;&#26426;&#65289;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#24322;&#24120;&#23494;&#38598;&#30340;&#23376;&#22270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#21464;&#20307;&#65292;&#21363;&#21482;&#33021;&#20351;&#29992;&#33258;&#36866;&#24212;&#36793;&#26597;&#35810;&#26469;&#35266;&#23519;&#22270;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26816;&#27979;&#31181;&#26893;&#23376;&#22270;&#23384;&#22312;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#37327;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;&#65288;&#21487;&#33021;&#26159;&#38543;&#26426;&#21270;&#30340;&#65289;&#31639;&#27861;&#24517;&#39035;&#36827;&#34892; $\mathsf{Q} = \Omega(\frac{n^2}{k^2\chi^4(p||q)}\log^2n)$ &#20010;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The planted densest subgraph detection problem refers to the task of testing whether in a given (random) graph there is a subgraph that is unusually dense. Specifically, we observe an undirected and unweighted graph on $n$ nodes. Under the null hypothesis, the graph is a realization of an Erd\H{o}s-R\'{e}nyi graph with edge probability (or, density) $q$. Under the alternative, there is a subgraph on $k$ vertices with edge probability $p&gt;q$. The statistical as well as the computational barriers of this problem are well-understood for a wide range of the edge parameters $p$ and $q$. In this paper, we consider a natural variant of the above problem, where one can only observe a small part of the graph using adaptive edge queries.  For this model, we determine the number of queries necessary and sufficient for detecting the presence of the planted subgraph. Specifically, we show that any (possibly randomized) algorithm must make $\mathsf{Q} = \Omega(\frac{n^2}{k^2\chi^4(p||q)}\log^2n)$ ada
&lt;/p&gt;</description></item></channel></rss>