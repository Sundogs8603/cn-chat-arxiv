<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;SA-Solver&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;SOTA FID&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.05019</link><description>&lt;p&gt;
SA-Solver&#65306;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;&#30340;&#38543;&#26426;&#20122;&#24403;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models. (arXiv:2309.05019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;SA-Solver&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;SOTA FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;&#20174;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30456;&#24403;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#25913;&#36827;&#30340;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#12290;&#36825;&#20123;&#25216;&#26415;&#20013;&#30340;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#32771;&#34385;&#35299;&#25193;&#25955;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#37319;&#26679;&#21487;&#20197;&#22312;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#25968;&#25454;&#26041;&#38754;&#25552;&#20379;&#39069;&#22806;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#23545;&#38543;&#26426;&#37319;&#26679;&#30340;&#32508;&#21512;&#20998;&#26512;&#65306;&#26041;&#24046;&#25511;&#21046;&#30340;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#32447;&#24615;&#22810;&#27493;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SA-Solver&#65292;&#23427;&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SA-Solver&#23454;&#29616;&#20102;&#65306;1&#65289;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#24615;&#33021;&#65307;2&#65289;SOTA FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID scores o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#27719;&#32858;&#26799;&#24230;&#26041;&#27861;&#22312;&#27969;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#21152;&#36895;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#30830;&#23450;&#24615;&#26799;&#24230;&#24773;&#20917;&#19979;&#65292;&#27969;IAG&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#65292;&#24182;&#19988;&#21363;&#20351;&#25968;&#25454;&#26679;&#26412;&#20998;&#24067;&#19981;&#22343;&#21248;&#65292;&#21482;&#35201;&#24037;&#20154;&#39057;&#32321;&#26356;&#26032;&#65292;&#26399;&#26395;&#30340;&#26368;&#20248;&#35299;&#24179;&#26041;&#36317;&#31163;&#21487;&#20197;&#20197;O((1+T)/(nt))&#30340;&#36895;&#24230;&#34928;&#20943;&#12290;</title><link>http://arxiv.org/abs/2309.04980</link><description>&lt;p&gt;
&#22686;&#37327;&#27719;&#32858;&#26799;&#24230;&#26041;&#27861;&#22312;&#27969;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Linear Speedup of Incremental Aggregated Gradient Methods on Streaming Data. (arXiv:2309.04980v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#37327;&#27719;&#32858;&#26799;&#24230;&#26041;&#27861;&#22312;&#27969;&#25968;&#25454;&#19978;&#30340;&#32447;&#24615;&#21152;&#36895;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#30830;&#23450;&#24615;&#26799;&#24230;&#24773;&#20917;&#19979;&#65292;&#27969;IAG&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#65292;&#24182;&#19988;&#21363;&#20351;&#25968;&#25454;&#26679;&#26412;&#20998;&#24067;&#19981;&#22343;&#21248;&#65292;&#21482;&#35201;&#24037;&#20154;&#39057;&#32321;&#26356;&#26032;&#65292;&#26399;&#26395;&#30340;&#26368;&#20248;&#35299;&#24179;&#26041;&#36317;&#31163;&#21487;&#20197;&#20197;O((1+T)/(nt))&#30340;&#36895;&#24230;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#22686;&#37327;&#27719;&#32858;&#26799;&#24230;(IAG)&#26041;&#27861;&#12290;IAG&#26041;&#27861;&#38750;&#24120;&#36866;&#21512;&#21442;&#25968;&#26381;&#21153;&#22120;&#26550;&#26500;&#65292;&#22240;&#20026;&#21442;&#25968;&#26381;&#21153;&#22120;&#21487;&#20197;&#36731;&#26494;&#22320;&#27719;&#32858;&#24037;&#20154;&#36129;&#29486;&#30340;&#21487;&#33021;&#36807;&#26399;&#30340;&#26799;&#24230;&#12290;&#23613;&#31649;IAG&#22312;&#30830;&#23450;&#24615;&#26799;&#24230;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#24050;&#32463;&#26377;&#20102;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#22522;&#20110;&#27969;&#25968;&#25454;&#30340;&#38543;&#26426;&#21464;&#20307;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#32771;&#34385;&#24378;&#20984;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#24403;&#24037;&#20154;&#39057;&#32321;&#26356;&#26032;&#26102;&#65292;&#27969;IAG&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#65292;&#21363;&#20351;&#24037;&#20154;&#20043;&#38388;&#30340;&#25968;&#25454;&#26679;&#26412;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#23545;&#24102;&#26377;&#36807;&#26399;&#26799;&#24230;&#30340;&#26465;&#20214;&#26399;&#26395;&#30340;&#20180;&#32454;&#22788;&#29702;&#21644;&#36882;&#24402;&#31995;&#32479;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a type of incremental aggregated gradient (IAG) method for large-scale distributed optimization. The IAG method is well suited for the parameter server architecture as the latter can easily aggregate potentially staled gradients contributed by workers. Although the convergence of IAG in the case of deterministic gradient is well known, there are only a few results for the case of its stochastic variant based on streaming data. Considering strongly convex optimization, this paper shows that the streaming IAG method achieves linear speedup when the workers are updating frequently enough, even if the data sample distribution across workers are heterogeneous. We show that the expected squared distance to optimal solution decays at O((1+T)/(nt)), where $n$ is the number of workers, t is the iteration number, and T/n is the update frequency of workers. Our analysis involves careful treatments of the conditional expectations with staled gradients and a recursive system wi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26080;&#20154;&#26426;&#26469;&#32531;&#35299;&#24847;&#22806;&#30340;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25317;&#22581;&#12290;&#30740;&#31350;&#25351;&#20986;&#65292;&#20256;&#32479;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#25928;&#29575;&#20302;&#19979;&#65292;&#32780;&#22522;&#20110;&#25668;&#20687;&#26426;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31995;&#32479;&#26356;&#21152;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#26412;&#38382;&#39064;&#65292;&#26080;&#20154;&#26426;&#21487;&#33021;&#26159;&#19968;&#20010;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.04976</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#20154;&#26426;&#32531;&#35299;&#24847;&#22806;&#30340;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25317;&#22581;
&lt;/p&gt;
&lt;p&gt;
AVARS -- Alleviating Unexpected Urban Road Traffic Congestion using UAVs. (arXiv:2309.04976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#26080;&#20154;&#26426;&#26469;&#32531;&#35299;&#24847;&#22806;&#30340;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25317;&#22581;&#12290;&#30740;&#31350;&#25351;&#20986;&#65292;&#20256;&#32479;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#25928;&#29575;&#20302;&#19979;&#65292;&#32780;&#22522;&#20110;&#25668;&#20687;&#26426;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#31995;&#32479;&#26356;&#21152;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25104;&#26412;&#38382;&#39064;&#65292;&#26080;&#20154;&#26426;&#21487;&#33021;&#26159;&#19968;&#20010;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#30001;&#36884;&#20013;&#20107;&#20214;&#65288;&#20363;&#22914;&#36335;&#27573;&#20851;&#38381;&#65292;&#36710;&#31096;&#31561;&#65289;&#24341;&#36215;&#30340;&#24847;&#22806;&#22478;&#24066;&#20132;&#36890;&#25317;&#22581;&#36890;&#24120;&#38656;&#35201;&#24555;&#36895;&#20934;&#30830;&#22320;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20132;&#36890;&#20449;&#21495;&#12290;&#20256;&#32479;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#65292;&#22914;SCATS&#21644;SCOOT&#65292;&#30001;&#24863;&#24212;&#32447;&#22280;&#25552;&#20379;&#30340;&#20132;&#36890;&#25968;&#25454;&#26356;&#26032;&#39057;&#29575;&#36739;&#20302;&#65288;&#21363;&#36229;&#36807;1&#20998;&#38047;&#65289;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31995;&#32479;&#20351;&#29992;&#30340;&#20132;&#36890;&#20449;&#21495;&#28783;&#35745;&#21010;&#26159;&#20107;&#21457;&#21069;&#39044;&#20808;&#32534;&#31243;&#30340;&#20505;&#36873;&#35745;&#21010;&#30340;&#26377;&#38480;&#38598;&#21512;&#20013;&#36873;&#25321;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#25511;&#21046;&#30340;&#22522;&#20110;&#25668;&#20687;&#26426;&#30340;&#20132;&#36890;&#20449;&#21495;&#31995;&#32479;&#22312;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#26041;&#38754;&#26356;&#21152;&#26377;&#25928;&#65292;&#25668;&#20687;&#26426;&#21487;&#20197;&#25552;&#20379;&#39640;&#39057;&#39640;&#20998;&#36776;&#29575;&#30340;&#20132;&#36890;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#38656;&#35201;&#36807;&#22810;&#30340;&#28508;&#22312;&#21319;&#32423;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#37096;&#32626;&#25104;&#26412;&#22312;&#22823;&#22478;&#24066;&#20013;&#36739;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#26080;&#20154;&#26426;&#65288;UAVs&#65289;&#21487;&#20197;&#22312;&#32531;&#35299;&#24847;&#22806;&#30340;&#22478;&#24066;&#36947;&#36335;&#20132;&#36890;&#25317;&#22581;&#26041;&#38754;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing unexpected urban traffic congestion caused by en-route events (e.g., road closures, car crashes, etc.) often requires fast and accurate reactions to choose the best-fit traffic signals. Traditional traffic light control systems, such as SCATS and SCOOT, are not efficient as their traffic data provided by induction loops has a low update frequency (i.e., longer than 1 minute). Moreover, the traffic light signal plans used by these systems are selected from a limited set of candidate plans pre-programmed prior to unexpected events' occurrence. Recent research demonstrates that camera-based traffic light systems controlled by deep reinforcement learning (DRL) algorithms are more effective in reducing traffic congestion, in which the cameras can provide high-frequency high-resolution traffic data. However, these systems are costly to deploy in big cities due to the excessive potential upgrades required to road infrastructure. In this paper, we argue that Unmanned Aerial Vehicles (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#21644;&#24847;&#22270;&#23884;&#20837;&#20197;&#21450;&#34892;&#20026;&#23884;&#20837;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#25512;&#26029;&#20986;&#24403;&#21069;&#27491;&#22312;&#36827;&#34892;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04974</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#20219;&#21153;&#25512;&#26029;&#36827;&#34892;&#36830;&#32493;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Robot Learning using Self-Supervised Task Inference. (arXiv:2309.04974v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#21644;&#24847;&#22270;&#23884;&#20837;&#20197;&#21450;&#34892;&#20026;&#23884;&#20837;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#25512;&#26029;&#20986;&#24403;&#21069;&#27491;&#22312;&#36827;&#34892;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#23454;&#29616;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36171;&#20104;&#26426;&#22120;&#20154;&#20687;&#20154;&#31867;&#19968;&#26679;&#23398;&#20064;&#19981;&#26029;&#21457;&#23637;&#30340;&#25216;&#33021;&#32780;&#19981;&#26159;&#25484;&#25569;&#21333;&#19968;&#20219;&#21153;&#30340;&#33021;&#21147;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20182;&#20204;&#24456;&#23569;&#20851;&#27880;&#20219;&#21153;&#25512;&#26029;&#12290;&#20026;&#20102;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#39318;&#20808;&#38656;&#35201;&#25512;&#26029;&#20986;&#24403;&#21069;&#27491;&#22312;&#36827;&#34892;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#34920;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#20219;&#21153;&#25512;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#26410;&#26631;&#35760;&#31034;&#33539;&#30340;&#36816;&#21160;&#21644;&#25928;&#26524;&#37096;&#20998;&#30340;&#33258;&#32452;&#32455;&#26469;&#23398;&#20064;&#21160;&#20316;&#21644;&#24847;&#22270;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#21160;&#20316;-&#24847;&#22270;&#23884;&#20837;&#30340;&#33258;&#32452;&#32455;&#26469;&#23398;&#20064;&#39640;&#23618;&#34892;&#20026;&#23884;&#20837;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#34892;&#20026;&#21305;&#37197;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#26469;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#25512;&#26029;&#32593;&#32476;&#65288;TINet&#65289;&#23558;&#26410;&#26631;&#35760;&#31034;&#33539;&#26144;&#23556;&#21040;&#20854;&#26368;&#36817;&#30340;&#34892;&#20026;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#20219;&#21153;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Endowing robots with the human ability to learn a growing set of skills over the course of a lifetime as opposed to mastering single tasks is an open problem in robot learning. While multi-task learning approaches have been proposed to address this problem, they pay little attention to task inference. In order to continually learn new tasks, the robot first needs to infer the task at hand without requiring predefined task representations. In this paper, we propose a self-supervised task inference approach. Our approach learns action and intention embeddings from self-organization of the observed movement and effect parts of unlabeled demonstrations and a higher-level behavior embedding from self-organization of the joint action-intention embeddings. We construct a behavior-matching self-supervised learning objective to train a novel Task Inference Network (TINet) to map an unlabeled demonstration to its nearest behavior embedding, which we use as the task representation. A multi-task p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMBiS-Net&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#65292;&#20855;&#26377;&#26497;&#20302;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#25968;&#37327;&#21644;&#20248;&#21270;&#30340;&#27169;&#22411;&#25928;&#29575;&#65292;&#36890;&#36807;&#22810;&#36335;&#24452;&#29305;&#24449;&#25552;&#21462;&#21644;&#21452;&#21521;&#36339;&#36291;&#36830;&#25509;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04968</link><description>&lt;p&gt;
LMBiS-Net&#65306;&#22522;&#20110;&#36731;&#37327;&#32423;&#22810;&#36335;&#24452;&#21452;&#21521;&#36339;&#36291;&#36830;&#25509;&#30340;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LMBiS-Net: A Lightweight Multipath Bidirectional Skip Connection based CNN for Retinal Blood Vessel Segmentation. (arXiv:2309.04968v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMBiS-Net&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#65292;&#20855;&#26377;&#26497;&#20302;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#25968;&#37327;&#21644;&#20248;&#21270;&#30340;&#27169;&#22411;&#25928;&#29575;&#65292;&#36890;&#36807;&#22810;&#36335;&#24452;&#29305;&#24449;&#25552;&#21462;&#21644;&#21452;&#21521;&#36339;&#36291;&#36830;&#25509;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#26126;&#30524;&#30142;&#36890;&#24120;&#19982;&#25913;&#21464;&#30340;&#35270;&#32593;&#33180;&#24418;&#24577;&#30456;&#20851;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#21106;&#30524;&#24213;&#22270;&#20687;&#20013;&#30340;&#35270;&#32593;&#33180;&#32467;&#26500;&#22312;&#20020;&#24202;&#19978;&#36827;&#34892;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#20998;&#21106;&#31934;&#32454;&#34880;&#31649;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#36275;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#26159;&#23427;&#23545;&#37325;&#22797;&#30340;&#21367;&#31215;&#21644;&#27744;&#21270;&#25805;&#20316;&#30340;&#20381;&#36182;&#21487;&#33021;&#20250;&#38459;&#30861;&#36793;&#32536;&#20449;&#24687;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#25972;&#20307;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMBiS-Net&#30340;&#36731;&#37327;&#32423;&#20687;&#32032;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20197;&#26497;&#20302;&#25968;&#37327;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#65288;&#20165;0.172M&#65289;&#36827;&#34892;&#35270;&#32593;&#33180;&#34880;&#31649;&#30340;&#20998;&#21106;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#20102;&#22810;&#36335;&#24452;&#29305;&#24449;&#25552;&#21462;&#22359;&#65292;&#24182;&#23558;&#21452;&#21521;&#36339;&#36291;&#36830;&#25509;&#29992;&#20110;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#28388;&#27874;&#22120;&#30340;&#25968;&#37327;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#36991;&#20813;&#20102;&#28388;&#27874;&#22120;&#37325;&#21472;&#12290;&#36825;&#31181;&#20248;&#21270;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blinding eye diseases are often correlated with altered retinal morphology, which can be clinically identified by segmenting retinal structures in fundus images. However, current methodologies often fall short in accurately segmenting delicate vessels. Although deep learning has shown promise in medical image segmentation, its reliance on repeated convolution and pooling operations can hinder the representation of edge information, ultimately limiting overall segmentation accuracy. In this paper, we propose a lightweight pixel-level CNN named LMBiS-Net for the segmentation of retinal vessels with an exceptionally low number of learnable parameters \textbf{(only 0.172 M)}. The network used multipath feature extraction blocks and incorporates bidirectional skip connections for the information flow between the encoder and decoder. Additionally, we have optimized the efficiency of the model by carefully selecting the number of filters to avoid filter overlap. This optimization significantl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#22810;&#20010;k-means&#32858;&#31867;&#38598;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#32858;&#31867;&#24341;&#29992;&#36712;&#36857;&#12290;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#30693;&#35782;&#20256;&#25773;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#21442;&#25968;&#12289;&#23450;&#20041;&#27169;&#31946;&#21644;&#21482;&#25429;&#25417;&#26497;&#31471;&#36712;&#36857;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04949</link><description>&lt;p&gt;
&#29992;&#20110;&#32858;&#31867;&#24341;&#29992;&#36712;&#36857;&#30340;&#22810;&#20010;K-means&#32858;&#31867;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A multiple k-means cluster ensemble framework for clustering citation trajectories. (arXiv:2309.04949v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#22810;&#20010;k-means&#32858;&#31867;&#38598;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#32858;&#31867;&#24341;&#29992;&#36712;&#36857;&#12290;&#36825;&#26377;&#21161;&#20110;&#29702;&#35299;&#30693;&#35782;&#20256;&#25773;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#21442;&#25968;&#12289;&#23450;&#20041;&#27169;&#31946;&#21644;&#21482;&#25429;&#25417;&#26497;&#31471;&#36712;&#36857;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#29992;&#25104;&#29087;&#26102;&#38388;&#22240;&#25991;&#31456;&#32780;&#24322;&#65292;&#28982;&#32780;&#25152;&#26377;&#25991;&#31456;&#30340;&#24433;&#21709;&#21147;&#37117;&#26159;&#22312;&#19968;&#20010;&#22266;&#23450;&#31383;&#21475;&#20869;&#34913;&#37327;&#30340;&#12290;&#23545;&#23427;&#20204;&#30340;&#24341;&#29992;&#36712;&#36857;&#36827;&#34892;&#32858;&#31867;&#26377;&#21161;&#20110;&#29702;&#35299;&#30693;&#35782;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#25581;&#31034;&#24182;&#38750;&#25152;&#26377;&#25991;&#31456;&#22312;&#21457;&#34920;&#21518;&#37117;&#31435;&#21363;&#33719;&#24471;&#25104;&#21151;&#12290;&#27492;&#22806;&#65292;&#23545;&#36712;&#36857;&#36827;&#34892;&#32858;&#31867;&#20063;&#23545;&#35770;&#25991;&#24433;&#21709;&#21147;&#25512;&#33616;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#24341;&#29992;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#29305;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#32452;&#20219;&#24847;&#30340;&#38408;&#20540;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#22266;&#23450;&#26041;&#27861;&#12290;&#25152;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#37117;&#20381;&#36182;&#20110;&#21442;&#25968;&#65292;&#22240;&#27492;&#22312;&#23450;&#20041;&#30456;&#20284;&#30340;&#36712;&#36857;&#21644;&#20851;&#20110;&#29305;&#23450;&#25968;&#30446;&#30340;&#27169;&#31946;&#24615;&#26041;&#38754;&#23548;&#33268;&#20102;&#19981;&#19968;&#33268;&#24615;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#21482;&#25429;&#25417;&#20102;&#26497;&#31471;&#30340;&#36712;&#36857;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#36890;&#29992;&#30340;&#32858;&#31867;&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29305;&#24449;&#30340;&#22810;&#20010;k-means&#32858;&#31867;&#38598;&#25104;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Citation maturity time varies for different articles. However, the impact of all articles is measured in a fixed window. Clustering their citation trajectories helps understand the knowledge diffusion process and reveals that not all articles gain immediate success after publication. Moreover, clustering trajectories is necessary for paper impact recommendation algorithms. It is a challenging problem because citation time series exhibit significant variability due to non linear and non stationary characteristics. Prior works propose a set of arbitrary thresholds and a fixed rule based approach. All methods are primarily parameter dependent. Consequently, it leads to inconsistencies while defining similar trajectories and ambiguities regarding their specific number. Most studies only capture extreme trajectories. Thus, a generalised clustering framework is required. This paper proposes a feature based multiple k means cluster ensemble framework. 1,95,783 and 41,732 well cited articles f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$d$-DRFWL(2) GNNs&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#38480;&#21046;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23376;&#22270;GNNs&#30340;&#39044;&#22788;&#29702;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.04941</link><description>&lt;p&gt;
&#38480;&#21046;&#36317;&#31163;&#30340;&#27665;&#38388;&#20256;&#35828;Weisfeiler-Leman&#22270;&#31070;&#32463;&#32593;&#32476;&#21450;&#21487;&#35777;&#26126;&#30340;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power. (arXiv:2309.04941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$d$-DRFWL(2) GNNs&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#38480;&#21046;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23376;&#22270;GNNs&#30340;&#39044;&#22788;&#29702;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#33021;&#21147;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#25104;&#21151;&#35745;&#25968;&#29305;&#23450;&#30340;&#22270;&#23376;&#32467;&#26500;&#65292;&#23588;&#20854;&#26159;&#24490;&#29615;&#65292;&#23545;&#20110;GNNs&#30340;&#25104;&#21151;&#38750;&#24120;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#23427;&#24050;&#34987;&#29992;&#20316;&#35780;&#20272;GNNs&#34920;&#36798;&#33021;&#21147;&#30340;&#19968;&#31181;&#24120;&#29992;&#25351;&#26631;&#12290;&#35768;&#22810;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#24490;&#29615;&#35745;&#25968;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#37117;&#22522;&#20110;&#23376;&#22270;GNNs&#65292;&#21363;&#20174;&#36755;&#20837;&#22270;&#20013;&#25552;&#21462;&#19968;&#32452;&#23376;&#22270;&#65292;&#20026;&#27599;&#20010;&#23376;&#22270;&#29983;&#25104;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#22686;&#24378;&#36755;&#20837;&#22270;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#32321;&#37325;&#30340;&#39044;&#22788;&#29702;&#65292;&#24182;&#19988;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#36739;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;GNN&#31867;&#21035;-- $d$-Distance-Restricted FWL(2) GNNs&#65292;&#25110;&#32773; $d$-DRFWL(2) GNNs&#65292;&#20811;&#26381;&#20102;&#23376;&#22270;GNNs&#30340;&#19978;&#36848;&#38480;&#21046;&#12290;$d$-DRFWL(2) GNNs&#23558;&#20114;&#30456;&#20043;&#38388;&#36317;&#31163;&#19981;&#36229;&#36807;$d$&#30340;&#33410;&#28857;&#23545;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#30340;&#21333;&#20301;&#65292;&#20197;&#24179;&#34913;&#34920;&#36798;&#33021;&#21147;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of graph neural networks (GNNs) to count certain graph substructures, especially cycles, is important for the success of GNNs on a wide range of tasks. It has been recently used as a popular metric for evaluating the expressive power of GNNs. Many of the proposed GNN models with provable cycle counting power are based on subgraph GNNs, i.e., extracting a bag of subgraphs from the input graph, generating representations for each subgraph, and using them to augment the representation of the input graph. However, those methods require heavy preprocessing, and suffer from high time and memory costs. In this paper, we overcome the aforementioned limitations of subgraph GNNs by proposing a novel class of GNNs -- $d$-Distance-Restricted FWL(2) GNNs, or $d$-DRFWL(2) GNNs. $d$-DRFWL(2) GNNs use node pairs whose mutual distances are at most $d$ as the units for message passing to balance the expressive power and complexity. By performing message passing among distance-restricted node
&lt;/p&gt;</description></item><item><title>&#20113;&#35745;&#31639;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#23433;&#20840;&#39118;&#38505;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#22312;&#35782;&#21035;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#20174;&#32780;&#25913;&#21464;&#20102;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#22312;&#23433;&#20840;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04911</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20113;&#35745;&#31639;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Machine Learning-based Security in Cloud Computing. (arXiv:2309.04911v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04911
&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#23433;&#20840;&#39118;&#38505;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#21487;&#20197;&#22312;&#35782;&#21035;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#20943;&#23569;&#20154;&#24037;&#24178;&#39044;&#30340;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#20174;&#32780;&#25913;&#21464;&#20102;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#22312;&#23433;&#20840;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#65288;CC&#65289;&#27491;&#22312;&#25913;&#21464;&#21521;&#29992;&#25143;&#25552;&#20379;IT&#36164;&#28304;&#30340;&#26041;&#24335;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#20197;&#26356;&#39640;&#30340;&#25104;&#26412;&#25928;&#30410;&#21644;&#31616;&#21270;&#30340;&#22522;&#30784;&#35774;&#26045;&#26469;&#35775;&#38382;&#21644;&#31649;&#29702;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;CC&#30340;&#22686;&#38271;&#65292;&#20986;&#29616;&#20102;&#19968;&#31995;&#21015;&#23433;&#20840;&#39118;&#38505;&#65292;&#21253;&#25324;&#23545;&#21487;&#29992;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#26426;&#23494;&#24615;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;CSPs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#20943;&#23569;&#22312;&#35782;&#21035;&#21644;&#35299;&#20915;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#30340;&#20154;&#24037;&#24178;&#39044;&#12290;ML&#33021;&#22815;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#24182;&#36827;&#34892;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#21487;&#20197;&#25913;&#21464;CSPs&#22312;&#23433;&#20840;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#22312;&#20113;&#35745;&#31639;&#23433;&#20840;&#39046;&#22495;&#30340;&#19968;&#20123;&#26368;&#26032;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#19968;&#31995;&#21015;ML&#31639;&#27861;&#30340;&#29305;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#23427;&#20204;&#29420;&#29305;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#20851;&#20110;ML&#22312;&#20113;&#35745;&#31639;&#20013;&#30340;&#24403;&#21069;&#29366;&#20917;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud Computing (CC) is revolutionizing the way IT resources are delivered to users, allowing them to access and manage their systems with increased cost-effectiveness and simplified infrastructure. However, with the growth of CC comes a host of security risks, including threats to availability, integrity, and confidentiality. To address these challenges, Machine Learning (ML) is increasingly being used by Cloud Service Providers (CSPs) to reduce the need for human intervention in identifying and resolving security issues. With the ability to analyze vast amounts of data, and make high-accuracy predictions, ML can transform the way CSPs approach security. In this paper, we will explore some of the most recent research in the field of ML-based security in Cloud Computing. We will examine the features and effectiveness of a range of ML algorithms, highlighting their unique strengths and potential limitations. Our goal is to provide a comprehensive overview of the current state of ML in c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#23884;&#20837;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;SAH-GNN&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#36763;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#29289;&#29702;&#24847;&#20041;&#19978;&#30340;&#33021;&#37327;&#23432;&#24658;&#12290;</title><link>http://arxiv.org/abs/2309.04885</link><description>&lt;p&gt;
&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#65288;&#22270;&#65289;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Symplectic Structure-Aware Hamiltonian (Graph) Embeddings. (arXiv:2309.04885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#23884;&#20837;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;SAH-GNN&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#36763;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#29289;&#29702;&#24847;&#20041;&#19978;&#30340;&#33021;&#37327;&#23432;&#24658;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#65292;&#22266;&#23450;&#23884;&#20837;&#27969;&#24418;&#30340;&#20551;&#35774;&#24120;&#24120;&#38480;&#21046;&#20102;&#20854;&#23545;&#19981;&#21516;&#22270;&#20960;&#20309;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;GNNs&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#33410;&#28857;&#29305;&#24449;&#26356;&#26032;&#20013;&#65292;&#26469;&#35299;&#20915;&#36825;&#31867;&#23884;&#20837;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#25512;&#24191;&#21040;&#26356;&#28789;&#27963;&#30340;&#33410;&#28857;&#29305;&#24449;&#26356;&#26032;&#20013;&#12290;&#19982;&#29616;&#26377;&#30340;&#21463;&#21704;&#23494;&#39039;&#21551;&#21457;&#30340;GNNs&#19981;&#21516;&#65292;SAH-GNN&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#36763;&#26031;&#33922;&#36153;&#23572;&#27969;&#24418;&#19978;&#30340;&#40654;&#26364;&#20248;&#21270;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#28508;&#22312;&#30340;&#36763;&#32467;&#26500;&#65292;&#20174;&#32780;&#35268;&#36991;&#20102;&#29616;&#26377;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#21704;&#23494;&#39039;GNNs&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#20351;&#24471;SAH-GNN&#33021;&#22815;&#22312;&#27809;&#26377;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#36866;&#24212;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#33021;&#37327;&#23432;&#24658;&#65292;&#20351;&#24471;&#38544;&#24335;&#21704;&#23494;&#39039;&#31995;&#32479;&#20855;&#26377;&#29289;&#29702;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional Graph Neural Networks (GNNs), the assumption of a fixed embedding manifold often limits their adaptability to diverse graph geometries. Recently, Hamiltonian system-inspired GNNs are proposed to address the dynamic nature of such embeddings by incorporating physical laws into node feature updates. In this work, we present SAH-GNN, a novel approach that generalizes Hamiltonian dynamics for more flexible node feature updates. Unlike existing Hamiltonian-inspired GNNs, SAH-GNN employs Riemannian optimization on the symplectic Stiefel manifold to adaptively learn the underlying symplectic structure during training, circumventing the limitations of existing Hamiltonian GNNs that rely on a pre-defined form of standard symplectic structure. This innovation allows SAH-GNN to automatically adapt to various graph datasets without extensive hyperparameter tuning. Moreover, it conserves energy during training such that the implicit Hamiltonian system is physically meaningful. To thi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#28176;&#21464;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#20174;&#27169;&#24335;&#35782;&#21035;&#21040;&#20915;&#31574;&#21644;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#28041;&#21450;&#22343;&#34913;&#21644;&#21338;&#24328;&#35770;&#30340;&#25968;&#23398;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#20110;&#25552;&#20379;&#21160;&#26426;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.04877</link><description>&lt;p&gt;
&#28176;&#21464;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
A Gentle Introduction to Gradient-Based Optimization and Variational Inequalities for Machine Learning. (arXiv:2309.04877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#28176;&#21464;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24378;&#35843;&#20102;&#20174;&#27169;&#24335;&#35782;&#21035;&#21040;&#20915;&#31574;&#21644;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#36716;&#21464;&#65292;&#20197;&#21450;&#28041;&#21450;&#22343;&#34913;&#21644;&#21338;&#24328;&#35770;&#30340;&#25968;&#23398;&#25361;&#25112;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#20110;&#25552;&#20379;&#21160;&#26426;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#22522;&#20110;&#19982;&#28176;&#21464;&#20248;&#21270;&#30340;&#32039;&#23494;&#32852;&#31995;&#12290;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#37096;&#20998;&#21462;&#20915;&#20110;&#20174;&#27169;&#24335;&#35782;&#21035;&#21040;&#20915;&#31574;&#21644;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#36716;&#21464;&#12290;&#22312;&#36825;&#20123;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#65292;&#28041;&#21450;&#22343;&#34913;&#21644;&#21338;&#24328;&#35770;&#32780;&#19981;&#26159;&#26497;&#20540;&#30340;&#26032;&#30340;&#25968;&#23398;&#25361;&#25112;&#20986;&#29616;&#20102;&#12290;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;--&#32771;&#34385;&#21040;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#39640;&#32500;&#24230;&#21644;&#22823;&#35268;&#27169;--&#20294;&#31616;&#21333;&#30340;&#26799;&#24230;&#19979;&#38477;&#19981;&#20877;&#26159;&#31639;&#27861;&#35774;&#35745;&#30340;&#20986;&#21457;&#28857;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#30340;&#26356;&#24191;&#27867;&#26694;&#26550;&#30340;&#28201;&#21644;&#20171;&#32461;&#65292;&#20174;&#38797;&#28857;&#21644;&#21333;&#35843;&#21338;&#24328;&#24320;&#22987;&#65292;&#28982;&#21518;&#21040;&#19968;&#33324;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#12290;&#34429;&#28982;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#20960;&#20010;&#31639;&#27861;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#20294;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#25552;&#20379;&#21160;&#26426;&#21644;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress in machine learning in recent years has been based on a highly productive connection to gradient-based optimization. Further progress hinges in part on a shift in focus from pattern recognition to decision-making and multi-agent problems. In these broader settings, new mathematical challenges emerge that involve equilibria and game theory instead of optima. Gradient-based methods remain essential -- given the high dimensionality and large scale of machine-learning problems -- but simple gradient descent is no longer the point of departure for algorithm design. We provide a gentle introduction to a broader framework for gradient-based algorithms in machine learning, beginning with saddle points and monotone games, and proceeding to general variational inequalities. While we provide convergence proofs for several of the algorithms that we present, our main focus is that of providing motivation and intuition.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HummingBird&#30340;MPC&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#36739;&#23567;&#30340;&#29615;&#19978;&#20351;&#29992;&#19968;&#37096;&#20998;&#27604;&#29305;&#26469;&#26174;&#33879;&#20943;&#23567;ReLU&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#20197;&#25552;&#39640;MPC&#22522;&#20110;&#31169;&#26377;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.04875</link><description>&lt;p&gt;
&#22312;&#20943;&#23567;&#30340;&#29615;&#19978;&#36817;&#20284;ReLU&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#22522;&#20110;MPC&#30340;&#31169;&#26377;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Approximating ReLU on a Reduced Ring for Efficient MPC-based Private Inference. (arXiv:2309.04875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HummingBird&#30340;MPC&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#36739;&#23567;&#30340;&#29615;&#19978;&#20351;&#29992;&#19968;&#37096;&#20998;&#27604;&#29305;&#26469;&#26174;&#33879;&#20943;&#23567;ReLU&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#20197;&#25552;&#39640;MPC&#22522;&#20110;&#31169;&#26377;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#30340;&#22810;&#26041;&#35745;&#31639;(MPC)&#20801;&#35768;&#29992;&#25143;&#22312;&#19981;&#24517;&#20849;&#20139;&#20854;&#25935;&#24863;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23558;&#26426;&#22120;&#23398;&#20064;&#25512;&#29702;&#22806;&#21253;&#32473;&#19981;&#21487;&#20449;&#30340;&#26381;&#21153;&#22120;&#12290;&#23613;&#31649;MPC&#22522;&#20110;&#31169;&#26377;&#25512;&#29702;&#20855;&#26377;&#24456;&#24378;&#30340;&#23433;&#20840;&#24615;&#65292;&#20294;&#30001;&#20110;&#39640;&#36890;&#20449;&#24320;&#38144;&#65292;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24182;&#26410;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#24403;&#35780;&#20272;ReLU&#23618;&#26102;&#65292;MPC&#21327;&#35758;&#38656;&#35201;&#22823;&#37327;&#30340;&#36890;&#20449;&#65292;&#23548;&#33268;&#20854;&#25972;&#20307;&#25191;&#34892;&#26102;&#38388;&#27604;&#38750;&#31169;&#26377;&#25512;&#29702;&#24930;&#24456;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HummingBird&#30340;MPC&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#36739;&#23567;&#30340;&#29615;&#19978;&#20351;&#29992;&#19968;&#37096;&#20998;&#27604;&#29305;&#26469;&#26174;&#33879;&#20943;&#23567;ReLU&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#22522;&#20110;&#29702;&#35770;&#20998;&#26512;&#65292;HummingBird&#22312;ReLU&#35780;&#20272;&#20013;&#35782;&#21035;&#20102;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#31192;&#23494;&#20998;&#20139;&#30340;&#27604;&#29305;&#65292;&#24182;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#25490;&#38500;&#23427;&#20204;&#20197;&#20943;&#23569;&#36890;&#20449;&#12290;&#36890;&#36807;&#20854;&#39640;&#25928;&#30340;&#25628;&#32034;&#24341;&#25806;&#65292;HummingBird&#22312;ReLU&#26399;&#38388;&#33293;&#24323;&#20102;87-91%&#30340;&#27604;&#29305;&#65292;&#24182;&#20173;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Secure multi-party computation (MPC) allows users to offload machine learning inference on untrusted servers without having to share their privacy-sensitive data. Despite their strong security properties, MPC-based private inference has not been widely adopted in the real world due to their high communication overhead. When evaluating ReLU layers, MPC protocols incur a significant amount of communication between the parties, making the end-to-end execution time multiple orders slower than its non-private counterpart.  This paper presents HummingBird, an MPC framework that reduces the ReLU communication overhead significantly by using only a subset of the bits to evaluate ReLU on a smaller ring. Based on theoretical analyses, HummingBird identifies bits in the secret share that are not crucial for accuracy and excludes them during ReLU evaluation to reduce communication. With its efficient search engine, HummingBird discards 87--91% of the bits during ReLU and still maintains high accur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#20445;&#35777;&#65292;&#21033;&#29992;&#36830;&#32493;&#30340;&#35823;&#24046;&#33539;&#25968;&#23545;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#22312;&#27424;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#30456;&#23545;&#20110;&#24050;&#26377;&#30340;&#36924;&#36817;&#26041;&#27861;&#23384;&#22312;&#36924;&#36817;&#29575;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04860</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Approximation Results for Gradient Descent trained Neural Networks. (arXiv:2309.04860v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#20445;&#35777;&#65292;&#21033;&#29992;&#36830;&#32493;&#30340;&#35823;&#24046;&#33539;&#25968;&#23545;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#22312;&#27424;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#30456;&#23545;&#20110;&#24050;&#26377;&#30340;&#36924;&#36817;&#26041;&#27861;&#23384;&#22312;&#36924;&#36817;&#29575;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#37319;&#29992;&#26799;&#24230;&#27969;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#36817;&#20284;&#20445;&#35777;&#65292;&#20854;&#20013;&#35823;&#24046;&#20197;&#36830;&#32493;&#30340;$L_2(\mathbb{S}^{d-1})$&#33539;&#25968;&#22312;$d$&#32500;&#21333;&#20301;&#29699;&#38754;&#19978;&#27979;&#37327;&#65292;&#30446;&#26631;&#20026;Sobolev&#24179;&#28369;&#12290;&#32593;&#32476;&#26159;&#23436;&#20840;&#36830;&#25509;&#30340;&#65292;&#28145;&#24230;&#24658;&#23450;&#65292;&#23485;&#24230;&#36882;&#22686;&#12290;&#34429;&#28982;&#25152;&#26377;&#23618;&#37117;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#20294;&#26799;&#24230;&#27969;&#30340;&#25910;&#25947;&#24615;&#26159;&#22522;&#20110;&#23545;&#20110;&#38750;&#20984;&#30340;&#20498;&#25968;&#31532;&#20108;&#23618;&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#30340;&#35770;&#35777;&#12290;&#19982;&#26631;&#20934;&#30340;NTK&#20998;&#26512;&#19981;&#21516;&#65292;&#36830;&#32493;&#35823;&#24046;&#33539;&#25968;&#26263;&#31034;&#20102;&#19968;&#20010;&#27424;&#21442;&#25968;&#21270;&#30340;&#21306;&#22495;&#65292;&#22312;&#36924;&#36817;&#26102;&#38656;&#35201;&#33258;&#28982;&#30340;&#20809;&#28369;&#24615;&#20551;&#35774;&#12290;&#20856;&#22411;&#30340;&#36807;&#21442;&#25968;&#21270;&#36890;&#36807;&#36924;&#36817;&#29575;&#30340;&#25439;&#22833;&#20197;&#21450;&#30456;&#23545;&#20110;Sobolev&#24179;&#28369;&#20989;&#25968;&#30340;&#24050;&#24314;&#31435;&#30340;&#36924;&#36817;&#26041;&#27861;&#32780;&#37325;&#26032;&#36827;&#20837;&#32467;&#26524;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper contains approximation guarantees for neural networks that are trained with gradient flow, with error measured in the continuous $L_2(\mathbb{S}^{d-1})$-norm on the $d$-dimensional unit sphere and targets that are Sobolev smooth. The networks are fully connected of constant depth and increasing width. Although all layers are trained, the gradient flow convergence is based on a neural tangent kernel (NTK) argument for the non-convex second but last layer. Unlike standard NTK analysis, the continuous error norm implies an under-parametrized regime, possible by the natural smoothness assumption required for approximation. The typical over-parametrization re-enters the results in form of a loss in approximation rate relative to established approximation methods for Sobolev smooth functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#36870;&#21521;&#24037;&#31243;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#29983;&#25104;&#25991;&#26412;&#20197;&#21450;&#25581;&#31034;&#30001;&#20110;&#35299;&#30721;&#35774;&#32622;&#23548;&#33268;&#30340;&#20559;&#35265;&#30340;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.04858</link><description>&lt;p&gt;
&#36870;&#21521;&#24037;&#31243;&#35299;&#30721;&#31574;&#30053;&#65306;&#22312;&#23545;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;
&lt;/p&gt;
&lt;p&gt;
Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System. (arXiv:2309.04858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#36870;&#21521;&#24037;&#31243;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#29983;&#25104;&#25991;&#26412;&#20197;&#21450;&#25581;&#31034;&#30001;&#20110;&#35299;&#30721;&#35774;&#32622;&#23548;&#33268;&#30340;&#20559;&#35265;&#30340;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#22312;&#20801;&#35768;&#29992;&#25143;&#36755;&#20837;&#25552;&#31034;&#24182;&#25509;&#25910;&#29983;&#25104;&#25991;&#26412;&#30340;API&#21644;&#32593;&#31449;&#19978;&#12290;&#35768;&#22810;&#31995;&#32479;&#19981;&#20250;&#36879;&#38706;&#29983;&#25104;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#36870;&#21521;&#24037;&#31243;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#30721;&#26041;&#27861;&#65288;&#21363;&#65292;top-k&#25110;nucleus&#37319;&#26679;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#20351;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#23545;&#20110;&#26816;&#27979;&#29983;&#25104;&#25991;&#26412;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#21457;&#29616;&#35299;&#30721;&#31574;&#30053;&#30340;&#36807;&#31243;&#21487;&#20197;&#25581;&#31034;&#30001;&#20110;&#36873;&#25321;&#35299;&#30721;&#35774;&#32622;&#32780;&#23548;&#33268;&#30340;&#20559;&#35265;&#65292;&#36825;&#20005;&#37325;&#25130;&#26029;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#20197;&#21450;&#29983;&#20135;&#31995;&#32479;&#19978;&#65288;&#20363;&#22914;&#65292;ChatGPT&#65289;&#19978;&#25191;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-$k$ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model's predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).
&lt;/p&gt;</description></item><item><title>AmbientFlow&#26159;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04856</link><description>&lt;p&gt;
AmbientFlow: &#26469;&#33258;&#19981;&#23436;&#25972;&#12289;&#22122;&#22768;&#27979;&#37327;&#30340;&#21487;&#36870;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AmbientFlow: Invertible generative models from incomplete, noisy measurements. (arXiv:2309.04856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04856
&lt;/p&gt;
&lt;p&gt;
AmbientFlow&#26159;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#31185;&#23398;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#37325;&#24314;&#12289;&#21518;&#39564;&#37319;&#26679;&#21644;&#25968;&#25454;&#20849;&#20139;&#12290;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#20197;&#21487;&#34892;&#30340;&#26041;&#24335;&#25552;&#20379;&#31934;&#30830;&#30340;&#23494;&#24230;&#20272;&#35745;&#20197;&#21450;&#24555;&#36895;&#12289;&#24265;&#20215;&#21644;&#22810;&#26679;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#19968;&#20010;&#22823;&#22411;&#12289;&#39640;&#36136;&#37327;&#30340;&#23545;&#35937;&#25968;&#25454;&#38598;&#12290;&#22312;&#35745;&#31639;&#25104;&#20687;&#31561;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#38656;&#35201;&#38271;&#26102;&#38388;&#33719;&#21462;&#25110;&#39640;&#36752;&#23556;&#21058;&#37327;&#65292;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#23545;&#35937;&#30340;&#22122;&#22768;&#25110;&#37096;&#20998;&#35266;&#27979;&#27979;&#37327;&#26356;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AmbientFlow&#65292;&#19968;&#20010;&#20174;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#22122;&#22768;&#12289;&#19981;&#23436;&#25972;&#25968;&#25454;&#24314;&#31435;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have gained popularity for their potential applications in imaging science, such as image reconstruction, posterior sampling and data sharing. Flow-based generative models are particularly attractive due to their ability to tractably provide exact density estimates along with fast, inexpensive and diverse samples. Training such models, however, requires a large, high quality dataset of objects. In applications such as computed imaging, it is often difficult to acquire such data due to requirements such as long acquisition time or high radiation dose, while acquiring noisy or partially observed measurements of these objects is more feasible. In this work, we propose AmbientFlow, a framework for learning flow-based generative models directly from noisy and incomplete data. Using variational Bayesian methods, a novel framework for establishing flow-based generative models from noisy, incomplete data is proposed. Extensive numerical studies demonstrate the effectiveness o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04849</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#31934;&#28860;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#24773;&#24863;&#34920;&#31034;&#36827;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect Representations. (arXiv:2309.04849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#36827;&#34892;&#20449;&#24687;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#34920;&#26126;&#20854;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EmoDistill&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#20174;&#35821;&#38899;&#20013;&#33719;&#21462;&#24773;&#24863;&#30340;&#24378;&#22823;&#30340;&#35821;&#35328;&#21644;&#35821;&#38899;&#34920;&#31034;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;&#19968;&#20018;&#35821;&#38899;&#20449;&#21495;&#26469;&#36827;&#34892;&#21333;&#27169;&#24577;SER&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#24182;&#36991;&#20813;&#36816;&#34892;&#26102;&#30340;&#36716;&#24405;&#21644;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#38169;&#35823;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#19968;&#23545;&#32463;&#36807;SER&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#25945;&#24072;&#20013;&#30340;&#23884;&#20837;&#21644;&#36923;&#36753;&#23618;&#38754;&#33976;&#39311;&#20449;&#24687;&#12290;&#22312;IEMOCAP&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#24182;&#36798;&#21040;&#20102;77.49&#65285;&#30340;&#26080;&#26435;&#37325;&#20934;&#30830;&#29575;&#21644;78.91&#65285;&#30340;&#21152;&#26435;&#20934;&#30830;&#29575;&#30340;&#26368;&#26032;&#25104;&#32489;&#12290;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#22806;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;HAct&#28608;&#27963;&#30452;&#26041;&#22270;&#25551;&#36848;&#31526;&#23545;OOD&#36827;&#34892;&#26816;&#27979;&#65292;&#20855;&#26377;&#31616;&#21333;&#39640;&#25928;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;OOD&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04837</link><description>&lt;p&gt;
HAct&#65306;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#30452;&#26041;&#22270;&#30340;&#25968;&#25454;&#38598;&#22806;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
HAct: Out-of-Distribution Detection with Neural Net Activation Histograms. (arXiv:2309.04837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#22806;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;HAct&#28608;&#27963;&#30452;&#26041;&#22270;&#25551;&#36848;&#31526;&#23545;OOD&#36827;&#34892;&#26816;&#27979;&#65292;&#20855;&#26377;&#31616;&#21333;&#39640;&#25928;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;OOD&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#25968;&#25454;&#38598;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#30340;&#26816;&#27979;&#65292;&#36825;&#26159;&#29992;&#20110;OOD&#27867;&#21270;&#26041;&#27861;&#30340;&#28508;&#22312;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25551;&#36848;&#31526;&#65292;&#21363;HAct&#28608;&#27963;&#30452;&#26041;&#22270;&#65292;&#29992;&#20110;OOD&#26816;&#27979;&#65292;&#21363;&#36890;&#36807;&#30452;&#26041;&#22270;&#26469;&#36817;&#20284;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#20540;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;HAct&#22312;&#22810;&#31181;OOD&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;&#20363;&#22914;&#65292;&#22312;&#26631;&#20934;&#30340;OOD&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;Resnet-50&#23454;&#29616;&#20102;95%&#30340;&#30495;&#27491;&#20363;&#29575;&#65288;TPR&#65289;&#65292;&#32780;&#21482;&#26377;0.05%&#30340;&#35823;&#25253;&#29575;&#65292;&#20351;&#24471;&#20854;&#22312;&#35823;&#25253;&#29575;&#19978;&#30456;&#36739;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;20.66%&#65288;&#22312;&#30456;&#21516;&#30340;95%TPR&#19979;&#65289;&#12290;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#26131;&#20110;&#23454;&#29616;&#20351;&#24471;HAct&#36866;&#21512;&#22823;&#35268;&#27169;&#23454;&#36341;&#20013;&#22312;&#32447;&#30417;&#27979;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple, efficient, and accurate method for detecting out-of-distribution (OOD) data for trained neural networks, a potential first step in methods for OOD generalization. We propose a novel descriptor, HAct activation histograms, for OOD detection, that is, probability distributions (approximated by histograms) of output values of neural network layers under the influence of incoming data. We demonstrate that HAct is significantly more accurate than state-of-the-art on multiple OOD image classification benchmarks. For instance, our approach achieves a true positive rate (TPR) of 95% with only 0.05% false-positives using Resnet-50 on standard OOD benchmarks, outperforming previous state-of-the-art by 20.66% in the false positive rate (at the same TPR of 95%). The low computational complexity and the ease of implementation make HAct suitable for online implementation in monitoring deployed neural networks in practice at scale.
&lt;/p&gt;</description></item><item><title>RHPG&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#23398;&#20064;&#26368;&#20248;&#32447;&#24615;&#20272;&#35745;&#22120;&#35774;&#35745;&#26041;&#38754;&#20855;&#26377;&#21487;&#35777;&#26126;&#20840;&#23616;&#25910;&#25947;&#24615;&#30340;PG&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26222;&#36890;&#30340;PG&#38598;&#25104;&#21040;&#21160;&#24577;&#35268;&#21010;&#30340;&#22806;&#24490;&#29615;&#20013;&#65292;&#23558;&#26080;&#32422;&#26463;&#19988;&#24378;&#20984;&#30340;&#38745;&#24577;&#20272;&#35745;&#38382;&#39064;&#20998;&#35299;&#25104;&#21463;&#38480;&#19988;&#38750;&#20984;&#30340;&#26080;&#31351;&#26102;&#22495;KF&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20840;&#23616;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2309.04831</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#20272;&#35745;&#22120;&#35774;&#35745;&#20013;&#65292;&#36882;&#20943;&#26102;&#22495;&#31574;&#30053;&#25628;&#32034;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of Receding-Horizon Policy Search in Learning Estimator Designs. (arXiv:2309.04831v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04831
&lt;/p&gt;
&lt;p&gt;
RHPG&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#23398;&#20064;&#26368;&#20248;&#32447;&#24615;&#20272;&#35745;&#22120;&#35774;&#35745;&#26041;&#38754;&#20855;&#26377;&#21487;&#35777;&#26126;&#20840;&#23616;&#25910;&#25947;&#24615;&#30340;PG&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26222;&#36890;&#30340;PG&#38598;&#25104;&#21040;&#21160;&#24577;&#35268;&#21010;&#30340;&#22806;&#24490;&#29615;&#20013;&#65292;&#23558;&#26080;&#32422;&#26463;&#19988;&#24378;&#20984;&#30340;&#38745;&#24577;&#20272;&#35745;&#38382;&#39064;&#20998;&#35299;&#25104;&#21463;&#38480;&#19988;&#38750;&#20984;&#30340;&#26080;&#31351;&#26102;&#22495;KF&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20840;&#23616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#36882;&#20943;&#26102;&#22495;&#31574;&#30053;&#26799;&#24230;&#65288;RHPG&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#23398;&#20064;&#26368;&#20248;&#32447;&#24615;&#20272;&#35745;&#22120;&#35774;&#35745;&#65288;&#21363;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65289;&#26041;&#38754;&#20855;&#26377;&#21487;&#35777;&#26126;&#20840;&#23616;&#25910;&#25947;&#24615;&#30340;PG&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;RHPG&#31639;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#20316;&#20026;&#21021;&#22987;&#21270;&#65292;&#20063;&#19981;&#38656;&#35201;&#30446;&#26631;&#31995;&#32479;&#26159;&#24320;&#29615;&#31283;&#23450;&#30340;&#12290;RHPG&#30340;&#20851;&#38190;&#26159;&#23558;&#26222;&#36890;&#30340;PG&#65288;&#25110;&#20854;&#20182;&#31574;&#30053;&#25628;&#32034;&#26041;&#21521;&#65289;&#38598;&#25104;&#21040;&#21160;&#24577;&#35268;&#21010;&#30340;&#22806;&#24490;&#29615;&#20013;&#65292;&#23558;&#22312;&#31574;&#30053;&#21442;&#25968;&#20013;&#21463;&#38480;&#19988;&#38750;&#20984;&#30340;&#26080;&#31351;&#26102;&#22495;KF&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26080;&#32422;&#26463;&#19988;&#24378;&#20984;&#30340;&#38745;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;RHPG&#30340;&#20248;&#21270;&#36335;&#32447;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#38024;&#23545;&#25511;&#21046;&#22120;&#35774;&#35745;&#24320;&#23637;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#21021;&#27493;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the receding-horizon policy gradient (RHPG) algorithm, the first PG algorithm with provable global convergence in learning the optimal linear estimator designs, i.e., the Kalman filter (KF). Notably, the RHPG algorithm does not require any prior knowledge of the system for initialization and does not require the target system to be open-loop stable. The key of RHPG is that we integrate vanilla PG (or any other policy search directions) into a dynamic programming outer loop, which iteratively decomposes the infinite-horizon KF problem that is constrained and non-convex in the policy parameter into a sequence of static estimation problems that are unconstrained and strongly-convex, thus enabling global convergence. We further provide fine-grained analyses of the optimization landscape under RHPG and detail the convergence and sample complexity guarantees of the algorithm. This work serves as an initial attempt to develop reinforcement learning algorithms specifically for con
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#26032;&#21152;&#26435;&#20462;&#27491;&#37319;&#26679;&#20559;&#24046;&#30340;&#31354;&#38388;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23454;&#29616;&#30446;&#26631;&#38169;&#35823;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#20154;&#24037;&#25968;&#25454;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#20351;&#39044;&#27979;&#35823;&#24046;&#20174;7%&#38477;&#33267;2%&#65292;&#19988;&#26679;&#26412;&#37327;&#36234;&#22823;&#25928;&#26524;&#36234;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.04824</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#26032;&#21152;&#26435;&#32416;&#27491;&#37319;&#26679;&#20559;&#24046;&#30340;&#31354;&#38388;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Correcting sampling biases via importancereweighting for spatial modeling. (arXiv:2309.04824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04824
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#26032;&#21152;&#26435;&#20462;&#27491;&#37319;&#26679;&#20559;&#24046;&#30340;&#31354;&#38388;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23454;&#29616;&#30446;&#26631;&#38169;&#35823;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#20154;&#24037;&#25968;&#25454;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#20351;&#39044;&#27979;&#35823;&#24046;&#20174;7%&#38477;&#33267;2%&#65292;&#19988;&#26679;&#26412;&#37327;&#36234;&#22823;&#25928;&#26524;&#36234;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#30001;&#20110;&#20998;&#24067;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#29615;&#22659;&#30740;&#31350;&#20013;&#30340;&#31354;&#38388;&#25968;&#25454;&#65292;&#23545;&#38169;&#35823;&#30340;&#20272;&#35745;&#36890;&#24120;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#37319;&#26679;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#30446;&#26631;&#38169;&#35823;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#36890;&#36807;&#32771;&#34385;&#26399;&#26395;&#38169;&#35823;&#21644;&#21487;&#29992;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#26679;&#26412;&#28857;&#19978;&#37325;&#26032;&#21152;&#26435;&#38169;&#35823;&#24182;&#25269;&#28040;&#20559;&#31227;&#12290;&#25105;&#20204;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#25216;&#26415;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#36827;&#34892;&#37325;&#21152;&#26435;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#30495;&#23454;&#19990;&#30028;&#31354;&#38388;&#25968;&#25454;&#38598;&#30340;&#20154;&#24037;&#25968;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#38169;&#35823;&#20272;&#35745;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#20026;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#24635;&#20307;&#39044;&#27979;&#35823;&#24046;&#20174;7%&#38477;&#20302;&#21040;&#20165;&#20026;2%&#65292;&#19988;&#38543;&#30528;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#32780;&#36827;&#19968;&#27493;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning models, the estimation of errors is often complex due to distribution bias, particularly in spatial data such as those found in environmental studies. We introduce an approach based on the ideas of importance sampling to obtain an unbiased estimate of the target error. By taking into account difference between desirable error and available data, our method reweights errors at each sample point and neutralizes the shift. Importance sampling technique and kernel density estimation were used for reweighteing. We validate the effectiveness of our approach using artificial data that resemble real-world spatial datasets. Our findings demonstrate advantages of the proposed approach for the estimation of the target error, offering a solution to a distribution shift problem. Overall error of predictions dropped from 7% to just 2% and it gets smaller for larger samples.
&lt;/p&gt;</description></item><item><title>ABC123&#26159;&#19968;&#31181;&#26080;&#38656;&#20351;&#29992;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#25110;&#25512;&#26029;&#30340;&#22810;&#31867;&#21035;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#65292;&#23427;&#22312;&#22810;&#31181;&#23545;&#35937;&#21516;&#26102;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04820</link><description>&lt;p&gt;
ABC&#31616;&#21333;&#22914;123&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#20808;&#20363;&#22810;&#31867;&#21035;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#30340;&#30450;&#30446;&#35745;&#25968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ABC Easy as 123: A Blind Counter for Exemplar-Free Multi-Class Class-agnostic Counting. (arXiv:2309.04820v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04820
&lt;/p&gt;
&lt;p&gt;
ABC123&#26159;&#19968;&#31181;&#26080;&#38656;&#20351;&#29992;&#31034;&#20363;&#36827;&#34892;&#35757;&#32451;&#25110;&#25512;&#26029;&#30340;&#22810;&#31867;&#21035;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#33539;&#24335;&#65292;&#23427;&#22312;&#22810;&#31181;&#23545;&#35937;&#21516;&#26102;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#26041;&#27861;&#21487;&#20197;&#23545;&#20219;&#24847;&#31867;&#21035;&#30340;&#23545;&#35937;&#36827;&#34892;&#35745;&#25968;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#33021;&#36866;&#29992;&#20110;&#38656;&#35201;&#19968;&#32452;&#29305;&#23450;&#31867;&#22411;&#30340;&#31034;&#20363;&#25110;&#22270;&#20687;&#20013;&#20165;&#21253;&#21547;&#19968;&#31181;&#31867;&#22411;&#23545;&#35937;&#30340;&#24773;&#20917;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#20043;&#19968;&#26159;&#32570;&#20047;&#36866;&#29992;&#20110;&#22810;&#31181;&#23545;&#35937;&#21516;&#26102;&#23384;&#22312;&#30340;&#35745;&#25968;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#31867;&#21035;&#12289;&#31867;&#21035;&#26080;&#20851;&#35745;&#25968;&#25968;&#25454;&#38598;&#65288;MCAC&#65289;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;ABC123&#30340;&#30450;&#30446;&#35745;&#25968;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#35757;&#32451;&#25110;&#25512;&#26029;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992;&#29305;&#23450;&#31867;&#22411;&#31034;&#20363;&#26469;&#21516;&#26102;&#35745;&#25968;&#22810;&#31181;&#23545;&#35937;&#12290;ABC123&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#22312;&#35745;&#25968;&#38454;&#27573;&#21518;&#25214;&#21040;&#31034;&#20363;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#23548;&#26679;&#26412;&#26469;&#24341;&#23548;&#35745;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ABC123&#22312;MCAC&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-agnostic counting methods enumerate objects of an arbitrary class, providing tremendous utility in many fields. Prior works have limited usefulness as they require either a set of examples of the type to be counted or that the image contains only a single type of object. A significant factor in these shortcomings is the lack of a dataset to properly address counting in settings with more than one kind of object present. To address these issues, we propose the first Multi-class, Class-Agnostic Counting dataset (MCAC) and A Blind Counter (ABC123), a method that can count multiple types of objects simultaneously without using examples of type during training or inference. ABC123 introduces a new paradigm where instead of requiring exemplars to guide the enumeration, examples are found after the counting stage to help a user understand the generated outputs. We show that ABC123 outperforms contemporary methods on MCAC without the requirement of human in-the-loop annotations. We also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#37327;&#23376;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#36829;&#35268;&#34892;&#20026;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#26816;&#27979;&#31639;&#27861;&#12290;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#24352;&#37327;&#32593;&#32476;&#20316;&#20026;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#22312;TensorFlow Quantum&#21644;TorchQuantum&#19978;&#25191;&#34892;&#65292;&#26469;&#39564;&#35777;&#37327;&#23376;&#31639;&#27861;&#26159;&#21542;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#12290;&#24403;&#24046;&#20998;&#38544;&#31169;&#36829;&#35268;&#25253;&#21578;&#26102;&#65292;&#31639;&#27861;&#20250;&#33258;&#21160;&#29983;&#25104;&#38169;&#35823;&#20449;&#24687;&#65292;&#21253;&#25324;&#36829;&#21453;&#38544;&#31169;&#30340;&#37327;&#23376;&#29366;&#24577;&#65292;&#20197;&#35828;&#26126;&#36829;&#35268;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.04819</link><description>&lt;p&gt;
&#26816;&#27979;&#37327;&#23376;&#31639;&#27861;&#24046;&#20998;&#38544;&#31169;&#30340;&#36829;&#35268;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Detecting Violations of Differential Privacy for Quantum Algorithms. (arXiv:2309.04819v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#37327;&#23376;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#36829;&#35268;&#34892;&#20026;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#26816;&#27979;&#31639;&#27861;&#12290;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#24352;&#37327;&#32593;&#32476;&#20316;&#20026;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#22312;TensorFlow Quantum&#21644;TorchQuantum&#19978;&#25191;&#34892;&#65292;&#26469;&#39564;&#35777;&#37327;&#23376;&#31639;&#27861;&#26159;&#21542;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#12290;&#24403;&#24046;&#20998;&#38544;&#31169;&#36829;&#35268;&#25253;&#21578;&#26102;&#65292;&#31639;&#27861;&#20250;&#33258;&#21160;&#29983;&#25104;&#38169;&#35823;&#20449;&#24687;&#65292;&#21253;&#25324;&#36829;&#21453;&#38544;&#31169;&#30340;&#37327;&#23376;&#29366;&#24577;&#65292;&#20197;&#35828;&#26126;&#36829;&#35268;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#20363;&#22914;&#25968;&#25454;&#25628;&#32034;&#21644;&#20998;&#26512;&#12289;&#20135;&#21697;&#25512;&#33616;&#21644;&#20449;&#29992;&#35780;&#20998;&#12290;&#23545;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#38544;&#31169;&#21644;&#20854;&#20182;&#20262;&#29702;&#38382;&#39064;&#30340;&#20851;&#27880;&#33258;&#28982;&#32780;&#28982;&#22320;&#22686;&#21152;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#24418;&#24335;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#37327;&#23376;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#36829;&#35268;&#34892;&#20026;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#19968;&#20010;&#65288;&#24102;&#26377;&#22122;&#22768;&#30340;&#65289;&#37327;&#23376;&#31639;&#27861;&#26159;&#21542;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#22312;&#25253;&#21578;&#24046;&#20998;&#38544;&#31169;&#36829;&#35268;&#26102;&#33258;&#21160;&#29983;&#25104;&#38169;&#35823;&#20449;&#24687;&#12290;&#35813;&#20449;&#24687;&#21253;&#25324;&#19968;&#23545;&#36829;&#21453;&#38544;&#31169;&#30340;&#37327;&#23376;&#29366;&#24577;&#65292;&#20197;&#35828;&#26126;&#36829;&#35268;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#37319;&#29992;&#20102;&#24352;&#37327;&#32593;&#32476;&#20316;&#20026;&#39640;&#25928;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#22312;TensorFlow Quantum&#21644;TorchQuantum&#19978;&#25191;&#34892;&#65292;&#23427;&#20204;&#20998;&#21035;&#26159;&#33879;&#21517;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;TensorFlow&#21644;PyTorch&#30340;&#37327;&#23376;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum algorithms for solving a wide range of practical problems have been proposed in the last ten years, such as data search and analysis, product recommendation, and credit scoring. The concern about privacy and other ethical issues in quantum computing naturally rises up. In this paper, we define a formal framework for detecting violations of differential privacy for quantum algorithms. A detection algorithm is developed to verify whether a (noisy) quantum algorithm is differentially private and automatically generate bugging information when the violation of differential privacy is reported. The information consists of a pair of quantum states that violate the privacy, to illustrate the cause of the violation. Our algorithm is equipped with Tensor Networks, a highly efficient data structure, and executed both on TensorFlow Quantum and TorchQuantum which are the quantum extensions of famous machine learning platforms -- TensorFlow and PyTorch, respectively. The effectiveness and e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;(NLGS)&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#26469;&#33258;&#21160;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20339;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04810</link><description>&lt;p&gt;
&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;&#65306;&#36890;&#36807;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#20449;&#24687;&#39537;&#21160;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#36827;&#34892;&#20056;&#31215;&#27969;&#24418;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization. (arXiv:2309.04810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;(NLGS)&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#26469;&#33258;&#21160;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20339;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#19982;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#40784;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#24658;&#23450;&#26354;&#29575;&#30340;&#21452;&#26354;&#21644;&#29699;&#24418;&#31354;&#38388;&#65292;&#25110;&#32773;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#28508;&#22312;&#31354;&#38388;&#24182;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#33258;&#21160;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20339;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#36824;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#20851;&#27880;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#23450;&#20041;&#20102;&#36825;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;(NLGS)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#30340;&#20505;&#36873;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#26032;&#27010;&#24565;&#36317;&#31163;&#65292;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20026;&#20102;&#35745;&#31639;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#26597;&#35810;&#35780;&#20272;&#25628;&#32034;&#30001;&#24658;&#23450;&#26354;&#29575;&#27169;&#22411;&#31354;&#38388;&#20056;&#31215;&#32452;&#25104;&#30340;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research indicates that the performance of machine learning models can be improved by aligning the geometry of the latent space with the underlying data structure. Rather than relying solely on Euclidean space, researchers have proposed using hyperbolic and spherical spaces with constant curvature, or combinations thereof, to better model the latent space and enhance model performance. However, little attention has been given to the problem of automatically identifying the optimal latent geometry for the downstream task. We mathematically define this novel formulation and coin it as neural latent geometry search (NLGS). More specifically, we introduce a principled method that searches for a latent geometry composed of a product of constant curvature model spaces with minimal query evaluations. To accomplish this, we propose a novel notion of distance between candidate latent geometries based on the Gromov-Hausdorff distance from metric geometry. In order to compute the Gromov-Ha
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#38754;&#25552;&#20132;&#28040;&#24687;&#36136;&#37327;&#26816;&#26597;&#22120;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#25552;&#20132;&#28040;&#24687;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#30340;&#26816;&#26597;&#12290;</title><link>http://arxiv.org/abs/2309.04797</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#38754;&#30340;&#25552;&#20132;&#28040;&#24687;&#36136;&#37327;&#26816;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Full-fledged Commit Message Quality Checker Based on Machine Learning. (arXiv:2309.04797v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04797
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20840;&#38754;&#25552;&#20132;&#28040;&#24687;&#36136;&#37327;&#26816;&#26597;&#22120;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#25552;&#20132;&#28040;&#24687;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#30340;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20132;&#28040;&#24687;&#65288;CMs&#65289;&#26159;&#29256;&#26412;&#25511;&#21046;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#36890;&#36807;&#25552;&#20379;&#26377;&#20851;&#26356;&#25913;&#20869;&#23481;&#21644;&#21407;&#22240;&#30340;&#37325;&#35201;&#19978;&#19979;&#25991;&#65292;&#23427;&#20204;&#26497;&#22823;&#22320;&#25903;&#25345;&#36719;&#20214;&#32500;&#25252;&#21644;&#28436;&#36827;&#12290;&#20294;&#26159;&#25776;&#20889;&#33391;&#22909;&#30340;CMs&#26159;&#22256;&#38590;&#30340;&#65292;&#24320;&#21457;&#20154;&#21592;&#32463;&#24120;&#24573;&#35270;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#36866;&#21512;&#23454;&#36341;&#30340;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;CM&#30340;&#32534;&#20889;&#36136;&#37327;&#65292;&#21253;&#25324;&#20854;&#21547;&#20041;&#21644;&#19978;&#19979;&#25991;&#12290;&#37492;&#20110;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30740;&#31350;&#38382;&#39064;&#65306;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22914;&#20309;&#34913;&#37327;CM&#36136;&#37327;&#65292;&#21253;&#25324;&#35821;&#20041;&#21644;&#19978;&#19979;&#25991;&#65311;&#36890;&#36807;&#32771;&#34385;&#26368;&#27969;&#34892;&#30340;CM&#36136;&#37327;&#25351;&#21335;&#30340;&#25152;&#26377;&#35268;&#21017;&#65292;&#21019;&#24314;&#36825;&#20123;&#35268;&#21017;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#21644;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#26597;&#36825;&#20123;&#35268;&#21017;&#65292;&#25105;&#20204;&#21487;&#20197;&#22238;&#31572;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;&#23545;&#20110;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23454;&#36341;&#20013;&#20855;&#26377;82.9&#65285;&#30340;&#26368;&#20302;F1&#20998;&#25968;&#36275;&#22815;&#22909;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#26816;&#26597;&#25152;&#26377;&#36825;&#20123;CM&#36136;&#37327;&#35268;&#21017;&#12290;&#23427;&#21487;&#20197;&#29992;&#26469;&#36741;&#21161;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#26356;&#22909;&#30340;CMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Commit messages (CMs) are an essential part of version control. By providing important context in regard to what has changed and why, they strongly support software maintenance and evolution. But writing good CMs is difficult and often neglected by developers. So far, there is no tool suitable for practice that automatically assesses how well a CM is written, including its meaning and context. Since this task is challenging, we ask the research question: how well can the CM quality, including semantics and context, be measured with machine learning methods? By considering all rules from the most popular CM quality guideline, creating datasets for those rules, and training and evaluating state-of-the-art machine learning models to check those rules, we can answer the research question with: sufficiently well for practice, with the lowest F$_1$ score of 82.9\%, for the most challenging task. We develop a full-fledged open-source framework that checks all these CM quality rules. It is use
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21160;&#21147;&#23398;&#22343;&#22330;&#29702;&#35770;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#39640;&#32500;&#38750;&#20984;&#25104;&#26412;&#20989;&#25968;&#20248;&#21270;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SGD&#22312;&#24674;&#22797;&#39640;&#32500;&#38750;&#32447;&#24615;&#21152;&#23494;&#20449;&#21495;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#12290;</title><link>http://arxiv.org/abs/2309.04788</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#39640;&#32500;&#20449;&#21495;&#24674;&#22797;&#30340;&#29627;&#29827;&#33021;&#37327;&#26223;&#35266;&#20013;&#34920;&#29616;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape. (arXiv:2309.04788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21160;&#21147;&#23398;&#22343;&#22330;&#29702;&#35770;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#39640;&#32500;&#38750;&#20984;&#25104;&#26412;&#20989;&#25968;&#20248;&#21270;&#20013;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SGD&#22312;&#24674;&#22797;&#39640;&#32500;&#38750;&#32447;&#24615;&#21152;&#23494;&#20449;&#21495;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#19968;&#31181;&#38750;&#24179;&#34913;&#31639;&#27861;&#65292;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;SGD&#22312;&#36825;&#39033;&#25216;&#26415;&#30340;&#25104;&#21151;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#31243;&#24230;&#20197;&#21450;&#30456;&#23545;&#20110;&#20854;&#20182;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;&#26799;&#24230;&#19979;&#38477;&#65289;&#22312;&#20248;&#21270;&#39640;&#32500;&#38750;&#20984;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#30340;&#25928;&#26524;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21160;&#21147;&#23398;&#22343;&#22330;&#29702;&#35770;&#22312;&#39640;&#32500;&#26497;&#38480;&#20013;&#20934;&#30830;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#24674;&#22797;&#38544;&#34255;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#21152;&#23494;&#20449;&#21495;&#38382;&#39064;&#65292;&#21363;&#19968;&#20010;&#20856;&#22411;&#30340;&#39640;&#32500;&#38750;&#20984;&#30340;&#38590;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;SGD&#21644;GD&#30340;&#24615;&#33021;&#65292;&#24182;&#34920;&#26126;SGD&#22823;&#22823;&#20248;&#20110;GD&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#24347;&#35947;&#26102;&#38388;&#36827;&#34892;&#24130;&#24459;&#25311;&#21512;&#34920;&#26126;&#65292;SGD&#22312;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#30340;&#24674;&#22797;&#38408;&#20540;&#23567;&#20110;GD&#30340;&#23545;&#24212;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) is an out-of-equilibrium algorithm used extensively to train artificial neural networks. However very little is known on to what extent SGD is crucial for to the success of this technology and, in particular, how much it is effective in optimizing high-dimensional non-convex cost functions as compared to other optimization algorithms such as Gradient Descent (GD). In this work we leverage dynamical mean field theory to analyze exactly its performances in the high-dimensional limit. We consider the problem of recovering a hidden high-dimensional non-linearly encrypted signal, a prototype high-dimensional non-convex hard optimization problem. We compare the performances of SGD to GD and we show that SGD largely outperforms GD. In particular, a power law fit of the relaxation time of these algorithms shows that the recovery threshold for SGD with small batch size is smaller than the corresponding one of GD.
&lt;/p&gt;</description></item><item><title>RRCNN$^{+}$&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#27531;&#24046;&#36882;&#24402;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#12290;&#23427;&#33021;&#26356;&#31283;&#23450;&#22320;&#20998;&#35299;&#20449;&#21495;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#28145;&#24230;&#23398;&#20064;&#20026;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2309.04782</link><description>&lt;p&gt;
RRCNN$^{+}$&#65306;&#19968;&#31181;&#22686;&#24378;&#30340;&#29992;&#20110;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#30340;&#27531;&#24046;&#36882;&#24402;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RRCNN$^{+}$: An Enhanced Residual Recursive Convolutional Neural Network for Non-stationary Signal Decomposition. (arXiv:2309.04782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04782
&lt;/p&gt;
&lt;p&gt;
RRCNN$^{+}$&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#27531;&#24046;&#36882;&#24402;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#12290;&#23427;&#33021;&#26356;&#31283;&#23450;&#22320;&#20998;&#35299;&#20449;&#21495;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#28145;&#24230;&#23398;&#20064;&#20026;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#23567;&#27874;&#20998;&#26512;&#26159;&#20004;&#31181;&#32463;&#20856;&#26041;&#27861;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#20449;&#21495;&#26102;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38750;&#32447;&#24615;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20197;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#20026;&#20808;&#39537;&#12290;&#23427;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#20026;&#20934;&#24179;&#31283;&#20998;&#37327;&#65292;&#20197;&#22312;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#20013;&#25581;&#31034;&#26356;&#22909;&#30340;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#21463;&#28145;&#24230;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#27531;&#24046;&#36882;&#24402;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RRCNN&#65289;&#12290;RRCNN&#19981;&#20165;&#21487;&#20197;&#22312;&#23545;&#22823;&#35268;&#27169;&#20449;&#21495;&#36827;&#34892;&#25209;&#22788;&#29702;&#26102;&#23454;&#29616;&#26356;&#31283;&#23450;&#30340;&#20998;&#35299;&#65292;&#32780;&#19988;&#36824;&#20026;&#38750;&#24179;&#31283;&#20449;&#21495;&#20998;&#35299;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20511;&#21161;&#26381;&#21153;&#22120;&#30340;&#24110;&#21161;&#36827;&#19968;&#27493;&#25913;&#36827;RRCNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-frequency analysis is an important and challenging task in many applications. Fourier and wavelet analysis are two classic methods that have achieved remarkable success in many fields. They also exhibit limitations when applied to nonlinear and non-stationary signals. To address this challenge, a series of nonlinear and adaptive methods, pioneered by the empirical mode decomposition method have been proposed. Their aim is to decompose a non-stationary signal into quasi-stationary components which reveal better features in the time-frequency analysis. Recently, inspired by deep learning, we proposed a novel method called residual recursive convolutional neural network (RRCNN). Not only RRCNN can achieve more stable decomposition than existing methods while batch processing large-scale signals with low computational cost, but also deep learning provides a unique perspective for non-stationary signal decomposition. In this study, we aim to further improve RRCNN with the help of sever
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#24369;&#28857;&#26469;&#25913;&#36827;&#27169;&#22411;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#36817;&#37051;&#20013;&#25214;&#21040;&#24182;&#24674;&#22797;&#21435;&#27700;&#21360;&#27169;&#22411;&#30340;&#27700;&#21360;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.04777</link><description>&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#30340;&#24369;&#28857; &#25913;&#36827;&#31283;&#20581;&#27169;&#22411;&#25968;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Model Watermark via Reducing Parametric Vulnerability. (arXiv:2309.04777v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20943;&#23569;&#21442;&#25968;&#24369;&#28857;&#26469;&#25913;&#36827;&#27169;&#22411;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#36817;&#37051;&#20013;&#25214;&#21040;&#24182;&#24674;&#22797;&#21435;&#27700;&#21360;&#27169;&#22411;&#30340;&#27700;&#21360;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#20854;&#21830;&#19994;&#20215;&#20540;&#21644;&#23545;&#36164;&#28304;&#30340;&#24040;&#22823;&#38656;&#27714;&#32780;&#25104;&#20026;&#23453;&#36149;&#30340;&#36164;&#20135;&#65292;&#28982;&#32780;&#20026;&#20102;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29256;&#26435;&#65292;&#26368;&#36817;&#22522;&#20110;&#21518;&#38376;&#30340;&#25317;&#26377;&#26435;&#39564;&#35777;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#22312;&#36825;&#31181;&#39564;&#35777;&#26041;&#24335;&#20013;&#65292;&#27169;&#22411;&#25152;&#26377;&#32773;&#21487;&#20197;&#22312;&#21457;&#24067;&#20043;&#21069;&#36890;&#36807;&#23884;&#20837;&#29305;&#23450;&#30340;&#21518;&#38376;&#34892;&#20026;&#23545;&#27169;&#22411;&#36827;&#34892;&#27700;&#21360;&#26631;&#35760;&#12290;&#38450;&#24481;&#26041;&#65288;&#36890;&#24120;&#26159;&#27169;&#22411;&#25152;&#26377;&#32773;&#65289;&#21487;&#20197;&#26681;&#25454;&#34892;&#20026;&#30340;&#23384;&#22312;&#26469;&#21028;&#26029;&#21487;&#30097;&#30340;&#31532;&#19977;&#26041;&#27169;&#22411;&#26159;&#21542;&#26159;&#20174;&#20182;&#20204;&#37027;&#37324;&#8220;&#20599;&#8221;&#26469;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#27700;&#21360;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#31227;&#38500;&#25915;&#20987;&#65288;&#29978;&#33267;&#22914;&#24494;&#35843;&#65289;&#38750;&#24120;&#33030;&#24369;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#36825;&#31181;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#23545;&#21442;&#25968;&#31354;&#38388;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#35768;&#22810;&#22312;&#27700;&#21360;&#27169;&#22411;&#38468;&#36817;&#30340;&#21435;&#27700;&#21360;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#24456;&#23481;&#26131;&#34987;&#29992;&#20110;&#31227;&#38500;&#25915;&#20987;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36855;&#20320;&#26368;&#22823;&#21270;&#26368;&#23567;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;&#36825;&#20123;&#21435;&#27700;&#21360;&#27169;&#22411;&#24182;&#24674;&#22797;&#23427;&#20204;&#30340;&#27700;&#21360;&#34892;&#20026;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25214;&#21040;&#24182;&#24674;&#22797;&#21435;&#27700;&#21360;&#27169;&#22411;&#30340;&#27700;&#21360;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are valuable assets considering their commercial benefits and huge demands for costly annotation and computation resources. To protect the copyright of DNNs, backdoor-based ownership verification becomes popular recently, in which the model owner can watermark the model by embedding a specific backdoor behavior before releasing it. The defenders (usually the model owners) can identify whether a suspicious third-party model is ``stolen'' from them based on the presence of the behavior. Unfortunately, these watermarks are proven to be vulnerable to removal attacks even like fine-tuning. To further explore this vulnerability, we investigate the parameter space and find there exist many watermark-removed models in the vicinity of the watermarked one, which may be easily used by removal attacks. Inspired by this finding, we propose a mini-max formulation to find these watermark-removed models and recover their watermark behavior. Extensive experiments demonstrate that o
&lt;/p&gt;</description></item><item><title>AudRandAug&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#38899;&#39057;&#25968;&#25454;&#30340;&#38543;&#26426;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#20174;&#19987;&#38376;&#30340;&#38899;&#39057;&#25628;&#32034;&#31354;&#38388;&#20013;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;&#34920;&#29616;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04762</link><description>&lt;p&gt;
AudRandAug&#65306;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#30340;&#38543;&#26426;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AudRandAug: Random Image Augmentations for Audio Classification. (arXiv:2309.04762v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04762
&lt;/p&gt;
&lt;p&gt;
AudRandAug&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#38899;&#39057;&#25968;&#25454;&#30340;&#38543;&#26426;&#22270;&#20687;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#20174;&#19987;&#38376;&#30340;&#38899;&#39057;&#25628;&#32034;&#31354;&#38388;&#20013;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;&#34920;&#29616;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#34987;&#35777;&#26126;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RandAug&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#39044;&#23450;&#20041;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#38543;&#26426;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;RandAug&#22312;&#22270;&#20687;&#30456;&#20851;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#38468;&#21152;&#30340;&#35745;&#31639;&#24320;&#38144;&#24456;&#23567;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#23578;&#26410;&#25506;&#32034;&#23558;RandAug&#24212;&#29992;&#20110;&#23558;&#38899;&#39057;&#36716;&#21270;&#20026;&#22270;&#20687;&#27169;&#24335;&#30340;&#38899;&#39057;&#25968;&#25454;&#22686;&#24378;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AudRandAug&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#38899;&#39057;&#25968;&#25454;&#30340;RandAug&#25913;&#36827;&#26041;&#27861;&#12290;AudRandAug&#20174;&#19987;&#38376;&#30340;&#38899;&#39057;&#25628;&#32034;&#31354;&#38388;&#20013;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#35780;&#20272;AudRandAug&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;AudRandAug&#22312;&#20934;&#30830;&#29575;&#34920;&#29616;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation has proven to be effective in training neural networks. Recently, a method called RandAug was proposed, randomly selecting data augmentation techniques from a predefined search space. RandAug has demonstrated significant performance improvements for image-related tasks while imposing minimal computational overhead. However, no prior research has explored the application of RandAug specifically for audio data augmentation, which converts audio into an image-like pattern. To address this gap, we introduce AudRandAug, an adaptation of RandAug for audio data. AudRandAug selects data augmentation policies from a dedicated audio search space. To evaluate the effectiveness of AudRandAug, we conducted experiments using various models and datasets. Our findings indicate that AudRandAug outperforms other existing data augmentation methods regarding accuracy performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.04761</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning Techniques in Educational Data Mining. (arXiv:2309.04761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#32508;&#21512;&#23457;&#26597;&#20102;&#22312;&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;&#20013;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#23545;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#25552;&#20379;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#25968;&#25454;&#25366;&#25496;(EDM)&#20316;&#20026;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#65292;&#21033;&#29992;&#35745;&#31639;&#25216;&#26415;&#26469;&#20998;&#26512;&#25945;&#32946;&#25968;&#25454;&#12290;&#38543;&#30528;&#25945;&#32946;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#20998;&#26512;&#21644;&#24314;&#27169;&#36825;&#20123;&#25968;&#25454;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#23457;&#26597;&#28145;&#24230;&#23398;&#20064;&#22312;EDM&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#20851;&#20110;EDM&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#29616;&#20195;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35814;&#32454;&#22238;&#39038;&#20102;&#22312;&#22235;&#20010;&#20856;&#22411;&#25945;&#32946;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#30693;&#35782;&#36319;&#36394;&#12289;&#23398;&#29983;&#19981;&#33391;&#34892;&#20026;&#26816;&#27979;&#12289;&#24615;&#33021;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;EDM&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#24037;&#20855;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26032;&#20852;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational Data Mining (EDM) has emerged as a vital field of research, which harnesses the power of computational techniques to analyze educational data. With the increasing complexity and diversity of educational data, Deep Learning techniques have shown significant advantages in addressing the challenges associated with analyzing and modeling this data. This survey aims to systematically review the state-of-the-art in EDM with Deep Learning. We begin by providing a brief introduction to EDM and Deep Learning, highlighting their relevance in the context of modern education. Next, we present a detailed review of Deep Learning techniques applied in four typical educational scenarios, including knowledge tracing, undesirable student detecting, performance prediction, and personalized recommendation. Furthermore, a comprehensive overview of public datasets and processing tools for EDM is provided. Finally, we point out emerging trends and future directions in this research area.
&lt;/p&gt;</description></item><item><title>RR-CP &#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#21306;&#22495;&#22522;&#30784;&#30340;&#30830;&#35748;&#39044;&#27979;&#26041;&#27861;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#25552;&#20379;&#39640;&#25928;&#24178;&#39044;&#21644;&#36136;&#37327;&#26816;&#26597;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;&#65292;&#22312;&#29992;&#25143;&#25351;&#23450;&#30340;&#38169;&#35823;&#29575;&#19979;&#25552;&#20379;&#26356;&#24378;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.04760</link><description>&lt;p&gt;
RR-CP: &#21487;&#38752;&#21306;&#22495;&#22522;&#30784;&#30340;&#21487;&#20449;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#19968;&#31181;&#30830;&#35748;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RR-CP: Reliable-Region-Based Conformal Prediction for Trustworthy Medical Image Classification. (arXiv:2309.04760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04760
&lt;/p&gt;
&lt;p&gt;
RR-CP &#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#21306;&#22495;&#22522;&#30784;&#30340;&#30830;&#35748;&#39044;&#27979;&#26041;&#27861;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#25552;&#20379;&#39640;&#25928;&#24178;&#39044;&#21644;&#36136;&#37327;&#26816;&#26597;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;&#65292;&#22312;&#29992;&#25143;&#25351;&#23450;&#30340;&#38169;&#35823;&#29575;&#19979;&#25552;&#20379;&#26356;&#24378;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#35748;&#39044;&#27979;&#65288;CP&#65289;&#20026;&#32473;&#23450;&#30340;&#27979;&#35797;&#26679;&#26412;&#29983;&#25104;&#19968;&#32452;&#39044;&#27979;&#65292;&#20351;&#24471;&#39044;&#27979;&#38598;&#20960;&#20046;&#24635;&#26159;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#65288;&#20363;&#22914;&#65292;99.5&#65285;&#30340;&#26102;&#38388;&#65289;&#12290; CP&#23545;&#32473;&#23450;&#27979;&#35797;&#26679;&#26412;&#30340;&#21487;&#33021;&#26631;&#31614;&#25552;&#20379;&#20840;&#38754;&#30340;&#39044;&#27979;&#65292;&#32780;&#38598;&#21512;&#30340;&#22823;&#23567;&#34920;&#31034;&#39044;&#27979;&#30340;&#30830;&#23450;&#31243;&#24230;&#65288;&#20363;&#22914;&#65292;&#22823;&#20110;&#19968;&#30340;&#38598;&#21512;&#26159;&#8220;&#19981;&#30830;&#23450;&#30340;&#8221;&#65289;&#12290; CP&#30340;&#36825;&#20123;&#29420;&#29305;&#23646;&#24615;&#20351;&#20154;&#31867;&#19987;&#23478;&#21644;&#21307;&#23398;AI&#27169;&#22411;&#20043;&#38388;&#33021;&#22815;&#26377;&#25928;&#21512;&#20316;&#65292;&#22312;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#24178;&#39044;&#21644;&#36136;&#37327;&#26816;&#26597;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#38752;&#21306;&#22495;&#22522;&#30784;&#30340;&#30830;&#35748;&#39044;&#27979;&#65288;RR-CP&#65289;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#24378;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#20197;&#20415;&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#36798;&#21040;&#29992;&#25143;&#25351;&#23450;&#30340;&#38169;&#35823;&#29575;&#65288;&#20363;&#22914;&#65292;0.5&#65285;&#65289;&#65292;&#24182;&#22312;&#27492;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#20248;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;&#65288;&#23613;&#37327;&#23567;&#65289;&#12290; &#24403;&#29992;&#25143;&#25351;&#23450;&#30340;&#38169;&#35823;&#29575;&#36798;&#21040;&#26102;&#65292;&#25105;&#20204;&#35748;&#20026;&#39044;&#27979;&#38598;&#22823;&#23567;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#34913;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction (CP) generates a set of predictions for a given test sample such that the prediction set almost always contains the true label (e.g., 99.5\% of the time). CP provides comprehensive predictions on possible labels of a given test sample, and the size of the set indicates how certain the predictions are (e.g., a set larger than one is `uncertain'). Such distinct properties of CP enable effective collaborations between human experts and medical AI models, allowing efficient intervention and quality check in clinical decision-making. In this paper, we propose a new method called Reliable-Region-Based Conformal Prediction (RR-CP), which aims to impose a stronger statistical guarantee so that the user-specified error rate (e.g., 0.5\%) can be achieved in the test time, and under this constraint, the size of the prediction set is optimized (to be small). We consider a small prediction set size an important measure only when the user-specified error rate is achieved. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#23556;&#19981;&#21464;&#38598;&#25104;&#21464;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;ReLU&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#29992;&#20110;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04742</link><description>&lt;p&gt;
&#25913;&#36827;ReLU&#32593;&#32476;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20223;&#23556;&#19981;&#21464;&#38598;&#25104;&#21464;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in ReLU Networks. (arXiv:2309.04742v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#23556;&#19981;&#21464;&#38598;&#25104;&#21464;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;ReLU&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#29992;&#20110;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#21512;&#36866;&#30340;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#25193;&#23637;&#36827;&#34892;&#36923;&#36753;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#20174;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#24182;&#35777;&#26126;&#24403;&#31890;&#23376;&#25968;&#37327;&#36235;&#20110;&#26080;&#31351;&#26102;&#65292;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#25910;&#25947;&#21040;&#22343;&#22330;&#26497;&#38480;&#30340;&#37327;&#21270;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#24182;&#32771;&#23519;&#23427;&#20204;&#20316;&#20026;&#36125;&#21494;&#26031;&#36817;&#20284;&#26041;&#27861;&#22312;ReLU&#32593;&#32476;&#20013;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of performing Bayesian inference for logistic regression using appropriate extensions of the ensemble Kalman filter. Two interacting particle systems are proposed that sample from an approximate posterior and prove quantitative convergence rates of these interacting particle systems to their mean-field limit as the number of particles tends to infinity. Furthermore, we apply these techniques and examine their effectiveness as methods of Bayesian approximation for quantifying predictive uncertainty in ReLU networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35838;&#31243;&#23398;&#20064;&#24341;&#20837;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#29983;&#29289;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04737</link><description>&lt;p&gt;
Spiking Neural Network&#32852;&#21512;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Training of Spiking Neural Network joint Curriculum Learning Strategy. (arXiv:2309.04737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35838;&#31243;&#23398;&#20064;&#24341;&#20837;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#29983;&#29289;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#65292;&#36880;&#28176;&#24341;&#20837;&#38590;&#24230;&#30340;&#27010;&#24565;&#26159;&#20154;&#31867;&#23398;&#20064;&#30340;&#33258;&#28982;&#36807;&#31243;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#20449;&#24687;&#22788;&#29702;&#30340;&#26041;&#24335;&#65292;&#20294;&#30446;&#21069;&#30340;SNNs&#27169;&#22411;&#23558;&#25152;&#26377;&#26679;&#26412;&#35270;&#20026;&#24179;&#31561;&#65292;&#36825;&#19982;&#20154;&#31867;&#23398;&#20064;&#30340;&#21407;&#21017;&#19981;&#31526;&#65292;&#24182;&#24573;&#35270;&#20102;SNNs&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#35838;&#31243;&#23398;&#20064;&#65288;CL&#65289;&#24341;&#20837;SNNs&#30340;CL-SNN&#27169;&#22411;&#65292;&#20351;SNNs&#26356;&#20687;&#20154;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#29983;&#29289;&#35299;&#37322;&#24615;&#12290;CL&#26159;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#25552;&#20513;&#22312;&#36880;&#28176;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#20043;&#21069;&#21521;&#27169;&#22411;&#23637;&#31034;&#26356;&#23481;&#26131;&#30340;&#25968;&#25454;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#20449;&#24515;&#24863;&#30693;&#30340;&#25439;&#22833;&#26469;&#34913;&#37327;&#21644;&#22788;&#29702;&#19981;&#21516;&#38590;&#24230;&#27700;&#24179;&#30340;&#26679;&#26412;&#12290;&#36890;&#36807;&#23398;&#20064;&#19981;&#21516;&#26679;&#26412;&#30340;&#32622;&#20449;&#24230;&#65292;&#27169;&#22411;&#33258;&#21160;&#20943;&#23569;&#20102;&#38590;&#26679;&#26412;&#23545;&#21442;&#25968;&#20248;&#21270;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Starting with small and simple concepts, and gradually introducing complex and difficult concepts is the natural process of human learning. Spiking Neural Networks (SNNs) aim to mimic the way humans process information, but current SNNs models treat all samples equally, which does not align with the principles of human learning and overlooks the biological plausibility of SNNs. To address this, we propose a CL-SNN model that introduces Curriculum Learning(CL) into SNNs, making SNNs learn more like humans and providing higher biological interpretability. CL is a training strategy that advocates presenting easier data to models before gradually introducing more challenging data, mimicking the human learning process. We use a confidence-aware loss to measure and process the samples with different difficulty levels. By learning the confidence of different samples, the model reduces the contribution of difficult samples to parameter optimization automatically. We conducted experiments on st
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#26102;&#31354;&#39118;&#21147;&#39044;&#27979;&#30340;&#22810;&#26102;&#31354;&#32593;&#32476;&#27169;&#22411;(MHSTN)&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20135;&#29983;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04733</link><description>&lt;p&gt;
&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#26102;&#31354;&#39118;&#21147;&#39044;&#27979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Spatiotemporal Deep Neural Network for Fine-Grained Multi-Horizon Wind Prediction. (arXiv:2309.04733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04733
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#22810;&#26102;&#31354;&#39118;&#21147;&#39044;&#27979;&#30340;&#22810;&#26102;&#31354;&#32593;&#32476;&#27169;&#22411;(MHSTN)&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20135;&#29983;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#36895;&#21644;&#39118;&#21521;&#30340;&#39044;&#27979;&#23545;&#20110;&#33322;&#31354;&#21644;&#39118;&#33021;&#21457;&#30005;&#31561;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#30001;&#20110;&#22825;&#27668;&#25968;&#25454;&#20013;&#30340;&#39640;&#38543;&#26426;&#24615;&#21644;&#22797;&#26434;&#30456;&#20851;&#24615;&#65292;&#36825;&#19968;&#39044;&#27979;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#19968;&#37096;&#20998;&#24433;&#21709;&#22240;&#32032;&#65292;&#22240;&#27492;&#32570;&#20047;&#23545;&#38382;&#39064;&#30340;&#31995;&#32479;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#22312;&#25991;&#29486;&#20013;&#23545;&#20110;&#32454;&#31890;&#24230;&#39044;&#27979;&#30340;&#20851;&#27880;&#36739;&#23569;&#65292;&#32780;&#32454;&#31890;&#24230;&#39044;&#27979;&#23545;&#20110;&#39640;&#25928;&#30340;&#34892;&#19994;&#36816;&#33829;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#21363;&#22810;&#26102;&#31354;&#39118;&#21147;&#32593;&#32476;(MHSTN)&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#21644;&#39640;&#25928;&#30340;&#32454;&#31890;&#24230;&#39118;&#21147;&#39044;&#27979;&#12290;MHSTN&#23558;&#38024;&#23545;&#19981;&#21516;&#22240;&#32032;&#30340;&#22810;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;(Seq2Seq)&#39592;&#26550;&#20013;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#21508;&#31181;&#25968;&#25454;&#28304;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#20026;&#32473;&#23450;&#21306;&#22495;&#20869;&#30340;&#25152;&#26377;&#31449;&#28857;&#20135;&#29983;&#22810;&#26102;&#31354;&#30340;&#39044;&#27979;&#12290;MHSTN&#30001;&#22235;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#26102;&#38388;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
The prediction of wind in terms of both wind speed and direction, which has a crucial impact on many real-world applications like aviation and wind power generation, is extremely challenging due to the high stochasticity and complicated correlation in the weather data. Existing methods typically focus on a sub-set of influential factors and thus lack a systematic treatment of the problem. In addition, fine-grained forecasting is essential for efficient industry operations, but has been less attended in the literature. In this work, we propose a novel data-driven model, Multi-Horizon SpatioTemporal Network (MHSTN), generally for accurate and efficient fine-grained wind prediction. MHSTN integrates multiple deep neural networks targeting different factors in a sequence-to-sequence (Seq2Seq) backbone to effectively extract features from various data sources and produce multi-horizon predictions for all sites within a given region. MHSTN is composed of four major modules. First, a temporal
&lt;/p&gt;</description></item><item><title>TCGAN&#26159;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#30340;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#25239;&#21338;&#24328;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#26080;&#38656;&#26631;&#35760;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.04732</link><description>&lt;p&gt;
TCGAN: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#32858;&#31867;&#30340;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TCGAN: Convolutional Generative Adversarial Network for Time Series Classification and Clustering. (arXiv:2309.04732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04732
&lt;/p&gt;
&lt;p&gt;
TCGAN&#26159;&#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#30340;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#25239;&#21338;&#24328;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#26080;&#38656;&#26631;&#35760;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30417;&#30563;&#24335;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#23618;&#34920;&#31034;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#65292;&#21487;&#29992;&#20110;&#25104;&#21151;&#30340;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36275;&#22815;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#31283;&#23450;&#23398;&#20064;&#65292;&#20294;&#26159;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#20063;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#22686;&#24378;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;GANs&#22914;&#20309;&#26377;&#25928;&#22320;&#20316;&#20026;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#65288;&#21363;&#20998;&#31867;&#21644;&#32858;&#31867;&#65289;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#19978;&#36848;&#32771;&#34385;&#28608;&#21457;&#20102;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#21367;&#31215;GAN&#65288;TCGAN&#65289;&#12290;TCGAN&#36890;&#36807;&#22312;&#27809;&#26377;&#26631;&#31614;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20004;&#20010;&#19968;&#32500;CNN&#65288;&#21363;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#65289;&#20043;&#38388;&#36827;&#34892;&#23545;&#25239;&#21338;&#24328;&#26469;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#35757;&#32451;&#30340;TCGAN&#30340;&#19968;&#37096;&#20998;&#34987;&#37325;&#22797;&#21033;&#29992;&#26469;&#26500;&#24314;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated the superiority of supervised Convolutional Neural Networks (CNNs) in learning hierarchical representations from time series data for successful classification. These methods require sufficiently large labeled data for stable learning, however acquiring high-quality labeled time series data can be costly and potentially infeasible. Generative Adversarial Networks (GANs) have achieved great success in enhancing unsupervised and semi-supervised learning. Nonetheless, to our best knowledge, it remains unclear how effectively GANs can serve as a general-purpose solution to learn representations for time series recognition, i.e., classification and clustering. The above considerations inspire us to introduce a Time-series Convolutional GAN (TCGAN). TCGAN learns by playing an adversarial game between two one-dimensional CNNs (i.e., a generator and a discriminator) in the absence of label information. Parts of the trained TCGAN are then reused to construct a rep
&lt;/p&gt;</description></item><item><title>&#22238;&#22768;&#25351;&#25968;&#26159;&#19968;&#20010;&#38750;&#33258;&#27835;&#21160;&#21147;&#31995;&#32479;&#20013;&#21516;&#26102;&#31283;&#23450;&#28176;&#36817;&#21709;&#24212;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22238;&#22768;&#25351;&#25968;&#23545;&#20381;&#36182;&#20110;&#21442;&#25968;&#20197;&#21450;&#36755;&#20837;&#37325;&#22797;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.04728</link><description>&lt;p&gt;
&#22238;&#22768;&#25351;&#25968;&#30340;&#36716;&#21464;&#21450;&#23545;&#36755;&#20837;&#37325;&#22797;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Transitions in echo index and dependence on input repetitions. (arXiv:2309.04728v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04728
&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#25351;&#25968;&#26159;&#19968;&#20010;&#38750;&#33258;&#27835;&#21160;&#21147;&#31995;&#32479;&#20013;&#21516;&#26102;&#31283;&#23450;&#28176;&#36817;&#21709;&#24212;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22238;&#22768;&#25351;&#25968;&#23545;&#20381;&#36182;&#20110;&#21442;&#25968;&#20197;&#21450;&#36755;&#20837;&#37325;&#22797;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#25351;&#25968;&#26159;&#19968;&#20010;&#38750;&#33258;&#27835;&#65288;&#21363;&#21463;&#36755;&#20837;&#39537;&#21160;&#65289;&#21160;&#21147;&#31995;&#32479;&#20013;&#21516;&#26102;&#31283;&#23450;&#28176;&#36817;&#21709;&#24212;&#30340;&#25968;&#37327;&#12290;&#23427;&#25512;&#24191;&#20102;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#22768;&#29366;&#24577;&#24615;&#36136;&#65292;&#36825;&#23545;&#24212;&#20110;&#22238;&#22768;&#25351;&#25968;&#31561;&#20110;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22238;&#22768;&#25351;&#25968;&#22914;&#20309;&#20381;&#36182;&#20110;&#25511;&#21046;&#31995;&#32479;&#23545;&#24378;&#36843;&#21160;&#21147;&#23398;&#30340;&#26377;&#38480;&#29366;&#24577;&#38543;&#26426;&#22806;&#37096;&#36755;&#20837;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22312;&#26377;&#38480;&#19968;&#32452;&#26144;&#23556;&#20043;&#38388;&#20999;&#25442;&#30340;&#38750;&#33258;&#27835;&#31995;&#32479;&#30340;&#22238;&#22768;&#25351;&#25968;&#65292;&#20854;&#20013;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#26144;&#23556;&#20855;&#26377;&#26377;&#38480;&#19968;&#32452;&#21452;&#26354;&#22411;&#24179;&#34913;&#21560;&#24341;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#27599;&#20010;&#26144;&#23556;&#30340;&#26368;&#23567;&#21644;&#26368;&#22823;&#37325;&#22797;&#23545;&#20110;&#24471;&#21040;&#30340;&#22238;&#22768;&#25351;&#25968;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#23558;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#29992;RNN&#35745;&#31639;&#26694;&#26550;&#34920;&#31034;&#65292;&#25105;&#20204;&#24471;&#21040;&#23545;&#20110;&#23567;&#24133;&#24378;&#36843;&#65292;&#22238;&#22768;&#25351;&#25968;&#23545;&#24212;&#20110;&#26080;&#36755;&#20837;&#31995;&#32479;&#30340;&#21560;&#24341;&#23376;&#25968;&#37327;&#65292;&#32780;&#23545;&#20110;&#22823;&#24133;&#24378;&#36843;&#65292;&#22238;&#22768;&#25351;&#25968;&#20943;&#23569;&#21040;&#19968;&#12290;&#20013;&#38388;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
The echo index counts the number of simultaneously stable asymptotic responses of a nonautonomous (i.e. input-driven) dynamical system. It generalizes the well-known echo state property for recurrent neural networks this corresponds to the echo index being equal to one. In this paper, we investigate how the echo index depends on parameters that govern typical responses to a finite-state ergodic external input that forces the dynamics. We consider the echo index for a nonautonomous system that switches between a finite set of maps, where we assume that each map possesses a finite set of hyperbolic equilibrium attractors. We find the minimum and maximum repetitions of each map are crucial for the resulting echo index. Casting our theoretical findings in the RNN computing framework, we obtain that for small amplitude forcing the echo index corresponds to the number of attractors for the input-free system, while for large amplitude forcing, the echo index reduces to one. The intermediate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;&#65292;&#36890;&#36807;&#19968;&#20010;&#23567;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#65292;&#24182;&#20197;ChatGPT&#20026;&#24037;&#20855;&#37325;&#29616;&#20102;&#19981;&#21516;&#21457;&#34920;&#20110;&#33879;&#21517;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#32593;&#32476;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.04716</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Toward Reproducing Network Research Results Using Large Language Models. (arXiv:2309.04716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;&#65292;&#36890;&#36807;&#19968;&#20010;&#23567;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#65292;&#24182;&#20197;ChatGPT&#20026;&#24037;&#20855;&#37325;&#29616;&#20102;&#19981;&#21516;&#21457;&#34920;&#20110;&#33879;&#21517;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#32593;&#32476;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#65292;&#37325;&#29616;&#30740;&#31350;&#32467;&#26524;&#38750;&#24120;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26368;&#20339;&#23454;&#36341;&#36890;&#24120;&#26377;&#19977;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;&#23547;&#25214;&#20844;&#24320;&#21487;&#29992;&#30340;&#21407;&#22411;&#65307;&#65288;2&#65289;&#32852;&#31995;&#20316;&#32773;&#33719;&#21462;&#31169;&#26377;&#21407;&#22411;&#65307;&#20197;&#21450;&#65288;3&#65289;&#26681;&#25454;&#35770;&#25991;&#25551;&#36848;&#25163;&#21160;&#23454;&#29616;&#21407;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24050;&#21457;&#34920;&#30340;&#32593;&#32476;&#30740;&#31350;&#27809;&#26377;&#20844;&#24320;&#21407;&#22411;&#65292;&#32780;&#33719;&#21462;&#31169;&#26377;&#21407;&#22411;&#20063;&#24456;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22823;&#37096;&#20998;&#37325;&#29616;&#24037;&#20316;&#37117;&#33457;&#36153;&#22312;&#26681;&#25454;&#35770;&#25991;&#25551;&#36848;&#36827;&#34892;&#25163;&#21160;&#23454;&#29616;&#19978;&#65292;&#36825;&#26082;&#32791;&#26102;&#21448;&#36153;&#21147;&#65292;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#25991;&#22823;&#32966;&#22320;&#25552;&#20986;&#20351;&#29992;&#26032;&#20852;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#37325;&#29616;&#32593;&#32476;&#30740;&#31350;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23567;&#35268;&#27169;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#65292;&#20854;&#20013;&#22235;&#21517;&#20855;&#22791;&#24517;&#35201;&#32593;&#32476;&#30693;&#35782;&#30340;&#23398;&#29983;&#20351;&#29992;ChatGPT&#36827;&#34892;&#20102;&#19981;&#21516;&#21457;&#34920;&#20110;&#33879;&#21517;&#20250;&#35758;&#21644;&#26399;&#21002;&#30340;&#32593;&#32476;&#31995;&#32479;&#30340;&#37325;&#29616;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducing research results in the networking community is important for both academia and industry. The current best practice typically resorts to three approaches: (1) looking for publicly available prototypes; (2) contacting the authors to get a private prototype; and (3) manually implementing a prototype following the description of the publication. However, most published network research does not have public prototypes and private prototypes are hard to get. As such, most reproducing efforts are spent on manual implementation based on the publications, which is both time and labor consuming and error-prone. In this paper, we boldly propose reproducing network research results using the emerging large language models (LLMs). In particular, we first prove its feasibility with a small-scale experiment, in which four students with essential networking knowledge each reproduces a different networking system published in prominent conferences and journals by prompt engineering ChatGPT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Reasoner&#30340;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#21592;&#65288;A2CR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#23450;&#20041;&#21644;&#20998;&#31867;&#28436;&#21592;&#34892;&#20026;&#30340;&#28508;&#22312;&#30446;&#30340;&#65292;&#33258;&#21160;&#29983;&#25104;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#29702;&#35299;&#20195;&#29702;&#20915;&#31574;&#36807;&#31243;&#30340;&#33539;&#20363;&#12290;</title><link>http://arxiv.org/abs/2309.04707</link><description>&lt;p&gt;
&#20174;&#25506;&#32034;&#24615;&#35270;&#35282;&#35299;&#37322;&#20195;&#29702;&#34892;&#20026;&#30340;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#21592;
&lt;/p&gt;
&lt;p&gt;
Advantage Actor-Critic with Reasoner: Explaining the Agent's Behavior from an Exploratory Perspective. (arXiv:2309.04707v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Reasoner&#30340;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#21592;&#65288;A2CR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#23450;&#20041;&#21644;&#20998;&#31867;&#28436;&#21592;&#34892;&#20026;&#30340;&#28508;&#22312;&#30446;&#30340;&#65292;&#33258;&#21160;&#29983;&#25104;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#29702;&#35299;&#20195;&#29702;&#20915;&#31574;&#36807;&#31243;&#30340;&#33539;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#23427;&#30340;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#22312;&#20915;&#31574;&#20855;&#26377;&#23454;&#38469;&#21518;&#26524;&#30340;&#39046;&#22495;&#20013;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Reasoner&#30340;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#21592;&#65288;A2CR&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#21592;&#30340;RL&#27169;&#22411;&#65292;&#24182;&#20351;&#20854;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;A2CR&#30001;&#19977;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#32593;&#32476;&#32452;&#25104;&#65306;&#31574;&#30053;&#32593;&#32476;&#65292;&#20215;&#20540;&#32593;&#32476;&#21644;Reasoner&#32593;&#32476;&#12290;&#36890;&#36807;&#39044;&#23450;&#20041;&#21644;&#20998;&#31867;&#28436;&#21592;&#34892;&#20026;&#30340;&#28508;&#22312;&#30446;&#30340;&#65292;A2CR&#33258;&#21160;&#29983;&#25104;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#21487;&#35299;&#37322;&#30340;&#29702;&#35299;&#20195;&#29702;&#20915;&#31574;&#36807;&#31243;&#30340;&#33539;&#20363;&#12290;&#23427;&#25552;&#20379;&#20102;&#35832;&#22914;&#22522;&#20110;&#30446;&#30340;&#30340;&#26174;&#33879;&#24615;&#12289;&#26089;&#26399;&#22833;&#36133;&#26816;&#27979;&#21644;&#27169;&#22411;&#30417;&#31649;&#31561;&#19968;&#31995;&#21015;&#21151;&#33021;&#65292;&#20174;&#32780;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#21487;&#20449;&#36182;&#30340;RL&#12290;&#22312;&#21160;&#20316;&#20016;&#23500;&#30340;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#35780;&#20272;&#20135;&#29983;&#20102;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;Reasoner-&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a powerful tool for solving complex decision-making problems, but its lack of transparency and interpretability has been a major challenge in domains where decisions have significant real-world consequences. In this paper, we propose a novel Advantage Actor-Critic with Reasoner (A2CR), which can be easily applied to Actor-Critic-based RL models and make them interpretable. A2CR consists of three interconnected networks: the Policy Network, the Value Network, and the Reasoner Network. By predefining and classifying the underlying purpose of the actor's actions, A2CR automatically generates a more comprehensive and interpretable paradigm for understanding the agent's decision-making process. It offers a range of functionalities such as purpose-based saliency, early failure detection, and model supervision, thereby promoting responsible and trustworthy RL. Evaluations conducted in action-rich Super Mario Bros environments yield intriguing findings: Reasoner-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;LLM&#27169;&#22411;&#36890;&#36807;&#32454;&#35843;&#23454;&#29616;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#65292;&#24182;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#24773;&#24863;&#65292;&#20197;&#27492;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.04704</link><description>&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Disinformation and Fake News Detection Using Fine-Tuned Large Language Model. (arXiv:2309.04704v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20351;&#29992;LLM&#27169;&#22411;&#36890;&#36807;&#32454;&#35843;&#23454;&#29616;&#34394;&#20551;&#20449;&#24687;&#21644;&#20551;&#26032;&#38395;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#65292;&#24182;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#30340;&#24773;&#24863;&#65292;&#20197;&#27492;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;LLM&#65288;Llama 2&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#36890;&#36807;&#32454;&#35843;&#36827;&#34892;&#34394;&#20551;&#20449;&#24687;&#20998;&#26512;&#21644;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;PEFT/LoRA&#30340;&#32454;&#35843;&#26041;&#27861;&#12290;&#30740;&#31350;&#20013;&#65292;&#35813;&#27169;&#22411;&#23545;&#20197;&#19979;&#20219;&#21153;&#36827;&#34892;&#20102;&#32454;&#35843;&#65306;&#25581;&#31034;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#21465;&#20107;&#30340;&#25991;&#26412;&#20998;&#26512;&#65292;&#20107;&#23454;&#26680;&#26597;&#65292;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#25805;&#32437;&#20998;&#26512;&#20197;&#21450;&#25552;&#21462;&#24102;&#26377;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#32454;&#35843;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#23545;&#25991;&#26412;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#22797;&#26434;&#30340;&#39118;&#26684;&#21644;&#21465;&#20107;&#12290;&#24102;&#26377;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#21487;&#20197;&#20316;&#20026;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility of fine-tuning Llama 2 large language model (LLM) for the disinformation analysis and fake news detection. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text on revealing disinformation and propaganda narratives, fact checking, fake news detection, manipulation analytics, extracting named entities with their sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a deep analysis of texts and reveal complex styles and narratives. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models.
&lt;/p&gt;</description></item><item><title>&#24369;-PDE-LEARN&#26159;&#19968;&#31181;&#22522;&#20110;&#24369;&#24418;&#24335;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#26377;&#22122;&#22768;&#12289;&#26377;&#38480;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;PDEs&#12290;</title><link>http://arxiv.org/abs/2309.04699</link><description>&lt;p&gt;
&#24369;-PDE-LEARN&#65306;&#19968;&#31181;&#22522;&#20110;&#24369;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#20174;&#26377;&#22122;&#22768;&#12289;&#26377;&#38480;&#25968;&#25454;&#20013;&#21457;&#29616;PDEs
&lt;/p&gt;
&lt;p&gt;
Weak-PDE-LEARN: A Weak Form Based Approach to Discovering PDEs From Noisy, Limited Data. (arXiv:2309.04699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04699
&lt;/p&gt;
&lt;p&gt;
&#24369;-PDE-LEARN&#26159;&#19968;&#31181;&#22522;&#20110;&#24369;&#24418;&#24335;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#26377;&#22122;&#22768;&#12289;&#26377;&#38480;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;PDEs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24369;-PDE-LEARN&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20174;&#20855;&#26377;&#22122;&#22768;&#12289;&#26377;&#38480;&#27979;&#37327;&#35299;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#38750;&#32447;&#24615;PDEs&#12290;&#24369;-PDE-LEARN&#20351;&#29992;&#22522;&#20110;&#24369;&#24418;&#24335;&#30340;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;U&#65292;&#20197;&#36817;&#20284;PDE&#35299;&#65292;&#24182;&#21516;&#26102;&#35782;&#21035;&#25511;&#21046;PDE&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20174;&#20855;&#26377;&#22122;&#22768;&#12289;&#26377;&#38480;&#27979;&#37327;&#35299;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#22810;&#31181;PDEs&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#20960;&#20010;&#22522;&#20934;PDEs&#26469;&#35777;&#26126;&#24369;-PDE-LEARN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Weak-PDE-LEARN, a Partial Differential Equation (PDE) discovery algorithm that can identify non-linear PDEs from noisy, limited measurements of their solutions. Weak-PDE-LEARN uses an adaptive loss function based on weak forms to train a neural network, $U$, to approximate the PDE solution while simultaneously identifying the governing PDE. This approach yields an algorithm that is robust to noise and can discover a range of PDEs directly from noisy, limited measurements of their solutions. We demonstrate the efficacy of Weak-PDE-LEARN by learning several benchmark PDEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26080;&#20887;&#20313;&#20851;&#31995;&#22270;&#32858;&#31867;&#65288;R^2FGC&#65289;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#25552;&#21462;&#23646;&#24615;&#21644;&#32467;&#26500;&#32423;&#21035;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.04694</link><description>&lt;p&gt;
&#26080;&#20887;&#20313;&#30340;&#33258;&#30417;&#30563;&#20851;&#31995;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Redundancy-Free Self-Supervised Relational Learning for Graph Clustering. (arXiv:2309.04694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26080;&#20887;&#20313;&#20851;&#31995;&#22270;&#32858;&#31867;&#65288;R^2FGC&#65289;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#25552;&#21462;&#23646;&#24615;&#21644;&#32467;&#26500;&#32423;&#21035;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26159;&#25968;&#25454;&#20998;&#26512;&#20013;&#19968;&#39033;&#22522;&#30784;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#26469;&#36827;&#34892;&#26377;&#25928;&#30340;&#32858;&#31867;&#20998;&#37197;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#20276;&#38543;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#27880;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#24573;&#35270;&#20102;&#22270;&#20013;&#38750;&#29420;&#31435;&#21644;&#38750;&#21516;&#20998;&#24067;&#33410;&#28857;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#20449;&#24687;&#12290;&#30001;&#20110;&#23545;&#20851;&#31995;&#23646;&#24615;&#30340;&#25506;&#32034;&#19981;&#36275;&#65292;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#35821;&#20041;&#20449;&#24687;&#26080;&#27861;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#23548;&#33268;&#32858;&#31867;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26080;&#20887;&#20313;&#20851;&#31995;&#22270;&#32858;&#31867;&#65288;R^2FGC&#65289;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#21644;&#22270;&#33258;&#32534;&#30721;&#22120;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#35270;&#35282;&#25552;&#21462;&#23646;&#24615;&#21644;&#32467;&#26500;&#32423;&#21035;&#30340;&#20851;&#31995;&#20449;&#24687;&#12290;&#20026;&#20102;&#33719;&#24471;&#35821;&#20041;&#20449;&#24687;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#25105;&#20204;&#20445;&#30041;&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#19968;&#33268;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering, which learns the node representations for effective cluster assignments, is a fundamental yet challenging task in data analysis and has received considerable attention accompanied by graph neural networks in recent years. However, most existing methods overlook the inherent relational information among the non-independent and non-identically distributed nodes in a graph. Due to the lack of exploration of relational attributes, the semantic information of the graph-structured data fails to be fully exploited which leads to poor clustering performance. In this paper, we propose a novel self-supervised deep graph clustering method named Relational Redundancy-Free Graph Clustering (R$^2$FGC) to tackle the problem. It extracts the attributeand structure-level relational information from both global and local views based on an autoencoder and a graph autoencoder. To obtain effective representations of the semantic information, we preserve the consistent relation among aug
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEMSP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#38480;&#21046;&#24322;&#24120;&#29305;&#24449;&#30340;&#21464;&#21270;&#20540;&#26469;&#25552;&#20379;&#28789;&#27963;&#19988;&#20581;&#22766;&#30340;&#23545;&#20107;&#23454;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.04676</link><description>&lt;p&gt;
&#28789;&#27963;&#32780;&#20581;&#22766;&#30340;&#20855;&#26377;&#26368;&#23567;&#28385;&#36275;&#25200;&#21160;&#30340;&#23545;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations. (arXiv:2309.04676v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CEMSP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#38480;&#21046;&#24322;&#24120;&#29305;&#24449;&#30340;&#21464;&#21270;&#20540;&#26469;&#25552;&#20379;&#28789;&#27963;&#19988;&#20581;&#22766;&#30340;&#23545;&#20107;&#23454;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20107;&#23454;&#35299;&#37322;&#65288;CFEs&#65289;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#20462;&#25913;&#29305;&#24449;&#21521;&#37327;&#26469;&#23454;&#29616;&#19981;&#21516;&#23454;&#20363;&#30340;&#39044;&#27979;&#12290;CFEs&#21487;&#20197;&#22686;&#24378;&#20449;&#24687;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#20026;&#25910;&#21040;&#19981;&#21033;&#39044;&#27979;&#30340;&#29992;&#25143;&#25552;&#20379;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#30456;&#21516;&#23454;&#20363;&#25110;&#31245;&#26377;&#24046;&#24322;&#30340;&#23454;&#20363;&#65292;&#21487;&#20197;&#25552;&#20379;&#22810;&#20010;CFEs&#12290;&#22810;&#20010;CFEs&#20026;&#29992;&#25143;&#36873;&#25321;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36820;&#22238;&#19981;&#31283;&#23450;&#30340;CFEs&#19988;&#25104;&#26412;&#19981;&#21516;&#65292;&#23558;&#20250;&#25439;&#23475;&#20010;&#20307;&#20844;&#24179;&#24615;&#21644;&#27169;&#22411;&#21487;&#38752;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#21033;&#29992;&#28789;&#27963;&#24615;&#24182;&#35299;&#20915;&#38750;&#20581;&#22766;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#19978;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21629;&#21517;&#20026;&#20855;&#26377;&#26368;&#23567;&#28385;&#36275;&#25200;&#21160;&#30340;&#23545;&#20107;&#23454;&#35299;&#37322;&#65288;CEMSP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations (CFEs) exemplify how to minimally modify a feature vector to achieve a different prediction for an instance. CFEs can enhance informational fairness and trustworthiness, and provide suggestions for users who receive adverse predictions. However, recent research has shown that multiple CFEs can be offered for the same instance or instances with slight differences. Multiple CFEs provide flexible choices and cover diverse desiderata for user selection. However, individual fairness and model reliability will be damaged if unstable CFEs with different costs are returned. Existing methods fail to exploit flexibility and address the concerns of non-robustness simultaneously. To address these issues, we propose a conceptually simple yet effective solution named Counterfactual Explanations with Minimal Satisfiable Perturbations (CEMSP). Specifically, CEMSP constrains changing values of abnormal features with the help of their semantically meaningful normal ranges. Fo
&lt;/p&gt;</description></item><item><title>Compact&#25552;&#20986;&#20102;&#19968;&#31181;&#36924;&#36817;&#22797;&#26434;&#28608;&#27963;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;MPC&#25216;&#26415;&#39640;&#25928;&#37197;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#20250;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04664</link><description>&lt;p&gt;
Compact&#65306;&#36924;&#36817;&#22797;&#26434;&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#23433;&#20840;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Compact: Approximating Complex Activation Functions for Secure Computation. (arXiv:2309.04664v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04664
&lt;/p&gt;
&lt;p&gt;
Compact&#25552;&#20986;&#20102;&#19968;&#31181;&#36924;&#36817;&#22797;&#26434;&#28608;&#27963;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;MPC&#25216;&#26415;&#39640;&#25928;&#37197;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#20250;&#25439;&#22833;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;Secure multi-party computation&#65292;MPC&#65289;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#22312;&#29992;&#25143;&#26597;&#35810;&#37096;&#32626;&#22312;&#20844;&#20849;&#20113;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#26102;&#25552;&#20379;&#25968;&#25454;&#38544;&#31169;&#12290;&#26368;&#20808;&#36827;&#30340;MPC&#25216;&#26415;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#20351;&#29992;&#31616;&#21333;&#28608;&#27963;&#20989;&#25968;&#65288;AFs&#65289;&#30340;DNN&#27169;&#22411;&#65292;&#22914;ReLU&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26368;&#26032;&#30340;&#24212;&#29992;&#35774;&#35745;&#30340;DNN&#27169;&#22411;&#26550;&#26500;&#36890;&#24120;&#20351;&#29992;&#22797;&#26434;&#19988;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;AFs&#12290;&#20026;&#36825;&#20123;&#22797;&#26434;AFs&#35774;&#35745;&#39640;&#25928;&#30340;MPC&#25216;&#26415;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Compact&#65292;&#23427;&#20135;&#29983;&#22797;&#26434;AFs&#30340;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#20197;&#20415;&#19982;&#26368;&#20808;&#36827;&#30340;MPC&#25216;&#26415;&#39640;&#25928;&#37197;&#21512;&#20351;&#29992;&#12290;Compact&#26082;&#19981;&#38656;&#35201;&#20063;&#19981;&#24378;&#21152;&#20219;&#20309;&#27169;&#22411;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#23548;&#33268;&#20960;&#20046;&#30456;&#21516;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#27969;&#34892;&#30340;&#22797;&#26434;AFs SiLU&#12289;GeLU&#21644;Mish&#30340;DNN&#26550;&#26500;&#30340;&#22235;&#20010;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#19978;&#23545;Compact&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;Compact&#20960;&#20046;&#19981;&#20250;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Secure multi-party computation (MPC) techniques can be used to provide data privacy when users query deep neural network (DNN) models hosted on a public cloud. State-of-the-art MPC techniques can be directly leveraged for DNN models that use simple activation functions (AFs) such as ReLU. However, DNN model architectures designed for cutting-edge applications often use complex and highly non-linear AFs. Designing efficient MPC techniques for such complex AFs is an open problem.  Towards this, we propose Compact, which produces piece-wise polynomial approximations of complex AFs to enable their efficient use with state-of-the-art MPC techniques. Compact neither requires nor imposes any restriction on model training and results in near-identical model accuracy. We extensively evaluate Compact on four different machine-learning tasks with DNN architectures that use popular complex AFs SiLU, GeLU, and Mish. Our experimental results show that Compact incurs negligible accuracy loss compared
&lt;/p&gt;</description></item><item><title>MADLAD-400&#26159;&#19968;&#31181;&#35206;&#30422;419&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25991;&#26723;&#32423;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#33258;&#25105;&#23457;&#26680;&#25581;&#31034;&#20102;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#20247;&#22810;&#21442;&#25968;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#32473;&#30740;&#31350;&#30028;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.04662</link><description>&lt;p&gt;
MADLAD-400: &#19968;&#31181;&#22810;&#35821;&#35328;&#21644;&#25991;&#26723;&#32423;&#30340;&#22823;&#35268;&#27169;&#23457;&#26680;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MADLAD-400: A Multilingual And Document-Level Large Audited Dataset. (arXiv:2309.04662v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04662
&lt;/p&gt;
&lt;p&gt;
MADLAD-400&#26159;&#19968;&#31181;&#35206;&#30422;419&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#25991;&#26723;&#32423;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#33258;&#25105;&#23457;&#26680;&#25581;&#31034;&#20102;&#23616;&#38480;&#24615;&#65292;&#36890;&#36807;&#35757;&#32451;&#20247;&#22810;&#21442;&#25968;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21462;&#24471;&#20102;&#31454;&#20105;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#32473;&#30740;&#31350;&#30028;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MADLAD-400&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;CommonCrawl&#30340;&#25163;&#21160;&#23457;&#26680;&#30340;&#36890;&#29992;&#39046;&#22495;3T token&#21333;&#35821;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;419&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36890;&#36807;&#33258;&#25105;&#23457;&#26680;MADLAD-400&#25581;&#31034;&#20986;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#25968;&#25454;&#23457;&#26680;&#22312;&#25968;&#25454;&#38598;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#28085;&#30422;450&#22810;&#31181;&#35821;&#35328;&#12289;2500&#20159;&#20010;&#26631;&#35760;&#30340;10.7B&#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#19981;&#21516;&#39046;&#22495;&#19978;&#19982;&#35268;&#27169;&#26356;&#22823;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#25253;&#21578;&#20102;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;8B&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#23569;&#26679;&#26412;&#32763;&#35793;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#22522;&#20934;&#27169;&#22411;&#25552;&#20379;&#32473;&#30740;&#31350;&#30028;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community.
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#22312;&#20110;&#24341;&#20837;&#20102;&#20113;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;&#23884;&#20837;&#24335;&#26580;&#24615;&#20256;&#24863;&#22120;&#65292;&#23454;&#29616;&#20102;&#26234;&#33021;&#19978;&#32930;&#22806;&#39592;&#39612;&#31995;&#32479;&#26469;&#39044;&#27979;&#20154;&#31867;&#19978;&#32930;&#36816;&#21160;&#30340;&#24847;&#22270;&#24182;&#25552;&#20379;&#24863;&#35273;&#21453;&#39304;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;96.2%&#65292;&#33021;&#20197;&#20154;&#31867;&#24847;&#22270;&#20026;&#22522;&#30784;&#36827;&#34892;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.04655</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#20154;&#31867;&#24847;&#22270;&#30340;&#26234;&#33021;&#19978;&#32930;&#22806;&#39592;&#39612;&#31995;&#32479;&#20197;&#22686;&#24378;&#24863;&#35273;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Intelligent upper-limb exoskeleton using deep learning to predict human intention for sensory-feedback augmentation. (arXiv:2309.04655v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04655
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#22312;&#20110;&#24341;&#20837;&#20102;&#20113;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;&#23884;&#20837;&#24335;&#26580;&#24615;&#20256;&#24863;&#22120;&#65292;&#23454;&#29616;&#20102;&#26234;&#33021;&#19978;&#32930;&#22806;&#39592;&#39612;&#31995;&#32479;&#26469;&#39044;&#27979;&#20154;&#31867;&#19978;&#32930;&#36816;&#21160;&#30340;&#24847;&#22270;&#24182;&#25552;&#20379;&#24863;&#35273;&#21453;&#39304;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;96.2%&#65292;&#33021;&#20197;&#20154;&#31867;&#24847;&#22270;&#20026;&#22522;&#30784;&#36827;&#34892;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24180;&#40836;&#22686;&#38271;&#21644;&#20013;&#39118;&#31561;&#22240;&#32032;&#65292;&#20154;&#20307;&#32908;&#32905;&#39592;&#39612;&#21147;&#37327;&#19979;&#38477;&#65292;&#24433;&#21709;&#20102;&#20351;&#29992;&#19978;&#32930;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#29616;&#26377;&#19968;&#20123;&#22806;&#39592;&#39612;&#35013;&#32622;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#20256;&#24863;&#22120;&#21453;&#39304;&#21644;&#23545;&#36816;&#21160;&#24847;&#22270;&#30340;&#39044;&#27979;&#65292;&#38656;&#35201;&#25163;&#21160;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26234;&#33021;&#19978;&#32930;&#22806;&#39592;&#39612;&#31995;&#32479;&#65292;&#21033;&#29992;&#20113;&#31471;&#28145;&#24230;&#23398;&#20064;&#26469;&#39044;&#27979;&#20154;&#31867;&#30340;&#24847;&#22270;&#20197;&#22686;&#24378;&#21147;&#37327;&#12290;&#23884;&#20837;&#24335;&#26580;&#24615;&#20256;&#24863;&#22120;&#36890;&#36807;&#25910;&#38598;&#23454;&#26102;&#32908;&#32905;&#20449;&#21495;&#25552;&#20379;&#24863;&#35273;&#21453;&#39304;&#65292;&#24182;&#21516;&#26102;&#35745;&#31639;&#20197;&#30830;&#23450;&#29992;&#25143;&#30340;&#24847;&#22270;&#36816;&#21160;&#12290;&#20113;&#31471;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#22235;&#31181;&#19978;&#32930;&#20851;&#33410;&#36816;&#21160;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;96.2%&#65292;&#21709;&#24212;&#36895;&#24230;&#20026;200-250&#27627;&#31186;&#65292;&#34920;&#26126;&#22806;&#39592;&#39612;&#31995;&#32479;&#23436;&#20840;&#20381;&#38752;&#20154;&#31867;&#24847;&#22270;&#36827;&#34892;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#19968;&#32452;&#26580;&#24615;&#27668;&#21160;&#35013;&#32622;&#36890;&#36807;&#25552;&#20379;897&#29275;&#39039;&#21147;&#21644;78.7&#27627;&#31859;&#30340;&#20301;&#31227;&#26469;&#36741;&#21161;&#24847;&#22270;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
The age and stroke-associated decline in musculoskeletal strength degrades the ability to perform daily human tasks using the upper extremities. Although there are a few examples of exoskeletons, they need manual operations due to the absence of sensor feedback and no intention prediction of movements. Here, we introduce an intelligent upper-limb exoskeleton system that uses cloud-based deep learning to predict human intention for strength augmentation. The embedded soft wearable sensors provide sensory feedback by collecting real-time muscle signals, which are simultaneously computed to determine the user's intended movement. The cloud-based deep-learning predicts four upper-limb joint motions with an average accuracy of 96.2% at a 200-250 millisecond response rate, suggesting that the exoskeleton operates just by human intention. In addition, an array of soft pneumatics assists the intended movements by providing 897 newton of force and 78.7 millimeter of displacement at maximum. Col
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#23545;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#22349;&#22604;&#29616;&#35937;&#36827;&#34892;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#22312;&#25509;&#36817;&#26368;&#20248;&#26102;&#65292;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#33021;&#22815;&#20419;&#20351;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.04644</link><description>&lt;p&gt;
&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#65306;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay. (arXiv:2309.04644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#23545;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#22349;&#22604;&#29616;&#35937;&#36827;&#34892;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#22312;&#25509;&#36817;&#26368;&#20248;&#26102;&#65292;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#33021;&#22815;&#20419;&#20351;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#26159;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26368;&#21518;&#19968;&#23618;&#20013;&#20986;&#29616;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#25351;&#30340;&#26159;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32456;&#31471;&#38454;&#27573;&#65292;1&#65289;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#30340;&#31867;&#20869;&#21464;&#24322;&#36235;&#21521;&#20110;&#38646;&#65292;2&#65289;&#31867;&#29305;&#24449;&#22343;&#20540;&#26500;&#25104;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#65292;3&#65289;&#26368;&#21518;&#19968;&#23618;&#31867;&#29305;&#24449;&#21644;&#26435;&#37325;&#22312;&#32553;&#25918;&#19978;&#30456;&#31561;&#65292;4&#65289;&#20998;&#31867;&#34892;&#20026;&#23849;&#28291;&#21040;&#26368;&#36817;&#30340;&#31867;&#20013;&#24515;&#65288;NCC&#65289;&#20915;&#31574;&#35268;&#21017;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#23545;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#30452;&#35266;&#30340;&#31867;&#20869;&#21644;&#31867;&#38388;&#20313;&#24358;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#25429;&#25417;&#20102;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#22810;&#20010;&#26680;&#24515;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#65292;&#25105;&#20204;&#22312;&#27491;&#21017;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#25509;&#36817;&#26368;&#20248;&#26102;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#19979;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse is a recently observed geometric structure that emerges in the final layer of neural network classifiers. Specifically, Neural Collapse states that at the terminal phase of neural networks training, 1) the intra-class variability of last-layer features tends to zero, 2) the class feature means form an Equiangular Tight Frame (ETF), 3) last-layer class features and weights becomes equal up the scaling, and 4) classification behavior collapses to the nearest class center (NCC) decision rule. This paper investigates the effect of batch normalization and weight decay on the emergence of Neural Collapse. We propose the geometrically intuitive intra-class and inter-class cosine similarity measure which captures multiple core aspects of Neural Collapse. With this measure, we provide theoretical guarantees of Neural Collapse emergence with last-layer batch normalization and weight decay when the regularized cross-entropy loss is near optimal. We also perform further experiments
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#23569;&#37327;&#31034;&#33539;&#20013;&#23398;&#20064;&#21147;&#22522;&#21160;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35302;&#35273;&#34920;&#31034;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#31034;&#33539;&#23398;&#20064;&#26469;&#35757;&#32451;&#21160;&#20316;&#29983;&#25104;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#29289;&#29702;&#29305;&#24615;&#21644;&#29983;&#25104;&#25152;&#38656;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04640</link><description>&lt;p&gt;
&#20174;&#31034;&#33539;&#20013;&#39044;&#35757;&#32451;&#35302;&#35273;&#34920;&#31034;&#65292;&#23454;&#29616;&#21147;&#22522;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation. (arXiv:2309.04640v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20174;&#23569;&#37327;&#31034;&#33539;&#20013;&#23398;&#20064;&#21147;&#22522;&#21160;&#20316;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35302;&#35273;&#34920;&#31034;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#31034;&#33539;&#23398;&#20064;&#26469;&#35757;&#32451;&#21160;&#20316;&#29983;&#25104;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#29289;&#29702;&#29305;&#24615;&#21644;&#29983;&#25104;&#25152;&#38656;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#25509;&#35302;&#20016;&#23500;&#30340;&#20219;&#21153;&#20013;&#65292;&#21147;&#20256;&#24863;&#22312;&#36866;&#24212;&#25805;&#32437;&#23545;&#35937;&#30340;&#29289;&#29702;&#29305;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25429;&#25417;&#21040;&#23398;&#20064;&#30340;&#25805;&#32437;&#20219;&#21153;&#27867;&#21270;&#21040;&#26410;&#30693;&#23545;&#35937;&#25152;&#38656;&#30340;&#23545;&#35937;&#23646;&#24615;&#30340;&#22522;&#26412;&#20998;&#24067;&#65292;&#29616;&#26377;&#30340;&#31034;&#33539;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#26114;&#36149;&#30340;&#20154;&#31867;&#31034;&#33539;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21322;&#30417;&#30563;&#31034;&#33539;&#23398;&#20064;&#26041;&#27861;&#23558;&#23398;&#20064;&#27169;&#22411;&#20998;&#35299;&#20026;&#35302;&#35273;&#34920;&#31034;&#32534;&#30721;&#22120;&#21644;&#21160;&#20316;&#29983;&#25104;&#35299;&#30721;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#22823;&#37327;&#26080;&#30417;&#30563;&#25968;&#25454;&#39044;&#20808;&#35757;&#32451;&#35302;&#35273;&#34920;&#31034;&#32534;&#30721;&#22120;&#65292;&#21516;&#26102;&#20351;&#29992;&#23569;&#26679;&#26412;&#31034;&#33539;&#23398;&#20064;&#35757;&#32451;&#21160;&#20316;&#29983;&#25104;&#35299;&#30721;&#22120;&#65292;&#20174;&#20154;&#31867;&#23398;&#20064;&#25216;&#33021;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#19981;&#21516;&#21018;&#24230;&#21644;&#34920;&#38754;&#25705;&#25830;&#30340;&#28023;&#32501;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#25830;&#25325;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#31034;&#33539;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#29289;&#29702;&#29305;&#24615;&#21644;&#29983;&#25104;&#25152;&#38656;&#25830;&#25325;&#21160;&#20316;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many contact-rich tasks, force sensing plays an essential role in adapting the motion to the physical properties of the manipulated object. To enable robots to capture the underlying distribution of object properties necessary for generalising learnt manipulation tasks to unseen objects, existing Learning from Demonstration (LfD) approaches require a large number of costly human demonstrations. Our proposed semi-supervised LfD approach decouples the learnt model into an haptic representation encoder and a motion generation decoder. This enables us to pre-train the first using large amount of unsupervised data, easily accessible, while using few-shot LfD to train the second, leveraging the benefits of learning skills from humans. We validate the approach on the wiping task using sponges with different stiffness and surface friction. Our results demonstrate that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wipin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27010;&#29575;&#23433;&#20840;&#21306;&#22495;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#19968;&#20010;&#36755;&#20837;&#31354;&#38388;&#23376;&#38598;&#65292;&#22312;&#36825;&#20010;&#23376;&#38598;&#20013;&#65292;&#35823;&#20998;&#31867;&#23454;&#20363;&#30340;&#25968;&#37327;&#21487;&#20197;&#34987;&#27010;&#29575;&#19978;&#24471;&#21040;&#25511;&#21046;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#21487;&#20280;&#32553;&#20998;&#31867;&#22120;&#26469;&#23558;&#26426;&#22120;&#23398;&#20064;&#30340;&#35843;&#21442;&#19982;&#35823;&#24046;&#25511;&#21046;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.04627</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#20280;&#32553;&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#23433;&#20840;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Safety Regions Via Finite Families of Scalable Classifiers. (arXiv:2309.04627v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#27010;&#29575;&#23433;&#20840;&#21306;&#22495;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#25551;&#36848;&#19968;&#20010;&#36755;&#20837;&#31354;&#38388;&#23376;&#38598;&#65292;&#22312;&#36825;&#20010;&#23376;&#38598;&#20013;&#65292;&#35823;&#20998;&#31867;&#23454;&#20363;&#30340;&#25968;&#37327;&#21487;&#20197;&#34987;&#27010;&#29575;&#19978;&#24471;&#21040;&#25511;&#21046;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#21487;&#20280;&#32553;&#20998;&#31867;&#22120;&#26469;&#23558;&#26426;&#22120;&#23398;&#20064;&#30340;&#35843;&#21442;&#19982;&#35823;&#24046;&#25511;&#21046;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#20998;&#31867;&#21487;&#20197;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#20197;&#20998;&#31163;&#19981;&#21516;&#30340;&#34892;&#20026;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#20540;&#36924;&#36817;&#24615;&#36136;&#20915;&#23450;&#20102;&#20998;&#31867;&#31639;&#27861;&#19978;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;&#25968;&#25454;&#20998;&#26512;&#24072;&#21487;&#33021;&#20250;&#36890;&#36807;&#20943;&#23567;&#26576;&#20010;&#31867;&#21035;&#30340;&#38169;&#35823;&#26469;&#22686;&#21152;&#20854;&#20182;&#31867;&#21035;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#35745;&#38454;&#27573;&#30340;&#35823;&#24046;&#25511;&#21046;&#36890;&#24120;&#20197;&#21551;&#21457;&#24335;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21457;&#23637;&#19968;&#31181;&#29702;&#35770;&#22522;&#30784;&#65292;&#33021;&#22815;&#23545;&#33719;&#24471;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#35777;&#26126;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27010;&#29575;&#23433;&#20840;&#21306;&#22495;&#30340;&#27010;&#24565;&#65292;&#29992;&#26469;&#25551;&#36848;&#19968;&#20010;&#36755;&#20837;&#31354;&#38388;&#23376;&#38598;&#65292;&#20854;&#20013;&#35823;&#20998;&#31867;&#23454;&#20363;&#30340;&#25968;&#37327;&#21487;&#20197;&#27010;&#29575;&#19978;&#24471;&#21040;&#25511;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#20280;&#32553;&#20998;&#31867;&#22120;&#26469;&#23558;&#26426;&#22120;&#23398;&#20064;&#30340;&#35843;&#21442;&#19982;&#35823;&#24046;&#25511;&#21046;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#25552;&#20379;&#20102;&#22810;&#31181;&#27979;&#35797;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#20197;&#31361;&#20986;&#25152;&#26377;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised classification recognizes patterns in the data to separate classes of behaviours. Canonical solutions contain misclassification errors that are intrinsic to the numerical approximating nature of machine learning. The data analyst may minimize the classification error on a class at the expense of increasing the error of the other classes. The error control of such a design phase is often done in a heuristic manner. In this context, it is key to develop theoretical foundations capable of providing probabilistic certifications to the obtained classifiers. In this perspective, we introduce the concept of probabilistic safety region to describe a subset of the input space in which the number of misclassified instances is probabilistically controlled. The notion of scalable classifiers is then exploited to link the tuning of machine learning with error control. Several tests corroborate the approach. They are provided through synthetic data in order to highlight all the steps invo
&lt;/p&gt;</description></item><item><title>&#24863;&#30693;&#35843;&#25972;&#26597;&#35810;&#65288;PAQ&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#30340;&#26597;&#35810;&#26426;&#21046;&#65292;&#37319;&#29992;&#21453;&#21521;&#27979;&#37327;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#22522;&#25968;&#26597;&#35810;&#21644;&#24207;&#25968;&#26597;&#35810;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;PAQ&#24212;&#29992;&#20110;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;PAQ&#27979;&#37327;&#26469;&#23398;&#20064;&#26410;&#30693;&#30340;&#39532;&#27663;&#36317;&#31163;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.04626</link><description>&lt;p&gt;
&#20302;&#31209;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#24863;&#30693;&#35843;&#25972;&#26597;&#35810;&#21644;&#21453;&#21521;&#27979;&#37327;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Perceptual adjustment queries and an inverted measurement paradigm for low-rank metric learning. (arXiv:2309.04626v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04626
&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#35843;&#25972;&#26597;&#35810;&#65288;PAQ&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#30340;&#26597;&#35810;&#26426;&#21046;&#65292;&#37319;&#29992;&#21453;&#21521;&#27979;&#37327;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#22522;&#25968;&#26597;&#35810;&#21644;&#24207;&#25968;&#26597;&#35810;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;PAQ&#24212;&#29992;&#20110;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;PAQ&#27979;&#37327;&#26469;&#23398;&#20064;&#26410;&#30693;&#30340;&#39532;&#27663;&#36317;&#31163;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#30340;&#26597;&#35810;&#26426;&#21046;&#65292;&#31216;&#20026;&#24863;&#30693;&#35843;&#25972;&#26597;&#35810;&#65288;PAQ&#65289;&#12290;PAQ&#37319;&#29992;&#20102;&#21453;&#21521;&#27979;&#37327;&#26041;&#26696;&#65292;&#26082;&#20855;&#26377;&#20449;&#24687;&#37327;&#21448;&#36731;&#37327;&#32423;&#65292;&#32467;&#21512;&#20102;&#22522;&#25968;&#26597;&#35810;&#21644;&#24207;&#25968;&#26597;&#35810;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#23558;PAQ&#23637;&#31034;&#22312;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21033;&#29992;PAQ&#27979;&#37327;&#26469;&#23398;&#20064;&#26410;&#30693;&#30340;&#39532;&#27663;&#36317;&#31163;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#39640;&#32500;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#65292;&#26080;&#27861;&#24212;&#29992;&#26631;&#20934;&#30697;&#38453;&#20272;&#35745;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20174;PAQ&#20013;&#23398;&#20064;&#24230;&#37327;&#30340;&#20004;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#20272;&#35745;&#22120;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#23637;&#31034;&#20102;&#35813;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#21644;&#26174;&#33879;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new type of query mechanism for collecting human feedback, called the perceptual adjustment query ( PAQ). Being both informative and cognitively lightweight, the PAQ adopts an inverted measurement scheme, and combines advantages from both cardinal and ordinal queries. We showcase the PAQ in the metric learning problem, where we collect PAQ measurements to learn an unknown Mahalanobis distance. This gives rise to a high-dimensional, low-rank matrix estimation problem to which standard matrix estimators cannot be applied. Consequently, we develop a two-stage estimator for metric learning from PAQs, and provide sample complexity guarantees for this estimator. We present numerical simulations demonstrating the performance of the estimator and its notable properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;KDDT&#65292;&#29992;&#20110;&#21015;&#36710;&#25511;&#21046;&#21644;&#31649;&#29702;&#31995;&#32479;&#65288;TCMS&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;KDDT&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20998;&#21035;&#25552;&#21462;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#39034;&#24207;&#29305;&#24449;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#20016;&#23500;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;KDDT&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#20998;&#25968;0.931&#21644;0.91&#12290;</title><link>http://arxiv.org/abs/2309.04616</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation-Empowered Digital Twin for Anomaly Detection. (arXiv:2309.04616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;KDDT&#65292;&#29992;&#20110;&#21015;&#36710;&#25511;&#21046;&#21644;&#31649;&#29702;&#31995;&#32479;&#65288;TCMS&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;KDDT&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#20998;&#21035;&#25552;&#21462;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#39034;&#24207;&#29305;&#24449;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#20016;&#23500;&#25968;&#25454;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;KDDT&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#20998;&#25968;0.931&#21644;0.91&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#31995;&#32479;&#65292;&#22312;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#20013;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#22914;&#21015;&#36710;&#25511;&#21046;&#21644;&#31649;&#29702;&#31995;&#32479;&#65288;TCMS&#65289;&#12290;&#20316;&#20026;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#65292;&#30830;&#20445;&#20854;&#22312;&#25805;&#20316;&#36807;&#31243;&#20013;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25968;&#23383;&#23402;&#29983;&#65288;DTs&#65289;&#30001;&#20110;&#20855;&#26377;&#36816;&#34892;&#26102;&#30417;&#25511;&#21644;&#35686;&#21578;&#12289;&#24322;&#24120;&#39044;&#27979;&#21644;&#26816;&#27979;&#31561;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;TCMS&#20013;&#26500;&#24314;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#38656;&#35201;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#25552;&#21462;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#26102;&#38388;&#39034;&#24207;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KDDT&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;TCMS&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;KDDT&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#20998;&#21035;&#25552;&#21462;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#39034;&#24207;&#29305;&#24449;&#12290;&#20026;&#20102;&#22686;&#21152;&#25968;&#25454;&#37327;&#65292;KDDT&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#39046;&#22495;&#22806;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#24037;&#19994;&#21512;&#20316;&#20249;&#20276;&#38463;&#23572;&#26031;&#36890;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;KDDT&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#20102;0.931&#21644;0.91&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cyber-physical systems (CPSs), like train control and management systems (TCMS), are becoming ubiquitous in critical infrastructures. As safety-critical systems, ensuring their dependability during operation is crucial. Digital twins (DTs) have been increasingly studied for this purpose owing to their capability of runtime monitoring and warning, prediction and detection of anomalies, etc. However, constructing a DT for anomaly detection in TCMS necessitates sufficient training data and extracting both chronological and context features with high quality. Hence, in this paper, we propose a novel method named KDDT for TCMS anomaly detection. KDDT harnesses a language model (LM) and a long short-term memory (LSTM) network to extract contexts and chronological features, respectively. To enrich data volume, KDDT benefits from out-of-domain data with knowledge distillation (KD). We evaluated KDDT with two datasets from our industry partner Alstom and obtained the F1 scores of 0.931 and 0.91
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.04615</link><description>&lt;p&gt;
&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#20998;&#35299;&#22312;&#22522;&#20110;&#20540;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning. (arXiv:2309.04615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;Value Decomposition Framework with Disentangled World Model&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#30456;&#21516;&#29615;&#22659;&#20013;&#22810;&#20010;&#26234;&#33021;&#20307;&#36798;&#25104;&#20849;&#21516;&#30446;&#26631;&#26102;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#26080;&#27169;&#22411;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21253;&#25324;&#21160;&#20316;&#26465;&#20214;&#12289;&#26080;&#21160;&#20316;&#21644;&#38745;&#24577;&#20998;&#25903;&#65292;&#26469;&#35299;&#24320;&#29615;&#22659;&#21160;&#24577;&#24182;&#26681;&#25454;&#36807;&#21435;&#30340;&#32463;&#39564;&#20135;&#29983;&#24819;&#35937;&#20013;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#20174;&#30495;&#23454;&#29615;&#22659;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#19990;&#30028;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#23558;&#20854;&#19982;&#22522;&#20110;&#20540;&#30340;&#26694;&#26550;&#21512;&#24182;&#65292;&#20197;&#39044;&#27979;&#32852;&#21512;&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#24182;&#20248;&#21270;&#25972;&#20307;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20379;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model to address the challenge of achieving a common goal of multiple agents interacting in the same environment with reduced sample complexity. Due to scalability and non-stationarity problems posed by multi-agent systems, model-free methods rely on a considerable number of samples for training. In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment. We employ variational auto-encoders and variational graph auto-encoders to learn the latent representations for the world model, which is merged with a value-based framework to predict the joint action-value function and optimize the overall training objective. We present experimental results 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31867;&#21704;&#24076;&#34920;&#31034;&#21644;&#20998;&#23618;&#24378;&#21270;&#20132;&#21449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31995;&#32479;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#24847;&#20041;&#12289;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.04612</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#31867;&#21704;&#24076;&#34920;&#31034;&#21644;&#20998;&#23618;&#24378;&#21270;&#20132;&#21449;&#36827;&#34892;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing. (arXiv:2309.04612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04612
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31867;&#21704;&#24076;&#34920;&#31034;&#21644;&#20998;&#23618;&#24378;&#21270;&#20132;&#21449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31995;&#32479;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#24847;&#20041;&#12289;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#26032;&#30340;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#21306;&#20998;&#24230;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#24403;&#29983;&#25104;&#30340;&#29305;&#24449;&#26469;&#33258;&#20855;&#26377;&#22266;&#26377;&#29305;&#24449;&#20132;&#20114;&#30340;&#29305;&#24449;&#23545;&#26102;&#65292;&#29983;&#25104;&#30340;&#29305;&#24449;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#32463;&#39564;&#20016;&#23500;&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#21487;&#20197;&#35782;&#21035;&#20986;&#28508;&#22312;&#26377;&#29992;&#30340;&#29305;&#24449;-&#29305;&#24449;&#20132;&#20114;&#65292;&#24182;&#20174;&#25351;&#25968;&#32423;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#32500;&#24230;&#65292;&#22312;&#26368;&#20248;&#30340;&#29983;&#25104;&#36335;&#24452;&#19978;&#20197;&#26368;&#20248;&#30340;&#20132;&#21449;&#24418;&#24335;&#12290;&#20294;&#26159;&#65292;&#26426;&#22120;&#30340;&#20154;&#31867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#23558;&#36825;&#31867;&#23398;&#20064;&#20219;&#21153;&#27010;&#25324;&#20026;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;&#12290;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;&#23545;&#29616;&#26377;&#31995;&#32479;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65306;&#26377;&#24847;&#20041;&#12289;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#21644;&#36890;&#29992;&#30340;&#34920;&#31034;&#20132;&#21449;&#26694;&#26550;&#26469;&#35299;&#20915;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;&#12290;&#20026;&#20102;&#23454;&#29616;&#21704;&#24076;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#27861;&#65306;&#29305;&#24449;&#31163;&#25955;&#21270;&#12289;&#29305;&#24449;&#21704;&#24076;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Feature generation aims to generate new and meaningful features to create a discriminative representation space.A generated feature is meaningful when the generated feature is from a feature pair with inherent feature interaction. In the real world, experienced data scientists can identify potentially useful feature-feature interactions, and generate meaningful dimensions from an exponentially large search space, in an optimal crossing form over an optimal generation path. But, machines have limited human-like abilities.We generalize such learning tasks as self-optimizing feature generation. Self-optimizing feature generation imposes several under-addressed challenges on existing systems: meaningful, robust, and efficient generation. To tackle these challenges, we propose a principled and generic representation-crossing framework to solve self-optimizing feature generation.To achieve hashing representation, we propose a three-step approach: feature discretization, feature hashing, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#27169;&#24335;&#24863;&#30693;&#30340;&#23646;&#24615;&#23631;&#34109;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#37051;&#27169;&#24335;&#20013;&#30340;&#21407;&#23376;&#20449;&#24687;&#26469;&#25429;&#25417;&#27169;&#24335;&#38388;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#23376;&#22270;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04589</link><description>&lt;p&gt;
&#38754;&#21521;&#20998;&#23376;&#22270;&#39044;&#35757;&#32451;&#30340;&#27169;&#24335;&#24863;&#30693;&#23646;&#24615;&#23631;&#34109;
&lt;/p&gt;
&lt;p&gt;
Motif-aware Attribute Masking for Molecular Graph Pre-training. (arXiv:2309.04589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#27169;&#24335;&#24863;&#30693;&#30340;&#23646;&#24615;&#23631;&#34109;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#37051;&#27169;&#24335;&#20013;&#30340;&#21407;&#23376;&#20449;&#24687;&#26469;&#25429;&#25417;&#27169;&#24335;&#38388;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#23376;&#22270;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;&#65292;&#23646;&#24615;&#37325;&#26500;&#29992;&#20110;&#39044;&#27979;&#33410;&#28857;&#25110;&#36793;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#32473;&#23450;&#22823;&#37327;&#30340;&#20998;&#23376;&#65292;&#23427;&#20204;&#23398;&#20064;&#25429;&#25417;&#32467;&#26500;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#21508;&#31181;&#19979;&#28216;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#22312;&#21270;&#23398;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#31574;&#30053;&#26159;&#38543;&#26426;&#36873;&#25321;&#33410;&#28857;&#36827;&#34892;&#23646;&#24615;&#23631;&#34109;&#65292;&#21033;&#29992;&#23616;&#37096;&#37051;&#23621;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#37051;&#23621;&#30340;&#36807;&#24230;&#20381;&#36182;&#25233;&#21046;&#20102;&#27169;&#22411;&#20174;&#26356;&#39640;&#32423;&#30340;&#20122;&#32467;&#26500;&#20013;&#23398;&#20064;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#20174;&#39044;&#27979;&#33519;&#29615;&#20013;&#30340;&#19977;&#20010;&#30899;&#21407;&#23376;&#20013;&#23398;&#21040;&#30340;&#20449;&#24687;&#24456;&#23569;&#65292;&#20294;&#26159;&#21487;&#20197;&#20174;&#21151;&#33021;&#22522;&#22242;&#20043;&#38388;&#30340;&#30456;&#20114;&#36830;&#25509;&#20013;&#23398;&#21040;&#26356;&#22810;&#20449;&#24687;&#65292;&#20063;&#21487;&#20197;&#31216;&#20026;&#21270;&#23398;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#27169;&#24335;&#24863;&#30693;&#30340;&#23646;&#24615;&#23631;&#34109;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#37051;&#27169;&#24335;&#20013;&#30340;&#21407;&#23376;&#20449;&#24687;&#26469;&#25429;&#25417;&#27169;&#24335;&#38388;&#30340;&#32467;&#26500;&#12290;&#19968;&#26086;&#27599;&#20010;&#22270;&#34987;&#20998;&#35299;&#20026;&#19981;&#30456;&#20132;&#30340;
&lt;/p&gt;
&lt;p&gt;
Attribute reconstruction is used to predict node or edge features in the pre-training of graph neural networks. Given a large number of molecules, they learn to capture structural knowledge, which is transferable for various downstream property prediction tasks and vital in chemistry, biomedicine, and material science. Previous strategies that randomly select nodes to do attribute masking leverage the information of local neighbors However, the over-reliance of these neighbors inhibits the model's ability to learn from higher-level substructures. For example, the model would learn little from predicting three carbon atoms in a benzene ring based on the other three but could learn more from the inter-connections between the functional groups, or called chemical motifs. In this work, we propose and investigate motif-aware attribute masking strategies to capture inter-motif structures by leveraging the information of atoms in neighboring motifs. Once each graph is decomposed into disjoint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22810;&#36793;&#24418;&#32593;&#26684;&#19982;&#31070;&#32463;&#36752;&#23556;&#22330; (NeRF) &#30456;&#20114;&#32806;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#28210;&#26579;&#21644;&#27169;&#25311;&#36807;&#31243;&#20013;&#23545;&#32593;&#26684;&#36827;&#34892;&#29289;&#29702;&#19968;&#33268;&#24615;&#22788;&#29702;&#12290;&#35757;&#32451;&#20102;&#20351;&#29992;&#39640;&#21160;&#24577;&#33539;&#22260;&#22270;&#20687;&#30340; NeRF&#65292;&#24182;&#25552;&#20986;&#20102;&#20272;&#35745;&#20809;&#28304;&#21644;&#25237;&#23556;&#38452;&#24433;&#30340;&#31574;&#30053;&#12290;&#21516;&#26102;&#32771;&#34385;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#38598;&#25104;&#28151;&#21512;&#34920;&#38754;-&#20307;&#31215;&#34920;&#36798;&#24335;&#19982;&#22270;&#24418;&#28210;&#26579;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.04581</link><description>&lt;p&gt;
&#21160;&#24577;&#32593;&#26684;&#24863;&#30693;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Dynamic Mesh-Aware Radiance Fields. (arXiv:2309.04581v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22810;&#36793;&#24418;&#32593;&#26684;&#19982;&#31070;&#32463;&#36752;&#23556;&#22330; (NeRF) &#30456;&#20114;&#32806;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#28210;&#26579;&#21644;&#27169;&#25311;&#36807;&#31243;&#20013;&#23545;&#32593;&#26684;&#36827;&#34892;&#29289;&#29702;&#19968;&#33268;&#24615;&#22788;&#29702;&#12290;&#35757;&#32451;&#20102;&#20351;&#29992;&#39640;&#21160;&#24577;&#33539;&#22260;&#22270;&#20687;&#30340; NeRF&#65292;&#24182;&#25552;&#20986;&#20102;&#20272;&#35745;&#20809;&#28304;&#21644;&#25237;&#23556;&#38452;&#24433;&#30340;&#31574;&#30053;&#12290;&#21516;&#26102;&#32771;&#34385;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#38598;&#25104;&#28151;&#21512;&#34920;&#38754;-&#20307;&#31215;&#34920;&#36798;&#24335;&#19982;&#22270;&#24418;&#28210;&#26579;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23558;&#22810;&#36793;&#24418;&#32593;&#26684;&#36164;&#20135;&#23884;&#20837;&#21040;&#36924;&#30495;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330; (NeRF) &#20307;&#31215;&#20013;&#65292;&#20197;&#20415;&#20197;&#19982; NeRF &#29289;&#29702;&#19968;&#33268;&#30340;&#26041;&#24335;&#36827;&#34892;&#28210;&#26579;&#21644;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#31995;&#32479;&#35270;&#35282;&#12290;&#35774;&#35745;&#20102;&#22312;&#28210;&#26579;&#21644;&#27169;&#25311;&#36807;&#31243;&#20013;&#32593;&#26684;&#19982; NeRF &#20043;&#38388;&#30340;&#21452;&#21521;&#32806;&#21512;&#12290;&#39318;&#20808;&#22238;&#39038;&#20102;&#32593;&#26684;&#21644; NeRF &#30340;&#20809;&#20256;&#36755;&#26041;&#31243;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#31181;&#27839;&#30528;&#20219;&#24847;&#21453;&#23556;&#27425;&#25968;&#30340;&#20809;&#32447;&#26356;&#26032;&#36752;&#23556;&#21644;&#36879;&#36807;&#29575;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36335;&#24452;&#36861;&#36394;&#22120;&#25152;&#20551;&#35774;&#30340;&#32447;&#24615;&#33394;&#24425;&#31354;&#38388;&#21644;&#26631;&#20934; NeRF &#20351;&#29992;&#30340; sRGB &#33394;&#24425;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#21160;&#24577;&#33539;&#22260; (HDR) &#22270;&#20687;&#35757;&#32451;&#20102; NeRF&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745; NeRF &#19978;&#30340;&#20809;&#28304;&#24182;&#25237;&#23556;&#38452;&#24433;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#32771;&#34385;&#20102;&#22914;&#20309;&#23558;&#28151;&#21512;&#34920;&#38754;-&#20307;&#31215;&#34920;&#36798;&#24335;&#19982;&#39640;&#24615;&#33021;&#30340;&#22270;&#24418;&#28210;&#26579;&#26694;&#26550;&#39640;&#25928;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding polygonal mesh assets within photorealistic Neural Radience Fields (NeRF) volumes, such that they can be rendered and their dynamics simulated in a physically consistent manner with the NeRF, is under-explored from the system perspective of integrating NeRF into the traditional graphics pipeline. This paper designs a two-way coupling between mesh and NeRF during rendering and simulation. We first review the light transport equations for both mesh and NeRF, then distill them into an efficient algorithm for updating radiance and throughput along a cast ray with an arbitrary number of bounces. To resolve the discrepancy between the linear color space that the path tracer assumes and the sRGB color space that standard NeRF uses, we train NeRF with High Dynamic Range (HDR) images. We also present a strategy to estimate light sources and cast shadows on the NeRF. Finally, we consider how the hybrid surface-volumetric formulation can be efficiently integrated with a high-performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.04565</link><description>&lt;p&gt;
&#35299;&#25918;&#22270;&#23398;&#20064;&#30340;&#21147;&#37327;&#65306;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Graph Learning through LLM-based Autonomous Agents. (arXiv:2309.04565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#20197;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#23384;&#22312;&#21644;&#24212;&#29992;&#65292;&#20294;&#26377;&#25928;&#22320;&#22788;&#29702;&#36825;&#20123;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#21644;&#22312;&#22270;&#19978;&#36827;&#34892;&#23398;&#20064;&#20219;&#21153;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#38754;&#23545;&#22797;&#26434;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#19987;&#23478;&#20204;&#22312;&#36817;&#24180;&#26469;&#35774;&#35745;&#20102;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#20182;&#20204;&#36824;&#23454;&#26045;&#20102;&#22270;&#20013;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65292;&#20063;&#31216;&#20026;AutoGraph&#65292;&#20197;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20182;&#20204;&#22312;&#20197;&#19979;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#31649;&#29702;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#65292;&#65288;2&#65289;&#22788;&#29702;&#22270;&#23398;&#20064;&#20013;&#19981;&#21516;&#30340;&#27969;&#31243;&#65288;&#36229;&#36807;&#26550;&#26500;&#35774;&#35745;&#65289;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20351;&#29992;AutoGraph&#26102;&#23545;&#20808;&#39564;&#30693;&#35782;&#30340;&#24040;&#22823;&#38656;&#27714;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#26469;&#31616;&#21270;&#22810;&#26679;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#38024;&#23545;&#29992;&#25143;&#35831;&#27714;&#65288;&#35813;&#35831;&#27714;&#21487;&#33021;&#21253;&#21547;&#33410;&#28857;&#12289;&#36793;&#32536;&#25110;&#22270;&#32423;&#21035;&#30340;&#19981;&#21516;&#25968;&#25454;&#21644;&#23398;&#20064;&#30446;&#26631;&#65289;&#65292;&#22797;&#26434;&#22270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#23558;&#30001;LLM&#33258;&#20027;&#20195;&#29702;&#26426;&#21046;&#26469;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph structured data are widely existed and applied in the real-world applications, while it is a challenge to handling these diverse data and learning tasks on graph in an efficient manner. When facing the complicated graph learning tasks, experts have designed diverse Graph Neural Networks (GNNs) in recent years. They have also implemented AutoML in Graph, also known as AutoGraph, to automatically generate data-specific solutions. Despite their success, they encounter limitations in (1) managing diverse learning tasks at various levels, (2) dealing with different procedures in graph learning beyond architecture design, and (3) the huge requirements on the prior knowledge when using AutoGraph. In this paper, we propose to use Large Language Models (LLMs) as autonomous agents to simplify the learning process on diverse real-world graphs. Specifically, in response to a user request which may contain varying data and learning targets at the node, edge, or graph levels, the complex graph
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#31350;&#20102;&#25968;&#25454;&#20462;&#21098;&#23545;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#22120;&#21644;&#20462;&#21098;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21518;&#35757;&#32451;&#30340;LLMs&#65292;&#20182;&#20204;&#21457;&#29616;&#22256;&#24785;&#24230;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#20248;&#20110;&#26356;&#21152;&#35745;&#31639;&#23494;&#38598;&#30340;&#35780;&#20998;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.04564</link><description>&lt;p&gt;
&#24403;&#23569;&#23601;&#24847;&#21619;&#30528;&#26356;&#22810;&#65306;&#25506;&#31350;&#25968;&#25454;&#20462;&#21098;&#23545;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;( LLMS )&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale. (arXiv:2309.04564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04564
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#31350;&#20102;&#25968;&#25454;&#20462;&#21098;&#23545;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#27604;&#36739;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#22120;&#21644;&#20462;&#21098;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21518;&#35757;&#32451;&#30340;LLMs&#65292;&#20182;&#20204;&#21457;&#29616;&#22256;&#24785;&#24230;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#20248;&#20110;&#26356;&#21152;&#35745;&#31639;&#23494;&#38598;&#30340;&#35780;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22823;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMS )&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#36890;&#36807;&#20174;&#20114;&#32852;&#32593;&#19978;&#25235;&#21462;&#33719;&#21462;&#65292;&#23548;&#33268;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30001;&#22024;&#26434;&#30340;&#32593;&#32476;&#25991;&#26412;&#26500;&#25104;&#12290;&#36807;&#21435;&#65292;&#20026;&#20102;&#20943;&#23567;&#25968;&#25454;&#38598;&#24182;&#20351;&#20854;&#26356;&#39640;&#36136;&#37327;&#65292;&#37319;&#29992;&#20102;&#20197;&#35268;&#21017;&#20026;&#22522;&#30784;&#30340;&#25163;&#24037;&#21551;&#21457;&#24335;&#36807;&#28388;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#21487;&#20272;&#31639;&#30340;&#25968;&#25454;&#36136;&#37327;&#65292;&#20197;&#31995;&#32479;&#24615;&#22320;&#34913;&#37327;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27604;&#36739;&#65292;&#21253;&#25324;&#20351;&#29992;&#22256;&#24785;&#24230;&#30340;&#31616;&#21333;&#25968;&#25454;&#36136;&#37327;&#35780;&#20272;&#22120;&#65292;&#20197;&#21450;&#26356;&#22797;&#26434;&#21644;&#35745;&#31639;&#23494;&#38598;&#30340;&#38169;&#35823;L2-&#33539;&#25968;&#21644;&#35760;&#24518;&#21270;&#35780;&#20272;&#12290;&#36825;&#20123;&#25351;&#26631;&#29992;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#36827;&#34892;&#25490;&#24207;&#21644;&#20462;&#21098;&#65292;&#24182;&#38543;&#21518;&#27604;&#36739;&#22312;&#36825;&#20123;&#20462;&#21098;&#21518;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;LLMs&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22256;&#24785;&#24230;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#20248;&#20110;&#26356;&#21152;&#35745;&#31639;&#23494;&#38598;&#30340;&#35780;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large volumes of text data have contributed significantly to the development of large language models (LLMs) in recent years. This data is typically acquired by scraping the internet, leading to pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down to a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In this work, we take a wider view and explore scalable estimates of data quality that can be used to systematically measure the quality of pretraining data. We perform a rigorous comparison at scale of the simple data quality estimator of perplexity, as well as more sophisticated and computationally intensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and prune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets. Surprisingly, we find that the simple technique of perplexity outperforms our more computationally expensive scoring metho
&lt;/p&gt;</description></item><item><title>&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25913;&#36827;&#20102;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#8805;M1.0&#32423;&#22826;&#38451;&#32768;&#26001;&#30340;&#21457;&#29983;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.04558</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Solar Flare Prediction with Attention-based Deep Neural Networks. (arXiv:2309.04558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04558
&lt;/p&gt;
&lt;p&gt;
&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25913;&#36827;&#20102;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#8805;M1.0&#32423;&#22826;&#38451;&#32768;&#26001;&#30340;&#21457;&#29983;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#22270;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#32768;&#26001;&#30340;&#39044;&#27979;&#26159;&#31354;&#38388;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#21152;&#36895;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22797;&#26434;&#27169;&#22411;&#22312;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20316;&#20026;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27969;&#27700;&#32447;&#30340;&#25913;&#36827;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;&#25509;&#19979;&#26469;&#30340;24&#23567;&#26102;&#20869;&#26159;&#21542;&#20250;&#20986;&#29616;&#8805;M1.0&#32423;&#22826;&#38451;&#32768;&#26001;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#30001;&#20840;&#30424;&#30596;&#32447;&#65288;LoS&#65289;&#30913;&#22270;&#21019;&#24314;&#30340;&#21387;&#32553;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#36807;&#37319;&#26679;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25216;&#33021;&#32479;&#35745;&#65288;TSS&#65289;&#21644;Heidke&#25216;&#33021;&#35780;&#20998;&#65288;HSS&#65289;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#36755;&#20837;&#30913;&#22270;&#19978;&#21472;&#21152;&#27880;&#24847;&#21147;&#22270;&#26469;&#35299;&#37322;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#21487;&#35270;&#21270;&#27169;&#22411;&#20851;&#27880;&#30340;&#37325;&#35201;&#21306;&#22495;&#65292;&#20174;&#32780;&#23548;&#33268;&#26368;&#32456;&#30340;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#26377;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25104;&#21151;&#22320;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#22826;&#38451;&#32768;&#26001;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solar flare prediction is a central problem in space weather forecasting and recent developments in machine learning and deep learning accelerated the adoption of complex models for data-driven solar flare forecasting. In this work, we developed an attention-based deep learning model as an improvement over the standard convolutional neural network (CNN) pipeline to perform full-disk binary flare predictions for the occurrence of $\geq$M1.0-class flares within the next 24 hours. For this task, we collected compressed images created from full-disk line-of-sight (LoS) magnetograms. We used data-augmented oversampling to address the class imbalance issue and used true skill statistic (TSS) and Heidke skill score (HSS) as the evaluation metrics. Furthermore, we interpreted our model by overlaying attention maps on input magnetograms and visualized the important regions focused on by the model that led to the eventual decision. The significant findings of this study are: (i) We successfully 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36951;&#25022;&#26368;&#20248;&#31639;&#27861;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#26680;&#22238;&#24402;&#27169;&#22411;&#20013;&#20855;&#20307;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#36951;&#25022;&#26368;&#20248;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21487;&#20197;&#20943;&#23567;&#29983;&#25104;&#21442;&#25968;&#19982;&#19987;&#38376;&#21442;&#25968;&#20043;&#38388;&#30340;&#32047;&#31215;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.04557</link><description>&lt;p&gt;
&#29992;&#20110;&#26680;&#22238;&#24402;&#30340;&#36951;&#25022;&#26368;&#20248;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#21450;&#20854;&#22312;&#32654;&#24335;&#26399;&#26435;&#23450;&#20215;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing. (arXiv:2309.04557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36951;&#25022;&#26368;&#20248;&#31639;&#27861;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#26680;&#22238;&#24402;&#27169;&#22411;&#20013;&#20855;&#20307;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#36951;&#25022;&#26368;&#20248;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21487;&#20197;&#20943;&#23567;&#29983;&#25104;&#21442;&#25968;&#19982;&#19987;&#38376;&#21442;&#25968;&#20043;&#38388;&#30340;&#32047;&#31215;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65292;&#20854;&#20013;&#20013;&#24515;&#35745;&#21010;&#32773;&#21487;&#20197;&#35775;&#38382;&#21516;&#19968;&#23398;&#20064;&#27169;&#22411; $f_{\theta}$ &#30340;&#25968;&#25454;&#38598; ${\cal D}_1,\dots,{\cal D}_N$&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#23562;&#37325;&#27169;&#22411; $f_{\theta(T)}$ &#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#37327;&#20943;&#23567;&#29983;&#25104;&#21442;&#25968; $\{\theta_i(t)\}_{t=0}^T$ &#22312;&#25152;&#26377; $T$ &#27425;&#36845;&#20195;&#20013;&#19982;&#27599;&#20010;&#25968;&#25454;&#38598;&#24471;&#21040;&#30340;&#19987;&#38376;&#21442;&#25968;$\theta^\star_{1},\ldots,\theta^\star_N$ &#30340;&#32047;&#31215;&#20559;&#24046;&#12290;&#25105;&#20204;&#20165;&#20801;&#35768;&#27599;&#20010;&#19987;&#38376;&#27169;&#22411;&#65288;&#33410;&#28857;/&#20195;&#29702;&#65289;&#21644;&#20013;&#24515;&#35745;&#21010;&#32773;&#65288;&#26381;&#21153;&#22120;&#65289;&#22312;&#27599;&#27425;&#36845;&#20195;&#65288;&#36718;&#65289;&#20043;&#38388;&#36827;&#34892;&#25345;&#32493;&#36890;&#20449;&#12290;&#23545;&#20110;&#27169;&#22411; $f_{\theta}$ &#26159;&#26377;&#38480;&#31209;&#26680;&#22238;&#24402;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#36951;&#25022;&#26368;&#20248;&#31639;&#27861;&#30340;&#26174;&#24335;&#26356;&#26032;&#20844;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36951;&#25022;&#26368;&#20248;&#31639;&#27861;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#20960;&#20046;&#36951;&#25022;&#26368;&#20248;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#38656;&#35201;&#36739;&#23569;&#30340; $\mathcal{O}(Np^2)$ &#20010;&#22522;&#26412;&#36816;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an optimal iterative scheme for federated transfer learning, where a central planner has access to datasets ${\cal D}_1,\dots,{\cal D}_N$ for the same learning model $f_{\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\{\theta_i(t)\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\theta^\star_{1},\ldots,\theta^\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\mathcal{O}(Np^2)$ fewer elementary operati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#65292;&#29992;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2309.04522</link><description>&lt;p&gt;
&#36830;&#25509;NTK&#21644;NNGP&#65306;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#21147;&#23398;&#22312;&#26680;&#21306;&#22495;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime. (arXiv:2309.04522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#65292;&#29992;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#23398;&#20064;&#36807;&#31243;&#32570;&#20047;&#19968;&#20010;&#23436;&#25972;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#23545;&#20110;&#26080;&#38480;&#23485;&#24230;&#32593;&#32476;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#25551;&#36848;&#32593;&#32476;&#30340;&#36755;&#20986;&#65306;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#26694;&#26550;&#65292;&#20551;&#35774;&#20102;&#32447;&#24615;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#65307;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#19968;&#30452;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#65292;&#29992;&#20110;&#25551;&#36848;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21644;&#23398;&#20064;&#21518;&#30340;&#32593;&#32476;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#30340;&#31934;&#30830;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#30456;&#20851;&#30340;&#31070;&#32463;&#21160;&#24577;&#26680;&#65288;NDK&#65289;&#65292;&#36825;&#20010;&#26680;&#21487;&#20197;&#21516;&#26102;&#20135;&#29983;NTK&#21644;NNGP&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial progress has been made for infinitely wide networks. In this regime, two disparate theoretical frameworks have been used, in which the network's output is described using kernels: one framework is based on the Neural Tangent Kernel (NTK) which assumes linearized gradient descent dynamics, while the Neural Network Gaussian Process (NNGP) kernel assumes a Bayesian framework. However, the relation between these two frameworks has remained elusive. This work unifies these two distinct theories using a Markov proximal learning model for learning dynamics in an ensemble of randomly initialized infinitely wide deep networks. We derive an exact analytical expression for the network input-output function during and after learning, and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22768;&#23398;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#21644;&#28040;&#38500;&#35821;&#35328;&#36831;&#28382;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#19982;&#20004;&#38454;&#27573;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#29978;&#33267;&#36229;&#36234;&#12290;</title><link>http://arxiv.org/abs/2309.04516</link><description>&lt;p&gt;
&#21033;&#29992;&#22768;&#23398;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#21644;&#28040;&#38500;&#35821;&#35328;&#36831;&#28382;
&lt;/p&gt;
&lt;p&gt;
End-to-End Speech Recognition and Disfluency Removal with Acoustic Language Model Pretraining. (arXiv:2309.04516v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22768;&#23398;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#21644;&#28040;&#38500;&#35821;&#35328;&#36831;&#28382;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#19982;&#20004;&#38454;&#27573;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#19981;&#36830;&#36143;&#21644;&#23545;&#35805;&#35821;&#38899;&#30340;&#36716;&#24405;&#20013;&#65292;&#36817;&#24180;&#26469;&#65292;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#30340;&#27169;&#22411;&#65292;&#22312;&#36716;&#24405;&#21644;&#28165;&#29702;&#38454;&#27573;&#20043;&#38388;&#36827;&#34892;&#20998;&#31163;&#12290;&#25105;&#20204;&#35748;&#20026;&#20043;&#21069;&#30340;&#31471;&#21040;&#31471;&#28040;&#38500;&#35821;&#35328;&#36831;&#28382;&#30340;&#23581;&#35797;&#19981;&#22815;&#25104;&#21151;&#65292;&#26159;&#22240;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#32473;&#20104;&#20102;&#35789;&#27719;&#27169;&#22411;&#34920;&#31034;&#20248;&#21183;&#12290;&#30452;&#21040;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#30446;&#26631;&#23545;&#20110;&#23398;&#20064;&#26377;&#25928;&#38899;&#39057;&#34920;&#31034;&#30340;&#39640;&#32500;&#24230;&#21644;&#26377;&#38480;&#21487;&#29992;&#30340;&#22823;&#22411;&#38899;&#39057;&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#21463;&#21040;&#38480;&#21046;&#65292;&#20174;&#32780;&#32473;&#20004;&#38454;&#27573;&#26041;&#27861;&#24102;&#26469;&#20102;&#30456;&#23545;&#20248;&#21183;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#35789;&#27719;&#26631;&#35760;&#12290;&#22522;&#20110;&#26368;&#36817;&#22823;&#35268;&#27169;&#38899;&#39057;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#20004;&#38454;&#27573;&#21644;&#31471;&#21040;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#24369;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#24615;&#33021;&#19982;&#30456;&#20284;&#35757;&#32451;&#36807;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
The SOTA in transcription of disfluent and conversational speech has in recent years favored two-stage models, with separate transcription and cleaning stages. We believe that previous attempts at end-to-end disfluency removal have fallen short because of the representational advantage that large-scale language model pretraining has given to lexical models. Until recently, the high dimensionality and limited availability of large audio datasets inhibited the development of large-scale self-supervised pretraining objectives for learning effective audio representations, giving a relative advantage to the two-stage approach, which utilises pretrained representations for lexical tokens. In light of recent successes in large scale audio pretraining, we revisit the performance comparison between two-stage and end-to-end model and find that audio based language models pretrained using weak self-supervised objectives match or exceed the performance of similarly trained two-stage models, and fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21367;&#31215;&#21464;&#20998;&#29942;&#39048;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#21457;&#29616;&#20102;PRECODE&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#24433;&#21709;&#12290;PRECODE&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#26799;&#24230;&#38450;&#27490;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#25910;&#25947;&#65292;&#20294;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#26469;&#31105;&#29992;&#20854;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04515</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#21464;&#20998;&#29942;&#39048;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy Preserving Federated Learning with Convolutional Variational Bottlenecks. (arXiv:2309.04515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21367;&#31215;&#21464;&#20998;&#29942;&#39048;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#21457;&#29616;&#20102;PRECODE&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#24433;&#21709;&#12290;PRECODE&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#26799;&#24230;&#38450;&#27490;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#25910;&#25947;&#65292;&#20294;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#26469;&#31105;&#29992;&#20854;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#26159;&#32852;&#21512;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23041;&#32961;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#27844;&#28431;&#26469;&#37325;&#26500;&#26412;&#24212;&#26159;&#31169;&#23494;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#24314;&#27169;&#30340;&#38544;&#31169;&#22686;&#24378;&#27169;&#22359;&#65288;PRECODE&#65289;&#65292;&#20197;&#38450;&#27490;&#26799;&#24230;&#27844;&#28431;&#32780;&#19981;&#25439;&#22833;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#20294;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#20043;&#21069;&#65292;&#24050;&#32463;&#35777;&#26126;PRECODE&#25104;&#21151;&#38450;&#27490;&#20102;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#22810;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;PRECODE&#23545;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20197;&#25581;&#31034;&#20854;&#22522;&#26412;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21464;&#20998;&#24314;&#27169;&#23558;&#38543;&#26426;&#24615;&#24341;&#20837;&#20102;PRECODE&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#32493;&#23618;&#30340;&#26799;&#24230;&#20013;&#12290;&#36825;&#20123;&#23618;&#30340;&#38543;&#26426;&#26799;&#24230;&#38459;&#27490;&#20102;&#36845;&#20195;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#25910;&#25947;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#24847;&#22320;&#22312;&#25915;&#20987;&#20248;&#21270;&#36807;&#31243;&#20013;&#30465;&#30053;&#38543;&#26426;&#26799;&#24230;&#65292;&#26469;&#31105;&#29992;PRECODE&#30340;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient inversion attacks are an ubiquitous threat in federated learning as they exploit gradient leakage to reconstruct supposedly private training data. Recent work has proposed to prevent gradient leakage without loss of model utility by incorporating a PRivacy EnhanCing mODulE (PRECODE) based on variational modeling. Without further analysis, it was shown that PRECODE successfully protects against gradient inversion attacks. In this paper, we make multiple contributions. First, we investigate the effect of PRECODE on gradient inversion attacks to reveal its underlying working principle. We show that variational modeling introduces stochasticity into the gradients of PRECODE and the subsequent layers in a neural network. The stochastic gradients of these layers prevent iterative gradient inversion attacks from converging. Second, we formulate an attack that disables the privacy preserving effect of PRECODE by purposefully omitting stochastic gradients during attack optimization. To
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35760;&#24518;&#20462;&#21098;&#21644;&#26377;&#30028;&#20248;&#21270;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#20195;&#29702;&#27169;&#22411;&#21644;&#33719;&#21462;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.04510</link><description>&lt;p&gt;
&#20943;&#23569;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#35745;&#31639;&#26102;&#38388;&#65306;&#36890;&#29992;&#30340;&#35760;&#24518;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decreasing the Computing Time of Bayesian Optimization using Generalizable Memory Pruning. (arXiv:2309.04510v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04510
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#35760;&#24518;&#20462;&#21098;&#21644;&#26377;&#30028;&#20248;&#21270;&#65292;&#21487;&#20197;&#38477;&#20302;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#20195;&#29702;&#27169;&#22411;&#21644;&#33719;&#21462;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22788;&#29702;&#39640;&#32500;&#24230;&#25110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#25152;&#38656;&#30340;&#35745;&#31639;&#26102;&#38388;&#36739;&#38271;&#12290;&#36825;&#26159;&#30001;&#20110;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#19982;&#23454;&#39564;&#25968;&#37327;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#23548;&#33268;&#30340;&#12290;&#30001;&#20110;&#35745;&#31639;&#26102;&#38388;&#30340;&#36825;&#31181;&#22797;&#26434;&#24230;&#25193;&#23637;&#65292;&#36816;&#34892;BO&#22312;&#39640;&#32500;&#24230;&#25110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21464;&#24471;&#22256;&#38590;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23454;&#39564;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#26367;&#20195;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#38477;&#20302;BO&#36807;&#31243;&#30340;&#35745;&#31639;&#21033;&#29992;&#29575;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23545;&#32487;&#25215;&#30340;&#20195;&#29702;&#20989;&#25968;&#36827;&#34892;&#25968;&#23398;&#20462;&#25913;&#65292;&#38480;&#21046;&#20102;&#21482;&#33021;&#20351;&#29992;&#35813;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;BO&#21253;&#35013;&#22120;&#8212;&#8212;&#35760;&#24518;&#20462;&#21098;&#21644;&#26377;&#30028;&#20248;&#21270;&#65292;&#33021;&#22815;&#19982;&#20219;&#20309;&#20195;&#29702;&#27169;&#22411;&#21644;&#33719;&#21462;&#20989;&#25968;&#19968;&#36215;&#20351;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#35760;&#24518;&#20462;&#21098;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;BO&#27599;&#20010;&#23454;&#39564;&#30340;&#22681;&#38047;&#35745;&#31639;&#26102;&#38388;&#20174;&#22810;&#39033;&#24335;&#22686;&#21152;&#27169;&#24335;&#19979;&#38477;&#21040;&#38191;&#40831;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) suffers from long computing times when processing highly-dimensional or large data sets. These long computing times are a result of the Gaussian process surrogate model having a polynomial time complexity with the number of experiments. Running BO on high-dimensional or massive data sets becomes intractable due to this time complexity scaling, in turn, hindering experimentation. Alternative surrogate models have been developed to reduce the computing utilization of the BO procedure, however, these methods require mathematical alteration of the inherit surrogate function, pigeonholing use into only that function. In this paper, we demonstrate a generalizable BO wrapper of memory pruning and bounded optimization, capable of being used with any surrogate model and acquisition function. Using this memory pruning approach, we show a decrease in wall-clock computing times per experiment of BO from a polynomially increasing pattern to a sawtooth pattern that has a n
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22359;&#65292;&#36890;&#36807;&#34701;&#21512;&#20256;&#24863;&#22120;&#38453;&#21015;&#30340;&#25968;&#25454;&#26469;&#22686;&#24378;IoT&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#24179;&#21488;&#20013;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.04508</link><description>&lt;p&gt;
IoT&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#31995;&#32479;&#20013;&#30340;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#34701;&#21512;&#22120;&#29992;&#20110;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph Attention Fuser for Calibration in IoT Air Pollution Monitoring Systems. (arXiv:2309.04508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22359;&#65292;&#36890;&#36807;&#34701;&#21512;&#20256;&#24863;&#22120;&#38453;&#21015;&#30340;&#25968;&#25454;&#26469;&#22686;&#24378;IoT&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#24179;&#21488;&#20013;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;(IoT)&#20256;&#24863;&#22120;&#22312;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20302;&#25104;&#26412;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#36825;&#19968;&#36827;&#23637;&#65292;&#20934;&#30830;&#26657;&#20934;&#36825;&#20123;&#20256;&#24863;&#22120;&#22312;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#26465;&#20214;&#19979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22359;&#65292;&#36890;&#36807;&#34701;&#21512;&#20256;&#24863;&#22120;&#38453;&#21015;&#30340;&#25968;&#25454;&#26469;&#22686;&#24378;&#26657;&#20934;&#36807;&#31243;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#33879;&#25552;&#39640;IoT&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#24179;&#21488;&#20013;&#20256;&#24863;&#22120;&#30340;&#26657;&#20934;&#31934;&#24230;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Internet of Things (IoT) sensors for air pollution monitoring has significantly increased, resulting in the deployment of low-cost sensors. Despite this advancement, accurately calibrating these sensors in uncontrolled environmental conditions remains a challenge. To address this, we propose a novel approach that leverages graph neural networks, specifically the graph attention network module, to enhance the calibration process by fusing data from sensor arrays. Through our experiments, we demonstrate the effectiveness of our approach in significantly improving the calibration accuracy of sensors in IoT air pollution monitoring platforms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#36335;&#24452;&#31614;&#21517;&#26469;&#29983;&#25104;&#36924;&#36817;&#23454;&#38469;&#25968;&#25454;&#30340;&#37329;&#34701;&#20215;&#26684;&#36335;&#24452;&#65292;&#24182;&#24212;&#29992;&#20110;&#23450;&#20215;&#22238;&#25764;&#20445;&#38505;&#26399;&#26435;&#21644;&#25237;&#36164;&#32452;&#21512;&#22238;&#25764;&#25511;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.04507</link><description>&lt;p&gt;
&#20351;&#29992;&#36335;&#24452;&#31614;&#21517;&#29983;&#25104;&#36924;&#36817;&#23454;&#38469;&#25968;&#25454;&#30340;&#37329;&#34701;&#20215;&#26684;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Generating drawdown-realistic financial price paths using path signatures. (arXiv:2309.04507v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#36335;&#24452;&#31614;&#21517;&#26469;&#29983;&#25104;&#36924;&#36817;&#23454;&#38469;&#25968;&#25454;&#30340;&#37329;&#34701;&#20215;&#26684;&#36335;&#24452;&#65292;&#24182;&#24212;&#29992;&#20110;&#23450;&#20215;&#22238;&#25764;&#20445;&#38505;&#26399;&#26435;&#21644;&#25237;&#36164;&#32452;&#21512;&#22238;&#25764;&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#20855;&#26377;&#25509;&#36817;&#23454;&#38469;&#25968;&#25454;&#30340;&#22238;&#25764;&#30340;&#37329;&#34701;&#20215;&#26684;&#24207;&#21015;&#12290;&#24212;&#29992;&#22330;&#26223;&#22914;&#23450;&#20215;&#22238;&#25764;&#20445;&#38505;&#26399;&#26435;&#25110;&#24320;&#21457;&#25237;&#36164;&#32452;&#21512;&#22238;&#25764;&#25511;&#21046;&#31574;&#30053;&#38656;&#35201;&#20351;&#29992;&#25509;&#36817;&#30495;&#23454;&#30340;&#22238;&#25764;&#36335;&#24452;&#12290;&#21382;&#21490;&#24773;&#26223;&#21487;&#33021;&#19981;&#36275;&#20197;&#26377;&#25928;&#35757;&#32451;&#21644;&#22238;&#27979;&#31574;&#30053;&#65292;&#32780;&#26631;&#20934;&#30340;&#21442;&#25968;&#21270;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#20445;&#30041;&#22238;&#25764;&#12290;&#25105;&#20204;&#25552;&#20513;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#65292;&#23558;&#21464;&#20998;&#33258;&#32534;&#30721;&#29983;&#25104;&#27169;&#22411;&#19982;&#22238;&#25764;&#37325;&#24314;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#20811;&#26381;&#25968;&#20540;&#22797;&#26434;&#24615;&#21644;&#38750;&#21487;&#24494;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#22238;&#25764;&#36817;&#20284;&#20026;&#36335;&#24452;&#30340;&#30697;&#20989;&#25968;&#65292;&#21363;&#36335;&#24452;&#31614;&#21517;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22238;&#25764;&#20989;&#25968;&#30340;&#25152;&#38656;&#27491;&#21017;&#24615;&#21644;&#36817;&#20284;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#22238;&#24402;&#33719;&#24471;&#20102;&#25509;&#36817;&#30340;&#25968;&#20540;&#36817;&#20284;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel generative machine learning approach for the simulation of sequences of financial price data with drawdowns quantifiably close to empirical data is introduced. Applications such as pricing drawdown insurance options or developing portfolio drawdown control strategies call for a host of drawdown-realistic paths. Historical scenarios may be insufficient to effectively train and backtest the strategy, while standard parametric Monte Carlo does not adequately preserve drawdowns. We advocate a non-parametric Monte Carlo approach combining a variational autoencoder generative model with a drawdown reconstruction loss function. To overcome issues of numerical complexity and non-differentiability, we approximate drawdown as a linear function of the moments of the path, known in the literature as path signatures. We prove the required regularity of drawdown function and consistency of the approximation. Furthermore, we obtain close numerical approximations using linear regression for fr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#22768;&#23398;&#29305;&#24449;&#23545;&#21683;&#22013;&#38899;&#39057;&#20449;&#21495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;COVID-19&#26816;&#27979;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.04505</link><description>&lt;p&gt;
COVID-19&#26816;&#27979;&#31995;&#32479;&#65306;&#22522;&#20110;&#21683;&#22013;&#38899;&#39057;&#20449;&#21495;&#30340;&#22768;&#23398;&#29305;&#24449;&#31995;&#32479;&#24615;&#33021;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
COVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals. (arXiv:2309.04505v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#22768;&#23398;&#29305;&#24449;&#23545;&#21683;&#22013;&#38899;&#39057;&#20449;&#21495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;COVID-19&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#21628;&#21560;&#36947;&#30142;&#30149;&#22914;&#24863;&#20882;&#21644;&#27969;&#24863;&#12289;&#21742;&#21912;&#20197;&#21450;COVID-19&#31561;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24433;&#21709;&#30528;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#22312;&#21307;&#23398;&#23454;&#36341;&#20013;&#65292;&#21628;&#21560;&#22768;&#38899;&#34987;&#24191;&#27867;&#29992;&#20110;&#21307;&#30103;&#26381;&#21153;&#20013;&#65292;&#29992;&#20110;&#35786;&#26029;&#21508;&#31181;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#21644;&#32954;&#37096;&#30142;&#30149;&#12290;&#20256;&#32479;&#30340;&#35786;&#26029;&#26041;&#27861;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#25104;&#26412;&#39640;&#19988;&#20381;&#36182;&#20110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;&#21683;&#22013;&#38899;&#39057;&#35760;&#24405;&#34987;&#29992;&#26469;&#33258;&#21160;&#21270;&#26816;&#27979;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#30340;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#26816;&#26597;&#21508;&#31181;&#22768;&#23398;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21683;&#22013;&#20449;&#21495;&#20013;&#26816;&#27979;COVID-19&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19977;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65288;MFCC&#65292;Chroma&#21644;Spectral Contrast&#29305;&#24449;&#65289;&#22312;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;SVM&#21644;MLP&#65289;&#19978;&#30340;&#21151;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;COVID-19&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide range of respiratory diseases, such as cold and flu, asthma, and COVID-19, affect people's daily lives worldwide. In medical practice, respiratory sounds are widely used in medical services to diagnose various respiratory illnesses and lung disorders. The traditional diagnosis of such sounds requires specialized knowledge, which can be costly and reliant on human expertise. Recently, cough audio recordings have been used to automate the process of detecting respiratory conditions. This research aims to examine various acoustic features that enhance the performance of machine learning (ML) models in detecting COVID-19 from cough signals. This study investigates the efficacy of three feature extraction techniques, including Mel Frequency Cepstral Coefficients (MFCC), Chroma, and Spectral Contrast features, on two ML algorithms, Support Vector Machine (SVM) and Multilayer Perceptron (MLP), and thus proposes an efficient COVID-19 detection system. The proposed system produces a prac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20195;&#29702;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#12290;</title><link>http://arxiv.org/abs/2309.04504</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22522;&#20110;&#35270;&#35273;&#30340;&#27010;&#24565;&#32452;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositional Learning of Visually-Grounded Concepts Using Reinforcement. (arXiv:2309.04504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20195;&#29702;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#38656;&#35201;&#36890;&#36807;&#25968;&#30334;&#19975;&#20010;&#22238;&#21512;&#30340;&#35757;&#32451;&#25165;&#33021;&#36739;&#22909;&#22320;&#35299;&#20915;&#19982;&#25351;&#20196;&#30456;&#20851;&#30340;&#23548;&#33322;&#20219;&#21153;&#65292;&#24182;&#19988;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#25351;&#20196;&#32452;&#21512;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20799;&#31461;&#21487;&#20197;&#20998;&#35299;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#24182;&#23548;&#33322;&#21040;&#25351;&#23450;&#30340;&#29289;&#20307;&#65292;&#21363;&#20351;&#20182;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#26597;&#35810;&#30340;&#32452;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#20010;3D&#29615;&#22659;&#65292;&#30740;&#31350;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#21644;&#32452;&#21512;&#22522;&#20110;&#39068;&#33394;&#21644;&#24418;&#29366;&#30340;&#32452;&#21512;&#25351;&#20196;&#65292;&#20197;&#35299;&#20915;&#31354;&#38388;&#23548;&#33322;&#20219;&#21153;&#20013;&#30340;&#26032;&#39062;&#32452;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#35752;&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#36827;&#34892;&#32452;&#21512;&#23398;&#20064;&#65292;&#24182;&#19988;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#20923;&#32467;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#12289;BERT&#65289;&#22312;&#26356;&#23569;&#30340;&#22238;&#21512;&#20013;&#23398;&#20064;&#21333;&#35789;&#32452;&#21512;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#20195;&#29702;&#22312;&#24418;&#29366;&#25110;&#39068;&#33394;&#27010;&#24565;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#25152;&#38656;&#30340;&#35757;&#32451;&#22238;&#21512;&#25968;&#20943;&#23569;&#20102;20&#20493;&#65292;&#21487;&#20197;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#25351;&#20196;&#32452;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning agents need to be trained over millions of episodes to decently solve navigation tasks grounded to instructions. Furthermore, their ability to generalize to novel combinations of instructions is unclear. Interestingly however, children can decompose language-based instructions and navigate to the referred object, even if they have not seen the combination of queries prior. Hence, we created three 3D environments to investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task. First, we explore if agents can perform compositional learning, and whether they can leverage on frozen text encoders (e.g. CLIP, BERT) to learn word combinations in fewer episodes. Next, we demonstrate that when agents are pretrained on the shape or color concepts separately, they show a 20 times decrease in training episodes needed to solve unseen combinations of instructions. Lastly, we show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20960;&#20309;&#29305;&#24449;&#21644;&#24037;&#31243;&#24615;&#33021;&#30340;&#21152;&#26435;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24037;&#31243;&#24615;&#33021;&#39044;&#27979;&#12290;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.04499</link><description>&lt;p&gt;
&#32771;&#34385;&#20960;&#20309;&#29305;&#24449;&#21644;&#24037;&#31243;&#24615;&#33021;&#30340;&#21152;&#26435;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weighted Unsupervised Domain Adaptation Considering Geometry Features and Engineering Performance of 3D Design Data. (arXiv:2309.04499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20960;&#20309;&#29305;&#24449;&#21644;&#24037;&#31243;&#24615;&#33021;&#30340;&#21152;&#26435;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#19987;&#38376;&#29992;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24037;&#31243;&#24615;&#33021;&#39044;&#27979;&#12290;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#19994;&#20013;&#30340;&#20135;&#21697;&#35774;&#35745;&#36807;&#31243;&#28041;&#21450;&#36845;&#20195;&#30340;&#35774;&#35745;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#20197;&#23454;&#29616;&#30446;&#26631;&#24037;&#31243;&#24615;&#33021;&#65292;&#20294;&#36825;&#26679;&#30340;&#36845;&#20195;&#36807;&#31243;&#32791;&#26102;&#19988;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24037;&#31243;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#21152;&#36895;&#35774;&#35745;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21482;&#33021;&#20445;&#35777;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20110;&#26032;&#30340;&#39046;&#22495;&#25968;&#25454;&#26102;&#21487;&#33021;&#19981;&#20934;&#30830;&#12290;&#29305;&#21035;&#26159;&#65292;3D&#35774;&#35745;&#25968;&#25454;&#20855;&#26377;&#22797;&#26434;&#30340;&#29305;&#24449;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21463;&#38480;&#20110;&#22823;&#37327;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#35757;&#32451;&#36127;&#25285;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20960;&#20309;&#29305;&#24449;&#21644;&#24037;&#31243;&#24615;&#33021;&#30340;&#21452;&#21152;&#26435;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#19987;&#29992;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24037;&#31243;&#24615;&#33021;&#39044;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The product design process in manufacturing involves iterative design modeling and analysis to achieve the target engineering performance, but such an iterative process is time consuming and computationally expensive. Recently, deep learning-based engineering performance prediction models have been proposed to accelerate design optimization. However, they only guarantee predictions on training data and may be inaccurate when applied to new domain data. In particular, 3D design data have complex features, which means domains with various distributions exist. Thus, the utilization of deep learning has limitations due to the heavy data collection and training burdens. We propose a bi-weighted unsupervised domain adaptation approach that considers the geometry features and engineering performance of 3D design data. It is specialized for deep learning-based engineering performance predictions. Domain-invariant features can be extracted through an adversarial training strategy by using hypot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#31070;&#32463;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#38454;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35299;&#20915;&#20102;&#38750;&#20223;&#23556;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#27425;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04492</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#20223;&#23556;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#31070;&#32463;&#25511;&#21046;&#22120;&#19982;&#21487;&#24494;&#30340;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Safe Neural Control for Non-Affine Control Systems with Differentiable Control Barrier Functions. (arXiv:2309.04492v1 [cs.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#31070;&#32463;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#38454;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35299;&#20915;&#20102;&#38750;&#20223;&#23556;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#27425;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#38750;&#20223;&#23556;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBFs&#65289;&#65292;&#24050;&#32463;&#35777;&#26126;&#20248;&#21270;&#20108;&#27425;&#25104;&#26412;&#22312;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#19979;&#21487;&#20197;&#36890;&#36807;&#19968;&#31995;&#21015;&#20108;&#27425;&#35268;&#21010;&#65288;QPs&#65289;&#26469;&#36827;&#34892;&#27425;&#20248;&#21270;&#12290;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#39640;&#38454;CBFs&#65288;HOCBFs&#65289;&#21487;&#20197;&#23481;&#32435;&#20219;&#24847;&#30456;&#23545;&#38454;&#25968;&#30340;&#32422;&#26463;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#38656;&#35201;&#20223;&#23556;&#25511;&#21046;&#21160;&#24577;&#20197;&#21450;CBF-based QP&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#28857;&#35299;&#27861;&#65292;&#22240;&#27492;&#26159;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#39640;&#38454;CBFs&#25972;&#21512;&#21040;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20316;&#20026;&#21487;&#24494;&#30340;CBFs&#65292;&#20197;&#30830;&#20445;&#38750;&#20223;&#23556;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#21487;&#24494;&#30340;CBFs&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21442;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#21487;&#20197;&#35299;&#20915;CBFs&#36807;&#24230;&#20445;&#23432;&#30340;&#38382;&#39064;&#65292;&#20351;&#31995;&#32479;&#29366;&#24577;&#19981;&#24517;&#35201;&#22320;&#36828;&#31163;&#23433;&#20840;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of safety-critical control for non-affine control systems. It has been shown that optimizing quadratic costs subject to state and control constraints can be sub-optimally reduced to a sequence of quadratic programs (QPs) by using Control Barrier Functions (CBFs). Our recently proposed High Order CBFs (HOCBFs) can accommodate constraints of arbitrary relative degree. The main challenges in this approach are that it requires affine control dynamics and the solution of the CBF-based QP is sub-optimal since it is solved point-wise. To address these challenges, we incorporate higher-order CBFs into neural ordinary differential equation-based learning models as differentiable CBFs to guarantee safety for non-affine control systems. The differentiable CBFs are trainable in terms of their parameters, and thus, they can address the conservativeness of CBFs such that the system state will not stay unnecessarily far away from safe set boundaries. Moreover, the imi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24072;&#29983;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#32467;&#26500;&#27169;&#22411;&#20316;&#20026;&#8220;&#32769;&#24072;&#8221;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#21270;&#23398;&#32452;&#20998;&#30340;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#35813;&#31574;&#30053;&#22312;&#19981;&#21516;&#32593;&#32476;&#32467;&#26500;&#19978;&#37117;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04482</link><description>&lt;p&gt;
&#35299;&#20915;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#20013;&#30340;&#31934;&#30830;&#24230;&#21644;&#25104;&#26412;&#26435;&#34913;&#38382;&#39064;&#65306;&#19968;&#31181;&#24072;&#29983;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Addressing the Accuracy-Cost Tradeoff in Material Property Prediction: A Teacher-Student Strategy. (arXiv:2309.04482v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24072;&#29983;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#32467;&#26500;&#27169;&#22411;&#20316;&#20026;&#8220;&#32769;&#24072;&#8221;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#21270;&#23398;&#32452;&#20998;&#30340;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#39564;&#35777;&#65292;&#35813;&#31574;&#30053;&#22312;&#19981;&#21516;&#32593;&#32476;&#32467;&#26500;&#19978;&#37117;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#26032;&#26448;&#26009;&#21457;&#29616;&#30340;&#36807;&#31243;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#29616;&#22312;&#33021;&#22815;&#20165;&#22522;&#20110;&#21270;&#23398;&#32452;&#20998;&#39044;&#27979;&#26448;&#26009;&#24615;&#33021;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#26448;&#26009;&#32467;&#26500;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#27169;&#22411;&#31934;&#30830;&#24230;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#21270;&#23398;&#32452;&#20998;&#30340;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#65288;CPMs&#65289;&#30340;&#20934;&#30830;&#24230;&#26174;&#33879;&#33853;&#21518;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#27169;&#22411;&#65288;SPMs&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24072;&#29983;&#65288;T-S&#65289;&#31574;&#30053;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#30340; SPM &#20805;&#24403;&#8220;&#32769;&#24072;&#8221;&#65292;&#20197;&#25552;&#39640; CPM &#30340;&#20934;&#30830;&#24615;&#12290;&#21033;&#29992;&#24072;&#29983;&#31574;&#30053;&#65292;T-S CrabNet &#24050;&#32463;&#25104;&#20026;&#30446;&#21069; CPM &#20013;&#26368;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#31574;&#30053;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312; Materials Project&#65288;MP&#65289;&#21644; Jarvis &#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102; T-S &#31574;&#30053;&#22312;&#25552;&#39640; CPM &#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has revolutionized the process of new material discovery, with state-of-the-art models now able to predict material properties based solely on chemical compositions, thus eliminating the necessity for material structures. However, this cost-effective method has led to a trade-off in model accuracy. Specifically, the accuracy of Chemical Composition-based Property Prediction Models (CPMs) significantly lags behind that of Structure-based Property Prediction Models (SPMs). To tackle this challenge, we propose an innovative Teacher-Student (T-S) strategy, where a pre-trained SPM serves as the 'teacher' to enhance the accuracy of the CPM. Leveraging the T-S strategy, T-S CrabNet has risen to become the most accurate model among current CPMs. Initially, we demonstrated the universality of this strategy. On the Materials Project (MP) and Jarvis datasets, we validated the effectiveness of the T-S strategy in boosting the accuracy of CPMs with two distinct network structures, nam
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25104;&#20998;-&#32467;&#26500;&#21452;&#27169;&#24577;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#23545;&#23454;&#39564;&#27979;&#37327;&#26448;&#26009;&#24615;&#36136;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#65292;&#20174;&#32780;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.04478</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#25104;&#20998;-&#32467;&#26500;&#21452;&#27169;&#24577;&#23398;&#20064;&#30340;&#23454;&#39564;&#27979;&#37327;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Multimodal machine learning for materials science: composition-structure bimodal learning for experimentally measured properties. (arXiv:2309.04478v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25104;&#20998;-&#32467;&#26500;&#21452;&#27169;&#24577;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#23545;&#23454;&#39564;&#27979;&#37327;&#26448;&#26009;&#24615;&#36136;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#65292;&#20174;&#32780;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#24212;&#29992;&#30340;GPT-4&#31561;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#22312;&#26448;&#26009;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#23613;&#31649;&#23384;&#22312;&#21508;&#31181;&#27169;&#24577;&#30340;&#26448;&#26009;&#25968;&#25454;&#65292;&#27604;&#22914;&#25104;&#20998;&#21644;&#32467;&#26500;&#65292;&#20294;&#20854;&#23454;&#38469;&#24212;&#29992;&#36824;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22823;&#35268;&#27169;&#35745;&#31639;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#20381;&#36182;&#20110;&#35745;&#31639;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23454;&#39564;&#25968;&#25454;&#38598;&#24448;&#24448;&#25968;&#25454;&#26377;&#38480;&#19988;&#20449;&#24687;&#19981;&#23436;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25104;&#20998;-&#32467;&#26500;&#21452;&#27169;&#24577;&#23398;&#20064;&#26469;&#22686;&#24378;&#23545;&#23454;&#39564;&#27979;&#37327;&#26448;&#26009;&#24615;&#36136;&#30340;&#23398;&#20064;&#21644;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;COmposition-Structure&#21452;&#27169;&#24577;&#32593;&#32476;&#65288;COSNet&#65289;&#26088;&#22312;&#20943;&#23569;&#23545;&#20855;&#26377;&#19981;&#23436;&#25972;&#32467;&#26500;&#20449;&#24687;&#30340;&#26448;&#26009;&#24615;&#36136;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread application of multimodal machine learning models like GPT-4 has revolutionized various research fields including computer vision and natural language processing. However, its implementation in materials informatics remains underexplored, despite the presence of materials data across diverse modalities, such as composition and structure. The effectiveness of machine learning models trained on large calculated datasets depends on the accuracy of calculations, while experimental datasets often have limited data availability and incomplete information. This paper introduces a novel approach to multimodal machine learning in materials science via composition-structure bimodal learning. The proposed COmposition-Structure Bimodal Network (COSNet) is designed to enhance learning and predictions of experimentally measured materials properties that have incomplete structure information. Bimodal learning significantly reduces prediction errors across distinct materials properties 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DiffCSP&#65292;&#19968;&#31181;&#21033;&#29992;&#21608;&#26399;-E(3)-&#31561;&#21464;&#21435;&#22122;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#32852;&#21512;&#29983;&#25104;&#26230;&#20307;&#30340;&#26230;&#26684;&#21644;&#21407;&#23376;&#22352;&#26631;&#65292;&#20197;&#35299;&#20915;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#20013;&#30340;&#23545;&#31216;&#24615;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.04475</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#31561;&#21464;&#25193;&#25955;&#36827;&#34892;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Crystal Structure Prediction by Joint Equivariant Diffusion. (arXiv:2309.04475v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiffCSP&#65292;&#19968;&#31181;&#21033;&#29992;&#21608;&#26399;-E(3)-&#31561;&#21464;&#21435;&#22122;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#32852;&#21512;&#29983;&#25104;&#26230;&#20307;&#30340;&#26230;&#26684;&#21644;&#21407;&#23376;&#22352;&#26631;&#65292;&#20197;&#35299;&#20915;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#20013;&#30340;&#23545;&#31216;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#22312;&#21508;&#20010;&#31185;&#23398;&#23398;&#31185;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#30446;&#21069;&#27969;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#20294;&#30001;&#20110;&#26230;&#20307;&#32467;&#26500;&#30340;&#23545;&#31216;&#20960;&#20309;&#29305;&#24615;&#65288;&#24179;&#31227;&#12289;&#26059;&#36716;&#21644;&#21608;&#26399;&#24615;&#30340;&#19981;&#21464;&#24615;&#65289;&#65292;&#35813;&#20219;&#21153;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#34701;&#20837;&#20197;&#19978;&#23545;&#31216;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DiffCSP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#21608;&#26399;-E(3)-&#31561;&#21464;&#21435;&#22122;&#27169;&#22411;&#65292;&#32852;&#21512;&#29983;&#25104;&#27599;&#20010;&#26230;&#20307;&#30340;&#26230;&#26684;&#21644;&#21407;&#23376;&#22352;&#26631;&#65292;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#26230;&#20307;&#20960;&#20309;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#30456;&#20851;&#30340;&#31561;&#21464;&#29983;&#25104;&#26041;&#27861;&#19981;&#21516;&#65292;DiffCSP&#21033;&#29992;&#20998;&#25968;&#22352;&#26631;&#32780;&#19981;&#26159;&#31515;&#21345;&#23572;&#22352;&#26631;&#26469;&#34920;&#31034;&#26230;&#20307;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#21407;&#23376;&#20301;&#32622;&#30340;&#25193;&#25955;&#21644;&#29983;&#25104;&#36807;&#31243;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;DiffCSP&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crystal Structure Prediction (CSP) is crucial in various scientific disciplines. While CSP can be addressed by employing currently-prevailing generative models (e.g. diffusion models), this task encounters unique challenges owing to the symmetric geometry of crystal structures -- the invariance of translation, rotation, and periodicity. To incorporate the above symmetries, this paper proposes DiffCSP, a novel diffusion model to learn the structure distribution from stable crystals. To be specific, DiffCSP jointly generates the lattice and atom coordinates for each crystal by employing a periodic-E(3)-equivariant denoising model, to better model the crystal geometry. Notably, different from related equivariant generative approaches, DiffCSP leverages fractional coordinates other than Cartesian coordinates to represent crystals, remarkably promoting the diffusion and the generation process of atom positions. Extensive experiments verify that our DiffCSP significantly outperforms existing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#20018;&#34892;&#39134;&#31186;&#26230;&#20307;&#23398;&#20013;&#36890;&#36807;&#24369;&#30417;&#30563;&#31639;&#27861;&#23545;&#34893;&#23556;&#22270;&#36827;&#34892;&#20998;&#31867;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#20943;&#23569;&#35757;&#32451;&#25152;&#38656;&#30340;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;</title><link>http://arxiv.org/abs/2309.04474</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#22312;&#20018;&#34892;&#39134;&#31186;&#26230;&#20307;&#23398;&#20013;&#30340;&#27169;&#24335;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised learning for pattern classification in serial femtosecond crystallography. (arXiv:2309.04474v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#20018;&#34892;&#39134;&#31186;&#26230;&#20307;&#23398;&#20013;&#36890;&#36807;&#24369;&#30417;&#30563;&#31639;&#27861;&#23545;&#34893;&#23556;&#22270;&#36827;&#34892;&#20998;&#31867;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#20943;&#23569;&#35757;&#32451;&#25152;&#38656;&#30340;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
X&#23556;&#32447;&#33258;&#30001;&#30005;&#23376;&#28608;&#20809;&#35774;&#26045;&#19978;&#30340;&#20018;&#34892;&#39134;&#31186;&#26230;&#20307;&#23398;&#20026;&#26230;&#20307;&#32467;&#26500;&#30340;&#30830;&#23450;&#24320;&#36767;&#20102;&#26032;&#30340;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23454;&#39564;&#30340;&#25968;&#25454;&#22788;&#29702;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#30830;&#23450;&#39640;&#20998;&#36776;&#29575;&#32467;&#26500;&#25152;&#38656;&#30340;&#34893;&#23556;&#22270;&#30340;&#24635;&#25968;&#24040;&#22823;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22788;&#29702;&#22914;&#27492;&#22823;&#37327;&#30340;&#25968;&#25454;&#26041;&#38754;&#21487;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#24335;&#20998;&#31867;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#38656;&#35201;&#24102;&#26377;&#26631;&#31614;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#23545;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#24378;&#20381;&#36182;&#20005;&#37325;&#38480;&#21046;&#20102;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#22240;&#20026;&#27880;&#37322;&#22823;&#37327;&#34893;&#23556;&#22270;&#38750;&#24120;&#26114;&#36149;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#24369;&#30417;&#30563;&#31639;&#27861;&#19979;&#23545;&#34893;&#23556;&#22270;&#36827;&#34892;&#20998;&#31867;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#20943;&#23569;&#35757;&#32451;&#25152;&#38656;&#30340;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Serial femtosecond crystallography at X-ray free electron laser facilities opens a new era for the determination of crystal structure. However, the data processing of those experiments is facing unprecedented challenge, because the total number of diffraction patterns needed to determinate a high-resolution structure is huge. Machine learning methods are very likely to play important roles in dealing with such a large volume of data. Convolutional neural networks have made a great success in the field of pattern classification, however, training of the networks need very large datasets with labels. Th is heavy dependence on labeled datasets will seriously restrict the application of networks, because it is very costly to annotate a large number of diffraction patterns. In this article we present our job on the classification of diffraction pattern by weakly supervised algorithms, with the aim of reducing as much as possible the size of the labeled dataset required for training. Our res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#35889;&#26041;&#27861;&#25913;&#36827;&#20102;&#25490;&#21517;&#32858;&#21512;&#38382;&#39064;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#26410;&#24402;&#19968;&#21270;&#21644;&#24402;&#19968;&#21270;&#25968;&#25454;&#30697;&#38453;&#30340;&#35889;&#25490;&#21517;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25200;&#21160;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.03808</link><description>&lt;p&gt;
&#36890;&#36807;&#35889;&#26041;&#27861;&#25913;&#36827;&#20102;&#25490;&#21517;&#32858;&#21512;&#30340;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Improved theoretical guarantee for rank aggregation via spectral method. (arXiv:2309.03808v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#35889;&#26041;&#27861;&#25913;&#36827;&#20102;&#25490;&#21517;&#32858;&#21512;&#38382;&#39064;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36890;&#36807;&#30740;&#31350;&#22522;&#20110;&#26410;&#24402;&#19968;&#21270;&#21644;&#24402;&#19968;&#21270;&#25968;&#25454;&#30697;&#38453;&#30340;&#35889;&#25490;&#21517;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#25200;&#21160;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#22810;&#20010;&#39033;&#30446;&#20043;&#38388;&#30340;&#25104;&#23545;&#27604;&#36739;&#65292;&#22914;&#20309;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#21517;&#20197;&#20351;&#24471;&#25490;&#21517;&#19982;&#35266;&#23519;&#30456;&#21305;&#37197;&#65311;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#25490;&#21517;&#32858;&#21512;&#65292;&#22312;&#20307;&#32946;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#20854;&#20182;&#32593;&#32476;&#24212;&#29992;&#20013;&#24050;&#32463;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#30001;&#20110;&#25214;&#21040;&#26368;&#23567;&#21270;&#19981;&#21305;&#37197;&#30340;&#20840;&#23616;&#25490;&#21517;&#36890;&#24120;&#26159;NP&#22256;&#38590;&#30340;&#65288;&#31216;&#20026;Kemeny&#20248;&#21270;&#65289;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;Erd\"os-R\'enyi&#31163;&#32676;&#28857;&#65288;ERO&#65289;&#27169;&#22411;&#19978;&#12290;&#22312;&#36825;&#20010;&#25490;&#21517;&#38382;&#39064;&#20013;&#65292;&#27599;&#20010;&#25104;&#23545;&#27604;&#36739;&#26159;&#30495;&#23454;&#20998;&#25968;&#24046;&#24322;&#30340;&#34987;&#25439;&#22351;&#21103;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#26410;&#24402;&#19968;&#21270;&#21644;&#24402;&#19968;&#21270;&#25968;&#25454;&#30697;&#38453;&#30340;&#35889;&#25490;&#21517;&#31639;&#27861;&#12290;&#20851;&#38190;&#26159;&#29702;&#35299;&#23427;&#20204;&#22312;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#24674;&#22797;&#27599;&#20010;&#39033;&#30446;&#30340;&#28508;&#22312;&#20998;&#25968;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#24402;&#32467;&#20026;&#25512;&#23548;&#26410;&#24402;&#19968;&#21270;/&#24402;&#19968;&#21270;&#25968;&#25454;&#30697;&#38453;&#30340;&#21069;&#20960;&#20010;&#29305;&#24449;&#21521;&#37327;&#21644;&#20854;&#24635;&#20307;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#36880;&#20010;&#39033;&#25200;&#21160;&#35823;&#24046;&#30028;&#38480;&#12290;&#36890;&#36807;&#20351;&#29992;&#30041;&#20986;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20934;&#30830;&#30340;$\ell_{\infty}$-norm&#25200;&#21160;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given pairwise comparisons between multiple items, how to rank them so that the ranking matches the observations? This problem, known as rank aggregation, has found many applications in sports, recommendation systems, and other web applications. As it is generally NP-hard to find a global ranking that minimizes the mismatch (known as the Kemeny optimization), we focus on the Erd\"os-R\'enyi outliers (ERO) model for this ranking problem. Here, each pairwise comparison is a corrupted copy of the true score difference. We investigate spectral ranking algorithms that are based on unnormalized and normalized data matrices. The key is to understand their performance in recovering the underlying scores of each item from the observed data. This reduces to deriving an entry-wise perturbation error bound between the top eigenvectors of the unnormalized/normalized data matrix and its population counterpart. By using the leave-one-out technique, we provide a sharper $\ell_{\infty}$-norm perturbati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35775;&#38382;&#65292;&#24182;&#22312;&#20248;&#21270;&#30005;&#36335;&#37197;&#32622;&#26102;&#21516;&#26102;&#32771;&#34385;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#38376;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26368;&#20339;&#30005;&#36335;&#37197;&#32622;&#20013;&#32416;&#32544;&#38376;&#38656;&#35201;&#30456;&#24212;&#30340;&#25968;&#37327;&#65292;&#19982;&#20043;&#21069;&#30740;&#31350;&#30456;&#21453;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;&#30830;&#23450;&#26368;&#20339;&#37197;&#32622;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03307</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generating quantum feature maps using multi-objective genetic algorithm. (arXiv:2309.03307v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#29983;&#25104;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35775;&#38382;&#65292;&#24182;&#22312;&#20248;&#21270;&#30005;&#36335;&#37197;&#32622;&#26102;&#21516;&#26102;&#32771;&#34385;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#38376;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#26368;&#20339;&#30005;&#36335;&#37197;&#32622;&#20013;&#32416;&#32544;&#38376;&#38656;&#35201;&#30456;&#24212;&#30340;&#25968;&#37327;&#65292;&#19982;&#20043;&#21069;&#30740;&#31350;&#30456;&#21453;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;&#30830;&#23450;&#26368;&#20339;&#37197;&#32622;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#29992;&#20110;&#37327;&#23376;&#22686;&#24378;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#39640;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35775;&#38382;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#26368;&#22823;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#37327;&#23376;&#29305;&#24449;&#26144;&#23556;&#30005;&#36335;&#30340;&#26412;&#22320;&#38376;&#25104;&#26412;&#21644;&#38750;&#26412;&#22320;&#38376;&#25104;&#26412;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20026;&#26412;&#22320;&#38376;&#21644;&#32416;&#32544;&#38376;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#36866;&#24212;&#24230;&#20989;&#25968;&#12290;&#19982;&#32463;&#20856;&#20998;&#31867;&#22120;&#30340;&#27604;&#36739;&#26377;&#21161;&#20110;&#29702;&#35299;&#20351;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#37327;&#23376;&#26680;&#26041;&#27861;&#30340;&#26368;&#20339;&#30005;&#36335;&#37197;&#32622;&#20013;&#21253;&#21547;&#20102;&#30456;&#24212;&#25968;&#37327;&#30340;&#38750;&#26412;&#22320;&#38376;&#29992;&#20110;&#32416;&#32544;&#65292;&#19982;&#20043;&#21069;&#30340;&#25991;&#29486;&#30456;&#21453;&#65292;&#20043;&#21069;&#30340;&#25991;&#29486;&#20013;&#38750;&#26412;&#22320;&#38376;&#34987;&#22823;&#37096;&#20998;&#25233;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#25968;&#25454;&#30340;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#30830;&#23450;&#26368;&#20339;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for efficiently generating quantum feature maps for quantum-enhanced support vector machines, a kernel-based classifier, enabling access to high-dimensional Hilbert space. Our method employs a multi-objective genetic algorithm that simultaneously maximizes classification accuracy while minimizing both the local and non-local gate costs of the quantum feature map's circuit. To achieve this, we define distinct fitness functions for local gates and entanglement gates. Comparisons with classical classifiers are given in order to understand the advantages of using quantum machine learning. Surprisingly, our experiments reveal that the optimal configuration of quantum circuits for the quantum kernel method incorporates a proportional number of non-local gates for entanglement, contrary to previous literature where non-local gates were largely suppressed.  Furthermore, we demonstrate that the separability indexes of data can be effectively leveraged to determine th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.03224</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#20381;&#28982;&#33021;&#33719;&#30410;&#65306;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03224
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#26469;&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#19981;&#36275;&#21644;&#38169;&#35823;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#27169;&#22411;&#21644;&#24341;&#20837;&#36335;&#24452;&#39564;&#35777;&#22120;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#30340;&#25628;&#32034;&#21644;&#25512;&#29702;&#36335;&#24452;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32972;&#26223;&#23398;&#20064;&#33021;&#21147;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36807;&#31243;&#30417;&#30563;&#65292;&#23558;PLMs&#24212;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#36890;&#24120;&#26080;&#27861;&#29983;&#25104;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#21644;&#26368;&#32456;&#31572;&#26696;&#65292;&#21363;&#20351;&#35299;&#20915;&#26041;&#26696;&#27010;&#29575;&#24456;&#39640;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#24494;&#35843;&#30340;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#21644;&#36731;&#37327;&#32423;&#33021;&#37327;&#20989;&#25968;&#20026;LLMs&#36171;&#20104;&#21363;&#26102;&#21453;&#24212;&#21644;&#31934;&#32454;&#25512;&#29702;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#24494;&#35843;&#30340;LLMs&#37325;&#26032;&#23450;&#20041;&#20026;&#22522;&#20110;&#27531;&#24046;&#30340;&#33021;&#37327;&#27169;&#22411;&#65288;Residual-EBM&#65289;&#65292;&#24182;&#24212;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#26469;&#20272;&#35745;&#33021;&#37327;&#20989;&#25968;&#30340;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#33021;&#37327;&#20989;&#25968;&#30340;MCTS&#20316;&#20026;&#36335;&#24452;&#39564;&#35777;&#22120;&#26469;&#25628;&#32034;&#36755;&#20986;&#31354;&#38388;&#24182;&#35780;&#20272;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit impressive language understanding and in-context learning abilities including natural language processing (NLP) tasks and challenging mathematical reasoning. However, due to the lack of process-supervision, applying PLMs to mathematical reasoning tasks often fail to generate correct reasoning steps and final answer even though solutions have high probabilities. To unleash the mathematical reasoning of finetuned-LLMs without any further fineutuning steps, we propose a method to endow LLMs with immediate reaction and delicate reasoning system via Monte Carlo Tree Search(MCTS) and a light energy function to rank the decision steps. In particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy Model~(Residual-EBM) and apply noise contrastive estimation to estimate the parameters of energy function . Then we use MCTS with energy function as path verifier to search the output space and evaluating the reasoning path. Through extensive 
&lt;/p&gt;</description></item><item><title>CR-VAE&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#23545;&#27604;&#30446;&#26631;&#26469;&#26368;&#22823;&#21270;&#31867;&#20284;&#35270;&#35273;&#36755;&#20837;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#21518;&#39564;&#22349;&#22604;&#29616;&#35937;&#65292;&#24182;&#22312;&#22810;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02968</link><description>&lt;p&gt;
CR-VAE:&#23545;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#23545;&#27604;&#27491;&#21017;&#21270;&#20197;&#38450;&#27490;&#21518;&#39564;&#22349;&#22604;
&lt;/p&gt;
&lt;p&gt;
CR-VAE: Contrastive Regularization on Variational Autoencoders for Preventing Posterior Collapse. (arXiv:2309.02968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02968
&lt;/p&gt;
&lt;p&gt;
CR-VAE&#26159;&#19968;&#31181;&#23545;&#27604;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#23545;&#27604;&#30446;&#26631;&#26469;&#26368;&#22823;&#21270;&#31867;&#20284;&#35270;&#35273;&#36755;&#20837;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#21518;&#39564;&#22349;&#22604;&#29616;&#35937;&#65292;&#24182;&#22312;&#22810;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#24050;&#30693;&#23384;&#22312;&#21518;&#39564;&#22349;&#22604;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#29983;&#25104;&#30340;&#28508;&#22312;&#34920;&#31034;&#19982;&#36755;&#20837;&#20043;&#38388;&#21464;&#24471;&#29420;&#31435;&#12290;&#36825;&#23548;&#33268;&#36755;&#20837;&#30340;&#34920;&#31034;&#36864;&#21270;&#65292;&#36825;&#24402;&#22240;&#20110;VAE&#30446;&#26631;&#20989;&#25968;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23545;&#27604;&#27491;&#21017;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CR-VAE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#22312;&#21407;&#22987;VAE&#20013;&#22686;&#21152;&#23545;&#27604;&#30446;&#26631;&#65292;&#26368;&#22823;&#21270;&#31867;&#20284;&#35270;&#35273;&#36755;&#20837;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#36825;&#31181;&#31574;&#30053;&#30830;&#20445;&#20102;&#36755;&#20837;&#19982;&#20854;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#26368;&#22823;&#21270;&#65292;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;&#21518;&#39564;&#22349;&#22604;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;CR-VAE&#22312;&#38450;&#27490;&#21518;&#39564;&#22349;&#22604;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Variational Autoencoder (VAE) is known to suffer from the phenomenon of \textit{posterior collapse}, where the latent representations generated by the model become independent of the inputs. This leads to degenerated representations of the input, which is attributed to the limitations of the VAE's objective function. In this work, we propose a novel solution to this issue, the Contrastive Regularization for Variational Autoencoders (CR-VAE). The core of our approach is to augment the original VAE with a contrastive objective that maximizes the mutual information between the representations of similar visual inputs. This strategy ensures that the information flow between the input and its latent representation is maximized, effectively avoiding posterior collapse. We evaluate our method on a series of visual datasets and demonstrate, that CR-VAE outperforms state-of-the-art approaches in preventing posterior collapse.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#38477;&#20302;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#19981;&#33391;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#38169;&#35823;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20013;&#25552;&#21462;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#26469;&#24809;&#32602;&#31995;&#32479;&#38169;&#35823;&#34892;&#20026;&#12290;&#36825;&#19968;&#26694;&#26550;&#22312;&#20445;&#25345;&#21331;&#36234;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20026;&#24037;&#31243;&#24072;&#25552;&#20379;&#20102;&#38024;&#23545;&#19981;&#33391;&#34892;&#20026;&#30340;&#21487;&#29702;&#35299;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.02869</link><description>&lt;p&gt;
&#38477;&#20302;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
On Reducing Undesirable Behavior in Deep Reinforcement Learning Models. (arXiv:2309.02869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#38477;&#20302;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#19981;&#33391;&#34892;&#20026;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#38169;&#35823;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20013;&#25552;&#21462;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#26469;&#24809;&#32602;&#31995;&#32479;&#38169;&#35823;&#34892;&#20026;&#12290;&#36825;&#19968;&#26694;&#26550;&#22312;&#20445;&#25345;&#21331;&#36234;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20026;&#24037;&#31243;&#24072;&#25552;&#20379;&#20102;&#38024;&#23545;&#19981;&#33391;&#34892;&#20026;&#30340;&#21487;&#29702;&#35299;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#22823;&#37327;&#24212;&#29992;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#26497;&#22823;&#30340;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#25104;&#21151;&#30340;&#22522;&#20110;DRL&#30340;&#36719;&#20214;&#20063;&#21487;&#33021;&#34920;&#29616;&#20986;&#26497;&#20854;&#19981;&#33391;&#30340;&#34892;&#20026;&#12290;&#36825;&#26159;&#22240;&#20026;DRL&#35757;&#32451;&#26159;&#22522;&#20110;&#26368;&#22823;&#21270;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#36890;&#24120;&#33021;&#25429;&#25417;&#21040;&#19968;&#33324;&#36235;&#21183;&#65292;&#20294;&#19981;&#33021;&#20934;&#30830;&#25429;&#25417;&#25110;&#25490;&#38500;&#31995;&#32479;&#30340;&#26576;&#20123;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22823;&#24133;&#38477;&#20302;&#22522;&#20110;DRL&#30340;&#36719;&#20214;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#24037;&#31243;&#24072;&#23545;&#36825;&#31181;&#19981;&#33391;&#34892;&#20026;&#36827;&#34892;&#21487;&#29702;&#35299;&#30340;&#34920;&#24449;&#12290;&#22312;&#24213;&#23618;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20174;&#38169;&#35823;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20013;&#25552;&#21462;&#20915;&#31574;&#26641;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#20915;&#31574;&#26641;&#25972;&#21512;&#21040;DRL&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#24403;&#31995;&#32479;&#21457;&#29983;&#38169;&#35823;&#26102;&#23545;&#20854;&#36827;&#34892;&#24809;&#32602;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25105;&#20204;&#26041;&#27861;&#30340;&#27010;&#24565;&#39564;&#35777;&#23454;&#29616;&#65292;&#24182;&#29992;&#23427;&#26469;&#35780;&#20272;&#35813;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has proven extremely useful in a large variety of application domains. However, even successful DRL-based software can exhibit highly undesirable behavior. This is due to DRL training being based on maximizing a reward function, which typically captures general trends but cannot precisely capture, or rule out, certain behaviors of the system. In this paper, we propose a novel framework aimed at drastically reducing the undesirable behavior of DRL-based software, while maintaining its excellent performance. In addition, our framework can assist in providing engineers with a comprehensible characterization of such undesirable behavior. Under the hood, our approach is based on extracting decision tree classifiers from erroneous state-action pairs, and then integrating these trees into the DRL training loop, penalizing the system whenever it performs an error. We provide a proof-of-concept implementation of our approach, and use it to evaluate the techniqu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#38450;&#24481;&#26367;&#20195;&#26041;&#26696;&#65292;&#24341;&#20837;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25200;&#21160;&#36755;&#20986;&#27010;&#29575;&#65292;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#25269;&#24481;&#26368;&#20808;&#36827;&#30340;&#31363;&#21462;&#25915;&#20987;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.01838</link><description>&lt;p&gt;
&#38024;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#39640;&#25928;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Defense Against Model Stealing Attacks on Convolutional Neural Networks. (arXiv:2309.01838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#38450;&#24481;&#26367;&#20195;&#26041;&#26696;&#65292;&#24341;&#20837;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25200;&#21160;&#36755;&#20986;&#27010;&#29575;&#65292;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#25269;&#24481;&#26368;&#20808;&#36827;&#30340;&#31363;&#21462;&#25915;&#20987;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#20854;&#40657;&#30418;API&#26469;&#31363;&#21462;&#24050;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#21644;&#20854;&#20182;&#23433;&#20840;&#19982;&#38544;&#31169;&#39118;&#38505;&#12290;&#30446;&#21069;&#38024;&#23545;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#26368;&#20808;&#36827;&#38450;&#24481;&#26041;&#27861;&#24314;&#35758;&#21521;&#39044;&#27979;&#27010;&#29575;&#28155;&#21152;&#25200;&#21160;&#65292;&#20294;&#20854;&#35745;&#31639;&#36739;&#37325;&#19988;&#23545;&#25915;&#20987;&#32773;&#25552;&#20986;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#36741;&#21161;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#19988;&#36164;&#28304;&#23494;&#38598;&#65292;&#22952;&#30861;&#20102;&#35813;&#38450;&#24481;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#38450;&#24481;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25200;&#21160;&#36755;&#20986;&#27010;&#29575;&#12290;&#35813;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38450;&#24481;&#26041;&#27861;&#22312;&#25269;&#24481;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#31363;&#21462;&#25915;&#20987;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model stealing attacks have become a serious concern for deep learning models, where an attacker can steal a trained model by querying its black-box API. This can lead to intellectual property theft and other security and privacy risks. The current state-of-the-art defenses against model stealing attacks suggest adding perturbations to the prediction probabilities. However, they suffer from heavy computations and make impracticable assumptions about the adversary. They often require the training of auxiliary models. This can be time-consuming and resource-intensive which hinders the deployment of these defenses in real-world applications. In this paper, we propose a simple yet effective and efficient defense alternative. We introduce a heuristic approach to perturb the output probabilities. The proposed defense can be easily integrated into models without additional training. We show that our defense is effective in defending against three state-of-the-art stealing attacks. We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20027;&#35201;&#20197;&#26080;&#31351;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#32593;&#32476;&#30340;&#24615;&#36136;&#12289;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.01592</link><description>&lt;p&gt;
&#22823;&#23610;&#24230;&#21644;&#26080;&#31351;&#23485;&#24230;&#19979;&#30340;&#28145;&#24230;&#23398;&#20064;&#21202;&#35753;&#28436;&#35762;
&lt;/p&gt;
&lt;p&gt;
Les Houches Lectures on Deep Learning at Large &amp; Infinite Width. (arXiv:2309.01592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20027;&#35201;&#20197;&#26080;&#31351;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#32593;&#32476;&#30340;&#24615;&#36136;&#12289;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#12289;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20123;&#28436;&#35762;&#26159;&#22312;2022&#24180;&#21202;&#35753;&#22799;&#23395;&#23398;&#26657;&#32479;&#35745;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#35838;&#31243;&#19978;&#23637;&#31034;&#30340;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#38480;&#23485;&#24230;&#21644;&#22823;&#23485;&#24230;&#33539;&#22260;&#20869;&#30340;&#24773;&#20917;&#12290;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#36825;&#20123;&#32593;&#32476;&#30340;&#21508;&#31181;&#32479;&#35745;&#21644;&#21160;&#21147;&#23398;&#29305;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#35762;&#24072;&#20204;&#35752;&#35770;&#20102;&#38543;&#26426;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65307;&#35757;&#32451;&#36807;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#32447;&#24615;&#27169;&#22411;&#65292;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#20123;&#32852;&#31995;&#22312;&#26080;&#31351;&#23485;&#24230;&#30340;&#26497;&#38480;&#19979;&#20986;&#29616;&#65307;&#20197;&#21450;&#22312;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#21518;&#23545;&#22823;&#20294;&#26377;&#38480;&#23485;&#24230;&#32593;&#32476;&#30340;&#25668;&#21160;&#21644;&#38750;&#25668;&#21160;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
These lectures, presented at the 2022 Les Houches Summer School on Statistical Physics and Machine Learning, focus on the infinite-width limit and large-width regime of deep neural networks. Topics covered include various statistical and dynamical properties of these networks. In particular, the lecturers discuss properties of random deep neural networks; connections between trained deep neural networks, linear models, kernels, and Gaussian processes that arise in the infinite-width limit; and perturbative and non-perturbative treatments of large but finite-width networks, at initialization and after training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20116;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20351;&#29992;BreaKHis&#25968;&#25454;&#38598;&#36827;&#34892;&#20083;&#33146;&#30284;&#35786;&#26029;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;Xception&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#36825;&#34920;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.01007</link><description>&lt;p&gt;
&#20351;&#29992;BreaKHis&#25968;&#25454;&#38598;&#36827;&#34892;&#20083;&#33146;&#30284;&#35786;&#26029;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of Deep Learning Architectures for Breast Cancer Diagnosis Using the BreaKHis Dataset. (arXiv:2309.01007v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20116;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20351;&#29992;BreaKHis&#25968;&#25454;&#38598;&#36827;&#34892;&#20083;&#33146;&#30284;&#35786;&#26029;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;Xception&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#36825;&#34920;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20083;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#19988;&#21361;&#38505;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20197;&#24456;&#22810;&#19981;&#21516;&#30340;&#26041;&#24335;&#34920;&#29616;&#20986;&#26469;&#65292;&#24433;&#21709;&#30528;&#35768;&#22810;&#19981;&#21516;&#30340;&#22120;&#23448;&#21644;&#32452;&#32455;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20351;&#29992;BreakHis&#25968;&#25454;&#38598;&#26469;&#27491;&#30830;&#35782;&#21035;&#20083;&#33146;&#30284;&#30149;&#20363;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;BreakHis&#25968;&#25454;&#38598;&#36890;&#36807;&#20854;&#22823;&#37327;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#29255;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20083;&#33146;&#30284;&#20122;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24182;&#27604;&#36739;&#20102;&#20116;&#20010;&#30693;&#21517;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;VGG&#65292;ResNet&#65292;Xception&#65292;Inception&#21644;InceptionResNet&#65289;&#22312;&#30284;&#30151;&#20998;&#31867;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Xception&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;F1&#24471;&#20998;&#20026;0.9&#65292;&#20934;&#30830;&#29575;&#20026;89%&#12290;&#21516;&#26102;&#65292;Inception&#21644;InceptionResNet&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#37117;&#36798;&#21040;&#20102;87%&#12290;&#28982;&#32780;&#65292;Inception&#27169;&#22411;&#30340;F1&#24471;&#20998;&#20026;87&#65292;&#32780;InceptionResNet&#27169;&#22411;&#30340;F1&#24471;&#20998;&#20026;86&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#27491;&#30830;&#35786;&#26029;&#20083;&#33146;&#30284;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer is an extremely difficult and dangerous health problem because it manifests in so many different ways and affects so many different organs and tissues. The primary goal of this research was to evaluate deep learning models' ability to correctly identify breast cancer cases using the BreakHis dataset. The BreakHis dataset covers a wide range of breast cancer subtypes through its huge collection of histopathological pictures. In this study, we use and compare the performance of five well-known deep learning models for cancer classification: VGG, ResNet, Xception, Inception, and InceptionResNet. The results placed the Xception model at the top, with an F1 score of 0.9 and an accuracy of 89%. At the same time, the Inception and InceptionResNet models both hit accuracy of 87% . However, the F1 score for the Inception model was 87, while that for the InceptionResNet model was 86. These results demonstrate the importance of deep learning methods in making correct breast cancer diagnose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22238;&#24402;&#30340;&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#38754;&#31215;&#20316;&#20026;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00417</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#20013;&#30340;&#38754;&#31215;&#35268;&#33539;COBRA
&lt;/p&gt;
&lt;p&gt;
Area-norm COBRA on Conditional Survival Prediction. (arXiv:2309.00417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22238;&#24402;&#30340;&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#38754;&#31215;&#20316;&#20026;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#32452;&#21512;&#22238;&#24402;&#31574;&#30053;&#26469;&#35745;&#31639;&#26465;&#20214;&#29983;&#23384;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#24369;&#23398;&#20064;&#22120;&#26469;&#21019;&#24314;&#25152;&#25552;&#20986;&#30340;&#38598;&#25104;&#25216;&#26415;&#12290;&#25152;&#25552;&#20986;&#30340;&#32452;&#21512;&#22238;&#24402;&#31574;&#30053;&#20351;&#29992;&#30456;&#20284;&#24230;&#24230;&#37327;&#20316;&#20026;&#20004;&#20010;&#29983;&#23384;&#26354;&#32447;&#20043;&#38388;&#30340;&#38754;&#31215;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#22238;&#24402;&#35774;&#32622;&#20013;&#36873;&#25321;&#26368;&#37325;&#35201;&#21464;&#37327;&#30340;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#27169;&#25311;&#30740;&#31350;&#65292;&#34920;&#26126;&#25105;&#20204;&#23545;&#21464;&#37327;&#30456;&#20851;&#24615;&#30340;&#25552;&#35758;&#25928;&#26524;&#24456;&#22909;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#35828;&#26126;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.16775</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#39044;&#27979;&#30340;&#38646;&#26679;&#26412;NAS&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16775
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#24471;&#21040;&#30340;&#24615;&#33021;&#25351;&#26631;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;one-hot&#32534;&#30721;&#23558;&#21069;&#39304;&#32467;&#26500;&#34920;&#31034;&#20026;&#32452;&#20214;&#22270;&#30340;&#36825;&#20123;&#25351;&#26631;&#38754;&#20020;&#19968;&#20010;&#38480;&#21046;&#65306;&#26080;&#27861;&#22312;&#19981;&#21516;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#25163;&#24037;&#24615;&#33021;&#25351;&#26631;&#65288;&#38646;&#26679;&#26412;NAS&#65289;&#21487;&#20197;&#22312;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#20013;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#26550;&#26500;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;NAS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#36827;&#34892;&#21367;&#31215;&#26680;&#30340;&#32534;&#30721;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20854;&#32467;&#26500;&#31867;&#20284;&#20110;&#27491;&#22312;&#35780;&#20272;&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#32534;&#30721;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#26550;&#26500;&#25299;&#25169;&#20449;&#24687;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#21518;&#65292;&#20276;&#38543;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23545;&#26550;&#26500;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
&lt;/p&gt;</description></item><item><title>SPEED&#26159;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#27969;&#24335;&#36793;&#21010;&#20998;&#21644;&#24182;&#34892;&#21152;&#36895;&#26041;&#24335;&#65292;&#22312;GPU&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#26102;&#38388;&#20132;&#20114;&#22270;&#30340;&#23884;&#20837;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2308.14129</link><description>&lt;p&gt;
SPEED: &#22522;&#20110;&#27969;&#24335;&#21010;&#20998;&#21644;&#24182;&#34892;&#21152;&#36895;&#30340;&#26102;&#38388;&#20132;&#20114;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
SPEED: Streaming Partition and Parallel Acceleration for Temporal Interaction Graph Embedding. (arXiv:2308.14129v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14129
&lt;/p&gt;
&lt;p&gt;
SPEED&#26159;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#27969;&#24335;&#36793;&#21010;&#20998;&#21644;&#24182;&#34892;&#21152;&#36895;&#26041;&#24335;&#65292;&#22312;GPU&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#26102;&#38388;&#20132;&#20114;&#22270;&#30340;&#23884;&#20837;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#20132;&#20114;&#22270;(TIGs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#65292;&#22914;&#37329;&#34701;&#31995;&#32479;&#21644;&#31038;&#20132;&#32593;&#32476;&#12290;&#20026;&#20102;&#25429;&#25417;&#33410;&#28857;&#30340;&#21160;&#24577;&#24615;&#21644;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#29616;&#26377;&#30340;TIG&#23884;&#20837;&#27169;&#22411;&#38656;&#35201;&#25353;&#29031;&#39034;&#24207;&#21644;&#26102;&#38388;&#39034;&#24207;&#22788;&#29702;&#36793;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35201;&#27714;&#38459;&#27490;&#20102;&#24182;&#34892;&#22788;&#29702;&#65292;&#24182;&#19988;&#38590;&#20197;&#36866;&#24212;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#37327;&#21040;GPU&#19978;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#22823;&#35268;&#27169;&#26102;&#38388;&#20132;&#20114;&#22270;&#34987;&#38480;&#21046;&#22312;CPU&#19978;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#31181;&#36890;&#29992;&#30340;GPU&#25193;&#23637;&#21644;&#21152;&#36895;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;GPU&#19978;&#21152;&#36895;&#22823;&#35268;&#27169;TIG&#30340;&#23454;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#27969;&#24335;&#36793;&#21010;&#20998;&#21644;&#24182;&#34892;&#21152;&#36895;&#30340;&#26102;&#38388;&#20132;&#20114;&#22270;&#23884;&#20837;(SPEED)&#12290;SPEED&#30001;&#27969;&#24335;&#36793;&#21010;&#20998;&#32452;&#20214;(SEP)&#21644;&#24182;&#34892;&#21152;&#36895;&#32452;&#20214;(PAC)&#32452;&#25104;&#65292;SEP&#36890;&#36807;&#23558;&#36739;&#23569;&#30340;&#33410;&#28857;&#20998;&#37197;&#32473;&#27599;&#20010;GPU&#26469;&#35299;&#20915;&#31354;&#38388;&#24320;&#38144;&#38382;&#39064;&#65292;&#32780;PAC&#21017;&#23454;&#29616;&#24182;&#34892;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Interaction Graphs (TIGs) are widely employed to model intricate real-world systems such as financial systems and social networks. To capture the dynamism and interdependencies of nodes, existing TIG embedding models need to process edges sequentially and chronologically. However, this requirement prevents it from being processed in parallel and struggle to accommodate burgeoning data volumes to GPU. Consequently, many large-scale temporal interaction graphs are confined to CPU processing. Furthermore, a generalized GPU scaling and acceleration approach remains unavailable. To facilitate large-scale TIGs' implementation on GPUs for acceleration, we introduce a novel training approach namely Streaming Edge Partitioning and Parallel Acceleration for Temporal Interaction Graph Embedding (SPEED). The SPEED is comprised of a Streaming Edge Partitioning Component (SEP) which addresses space overhead issue by assigning fewer nodes to each GPU, and a Parallel Acceleration Component (P
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#29256;&#30340;GentleAdaboost&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#23558;&#24369;&#20998;&#31867;&#22120;&#19982;&#24378;&#20998;&#31867;&#22120;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32447;&#25628;&#32034;&#23558;&#25209;&#22788;&#29702;&#26041;&#27861;&#25193;&#23637;&#20026;&#22312;&#32447;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#20854;&#20182;&#22312;&#32447;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2308.14004</link><description>&lt;p&gt;
&#22312;&#32447;GentleAdaBoost -- &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Online GentleAdaBoost -- Technical Report. (arXiv:2308.14004v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14004
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#29256;&#30340;GentleAdaboost&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26041;&#24335;&#23558;&#24369;&#20998;&#31867;&#22120;&#19982;&#24378;&#20998;&#31867;&#22120;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32447;&#25628;&#32034;&#23558;&#25209;&#22788;&#29702;&#26041;&#27861;&#25193;&#23637;&#20026;&#22312;&#32447;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#20854;&#20182;&#22312;&#32447;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#29256;&#30340;GentleAdaboost&#65292;&#20854;&#20013;&#25105;&#20204;&#20197;&#22312;&#32447;&#26041;&#24335;&#23558;&#19968;&#20010;&#24369;&#20998;&#31867;&#22120;&#19982;&#19968;&#20010;&#24378;&#20998;&#31867;&#22120;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#32447;&#25628;&#32034;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#23558;&#25209;&#22788;&#29702;&#26041;&#27861;&#25193;&#23637;&#20026;&#22312;&#32447;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#22312;&#32447;boosting&#26041;&#27861;&#19982;&#20854;&#20182;&#22312;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the online variant of GentleAdaboost, where we combine a weak learner to a strong learner in an online fashion. We provide an approach to extend the batch approach to an online approach with theoretical justifications through application of line search. Finally we compare our online boosting approach with other online approaches across a variety of benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLLM-DataEngine&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#24369;&#28857;&#65292;&#29983;&#25104;&#36866;&#24403;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.13566</link><description>&lt;p&gt;
MLLM-DataEngine&#65306;&#19968;&#31181;MLLM&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MLLM-DataEngine: An Iterative Refinement Approach for MLLM. (arXiv:2308.13566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLLM-DataEngine&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#24369;&#28857;&#65292;&#29983;&#25104;&#36866;&#24403;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25351;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#29420;&#31435;&#24615;&#20351;&#24471;&#24403;&#21069;&#30340;MLLM&#24456;&#38590;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#20154;&#21147;&#25104;&#26412;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23553;&#38381;&#24490;&#29615;&#31995;&#32479;MLLM-DataEngine&#65292;&#23427;&#36830;&#25509;&#20102;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22312;&#27599;&#20010;&#24490;&#29615;&#36845;&#20195;&#20013;&#65292;MLLM-DataEngine&#39318;&#20808;&#26681;&#25454;&#35780;&#20272;&#32467;&#26524;&#20998;&#26512;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#28982;&#21518;&#29983;&#25104;&#21512;&#36866;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#29992;&#20110;&#19979;&#19968;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#19982;&#22522;&#20934;&#27979;&#35797;&#20998;&#31163;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#37117;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24809;&#32602;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#25514;&#26045;&#12290;&#22312;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13451</link><description>&lt;p&gt;
&#25235;&#20303;&#23427;&#20204;&#65306;&#22270;&#21305;&#37197;&#21305;&#37197;&#28388;&#27874;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gotta match 'em all: Solution diversification in graph matching matched filters. (arXiv:2308.13451v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24809;&#32602;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#25514;&#26045;&#12290;&#22312;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#24120;&#22823;&#30340;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#22312;&#20854;&#20013;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Sussman&#31561;&#20154;&#25552;&#20986;&#30340;&#22270;&#21305;&#37197;&#21305;&#37197;&#28388;&#27874;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#21305;&#37197;&#28388;&#27874;&#31639;&#27861;&#20013;&#36845;&#20195;&#22320;&#24809;&#32602;&#21512;&#36866;&#30340;&#33410;&#28857;&#23545;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25105;&#20204;&#30340;&#21305;&#37197;&#28388;&#27874;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#22312;&#30456;&#20851;&#30340;Erdos-Renyi&#22270;&#35774;&#32622;&#20013;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#19978;&#30340;&#39564;&#35777;&#65292;&#26174;&#31034;&#20854;&#22312;&#28201;&#21644;&#30340;&#27169;&#22411;&#26465;&#20214;&#19979;&#33021;&#22815;&#39034;&#24207;&#22320;&#21457;&#29616;&#22810;&#20010;&#27169;&#26495;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#27169;&#22411;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#20154;&#33041;&#36830;&#25509;&#32452;&#21644;&#22823;&#22411;&#20132;&#26131;&#30693;&#35782;&#24211;&#65289;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;VAE&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#20013;&#24674;&#22797;&#20302;&#32500;&#27969;&#24418;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#20998;&#23376;&#30340;&#23646;&#24615;&#32479;&#35745;&#65292;&#32780;&#26080;&#38656;&#38598;&#25104;&#23646;&#24615;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.13066</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#38454;&#27573;VAE&#23545;&#20998;&#23376;&#23646;&#24615;&#36827;&#34892;&#23458;&#35266;&#26080;&#20851;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE. (arXiv:2308.13066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;VAE&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#20013;&#24674;&#22797;&#20302;&#32500;&#27969;&#24418;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#20998;&#23376;&#30340;&#23646;&#24615;&#32479;&#35745;&#65292;&#32780;&#26080;&#38656;&#38598;&#25104;&#23646;&#24615;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#33647;&#29289;&#21457;&#29616;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26550;&#26500;&#21644;&#27969;&#31243;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;VAE&#26041;&#27861;&#22312;&#25968;&#25454;&#20301;&#20110;&#39640;&#32500;&#29615;&#22659;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#20302;&#32500;&#27969;&#24418;&#19978;&#26102;&#65292;&#24456;&#38590;&#24674;&#22797;&#27969;&#24418;&#12290;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#65292;&#36825;&#19968;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#27969;&#24418;&#24674;&#22797;&#30340;&#22810;&#38454;&#27573;VAE&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#21040;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;ChEMBL&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#22810;&#38454;&#27573;VAE&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#23558;&#23646;&#24615;&#39044;&#27979;&#22120;&#32435;&#20837;&#35757;&#32451;&#27969;&#31243;&#30340;&#21069;&#25552;&#19979;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25152;&#29983;&#25104;&#20998;&#23376;&#30340;&#23646;&#24615;&#32479;&#35745;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20004;&#20010;&#31934;&#24515;&#31574;&#21010;&#19988;&#36739;&#23567;&#30340;&#20998;&#23376;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#38024;&#23545;&#19981;&#21516;&#30340;&#34507;&#30333;&#36136;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#23646;&#24615;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoder (VAE) is a popular method for drug discovery and various architectures and pipelines have been proposed to improve its performance. However, VAE approaches are known to suffer from poor manifold recovery when the data lie on a low-dimensional manifold embedded in a higher dimensional ambient space [Dai and Wipf, 2019]. The consequences of it in drug discovery are somewhat under-explored. In this paper, we explore applying a multi-stage VAE approach, that can improve manifold recovery on a synthetic dataset, to the field of drug discovery. We experimentally evaluate our multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability to improve the property statistics of generated molecules substantially from pre-existing methods without incorporating property predictors into the training pipeline. We further fine-tune our models on two curated and much smaller molecule datasets that target different proteins. Our experiments show an increase in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.13032</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 GPT&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 Large Language Model (LLM) &#23545;&#37329;&#34701;&#26032;&#38395;&#36827;&#34892;&#22810;&#20219;&#21153;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;PEFT/LoRA&#26041;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#20027;&#35201;&#21253;&#25324;&#20174;&#37329;&#34701;&#24066;&#22330;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#12289;&#31361;&#20986;&#25991;&#26412;&#30340;&#20027;&#35201;&#35266;&#28857;&#12289;&#23545;&#25991;&#26412;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20855;&#26377;&#36866;&#24403;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#31561;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#22810;&#20219;&#21153;&#30340;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;&#65292;&#20854;&#21709;&#24212;&#30340;&#32467;&#26500;&#21487;&#20197;&#37096;&#20998;&#20026;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#21478;&#19968;&#37096;&#20998;&#25968;&#25454;&#21487;&#20197;&#37319;&#29992;JSON&#26684;&#24335;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#23450;&#37327;&#30446;&#26631;&#21464;&#37327;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;85.9%&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#20102;&#20854;&#24615;&#33021;&#19982;wav2vec2&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11241</link><description>&lt;p&gt;
&#19968;&#20010;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification. (arXiv:2308.11241v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#21644;&#26102;&#38388;&#38376;&#27744;&#21270;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#65292;&#24182;&#22312;&#20934;&#30830;&#29575;85.9%&#30340;&#24773;&#20917;&#19979;&#27604;&#36739;&#20102;&#20854;&#24615;&#33021;&#19982;wav2vec2&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wav2vec2&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;Transformer&#26550;&#26500;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26368;&#36817;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#65292;&#36824;&#29992;&#20110;&#25972;&#20010;&#35821;&#38899;&#22788;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;&#26377;&#25928;&#31471;&#21040;&#31471;&#35828;&#35805;&#20154;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21442;&#25968;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#30830;&#23450;&#19968;&#20010;&#26377;&#25928;&#27169;&#22411;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#23398;&#20064;&#33021;&#21147;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#26102;&#38388;&#38376;&#27744;&#21270;(Temporal Gate Pooling)&#65292;&#29992;&#20110;&#35828;&#35805;&#20154;&#35782;&#21035;&#12290;&#25105;&#20204;&#23558;Conformer&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24182;&#21033;&#29992;BEST-RQ&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;VoxCeleb1&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;&#20165;&#26377;28.5M&#20010;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;85.9%&#30340;&#20934;&#30830;&#29575;&#65292;&#19982;&#20855;&#26377;317.7M&#20010;&#21442;&#25968;&#30340;wav2vec2&#30456;&#24403;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/HarunoriKawano/speaker-identification-with-tgp&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;(DMD)&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;Koopman&#26694;&#26550;&#20013;&#24341;&#20837;&#26041;&#24046;&#26469;&#35299;&#20915;&#25361;&#25112;&#65292;&#21253;&#25324;&#34394;&#20551;&#27169;&#24335;&#21644;&#26412;&#24449;&#35889;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38656;&#35201;&#36229;&#36234;&#39044;&#26399;&#26469;&#26377;&#25928;&#22788;&#29702;&#38543;&#26426;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.10697</link><description>&lt;p&gt;
&#36229;&#36234;&#39044;&#26399;&#65306;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#30340;&#21097;&#20313;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;&#21644;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems. (arXiv:2308.10697v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;(DMD)&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;Koopman&#26694;&#26550;&#20013;&#24341;&#20837;&#26041;&#24046;&#26469;&#35299;&#20915;&#25361;&#25112;&#65292;&#21253;&#25324;&#34394;&#20551;&#27169;&#24335;&#21644;&#26412;&#24449;&#35889;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38656;&#35201;&#36229;&#36234;&#39044;&#26399;&#26469;&#26377;&#25928;&#22788;&#29702;&#38543;&#26426;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#31526;&#23558;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#32447;&#24615;&#21270;&#65292;&#20351;&#24471;&#20854;&#35889;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#31639;&#27861;&#26469;&#36817;&#20284;&#36825;&#20123;&#35889;&#29305;&#24615;&#65292;&#20854;&#20013;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;(DMD)&#26159;&#22522;&#20110;&#25237;&#24433;&#26041;&#27861;&#30340;&#20856;&#22411;&#20195;&#34920;&#12290;&#23613;&#31649;Koopman&#31639;&#31526;&#26412;&#36523;&#26159;&#32447;&#24615;&#30340;&#65292;&#20294;&#23427;&#22312;&#26080;&#31351;&#32500;&#30340;&#21487;&#35266;&#27979;&#31354;&#38388;&#20013;&#36215;&#20316;&#29992;&#65292;&#36825;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#21253;&#25324;&#34394;&#20551;&#27169;&#24335;&#65292;&#26412;&#24449;&#35889;&#21644;Koopman&#27169;&#24335;&#20998;&#35299;&#30340;&#39564;&#35777;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#30830;&#23450;&#24615;&#31995;&#32479;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#23545;&#20110;&#38543;&#26426;&#31995;&#32479;&#30340;&#24050;&#39564;&#35777;&#30340;DMD&#26041;&#27861;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#65292;&#20854;&#20013;Koopman&#31639;&#31526;&#27979;&#37327;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#38656;&#35201;&#36229;&#36234;&#39044;&#26399;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#26041;&#24046;&#32435;&#20837;Koopman&#26694;&#26550;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#39069;&#22806;&#30340;&#31867;&#20284;DMD&#30340;&#30697;&#38453;&#65292;&#25105;&#20204;&#36817;&#20284;&#19968;&#20010;&#27531;&#24046;&#24179;&#26041;&#21644;&#30340;&#21644;&#20197;&#21450;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Koopman operators linearize nonlinear dynamical systems, making their spectral information of crucial interest. Numerous algorithms have been developed to approximate these spectral properties, and Dynamic Mode Decomposition (DMD) stands out as the poster child of projection-based methods. Although the Koopman operator itself is linear, the fact that it acts in an infinite-dimensional space of observables poses challenges. These include spurious modes, essential spectra, and the verification of Koopman mode decompositions. While recent work has addressed these challenges for deterministic systems, there remains a notable gap in verified DMD methods for stochastic systems, where the Koopman operator measures the expectation of observables. We show that it is necessary to go beyond expectations to address these issues. By incorporating variance into the Koopman framework, we address these challenges. Through an additional DMD-type matrix, we approximate the sum of a squared residual and 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#20266;&#26631;&#31614;&#23398;&#20064;&#30340;&#38750;&#20405;&#20837;&#24335;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65288;MTQ-Net&#65289;&#65292;&#36890;&#36807;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#20266;&#26631;&#31614;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.09262</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#20266;&#26631;&#31614;&#23398;&#20064;&#22312;&#38750;&#20405;&#20837;&#24335;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model. (arXiv:2308.09262v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#20266;&#26631;&#31614;&#23398;&#20064;&#30340;&#38750;&#20405;&#20837;&#24335;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65288;MTQ-Net&#65289;&#65292;&#36890;&#36807;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#20266;&#26631;&#31614;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#20266;&#26631;&#31614;&#23398;&#20064;&#65288;MPL&#65289;&#30340;&#38750;&#20405;&#20837;&#24335;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;MTQ-Net&#12290;MPL&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#33719;&#21462;&#20266;&#26631;&#31614;&#20998;&#25968;&#21644;&#25191;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#35780;&#20272;&#30446;&#26631;&#26159;&#19977;&#20010;3QUEST&#25351;&#26631;&#65292;&#21363;&#35821;&#38899;MOS&#65288;S-MOS&#65289;&#65292;&#22122;&#22768;MOS&#65288;N-MOS&#65289;&#21644;&#36890;&#29992;MOS&#65288;G-MOS&#65289;&#12290;&#39044;&#35757;&#32451;&#30340;MOSA-Net&#27169;&#22411;&#34987;&#29992;&#26469;&#20272;&#35745;&#19977;&#20010;&#20266;&#26631;&#31614;&#65306;&#20027;&#35266;&#35780;&#20272;&#35821;&#38899;&#36136;&#37327;&#65288;PESQ&#65289;&#65292;&#30701;&#26102;&#23458;&#35266;&#21487;&#25026;&#24615;&#65288;STOI&#65289;&#21644;&#35821;&#38899;&#22833;&#30495;&#25351;&#25968;&#65288;SDI&#65289;&#12290;&#28982;&#21518;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#36890;&#36807;&#32467;&#21512;&#26377;&#30417;&#30563;&#25439;&#22833;&#65288;&#36890;&#36807;&#20272;&#35745;&#20998;&#25968;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#24046;&#24322;&#23548;&#20986;&#65289;&#21644;&#21322;&#30417;&#30563;&#25439;&#22833;&#65288;&#36890;&#36807;&#20272;&#35745;&#20998;&#25968;&#19982;&#20266;&#26631;&#31614;&#20043;&#38388;&#30340;&#24046;&#24322;&#23548;&#20986;&#65289;&#26469;&#35757;&#32451;MTQ-Net&#65292;&#20854;&#20013;&#20351;&#29992;Huber&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#39318;&#20808;&#35777;&#26126;&#20102;MPL&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a multi-task pseudo-label learning (MPL)-based non-intrusive speech quality assessment model called MTQ-Net. MPL consists of two stages: obtaining pseudo-label scores from a pretrained model and performing multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS), Noise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The pretrained MOSA-Net model is utilized to estimate three pseudo labels: perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and speech distortion index (SDI). Multi-task learning is then employed to train MTQ-Net by combining a supervised loss (derived from the difference between the estimated score and the ground-truth label) and a semi-supervised loss (derived from the difference between the estimated score and the pseudo label), where the Huber loss is employed as the loss function. Experimental results first demonstrate the advantages of MPL compared to training a model from scra
&lt;/p&gt;</description></item><item><title>SciRE-Solver&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#37319;&#26679;&#36807;&#31243;&#32531;&#24930;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07896</link><description>&lt;p&gt;
SciRE-Solver: &#29992;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation. (arXiv:2308.07896v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07896
&lt;/p&gt;
&lt;p&gt;
SciRE-Solver&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#37319;&#26679;&#36807;&#31243;&#32531;&#24930;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20854;&#29983;&#25104;&#39640;&#20445;&#30495;&#22270;&#20687;&#26679;&#26412;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;DPMs&#30340;&#23454;&#29616;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#37319;&#26679;&#36807;&#31243;&#32531;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;DPMs&#37319;&#26679;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38024;&#23545;&#19982;DPMs&#37319;&#26679;&#36807;&#31243;&#23545;&#24212;&#30340;&#25193;&#25955;ODE&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24471;&#20998;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#20026;&#27714;&#35299;&#25193;&#25955;ODE&#30340;&#25968;&#20540;&#31639;&#27861;&#24320;&#21457;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#37319;&#26679;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;(RDE)&#26041;&#27861;&#26469;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#33539;&#24335;&#21644;RDE&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#25910;&#25947;&#39034;&#24207;&#20445;&#35777;&#30340;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;(SciRE-Solver)&#26469;&#35299;&#20915;&#25193;&#25955;ODEs&#12290;SciRE-Solver&#22312;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;DPMs&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#24615;&#33021;&#65292;&#24182;&#19988;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#30340;&#24471;&#20998;&#20989;&#25968;&#35780;&#20272;(NFE)&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) are a powerful class of generative models known for their ability to generate high-fidelity image samples. A major challenge in the implementation of DPMs is the slow sampling process. In this work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, which introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. To achieve an efficient sampler, we propose a recursive derivative estimation (RDE) method to reduce the estimation error. With our proposed solution paradigm and RDE method, we propose the score-integrand solver with the convergence order guarantee as efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver attains state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#21018;&#20307;&#35282;&#33394;&#20223;&#30495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#33258;&#36866;&#24212;&#21508;&#31181;&#29615;&#22659;&#21464;&#21270;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.07491</link><description>&lt;p&gt;
&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23545;&#21333;&#21018;&#20307;&#35282;&#33394;&#30340;&#33258;&#36866;&#24212;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Adaptive Tracking of a Single-Rigid-Body Character in Various Environments. (arXiv:2308.07491v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#21018;&#20307;&#35282;&#33394;&#20223;&#30495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#33258;&#36866;&#24212;&#21508;&#31181;&#29615;&#22659;&#21464;&#21270;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;DeepMimic&#30340;&#24341;&#20837;&#20197;&#26469;&#65292;&#21518;&#32493;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#25193;&#23637;&#27169;&#25311;&#21160;&#20316;&#30340;&#33539;&#30068;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#26041;&#27861;&#65292;&#19968;&#31181;&#22522;&#20110;&#21333;&#21018;&#20307;&#35282;&#33394;&#20223;&#30495;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#21033;&#29992;&#36136;&#24515;&#21160;&#21147;&#23398;&#27169;&#22411;&#65288;CDM&#65289;&#23558;&#20840;&#36523;&#35282;&#33394;&#34920;&#31034;&#20026;&#21333;&#21018;&#20307;&#65288;SRB&#65289;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#36319;&#36394;&#21442;&#32771;&#21160;&#20316;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#26410;&#35266;&#27979;&#29615;&#22659;&#21464;&#21270;&#21644;&#25511;&#21046;&#22120;&#36716;&#25442;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;&#30001;&#20110;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#38477;&#32500;&#65292;&#23398;&#20064;&#36807;&#31243;&#20855;&#26377;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#26368;&#32456;&#30340;&#20840;&#36523;&#21160;&#20316;&#20197;&#29289;&#29702;&#21512;&#29702;&#30340;&#26041;&#24335;&#22522;&#20110;&#27169;&#25311;SRB&#35282;&#33394;&#30340;&#29366;&#24577;&#36827;&#34892;&#36816;&#21160;&#29983;&#25104;&#12290;SRB&#20223;&#30495;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;&#31574;&#30053;&#36755;&#20986;&#19968;&#20010;&#21160;&#20316;&#65292;&#20801;&#35768;&#35282;&#33394;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of DeepMimic [Peng et al. 2018], subsequent research has focused on expanding the repertoire of simulated motions across various scenarios. In this study, we propose an alternative approach for this goal, a deep reinforcement learning method based on the simulation of a single-rigid-body character. Using the centroidal dynamics model (CDM) to express the full-body character as a single rigid body (SRB) and training a policy to track a reference motion, we can obtain a policy that is capable of adapting to various unobserved environmental changes and controller transitions without requiring any additional learning. Due to the reduced dimension of state and action space, the learning process is sample-efficient. The final full-body motion is kinematically generated in a physically plausible way, based on the state of the simulated SRB character. The SRB simulation is formulated as a quadratic programming (QP) problem, and the policy outputs an action that allows th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#34880;&#27969;&#21160;&#21147;&#23398;&#25512;&#29702;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#20405;&#20837;&#24615;&#20449;&#21495;&#36827;&#34892;&#24515;&#33039;&#21387;&#21147;&#30340;&#35780;&#20272;&#65292;&#26082;&#21487;&#20197;&#22312;&#20303;&#38498;&#29615;&#22659;&#20013;&#29992;&#20110;&#35786;&#26029;&#21644;&#27835;&#30103;&#39044;&#21518;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38376;&#35786;&#35774;&#32622;&#20013;&#12290;</title><link>http://arxiv.org/abs/2308.04650</link><description>&lt;p&gt;
&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#34880;&#27969;&#21160;&#21147;&#23398;&#25512;&#29702;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Metric Learning for the Hemodynamics Inference with Electrocardiogram Signals. (arXiv:2308.04650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#34880;&#27969;&#21160;&#21147;&#23398;&#25512;&#29702;&#30340;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#38750;&#20405;&#20837;&#24615;&#20449;&#21495;&#36827;&#34892;&#24515;&#33039;&#21387;&#21147;&#30340;&#35780;&#20272;&#65292;&#26082;&#21487;&#20197;&#22312;&#20303;&#38498;&#29615;&#22659;&#20013;&#29992;&#20110;&#35786;&#26029;&#21644;&#27835;&#30103;&#39044;&#21518;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#38376;&#35786;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#26159;&#19968;&#31181;&#24433;&#21709;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#30340;&#33268;&#27531;&#30142;&#24739;&#65292;&#23545;&#20182;&#20204;&#30340;&#29983;&#27963;&#36136;&#37327;&#21644;&#27515;&#20129;&#29575;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#24515;&#33039;&#21387;&#21147;&#30340;&#23458;&#35266;&#35780;&#20272;&#20173;&#28982;&#26159;&#35786;&#26029;&#21644;&#27835;&#30103;&#39044;&#21518;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#23613;&#31649;&#24515;&#33039;&#23548;&#31649;&#21270;&#39564;&#26159;&#20272;&#35745;&#20013;&#24515;&#34880;&#28082;&#21160;&#21147;&#23398;&#21387;&#21147;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#23427;&#26159;&#19968;&#31181;&#26377;&#28508;&#22312;&#39118;&#38505;&#30340;&#20405;&#20837;&#24615;&#25805;&#20316;&#65292;&#23545;&#26576;&#20123;&#24739;&#32773;&#26469;&#35828;&#21487;&#33021;&#26159;&#21361;&#38505;&#30340;&#12290;&#21033;&#29992;&#38750;&#20405;&#20837;&#24615;&#20449;&#21495;&#65288;&#22914;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#65289;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#24120;&#35268;&#20272;&#35745;&#24515;&#33039;&#21387;&#21147;&#22312;&#20303;&#38498;&#21644;&#38376;&#35786;&#29615;&#22659;&#20013;&#25104;&#20026;&#21487;&#33021;&#12290;&#20808;&#21069;&#35757;&#32451;&#29992;&#20110;&#20197;&#30417;&#30563;&#30340;&#26041;&#24335;&#20272;&#35745;&#24515;&#33039;&#20869;&#21387;&#21147;&#65288;&#20363;&#22914;&#24179;&#22343;&#32954;&#27611;&#32454;&#34880;&#31649;&#26964;&#21387;&#65288;mPCWP&#65289;&#65289;&#30340;&#27169;&#22411;&#26174;&#31034;&#20102;&#33391;&#22909;&#30340;&#37492;&#21035;&#33021;&#21147;&#65292;&#20294;&#20165;&#38480;&#20110;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart failure is a debilitating condition that affects millions of people worldwide and has a significant impact on their quality of life and mortality rates. An objective assessment of cardiac pressures remains an important method for the diagnosis and treatment prognostication for patients with heart failure. Although cardiac catheterization is the gold standard for estimating central hemodynamic pressures, it is an invasive procedure that carries inherent risks, making it a potentially dangerous procedure for some patients. Approaches that leverage non-invasive signals - such as electrocardiogram (ECG) - have the promise to make the routine estimation of cardiac pressures feasible in both inpatient and outpatient settings. Prior models trained to estimate intracardiac pressures (e.g., mean pulmonary capillary wedge pressure (mPCWP)) in a supervised fashion have shown good discriminatory ability but have been limited to the labeled dataset from the heart failure cohort. To address th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;SYNAuG&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.00994</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65306;&#22522;&#20110;&#25968;&#25454;&#35282;&#24230;&#30340;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective. (arXiv:2308.00994v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;SYNAuG&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#29983;&#27963;&#22312;&#19968;&#20010;&#24222;&#22823;&#30340;&#25968;&#25454;&#28023;&#27915;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20063;&#19981;&#20363;&#22806;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#34920;&#29616;&#20986;&#19968;&#31181;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#29616;&#35937;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#32473;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#20559;&#35265;&#30340;&#39044;&#27979;&#24102;&#26469;&#20102;&#39118;&#38505;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#36947;&#24503;&#21644;&#31038;&#20250;&#21518;&#26524;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#37492;&#20110;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;SYNAuG&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#20043;&#21069;&#20351;&#29992;&#30340;&#20219;&#21153;&#29305;&#23450;&#31639;&#27861;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;&#36825;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#22312;CIFAR100-LT&#12289;ImageNet100-LT&#12289;UTKFace&#21644;Waterbird&#31561;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20219;&#21153;&#29305;&#23450;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#25105;&#20204;&#24182;&#19981;&#22768;&#31216;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#38382;&#39064;&#30340;&#23436;&#25972;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
We live in a vast ocean of data, and deep neural networks are no exception to this. However, this data exhibits an inherent phenomenon of imbalance. This imbalance poses a risk of deep neural networks producing biased predictions, leading to potentially severe ethical and social consequences. To address these challenges, we believe that the use of generative models is a promising approach for comprehending tasks, given the remarkable advancements demonstrated by recent diffusion models in generating high-quality images. In this work, we propose a simple yet effective baseline, SYNAuG, that utilizes synthetic data as a preliminary step before employing task-specific algorithms to address data imbalance problems. This straightforward approach yields impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT, UTKFace, and Waterbird, surpassing the performance of existing task-specific methods. While we do not claim that our approach serves as a complete solution to the problem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2307.15217</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback. (arXiv:2307.15217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21152;&#24378;&#31038;&#20250;&#30417;&#30563;&#30340;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#30340;&#25216;&#26415;&#12290;RLHF&#24050;&#25104;&#20026;&#24494;&#35843;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26680;&#24515;&#26041;&#27861;&#12290;&#23613;&#31649;&#22914;&#27492;&#21463;&#27426;&#36814;&#65292;&#20294;&#31995;&#32479;&#24615;&#22320;&#31995;&#32479;&#21270;&#20854;&#32570;&#38519;&#30340;&#20844;&#24320;&#24037;&#20316;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#35843;&#26597;&#20102;RLHF&#21450;&#30456;&#20851;&#26041;&#27861;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#22522;&#26412;&#38480;&#21046;&#65307;&#65288;2&#65289;&#27010;&#36848;&#20102;&#20102;&#35299;&#12289;&#25913;&#36827;&#21644;&#34917;&#20805;RLHF&#30340;&#23454;&#36341;&#25216;&#26415;&#65307;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20986;&#20102;&#23457;&#35745;&#21644;&#25259;&#38706;&#26631;&#20934;&#20197;&#25913;&#36827;RLHF&#31995;&#32479;&#30340;&#31038;&#20250;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;RLHF&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20197;&#22810;&#26041;&#38754;&#26041;&#27861;&#24320;&#21457;&#26356;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.
&lt;/p&gt;</description></item><item><title>InVAErt&#32593;&#32476;&#26159;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#21512;&#25104;&#29289;&#29702;&#31995;&#32479;&#65292;&#20855;&#26377;&#27169;&#22411;&#21453;&#28436;&#21644;&#21487;&#35782;&#21035;&#24615;&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.12586</link><description>&lt;p&gt;
InVAErt&#32593;&#32476;&#65306;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#29992;&#20110;&#20223;&#30495;&#12289;&#25512;&#29702;&#21644;&#21487;&#35782;&#21035;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
InVAErt networks: a data-driven framework for emulation, inference and identifiability analysis. (arXiv:2307.12586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12586
&lt;/p&gt;
&lt;p&gt;
InVAErt&#32593;&#32476;&#26159;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#21512;&#25104;&#29289;&#29702;&#31995;&#32479;&#65292;&#20855;&#26377;&#27169;&#22411;&#21453;&#28436;&#21644;&#21487;&#35782;&#21035;&#24615;&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22522;&#20110;&#29289;&#29702;&#30340;&#31995;&#32479;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#20027;&#35201;&#29992;&#20110;&#20223;&#30495;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#32467;&#26500;&#25552;&#20379;&#30340;&#20986;&#33394;&#28789;&#27963;&#24615;&#34920;&#26126;&#24212;&#23558;&#35813;&#34920;&#31034;&#25193;&#23637;&#21040;&#31995;&#32479;&#32508;&#21512;&#30340;&#20854;&#20182;&#26041;&#38754;&#65292;&#21253;&#25324;&#27169;&#22411;&#21453;&#28436;&#21644;&#21487;&#35782;&#21035;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;InVAErt&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#21644;&#21512;&#25104;&#21442;&#25968;&#21270;&#29289;&#29702;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#30830;&#23450;&#24615;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#34920;&#31034;&#21069;&#21521;&#21644;&#36870;&#21521;&#35299;&#26144;&#23556;&#65292;&#29992;&#24402;&#19968;&#21270;&#27969;&#26469;&#25429;&#25417;&#31995;&#32479;&#36755;&#20986;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21464;&#20998;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#32570;&#20047;&#21452;&#23556;&#24615;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#20102;&#25439;&#22833;&#20989;&#25968;&#20013;&#24809;&#32602;&#31995;&#25968;&#30340;&#36873;&#25321;&#21644;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#31574;&#30053;&#65292;&#22240;&#20026;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#22240;&#32032;&#20250;&#26174;&#33879;&#24433;&#21709;&#35757;&#32451;&#21644;&#27979;&#35797;&#24615;&#33021;&#12290;&#25105;&#20204;&#26377;&#25928;&#22320;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Use of generative models and deep learning for physics-based systems is currently dominated by the task of emulation. However, the remarkable flexibility offered by data-driven architectures would suggest to extend this representation to other aspects of system synthesis including model inversion and identifiability. We introduce inVAErt (pronounced \emph{invert}) networks, a comprehensive framework for data-driven analysis and synthesis of parametric physical systems which uses a deterministic encoder and decoder to represent the forward and inverse solution maps, normalizing flow to capture the probabilistic distribution of system outputs, and a variational encoder designed to learn a compact latent representation for the lack of bijectivity between inputs and outputs. We formally investigate the selection of penalty coefficients in the loss function and strategies for latent space sampling, since we find that these significantly affect both training and testing performance. We valid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;PIP-Net&#24320;&#23637;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;PIP-Net&#33021;&#22815;&#36731;&#26494;&#35782;&#21035;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.10404</link><description>&lt;p&gt;
&#20351;&#29992;PIP-Net&#35299;&#37322;&#21644;&#32416;&#27491;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Interpreting and Correcting Medical Image Classification with PIP-Net. (arXiv:2307.10404v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;PIP-Net&#24320;&#23637;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;PIP-Net&#33021;&#22815;&#36731;&#26494;&#35782;&#21035;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#21457;&#29616;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#24615;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#26159;&#40657;&#30418;&#20154;&#24037;&#26234;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#30340;&#33258;&#21160;&#35786;&#26029;&#25903;&#25345;&#12290;PIP-Net&#23398;&#20064;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20856;&#22411;&#22270;&#20687;&#37096;&#20998;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#39592;&#25240;&#26816;&#27979;&#21644;&#30382;&#32932;&#30284;&#35786;&#26029;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;PIP-Net&#30340;&#20915;&#31574;&#36807;&#31243;&#31526;&#21512;&#21307;&#23398;&#20998;&#31867;&#26631;&#20934;&#65292;&#20165;&#25552;&#20379;&#22270;&#20687;&#32423;&#21035;&#30340;&#31867;&#26631;&#31614;&#12290;&#30001;&#20110;PIP-Net&#23545;&#21407;&#22411;&#36827;&#34892;&#20102;&#26080;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#65292;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;X&#20809;&#20013;&#30340;&#19981;&#33391;&#25991;&#26412;&#25110;&#26631;&#31614;&#38169;&#35823;&#31561;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#26174;&#31034;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#31105;&#29992;&#19981;&#33391;&#21407;&#22411;&#26469;&#25163;&#21160;&#32416;&#27491;PIP-Net&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#37096;&#20998;&#21407;&#22411;&#27169;&#22411;&#23545;&#21307;&#23398;&#24212;&#29992;&#20855;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#30456;&#20114;&#21442;&#32771;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Part-prototype models are explainable-by-design image classifiers, and a promising alternative to black box AI. This paper explores the applicability and potential of interpretable machine learning, in particular PIP-Net, for automated diagnosis support on real-world medical imaging data. PIP-Net learns human-understandable prototypical image parts and we evaluate its accuracy and interpretability for fracture detection and skin cancer diagnosis. We find that PIP-Net's decision making process is in line with medical classification standards, while only provided with image-level class labels. Because of PIP-Net's unsupervised pretraining of prototypes, data quality problems such as undesired text in an X-ray or labelling errors can be easily identified. Additionally, we are the first to show that humans can manually correct the reasoning of PIP-Net by directly disabling undesired prototypes. We conclude that part-prototype models are promising for medical applications due to their inter
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.07686</link><description>&lt;p&gt;
&#21019;&#24314;&#19968;&#20010;&#25903;&#25345;OpenMP Fortran&#21644;C++&#20195;&#30721;&#30456;&#20114;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#30830;&#20445;&#20102;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;OpenMP Fortran&#21644;C++&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#31934;&#32454;&#30340;&#20195;&#30721;&#30456;&#20284;&#24615;&#27979;&#35797;&#65292;&#25105;&#20204;&#30830;&#20445;&#20102;&#25968;&#25454;&#38598;&#30340;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23450;&#37327;&#65288;CodeBLEU&#65289;&#21644;&#23450;&#24615;&#65288;&#20154;&#24037;&#35780;&#20272;&#65289;&#26041;&#27861;&#35780;&#20272;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#32534;&#30721;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;5.1&#20493;&#65292;&#23545;&#20110;&#20855;&#26377;&#19968;&#23450;&#32534;&#30721;&#29087;&#24713;&#24230;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;9.9&#20493;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#20195;&#30721;&#32763;&#35793;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a novel dataset for training machine learning models translating between OpenMP Fortran and C++ code. To ensure reliability and applicability, the dataset is initially refined using a meticulous code similarity test. The effectiveness of our dataset is assessed using both quantitative (CodeBLEU) and qualitative (human evaluation) methods. We demonstrate how this dataset can significantly improve the translation capabilities of large-scale language models, with improvements of \times 5.1 for models with no prior coding knowledge and \times 9.9 for models with some coding familiarity. Our work highlights the potential of this dataset to advance the field of code translation for high-performance computing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;$k$-WL&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22806;&#24310;&#23450;&#20041;&#21644;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.05775</link><description>&lt;p&gt;
Weisfeiler&#21644;Lehman&#36827;&#34892;&#24230;&#37327;&#24314;&#27169;&#65306;&#25506;&#32034;WL&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test. (arXiv:2307.05775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;$k$-WL&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22806;&#24310;&#23450;&#20041;&#21644;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#27604;&#36739;&#19968;&#20010;&#26550;&#26500;&#33021;&#22815;&#21306;&#20998;&#30340;&#38750;&#21516;&#26500;&#22270;&#25110;&#33410;&#28857;&#23545;&#30340;&#25968;&#37327;&#19982;$k$-&#32500;Weisfeiler-Lehman ($k$-WL)&#27979;&#35797;&#33021;&#22815;&#21306;&#20998;&#30340;&#25968;&#37327;&#26469;&#34913;&#37327;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;$k$-WL&#30340;&#21487;&#38752;&#24615;&#21644;&#26377;&#25928;&#24615;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20174;&#19994;&#32773;&#23545;&#34920;&#36798;&#33021;&#21147;&#21644;$k$-WL&#30340;&#27010;&#24565;&#21270;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#20102;&#23545;&#20174;&#19994;&#32773;&#30340;&#35843;&#26597;&#65288;n=18&#65289;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#27010;&#24565;&#20197;&#21450;&#23545;$k$-WL&#30340;&#20551;&#35774;&#12290;&#19982;&#20174;&#19994;&#32773;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#65288;&#20511;&#37492;&#20102;&#22270;&#35770;&#21644;&#22522;&#20934;&#23457;&#26680;&#65289;&#25581;&#31034;&#20102;$k$-WL&#19981;&#33021;&#20445;&#35777;&#31561;&#36317;&#12289;&#21487;&#33021;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20219;&#21153;&#26080;&#20851;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#20419;&#36827;&#27867;&#21270;&#25110;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#20027;&#24352;&#22522;&#20110;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#22806;&#24310;&#23450;&#20041;&#21644;&#27979;&#37327;&#65292;&#36827;&#19968;&#27493;&#36129;&#29486;&#20102;&#26500;&#24314;&#27492;&#31867;&#22522;&#20934;&#30340;&#25351;&#23548;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressive power of graph neural networks is usually measured by comparing how many pairs of graphs or nodes an architecture can possibly distinguish as non-isomorphic to those distinguishable by the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test. In this paper, we uncover misalignments between practitioners' conceptualizations of expressive power and $k$-WL through a systematic analysis of the reliability and validity of $k$-WL. We further conduct a survey ($n = 18$) of practitioners to surface their conceptualizations of expressive power and their assumptions about $k$-WL. In contrast to practitioners' opinions, our analysis (which draws from graph theory and benchmark auditing) reveals that $k$-WL does not guarantee isometry, can be irrelevant to real-world graph tasks, and may not promote generalization or trustworthiness. We argue for extensional definitions and measurement of expressive power based on benchmarks; we further contribute guiding questions for constructing such 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.05232</link><description>&lt;p&gt;
&#20174;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#21040;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey From Distributed Machine Learning to Distributed Deep Learning. (arXiv:2307.05232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05232
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#12290;&#36825;&#19968;&#25104;&#21151;&#24402;&#21151;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#30828;&#20214;&#21152;&#36895;&#30340;&#36827;&#27493;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#21644;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#31639;&#27861;&#24517;&#39035;&#29992;&#26356;&#22810;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20040;&#22823;&#37327;&#30340;&#25968;&#25454;&#21487;&#33021;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#22788;&#29702;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#21644;&#31639;&#27861;&#20998;&#24067;&#22312;&#22810;&#21488;&#26426;&#22120;&#19978;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#34987;&#31216;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#65292;&#24050;&#32463;&#25237;&#20837;&#20102;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#65292;&#30446;&#21069;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#32508;&#36848;&#65292;&#23545;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#29366;&#24577;&#36827;&#34892;&#20840;&#38754;&#24635;&#32467;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#31639;&#27861;&#20998;&#25104;&#20998;&#31867;&#21644;&#32858;&#31867;&#65288;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#65289;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20960;&#32452;&#12290;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence has achieved significant success in handling complex tasks in recent years. This success is due to advances in machine learning algorithms and hardware acceleration. In order to obtain more accurate results and solve more complex problems, algorithms must be trained with more data. This huge amount of data could be time-consuming to process and require a great deal of computation. This solution could be achieved by distributing the data and algorithm across several machines, which is known as distributed machine learning. There has been considerable effort put into distributed machine learning algorithms, and different methods have been proposed so far. In this article, we present a comprehensive summary of the current state-of-the-art in the field through the review of these algorithms. We divide this algorithms in classification and clustering (traditional machine learning), deep learning and deep reinforcement learning groups. Distributed deep learning has ga
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32500;&#24230;&#20381;&#36182;&#20248;&#21270;&#24230;&#20026;$O(d\delta^{-1}\epsilon^{-3})$&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#38750;&#20984;&#38543;&#26426;&#38646;&#38454;&#35774;&#32622;&#20013;&#38750;&#20809;&#28369;&#20248;&#21270;&#19982;&#20809;&#28369;&#20248;&#21270;&#30340;&#19968;&#26679;&#23481;&#26131;&#12290;</title><link>http://arxiv.org/abs/2307.04504</link><description>&lt;p&gt;
&#38646;&#38454;&#38750;&#20809;&#28369;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#26368;&#20248;&#32500;&#24230;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
An Algorithm with Optimal Dimension-Dependence for Zero-Order Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2307.04504v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04504
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32500;&#24230;&#20381;&#36182;&#20248;&#21270;&#24230;&#20026;$O(d\delta^{-1}\epsilon^{-3})$&#30340;&#26368;&#20248;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#38750;&#20984;&#38543;&#26426;&#38646;&#38454;&#35774;&#32622;&#20013;&#38750;&#20809;&#28369;&#20248;&#21270;&#19982;&#20809;&#28369;&#20248;&#21270;&#30340;&#19968;&#26679;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20165;&#26377;&#22024;&#26434;&#20989;&#25968;&#35780;&#20272;&#26469;&#20135;&#29983;Lipschitz&#30446;&#26631;&#30340;$(\delta, \epsilon)$-&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#30446;&#26631;&#21487;&#33021;&#26082;&#19981;&#20809;&#28369;&#20063;&#19981;&#20984;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#30340;&#38543;&#26426;&#38646;&#38454;&#31639;&#27861;&#65292;&#25152;&#26377;&#36825;&#20123;&#31639;&#27861;&#37117;&#21463;&#21040;&#20102;$\Omega(d^{3/2})$&#32500;&#24230;&#20381;&#36182;&#24615;&#30340;&#22256;&#25200;&#65292;&#20854;&#20013;$d$&#26159;&#38382;&#39064;&#30340;&#32500;&#24230;&#65292;&#36825;&#34987;&#25512;&#27979;&#20026;&#26368;&#20248;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26356;&#24555;&#30340;&#31639;&#27861;&#26469;&#39539;&#26021;&#36825;&#20010;&#29468;&#24819;&#65292;&#35813;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#20026;$O(d\delta^{-1}\epsilon^{-3})$&#65292;&#36825;&#26159;&#20851;&#20110;$d$&#30340;&#26368;&#20248;&#65288;&#22312;&#25968;&#20540;&#24120;&#25968;&#19978;&#65289;&#65292;&#23545;&#20110;&#31934;&#24230;&#21442;&#25968;$\delta, \epsilon$&#20063;&#26159;&#26368;&#20248;&#30340;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;Lin&#31561;&#20154;&#30041;&#19979;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65288;NeurIPS'22&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31639;&#27861;&#23454;&#29616;&#30340;&#25910;&#25947;&#36895;&#24230;&#23545;&#20110;&#20809;&#28369;&#30446;&#26631;&#20063;&#26159;&#26368;&#20248;&#30340;&#65292;&#35777;&#26126;&#22312;&#38750;&#20984;&#38543;&#26426;&#38646;&#38454;&#35774;&#32622;&#20013;&#65292;&#38750;&#20809;&#28369;&#20248;&#21270;&#19982;&#20809;&#28369;&#20248;&#21270;&#19968;&#26679;&#23481;&#26131;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29616;&#19978;&#36848;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of producing $(\delta,\epsilon)$-stationary points of Lipschitz objectives which are possibly neither smooth nor convex, using only noisy function evaluations. Recent works proposed several stochastic zero-order algorithms that solve this task, all of which suffer from a dimension-dependence of $\Omega(d^{3/2})$ where $d$ is the dimension of the problem, which was conjectured to be optimal. We refute this conjecture by providing a faster algorithm that has complexity $O(d\delta^{-1}\epsilon^{-3})$, which is optimal (up to numerical constants) with respect to $d$ and also optimal with respect to the accuracy parameters $\delta,\epsilon$, thus solving an open question due to Lin et al. (NeurIPS'22). Moreover, the convergence rate achieved by our algorithm is also optimal for smooth objectives, proving that in the nonconvex stochastic zero-order setting, nonsmooth optimization is as easy as smooth optimization. We provide algorithms that achieve the aforementioned 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.14115</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#35299;&#37322;&#65306;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Explanation: On Causal Rationalization. (arXiv:2306.14115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35299;&#37322;&#20013;&#24341;&#20837;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65292;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#22240;&#26524;&#27010;&#29575;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26377;&#26356;&#21152;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35299;&#37322;&#25104;&#20026;&#20102;&#36890;&#36807;&#36873;&#25321;&#36755;&#20837;&#25991;&#26412;&#30340;&#23376;&#38598;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#20013;&#20027;&#35201;&#21464;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#30340;&#33258;&#25105;&#35299;&#37322;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20851;&#32852;&#30340;&#35299;&#37322;&#26041;&#27861;&#22312;&#20004;&#20010;&#25110;&#22810;&#20010;&#29255;&#27573;&#39640;&#24230;&#20114;&#30456;&#20851;&#32852;&#26102;&#26080;&#27861;&#35782;&#21035;&#30495;&#27491;&#30340;&#35299;&#37322;&#65292;&#22240;&#27492;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#31867;&#20284;&#30340;&#36129;&#29486;&#65292;&#25152;&#35859;&#30340;&#34394;&#20551;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#26032;&#39062;&#22320;&#23558;&#20004;&#20010;&#22240;&#26524;&#26399;&#26395;&#20540;&#65288;&#38750;&#34394;&#20551;&#24615;&#21644;&#25928;&#29575;&#65289;&#24341;&#20837;&#20102;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#35299;&#37322;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#30340;&#22240;&#26524;&#27010;&#29575;&#65292;&#36890;&#36807;&#20854;&#29702;&#35770;&#37492;&#23450;&#65292;&#24314;&#31435;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#35780;&#35770;&#21644;&#21307;&#30103;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advances in natural language processing, rationalization becomes an essential self-explaining diagram to disentangle the black box by selecting a subset of input texts to account for the major variation in prediction. Yet, existing association-based approaches on rationalization cannot identify true rationales when two or more snippets are highly inter-correlated and thus provide a similar contribution to prediction accuracy, so-called spuriousness. To address this limitation, we novelly leverage two causal desiderata, non-spuriousness and efficiency, into rationalization from the causal inference perspective. We formally define a series of probabilities of causation based on a newly proposed structural causal model of rationalization, with its theoretical identification established as the main component of learning necessary and sufficient rationales. The superior performance of the proposed causal rationalization is demonstrated on real-world review and medical datasets w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#24314;&#27169;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#24369;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#26377;&#26395;&#23454;&#29616;&#23545;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#26356;&#39640;&#25928;&#12289;&#26356;&#31934;&#20934;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2306.13830</link><description>&lt;p&gt;
&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#30340;&#24230;&#37327;&#23398;&#20064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Aircraft Environmental Impact Segmentation via Metric Learning. (arXiv:2306.13830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#24314;&#27169;&#20013;&#30340;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#24369;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#26377;&#26395;&#23454;&#29616;&#23545;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#26356;&#39640;&#25928;&#12289;&#26356;&#31934;&#20934;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24230;&#37327;&#23398;&#20064;&#26159;&#25351;&#20026;&#29305;&#23450;&#20219;&#21153;&#23398;&#20064;&#23450;&#21046;&#36317;&#31163;&#24230;&#37327;&#30340;&#36807;&#31243;&#12290;&#36825;&#19968;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#23376;&#39046;&#22495;&#23545;&#20110;&#20381;&#38752;&#35745;&#31639;&#23545;&#35937;&#20043;&#38388;&#36317;&#31163;&#25110;&#30456;&#20284;&#24230;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25110;&#25968;&#25454;&#25366;&#25496;&#20219;&#21153;&#30340;&#20219;&#20309;&#24212;&#29992;&#37117;&#26159;&#26377;&#29992;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#33322;&#31354;&#33322;&#22825;&#24037;&#31243;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#12289;&#25552;&#21462;&#27169;&#24335;&#12289;&#21457;&#29616;&#30693;&#35782;&#31561;&#12290;&#28982;&#32780;&#65292;&#24230;&#37327;&#23398;&#20064;&#20316;&#20026;&#19968;&#20010;&#21487;&#20197;&#25552;&#39640;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#30340;&#20803;&#32032;&#65292;&#36804;&#20170;&#22312;&#30456;&#20851;&#25991;&#29486;&#20013;&#24456;&#23569;&#34987;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#23558;&#32463;&#20856;&#30340;&#24230;&#37327;&#23398;&#20064;&#20844;&#24335;&#19982;&#26032;&#39062;&#30340;&#20803;&#32032;&#24212;&#29992;&#20110;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#24314;&#27169;&#20013;&#65292;&#24182;&#36890;&#36807;&#24369;&#30417;&#30563;&#24230;&#37327;&#23398;&#20064;&#20219;&#21153;&#65292;&#22312;&#26032;&#20986;&#29616;&#30340;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#25551;&#36848;&#21644;&#21010;&#20998;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#36825;&#19968;&#32467;&#26524;&#23558;&#23454;&#29616;&#23545;&#26426;&#36733;&#29615;&#22659;&#24433;&#21709;&#26356;&#39640;&#25928;&#12289;&#26356;&#31934;&#20934;&#30340;&#24314;&#27169;&#65292;&#23545;&#20110;&#33322;&#31354;&#20135;&#19994;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metric learning is the process of learning a tailored distance metric for a particular task. This advanced subfield of machine learning is useful to any machine learning or data mining task that relies on the computation of distances or similarities over objects. In recently years, machine learning techniques have been extensively used in aviation and aerospace engineering to make predictions, extract patterns, discover knowledge, etc. Nevertheless, metric learning, an element that can advance the performance of complex machine learning tasks, has so far been hardly utilized in relevant literature. In this study, we apply classic metric learning formulations with novel components on aviation environmental impact modeling. Through a weakly-supervised metric learning task, we achieve significant improvement in the newly emerged problem of aircraft characterization and segmentation for environmental impacts. The result will enable the more efficient and accurate modeling of aircraft envir
&lt;/p&gt;</description></item><item><title>QNNRepair &#26159;&#19968;&#31181;&#29992;&#20110;&#20462;&#22797;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#31070;&#32463;&#20803;&#26435;&#37325;&#21442;&#25968;&#20197;&#20462;&#22797;&#22312;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36807;&#31243;&#20013;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#19981;&#24433;&#21709;&#36890;&#36807;&#27979;&#35797;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22312;&#22833;&#36133;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13793</link><description>&lt;p&gt;
QNNRepair&#65306;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QNNRepair: Quantized Neural Network Repair. (arXiv:2306.13793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13793
&lt;/p&gt;
&lt;p&gt;
QNNRepair &#26159;&#19968;&#31181;&#29992;&#20110;&#20462;&#22797;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#31070;&#32463;&#20803;&#26435;&#37325;&#21442;&#25968;&#20197;&#20462;&#22797;&#22312;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36807;&#31243;&#20013;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#19981;&#24433;&#21709;&#36890;&#36807;&#27979;&#35797;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22312;&#22833;&#36133;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;QNNRepair&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#20010;&#20462;&#27491;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#30340;&#26041;&#27861;&#12290;&#20854;&#26088;&#22312;&#25552;&#39640;&#22312;&#37327;&#21270;&#20043;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20854;&#25509;&#21463;&#20840;&#31934;&#24230;&#21644;&#26435;&#37325;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#19968;&#20010;&#20462;&#22797;&#25968;&#25454;&#38598;&#12290;QNNRepair&#23558;&#20462;&#22797;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#35268;&#21010;&#65292;&#36890;&#36807;&#35299;&#20915;&#31070;&#32463;&#20803;&#26435;&#37325;&#21442;&#25968;&#20197;&#20462;&#22797;&#22312;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36807;&#31243;&#20013;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#19981;&#24433;&#21709;&#36890;&#36807;&#27979;&#35797;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22312;&#22833;&#36133;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present QNNRepair, the first method in the literature for repairing quantized neural networks (QNNs). QNNRepair aims to improve the accuracy of a neural network model after quantization. It accepts the full-precision and weight-quantized neural networks and a repair dataset of passing and failing tests. At first, QNNRepair applies a software fault localization method to identify the neurons that cause performance degradation during neural network quantization. Then, it formulates the repair problem into a linear programming problem of solving neuron weights parameters, which corrects the QNN's performance on failing tests while not compromising its performance on passing tests. We evaluate QNNRepair with widely used neural network architectures such as MobileNetV2, ResNet, and VGGNet on popular datasets, including high-resolution images. We also compare QNNRepair with the state-of-the-art data-free quantization method SQuant. According to the experiment results, we conclude that QNN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#35777;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#31639;&#35777;&#25454;&#20915;&#31574;&#30340;&#20215;&#20540;&#21644;&#32479;&#35745;&#31934;&#24230;&#25237;&#36164;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2306.13681</link><description>&lt;p&gt;
&#20272;&#31639;&#22522;&#20110;&#35777;&#25454;&#20915;&#31574;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Estimating the Value of Evidence-Based Decision Making. (arXiv:2306.13681v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#35777;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#31639;&#35777;&#25454;&#20915;&#31574;&#30340;&#20215;&#20540;&#21644;&#32479;&#35745;&#31934;&#24230;&#25237;&#36164;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;/&#25919;&#31574;&#20915;&#31574;&#36890;&#24120;&#22522;&#20110;&#38543;&#26426;&#23454;&#39564;&#21644;&#35266;&#23519;&#24615;&#30740;&#31350;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#35777;&#26694;&#26550;&#26469;&#20272;&#31639;&#22522;&#20110;&#35777;&#25454;&#30340;&#20915;&#31574;&#65288;EBDM&#65289;&#30340;&#20215;&#20540;&#21644;&#32479;&#35745;&#31934;&#24230;&#25237;&#36164;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Business/policy decisions are often based on evidence from randomized experiments and observational studies. In this article we propose an empirical framework to estimate the value of evidence-based decision making (EBDM) and the return on the investment in statistical precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12001</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
An Overview of Catastrophic AI Risks. (arXiv:2306.12001v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#22235;&#20010;&#20027;&#35201;&#26469;&#28304;&#65292;&#21253;&#25324;&#24694;&#24847;&#20351;&#29992;&#12289;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#12289;&#32452;&#32455;&#39118;&#38505;&#21644;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#19987;&#23478;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#19990;&#30028;&#21508;&#22269;&#39046;&#23548;&#20154;&#23545;&#36234;&#26469;&#36234;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#39118;&#38505;&#34987;&#21333;&#29420;&#35814;&#32454;&#20171;&#32461;&#36807;&#65292;&#20294;&#36843;&#20999;&#38656;&#35201;&#31995;&#32479;&#22320;&#35752;&#35770;&#21644;&#35828;&#26126;&#28508;&#22312;&#21361;&#38505;&#65292;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20154;&#24037;&#26234;&#33021;&#28798;&#38590;&#24615;&#39118;&#38505;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65306;&#24694;&#24847;&#20351;&#29992;&#65292;&#21363;&#20010;&#20154;&#25110;&#22242;&#20307;&#26377;&#24847;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#36896;&#25104;&#20260;&#23475;&#65307;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#65292;&#21363;&#31454;&#20105;&#29615;&#22659;&#20419;&#20351;&#34892;&#21160;&#32773;&#37096;&#32626;&#19981;&#23433;&#20840;&#30340;&#20154;&#24037;&#26234;&#33021;&#25110;&#25918;&#24323;&#25511;&#21046;&#26435;&#20132;&#32473;&#20154;&#24037;&#26234;&#33021;&#65307;&#32452;&#32455;&#39118;&#38505;&#65292;&#31361;&#20986;&#20154;&#20026;&#21644;&#22797;&#26434;&#31995;&#32479;&#22914;&#20309;&#22686;&#21152;&#28798;&#38590;&#24615;&#20107;&#25925;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#65307;&#20197;&#21450;&#27969;&#27667;&#20154;&#24037;&#26234;&#33021;&#65292;&#25551;&#36848;&#20102;&#25511;&#21046;&#27604;&#20154;&#31867;&#26234;&#33021;&#26356;&#39640;&#30340;&#20195;&#29702;&#31243;&#24207;&#22256;&#38590;&#30340;&#22266;&#26377;&#38590;&#39064;&#12290;&#23545;&#20110;&#27599;&#20010;&#39118;&#38505;&#31867;&#21035;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#38382;&#39064;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#30340;&#25968;&#25454;&#25910;&#38598;&#24179;&#21488;&#65292;&#35299;&#20915;&#20102;ML&#27169;&#22411;&#22312;&#19981;&#21516;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#19968;&#33324;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#25351;&#23548;&#32593;&#32476;&#25968;&#25454;&#25910;&#38598;&#30340;&#22686;&#24378;ML&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.08853</link><description>&lt;p&gt;
&#36861;&#23547;&#32593;&#32476;&#29420;&#35282;&#20861;&#65306;&#19968;&#20010;&#29992;&#20110;&#24320;&#21457;&#32593;&#32476;&#23433;&#20840;&#38382;&#39064;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#25910;&#38598;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
In Search of netUnicorn: A Data-Collection Platform to Develop Generalizable ML Models for Network Security Problems. (arXiv:2306.08853v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08853
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#38382;&#39064;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#30340;&#25968;&#25454;&#25910;&#38598;&#24179;&#21488;&#65292;&#35299;&#20915;&#20102;ML&#27169;&#22411;&#22312;&#19981;&#21516;&#32593;&#32476;&#29615;&#22659;&#20013;&#30340;&#19968;&#33324;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26469;&#25351;&#23548;&#32593;&#32476;&#25968;&#25454;&#25910;&#38598;&#30340;&#22686;&#24378;ML&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#38382;&#39064;&#30340;&#26174;&#33879;&#25104;&#21151;&#21463;&#21040;&#20102;ML&#27169;&#22411;&#22312;&#19981;&#21516;&#32593;&#32476;&#29615;&#22659;&#20013;&#20351;&#29992;&#26102;&#26080;&#27861;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#19981;&#21516;&#32593;&#32476;&#29615;&#22659;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#32593;&#32476;&#34892;&#20026;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#34987;&#31216;&#20026;ML&#27169;&#22411;&#30340;&#19968;&#33324;&#21270;&#38382;&#39064;&#12290;&#31038;&#21306;&#35748;&#35782;&#21040;&#35757;&#32451;&#25968;&#25454;&#38598;&#22312;&#36825;&#20010;&#19978;&#19979;&#25991;&#20013;&#25198;&#28436;&#30340;&#20851;&#38190;&#35282;&#33394;&#65292;&#24182;&#24320;&#21457;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#25913;&#21892;&#25968;&#25454;&#38598;&#30340;&#31574;&#21010;&#65292;&#20197;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#19981;&#36866;&#29992;&#65292;&#29978;&#33267;&#21487;&#33021;&#20135;&#29983;&#19981;&#29616;&#23454;&#25110;&#36136;&#37327;&#20302;&#19979;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20197;&#36845;&#20195;&#30340;&#26041;&#24335;&#25351;&#23548;&#32593;&#32476;&#25968;&#25454;&#30340;&#25910;&#38598;&#12290;&#20026;&#20102;&#30830;&#20445;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#35201;&#27714;&#26032;&#30340;&#25968;&#25454;&#38598;&#22312;&#36825;&#20010;&#36845;&#20195;&#36807;&#31243;&#20013;&#24212;&#35813;&#34987;&#20869;&#29983;&#22320;&#25910;&#38598;&#65292;&#20174;&#32780;&#35843;&#25972;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of the use of machine learning-based solutions for network security problems has been impeded by the developed ML models' inability to maintain efficacy when used in different network environments exhibiting different network behaviors. This issue is commonly referred to as the generalizability problem of ML models. The community has recognized the critical role that training datasets play in this context and has developed various techniques to improve dataset curation to overcome this problem. Unfortunately, these methods are generally ill-suited or even counterproductive in the network security domain, where they often result in unrealistic or poor-quality datasets.  To address this issue, we propose an augmented ML pipeline that leverages explainable ML tools to guide the network data collection in an iterative fashion. To ensure the data's realism and quality, we require that the new datasets should be endogenously collected in this iterative process, thus ad
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06871</link><description>&lt;p&gt;
&#25552;&#21319;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;Q-Ensembles&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Offline-to-Online Reinforcement Learning with Q-Ensembles. (arXiv:2306.06871v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06871
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#20195;&#29702;&#26681;&#25454;&#22266;&#23450;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#33021;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#32570;&#20047;&#25506;&#32034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#31163;&#32447;&#39044;&#35757;&#32451;&#19982;&#22312;&#32447;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#35753;&#20195;&#29702;&#19982;&#29615;&#22659;&#23454;&#26102;&#20132;&#20114;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20854;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#22312;&#32447;&#38454;&#27573;&#25913;&#36827;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21152;&#24555;&#22312;&#32447;&#24615;&#33021;&#25552;&#21319;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is a learning paradigm where an agent learns from a fixed dataset of experience. However, learning solely from a static dataset can limit the performance due to the lack of exploration. To overcome it, offline-to-online RL combines offline pre-training with online fine-tuning, which enables the agent to further refine its policy by interacting with the environment in real-time. Despite its benefits, existing offline-to-online RL methods suffer from performance degradation and slow improvement during the online phase. To tackle these challenges, we propose a novel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing the number of Q-networks, we seamlessly bridge offline pre-training and online fine-tuning without degrading performance. Moreover, to expedite online performance enhancement, we appropriately loosen the pessimism of Q-value estimation and incorporate ensemble-based exploration mechanisms into our framework. Experiment
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#26469;&#23545;&#38750;&#20998;&#24067;&#24335;&#30340;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#33021;&#28304;&#39044;&#27979;&#65292;&#20197;&#36866;&#24212;&#20837;&#20303;&#27169;&#24335;&#30340;&#21464;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#35843;&#25972;&#33021;&#28304;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.06385</link><description>&lt;p&gt;
&#23545;&#20110;&#40065;&#26834;&#30340;&#33021;&#28304;&#39044;&#27979;&#65292;&#25345;&#32493;&#23398;&#20064;&#24102;&#26377;&#26102;&#31354;&#29305;&#24449;&#30340;&#38750;&#20998;&#24067;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Continually learning out-of-distribution spatiotemporal data for robust energy forecasting. (arXiv:2306.06385v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06385
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#26469;&#23545;&#38750;&#20998;&#24067;&#24335;&#30340;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#33021;&#28304;&#39044;&#27979;&#65292;&#20197;&#36866;&#24212;&#20837;&#20303;&#27169;&#24335;&#30340;&#21464;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#35843;&#25972;&#33021;&#28304;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#24314;&#31569;&#33021;&#28304;&#20351;&#29992;&#36827;&#34892;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21487;&#25345;&#32493;&#21457;&#23637;&#21644;&#20943;&#23569;&#28010;&#36153;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20351;&#24314;&#31569;&#31649;&#29702;&#32773;&#33021;&#22815;&#20248;&#21270;&#33021;&#28304;&#28040;&#32791;&#24182;&#38477;&#20302;&#25104;&#26412;&#12290;&#36825;&#19968;&#37325;&#35201;&#24615;&#22312;&#24322;&#24120;&#26399;&#38388;&#34987;&#25918;&#22823;&#65292;&#20363;&#22914;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#23427;&#25200;&#20081;&#20102;&#20837;&#20303;&#27169;&#24335;&#65292;&#20351;&#20934;&#30830;&#30340;&#39044;&#27979;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#24322;&#24120;&#26399;&#38388;&#39044;&#27979;&#33021;&#28304;&#20351;&#29992;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#20837;&#20303;&#27169;&#24335;&#21644;&#33021;&#28304;&#20351;&#29992;&#34892;&#20026;&#21457;&#29983;&#21464;&#21270;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#20837;&#20303;&#27169;&#24335;&#20998;&#24067;&#30340;&#36716;&#21464;&#65292;&#35768;&#22810;&#20154;&#22312;&#23478;&#24037;&#20316;&#25110;&#23398;&#20064;&#12290;&#36825;&#23601;&#20135;&#29983;&#20102;&#23545;&#33021;&#22815;&#36866;&#24212;&#21464;&#21270;&#30340;&#20837;&#20303;&#27169;&#24335;&#30340;&#26032;&#39044;&#27979;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#22312;&#32447;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20351;&#24314;&#31569;&#31649;&#29702;&#32773;&#33021;&#22815;&#36866;&#24212;&#20837;&#20303;&#27169;&#24335;&#30340;&#21464;&#21270;&#65292;&#24182;&#30456;&#24212;&#22320;&#35843;&#25972;&#33021;&#28304;&#20351;&#29992;&#12290;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#27599;&#20010;&#26032;&#25968;&#25454;&#28857;&#20026;&#22522;&#30784;&#36827;&#34892;&#22686;&#37327;&#26356;&#26032;&#65292;&#20351;&#20854;&#33021;&#22815;&#19981;&#26029;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting building energy usage is essential for promoting sustainability and reducing waste, as it enables building managers to optimize energy consumption and reduce costs. This importance is magnified during anomalous periods, such as the COVID-19 pandemic, which have disrupted occupancy patterns and made accurate forecasting more challenging. Forecasting energy usage during anomalous periods is difficult due to changes in occupancy patterns and energy usage behavior. One of the primary reasons for this is the shift in distribution of occupancy patterns, with many people working or learning from home. This has created a need for new forecasting methods that can adapt to changing occupancy patterns. Online learning has emerged as a promising solution to this challenge, as it enables building managers to adapt to changes in occupancy patterns and adjust energy usage accordingly. With online learning, models can be updated incrementally with each new data point, allowing them to lear
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#25968;&#30721;&#28779;&#26143;&#22270;&#20687;&#65292;&#20197;&#20998;&#26512;&#23547;&#25214;&#28779;&#26143;&#19978;&#30340;&#30701;&#26242;&#28082;&#24577;&#27700;&#26001;&#22359;&#12290;&#20808;&#21069;&#30340;&#25163;&#21160;&#22270;&#20687;&#20998;&#26512;&#24050;&#32463;&#21457;&#29616;37&#24352;&#22270;&#20687;&#21547;&#26377;&#36739;&#23567;&#20912;&#26001;&#65292;&#24182;&#36890;&#36807;&#20142;&#24230;&#12289;&#39068;&#33394;&#21644;&#19982;&#22320;&#24418;&#36974;&#34109;&#30340;&#24378;&#20851;&#32852;&#24615;&#36827;&#34892;&#20102;&#21306;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.19958</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#39640;&#20998;&#36776;&#29575;&#25968;&#30721;&#28779;&#26143;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Analysing high resolution digital Mars images using machine learning. (arXiv:2305.19958v2 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#25968;&#30721;&#28779;&#26143;&#22270;&#20687;&#65292;&#20197;&#20998;&#26512;&#23547;&#25214;&#28779;&#26143;&#19978;&#30340;&#30701;&#26242;&#28082;&#24577;&#27700;&#26001;&#22359;&#12290;&#20808;&#21069;&#30340;&#25163;&#21160;&#22270;&#20687;&#20998;&#26512;&#24050;&#32463;&#21457;&#29616;37&#24352;&#22270;&#20687;&#21547;&#26377;&#36739;&#23567;&#20912;&#26001;&#65292;&#24182;&#36890;&#36807;&#20142;&#24230;&#12289;&#39068;&#33394;&#21644;&#19982;&#22320;&#24418;&#36974;&#34109;&#30340;&#24378;&#20851;&#32852;&#24615;&#36827;&#34892;&#20102;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28779;&#26143;&#19978;&#23547;&#25214;&#30701;&#26242;&#28082;&#24577;&#27700;&#30340;&#27963;&#21160;&#26159;&#19968;&#39033;&#25345;&#32493;&#36827;&#34892;&#30340;&#20219;&#21153;&#12290;&#22312;&#28779;&#26143;&#23395;&#33410;&#24615;&#26497;&#22320;&#20912;&#24125;&#28040;&#36864;&#21518;&#65292;&#30001;&#20110;&#28779;&#26143;&#34920;&#38754;&#21644;&#22823;&#27668;&#23618;&#30340;&#20302;&#28909;&#23548;&#29575;&#65292;&#23567;&#27700;&#20912;&#26001;&#22359;&#21487;&#33021;&#20250;&#27531;&#30041;&#22312;&#38452;&#26263;&#30340;&#22320;&#26041;&#12290;&#22312;&#26149;&#26411;&#22799;&#21021;&#65292;&#36825;&#20123;&#26001;&#22359;&#21487;&#33021;&#20250;&#26292;&#38706;&#22312;&#38451;&#20809;&#19979;&#65292;&#24182;&#36805;&#36895;&#21319;&#28201;&#65292;&#20351;&#28082;&#24577;&#30456;&#20986;&#29616;&#12290;&#20026;&#20102;&#20102;&#35299;&#27492;&#31867;&#20912;&#26001;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#24067;&#65292;&#24212;&#35813;&#23545;&#20809;&#23398;&#22270;&#20687;&#36827;&#34892;&#25628;&#32034;&#21644;&#26816;&#26597;&#12290;&#20808;&#21069;&#23545;&#30001;&#28779;&#26143;&#21208;&#27979;&#36712;&#36947;&#39134;&#34892;&#22120;&#19978;&#30340;&#39640;&#20998;&#36776;&#29575;&#25104;&#20687;&#31185;&#23398;&#23454;&#39564;&#65288;HiRISE&#65289;&#30456;&#26426;&#25429;&#33719;&#30340;&#21335;&#21322;&#29699;110&#24352;&#22270;&#20687;&#36827;&#34892;&#20102;&#25163;&#21160;&#22270;&#20687;&#20998;&#26512;&#12290;&#20854;&#20013;&#65292;37&#24352;&#22270;&#20687;&#34987;&#37492;&#23450;&#20026;&#21547;&#26377;&#36739;&#23567;&#20912;&#26001;&#65292;&#36825;&#20123;&#20912;&#26001;&#21487;&#36890;&#36807;&#23427;&#20204;&#30340;&#20142;&#24230;&#12289;&#39068;&#33394;&#21644;&#19982;&#23616;&#37096;&#22320;&#24418;&#36974;&#34109;&#30340;&#24378;&#20851;&#32852;&#24615;&#26469;&#21306;&#20998;&#12290;&#26412;&#30740;&#31350;&#24212;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#23547;&#25214;&#26356;&#22810;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The search for ephemeral liquid water on Mars is an ongoing activity. After the recession of the seasonal polar ice cap on Mars, small water ice patches may be left behind in shady places due to the low thermal conductivity of the Martian surface and atmosphere. During late spring and early summer, these patches may be exposed to direct sunlight and warm up rapidly enough for the liquid phase to emerge. To see the spatial and temporal occurrence of such ice patches, optical images should be searched for and checked. Previously a manual image analysis was conducted on 110 images from the southern hemisphere, captured by the High Resolution Imaging Science Experiment (HiRISE) camera onboard the Mars Reconnaissance Orbiter space mission. Out of these, 37 images were identified with smaller ice patches, which were distinguishable by their brightness, colour and strong connection to local topographic shading. In this study, a convolutional neural network (CNN) is applied to find further ima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.17256</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26159;&#25042;&#24816;&#30340;&#23398;&#20064;&#32773;&#65306;&#20998;&#26512;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning. (arXiv:2305.17256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#22823;&#22411;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#65292;&#36825;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20854;&#20013;LLM&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;-&#26631;&#31614;&#23545;&#65288;&#25552;&#31034;&#65289;&#30340;&#26465;&#20214;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#28508;&#21147;&#24040;&#22823;&#65292;&#20294;&#25105;&#20204;&#23545;&#24433;&#21709;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31283;&#20581;&#24615;&#30340;&#22240;&#32032;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;LLM&#23545;&#25552;&#31034;&#20869;&#25463;&#24452;&#25110;&#20551;&#30456;&#20851;&#30340;&#20381;&#36182;&#20851;&#31995;&#26469;&#24357;&#34917;&#36825;&#19968;&#30693;&#35782;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#31867;&#21644;&#25277;&#21462;&#20219;&#21153;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LLM&#26159;&#8220;&#25042;&#24816;&#23398;&#20064;&#32773;&#8221;&#30340;&#20107;&#23454;&#65292;&#23427;&#24448;&#24448;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#26469;&#33719;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#21363;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#21487;&#33021;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25552;&#31034;&#20013;&#30340;&#25463;&#24452;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35780;&#20272;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31283;&#20581;&#24615;&#21644;&#26816;&#27979;&#21644;&#32531;&#35299;&#25552;&#31034;&#20013;&#25463;&#24452;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are "lazy learners" that tend to exploit shortcuts in prompts for downstream tasks. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#26469;&#22521;&#35757;&#21644;&#37325;&#26032;&#26657;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#21512;&#22863;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24102;&#26377;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#37327;&#21644;&#21147;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26448;&#26009;&#30340;&#32467;&#26500;&#20248;&#21270;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2305.16325</link><description>&lt;p&gt;
&#20855;&#26377;&#26657;&#20934;&#30340;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#20114;&#20316;&#29992;&#21183;&#21512;&#22863;&#36816;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network Interatomic Potential Ensembles with Calibrated Aleatoric and Epistemic Uncertainty on Energy and Forces. (arXiv:2305.16325v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#26469;&#22521;&#35757;&#21644;&#37325;&#26032;&#26657;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#21512;&#22863;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24102;&#26377;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#37327;&#21644;&#21147;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26448;&#26009;&#30340;&#32467;&#26500;&#20248;&#21270;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24265;&#20215;&#30340;&#26426;&#22120;&#23398;&#20064;&#21183;&#22330;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#21152;&#36895;&#26448;&#26009;&#30340;&#32467;&#26500;&#20248;&#21270;&#21644;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#36890;&#36807;&#36845;&#20195;&#24615;&#22320;&#39044;&#27979;&#21644;&#24212;&#29992;&#21407;&#23376;&#38388;&#21147;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#21040;&#39044;&#27979;&#30340;&#19981;&#21487;&#38752;&#24615;&#20197;&#36991;&#20813;&#38169;&#35823;&#25110;&#35823;&#23548;&#24615;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26694;&#26550;&#26469;&#22521;&#35757;&#21644;&#37325;&#26032;&#26657;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#21512;&#22863;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24102;&#26377;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#33021;&#37327;&#21644;&#21147;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;Epistemic&#21644;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#32553;&#25918;&#20989;&#25968;&#22312;&#21518;&#26399;&#37325;&#26032;&#35843;&#25972;&#20102;&#24635;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#22312;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#21644;&#35780;&#20272;&#65292;ANI-1x&#65288;Smith&#31561;&#20154;&#65289;&#21644;Transition1x&#65288;Schreiner&#31561;&#20154;&#65289;&#65292;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#37117;&#21253;&#21547;&#36828;&#31163;&#24179;&#34913;&#29366;&#24577;&#30340;&#22810;&#26679;&#21270;&#26500;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inexpensive machine learning potentials are increasingly being used to speed up structural optimization and molecular dynamics simulations of materials by iteratively predicting and applying interatomic forces. In these settings, it is crucial to detect when predictions are unreliable to avoid wrong or misleading results. Here, we present a complete framework for training and recalibrating graph neural network ensemble models to produce accurate predictions of energy and forces with calibrated uncertainty estimates. The proposed method considers both epistemic and aleatoric uncertainty and the total uncertainties are recalibrated post hoc using a nonlinear scaling function to achieve good calibration on previously unseen data, without loss of predictive accuracy. The method is demonstrated and evaluated on two challenging, publicly available datasets, ANI-1x (Smith et al.) and Transition1x (Schreiner et al.), both containing diverse conformations far from equilibrium. A detailed analys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#23398;&#20064;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#27010;&#24565;&#30340;Transformer&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15775</link><description>&lt;p&gt;
&#20197;&#27010;&#24565;&#20026;&#20013;&#24515;&#30340;Transformer&#65306;&#20855;&#26377;&#38754;&#21521;&#29289;&#20307;&#30340;&#27010;&#24565;&#23398;&#20064;&#65292;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability. (arXiv:2305.15775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#23398;&#20064;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#27010;&#24565;&#30340;Transformer&#27169;&#22411;&#30340;&#20998;&#31867;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#22823;&#22823;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35270;&#35273;&#12289;NLP&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#20102;&#24037;&#20855;&#26469;&#24110;&#21161;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#27010;&#24565;Transformer&#65288;CT&#65289;&#23558;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#20302;&#32423;&#36755;&#20837;&#29305;&#24449;&#27867;&#21270;&#21040;&#26356;&#25277;&#35937;&#30340;&#20013;&#38388;&#23618;&#28508;&#22312;&#27010;&#24565;&#65292;&#26356;&#22909;&#22320;&#20801;&#35768;&#20154;&#31867;&#20998;&#26512;&#21592;&#30452;&#25509;&#35780;&#20272;&#35299;&#37322;&#20851;&#20110;&#20219;&#20309;&#29305;&#23450;&#36755;&#20986;&#20998;&#31867;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;CT&#37319;&#29992;&#30340;&#27010;&#24565;&#23398;&#20064;&#40664;&#35748;&#20551;&#35774;&#31867;&#21035;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#37117;&#23545;&#34920;&#24449;&#35813;&#31867;&#21035;&#30340;&#27010;&#24565;&#20316;&#20986;&#20102;&#30456;&#21516;&#30340;&#36129;&#29486;&#65292;&#32780;&#20351;&#29992;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#27010;&#24565;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms have greatly improved the performance of deep-learning models on visual, NLP, and multimodal tasks while also providing tools to aid in the model's interpretability. In particular, attention scores over input regions or concrete image features can be used to measure how much the attended elements contribute to the model inference. The recently proposed Concept Transformer (CT) generalizes the Transformer attention mechanism from such low-level input features to more abstract, intermediate-level latent concepts that better allow human analysts to more directly assess an explanation for the reasoning of the model about any particular output classification. However, the concept learning employed by CT implicitly assumes that across every image in a class, each image patch makes the same contribution to concepts that characterize membership in that class. Instead of using the CT's image-patch-centric concepts, object-centric concepts could lead to better classification
&lt;/p&gt;</description></item><item><title>&#21518;&#32493;&#21069;&#23548;&#20869;&#22312;&#25506;&#32034;&#26159;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#20869;&#22312;&#22870;&#21169;&#30340;&#25506;&#32034;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#22238;&#39038;&#20449;&#24687;&#24182;&#32467;&#21512;&#21069;&#30651;&#20449;&#24687;&#65292;&#20197;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#19988;&#29983;&#24577;&#23398;&#21512;&#29702;&#30340;&#25506;&#32034;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.15277</link><description>&lt;p&gt;
&#21518;&#32493;&#21069;&#23548;&#20869;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Successor-Predecessor Intrinsic Exploration. (arXiv:2305.15277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15277
&lt;/p&gt;
&lt;p&gt;
&#21518;&#32493;&#21069;&#23548;&#20869;&#22312;&#25506;&#32034;&#26159;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#20869;&#22312;&#22870;&#21169;&#30340;&#25506;&#32034;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#22238;&#39038;&#20449;&#24687;&#24182;&#32467;&#21512;&#21069;&#30651;&#20449;&#24687;&#65292;&#20197;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#39640;&#25928;&#19988;&#29983;&#24577;&#23398;&#21512;&#29702;&#30340;&#25506;&#32034;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#23545;&#20110;&#37027;&#20123;&#22806;&#37096;&#22870;&#21169;&#31232;&#32570;&#30340;&#29615;&#22659;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#21033;&#29992;&#20869;&#22312;&#22870;&#21169;&#36827;&#34892;&#25506;&#32034;&#65292;&#21363;&#20195;&#29702;&#22120;&#20351;&#29992;&#33258;&#25105;&#29983;&#25104;&#30340;&#20869;&#22312;&#22870;&#21169;&#20020;&#26102;&#22686;&#21152;&#22806;&#37096;&#22870;&#21169;&#12290;&#23613;&#31649;&#20869;&#22312;&#22870;&#21169;&#30340;&#30740;&#31350;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#38598;&#20013;&#20110;&#26681;&#25454;&#29366;&#24577;&#30340;&#26410;&#26469;&#21069;&#26223;&#24230;&#37327;&#26469;&#26500;&#25104;&#20869;&#22312;&#22870;&#21169;&#65292;&#24573;&#35270;&#20102;&#36716;&#31227;&#24207;&#21015;&#30340;&#22238;&#39038;&#32467;&#26500;&#20013;&#25152;&#34164;&#21547;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20195;&#29702;&#22120;&#21487;&#20197;&#21033;&#29992;&#22238;&#39038;&#20449;&#24687;&#26469;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#24863;&#30693;&#33021;&#21147;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#20197;&#22522;&#20110;&#25972;&#20307;&#32780;&#38750;&#23616;&#37096;&#20449;&#24687;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#20869;&#22312;&#22870;&#21169;&#30340;&#27169;&#22411;&#8212;&#8212;&#21518;&#32493;&#21069;&#23548;&#20869;&#22312;&#25506;&#32034; (SPIE) &#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPIE &#33021;&#22815;&#20135;&#29983;&#26356;&#21152;&#39640;&#25928;&#21644;&#29983;&#24577;&#23398;&#21512;&#29702;&#30340;&#25506;&#32034;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration is essential in reinforcement learning, particularly in environments where external rewards are sparse. Here we focus on exploration with intrinsic rewards, where the agent transiently augments the external rewards with self-generated intrinsic rewards. Although the study of intrinsic rewards has a long history, existing methods focus on composing the intrinsic reward based on measures of future prospects of states, ignoring the information contained in the retrospective structure of transition sequences. Here we argue that the agent can utilise retrospective information to generate explorative behaviour with structure-awareness, facilitating efficient exploration based on global instead of local information. We propose Successor-Predecessor Intrinsic Exploration (SPIE), an exploration algorithm based on a novel intrinsic reward combining prospective and retrospective information. We show that SPIE yields more efficient and ethologically plausible exploratory behaviour in e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36125;&#21494;&#26031;&#25968;&#20540;&#31215;&#20998;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031; Stein &#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#21487;&#39640;&#25928;&#22320;&#32534;&#30721;&#31215;&#20998;&#20808;&#39564;&#20449;&#24687;&#24182;&#35745;&#31639;&#31215;&#20998;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20986;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.13248</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25968;&#20540;&#31215;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Numerical Integration with Neural Networks. (arXiv:2305.13248v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36125;&#21494;&#26031;&#25968;&#20540;&#31215;&#20998;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031; Stein &#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#21487;&#39640;&#25928;&#22320;&#32534;&#30721;&#31215;&#20998;&#20808;&#39564;&#20449;&#24687;&#24182;&#35745;&#31639;&#31215;&#20998;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20986;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#23545;&#20110;&#25968;&#20540;&#31215;&#20998;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65306;&#21487;&#20197;&#32534;&#30721;&#31215;&#20998;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#21487;&#20197;&#37327;&#21270;&#31215;&#20998;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20294;&#26159;&#65292;&#36825;&#31867;&#26041;&#27861;&#20013;&#26368;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#31215;&#20998;&#31639;&#27861;&#65288;Bayesian Quadrature&#65289;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#22240;&#27492;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031; Stein &#31070;&#32463;&#32593;&#32476;&#12290;&#20851;&#38190;&#25104;&#20998;&#26159;&#22522;&#20110; Stein &#31639;&#23376;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20197;&#21450;&#22522;&#20110; Laplace &#36817;&#20284;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#23548;&#33268;&#20102;&#22312;&#27969;&#34892;&#30340; Genz &#20989;&#25968;&#22522;&#20934;&#27979;&#35797;&#21644;&#22312;&#36125;&#21494;&#26031;&#21160;&#21147;&#31995;&#32479;&#20998;&#26512;&#20197;&#21450;&#22823;&#35268;&#27169;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#20013;&#35268;&#27169;&#30340;&#25968;&#37327;&#32423;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian probabilistic numerical methods for numerical integration offer significant advantages over their non-Bayesian counterparts: they can encode prior information about the integrand, and can quantify uncertainty over estimates of an integral. However, the most popular algorithm in this class, Bayesian quadrature, is based on Gaussian process models and is therefore associated with a high computational cost. To improve scalability, we propose an alternative approach based on Bayesian neural networks which we call Bayesian Stein networks. The key ingredients are a neural network architecture based on Stein operators, and an approximation of the Bayesian posterior based on the Laplace approximation. We show that this leads to orders of magnitude speed-ups on the popular Genz functions benchmark, and on challenging problems arising in the Bayesian analysis of dynamical systems, and the prediction of energy production for a large-scale wind farm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RHIP&#65289;&#65292;&#36890;&#36807;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#35299;&#20915;&#20102;&#20840;&#29699;&#35268;&#27169;&#30340;MDPs&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#22312;&#35895;&#27468;&#22320;&#22270;&#20013;&#23454;&#29616;&#20102;16-24%&#30340;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.11290</link><description>&lt;p&gt;
&#35895;&#27468;&#22320;&#22270;&#20013;&#30340;&#22823;&#35268;&#27169;&#21487;&#25193;&#23637;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Massively Scalable Inverse Reinforcement Learning in Google Maps. (arXiv:2305.11290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RHIP&#65289;&#65292;&#36890;&#36807;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#35299;&#20915;&#20102;&#20840;&#29699;&#35268;&#27169;&#30340;MDPs&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#22312;&#35895;&#27468;&#22320;&#22270;&#20013;&#23454;&#29616;&#20102;16-24%&#30340;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#20154;&#31867;&#28508;&#22312;&#20559;&#22909;&#26159;&#36335;&#32447;&#25512;&#33616;&#20013;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#20840;&#29699;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36807;&#21435;&#30340;&#30740;&#31350;&#20026;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#21019;&#24314;&#20102;&#36234;&#26469;&#36234;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#23578;&#26410;&#25104;&#21151;&#25193;&#23637;&#21040;&#19990;&#30028;&#35268;&#27169;&#30340;MDP&#12289;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#65292;&#20998;&#21035;&#28041;&#21450;&#25968;&#20159;&#20010;&#29366;&#24577;&#12289;&#36712;&#36857;&#21644;&#21442;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25913;&#36827;&#65292;&#32858;&#28966;&#20110;&#22270;&#21387;&#32553;&#12289;&#24182;&#34892;&#21270;&#21644;&#22522;&#20110;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#38382;&#39064;&#21021;&#22987;&#21270;&#65292;&#31361;&#30772;&#20197;&#24448;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36870;&#21521;&#35268;&#21010;&#36882;&#36827;&#22320;&#24179;&#38754;(RHIP)&#65292;&#23427;&#21487;&#20197;&#27010;&#25324;&#29616;&#26377;&#30340;&#24037;&#20316;&#65292;&#24182;&#36890;&#36807;&#20854;&#35268;&#21010;&#27700;&#24179;&#25511;&#21046;&#20851;&#38190;&#24615;&#33021;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#20840;&#29699;&#36335;&#32447;&#36136;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;16-24%&#30340;&#25913;&#36827;&#65292;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#26159;&#36804;&#20170;&#20026;&#27490;&#23454;&#29616;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#26368;&#22823;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#26356;&#22909;&#30340;&#23548;&#33322;&#34892;&#20026;&#21644;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing for humans' latent preferences is a grand challenge in route recommendation, where globally-scalable solutions remain an open problem. Although past work created increasingly general solutions for the application of inverse reinforcement learning (IRL), these have not been successfully scaled to world-sized MDPs, large datasets, and highly parameterized models; respectively hundreds of millions of states, trajectories, and parameters. In this work, we surpass previous limitations through a series of advancements focused on graph compression, parallelization, and problem initialization based on dominant eigenvectors. We introduce Receding Horizon Inverse Planning (RHIP), which generalizes existing work and enables control of key performance trade-offs via its planning horizon. Our policy achieves a 16-24% improvement in global route quality, and, to our knowledge, represents the largest instance of IRL in a real-world setting to date. Our results show critical benefits to mor
&lt;/p&gt;</description></item><item><title>DUDE&#25512;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#22522;&#20934;&#27979;&#35797;&#24182;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;</title><link>http://arxiv.org/abs/2305.08455</link><description>&lt;p&gt;
&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#65288;DUDE&#65289;
&lt;/p&gt;
&lt;p&gt;
Document Understanding Dataset and Evaluation (DUDE). (arXiv:2305.08455v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08455
&lt;/p&gt;
&lt;p&gt;
DUDE&#25512;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#26088;&#22312;&#21019;&#36896;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#22522;&#20934;&#27979;&#35797;&#24182;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21628;&#21505;&#25991;&#26723;AI&#31038;&#21306;&#37325;&#26032;&#35780;&#20272;&#24403;&#21069;&#30340;&#26041;&#27861;&#35770;&#65292;&#25317;&#25265;&#21019;&#24314;&#26356;&#23454;&#38469;&#21462;&#21521;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;&#25991;&#26723;&#29702;&#35299;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#65288;DUDE&#65289;&#26088;&#22312;&#32416;&#27491;&#22312;&#29702;&#35299;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#65288;VRD&#65289;&#26041;&#38754;&#30340;&#30740;&#31350;&#36827;&#23637;&#20572;&#28382;&#19981;&#21069;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#22810;&#34892;&#19994;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#39029;VRD&#30456;&#20851;&#30340;&#38382;&#39064;&#31867;&#22411;&#12289;&#31572;&#26696;&#21644;&#25991;&#26723;&#24067;&#23616;&#30340;&#21019;&#26032;&#65292;&#20855;&#26377;&#21508;&#31181;&#26469;&#28304;&#21644;&#26085;&#26399;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#30340;&#35780;&#20272;&#35774;&#32622;&#26469;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#36793;&#30028;&#65292;&#36825;&#20123;&#35774;&#32622;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#38656;&#35201;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36827;&#34892;&#24378;&#22823;&#30340;&#27867;&#21270;&#21644;&#36866;&#24212;&#12290;DUDE&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#26356;&#23454;&#38469;&#12289;&#26356;&#38271;&#26399;&#30340;&#22522;&#20934;&#27979;&#35797;&#26631;&#20934;&#65292;&#24182;&#24076;&#26395;&#23427;&#20250;&#24341;&#39046;&#26410;&#26469;&#30340;&#25193;&#23637;&#21644;&#36129;&#29486;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35828;&#26126;&#20102;&#20197;&#19979;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We call on the Document AI (DocAI) community to reevaluate current methodologies and embrace the challenge of creating more practically-oriented benchmarks. Document Understanding Dataset and Evaluation (DUDE) seeks to remediate the halted research progress in understanding visually-rich documents (VRDs). We present a new dataset with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins, and dates. Moreover, we are pushing the boundaries of current methods by creating multi-task and multi-domain evaluation setups that more accurately simulate real-world situations where powerful generalization and adaptation under low-resource settings are desired. DUDE aims to set a new standard as a more practical, long-standing benchmark for the community, and we hope that it will lead to future extensions and contributions that address real-world challenges. Finally, our work illustrates the importance o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07247</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#30340;Schr\"odinger bridge&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provably Convergent Schr\"odinger Bridge with Applications to Probabilistic Time Series Imputation. (arXiv:2305.07247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Schr\"odinger bridge&#38382;&#39064;&#65288;SBP&#65289;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#30340;&#25237;&#24433;&#26159;&#21807;&#19968;&#21487;&#29992;&#30340;&#65292;&#20854;&#25910;&#25947;&#24615;&#36824;&#19981;&#26159;&#21313;&#20998;&#28165;&#26970;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#25237;&#24433;&#30340;Schr\"odinger bridge&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;SBP&#24212;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#22635;&#20805;&#65292;&#23637;&#31034;&#20102;&#20248;&#21270;&#20256;&#36755;&#25104;&#26412;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#29615;&#22659;&#25968;&#25454;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Schr\"odinger bridge problem (SBP) is gaining increasing attention in generative modeling and showing promising potential even in comparison with the score-based generative models (SGMs). SBP can be interpreted as an entropy-regularized optimal transport problem, which conducts projections onto every other marginal alternatingly. However, in practice, only approximated projections are accessible and their convergence is not well understood. To fill this gap, we present a first convergence analysis of the Schr\"odinger bridge algorithm based on approximated projections. As for its practical applications, we apply SBP to probabilistic time series imputation by generating missing values conditioned on observed data. We show that optimizing the transport cost improves the performance and the proposed algorithm achieves the state-of-the-art result in healthcare and environmental data while exhibiting the advantage of exploring both temporal and feature patterns in probabilistic time ser
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#36890;&#25968;&#25454;&#25554;&#20540;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#21644;&#21452;&#21521;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22788;&#29702;&#32570;&#22833;&#20540;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.06480</link><description>&lt;p&gt;
ST-GIN:&#19968;&#31181;&#20855;&#26377;&#26102;&#31354;&#22270;&#27880;&#24847;&#21147;&#21644;&#21452;&#21521;&#24490;&#29615;&#32852;&#21512;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#36890;&#25968;&#25454;&#25554;&#20540;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ST-GIN: An Uncertainty Quantification Approach in Traffic Data Imputation with Spatio-temporal Graph Attention and Bidirectional Recurrent United Neural Networks. (arXiv:2305.06480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20132;&#36890;&#25968;&#25454;&#25554;&#20540;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#21644;&#21452;&#21521;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#26102;&#31354;&#30456;&#20851;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#22788;&#29702;&#32570;&#22833;&#20540;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25968;&#25454;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20174;&#29615;&#36335;&#26816;&#27979;&#22120;&#25110;&#31867;&#20284;&#26469;&#28304;&#25910;&#38598;&#30340;&#29616;&#23454;&#19990;&#30028;&#20132;&#36890;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#32570;&#22833;&#20540;(MVs)&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#30456;&#20851;&#24212;&#29992;&#21644;&#30740;&#31350;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#24674;&#22797;&#36825;&#20123;&#32570;&#22833;&#20540;&#65292;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#25968;&#23383;&#32479;&#35745;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#31561;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#25454;&#25554;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25554;&#20540;&#32570;&#22833;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22270;&#27880;&#24847;&#26550;&#26500;&#26469;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#21452;&#21521;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#22522;&#20934;&#25216;&#26415;&#65292;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic data serves as a fundamental component in both research and applications within intelligent transportation systems. However, real-world transportation data, collected from loop detectors or similar sources, often contain missing values (MVs), which can adversely impact associated applications and research. Instead of discarding this incomplete data, researchers have sought to recover these missing values through numerical statistics, tensor decomposition, and deep learning techniques. In this paper, we propose an innovative deep-learning approach for imputing missing data. A graph attention architecture is employed to capture the spatial correlations present in traffic data, while a bidirectional neural network is utilized to learn temporal information. Experimental results indicate that our proposed method outperforms all other benchmark techniques, thus demonstrating its effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.04940</link><description>&lt;p&gt;
&#26089;&#36215;&#30340;&#40479;&#20799;&#25417;&#21040;&#34411;&#65306;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#36827;&#34892;&#26356;&#26377;&#25928;&#30340;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#22914;&#28431;&#27934;&#26816;&#27979;&#21644;&#31867;&#22411;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#20013;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#30340;&#26368;&#20339;&#21033;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#26500;&#24314;&#20195;&#30721;&#30340;&#22797;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;12&#31181;&#21019;&#24314;&#22797;&#21512;&#34920;&#31034;&#30340;&#31574;&#30053;&#19982;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#20010;&#32534;&#30721;&#22120;&#23618;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#22312;CodeBERT&#27169;&#22411;&#19978;&#23454;&#35777;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#22312;4&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20960;&#20010;&#26089;&#26399;&#23618;&#30340;&#32452;&#21512;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#20135;&#29983;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19968;&#20123;&#32452;&#21512;&#21017;&#25913;&#36827;&#20102;&#22810;&#31867;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#24179;&#22343;&#26816;&#27979;&#22686;&#24378;2&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.10701</link><description>&lt;p&gt;
&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21305;&#37197;&#30340;&#29983;&#25104;&#27169;&#22411;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#20272;&#20540;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#20272;&#20540;&#25928;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20272;&#20540;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#26377;&#21161;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#24182;&#20445;&#25252;&#25968;&#25454;&#29305;&#24615;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21028;&#21035;&#27169;&#22411;&#19978;&#65292;&#24573;&#30053;&#20102;&#26368;&#36817;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#21028;&#21035;&#27169;&#22411;&#31867;&#20284;&#65292;&#38656;&#35201;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#25968;&#25454;&#36129;&#29486;&#30340;&#32039;&#36843;&#38656;&#27714;&#20063;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21028;&#21035;&#27169;&#22411;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#22312;&#23454;&#38469;&#20013;&#30452;&#25509;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#36817;&#26399;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#35282;&#24230;&#23545;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#20272;&#20540;&#38382;&#39064;&#36827;&#34892;&#20102;&#26500;&#24314;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;Generative Model Valuator&#8221;&#65288;GMValuator&#65289;&#8212;&#8212;&#31532;&#19968;&#20010;&#38024;&#23545;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#26088;&#22312;&#20026;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#25968;&#25454;&#20272;&#20540;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#23454;&#20363;&#21450;&#30001;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#24212;&#21512;&#25104;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#20272;&#35745;&#21407;&#22987;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20026;&#19981;&#21516;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#35780;&#20272;&#25968;&#25454;&#23454;&#20363;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data valuation is critical in machine learning, as it helps enhance model transparency and protect data properties. Existing data valuation methods have primarily focused on discriminative models, neglecting deep generative models that have recently gained considerable attention. Similar to discriminative models, there is an urgent need to assess data contributions in deep generative models as well. However, previous data valuation approaches mainly relied on discriminative model performance metrics and required model retraining. Consequently, they cannot be applied directly and efficiently to recent deep generative models, such as generative adversarial networks and diffusion models, in practice. To bridge this gap, we formulate the data valuation problem in generative models from a similarity-matching perspective. Specifically, we introduce Generative Model Valuator (GMValuator), the first model-agnostic approach for any generative models, designed to provide data valuation for gener
&lt;/p&gt;</description></item><item><title>SURFSUP&#37319;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#36830;&#32493;&#34920;&#31034;&#23545;&#35937;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#27969;&#20307;&#23545;&#35937;&#30456;&#20114;&#20316;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#65307;&#19988;&#33021;&#22815;&#36866;&#29992;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#22797;&#26434;&#30495;&#23454;&#22330;&#26223;&#21644;&#23545;&#35937;&#65292;&#21487;&#20197;&#21453;&#28436;&#36866;&#29992;&#20110;&#29289;&#20307;&#25805;&#32437;&#27969;&#20307;&#27969;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.06197</link><description>&lt;p&gt;
SURFSUP&#65306;&#23398;&#20064;&#26032;&#39062;&#34920;&#38754;&#27969;&#20307;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
SURFSUP: Learning Fluid Simulation for Novel Surfaces. (arXiv:2304.06197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06197
&lt;/p&gt;
&lt;p&gt;
SURFSUP&#37319;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#36830;&#32493;&#34920;&#31034;&#23545;&#35937;&#65292;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#27969;&#20307;&#23545;&#35937;&#30456;&#20114;&#20316;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#65307;&#19988;&#33021;&#22815;&#36866;&#29992;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#22797;&#26434;&#30495;&#23454;&#22330;&#26223;&#21644;&#23545;&#35937;&#65292;&#21487;&#20197;&#21453;&#28436;&#36866;&#29992;&#20110;&#29289;&#20307;&#25805;&#32437;&#27969;&#20307;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#35745;&#12289;&#22270;&#24418;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#27169;&#25311;&#22797;&#26434;&#22330;&#26223;&#20013;&#27969;&#20307;&#30340;&#21147;&#23398;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#24555;&#36895;&#21644;&#21487;&#24494;&#30340;&#27969;&#20307;&#27169;&#25311;&#22120;&#65292;&#20294;&#26159;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#19981;&#33021;&#31934;&#30830;&#22320;&#27169;&#25311;&#27969;&#20307;&#22914;&#20309;&#19982;&#22312;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#39062;&#34920;&#38754;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SURFSUP&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#38544;&#24335;&#34920;&#31034;&#23545;&#35937;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#26174;&#24335;&#34920;&#31034;&#32593;&#26684;&#25110;&#31890;&#23376;&#12290;&#20960;&#20309;&#20307;&#30340;&#36825;&#31181;&#36830;&#32493;&#34920;&#31034;&#20351;&#24471;&#22312;&#38271;&#26102;&#38388;&#27573;&#20869;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#27969;&#20307;&#23545;&#35937;&#30456;&#20114;&#20316;&#29992;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20351;&#35745;&#31639;&#26356;&#21152;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;SURFSUP&#22312;&#31616;&#21333;&#30340;&#24418;&#29366;&#22522;&#20803;&#19978;&#35757;&#32451;&#65292;&#33021;&#22815;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#20998;&#24067;&#20043;&#22806;&#65292;&#29978;&#33267;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#30495;&#23454;&#22330;&#26223;&#21644;&#23545;&#35937;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#21453;&#28436;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#35774;&#35745;&#31616;&#21333;&#30340;&#29289;&#20307;&#26469;&#25805;&#32437;&#27969;&#20307;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the mechanics of fluid in complex scenes is vital to applications in design, graphics, and robotics. Learning-based methods provide fast and differentiable fluid simulators, however most prior work is unable to accurately model how fluids interact with genuinely novel surfaces not seen during training. We introduce SURFSUP, a framework that represents objects implicitly using signed distance functions (SDFs), rather than an explicit representation of meshes or particles. This continuous representation of geometry enables more accurate simulation of fluid-object interactions over long time periods while simultaneously making computation more efficient. Moreover, SURFSUP trained on simple shape primitives generalizes considerably out-of-distribution, even to complex real-world scenes and objects. Finally, we show we can invert our model to design simple objects to manipulate fluid flow.
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31995;&#32479;&#35782;&#21035;&#23398;&#20064;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23558;&#22810;&#20010;&#31995;&#32479;&#21010;&#20998;&#20026;&#32676;&#38598;&#65292;&#21516;&#19968;&#31751;&#20013;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#20854;&#20182;&#31995;&#32479;&#30340;&#35266;&#23519;&#20013;&#33719;&#30410;&#12290;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#27491;&#30830;&#20272;&#35745;&#32676;&#38598;&#26631;&#35782;&#24182;&#20855;&#26377;&#39640;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#31995;&#32479;&#35782;&#21035;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.01395</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#31995;&#32479;&#35782;&#21035;&#23398;&#20064;&#20010;&#24615;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Personalized Models with Clustered System Identification. (arXiv:2304.01395v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31995;&#32479;&#35782;&#21035;&#23398;&#20064;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23558;&#22810;&#20010;&#31995;&#32479;&#21010;&#20998;&#20026;&#32676;&#38598;&#65292;&#21516;&#19968;&#31751;&#20013;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#20854;&#20182;&#31995;&#32479;&#30340;&#35266;&#23519;&#20013;&#33719;&#30410;&#12290;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#27491;&#30830;&#20272;&#35745;&#32676;&#38598;&#26631;&#35782;&#24182;&#20855;&#26377;&#39640;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#31995;&#32479;&#35782;&#21035;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20174;&#19981;&#21516;&#31995;&#32479;&#21160;&#21147;&#23398;&#35266;&#23519;&#22810;&#20010;&#36712;&#36857;&#20013;&#23398;&#20064;&#32447;&#24615;&#31995;&#32479;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#21253;&#25324;&#21327;&#20316;&#22330;&#26223;&#65292;&#20854;&#20013;&#23547;&#27714;&#20272;&#35745;&#20854;&#21160;&#21147;&#23398;&#30340;&#22810;&#20010;&#31995;&#32479;&#34987;&#21010;&#20998;&#20026;&#26681;&#25454;&#20854;&#31995;&#32479;&#30456;&#20284;&#24615;&#30340;&#32676;&#38598;&#12290;&#22240;&#27492;&#65292;&#21516;&#19968;&#31751;&#20013;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#20854;&#20182;&#31995;&#32479;&#30340;&#35266;&#23519;&#20013;&#33719;&#30410;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#27599;&#20010;&#31995;&#32479;&#20132;&#26367;&#20272;&#35745;&#20854;&#32676;&#38598;&#26631;&#35782;&#24182;&#25191;&#34892;&#21160;&#24577;&#20272;&#35745;&#12290;&#28982;&#21518;&#32858;&#21512;&#20197;&#26356;&#26032;&#27599;&#20010;&#32676;&#38598;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#27491;&#30830;&#22320;&#20272;&#35745;&#20102;&#32676;&#38598;&#26631;&#35782;&#65292;&#24182;&#23454;&#29616;&#20102;&#36817;&#20284;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20854;&#19982;&#32676;&#38598;&#20013;&#31995;&#32479;&#25968;&#25104;&#21453;&#27604;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#26356;&#39640;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#31995;&#32479;&#35782;&#21035;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of learning linear system models from observing multiple trajectories from different system dynamics. This framework encompasses a collaborative scenario where several systems seeking to estimate their dynamics are partitioned into clusters according to their system similarity. Thus, the systems within the same cluster can benefit from the observations made by the others. Considering this framework, we present an algorithm where each system alternately estimates its cluster identity and performs an estimation of its dynamics. This is then aggregated to update the model of each cluster. We show that under mild assumptions, our algorithm correctly estimates the cluster identities and achieves an approximate sample complexity that scales inversely with the number of systems in the cluster, thus facilitating a more efficient and personalized system identification process.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#30340;&#29366;&#24577;&#20998;&#24067;&#26469;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#23376;&#20363;&#31243;&#30340;&#20840;&#23616;&#25506;&#32034;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#30340;&#21152;&#36895;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14623</link><description>&lt;p&gt;
&#26080;&#38656;&#24378;&#21270;&#23398;&#20064;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning without Reinforcement Learning. (arXiv:2303.14623v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#30340;&#29366;&#24577;&#20998;&#24067;&#26469;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#23376;&#20363;&#31243;&#30340;&#20840;&#23616;&#25506;&#32034;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#30340;&#21152;&#36895;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064; (IRL) &#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#23398;&#20064;&#21512;&#20046;&#36923;&#36753;&#30340;&#19987;&#23478;&#28436;&#31034;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;IRL&#26041;&#27861;&#23384;&#22312;&#35745;&#31639;&#19978;&#30340;&#24369;&#28857;&#65306;&#23427;&#20204;&#38656;&#35201;&#23558;&#35299;&#20915;&#38590;&#24230;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#20316;&#20026;&#23376;&#20363;&#31243;&#36827;&#34892;&#21453;&#22797;&#27714;&#35299;&#12290;&#36825;&#19982;&#24402;&#32422;&#30340;&#35266;&#28857;&#30456;&#30683;&#30462;&#65306;&#25105;&#20204;&#24050;&#23558;&#27169;&#20223;&#23398;&#20064;&#30340;&#36739;&#26131;&#38382;&#39064;&#24402;&#32422;&#20026;&#21453;&#22797;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#30340;&#26356;&#38590;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#24037;&#20316;&#35777;&#26126;&#65292;&#35775;&#38382;&#24378;&#31574;&#30053;&#33457;&#36153;&#26102;&#38388;&#30340;&#29366;&#24577;&#20998;&#24067;&#30340;&#20391;&#38754;&#20449;&#24687;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#35299;&#20915;RL&#38382;&#39064;&#30340;&#26679;&#26412;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#21152;&#26126;&#26234;&#30340;&#27169;&#20223;&#23398;&#20064;&#31616;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#30340;&#29366;&#24577;&#20998;&#24067;&#26469;&#32531;&#35299;RL&#23376;&#20363;&#31243;&#30340;&#20840;&#23616;&#25506;&#32034;&#37096;&#20998;&#65292;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#25351;&#25968;&#32423;&#30340;&#21152;&#36895;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#20013;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#37117;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;IRL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice
&lt;/p&gt;</description></item><item><title>&#20803;&#23431;&#23449;&#20013;&#30340;&#28145;&#24230;&#20266;&#36896;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#34394;&#25311;&#28216;&#25103;&#12289;&#20250;&#35758;&#21644;&#21150;&#20844;&#22330;&#25152;&#31561;&#22330;&#26223;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#20266;&#36896;&#20882;&#20805;&#20182;&#20154;&#36827;&#34892;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.14612</link><description>&lt;p&gt;
&#20803;&#23431;&#23449;&#20013;&#30340;&#28145;&#24230;&#20266;&#36896;: &#23545;&#34394;&#25311;&#28216;&#25103;&#12289;&#20250;&#35758;&#21644;&#21150;&#20844;&#22330;&#25152;&#30340;&#23433;&#20840;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Deepfake in the Metaverse: Security Implications for Virtual Gaming, Meetings, and Offices. (arXiv:2303.14612v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14612
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#20013;&#30340;&#28145;&#24230;&#20266;&#36896;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#34394;&#25311;&#28216;&#25103;&#12289;&#20250;&#35758;&#21644;&#21150;&#20844;&#22330;&#25152;&#31561;&#22330;&#26223;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#20266;&#36896;&#20882;&#20805;&#20182;&#20154;&#36827;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#30001;&#20110;&#20854;&#21019;&#36896;&#23436;&#20840;&#27785;&#28024;&#24335;&#21644;&#20132;&#20114;&#24335;&#34394;&#25311;&#19990;&#30028;&#30340;&#28508;&#21147;&#65292;&#24341;&#36215;&#20102;&#21508;&#34892;&#21508;&#19994;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#20266;&#36896;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#25972;&#21512;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#20882;&#20805;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20266;&#36896;&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#23433;&#20840;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#28216;&#25103;&#12289;&#22312;&#32447;&#20250;&#35758;&#21644;&#34394;&#25311;&#21150;&#20844;&#22330;&#25152;&#30340;&#32972;&#26223;&#19979;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#28145;&#24230;&#20266;&#36896;&#22914;&#20309;&#29992;&#20110;&#20882;&#20805;&#28216;&#25103;&#22330;&#26223;&#20013;&#30340;&#35282;&#33394;&#65292;&#22914;&#20309;&#22312;&#20803;&#23431;&#23449;&#30340;&#22312;&#32447;&#20250;&#35758;&#20013;&#25171;&#24320;&#20882;&#20805;&#30340;&#22823;&#38376;&#65292;&#20197;&#21450;&#22312;&#20803;&#23431;&#23449;&#30340;&#34394;&#25311;&#21150;&#20844;&#22330;&#25152;&#32570;&#20047;&#36523;&#20221;&#35748;&#35777;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#26356;&#23481;&#26131;&#20882;&#20805;&#20182;&#20154;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20123;&#23433;&#20840;&#38382;&#39064;&#23545;&#26426;&#23494;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#65288;CIA&#19977;&#20803;&#24615;&#65289;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#19982;&#12300;&#26263;&#40657;&#23431;&#23449;&#12301;&#21644;&#25968;&#23383;&#22797;&#21046;&#31561;&#30456;&#20851;&#38382;&#39064;&#65292;&#20197;&#21450;&#19982;&#27492;&#30456;&#20851;&#30340;&#30417;&#31649;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The metaverse has gained significant attention from various industries due to its potential to create a fully immersive and interactive virtual world. However, the integration of deepfakes in the metaverse brings serious security implications, particularly with regard to impersonation. This paper examines the security implications of deepfakes in the metaverse, specifically in the context of gaming, online meetings, and virtual offices. The paper discusses how deepfakes can be used to impersonate in gaming scenarios, how online meetings in the metaverse open the door for impersonation, and how virtual offices in the metaverse lack physical authentication, making it easier for attackers to impersonate someone. The implications of these security concerns are discussed in relation to the confidentiality, integrity, and availability (CIA) triad. The paper further explores related issues such as the darkverse, and digital cloning, as well as regulatory and privacy concerns associated with a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07925</link><description>&lt;p&gt;
&#36890;&#36807; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#26696;&#20363;&#65292;&#29702;&#35299;&#26102;&#38388;&#34920;&#26684;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#20174; Numerai &#25968;&#25454;&#31454;&#36187;&#21019;&#24314;&#30340;&#29305;&#24449;&#30446;&#26631;&#20132;&#21449;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#39044;&#27979;&#20250;&#25910;&#25947;&#21040;&#21487;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21051;&#30011;&#30340;&#30456;&#21516;&#24179;&#34913;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#38543;&#21518;&#37319;&#29992;&#23725;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#19982;&#19968;&#20123;&#24120;&#29992;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; LSTM &#21644; transformer&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65288;&#22312;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#19979;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#26041;&#24046;&#65292;&#19988;&#23545;&#26550;&#26500;&#30340;&#36873;&#25321;&#19981;&#22826;&#25935;&#24863;&#65289;&#65292;&#24182;&#19988;&#26356;&#26377;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#24517;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06053</link><description>&lt;p&gt;
TSMixer&#65306;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06053
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22810;&#21464;&#37327;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#12290;&#20026;&#20102;&#25429;&#33719;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#20687;&#24490;&#29615;&#25110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36825;&#26679;&#30340;&#39640;&#23481;&#37327;&#32467;&#26500;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#22312;&#20960;&#20010;&#24120;&#29992;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25193;&#23637;&#23427;&#20204;&#65292;&#26412;&#25991;&#30740;&#31350;&#32447;&#24615;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#24207;&#28151;&#21512;&#22120;&#65288;TSMixer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#26032;&#22411;&#32467;&#26500;&#12290; TSMixer&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#30340;&#28151;&#21512;&#25805;&#20316;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#22312;&#27969;&#34892;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#31616;&#21333;&#26131;&#34892;&#30340;TSMixer&#19982;&#21033;&#29992;&#29305;&#23450;&#22522;&#20934;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#19987;&#19994;&#20808;&#36827;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22823;&#35268;&#27169;&#30340;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21363;&#19968;&#20010;&#23454;&#38469;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#19978;&#65292;TSMixer&#34920;&#29616;&#20986;&#38750;&#24120;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world time-series datasets are often multivariate with complex dynamics. To capture this complexity, high capacity architectures like recurrent- or attention-based sequential deep learning models have become popular. However, recent work demonstrates that simple univariate linear models can outperform such deep learning models on several commonly used academic benchmarks. Extending them, in this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), a novel architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along both the time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates super
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#22495;&#20013;&#23454;&#29616;&#21487;&#20445;&#35777;&#23433;&#20840;&#30340;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#35843;&#25972;CBF&#30340;&#36229;&#21442;&#25968;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#29615;&#22659;&#26465;&#20214;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26080;&#27169;&#22411;&#30340;CBF&#35843;&#25972;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.04313</link><description>&lt;p&gt;
&#22312;&#32447;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#29992;&#20110;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Online Control Barrier Functions for Decentralized Multi-Agent Navigation. (arXiv:2303.04313v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36830;&#32493;&#22495;&#20013;&#23454;&#29616;&#21487;&#20445;&#35777;&#23433;&#20840;&#30340;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#35843;&#25972;CBF&#30340;&#36229;&#21442;&#25968;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#29615;&#22659;&#26465;&#20214;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26080;&#27169;&#22411;&#30340;CBF&#35843;&#25972;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;(CBF)&#22312;&#36830;&#32493;&#22495;&#20013;&#23454;&#29616;&#20102;&#21487;&#20445;&#35777;&#23433;&#20840;&#30340;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#24471;&#21040;&#30340;&#23548;&#33322;&#24615;&#33021;&#39640;&#24230;&#25935;&#24863;&#20110;&#24213;&#23618;&#36229;&#21442;&#25968;&#12290;&#20256;&#32479;&#26041;&#27861;&#32771;&#34385;&#20102;&#22266;&#23450;&#30340;CBF&#65288;&#21442;&#25968;&#39044;&#20808;&#35843;&#25972;&#65289;&#65292;&#22240;&#27492;&#36890;&#24120;&#22312;&#26434;&#20081;&#21644;&#39640;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#65306;&#20445;&#23432;&#30340;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#20195;&#29702;&#36712;&#36857;&#20302;&#25928;&#65292;&#29978;&#33267;&#26080;&#27861;&#36798;&#21040;&#30446;&#26631;&#20301;&#32622;&#65292;&#32780;&#28608;&#36827;&#30340;&#21442;&#25968;&#20540;&#21487;&#33021;&#23548;&#33268;&#21453;&#24212;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#32447;CBFs&#65292;&#20854;&#20013;&#36229;&#21442;&#25968;&#38543;&#30528;&#20195;&#29702;&#22312;&#20854;&#37051;&#36817;&#29615;&#22659;&#20013;&#30340;&#24863;&#30693;&#23454;&#26102;&#35843;&#25972;&#12290;&#30001;&#20110;CBFs&#21644;&#23548;&#33322;&#24615;&#33021;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#31995;&#24456;&#38590;&#24314;&#27169;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20197;&#26080;&#27169;&#22411;&#30340;&#26041;&#24335;&#23398;&#20064;CBF&#35843;&#25972;&#31574;&#30053;&#12290;&#30001;&#20110;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#23545;&#31574;&#30053;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#26102;&#36866;&#24212;&#19981;&#21516;&#30340;&#29615;&#22659;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Control barrier functions (CBFs) enable guaranteed safe multi-agent navigation in the continuous domain. The resulting navigation performance, however, is highly sensitive to the underlying hyperparameters. Traditional approaches consider fixed CBFs (where parameters are tuned apriori), and hence, typically do not perform well in cluttered and highly dynamic environments: conservative parameter values can lead to inefficient agent trajectories, or even failure to reach goal positions, whereas aggressive parameter values can lead to infeasible controls. To overcome these issues, in this paper, we propose online CBFs, whereby hyperparameters are tuned in real-time, as a function of what agents perceive in their immediate neighborhood. Since the explicit relationship between CBFs and navigation performance is hard to model, we leverage reinforcement learning to learn CBF-tuning policies in a model-free manner. Because we parameterize the policies with graph neural networks (GNNs), we are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#22909;&#22855;&#24515;&#35302;&#21457;&#25506;&#32034;&#30340;&#25913;&#36827;&#29256;&#26412;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#65288;CCEM&#65289;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#35268;&#21010;&#26041;&#27861;&#22312;&#22797;&#26434;&#39640;&#32500;&#29615;&#22659;&#19979;&#26080;&#27861;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#35268;&#21010;&#35270;&#35282;&#20869;&#29366;&#24577;-&#21160;&#20316;Q&#20540;&#30340;&#21644;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#40723;&#21169;&#36798;&#21040;&#26032;&#39062;&#30340;&#35266;&#23519;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.03787</link><description>&lt;p&gt;
&#29992;&#22909;&#22855;&#24515;&#20132;&#21449;&#29109;&#26041;&#27861;&#21644;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#30340;&#26679;&#26412;&#39640;&#25928;&#23454;&#26102;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning. (arXiv:2303.03787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#22909;&#22855;&#24515;&#35302;&#21457;&#25506;&#32034;&#30340;&#25913;&#36827;&#29256;&#26412;&#30340;&#20132;&#21449;&#29109;&#26041;&#27861;&#65288;CCEM&#65289;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#35268;&#21010;&#26041;&#27861;&#22312;&#22797;&#26434;&#39640;&#32500;&#29615;&#22659;&#19979;&#26080;&#27861;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#35268;&#21010;&#35270;&#35282;&#20869;&#29366;&#24577;-&#21160;&#20316;Q&#20540;&#30340;&#21644;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#40723;&#21169;&#36798;&#21040;&#26032;&#39062;&#30340;&#35266;&#23519;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#19982;&#23454;&#26102;&#35268;&#21010;&#22312;&#36816;&#21160;&#21644;&#25805;&#20316;&#25511;&#21046;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#22914;&#20132;&#21449;&#29109;&#26041;&#27861;&#65288;CEM&#65289;&#65292;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#26080;&#27861;&#24456;&#22909;&#22320;&#25193;&#23637;&#12290;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#32570;&#20047;&#25506;&#32034;&#65292;&#22240;&#20026;&#36825;&#20123;&#35268;&#21010;&#26041;&#27861;&#21482;&#26088;&#22312;&#26368;&#22823;&#21270;&#35268;&#21010;&#35270;&#35282;&#20869;&#30340;&#32047;&#31215;&#22806;&#22312;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#22312;&#27809;&#26377;&#35266;&#23519;&#21040;&#30340;&#32039;&#20945;&#28508;&#22312;&#31354;&#38388;&#20869;&#36827;&#34892;&#35268;&#21010;&#20351;&#24471;&#20351;&#29992;&#22522;&#20110;&#22909;&#22855;&#24515;&#30340;&#20869;&#22312;&#21160;&#26426;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22909;&#22855;&#24515;&#20132;&#21449;&#29109;&#26041;&#27861;&#65288;CCEM&#65289;&#65292;&#36825;&#26159;&#23545;CEM&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#29992;&#20110;&#36890;&#36807;&#22909;&#22855;&#24515;&#40723;&#21169;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35268;&#21010;&#35270;&#35282;&#20869;&#26368;&#22823;&#21270;&#29366;&#24577;-&#21160;&#20316;Q&#20540;&#30340;&#21644;&#65292;&#20854;&#20013;&#36825;&#20123;Q&#20540;&#20272;&#35745;&#26410;&#26469;&#30340;&#22806;&#22312;&#21644;&#20869;&#22312;&#22870;&#21169;&#65292;&#20174;&#32780;&#40723;&#21169;&#36798;&#21040;&#26032;&#39062;&#30340;&#35266;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL) with real-time planning has shown great potential in locomotion and manipulation control tasks. However, the existing planning methods, such as the Cross-Entropy Method (CEM), do not scale well to complex high-dimensional environments. One of the key reasons for underperformance is the lack of exploration, as these planning methods only aim to maximize the cumulative extrinsic reward over the planning horizon. Furthermore, planning inside the compact latent space in the absence of observations makes it challenging to use curiosity-based intrinsic motivation. We propose Curiosity CEM (CCEM), an improved version of the CEM algorithm for encouraging exploration via curiosity. Our proposed method maximizes the sum of state-action Q values over the planning horizon, in which these Q values estimate the future extrinsic and intrinsic reward, hence encouraging reaching novel observations. In addition, our model uses contrastive representation learning
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#24555;&#36895;&#21457;&#23637;&#30340;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#23545;&#20154;&#31867;&#36523;&#20221;&#39564;&#35777;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#24050;&#25552;&#20986;&#22810;&#31181;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31639;&#27861;&#65292;&#20294;&#26816;&#27979;&#22120;&#32463;&#24120;&#19981;&#21487;&#38752;&#19988;&#26080;&#27861;&#26816;&#27979;&#21040;&#28145;&#24230;&#20266;&#36896;&#20869;&#23481;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#26816;&#27979;&#22120;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#39044;&#22788;&#29702;&#20013;&#30340;&#20266;&#36896;&#30165;&#36857;&#21644;&#26410;&#32771;&#34385;&#21040;&#26032;&#26679;&#26412;&#29983;&#25104;&#22120;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#20197;&#21019;&#24314;&#26356;&#31283;&#20581;&#21487;&#38752;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.13156</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#20250;&#22833;&#36133;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Do Facial Deepfake Detectors Fail?. (arXiv:2302.13156v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13156
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#24555;&#36895;&#21457;&#23637;&#30340;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#23545;&#20154;&#31867;&#36523;&#20221;&#39564;&#35777;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#24050;&#25552;&#20986;&#22810;&#31181;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31639;&#27861;&#65292;&#20294;&#26816;&#27979;&#22120;&#32463;&#24120;&#19981;&#21487;&#38752;&#19988;&#26080;&#27861;&#26816;&#27979;&#21040;&#28145;&#24230;&#20266;&#36896;&#20869;&#23481;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#26816;&#27979;&#22120;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#39044;&#22788;&#29702;&#20013;&#30340;&#20266;&#36896;&#30165;&#36857;&#21644;&#26410;&#32771;&#34385;&#21040;&#26032;&#26679;&#26412;&#29983;&#25104;&#22120;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#20197;&#21019;&#24314;&#26356;&#31283;&#20581;&#21487;&#38752;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#24555;&#36895;&#21457;&#23637;&#65292;&#20351;&#24471;&#21019;&#24314;&#39640;&#24230;&#36924;&#30495;&#30340;&#20266;&#36896;&#23186;&#20307;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#38899;&#39057;&#12290;&#36825;&#20123;&#26448;&#26009;&#23545;&#20154;&#31867;&#36523;&#20221;&#39564;&#35777;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20363;&#22914;&#20882;&#20805;&#12289;&#35823;&#23548;&#29978;&#33267;&#23545;&#22269;&#23478;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#12290;&#20026;&#20102;&#36319;&#19978;&#36825;&#20123;&#24555;&#36895;&#21457;&#23637;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#31639;&#27861;&#65292;&#20174;&#32780;&#24341;&#21457;&#20102;&#20266;&#36896;&#32773;&#21644;&#26816;&#27979;&#22120;&#20043;&#38388;&#30340;&#25345;&#32493;&#21338;&#24328;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#32463;&#24120;&#23384;&#22312;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#19988;&#32463;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#28145;&#24230;&#20266;&#36896;&#30340;&#20869;&#23481;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#20171;&#32461;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#65288;1&#65289;&#39044;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#20266;&#36896;&#22270;&#20687;&#30165;&#36857;&#65288;artifact&#65289;&#21644;&#65288;2&#65289;&#22312;&#26500;&#24314;&#38450;&#24481;&#27169;&#22411;&#26102;&#26410;&#32771;&#34385;&#21040;&#29983;&#25104;&#26032;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#28145;&#24230;&#20266;&#36896;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#22312;&#36825;&#20010;&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#20197;&#21019;&#24314;&#26356;&#20026;&#31283;&#20581;&#21487;&#38752;&#30340;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent rapid advancements in deepfake technology have allowed the creation of highly realistic fake media, such as video, image, and audio. These materials pose significant challenges to human authentication, such as impersonation, misinformation, or even a threat to national security. To keep pace with these rapid advancements, several deepfake detection algorithms have been proposed, leading to an ongoing arms race between deepfake creators and deepfake detectors. Nevertheless, these detectors are often unreliable and frequently fail to detect deepfakes. This study highlights the challenges they face in detecting deepfakes, including (1) the pre-processing pipeline of artifacts and (2) the fact that generators of new, unseen deepfake samples have not been considered when building the defense models. Our work sheds light on the need for further research and development in this field to create more robust and reliable detectors.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#25200;&#21160;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#28145;&#20551;&#26816;&#27979;&#20013;&#32531;&#35299;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#39033;&#36843;&#20999;&#38656;&#35201;&#35299;&#20915;&#30340;&#23433;&#20840;&#21644;&#36947;&#24503;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11704</link><description>&lt;p&gt;
&#32531;&#35299;&#28145;&#20551;&#26816;&#27979;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#65306;&#23545;&#25200;&#21160;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Mitigating Adversarial Attacks in Deepfake Detection: An Exploration of Perturbation and AI Techniques. (arXiv:2302.11704v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#25200;&#21160;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#28145;&#20551;&#26816;&#27979;&#20013;&#32531;&#35299;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#39033;&#36843;&#20999;&#38656;&#35201;&#35299;&#20915;&#30340;&#23433;&#20840;&#21644;&#36947;&#24503;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20855;&#26377;&#35782;&#21035;&#22270;&#20687;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24378;&#22823;&#30340;&#29305;&#24615;&#20063;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#29616;&#35937;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#36825;&#20123;&#23545;&#25239;&#24615;&#31034;&#20363;&#36890;&#36807;&#24039;&#22937;&#22320;&#27880;&#20837;&#24494;&#23567;&#30340;&#25200;&#21160;&#21040;&#28165;&#26224;&#30340;&#22270;&#20687;&#25110;&#35270;&#39057;&#20013;&#65292;&#20174;&#32780;&#23548;&#33268;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35823;&#20998;&#31867;&#25110;&#20135;&#29983;&#38169;&#35823;&#36755;&#20986;&#12290;&#36825;&#31181;&#26131;&#21463;&#25915;&#20987;&#30340;&#24773;&#20917;&#19981;&#20165;&#23616;&#38480;&#20110;&#25968;&#23383;&#39046;&#22495;&#65292;&#22240;&#20026;&#23545;&#25239;&#24615;&#31034;&#20363;&#20063;&#21487;&#20197;&#34987;&#31934;&#24515;&#35774;&#35745;&#26469;&#38024;&#23545;&#20154;&#31867;&#35748;&#30693;&#65292;&#20174;&#32780;&#20135;&#29983;&#27450;&#39575;&#24615;&#23186;&#20307;&#65292;&#22914;&#28145;&#24230;&#20266;&#36896;&#12290;&#29305;&#21035;&#26159;&#65292;&#28145;&#24230;&#20266;&#36896;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#25805;&#25511;&#33286;&#35770;&#21644;&#30772;&#22351;&#20844;&#20247;&#20154;&#29289;&#22768;&#35465;&#30340;&#26377;&#21147;&#24037;&#20855;&#65292;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#35299;&#20915;&#19982;&#23545;&#25239;&#25915;&#20987;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#36947;&#24503;&#38382;&#39064;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning constitutes a pivotal component within the realm of machine learning, offering remarkable capabilities in tasks ranging from image recognition to natural language processing. However, this very strength also renders deep learning models susceptible to adversarial examples, a phenomenon pervasive across a diverse array of applications. These adversarial examples are characterized by subtle perturbations artfully injected into clean images or videos, thereby causing deep learning algorithms to misclassify or produce erroneous outputs. This susceptibility extends beyond the confines of digital domains, as adversarial examples can also be strategically designed to target human cognition, leading to the creation of deceptive media, such as deepfakes. Deepfakes, in particular, have emerged as a potent tool to manipulate public opinion and tarnish the reputations of public figures, underscoring the urgent need to address the security and ethical implications associated with adve
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#24182;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#22320;&#32534;&#30721;&#34920;&#36798;&#24335;&#12290;&#36890;&#36807;&#20248;&#21270;&#26041;&#27861;&#25506;&#32034;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#33021;&#22815;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#65292;&#19988;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09893</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#29983;&#25104;&#22120;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Efficient Generator of Mathematical Expressions for Symbolic Regression. (arXiv:2302.09893v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09893
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#24182;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#22320;&#32534;&#30721;&#34920;&#36798;&#24335;&#12290;&#36890;&#36807;&#20248;&#21270;&#26041;&#27861;&#25506;&#32034;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#33021;&#22815;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#38382;&#39064;&#65292;&#19988;&#25928;&#26524;&#20248;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;HVAE&#65289;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#23618;&#32423;&#32467;&#26500;&#12290;&#23427;&#23558;&#31616;&#21333;&#30340;&#21407;&#23376;&#21333;&#20803;&#19982;&#20849;&#20139;&#26435;&#37325;&#30456;&#32467;&#21512;&#65292;&#20197;&#36882;&#24402;&#22320;&#32534;&#30721;&#21644;&#35299;&#30721;&#23618;&#32423;&#20013;&#30340;&#21508;&#20010;&#33410;&#28857;&#12290;&#32534;&#30721;&#26159;&#33258;&#19979;&#32780;&#19978;&#36827;&#34892;&#30340;&#65292;&#35299;&#30721;&#26159;&#33258;&#19978;&#32780;&#19979;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;HVAE&#21487;&#20197;&#22312;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#23567;&#35821;&#26009;&#24211;&#19978;&#39640;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#23558;&#34920;&#36798;&#24335;&#20934;&#30830;&#22320;&#32534;&#30721;&#25104;&#24179;&#28369;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#12290;&#21518;&#32773;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#39640;&#25928;&#22320;&#36827;&#34892;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#31526;&#21495;&#22238;&#24402;&#30340;&#20219;&#21153;&#12290;&#20107;&#23454;&#19978;&#65292;&#36890;&#36807;HVAE&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#38543;&#26426;&#25628;&#32034;&#27604;&#36890;&#36807;&#25163;&#24037;&#35774;&#35745;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#38543;&#26426;&#25628;&#32034;&#25928;&#26524;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#24212;&#29992;&#36827;&#21270;&#31639;&#27861;&#21040;HVAE&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;EDHiE&#31995;&#32479;&#21487;&#20197;&#26356;&#22909;&#22320;&#20174;&#26631;&#20934;&#31526;&#21495;&#22238;&#24402;&#22522;&#20934;&#20013;&#37325;&#24314;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach to symbolic regression based on a novel variational autoencoder for generating hierarchical structures, HVAE. It combines simple atomic units with shared weights to recursively encode and decode the individual nodes in the hierarchy. Encoding is performed bottom-up and decoding top-down. We empirically show that HVAE can be trained efficiently with small corpora of mathematical expressions and can accurately encode expressions into a smooth low-dimensional latent space. The latter can be efficiently explored with various optimization methods to address the task of symbolic regression. Indeed, random search through the latent space of HVAE performs better than random search through expressions generated by manually crafted probabilistic grammars for mathematical expressions. Finally, EDHiE system for symbolic regression, which applies an evolutionary algorithm to the latent space of HVAE, reconstructs equations from a standard symbolic regression benchmark better 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#32447;&#24066;&#22330;&#20013;&#20080;&#23478;&#21644;&#21334;&#23478;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#23450;&#20215;&#38382;&#39064;&#65292;&#20080;&#23478;&#21033;&#29992;&#20854;&#20182;&#20855;&#26377;&#30456;&#21516;&#23646;&#24615;&#30340;&#20080;&#23478;&#30340;&#35780;&#35770;&#26469;&#20272;&#31639;&#20135;&#21697;&#20215;&#20540;&#65292;&#21334;&#23478;&#20351;&#29992;&#35780;&#35770;&#26469;&#34913;&#37327;&#21830;&#21697;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2302.09700</link><description>&lt;p&gt;
&#21033;&#29992;&#35780;&#35770;&#65306;&#23398;&#20064;&#22312;&#20080;&#23478;&#21644;&#21334;&#23478;&#19981;&#30830;&#23450;&#24615;&#20013;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Leveraging Reviews: Learning to Price with Buyer and Seller Uncertainty. (arXiv:2302.09700v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09700
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#32447;&#24066;&#22330;&#20013;&#20080;&#23478;&#21644;&#21334;&#23478;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#23450;&#20215;&#38382;&#39064;&#65292;&#20080;&#23478;&#21033;&#29992;&#20854;&#20182;&#20855;&#26377;&#30456;&#21516;&#23646;&#24615;&#30340;&#20080;&#23478;&#30340;&#35780;&#35770;&#26469;&#20272;&#31639;&#20135;&#21697;&#20215;&#20540;&#65292;&#21334;&#23478;&#20351;&#29992;&#35780;&#35770;&#26469;&#34913;&#37327;&#21830;&#21697;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#24066;&#22330;&#20013;&#65292;&#39038;&#23458;&#21487;&#20197;&#33719;&#24471;&#26576;&#20010;&#20135;&#21697;&#30340;&#25968;&#30334;&#26465;&#35780;&#35770;&#12290;&#20080;&#23478;&#32463;&#24120;&#20351;&#29992;&#26469;&#33258;&#19982;&#20182;&#20204;&#20855;&#26377;&#30456;&#21516;&#23646;&#24615;&#30340;&#20854;&#20182;&#39038;&#23458;&#30340;&#35780;&#35770;&#26469;&#20272;&#31639;&#20182;&#20204;&#21487;&#33021;&#20043;&#21069;&#24182;&#19981;&#20102;&#35299;&#30340;&#20135;&#21697;&#20215;&#20540;&#65292;&#27604;&#22914;&#34915;&#29289;&#30340;&#36523;&#39640;&#12289;&#25252;&#32932;&#21697;&#30340;&#32932;&#36136;&#21644;&#25143;&#22806;&#23478;&#20855;&#30340;&#22320;&#29702;&#20301;&#32622;&#12290;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#35780;&#35770;&#65292;&#26377;&#20123;&#39038;&#23458;&#21487;&#33021;&#20250;&#29369;&#35947;&#26159;&#21542;&#36141;&#20080;&#65292;&#38500;&#38750;&#20215;&#26684;&#20302;&#65292;&#22240;&#27492;&#23545;&#20110;&#21334;&#23478;&#26469;&#35828;&#65292;&#35774;&#23450;&#39640;&#20215;&#26684;&#21644;&#30830;&#20445;&#26377;&#36275;&#22815;&#30340;&#35780;&#35770;&#35753;&#20080;&#23478;&#33021;&#22815;&#33258;&#20449;&#22320;&#20272;&#31639;&#20135;&#21697;&#20215;&#20540;&#20043;&#38388;&#23384;&#22312;&#30528;&#19968;&#31181;&#32039;&#24352;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#21334;&#23478;&#20063;&#21487;&#20197;&#21033;&#29992;&#35780;&#35770;&#26469;&#34913;&#37327;&#20182;&#20204;&#24076;&#26395;&#38144;&#21806;&#30340;&#21830;&#21697;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#29615;&#22659;&#20013;&#30340;&#36825;&#20010;&#23450;&#20215;&#38382;&#39064;&#65292;&#20854;&#20013;&#21334;&#23478;&#19982;&#19968;&#31995;&#21015;&#19981;&#21516;&#31867;&#22411;&#30340;&#20080;&#23478;&#36880;&#19968;&#20132;&#20114;&#65292;&#22312;&#19968;&#31995;&#21015;&#30340;T&#36718;&#20013;&#36827;&#34892;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#21334;&#23478;&#39318;&#20808;&#35774;&#23450;&#19968;&#20010;&#20215;&#26684;&#65292;&#28982;&#21518;&#19968;&#20010;&#20080;&#23478;&#21040;&#36798;&#24182;&#26597;&#30475;&#20855;&#26377;&#30456;&#21516;&#31867;&#22411;&#30340;&#20808;&#21069;&#20080;&#23478;&#30340;&#35780;&#35770;&#65292;&#36825;&#20123;&#35780;&#35770;&#36879;&#38706;&#20102;&#37027;&#20123;&#20080;&#23478;&#23545;&#20135;&#21697;&#30340;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online marketplaces, customers have access to hundreds of reviews for a single product. Buyers often use reviews from other customers that share their type -- such as height for clothing, skin type for skincare products, and location for outdoor furniture -- to estimate their values, which they may not know a priori. Customers with few relevant reviews may hesitate to make a purchase except at a low price, so for the seller, there is a tension between setting high prices and ensuring that there are enough reviews so that buyers can confidently estimate their values. Simultaneously, sellers may use reviews to gauge the demand for items they wish to sell.  In this work, we study this pricing problem in an online setting where the seller interacts with a set of buyers of finitely many types, one by one, over a series of $T$ rounds. At each round, the seller first sets a price. Then a buyer arrives and examines the reviews of the previous buyers with the same type, which reveal those bu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#22312;&#39044;&#27979;&#22810;&#21457;&#24615;&#30828;&#21270;&#36827;&#23637;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#26368;&#20339;&#30340;&#36830;&#32493;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#32988;&#36807;&#26368;&#20339;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#65292;&#24182;&#19988;&#26631;&#20934;&#21270;&#29616;&#26377;&#29305;&#24449;&#21487;&#20197;&#24102;&#26469;&#26356;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.07854</link><description>&lt;p&gt;
&#35780;&#20272;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#39044;&#27979;&#22810;&#21457;&#24615;&#30828;&#21270;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Continuous Time Models for Predicting Multiple Sclerosis Progression. (arXiv:2302.07854v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#22312;&#39044;&#27979;&#22810;&#21457;&#24615;&#30828;&#21270;&#36827;&#23637;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#26368;&#20339;&#30340;&#36830;&#32493;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#32988;&#36807;&#26368;&#20339;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#65292;&#24182;&#19988;&#26631;&#20934;&#21270;&#29616;&#26377;&#29305;&#24449;&#21487;&#20197;&#24102;&#26469;&#26356;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21457;&#24615;&#30828;&#21270;&#26159;&#19968;&#31181;&#24433;&#21709;&#22823;&#33041;&#21644;&#33034;&#39635;&#30340;&#30142;&#30149;&#65292;&#20250;&#23548;&#33268;&#20005;&#37325;&#27531;&#30142;&#19988;&#27809;&#26377;&#24050;&#30693;&#30340;&#27835;&#24840;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#22810;&#21457;&#24615;&#30828;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#25195;&#25551;&#25110;&#23454;&#39564;&#23460;&#27979;&#35797;&#65292;&#36825;&#20123;&#27169;&#24577;&#37117;&#26114;&#36149;&#19988;&#19981;&#21487;&#38752;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#24615;&#33021;&#32467;&#26524;&#21644;&#20154;&#21475;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#39044;&#27979;&#30142;&#30149;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#27492;&#22522;&#30784;&#19978;&#30740;&#31350;&#20102;&#24314;&#27169;&#26041;&#38754;&#65292;&#20351;&#29992;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#26469;&#39044;&#27979;&#36827;&#23637;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#21457;&#24615;&#30828;&#21270;&#25968;&#25454;&#38598;&#23545;&#22235;&#31181;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#30340;&#36830;&#32493;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#20248;&#20110;&#26368;&#20339;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#23454;&#39564;&#20197;&#21457;&#29616;&#24615;&#33021;&#25552;&#21319;&#30340;&#26469;&#28304;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#29616;&#26377;&#29305;&#24449;&#36827;&#34892;&#26631;&#20934;&#21270;&#27604;&#25554;&#20540;&#22788;&#29702;&#33021;&#22815;&#24102;&#26469;&#26356;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiple sclerosis is a disease that affects the brain and spinal cord, it can lead to severe disability and has no known cure. The majority of prior work in machine learning for multiple sclerosis has been centered around using Magnetic Resonance Imaging scans or laboratory tests; these modalities are both expensive to acquire and can be unreliable. In a recent paper it was shown that disease progression can be predicted effectively using performance outcome measures and demographic data. In our work we build on this to investigate the modeling side, using continuous time models to predict progression. We benchmark four continuous time models using a publicly available multiple sclerosis dataset. We find that the best continuous model is often able to outperform the best benchmarked discrete time model. We also carry out an extensive ablation to discover the sources of performance gains, we find that standardizing existing features leads to a larger performance increase than interpola
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#21442;&#25968;&#30340;&#28789;&#27963;&#20808;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#39532;&#23572;&#21487;&#22827;&#38142;&#26367;&#20195;&#27169;&#22411;&#30340;&#27169;&#25311;&#24471;&#20986;&#65292;&#35813;&#26041;&#27861;&#22312;&#20272;&#35745;&#31995;&#32479;&#21457;&#32946;&#21442;&#25968;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.02522</link><description>&lt;p&gt;
&#21464;&#20998;&#36125;&#21494;&#26031;&#31995;&#32479;&#21457;&#32946;&#21442;&#25968;&#25512;&#26029;&#20013;&#30340;&#20808;&#39564;&#23494;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prior Density Learning in Variational Bayesian Phylogenetic Parameters Inference. (arXiv:2302.02522v2 [q-bio.PE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23398;&#20064;&#21442;&#25968;&#30340;&#28789;&#27963;&#20808;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#39532;&#23572;&#21487;&#22827;&#38142;&#26367;&#20195;&#27169;&#22411;&#30340;&#27169;&#25311;&#24471;&#20986;&#65292;&#35813;&#26041;&#27861;&#22312;&#20272;&#35745;&#31995;&#32479;&#21457;&#32946;&#21442;&#25968;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#22312;&#36125;&#21494;&#26031;&#20272;&#35745;&#38382;&#39064;&#20013;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36335;&#24452;&#12290;&#36825;&#20123;&#36827;&#27493;&#20351;&#24471;&#21464;&#20998;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#25104;&#20026;&#36924;&#36817;&#31995;&#32479;&#21457;&#32946;&#21518;&#39564;&#30340;&#33945;&#29305;&#21345;&#32599;&#39532;&#23572;&#31185;&#22827;&#38142;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#20043;&#19968;&#26159;&#36890;&#36807;&#22266;&#23450;&#20998;&#24067;&#26469;&#24314;&#27169;&#20808;&#39564;&#65292;&#22914;&#26524;&#23427;&#20204;&#36828;&#31163;&#24403;&#21069;&#25968;&#25454;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#20559;&#20506;&#21518;&#39564;&#36924;&#36817;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#21644;&#23454;&#26045;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#23427;&#20204;&#30340;&#21442;&#25968;&#65292;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#26469;&#25918;&#26494;&#20808;&#39564;&#23494;&#24230;&#30340;&#20005;&#26684;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22810;&#20010;&#39532;&#23572;&#21487;&#22827;&#38142;&#26367;&#20195;&#27169;&#22411;&#19979;&#30340;&#25903;&#38271;&#24230;&#21644;&#36827;&#21270;&#21442;&#25968;&#20272;&#35745;&#12290;&#25152;&#36827;&#34892;&#30340;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#20986;&#36825;&#31181;&#26041;&#27861;&#22312;&#20272;&#35745;&#25903;&#38271;&#24230;&#21644;&#36827;&#21270;&#27169;&#22411;&#21442;&#25968;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#23427;&#20204;&#36824;&#34920;&#26126;&#28789;&#27963;&#30340;&#20808;&#39564;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advances in variational inference are providing promising paths in Bayesian estimation problems. These advances make variational phylogenetic inference an alternative approach to Markov Chain Monte Carlo methods for approximating the phylogenetic posterior. However, one of the main drawbacks of such approaches is the modelling of the prior through fixed distributions, which could bias the posterior approximation if they are distant from the current data distribution. In this paper, we propose an approach and an implementation framework to relax the rigidity of the prior densities by learning their parameters using a gradient-based method and a neural network-based parameterization. We applied this approach for branch lengths and evolutionary parameters estimation under several Markov chain substitution models. The results of performed simulations show that the approach is powerful in estimating branch lengths and evolutionary model parameters. They also show that a flexible prior m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#24191;&#20041;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20272;&#35745;&#19981;&#21487;&#35266;&#27979;&#26799;&#24230;&#26469;&#20943;&#23569;&#26410;&#30693;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#24191;&#20041;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.01497</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#26410;&#30693;&#39046;&#22495;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26799;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Gradient Estimation for Unseen Domain Risk Minimization with Pre-Trained Models. (arXiv:2302.01497v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#24191;&#20041;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20272;&#35745;&#19981;&#21487;&#35266;&#27979;&#26799;&#24230;&#26469;&#20943;&#23569;&#26410;&#30693;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#30340;&#24191;&#20041;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#24191;&#20041;&#21270;&#26088;&#22312;&#26500;&#24314;&#22312;&#21482;&#26377;&#28304;&#39046;&#22495;&#29992;&#20110;&#27169;&#22411;&#20248;&#21270;&#26102;&#22312;&#26410;&#30693;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#24191;&#20041;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#24191;&#20041;&#33021;&#21147;&#26469;&#22686;&#24378;&#39046;&#22495;&#24191;&#20041;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#30446;&#26631;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#32570;&#20047;&#30446;&#26631;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#20174;&#28304;&#39046;&#22495;&#20013;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#20294;&#30001;&#20110;&#26799;&#24230;&#20559;&#24046;&#23548;&#33268;&#23545;&#28304;&#39046;&#22495;&#30340;&#24191;&#20041;&#33021;&#21147;&#21463;&#25439;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#24191;&#20041;&#21270;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20272;&#35745;&#19981;&#21487;&#35266;&#27979;&#30340;&#26799;&#24230;&#65292;&#22312;&#26410;&#30693;&#39046;&#22495;&#20013;&#20943;&#23569;&#28508;&#22312;&#39118;&#38505;&#12290;&#36825;&#20123;&#20272;&#35745;&#30340;&#19981;&#21487;&#35266;&#27979;&#26799;&#24230;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#36827;&#19968;&#27493;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24191;&#20041;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization aims to build generalized models that perform well on unseen domains when only source domains are available for model optimization. Recent studies have shown that large-scale pre-trained models can enhance domain generalization by leveraging their generalization power. However, these pre-trained models lack target task-specific knowledge yet due to discrepancies between the pre-training objectives and the target task. Although the task-specific knowledge could be learned from source domains by fine-tuning, this hurts the generalization power of pre-trained models due to gradient bias toward the source domains. To alleviate this problem, we propose a new domain generalization method that estimates unobservable gradients that reduce potential risks in unseen domains using a large-scale pre-trained model. These estimated unobservable gradients allow the pre-trained model to learn task-specific knowledge further while preserving its generalization ability by relieving
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#23567;&#27874;&#21464;&#25442;(QRT)&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#36890;&#36807;&#37327;&#23376;&#35745;&#31639;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#24577;&#30340;&#23567;&#27874;&#21464;&#25442;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#31232;&#30095;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2301.11936</link><description>&lt;p&gt;
&#37327;&#23376;&#23567;&#27874;&#21464;&#25442;&#65306;&#20855;&#26377;&#37327;&#23376;&#35745;&#31639;&#20248;&#21183;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum Ridgelet Transform: Winning Lottery Ticket of Neural Networks with Quantum Computation. (arXiv:2301.11936v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11936
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#37327;&#23376;&#23567;&#27874;&#21464;&#25442;(QRT)&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#36890;&#36807;&#37327;&#23376;&#35745;&#31639;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#24577;&#30340;&#23567;&#27874;&#21464;&#25442;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#31232;&#30095;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;(QML)&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#24314;&#31435;&#37327;&#23376;&#35745;&#31639;&#22312;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#31561;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#23567;&#27874;&#21464;&#25442;&#19968;&#30452;&#26159;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#30740;&#31350;&#20013;&#30340;&#22522;&#26412;&#25968;&#23398;&#24037;&#20855;&#65292;&#20294;&#30001;&#20110;&#20256;&#32479;&#32463;&#20856;&#35745;&#31639;&#30340;&#25968;&#20540;&#23454;&#29616;&#38656;&#35201;&#25351;&#25968;&#32423;&#36816;&#34892;&#26102;&#38388;$\exp(O(D))$&#65292;&#22240;&#27492;&#23567;&#27874;&#21464;&#25442;&#30340;&#23454;&#38469;&#24212;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#22312;&#25968;&#25454;&#32500;&#24230;$D$&#22686;&#21152;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#37327;&#23376;&#23567;&#27874;&#21464;&#25442;(QRT)&#65292;&#23427;&#22312;&#37327;&#23376;&#35745;&#31639;&#30340;&#32447;&#24615;&#36816;&#34892;&#26102;&#38388;$O(D)$&#20869;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#24577;&#30340;&#23567;&#27874;&#21464;&#25442;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21033;&#29992;QRT&#20316;&#20026;QML&#30340;&#22522;&#26412;&#23376;&#31243;&#24207;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#22823;&#22411;&#27973;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#31232;&#30095;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#23545;&#21407;&#22987;&#32593;&#32476;&#36827;&#34892;&#22823;&#35268;&#27169;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant challenge in the field of quantum machine learning (QML) is to establish applications of quantum computation to accelerate common tasks in machine learning such as those for neural networks. Ridgelet transform has been a fundamental mathematical tool in the theoretical studies of neural networks, but the practical applicability of ridgelet transform to conducting learning tasks was limited since its numerical implementation by conventional classical computation requires an exponential runtime $\exp(O(D))$ as data dimension $D$ increases. To address this problem, we develop a quantum ridgelet transform (QRT), which implements the ridgelet transform of a quantum state within a linear runtime $O(D)$ of quantum computation. As an application, we also show that one can use QRT as a fundamental subroutine for QML to efficiently find a sparse trainable subnetwork of large shallow wide neural networks without conducting large-scale optimization of the original network. This appli
&lt;/p&gt;</description></item><item><title>PECAN&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#30456;&#20132;&#20998;&#21306;&#19978;&#35757;&#32451;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#24182;&#24212;&#29992;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#35748;&#35777;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38450;&#24481;&#24378;&#24230;&#21644;&#25928;&#29575;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.11824</link><description>&lt;p&gt;
PECAN: &#19968;&#31181;&#38024;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#30830;&#23450;&#24615;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PECAN: A Deterministic Certified Defense Against Backdoor Attacks. (arXiv:2301.11824v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11824
&lt;/p&gt;
&lt;p&gt;
PECAN&#26159;&#19968;&#31181;&#26377;&#25928;&#19988;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#30456;&#20132;&#20998;&#21306;&#19978;&#35757;&#32451;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#24182;&#24212;&#29992;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#35748;&#35777;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38450;&#24481;&#24378;&#24230;&#21644;&#25928;&#29575;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#24694;&#24847;&#27745;&#26579;&#35757;&#32451;&#38598;&#24182;&#22312;&#27979;&#35797;&#36755;&#20837;&#20013;&#25554;&#20837;&#35302;&#21457;&#22120;&#20197;&#25913;&#21464;&#21463;&#23475;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#35777;&#65292;&#35201;&#20040;&#20855;&#26377;&#35745;&#31639;&#26114;&#36149;&#19988;&#26080;&#25928;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;PECAN&#12290;PECAN&#30340;&#26680;&#24515;&#27934;&#35265;&#26159;&#22312;&#25968;&#25454;&#30340;&#19981;&#30456;&#20132;&#20998;&#21306;&#19978;&#35757;&#32451;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24212;&#29992;&#29616;&#25104;&#30340;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#35748;&#35777;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;PECAN&#12290;&#32467;&#26524;&#34920;&#26126;PECAN&#22312;&#38450;&#24481;&#24378;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#32463;&#36807;&#35748;&#35777;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#30495;&#23454;&#21518;&#38376;&#25915;&#20987;&#20013;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#19968;&#31995;&#21015;&#22522;&#32447;&#30456;&#27604;&#65292;PECAN&#21487;&#20197;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are vulnerable to backdoor poisoning attacks, where the attackers maliciously poison the training set and insert triggers into the test input to change the prediction of the victim model. Existing defenses for backdoor attacks either provide no formal guarantees or come with expensive-to-compute and ineffective probabilistic guarantees. We present PECAN, an efficient and certified approach for defending against backdoor attacks. The key insight powering PECAN is to apply off-the-shelf test-time evasion certification techniques on a set of neural networks trained on disjoint partitions of the data. We evaluate PECAN on image classification and malware detection datasets. Our results demonstrate that PECAN can (1) significantly outperform the state-of-the-art certified backdoor defense, both in defense strength and efficiency, and (2) on real back-door attacks, PECAN can reduce attack success rate by order of magnitude when compared to a range of baselines from the litera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21160;&#24577;&#35745;&#31639;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#35270;&#35282;&#12290;&#36890;&#36807;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#21516;&#26102;&#23398;&#20064;&#23494;&#38598;&#21644;&#38376;&#25511;&#23376;&#32593;&#32476;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#21098;&#26525;&#27493;&#39588;&#65292;&#21487;&#20197;&#33719;&#24471;&#19968;&#20010;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#24037;&#19994;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2301.09164</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21160;&#24577;&#35745;&#31639;&#20043;&#38388;&#30340;&#32479;&#19968;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unifying Synergies between Self-supervised Learning and Dynamic Computation. (arXiv:2301.09164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21160;&#24577;&#35745;&#31639;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#35270;&#35282;&#12290;&#36890;&#36807;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#21516;&#26102;&#23398;&#20064;&#23494;&#38598;&#21644;&#38376;&#25511;&#23376;&#32593;&#32476;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#21098;&#26525;&#27493;&#39588;&#65292;&#21487;&#20197;&#33719;&#24471;&#19968;&#20010;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#24037;&#19994;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24037;&#19994;&#29615;&#22659;&#20013;&#65292;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#31574;&#30053;&#20351;&#24471;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#33719;&#21462;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#36890;&#24120;&#20250;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#12289;&#21160;&#24577;&#35745;&#31639;&#65288;DC&#65289;&#21644;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#36825;&#36890;&#24120;&#28041;&#21450;&#21040;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27425;&#24494;&#35843;&#65288;&#25110;&#33976;&#39311;&#27493;&#39588;&#65289;&#65292;&#20351;&#24471;&#35745;&#31639;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#21160;&#24577;&#35745;&#31639;&#33539;&#24335;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#35270;&#35282;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35774;&#32622;&#20013;&#65292;&#21516;&#26102;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#23494;&#38598;&#21644;&#38376;&#25511;&#23376;&#32593;&#32476;&#26159;&#21487;&#34892;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#25110;&#21098;&#26525;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#26399;&#38388;&#23494;&#38598;&#21644;&#38376;&#25511;&#32534;&#30721;&#22120;&#30340;&#20849;&#21516;&#28436;&#21270;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#20026;&#24212;&#29992;&#29305;&#23450;&#30340;&#24037;&#19994;&#29615;&#22659;&#25552;&#20379;&#20102;&#36890;&#29992;&#19988;&#22810;&#21151;&#33021;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;CIFAR-10/100&#31561;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computationally expensive training strategies make self-supervised learning (SSL) impractical for resource constrained industrial settings. Techniques like knowledge distillation (KD), dynamic computation (DC), and pruning are often used to obtain a lightweightmodel, which usually involves multiple epochs of fine-tuning (or distilling steps) of a large pre-trained model, making it more computationally challenging. In this work we present a novel perspective on the interplay between SSL and DC paradigms. In particular, we show that it is feasible to simultaneously learn a dense and gated sub-network from scratch in a SSL setting without any additional fine-tuning or pruning steps. The co-evolution during pre-training of both dense and gated encoder offers a good accuracy-efficiency trade-off and therefore yields a generic and multi-purpose architecture for application specific industrial settings. Extensive experiments on several image classification benchmarks including CIFAR-10/100, S
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#21464;&#20998;&#25512;&#29702;&#30340;&#25237;&#24433;&#31215;&#20998;&#26356;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38477;&#20302;&#21442;&#25968;&#25935;&#24863;&#24615;&#26469;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.08374</link><description>&lt;p&gt;
&#39640;&#32500;&#21464;&#20998;&#25512;&#29702;&#30340;&#25237;&#24433;&#31215;&#20998;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Projective Integral Updates for High-Dimensional Variational Inference. (arXiv:2301.08374v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39640;&#32500;&#21464;&#20998;&#25512;&#29702;&#30340;&#25237;&#24433;&#31215;&#20998;&#26356;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#38477;&#20302;&#21442;&#25968;&#25935;&#24863;&#24615;&#26469;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#29702;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#36817;&#20284;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#31616;&#21270;&#30340;&#21442;&#25968;&#20998;&#24067;&#26469;&#20195;&#26367;&#23436;&#25972;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#27979;&#20013;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#25429;&#25417;&#19982;&#35757;&#32451;&#25968;&#25454;&#19968;&#33268;&#30340;&#27169;&#22411;&#21464;&#21270;&#21487;&#20197;&#36890;&#36807;&#38477;&#20302;&#21442;&#25968;&#25935;&#24863;&#24615;&#26469;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#22266;&#23450;&#28857;&#26368;&#20248;&#21270;&#26041;&#27861;&#65292;&#24403;&#27599;&#20010;&#21487;&#34892;&#30340;&#23545;&#25968;&#23494;&#24230;&#21487;&#20197;&#34920;&#31034;&#20026;&#32473;&#23450;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#26102;&#65292;&#35813;&#26041;&#27861;&#29983;&#25928;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#22120;&#25104;&#20026;&#25237;&#24433;&#31215;&#20998;&#26356;&#26032;&#30340;&#19968;&#20010;&#19981;&#21160;&#28857;&#12290;&#24403;&#22522;&#20989;&#25968;&#36328;&#36234;&#27599;&#20010;&#21442;&#25968;&#30340;&#20108;&#27425;&#20989;&#25968;&#26102;&#65292;&#21487;&#34892;&#23494;&#24230;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#25237;&#24433;&#31215;&#20998;&#26356;&#26032;&#20135;&#29983;&#20102;&#20934;&#29275;&#39039;&#21464;&#20998;&#36125;&#21494;&#26031; (QNVB)&#12290;&#20854;&#20182;&#22522;&#20989;&#25968;&#21644;&#26356;&#26032;&#26041;&#27861;&#20063;&#26159;&#21487;&#33021;&#30340;&#12290;&#30001;&#20110;&#36825;&#20123;&#26356;&#26032;&#38656;&#35201;&#39640;&#32500;&#31215;&#20998;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20934;&#38543;&#26426;&#31215;&#20998;&#24207;&#21015;&#29992;&#20110;&#22343;&#21248;&#22343;&#21248;&#22343;&#21248;&#22343;&#21248;&#22343;&#21248;&#22343;&#21248;&#31215;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference is an approximation framework for Bayesian inference that seeks to improve quantified uncertainty in predictions by optimizing a simplified distribution over parameters to stand in for the full posterior. Capturing model variations that remain consistent with training data enables more robust predictions by reducing parameter sensitivity. This work introduces a fixed-point optimization for variational inference that is applicable when every feasible log density can be expressed as a linear combination of functions from a given basis. In such cases, the optimizer becomes a fixed-point of projective integral updates. When the basis spans univariate quadratics in each parameter, feasible densities are Gaussian and the projective integral updates yield quasi-Newton variational Bayes (QNVB). Other bases and updates are also possible. As these updates require high-dimensional integration, this work first proposes an efficient quasirandom quadrature sequence for mean-fie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30005;&#21147;&#22871;&#21033;&#20132;&#26131;&#12290;&#36890;&#36807;&#21033;&#29992;&#22870;&#21169;&#24037;&#31243;&#21644;&#35746;&#21333;&#20998;&#27573;&#30340;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#65292;&#22686;&#21152;&#31454;&#26631;&#25104;&#21151;&#29575;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#21033;&#28070;&#21644;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2301.08360</link><description>&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#27169;&#20223;&#65306;&#29992;&#20110;&#30005;&#21147;&#22871;&#21033;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain-adapted Learning and Imitation: DRL for Power Arbitrage. (arXiv:2301.08360v2 [q-fin.TR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30005;&#21147;&#22871;&#21033;&#20132;&#26131;&#12290;&#36890;&#36807;&#21033;&#29992;&#22870;&#21169;&#24037;&#31243;&#21644;&#35746;&#21333;&#20998;&#27573;&#30340;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#65292;&#22686;&#21152;&#31454;&#26631;&#25104;&#21151;&#29575;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#21033;&#28070;&#21644;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#33655;&#20848;&#30005;&#21147;&#24066;&#22330;&#65292;&#30001;&#26085;&#21069;&#24066;&#22330;&#21644;&#21363;&#26102;&#24179;&#34913;&#24066;&#22330;&#32452;&#25104;&#65292;&#24182;&#19988;&#36816;&#20316;&#26041;&#24335;&#31867;&#20284;&#25293;&#21334;&#12290;&#30001;&#20110;&#20379;&#38656;&#27874;&#21160;&#65292;&#36890;&#24120;&#23384;&#22312;&#19981;&#24179;&#34913;&#23548;&#33268;&#20004;&#20010;&#24066;&#22330;&#20215;&#26684;&#19981;&#21516;&#65292;&#20174;&#32780;&#25552;&#20379;&#22871;&#21033;&#26426;&#20250;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;&#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24335;&#21452;&#20195;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#27431;&#27954;&#30005;&#21147;&#22871;&#21033;&#20132;&#26131;&#30340;&#21452;&#23618;&#20223;&#30495;&#19982;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#23454;&#29616;&#26041;&#24335;&#65292;&#36890;&#36807;&#27169;&#20223;&#30005;&#21147;&#20132;&#26131;&#21592;&#30340;&#20132;&#26131;&#34892;&#20026;&#26469;&#34701;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#22870;&#21169;&#24037;&#31243;&#26469;&#27169;&#20223;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#25105;&#20204;&#33021;&#22815;&#37325;&#26032;&#35774;&#35745;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#31995;&#32479;&#65292;&#25913;&#21892;&#35757;&#32451;&#20013;&#30340;&#25910;&#25947;&#24615;&#24182;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35746;&#21333;&#20998;&#27573;&#22686;&#21152;&#20102;&#31454;&#26631;&#25104;&#21151;&#29575;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21033;&#28070;&#21644;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28436;&#31034;&#20102;&#36890;&#36807;&#39046;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#27169;&#20223;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#30005;&#21147;&#22871;&#21033;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we discuss the Dutch power market, which is comprised of a day-ahead market and an intraday balancing market that operates like an auction. Due to fluctuations in power supply and demand, there is often an imbalance that leads to different prices in the two markets, providing an opportunity for arbitrage. To address this issue, we restructure the problem and propose a collaborative dual-agent reinforcement learning approach for this bi-level simulation and optimization of European power arbitrage trading. We also introduce two new implementations designed to incorporate domain-specific knowledge by imitating the trading behaviours of power traders. By utilizing reward engineering to imitate domain expertise, we are able to reform the reward system for the RL agent, which improves convergence during training and enhances overall performance. Additionally, the tranching of orders increases bidding success rates and significantly boosts profit and loss (P&amp;L). Our study demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#23545;&#35770;&#25968;&#23383;&#23402;&#29983;&#65288;RDT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#26029;&#35266;&#23519;&#29289;&#32852;&#32593;&#23454;&#20307;&#30340;&#30495;&#23454;&#23545;&#24212;&#29289;&#26469;&#33258;&#21160;&#29983;&#25104;&#36890;&#29992;&#22411;&#25968;&#23383;&#23402;&#29983;&#65292;&#24182;&#35843;&#25972;&#20854;&#34892;&#20026;&#27169;&#22411;&#12290;&#26694;&#26550;&#21033;&#29992;&#29289;&#32852;&#32593;&#20043;&#29289;&#65288;WoT&#65289;&#25552;&#20379;&#26631;&#20934;&#21270;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2301.07390</link><description>&lt;p&gt;
&#30456;&#23545;&#35770;&#25968;&#23383;&#23402;&#29983;&#65306;&#23558;&#29289;&#32852;&#32593;&#24341;&#20837;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Relativistic Digital Twin: Bringing the IoT to the Future. (arXiv:2301.07390v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#23545;&#35770;&#25968;&#23383;&#23402;&#29983;&#65288;RDT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#26029;&#35266;&#23519;&#29289;&#32852;&#32593;&#23454;&#20307;&#30340;&#30495;&#23454;&#23545;&#24212;&#29289;&#26469;&#33258;&#21160;&#29983;&#25104;&#36890;&#29992;&#22411;&#25968;&#23383;&#23402;&#29983;&#65292;&#24182;&#35843;&#25972;&#20854;&#34892;&#20026;&#27169;&#22411;&#12290;&#26694;&#26550;&#21033;&#29992;&#29289;&#32852;&#32593;&#20043;&#29289;&#65288;WoT&#65289;&#25552;&#20379;&#26631;&#20934;&#21270;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#20854;&#29289;&#29702;&#36164;&#20135;&#30340;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#26469;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#21644;&#27169;&#25311;&#8220;&#20551;&#35774;&#8221;&#24773;&#26223;&#12290;&#25968;&#23383;&#23402;&#29983;&#33021;&#22815;&#22797;&#21046;&#29289;&#32852;&#32593;&#35774;&#22791;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36866;&#24212;&#20854;&#34892;&#20026;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#29289;&#32852;&#32593;&#20013;&#30340;&#25968;&#23383;&#23402;&#29983;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#29992;&#20363;&#65292;&#26080;&#27861;&#26080;&#32541;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#22659;&#12290;&#27492;&#22806;&#65292;&#29289;&#32852;&#32593;&#30340;&#30862;&#29255;&#21270;&#20351;&#24471;&#22312;&#20855;&#26377;&#22810;&#31181;&#25968;&#25454;&#26684;&#24335;&#21644;&#29289;&#32852;&#32593;&#32593;&#32476;&#21327;&#35758;&#30340;&#24322;&#26500;&#24773;&#26223;&#20013;&#37096;&#32626;&#25968;&#23383;&#23402;&#29983;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#35770;&#25968;&#23383;&#23402;&#29983;&#65288;RDT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25105;&#20204;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#29289;&#32852;&#32593;&#23454;&#20307;&#30340;&#36890;&#29992;&#22411;&#25968;&#23383;&#23402;&#29983;&#65292;&#24182;&#36890;&#36807;&#19981;&#26029;&#35266;&#23519;&#20854;&#30495;&#23454;&#23545;&#24212;&#29289;&#26469;&#35843;&#25972;&#20854;&#34892;&#20026;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#20381;&#36182;&#20110;&#36890;&#36807;&#29289;&#32852;&#32593;&#20043;&#29289;&#65288;WoT&#65289;&#26469;&#36827;&#34892;&#23545;&#35937;&#34920;&#31034;&#65292;&#20026;&#27599;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#21450;&#20854;&#25968;&#23383;&#23402;&#29983;&#25552;&#20379;&#26631;&#20934;&#21270;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex IoT ecosystems often require the usage of Digital Twins (DTs) of their physical assets in order to perform predictive analytics and simulate what-if scenarios. DTs are able to replicate IoT devices and adapt over time to their behavioral changes. However, DTs in IoT are typically tailored to a specific use case, without the possibility to seamlessly adapt to different scenarios. Further, the fragmentation of IoT poses additional challenges on how to deploy DTs in heterogeneous scenarios characterized by the usage of multiple data formats and IoT network protocols. In this paper, we propose the Relativistic Digital Twin (RDT) framework, through which we automatically generate general-purpose DTs of IoT entities and tune their behavioral models over time by constantly observing their real counterparts. The framework relies on the object representation via the Web of Things (WoT), to offer a standardized interface to each of the IoT devices as well as to their DTs. To this purpose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#21322;&#30417;&#30563;&#37322;&#20041;&#29983;&#25104;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#22833;&#30446;&#26631;&#23545;&#24314;&#27169;&#20026;&#28508;&#22312;&#37322;&#20041;&#24207;&#21015;&#65292;&#24182;&#32467;&#21512;&#21452;&#21521;&#23398;&#20064;&#21644;&#25913;&#36827;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.02275</link><description>&lt;p&gt;
&#35821;&#35328;&#20316;&#20026;&#28508;&#22312;&#24207;&#21015;&#65306;&#29992;&#20110;&#21322;&#30417;&#30563;&#37322;&#20041;&#29983;&#25104;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language as a Latent Sequence: deep latent variable models for semi-supervised paraphrase generation. (arXiv:2301.02275v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#21322;&#30417;&#30563;&#37322;&#20041;&#29983;&#25104;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#22833;&#30446;&#26631;&#23545;&#24314;&#27169;&#20026;&#28508;&#22312;&#37322;&#20041;&#24207;&#21015;&#65292;&#24182;&#32467;&#21512;&#21452;&#21521;&#23398;&#20064;&#21644;&#25913;&#36827;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#21322;&#30417;&#30563;&#37322;&#20041;&#29983;&#25104;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#22833;&#30446;&#26631;&#23545;&#34987;&#24314;&#27169;&#20026;&#28508;&#22312;&#37322;&#20041;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21464;&#20998;&#24207;&#21015;&#33258;&#32534;&#30721;&#37325;&#26500;&#65288;VSAR&#65289;&#30340;&#26032;&#22411;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#22312;&#32473;&#23450;&#35266;&#23519;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#28508;&#22312;&#24207;&#21015;&#25512;&#26029;&#12290;&#20026;&#20102;&#21033;&#29992;&#25991;&#26412;&#23545;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#21521;&#23398;&#20064;&#65288;DDL&#65289;&#30340;&#26032;&#22411;&#30417;&#30563;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26088;&#22312;&#19982;&#25105;&#20204;&#25552;&#20986;&#30340;VSAR&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;&#23558;VSAR&#19982;DDL&#65288;DDL+VSAR&#65289;&#32467;&#21512;&#36215;&#26469;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#32452;&#21512;&#27169;&#22411;&#23384;&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#22686;&#24378;&#23398;&#20064;&#65288;KRL&#65289;&#30340;&#26032;&#22411;&#20004;&#38454;&#27573;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#32452;&#21512;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#31454;&#20105;&#21147;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores deep latent variable models for semi-supervised paraphrase generation, where the missing target pair for unlabelled data is modelled as a latent paraphrase sequence. We present a novel unsupervised model named variational sequence auto-encoding reconstruction (VSAR), which performs latent sequence inference given an observed text. To leverage information from text pairs, we additionally introduce a novel supervised model we call dual directional learning (DDL), which is designed to integrate with our proposed VSAR model. Combining VSAR with DDL (DDL+VSAR) enables us to conduct semi-supervised learning. Still, the combined model suffers from a cold-start problem. To further combat this issue, we propose an improved weight initialisation solution, leading to a novel two-stage training scheme we call knowledge-reinforced-learning (KRL). Our empirical evaluations suggest that the combined model yields competitive performance against the state-of-the-art supervised basel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#31649;&#29702;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#20581;&#24247;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#32463;&#39564;&#25110;&#29289;&#29702;&#21160;&#24577;&#27169;&#22411;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#21508;&#31181;&#20449;&#24687;&#26469;&#28304;&#65292;&#25552;&#39640;&#39044;&#27979;&#21644;&#31649;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00776</link><description>&lt;p&gt;
&#21033;&#29992;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#23545;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for Prognostics and Health Management of Lithium-Ion Batteries. (arXiv:2301.00776v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#31649;&#29702;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#20581;&#24247;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#32463;&#39564;&#25110;&#29289;&#29702;&#21160;&#24577;&#27169;&#22411;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#21508;&#31181;&#20449;&#24687;&#26469;&#28304;&#65292;&#25552;&#39640;&#39044;&#27979;&#21644;&#31649;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#39044;&#27979;&#21644;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#65292;&#24050;&#24314;&#31435;&#20102;&#35768;&#22810;&#27169;&#22411;&#26469;&#25551;&#36848;&#20854;&#34928;&#20943;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#32463;&#39564;&#25110;&#29289;&#29702;&#27169;&#22411;&#21487;&#20197;&#25581;&#31034;&#26377;&#20851;&#34928;&#20943;&#21160;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#31181;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#36825;&#20123;&#27169;&#22411;&#25152;&#20195;&#34920;&#30340;&#20449;&#24687;&#12290;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#19968;&#31181;&#23558;&#32463;&#39564;&#25110;&#29289;&#29702;&#21160;&#24577;&#27169;&#22411;&#19982;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#24037;&#20855;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#21508;&#31181;&#20449;&#24687;&#26469;&#28304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PINN&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#26696;&#12290;&#36890;&#36807;&#24320;&#21457;&#21322;&#32463;&#39564;&#21322;&#29289;&#29702;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26469;&#24314;&#27169;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#34928;&#20943;&#21160;&#24577;&#12290;&#24403;&#23545;&#21160;&#24577;&#20102;&#35299;&#36739;&#23569;&#26102;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#38544;&#34255;&#29289;&#29702;&#27169;&#22411;&#65288;DeepHPM&#65289;&#26469;&#21457;&#29616;&#28508;&#22312;&#30340;&#20027;&#23548;&#21160;&#24577;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#21457;&#29616;&#30340;&#21160;&#24577;&#20449;&#24687;&#19982;&#25366;&#25496;&#30340;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
For Prognostics and Health Management (PHM) of Lithium-ion (Li-ion) batteries, many models have been established to characterize their degradation process. The existing empirical or physical models can reveal important information regarding the degradation dynamics. However, there are no general and flexible methods to fuse the information represented by those models. Physics-Informed Neural Network (PINN) is an efficient tool to fuse empirical or physical dynamic models with data-driven models. To take full advantage of various information sources, we propose a model fusion scheme based on PINN. It is implemented by developing a semi-empirical semi-physical Partial Differential Equation (PDE) to model the degradation dynamics of Li-ion batteries. When there is little prior knowledge about the dynamics, we leverage the data-driven Deep Hidden Physics Model (DeepHPM) to discover the underlying governing dynamic models. The uncovered dynamics information is then fused with that mined by 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36719;&#26368;&#36817;&#37051;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#26469;&#38750;&#32447;&#24615;&#22320;&#21010;&#20998;&#29305;&#24449;&#31354;&#38388;&#24182;&#38750;&#21442;&#25968;&#22320;&#24314;&#27169;&#28508;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20197;&#36991;&#20813;&#27169;&#22411;&#24536;&#35760;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#34920;&#31034;&#24182;&#36807;&#24230;&#25311;&#21512;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.05102</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36719;&#26368;&#36817;&#37051;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A soft nearest-neighbor framework for continual semi-supervised learning. (arXiv:2212.05102v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05102
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36719;&#26368;&#36817;&#37051;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#26469;&#38750;&#32447;&#24615;&#22320;&#21010;&#20998;&#29305;&#24449;&#31354;&#38388;&#24182;&#38750;&#21442;&#25968;&#22320;&#24314;&#27169;&#28508;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20197;&#36991;&#20813;&#27169;&#22411;&#24536;&#35760;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#34920;&#31034;&#24182;&#36807;&#24230;&#25311;&#21512;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#26368;&#20808;&#36827;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#34920;&#29616;&#20173;&#28982;&#20381;&#36182;&#20110;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#19981;&#29616;&#23454;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25345;&#32493;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#8212;&#8212;&#19968;&#31181;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26679;&#26412;&#19981;&#20840;&#37096;&#26631;&#35760;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#27169;&#22411;&#20250;&#24536;&#35760;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#24182;&#36807;&#24230;&#25311;&#21512;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#23041;&#21147;&#26469;&#38750;&#32447;&#24615;&#22320;&#21010;&#20998;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#30001;&#20110;&#20854;&#38750;&#21442;&#25968;&#24615;&#36136;&#32780;&#28789;&#27963;&#22320;&#23545;&#28508;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20026;&#24403;&#21069;&#20219;&#21153;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#20174;&#20197;&#21069;&#30340;&#20219;&#21153;&#20013;&#25552;&#28860;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#36739;&#22823;&#30340;&#20248;&#21183;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22312;&#25345;&#32493;&#21322;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#19978;&#26641;&#31435;&#20102;&#22362;&#23454;&#30340;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advances, the performance of state-of-the-art continual learning approaches hinges on the unrealistic scenario of fully labeled data. In this paper, we tackle this challenge and propose an approach for continual semi-supervised learning--a setting where not all the data samples are labeled. A primary issue in this scenario is the model forgetting representations of unlabeled data and overfitting the labeled samples. We leverage the power of nearest-neighbor classifiers to nonlinearly partition the feature space and flexibly model the underlying data distribution thanks to its non-parametric nature. This enables the model to learn a strong representation for the current task, and distill relevant information from previous tasks. We perform a thorough experimental evaluation and show that our method outperforms all the existing approaches by large margins, setting a solid state of the art on the continual semi-supervised learning paradigm. For example, on CIFAR-100 we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#27491;&#24358;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#36825;&#20123;&#32593;&#32476;&#20351;&#29992;&#27491;&#24358;&#20989;&#25968;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#12290;&#27491;&#24358;MLP&#20855;&#26377;&#24179;&#28369;&#24615;&#21644;&#32039;&#20945;&#24615;&#20004;&#20010;&#20851;&#38190;&#24615;&#36136;&#65292;&#24182;&#25552;&#20379;&#20102;&#25511;&#21046;&#26426;&#21046;&#26469;&#23450;&#20041;&#21644;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2212.01833</link><description>&lt;p&gt;
&#29702;&#35299;&#27491;&#24358;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Understanding Sinusoidal Neural Networks. (arXiv:2212.01833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#27491;&#24358;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21644;&#34920;&#36798;&#33021;&#21147;&#65292;&#36825;&#20123;&#32593;&#32476;&#20351;&#29992;&#27491;&#24358;&#20989;&#25968;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#12290;&#27491;&#24358;MLP&#20855;&#26377;&#24179;&#28369;&#24615;&#21644;&#32039;&#20945;&#24615;&#20004;&#20010;&#20851;&#38190;&#24615;&#36136;&#65292;&#24182;&#25552;&#20379;&#20102;&#25511;&#21046;&#26426;&#21046;&#26469;&#23450;&#20041;&#21644;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27491;&#24358;&#22810;&#23618;&#24863;&#30693;&#26426;&#32593;&#32476;&#65288;MLP&#65289;&#30340;&#32467;&#26500;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#27491;&#24358;&#20989;&#25968;&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#65288;&#20063;&#34987;&#31216;&#20026;&#31070;&#32463;&#22330;&#65289;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#20013;&#34920;&#31034;&#24120;&#35265;&#20449;&#21495;&#65288;&#22914;&#22270;&#20687;&#12289;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#21644;&#36752;&#23556;&#22330;&#65289;&#26041;&#38754;&#20855;&#26377;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#27491;&#24358;MLP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#22240;&#20110;&#20854;&#20004;&#20010;&#20851;&#38190;&#24615;&#36136;&#65306;&#24179;&#28369;&#24615;&#21644;&#32039;&#20945;&#24615;&#12290;&#36825;&#20123;&#20989;&#25968;&#26159;&#24179;&#28369;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#30001;&#20223;&#23556;&#26144;&#23556;&#19982;&#27491;&#24358;&#20989;&#25968;&#30340;&#32452;&#21512;&#24471;&#21040;&#12290;&#26412;&#24037;&#20316;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#26469;&#35777;&#26126;&#27491;&#24358;MLP&#30340;&#32039;&#20945;&#24615;&#65292;&#24182;&#22312;&#36825;&#20123;&#32593;&#32476;&#30340;&#23450;&#20041;&#21644;&#35757;&#32451;&#20013;&#25552;&#20379;&#20102;&#25511;&#21046;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23558;&#20854;&#23637;&#24320;&#20026;&#35856;&#27874;&#21644;&#26469;&#30740;&#31350;&#27491;&#24358;MLP&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20854;&#31532;&#19968;&#23618;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#35856;&#27874;&#23383;&#20856;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36755;&#20837;&#27491;&#24358;&#31070;&#32463;&#20803;&#12290;&#28982;&#21518;&#65292;&#38544;&#34255;&#23618;&#20351;&#29992;&#20223;&#23556;&#21464;&#25442;&#26469;&#32467;&#21512;&#36825;&#20010;&#23383;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the structure and representation capacity of sinusoidal MLPs - multilayer perceptron networks that use sine as the activation function. These neural networks (known as neural fields) have become fundamental in representing common signals in computer graphics, such as images, signed distance functions, and radiance fields. This success can be primarily attributed to two key properties of sinusoidal MLPs: smoothness and compactness. These functions are smooth because they arise from the composition of affine maps with the sine function. This work provides theoretical results to justify the compactness property of sinusoidal MLPs and provides control mechanisms in the definition and training of these networks.  We propose to study a sinusoidal MLP by expanding it as a harmonic sum. First, we observe that its first layer can be seen as a harmonic dictionary, which we call the input sinusoidal neurons. Then, a hidden layer combines this dictionary using an affin
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23545;&#28145;&#24230;&#22270;&#32858;&#31867;&#36827;&#34892;&#20102;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#24182;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2211.12875</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#32858;&#31867;&#32508;&#36848;&#65306;&#20998;&#31867;&#12289;&#25361;&#25112;&#12289;&#24212;&#29992;&#21644;&#24320;&#25918;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Graph Clustering: Taxonomy, Challenge, Application, and Open Resource. (arXiv:2211.12875v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12875
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23545;&#28145;&#24230;&#22270;&#32858;&#31867;&#36827;&#34892;&#20102;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#65292;&#24182;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20986;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26088;&#22312;&#23558;&#22270;&#20013;&#30340;&#33410;&#28857;&#21010;&#20998;&#20026;&#33509;&#24178;&#19981;&#21516;&#30340;&#31751;&#65292;&#26159;&#19968;&#39033;&#22522;&#30784;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#20511;&#21161;&#28145;&#24230;&#23398;&#20064;&#24378;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#39046;&#22495;&#30340;&#32508;&#36848;&#35770;&#25991;&#30456;&#23545;&#36739;&#23569;&#65292;&#32508;&#36848;&#35813;&#39046;&#22495;&#30340;&#24037;&#20316;&#21183;&#22312;&#24517;&#34892;&#12290;&#22522;&#20110;&#27492;&#21160;&#26426;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#30340;&#32508;&#36848;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#20844;&#24335;&#21270;&#23450;&#20041;&#12289;&#35780;&#20272;&#21644;&#21457;&#23637;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#22270;&#31867;&#22411;&#12289;&#32593;&#32476;&#26550;&#26500;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#32858;&#31867;&#26041;&#27861;&#31561;&#22235;&#20010;&#19981;&#21516;&#30340;&#26631;&#20934;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#30340;&#20998;&#31867;&#23398;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#20174;&#22270;&#25968;&#25454;&#36136;&#37327;&#12289;&#31283;&#23450;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#36776;&#21035;&#33021;&#21147;&#21644;&#26410;&#30693;&#31751;&#25968;&#31561;&#20116;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering, which aims to divide nodes in the graph into several distinct clusters, is a fundamental yet challenging task. Benefiting from the powerful representation capability of deep learning, deep graph clustering methods have achieved great success in recent years. However, the corresponding survey paper is relatively scarce, and it is imminent to make a summary of this field. From this motivation, we conduct a comprehensive survey of deep graph clustering. Firstly, we introduce formulaic definition, evaluation, and development in this field. Secondly, the taxonomy of deep graph clustering methods is presented based on four different criteria, including graph type, network architecture, learning paradigm, and clustering method. Thirdly, we carefully analyze the existing methods via extensive experiments and summarize the challenges and opportunities from five perspectives, including graph data quality, stability, scalability, discriminative capability, and unknown cluster nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27531;&#24046;&#31574;&#30053;&#23398;&#20064;&#65288;MBRPL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22825;&#32447;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#29616;&#26377;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#23454;&#38469;&#29615;&#22659;&#30340;&#20132;&#20114;&#27425;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21021;&#22987;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20026;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2211.08796</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#27531;&#24046;&#31574;&#30053;&#23398;&#20064;&#21450;&#20854;&#22312;&#22825;&#32447;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Model Based Residual Policy Learning with Applications to Antenna Control. (arXiv:2211.08796v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27531;&#24046;&#31574;&#30053;&#23398;&#20064;&#65288;MBRPL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22825;&#32447;&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#29616;&#26377;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#23454;&#38469;&#29615;&#22659;&#30340;&#20132;&#20114;&#27425;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21021;&#22987;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20026;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21487;&#24494;&#25511;&#21046;&#22120;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#31574;&#30053;&#24191;&#27867;&#24212;&#29992;&#20110;&#25511;&#21046;&#35832;&#22914;&#30005;&#20449;&#32593;&#32476;&#21644;&#26426;&#22120;&#20154;&#31561;&#23454;&#38469;&#31995;&#32479;&#12290;&#29305;&#21035;&#26159;&#65292;&#31227;&#21160;&#32593;&#32476;&#22522;&#31449;&#22825;&#32447;&#30340;&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#21160;&#24577;&#37197;&#32622;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#35206;&#30422;&#29575;&#21644;&#26381;&#21153;&#36136;&#37327;&#12290;&#21463;&#22825;&#32447;&#20542;&#26012;&#25511;&#21046;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#27169;&#22411;&#30340;&#27531;&#24046;&#31574;&#30053;&#23398;&#20064;&#65288;MBRPL&#65289;&#12290;MBRPL&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22686;&#24378;&#29616;&#26377;&#31574;&#30053;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19982;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20943;&#23569;&#19982;&#23454;&#38469;&#29615;&#22659;&#30340;&#20132;&#20114;&#27425;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#30740;&#31350;&#22522;&#20110;&#27169;&#22411;&#30340;&#22825;&#32447;&#25511;&#21046;&#26041;&#27861;&#30340;&#35770;&#25991;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21021;&#22987;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#26159;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-differentiable controllers and rule-based policies are widely used for controlling real systems such as telecommunication networks and robots. Specifically, parameters of mobile network base station antennas can be dynamically configured by these policies to improve users coverage and quality of service. Motivated by the antenna tilt control problem, we introduce Model-Based Residual Policy Learning (MBRPL), a practical reinforcement learning (RL) method. MBRPL enhances existing policies through a model-based approach, leading to improved sample efficiency and a decreased number of interactions with the actual environment when compared to off-the-shelf RL methods.To the best of our knowledge, this is the first paper that examines a model-based approach for antenna control. Experimental results reveal that our method delivers strong initial performance while improving sample efficiency over previous RL methods, which is one step towards deploying these algorithms in real networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#29255;&#27573;&#26816;&#27979;&#26694;&#26550;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36890;&#36807;&#21457;&#29616;&#12289;&#35299;&#37322;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#26410;&#26469;&#27169;&#22411;&#35774;&#35745;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.04476</link><description>&lt;p&gt;
&#21457;&#29616;&#12289;&#35299;&#37322;&#12289;&#25913;&#36827;&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33258;&#21160;&#29255;&#27573;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Discover, Explanation, Improvement: An Automatic Slice Detection Framework for Natural Language Processing. (arXiv:2211.04476v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#29255;&#27573;&#26816;&#27979;&#26694;&#26550;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36890;&#36807;&#21457;&#29616;&#12289;&#35299;&#37322;&#21644;&#25913;&#36827;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#26410;&#26469;&#27169;&#22411;&#35774;&#35745;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#31995;&#32479;&#24615;&#30340;&#38169;&#35823;&#12290;&#19982;&#25163;&#21160;&#38169;&#35823;&#20998;&#26512;&#19981;&#21516;&#65292;&#23545;&#20110;&#33258;&#21160;&#35782;&#21035;&#34920;&#29616;&#19981;&#20339;&#30340;&#25968;&#25454;&#32452;&#30340;&#29255;&#27573;&#26816;&#27979;&#27169;&#22411;&#65288;SDM&#65289;&#30340;&#30740;&#31350;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#21487;&#20197;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#24182;&#20026;&#26410;&#26469;&#27169;&#22411;&#35757;&#32451;&#21644;&#35774;&#35745;&#25552;&#20379;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;NLP&#20219;&#21153;&#19978;&#65292;&#23545;SDM&#30340;&#30740;&#31350;&#21644;&#20854;&#26377;&#25928;&#24615;&#30340;&#23450;&#37327;&#35780;&#20272;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;&#8220;Discover, Explain, Improve(DEIM)&#8221;&#30340;NLP&#20998;&#31867;&#20219;&#21153;&#22522;&#20934;&#21644;&#19968;&#20010;&#26032;&#30340;SDM&#27169;&#22411;Edisa&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;Edisa&#21457;&#29616;&#20102;&#19968;&#33268;&#19988;&#34920;&#29616;&#19981;&#20339;&#30340;&#25968;&#25454;&#32452;&#65307;DEIM&#23558;&#23427;&#20204;&#32479;&#19968;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#23450;&#37327;&#25351;&#26631;&#12290;&#22312;DEIM&#30340;&#35780;&#20272;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;Edisa&#33021;&#22815;&#20934;&#30830;&#36873;&#25321;&#26131;&#20986;&#38169;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained natural language processing (NLP) models have achieved high overall performance, but they still make systematic errors. Instead of manual error analysis, research on slice detection models (SDM), which automatically identify underperforming groups of datapoints, has caught escalated attention in Computer Vision for both understanding model behaviors and providing insights for future model training and designing. However, little research on SDM and quantitative evaluation of their effectiveness have been conducted on NLP tasks. Our paper fills the gap by proposing a benchmark named "Discover, Explain, Improve (DEIM)" for classification NLP tasks along with a new SDM Edisa. Edisa discovers coherent and underperforming groups of datapoints; DEIM then unites them under human-understandable concepts and provides comprehensive evaluation tasks and corresponding quantitative metrics. The evaluation in DEIM shows that Edisa can accurately select error-prone datapoints with informati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#35757;&#32451;&#23567;&#35268;&#27169;CNN&#65292;&#20811;&#26381;&#20102;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#19982;&#23454;&#20540;CNN&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#20351;&#29992;&#20102;&#26174;&#30528;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2211.02678</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#30340;&#39640;&#25928;ECG&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient ECG-based Atrial Fibrillation Detection via Parameterised Hypercomplex Neural Networks. (arXiv:2211.02678v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#35757;&#32451;&#23567;&#35268;&#27169;CNN&#65292;&#20811;&#26381;&#20102;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#19982;&#23454;&#20540;CNN&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#20351;&#29992;&#20102;&#26174;&#30528;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a lightweight convolutional neural network method based on parameterized hypercomplex neural networks for atrial fibrillation detection. The method trains small-scale CNNs on wearable devices, overcoming limited computing resources. The approach shows comparable performance to real-valued CNNs on two publicly available ECG datasets using significantly fewer model parameters.
&lt;/p&gt;
&lt;p&gt;
&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#24459;&#22833;&#24120;&#65292;&#19982;&#20013;&#39118;&#31561;&#20005;&#37325;&#30142;&#30149;&#30340;&#39640;&#39118;&#38505;&#30456;&#20851;&#12290;&#23884;&#20837;&#33258;&#21160;&#21644;&#21450;&#26102;&#30340;AF&#35780;&#20272;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#20351;&#29992;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#39044;&#38450;&#21361;&#21450;&#29983;&#21629;&#30340;&#24773;&#20917;&#26041;&#38754;&#20855;&#26377;&#21069;&#26223;&#12290;&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#21463;&#21040;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24102;&#26377;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#65288;PH&#65289;&#23618;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#22522;&#20110;ECG&#36827;&#34892;AF&#26816;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35757;&#32451;&#23567;&#35268;&#27169;CNN&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#30340;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#20351;&#29992;&#26174;&#30528;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19982;&#30456;&#24212;&#30340;&#23454;&#20540;CNN&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;PH&#27169;&#22411;&#27604;&#20854;&#20182;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#26356;&#28789;&#27963;&#65292;&#21487;&#20197;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Atrial fibrillation (AF) is the most common cardiac arrhythmia and associated with a high risk for serious conditions like stroke. The use of wearable devices embedded with automatic and timely AF assessment from electrocardiograms (ECGs) has shown to be promising in preventing life-threatening situations. Although deep neural networks have demonstrated superiority in model performance, their use on wearable devices is limited by the trade-off between model performance and complexity. In this work, we propose to use lightweight convolutional neural networks (CNNs) with parameterised hypercomplex (PH) layers for AF detection based on ECGs. The proposed approach trains small-scale CNNs, thus overcoming the limited computing resources on wearable devices. We show comparable performance to corresponding real-valued CNNs on two publicly available ECG datasets using significantly fewer model parameters. PH models are more flexible than other hypercomplex neural networks and can operate on an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#21464;&#23618;&#27425;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#29575;&#23618;&#27425;&#21270;&#24314;&#27169;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#36870;&#21160;&#21147;&#23398;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#34920;&#31034;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#22797;&#26434;&#24615;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01120</link><description>&lt;p&gt;
&#21487;&#21464;&#23618;&#27425;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#27010;&#29575;&#36870;&#21160;&#21147;&#23398;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Variational Hierarchical Mixtures for Probabilistic Learning of Inverse Dynamics. (arXiv:2211.01120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#21464;&#23618;&#27425;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#29575;&#23618;&#27425;&#21270;&#24314;&#27169;&#26469;&#23398;&#20064;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#36870;&#21160;&#21147;&#23398;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#34920;&#31034;&#21644;&#33258;&#36866;&#24212;&#25968;&#25454;&#22797;&#26434;&#24615;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#38598;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#20219;&#21153;&#30340;&#22797;&#26434;&#21270;&#65292;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#22238;&#24402;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#23398;&#20064;&#32452;&#25104;&#37096;&#20998;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#32463;&#20856;&#30340;&#22238;&#24402;&#27169;&#22411;&#36890;&#24120;&#35201;&#20040;&#26159;&#20855;&#26377;&#28789;&#27963;&#32467;&#26500;&#20294;&#19981;&#38543;&#25968;&#25454;&#20248;&#38597;&#25193;&#23637;&#30340;&#27010;&#29575;&#26680;&#26426;&#22120;&#65292;&#35201;&#20040;&#26159;&#20855;&#26377;&#38480;&#21046;&#21442;&#25968;&#24418;&#24335;&#21644;&#36739;&#24046;&#27491;&#21017;&#21270;&#30340;&#30830;&#23450;&#24615;&#19988;&#21487;&#25193;&#23637;&#30340;&#33258;&#21160;&#26426;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#27010;&#29575;&#23618;&#27425;&#24314;&#27169;&#33539;&#24335;&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#20379;&#20855;&#26377;&#20869;&#22312;&#22797;&#26434;&#24615;&#27491;&#21017;&#21270;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23545;&#23616;&#37096;&#22238;&#24402;&#25216;&#26415;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#36890;&#36807;&#19968;&#32452;&#23616;&#37096;&#32447;&#24615;&#25110;&#22810;&#39033;&#24335;&#21333;&#20803;&#26469;&#36924;&#36817;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#30340;&#21407;&#21017;&#65292;&#26500;&#24314;&#20102;&#36866;&#24212;&#25968;&#25454;&#22797;&#26434;&#24615;&#24182;&#19988;&#28508;&#22312;&#22320;&#28085;&#30422;&#26080;&#38480;&#20010;&#27169;&#22411;&#30340;&#28789;&#27963;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Well-calibrated probabilistic regression models are a crucial learning component in robotics applications as datasets grow rapidly and tasks become more complex. Unfortunately, classical regression models are usually either probabilistic kernel machines with a flexible structure that does not scale gracefully with data or deterministic and vastly scalable automata, albeit with a restrictive parametric form and poor regularization. In this paper, we consider a probabilistic hierarchical modeling paradigm that combines the benefits of both worlds to deliver computationally efficient representations with inherent complexity regularization. The presented approaches are probabilistic interpretations of local regression techniques that approximate nonlinear functions through a set of local linear or polynomial units. Importantly, we rely on principles from Bayesian nonparametrics to formulate flexible models that adapt their complexity to the data and can potentially encompass an infinite nu
&lt;/p&gt;</description></item><item><title>&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36880;&#23383;&#35760;&#24518;&#26080;&#27861;&#30495;&#27491;&#20445;&#25252;&#38544;&#31169;&#65292;&#26412;&#25991;&#35774;&#35745;&#30340;&#24067;&#38534;&#36807;&#28388;&#22120;&#34429;&#28982;&#38450;&#27490;&#20102;&#25152;&#26377;&#36880;&#23383;&#35760;&#24518;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#65292;&#23481;&#26131;&#34987;&#21512;&#29702;&#20462;&#25913;&#30340;&#8220;&#26679;&#24335;&#36716;&#25442;&#8221;&#25552;&#31034;&#32469;&#36807;&#12290;</title><link>http://arxiv.org/abs/2210.17546</link><description>&lt;p&gt;
&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#23383;&#35760;&#24518;&#20250;&#20135;&#29983;&#34394;&#20551;&#38544;&#31169;&#20445;&#25252;&#24863;
&lt;/p&gt;
&lt;p&gt;
Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy. (arXiv:2210.17546v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17546
&lt;/p&gt;
&lt;p&gt;
&#38450;&#27490;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#36880;&#23383;&#35760;&#24518;&#26080;&#27861;&#30495;&#27491;&#20445;&#25252;&#38544;&#31169;&#65292;&#26412;&#25991;&#35774;&#35745;&#30340;&#24067;&#38534;&#36807;&#28388;&#22120;&#34429;&#28982;&#38450;&#27490;&#20102;&#25152;&#26377;&#36880;&#23383;&#35760;&#24518;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#65292;&#23481;&#26131;&#34987;&#21512;&#29702;&#20462;&#25913;&#30340;&#8220;&#26679;&#24335;&#36716;&#25442;&#8221;&#25552;&#31034;&#32469;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#25968;&#25454;&#35760;&#24518;&#30340;&#29616;&#35937;&#65292;&#26412;&#30740;&#31350;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#19982;&#38544;&#31169;&#25110;&#29256;&#26435;&#30456;&#20851;&#30340;&#39118;&#38505;&#65292;&#24182;&#26377;&#21161;&#20110;&#35780;&#20272;&#23545;&#31574;&#12290;&#28982;&#32780;&#36880;&#23383;&#35760;&#24518;&#23450;&#20041;&#36807;&#20110;&#20005;&#26684;&#65292;&#26410;&#33021;&#25429;&#25417;&#26356;&#20026;&#24494;&#22937;&#30340;&#35760;&#24518;&#24418;&#24335;&#12290;&#26412;&#25991;&#22522;&#20110;&#24067;&#38534;&#36807;&#28388;&#22120;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#35813;&#8220;&#23436;&#32654;&#8221;&#36807;&#28388;&#22120;&#24182;&#19981;&#33021;&#38450;&#27490;&#35757;&#32451;&#25968;&#25454;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on "verbatim memorization", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this "perfect" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified "style-transfer" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLP-Mixer&#30340;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27969;&#37327;&#25968;&#25454;&#30340;&#20840;&#23616;&#20449;&#24687;&#20851;&#32852;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.16719</link><description>&lt;p&gt;
&#22522;&#20110;MLP-Mixer&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#24322;&#24120;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-view Multi-label Anomaly Network Traffic Classification based on MLP-Mixer Neural Network. (arXiv:2210.16719v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLP-Mixer&#30340;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27969;&#37327;&#25968;&#25454;&#30340;&#20840;&#23616;&#20449;&#24687;&#20851;&#32852;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#26159;&#35768;&#22810;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#24050;&#32463;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#24341;&#36215;&#20102;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#36890;&#24120;&#24378;&#35843;&#27969;&#37327;&#25968;&#25454;&#30340;&#23616;&#37096;&#27169;&#24335;&#65292;&#32780;&#24573;&#35270;&#20102;&#20840;&#23616;&#20449;&#24687;&#30340;&#20851;&#32852;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLP-Mixer&#30340;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;MLP-Mixer&#32467;&#26500;&#65292;&#36825;&#19982;&#25968;&#25454;&#21253;&#30340;&#32467;&#26500;&#26356;&#21152;&#31526;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#25968;&#25454;&#21253;&#34987;&#20998;&#20026;&#25968;&#25454;&#21253;&#22836;&#21644;&#25968;&#25454;&#21253;&#20027;&#20307;&#65292;&#21152;&#19978;&#19981;&#21516;&#35270;&#22270;&#19979;&#30340;&#27969;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#26631;&#31614;&#35774;&#32622;&#21516;&#26102;&#23398;&#20064;&#19981;&#21516;&#22330;&#26223;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#20511;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
Network traffic classification is the basis of many network security applications and has attracted enough attention in the field of cyberspace security. Existing network traffic classification based on convolutional neural networks (CNNs) often emphasizes local patterns of traffic data while ignoring global information associations. In this paper, we propose an MLP-Mixer based multi-view multi-label neural network for network traffic classification. Compared with the existing CNN-based methods, our method adopts the MLP-Mixer structure, which is more in line with the structure of the packet than the conventional convolution operation. In our method, one packet is divided into the packet header and the packet body, together with the flow features of the packet as input from different views. We utilize a multi-label setting to learn different scenarios simultaneously to improve the classification performance by exploiting the correlations between different scenarios. Taking advantage of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PopArt&#30340;&#39640;&#25928;&#31232;&#30095;&#32447;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;Lasso&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#32039;&#30340;$\ell_1$&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#22522;&#20110;&#27492;&#25512;&#23548;&#20986;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#31639;&#27861;&#65292;&#20855;&#26377;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#30340;&#21305;&#37197;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2210.15345</link><description>&lt;p&gt;
PopArt: &#39640;&#25928;&#31232;&#30095;&#22238;&#24402;&#21644;&#20248;&#21270;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#30340;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits. (arXiv:2210.15345v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PopArt&#30340;&#39640;&#25928;&#31232;&#30095;&#32447;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;Lasso&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#32039;&#30340;$\ell_1$&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#22522;&#20110;&#27492;&#25512;&#23548;&#20986;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#31639;&#27861;&#65292;&#20855;&#26377;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#30340;&#21305;&#37197;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#25353;&#39034;&#24207;&#36873;&#25321;&#19968;&#20010;&#21160;&#20316;&#24182;&#25509;&#25910;&#22870;&#21169;&#21453;&#39304;&#65292;&#32780;&#22870;&#21169;&#20989;&#25968;&#32447;&#24615;&#20381;&#36182;&#20110;&#21160;&#20316;&#30340;&#19968;&#20123;&#22352;&#26631;&#30340;&#21327;&#21464;&#37327;&#12290;&#36825;&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#37117;&#26377;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#35745;&#31639;&#39640;&#25928;&#30340;&#31232;&#30095;&#32447;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;PopArt&#65292;&#19982;Lasso&#65288;Tibshirani, 1996&#65289;&#30456;&#27604;&#65292;&#23427;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#20855;&#26377;&#26356;&#32039;&#30340;$\ell_1$&#24674;&#22797;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#33258;&#28982;&#22320;&#28608;&#21457;&#20102;&#19968;&#31181;&#20984;&#23454;&#39564;&#35774;&#35745;&#20934;&#21017;&#65292;&#22240;&#27492;&#22312;&#35745;&#31639;&#19978;&#26159;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26032;&#20272;&#35745;&#22120;&#21644;&#35774;&#35745;&#20934;&#21017;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#31639;&#27861;&#65292;&#20854;&#22312;&#32473;&#23450;&#21160;&#20316;&#38598;&#30340;&#20960;&#20309;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#65288;Hao et al., 2020&#65289;&#20855;&#26377;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#31232;&#30095;&#32447;&#24615;&#25671;&#33218;&#30340;&#21305;&#37197;&#19979;&#30028;&#65292;&#36825;&#22635;&#34917;&#20102;&#19978;&#30028;&#21644;&#19979;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
In sparse linear bandits, a learning agent sequentially selects an action and receive reward feedback, and the reward function depends linearly on a few coordinates of the covariates of the actions. This has applications in many real-world sequential decision making problems. In this paper, we propose a simple and computationally efficient sparse linear estimation method called PopArt that enjoys a tighter $\ell_1$ recovery guarantee compared to Lasso (Tibshirani, 1996) in many problems. Our bound naturally motivates an experimental design criterion that is convex and thus computationally efficient to solve. Based on our novel estimator and design criterion, we derive sparse linear bandit algorithms that enjoy improved regret upper bounds upon the state of the art (Hao et al., 2020), especially w.r.t. the geometry of the given action set. Finally, we prove a matching lower bound for sparse linear bandits in the data-poor regime, which closes the gap between upper and lower bounds in pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;Top-K&#26041;&#27861;&#29992;&#20110;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#31232;&#30095;&#21270;&#31243;&#24230;&#26469;&#20248;&#21270;&#25910;&#25947;&#24615;&#33021;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.13532</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;SGD&#20013;&#30340;Top-K&#26041;&#27861;&#29992;&#20110;&#24605;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Top-K in SGD for Communication-Efficient Distributed Learning. (arXiv:2210.13532v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13532
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;Top-K&#26041;&#27861;&#29992;&#20110;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#31232;&#30095;&#21270;&#31243;&#24230;&#26469;&#20248;&#21270;&#25910;&#25947;&#24615;&#33021;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#19982;&#26799;&#24230;&#21387;&#32553;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#12290;&#26799;&#24230;&#21387;&#32553;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;Top-K&#31232;&#30095;&#21270;&#65292;&#23427;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#22266;&#23450;&#30340;&#31243;&#24230;&#31232;&#30095;&#21270;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#35843;&#25972;&#31232;&#30095;&#21270;&#31243;&#24230;&#20197;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#25110;&#35757;&#32451;&#36895;&#24230;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;SGD&#20013;&#30340;Top-K&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20026;&#27599;&#20010;&#26799;&#24230;&#19979;&#38477;&#27493;&#39588;&#25552;&#20379;&#33258;&#36866;&#24212;&#31232;&#30095;&#21270;&#31243;&#24230;&#65292;&#36890;&#36807;&#24179;&#34913;&#36890;&#20449;&#25104;&#26412;&#21644;&#25910;&#25947;&#35823;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#26469;&#20248;&#21270;&#25910;&#25947;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25512;&#23548;&#20102;&#33258;&#36866;&#24212;&#31232;&#30095;&#21270;&#26041;&#26696;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#25910;&#25947;&#35823;&#24046;&#19978;&#30028;&#12290;&#20854;&#27425;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#22312;&#36890;&#20449;&#25104;&#26412;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#25910;&#25947;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#32473;&#20986;&#20102;&#22312;MNIS&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed stochastic gradient descent (SGD) with gradient compression has become a popular communication-efficient solution for accelerating distributed learning. One commonly used method for gradient compression is Top-K sparsification, which sparsifies the gradients by a fixed degree during model training. However, there has been a lack of an adaptive approach to adjust the sparsification degree to maximize the potential of the model's performance or training speed. This paper proposes a novel adaptive Top-K in SGD framework that enables an adaptive degree of sparsification for each gradient descent step to optimize the convergence performance by balancing the trade-off between communication cost and convergence error. Firstly, an upper bound of convergence error is derived for the adaptive sparsification scheme and the loss function. Secondly, an algorithm is designed to minimize the convergence error under the communication cost constraints. Finally, numerical results on the MNIS
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;TiDAL&#65292;&#21033;&#29992;&#35757;&#32451;&#21160;&#21147;&#23398;&#26469;&#37327;&#21270;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#36319;&#36394;&#38382;&#39064;&#65292;&#21033;&#29992;&#39044;&#27979;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#25968;&#25454;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2210.06788</link><description>&lt;p&gt;
TiDAL: &#23398;&#20064;&#20027;&#21160;&#23398;&#20064;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
TiDAL: Learning Training Dynamics for Active Learning. (arXiv:2210.06788v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06788
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;TiDAL&#65292;&#21033;&#29992;&#35757;&#32451;&#21160;&#21147;&#23398;&#26469;&#37327;&#21270;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#36319;&#36394;&#38382;&#39064;&#65292;&#21033;&#29992;&#39044;&#27979;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#25968;&#25454;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26088;&#22312;&#22312;&#26377;&#38480;&#39044;&#31639;&#19979;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#20013;&#36873;&#25321;&#26368;&#26377;&#29992;&#30340;&#25968;&#25454;&#26679;&#26412;&#24182;&#23545;&#20854;&#36827;&#34892;&#27880;&#37322;&#65292;&#20197;&#25193;&#23637;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#23588;&#20854;&#26159;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#36873;&#25321;&#26368;&#19981;&#30830;&#23450;&#30340;&#26679;&#26412;&#65292;&#22312;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#20027;&#21160;&#23398;&#20064;&#25991;&#29486;&#32463;&#24120;&#24573;&#35270;&#35757;&#32451;&#21160;&#21147;&#23398;&#65288;TD&#65289;&#65292;&#21363;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20248;&#21270;&#26102;&#27169;&#22411;&#34892;&#20026;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#23613;&#31649;&#25991;&#29486;&#30340;&#20854;&#20182;&#39046;&#22495;&#32463;&#39564;&#35777;&#26126;TD&#25552;&#20379;&#20102;&#34913;&#37327;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#32447;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#35757;&#32451;&#21160;&#21147;&#23398;&#20027;&#21160;&#23398;&#20064;&#65288;TiDAL&#65289;&#65292;&#23427;&#21033;&#29992;TD&#26469;&#37327;&#21270;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#30001;&#20110;&#36319;&#36394;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;TD&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#25152;&#20197;TiDAL&#21033;&#29992;&#19968;&#20010;&#39069;&#22806;&#30340;&#39044;&#27979;&#27169;&#22359;&#26469;&#23398;&#20064;&#26631;&#35760;&#25968;&#25454;&#30340;TD&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35777;&#26126;TiDAL&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning (AL) aims to select the most useful data samples from an unlabeled data pool and annotate them to expand the labeled dataset under a limited budget. Especially, uncertainty-based methods choose the most uncertain samples, which are known to be effective in improving model performance. However, AL literature often overlooks training dynamics (TD), defined as the ever-changing model behavior during optimization via stochastic gradient descent, even though other areas of literature have empirically shown that TD provides important clues for measuring the sample uncertainty. In this paper, we propose a novel AL method, Training Dynamics for Active Learning (TiDAL), which leverages the TD to quantify uncertainties of unlabeled data. Since tracking the TD of all the large-scale unlabeled data is impractical, TiDAL utilizes an additional prediction module that learns the TD of labeled data. To further justify the design of TiDAL, we provide theoretical and empirical evidence t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LODO&#30340;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#19982;&#25311;&#29275;&#39039;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23454;&#26102;&#23398;&#20064;&#24182;&#36866;&#24212;&#25439;&#22833;&#26223;&#35266;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#32447;&#20803;&#23398;&#20064;&#26368;&#20339;&#30340;&#39044;&#26465;&#20214;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2210.06171</link><description>&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#25311;&#29275;&#39039;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Optimize Quasi-Newton Methods. (arXiv:2210.06171v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LODO&#30340;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#19982;&#25311;&#29275;&#39039;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23454;&#26102;&#23398;&#20064;&#24182;&#36866;&#24212;&#25439;&#22833;&#26223;&#35266;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#20174;&#32780;&#22312;&#32447;&#20803;&#23398;&#20064;&#26368;&#20339;&#30340;&#39044;&#26465;&#20214;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#23545;&#20110;&#35745;&#31639;&#39640;&#25928;&#22320;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#25216;&#26415;&#26159;&#23558;&#26799;&#24230;&#20056;&#20197;&#19968;&#20010;&#39044;&#26465;&#20214;&#30697;&#38453;&#26469;&#20135;&#29983;&#19968;&#27493;&#65292;&#20294;&#26368;&#22909;&#30340;&#39044;&#26465;&#20214;&#30697;&#38453;&#26159;&#19981;&#28165;&#26970;&#30340;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;LODO&#65292;&#23427;&#23581;&#35797;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#22312;&#32447;&#20803;&#23398;&#20064;&#26368;&#20339;&#30340;&#39044;&#26465;&#20214;&#30697;&#38453;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#22120;&#23558;&#23398;&#20064;&#20248;&#21270;&#65288;L2O&#65289;&#25216;&#26415;&#19982;&#25311;&#29275;&#39039;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#31070;&#32463;&#32593;&#32476;&#20026;&#21442;&#25968;&#21270;&#30340;&#39044;&#26465;&#20214;&#30697;&#38453;&#36827;&#34892;&#23398;&#20064;&#65307;&#23427;&#20204;&#27604;&#20854;&#20182;&#25311;&#29275;&#39039;&#26041;&#27861;&#20013;&#30340;&#39044;&#26465;&#20214;&#30697;&#38453;&#26356;&#28789;&#27963;&#12290;&#19982;&#20854;&#20182;L2O&#26041;&#27861;&#19981;&#21516;&#65292;LODO&#19981;&#38656;&#35201;&#22312;&#35757;&#32451;&#20219;&#21153;&#20998;&#24067;&#19978;&#36827;&#34892;&#20803;&#35757;&#32451;&#65292;&#32780;&#26159;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#36827;&#34892;&#20248;&#21270;&#26102;&#23454;&#26102;&#23398;&#20064;&#20248;&#21270;&#65292;&#36866;&#24212;&#36941;&#21382;&#20013;&#30340;&#25439;&#22833;&#26223;&#35266;&#30340;&#23616;&#37096;&#29305;&#24449;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20248;&#21270;&#22120;&#36817;&#20284;&#22320;&#25311;&#21512;&#20102;&#36127;Hessian&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast gradient-based optimization algorithms have become increasingly essential for the computationally efficient training of machine learning models. One technique is to multiply the gradient by a preconditioner matrix to produce a step, but it is unclear what the best preconditioner matrix is. This paper introduces a novel machine learning optimizer called LODO, which tries to online meta-learn the best preconditioner during optimization. Specifically, our optimizer merges Learning to Optimize (L2O) techniques with quasi-Newton methods to learn preconditioners parameterized as neural networks; they are more flexible than preconditioners in other quasi-Newton methods. Unlike other L2O methods, LODO does not require any meta-training on a training task distribution, and instead learns to optimize on the fly while optimizing on the test task, adapting to the local characteristics of the loss landscape while traversing it. Theoretically, we show that our optimizer approximates the inverse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23545;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23614;&#24179;&#22343;TD&#33021;&#20197;&#26368;&#20248;&#36895;&#29575; $O(1/t)$ &#25910;&#25947;&#65292;&#24182;&#19988;&#21021;&#22987;&#35823;&#24046;&#34928;&#20943;&#36895;&#29575;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#24456;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.05918</link><description>&lt;p&gt;
&#26377;&#38480;&#26102;&#38388;&#20869;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#30340;&#20998;&#26512;&#65306;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Finite time analysis of temporal difference learning with linear function approximation: Tail averaging and regularisation. (arXiv:2210.05918v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23614;&#24179;&#22343;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23545;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23614;&#24179;&#22343;TD&#33021;&#20197;&#26368;&#20248;&#36895;&#29575; $O(1/t)$ &#25910;&#25947;&#65292;&#24182;&#19988;&#21021;&#22987;&#35823;&#24046;&#34928;&#20943;&#36895;&#29575;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#24456;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#27969;&#34892;&#30340;&#26102;&#24207;&#24046;&#24322;(TD)&#23398;&#20064;&#31639;&#27861;&#19982;&#23614;&#24179;&#22343;&#30456;&#32467;&#21512;&#26102;&#30340;&#26377;&#38480;&#26102;&#38388;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19981;&#38656;&#35201;&#20851;&#20110;&#24213;&#23618;&#25237;&#24433;TD&#19981;&#21160;&#28857;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20449;&#24687;&#30340;&#27493;&#38271;&#36873;&#25321;&#19979;&#65292;&#25512;&#23548;&#20102;&#23614;&#24179;&#22343;TD&#36845;&#20195;&#30340;&#21442;&#25968;&#35823;&#24046;&#30340;&#26377;&#38480;&#26102;&#38388;&#30028;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23614;&#24179;&#22343;TD&#20197;&#26399;&#26395;&#36895;&#29575;&#21644;&#39640;&#27010;&#29575;&#25910;&#25947;&#20110;&#26368;&#20248;&#30340; $O(1/t)$ &#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#23637;&#31034;&#20102;&#21021;&#22987;&#35823;&#24046;(&#20559;&#24046;)&#30340;&#26356;&#24555;&#34928;&#20943;&#36895;&#29575;&#65292;&#36825;&#26159;&#23545;&#25152;&#26377;&#36845;&#20195;&#30340;&#24179;&#22343;&#20540;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21017;&#21270;&#30340;TD&#21464;&#20307;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#35748;&#20026;&#27491;&#21017;&#21270;&#30340;TD&#29256;&#26412;&#22312;&#20855;&#26377;&#30149;&#24577;&#29305;&#24449;&#30340;&#38382;&#39064;&#19978;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the finite-time behaviour of the popular temporal difference (TD) learning algorithm when combined with tail-averaging. We derive finite time bounds on the parameter error of the tail-averaged TD iterate under a step-size choice that does not require information about the eigenvalues of the matrix underlying the projected TD fixed point. Our analysis shows that tail-averaged TD converges at the optimal $O\left(1/t\right)$ rate, both in expectation and with high probability. In addition, our bounds exhibit a sharper rate of decay for the initial error (bias), which is an improvement over averaging all iterates. We also propose and analyse a variant of TD that incorporates regularisation. From analysis, we conclude that the regularised version of TD is useful for problems with ill-conditioned features.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#26680;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#27861; (gKRLS)&#65292;&#35299;&#20915;&#20102;&#26680;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#27861; (KRLS) &#22312;&#24403;&#21069;&#20351;&#29992;&#20013;&#30340;&#20004;&#20010;&#38480;&#21046;&#65306;&#23427;&#30340;&#25193;&#23637;&#33021;&#21147;&#19981;&#36275;&#65292;&#19988;&#21363;&#20351;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#35745;&#31639;&#20195;&#20215;&#20063;&#38750;&#24120;&#39640;&#26114;&#12290;</title><link>http://arxiv.org/abs/2209.14355</link><description>&lt;p&gt;
&#24191;&#20041;&#26680;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalized Kernel Regularized Least Squares. (arXiv:2209.14355v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#26680;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#27861; (gKRLS)&#65292;&#35299;&#20915;&#20102;&#26680;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#27861; (KRLS) &#22312;&#24403;&#21069;&#20351;&#29992;&#20013;&#30340;&#20004;&#20010;&#38480;&#21046;&#65306;&#23427;&#30340;&#25193;&#23637;&#33021;&#21147;&#19981;&#36275;&#65292;&#19988;&#21363;&#20351;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#65292;&#20854;&#35745;&#31639;&#20195;&#20215;&#20063;&#38750;&#24120;&#39640;&#26114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#27861; (KRLS) &#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#20272;&#35745;&#20855;&#26377;&#22797;&#26434;&#21464;&#37327;&#20851;&#31995;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20854;&#21487;&#29992;&#24615;&#22240;&#20004;&#20010;&#21407;&#22240;&#32780;&#21463;&#21040;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#28789;&#27963;&#24615;&#65292;&#19981;&#20801;&#35768;&#23558;KRLS&#19982;&#29702;&#35770;&#21160;&#26426;&#19979;&#30340;&#25193;&#23637;&#22914;&#38543;&#26426;&#25928;&#24212;&#12289;&#26410;&#32463;&#27491;&#21017;&#21270;&#30340;&#22266;&#23450;&#25928;&#24212;&#25110;&#38750;&#39640;&#26031;&#32467;&#26524;&#32452;&#21512;&#20351;&#29992;&#12290;&#20854;&#27425;&#65292;&#21363;&#20351;&#26159;&#35268;&#27169;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20272;&#35745;&#20063;&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;KRLS (gKRLS) &#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;KRLS&#21487;&#20197;&#37325;&#26032;&#35774;&#23450;&#20026;&#20998;&#23618;&#27169;&#22411;&#65292;&#20174;&#32780;&#20801;&#35768;&#36731;&#26494;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#27169;&#22411;&#26500;&#24314;&#65292;&#22312;&#20854;&#20013;KRLS&#21487;&#20197;&#19982;&#38543;&#26426;&#25928;&#24212;&#12289;&#26679;&#26465;&#21644;&#26410;&#32463;&#27491;&#21017;&#21270;&#30340;&#22266;&#23450;&#25928;&#24212;&#24182;&#29992;&#12290;&#22312;&#35745;&#31639;&#26041;&#38754;&#65292;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;&#38543;&#26426;&#33609;&#22270;&#26041;&#27861;&#65292;&#20197;&#26497;&#22823;&#22320;&#21152;&#36895;&#20272;&#35745;&#65292;&#24182;&#22312;&#20272;&#35745;&#36136;&#37327;&#19978;&#25215;&#25285;&#26377;&#38480;&#30340;&#24809;&#32602;&#12290;&#25105;&#20204;&#35777;&#26126;gKRLS&#21487;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#30340;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel Regularized Least Squares (KRLS) is a popular method for flexibly estimating models that may have complex relationships between variables. However, its usefulness to many researchers is limited for two reasons. First, existing approaches are inflexible and do not allow KRLS to be combined with theoretically-motivated extensions such as random effects, unregularized fixed effects, or non-Gaussian outcomes. Second, estimation is extremely computationally intensive for even modestly sized datasets. Our paper addresses both concerns by introducing generalized KRLS (gKRLS). We note that KRLS can be re-formulated as a hierarchical model thereby allowing easy inference and modular model construction where KRLS can be used alongside random effects, splines, and unregularized fixed effects. Computationally, we also implement random sketching to dramatically accelerate estimation while incurring a limited penalty in estimation quality. We demonstrate that gKRLS can be fit on datasets with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23558;&#35813;&#20272;&#35745;&#22120;&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2206.10397</link><description>&lt;p&gt;
&#40065;&#26834;&#39134;&#34892;&#25511;&#21046;&#30340;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v10 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23558;&#35813;&#20272;&#35745;&#22120;&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#21644;&#24212;&#23545;&#24178;&#25200;&#23545;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#36890;&#24120;&#38656;&#35201;&#23545;&#29305;&#23450;&#39134;&#34892;&#22330;&#26223;&#36827;&#34892;&#22823;&#37327;&#35843;&#25972;&#65292;&#25110;&#32773;&#32463;&#36807;&#24191;&#27867;&#30340;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#35757;&#32451;&#65292;&#25165;&#33021;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#30001;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;MHE&#20272;&#35745;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#23558;MHE&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20197;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#26080;&#32541;&#34701;&#21512;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#20351;&#29992;&#36882;&#24402;&#24418;&#24335;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#26799;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating and reacting to disturbances is crucial for robust flight control of quadrotors. Existing estimators typically require significant tuning for a specific flight scenario or training with extensive ground-truth disturbance data to achieve satisfactory performance. In this paper, we propose a neural moving horizon estimator (NeuroMHE) that can automatically tune the key parameters modeled by a neural network and adapt to different flight scenarios. We achieve this by deriving the analytical gradients of the MHE estimates with respect to the weighting matrices, which enables a seamless embedding of the MHE as a learnable layer into neural networks for highly effective learning. Interestingly, we show that the gradients can be computed efficiently using a Kalman filter in a recursive form. Moreover, we develop a model-based policy gradient algorithm to train NeuroMHE directly from the quadrotor trajectory tracking error without needing the ground-truth disturbance data. The effec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.00979</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs. (arXiv:2206.00979v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00979
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65288;SP&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#26680;&#24515;&#20043;&#19968;&#12290;&#23427;&#23558;&#22270;&#20998;&#35299;&#20026;&#26368;&#30701;&#36335;&#24452;&#65292;&#24182;&#35745;&#31639;&#27599;&#20010;&#22270;&#20013;&#26368;&#30701;&#36335;&#24452;&#30340;&#39057;&#29575;&#12290;&#28982;&#32780;&#65292;SP&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#26368;&#30701;&#36335;&#24452;&#30340;&#19977;&#20803;&#34920;&#31034;&#22833;&#21435;&#20102;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;SP&#27604;&#36739;&#22270;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#22270;&#32467;&#26500;&#30340;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38142;&#29366;&#32467;&#26500;&#12289;&#29615;&#29366;&#32467;&#26500;&#21644;&#26143;&#29366;&#32467;&#26500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#12290;&#23427;&#20351;&#29992;&#20197;&#27599;&#20010;&#39030;&#28857;&#20026;&#26681;&#30340;&#26576;&#20010;&#28145;&#24230;&#30340;BFS&#26641;&#26469;&#38480;&#21046;&#32771;&#34385;&#26368;&#30701;&#36335;&#24452;&#30340;&#26368;&#22823;&#38271;&#24230;&#65292;&#32771;&#34385;&#21040;&#23567;&#19990;&#30028;&#29305;&#24615;&#12290;&#23427;&#32771;&#34385;&#20102;&#26368;&#30701;&#36335;&#24452;&#20013;&#25152;&#26377;&#39030;&#28857;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#26041;&#20415;&#22312;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#19978;&#27604;&#36739;&#22270;&#65292;&#23427;&#20174;&#39030;&#28857;&#21644;
&lt;/p&gt;
&lt;p&gt;
The traditional shortest-path graph kernel (SP) is one of the most popular graph kernels. It decomposes graphs into shortest paths and computes their frequencies in each graph. However, SP has two main challenges: Firstly, the triplet representation of the shortest path loses information. Secondly, SP compares graphs without considering the multiple different scales of the graph structure which is common in real-world graphs, e.g., the chain-, ring-, and star-structures in social networks. To overcome these two challenges, we develop a novel shortest-path graph kernel called the Multi-scale Wasserstein Shortest-Path Filtration graph kernel (MWSPF). It uses a BFS tree of a certain depth rooted at each vertex to restrict the maximum length of the shortest path considering the small world property. It considers the labels of all the vertices in the shortest path. To facilitate the comparison of graphs at multiple different scales, it augments graphs from both the aspects of the vertex and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#30456;&#20851;&#26435;&#37325;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#38480;&#34892;&#20026;&#65292;&#21457;&#29616;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#37327;&#26469;&#21051;&#30011;&#65292;&#24403;&#20854;&#20013;&#33267;&#23569;&#19968;&#23618;&#30340;&#37327;&#26159;&#38750;&#24179;&#20961;&#30340;&#26102;&#20505;&#65292;&#24471;&#21040;&#20102;&#39640;&#26031;&#36807;&#31243;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.08187</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20851;&#26435;&#37325;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65306;&#39640;&#26031;&#36807;&#31243;&#28151;&#21512;&#26497;&#38480;&#12289;&#37325;&#23614;&#12289;&#31232;&#30095;&#24615;&#21644;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks with dependent weights: Gaussian Process mixture limit, heavy tails, sparsity and compressibility. (arXiv:2205.08187v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.08187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#30456;&#20851;&#26435;&#37325;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#38480;&#34892;&#20026;&#65292;&#21457;&#29616;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#37327;&#26469;&#21051;&#30011;&#65292;&#24403;&#20854;&#20013;&#33267;&#23569;&#19968;&#23618;&#30340;&#37327;&#26159;&#38750;&#24179;&#20961;&#30340;&#26102;&#20505;&#65292;&#24471;&#21040;&#20102;&#39640;&#26031;&#36807;&#31243;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#30456;&#20851;&#26435;&#37325;&#24182;&#36890;&#36807;&#39640;&#26031;&#20998;&#24067;&#28151;&#21512;&#24314;&#27169;&#30340;&#26080;&#38480;&#23485;&#24230;&#21069;&#39304;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#38480;&#12290;&#32593;&#32476;&#30340;&#27599;&#20010;&#38544;&#34255;&#33410;&#28857;&#34987;&#20998;&#37197;&#19968;&#20010;&#38750;&#36127;&#38543;&#26426;&#21464;&#37327;&#65292;&#35813;&#38543;&#26426;&#21464;&#37327;&#25511;&#21046;&#35813;&#33410;&#28857;&#30340;&#36755;&#20986;&#26435;&#37325;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#33410;&#28857;&#38543;&#26426;&#21464;&#37327;&#20570;&#20102;&#26368;&#23567;&#30340;&#20551;&#35774;&#65306;&#23427;&#20204;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#24182;&#19988;&#22312;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#19979;&#65292;&#27599;&#19968;&#23618;&#30340;&#38543;&#26426;&#21464;&#37327;&#21644;&#25910;&#25947;&#21040;&#19968;&#20123;&#26377;&#38480;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#31616;&#21333;&#30340;&#37327;&#26469;&#21051;&#30011;&#65306;&#19968;&#20010;&#38750;&#36127;&#26631;&#37327;&#21442;&#25968;&#21644;&#19968;&#20010;&#27491;&#23454;&#25968;&#19978;&#30340;L&#233;vy&#27979;&#24230;&#12290;&#22914;&#26524;&#26631;&#37327;&#21442;&#25968;&#20005;&#26684;&#20026;&#27491;&#19988;&#25152;&#26377;&#38544;&#34255;&#23618;&#30340;L&#233;vy&#27979;&#24230;&#37117;&#26159;&#24179;&#20961;&#30340;&#65292;&#37027;&#20040;&#23601;&#24471;&#21040;&#20102;&#32463;&#20856;&#30340;&#39640;&#26031;&#36807;&#31243;(GP)&#26497;&#38480;&#65292;&#21363;&#36890;&#36807;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#26435;&#37325;&#33719;&#24471;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#22914;&#26524;&#33267;&#23569;&#19968;&#23618;&#30340;L&#233;vy&#27979;&#24230;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#39640;&#26031;&#36807;&#31243;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article studies the infinite-width limit of deep feedforward neural networks whose weights are dependent, and modelled via a mixture of Gaussian distributions. Each hidden node of the network is assigned a nonnegative random variable that controls the variance of the outgoing weights of that node. We make minimal assumptions on these per-node random variables: they are iid and their sum, in each layer, converges to some finite random variable in the infinite-width limit. Under this model, we show that each layer of the infinite-width neural network can be characterised by two simple quantities: a non-negative scalar parameter and a L\'evy measure on the positive reals. If the scalar parameters are strictly positive and the L\'evy measures are trivial at all hidden layers, then one recovers the classical Gaussian process (GP) limit, obtained with iid Gaussian weights. More interestingly, if the L\'evy measure of at least one layer is non-trivial, we obtain a mixture of Gaussian pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#30340;&#38543;&#26426;&#20998;&#25903;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20840;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#21151;&#33021;&#38750;&#32447;&#24615;&#21644;&#26799;&#24230;&#39033;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#33719;&#24471;&#30340;&#35299;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2203.03234</link><description>&lt;p&gt;
&#29992;&#20110;&#20840;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#20998;&#25903;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A deep branching solver for fully nonlinear partial differential equations. (arXiv:2203.03234v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#30340;&#38543;&#26426;&#20998;&#25903;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20840;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#21151;&#33021;&#38750;&#32447;&#24615;&#21644;&#26799;&#24230;&#39033;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#33719;&#24471;&#30340;&#35299;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#30340;&#38543;&#26426;&#20998;&#25903;&#31639;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#35299;&#20840;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#33945;&#29305;&#21345;&#32599;&#20998;&#25903;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#21253;&#21547;&#20219;&#24847;&#38454;&#26799;&#24230;&#39033;&#30340;&#21151;&#33021;&#38750;&#32447;&#24615;&#38382;&#39064;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#23427;&#36824;&#21487;&#20197;&#26816;&#26597;&#23398;&#20064;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;&#25152;&#23637;&#31034;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#20840;&#38750;&#32447;&#24615;&#31034;&#20363;&#20013;&#21487;&#20197;&#20248;&#20110;&#22522;&#20110;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#25110;Galerkin&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#33719;&#24471;&#30340;&#35299;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a multidimensional deep learning implementation of a stochastic branching algorithm for the numerical solution of fully nonlinear PDEs. This approach is designed to tackle functional nonlinearities involving gradient terms of any orders, by combining the use of neural networks with a Monte Carlo branching algorithm. In comparison with other deep learning PDE solvers, it also allows us to check the consistency of the learned neural network function. Numerical experiments presented show that this algorithm can outperform deep learning approaches based on backward stochastic differential equations or the Galerkin method, and provide solution estimates that are not obtained by those methods in fully nonlinear examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (c-MARL) &#20195;&#29702;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#29366;&#24577;&#25200;&#21160;&#20197;&#38477;&#20302;&#22242;&#38431;&#24635;&#22870;&#21169;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#39318;&#20010;&#21463;&#23475;&#20195;&#29702;&#36873;&#25321;&#31574;&#30053;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#23450;&#20041;&#30446;&#26631;&#22833;&#36133;&#29366;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#34987;&#27979;&#35797;&#30340;&#25152;&#26377;&#29615;&#22659;&#20013;&#19968;&#33268;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2202.03558</link><description>&lt;p&gt;
&#26356;&#26377;&#25928;&#22320;&#25915;&#20987;c-MARL&#65306;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attacking c-MARL More Effectively: A Data Driven Approach. (arXiv:2202.03558v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (c-MARL) &#20195;&#29702;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#29366;&#24577;&#25200;&#21160;&#20197;&#38477;&#20302;&#22242;&#38431;&#24635;&#22870;&#21169;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#39318;&#20010;&#21463;&#23475;&#20195;&#29702;&#36873;&#25321;&#31574;&#30053;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20197;&#23450;&#20041;&#30446;&#26631;&#22833;&#36133;&#29366;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#34987;&#27979;&#35797;&#30340;&#25152;&#26377;&#29615;&#22659;&#20013;&#19968;&#33268;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#38024;&#23545;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;c-MARL&#65289;&#24050;&#32463;&#28044;&#29616;&#20102;&#24456;&#22810;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;c-MARL&#20195;&#29702;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#30340;&#31283;&#20581;&#24615;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; c-MBA &#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272; c-MARL &#20195;&#29702;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#24615;&#29366;&#24577;&#25200;&#21160;&#65292;&#20174;&#32780;&#38477;&#20302;&#22242;&#38431;&#24635;&#22870;&#21169;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21463;&#23475;&#20195;&#29702;&#30340;&#36873;&#25321;&#31574;&#30053;&#21644;&#31532;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#30446;&#26631;&#22833;&#36133;&#29366;&#24577;&#65292;&#27599;&#20010;&#22833;&#36133;&#29366;&#24577;&#37117;&#33021;&#24110;&#21161;&#25105;&#20204;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#32780;&#26080;&#38656;&#23545;&#24213;&#23618;&#29615;&#22659;&#25317;&#26377;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340; MARL &#27979;&#35797;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#27979;&#35797;&#29615;&#22659;&#20013;&#37117;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#65306;&#25105;&#20204;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#25915;&#20987;&#22312;&#25152;&#26377;&#27979;&#35797;&#29615;&#22659;&#20013;&#37117;&#25345;&#32493;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a proliferation of methods were developed for cooperative multi-agent reinforcement learning (c-MARL). However, the robustness of c-MARL agents against adversarial attacks has been rarely explored. In this paper, we propose to evaluate the robustness of c-MARL agents via a model-based approach, named c-MBA. Our proposed formulation can craft much stronger adversarial state perturbations of c-MARL agents to lower total team rewards than existing model-free approaches. In addition, we propose the first victim-agent selection strategy and the first data-driven approach to define targeted failure states where each of them allows us to develop even stronger adversarial attack without the expert knowledge to the underlying environment. Our numerical experiments on two representative MARL benchmarks illustrate the advantage of our approach over other baselines: our model-based attack consistently outperforms other baselines in all tested environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;Temporal Difference&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#25511;&#21046;&#35770;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#26694;&#26550;&#65292;&#20174;&#25511;&#21046;&#35770;&#35282;&#24230;&#25552;&#20379;&#20102;&#23545;TD&#23398;&#20064;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2112.14417</link><description>&lt;p&gt;
Temporal Difference&#23398;&#20064;&#31639;&#27861;&#30340;&#25511;&#21046;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Control Theoretic Analysis of Temporal Difference Learning. (arXiv:2112.14417v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;Temporal Difference&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#25511;&#21046;&#35770;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#26694;&#26550;&#65292;&#20174;&#25511;&#21046;&#35770;&#35282;&#24230;&#25552;&#20379;&#20102;&#23545;TD&#23398;&#20064;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#26356;&#28145;&#20837;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;Temporal Difference (TD)&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#25511;&#21046;&#35770;&#20998;&#26512;&#12290;TD&#23398;&#20064;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#22522;&#30707;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;&#19982;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#32473;&#23450;&#31574;&#30053;&#30456;&#20851;&#30340;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#24050;&#23384;&#22312;&#22810;&#31687;&#20851;&#20110;TD&#23398;&#20064;&#29702;&#35770;&#29702;&#35299;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#30452;&#21040;&#26368;&#36817;&#20960;&#24180;&#65292;&#30740;&#31350;&#20154;&#21592;&#25165;&#33021;&#23545;&#20854;&#32479;&#35745;&#25928;&#29575;&#25552;&#20379;&#20855;&#20307;&#20445;&#35777;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#25511;&#21046;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;TD&#23398;&#20064;&#65292;&#20511;&#37492;&#20102;&#32447;&#24615;&#31995;&#32479;&#25511;&#21046;&#39046;&#22495;&#30340;&#24050;&#26377;&#27010;&#24565;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25511;&#21046;&#35770;&#23548;&#20986;&#30340;&#31616;&#21333;&#20998;&#26512;&#24037;&#20855;&#65292;&#20026;TD&#23398;&#20064;&#30340;&#26426;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26356;&#24191;&#38420;&#39046;&#22495;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this manuscript is to conduct a controltheoretic analysis of Temporal Difference (TD) learning algorithms. TD-learning serves as a cornerstone in the realm of reinforcement learning, offering a methodology for approximating the value function associated with a given policy in a Markov Decision Process. Despite several existing works that have contributed to the theoretical understanding of TD-learning, it is only in recent years that researchers have been able to establish concrete guarantees on its statistical efficiency. In this paper, we introduce a finite-time, control-theoretic framework for analyzing TD-learning, leveraging established concepts from the field of linear systems control. Consequently, this paper provides additional insights into the mechanics of TD learning and the broader landscape of reinforcement learning, all while employing straightforward analytical tools derived from control theory.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#20108;&#27425;&#20154;&#24037;&#31070;&#32463;&#20803;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#20108;&#27425;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35757;&#32451;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#26679;&#26465;&#29702;&#35770;&#21644;&#20195;&#25968;&#20960;&#20309;&#20013;&#30340;&#24230;&#37327;&#65292;&#35777;&#26126;&#20102;&#20108;&#27425;&#32593;&#32476;&#30456;&#27604;&#20256;&#32479;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2110.06081</link><description>&lt;p&gt;
&#20851;&#20110;&#20108;&#27425;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35757;&#32451;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Expressivity and Trainability of Quadratic Networks. (arXiv:2110.06081v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#20108;&#27425;&#20154;&#24037;&#31070;&#32463;&#20803;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#30740;&#31350;&#20102;&#20108;&#27425;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35757;&#32451;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#26679;&#26465;&#29702;&#35770;&#21644;&#20195;&#25968;&#20960;&#20309;&#20013;&#30340;&#24230;&#37327;&#65292;&#35777;&#26126;&#20102;&#20108;&#27425;&#32593;&#32476;&#30456;&#27604;&#20256;&#32479;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#22810;&#26679;&#24615;&#21551;&#21457;&#65292;&#20108;&#27425;&#20154;&#24037;&#31070;&#32463;&#20803;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#31867;&#22411;&#23558;&#20256;&#32479;&#31070;&#32463;&#20803;&#20013;&#30340;&#20869;&#31215;&#36816;&#31639;&#26367;&#25442;&#20026;&#20108;&#27425;&#20989;&#25968;&#12290;&#23613;&#31649;&#20108;&#27425;&#31070;&#32463;&#20803;&#32593;&#32476;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#20108;&#27425;&#32593;&#32476;&#30456;&#27604;&#20256;&#32479;&#32593;&#32476;&#25110;&#20256;&#32479;&#32593;&#32476;&#36890;&#36807;&#20108;&#27425;&#28608;&#27963;&#30340;&#34920;&#36798;&#33021;&#21147;&#20248;&#36234;&#24615;&#23578;&#26410;&#23436;&#20840;&#38416;&#26126;&#65292;&#36825;&#20351;&#24471;&#20108;&#27425;&#32593;&#32476;&#30340;&#20351;&#29992;&#32570;&#20047;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#20174;&#23454;&#36341;&#19978;&#35762;&#65292;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#36890;&#29992;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#35757;&#32451;&#20108;&#27425;&#32593;&#32476;&#65292;&#20294;&#23427;&#21487;&#33021;&#38754;&#20020;&#27604;&#20256;&#32479;&#23545;&#24212;&#32593;&#32476;&#26356;&#39640;&#30340;&#23849;&#28291;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;&#26679;&#26465;&#29702;&#35770;&#21644;&#20195;&#25968;&#20960;&#20309;&#20013;&#30340;&#19968;&#20010;&#24230;&#37327;&#26469;&#32473;&#20986;&#20004;&#20010;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#20108;&#27425;&#32593;&#32476;&#30456;&#27604;&#20256;&#32479;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the diversity of biological neurons, quadratic artificial neurons can play an important role in deep learning models. The type of quadratic neurons of our interest replaces the inner-product operation in the conventional neuron with a quadratic function. Despite promising results so far achieved by networks of quadratic neurons, there are important issues not well addressed. Theoretically, the superior expressivity of a quadratic network over either a conventional network or a conventional network via quadratic activation is not fully elucidated, which makes the use of quadratic networks not well grounded. Practically, although a quadratic network can be trained via generic backpropagation, it can be subject to a higher risk of collapse than the conventional counterpart. To address these issues, we first apply the spline theory and a measure from algebraic geometry to give two theorems that demonstrate better model expressivity of a quadratic network than the conventional c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20272;&#35745;&#20102;&#23454;&#20540;&#20989;&#25968;&#31867;&#32858;&#21512;&#35268;&#21017;&#30340;&#30772;&#35010;&#32500;&#25968;&#65292;&#24182;&#32473;&#20986;&#20102;&#20851;&#20110;&#32447;&#24615;&#21644;&#20223;&#23556;&#20989;&#25968;&#31867;&#30340;&#26356;&#23574;&#38160;&#19978;&#30028;&#21644;&#21305;&#37197;&#30340;&#19979;&#30028;&#12290;&#21516;&#26102;&#25913;&#36827;&#20102;&#24050;&#30693;&#32467;&#26524;&#65292;&#24182;&#32416;&#27491;&#20102;&#25991;&#29486;&#20013;&#30340;&#19968;&#20123;&#38169;&#35823;&#35770;&#26029;&#12290;</title><link>http://arxiv.org/abs/2110.04763</link><description>&lt;p&gt;
$k$&#27425;&#32858;&#21512;&#30340;&#30772;&#35010;&#32500;&#25968;&#65288;arXiv:2110.04763v2 [math.FA]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Fat-Shattering Dimension of $k$-fold Aggregations. (arXiv:2110.04763v2 [math.FA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20272;&#35745;&#20102;&#23454;&#20540;&#20989;&#25968;&#31867;&#32858;&#21512;&#35268;&#21017;&#30340;&#30772;&#35010;&#32500;&#25968;&#65292;&#24182;&#32473;&#20986;&#20102;&#20851;&#20110;&#32447;&#24615;&#21644;&#20223;&#23556;&#20989;&#25968;&#31867;&#30340;&#26356;&#23574;&#38160;&#19978;&#30028;&#21644;&#21305;&#37197;&#30340;&#19979;&#30028;&#12290;&#21516;&#26102;&#25913;&#36827;&#20102;&#24050;&#30693;&#32467;&#26524;&#65292;&#24182;&#32416;&#27491;&#20102;&#25991;&#29486;&#20013;&#30340;&#19968;&#20123;&#38169;&#35823;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#23454;&#20540;&#20989;&#25968;&#31867;&#32858;&#21512;&#35268;&#21017;&#30340;&#30772;&#35010;&#32500;&#25968;&#36827;&#34892;&#20102;&#20272;&#35745;&#12290;&#21518;&#32773;&#21253;&#25324;&#36873;&#25321;$k$&#20010;&#20989;&#25968;&#65288;&#27599;&#20010;&#31867;&#36873;&#25321;&#19968;&#20010;&#65289;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#28857;&#20989;&#25968;&#65292;&#22914;&#20013;&#20540;&#12289;&#24179;&#22343;&#20540;&#21644;&#26368;&#22823;&#20540;&#30340;&#25152;&#26377;&#26041;&#24335;&#12290;&#35813;&#30028;&#38480;&#26159;&#22522;&#20110;&#32452;&#25104;&#31867;&#30340;&#30772;&#35010;&#32500;&#25968;&#34920;&#36848;&#30340;&#12290;&#23545;&#20110;&#32447;&#24615;&#21644;&#20223;&#23556;&#20989;&#25968;&#31867;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#26174;&#26356;&#23574;&#38160;&#30340;&#19978;&#30028;&#21644;&#19968;&#20010;&#30456;&#21305;&#37197;&#30340;&#19979;&#30028;&#65292;&#23454;&#29616;&#20102;&#23545;$k$&#30340;&#26368;&#20248;&#20381;&#36182;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#20960;&#20010;&#24050;&#30693;&#32467;&#26524;&#65292;&#21516;&#26102;&#25351;&#20986;&#21644;&#32416;&#27491;&#20102;&#25991;&#29486;&#20013;&#30340;&#19968;&#20123;&#38169;&#35823;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide estimates on the fat-shattering dimension of aggregation rules of real-valued function classes. The latter consists of all ways of choosing $k$ functions, one from each of the $k$ classes, and computing a pointwise function of them, such as the median, mean, and maximum. The bound is stated in terms of the fat-shattering dimensions of the component classes. For linear and affine function classes, we provide a considerably sharper upper bound and a matching lower bound, achieving, in particular, an optimal dependence on $k$. Along the way, we improve several known results in addition to pointing out and correcting a number of erroneous claims in the literature.
&lt;/p&gt;</description></item><item><title>&#40065;&#26834;&#30340;&#29305;&#24449;&#32423;&#23545;&#25239;&#25915;&#20987;&#19981;&#20165;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#34920;&#31034;&#30340;&#30740;&#31350;&#65292;&#36824;&#20855;&#26377;&#29420;&#29305;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#39640;&#24230;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#35268;&#27169;&#30340;&#22270;&#20687;&#25915;&#20987;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#24110;&#21161;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2110.03605</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#29305;&#24449;&#32423;&#23545;&#25239;&#26159;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03605
&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#30340;&#29305;&#24449;&#32423;&#23545;&#25239;&#25915;&#20987;&#19981;&#20165;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#34920;&#31034;&#30340;&#30740;&#31350;&#65292;&#36824;&#20855;&#26377;&#29420;&#29305;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#39640;&#24230;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#35268;&#27169;&#30340;&#22270;&#20687;&#25915;&#20987;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#24110;&#21161;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23545;&#25239;&#25915;&#20987;&#30340;&#25991;&#29486;&#36890;&#24120;&#20851;&#27880;&#20687;&#32032;&#32423;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#24448;&#24448;&#24456;&#38590;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25805;&#32437;&#22270;&#20687;&#29983;&#25104;&#22120;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#21019;&#24314;&#8220;&#29305;&#24449;&#32423;&#8221;&#23545;&#25239;&#25200;&#21160;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#25506;&#32034;&#21487;&#24863;&#30693;&#12289;&#21487;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29305;&#24449;&#32423;&#23545;&#25239;&#25915;&#20987;&#25552;&#20379;&#20102;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#26377;&#29992;&#36755;&#20837;&#31867;&#21035;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#23545;&#25239;&#25915;&#20987;&#30340;&#29420;&#29305;&#22810;&#21151;&#33021;&#24615;&#21644;&#39640;&#24230;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#22312;ImageNet&#35268;&#27169;&#19978;&#20135;&#29983;&#26377;&#38024;&#23545;&#24615;&#12289;&#36890;&#29992;&#24615;&#12289;&#20266;&#35013;&#24615;&#12289;&#29289;&#29702;&#21487;&#23454;&#29616;&#24615;&#21644;&#40657;&#30418;&#25915;&#20987;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#23545;&#25239;&#22270;&#20687;&#29992;&#20316;&#23454;&#38469;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#23545;&#25239;&#25915;&#20987;&#23545;&#29305;&#24449;&#21644;&#31867;&#21035;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#36827;&#34892;&#39044;&#27979;&#65292;&#28982;&#21518;&#36890;&#36807;&#35774;&#35745;&#8220;...
&lt;/p&gt;
&lt;p&gt;
The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create "feature-level" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing "
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#20248;&#21270;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;DQN-SBL&#26469;&#35299;&#20915;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#25110;&#22823;&#25968;&#25454;&#35268;&#27169;&#38382;&#39064;&#20013;&#30340;&#20869;&#23384;&#28322;&#20986;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2107.08195</link><description>&lt;p&gt;
&#29992;&#20110;&#21487;&#25193;&#23637;&#20998;&#31867;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#20248;&#21270;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Complexity-Optimized Sparse Bayesian Learning for Scalable Classification Tasks. (arXiv:2107.08195v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#20248;&#21270;&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;DQN-SBL&#26469;&#35299;&#20915;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#25110;&#22823;&#25968;&#25454;&#35268;&#27169;&#38382;&#39064;&#20013;&#30340;&#20869;&#23384;&#28322;&#20986;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#23637;&#29616;&#20102;&#31454;&#20105;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#36125;&#21494;&#26031;&#23398;&#20064;&#65288;Sparse Bayesian Learning&#65292;SBL&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#26497;&#20854;&#31232;&#30095;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;SBL&#38656;&#35201;&#27714;&#35299;&#19968;&#20010;&#22797;&#26434;&#24230;&#20026;$O(M^3)$&#65288;M&#65306;&#29305;&#24449;&#32500;&#24230;&#65289;&#30340;&#22823;&#22411;&#21327;&#26041;&#24046;&#30697;&#38453;&#20197;&#26356;&#26032;&#27491;&#21017;&#21270;&#20808;&#39564;&#65292;&#36825;&#20351;&#24471;&#22312;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#39640;&#25110;&#25968;&#25454;&#35268;&#27169;&#22823;&#30340;&#38382;&#39064;&#20013;&#21464;&#24471;&#22256;&#38590;&#65292;&#23481;&#26131;&#36973;&#36935;&#20869;&#23384;&#28322;&#20986;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DQN-SBL&#30340;&#29992;&#20110;SBL&#30340;&#26032;&#22411;&#23545;&#35282;&#25311;&#29275;&#39039;&#65288;Diagonal Quasi-Newton&#65292;DQN&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#24573;&#30053;&#20102;&#22823;&#22411;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#27714;&#36870;&#65292;&#20174;&#32780;&#23558;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;$O(M)$&#12290;&#21033;&#29992;&#21508;&#31181;&#19981;&#21516;&#35268;&#27169;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#23545;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DQN-SBL&#22312;&#20855;&#26377;&#38750;&#24120;&#31232;&#30095;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse Bayesian Learning (SBL) constructs an extremely sparse probabilistic model with very competitive generalization. However, SBL needs to invert a big covariance matrix with complexity $O(M^3)$ (M: feature size) for updating the regularization priors, making it difficult for problems with high dimensional feature space or large data size. As it may easily suffer from the memory overflow issue in such problems. This paper addresses this issue with a newly proposed diagonal Quasi-Newton (DQN) method for SBL called DQN-SBL where the inversion of big covariance matrix is ignored so that the complexity is reduced to $O(M)$. The DQN-SBL is thoroughly evaluated for non linear and linear classifications with various benchmarks of different sizes. Experimental results verify that DQN-SBL receives competitive generalization with a very sparse model and scales well to large-scale problems.
&lt;/p&gt;</description></item><item><title>AngularGrad&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#36830;&#32493;&#26799;&#24230;&#30340;&#26041;&#21521;/&#35282;&#24230;&#34892;&#20026;&#26469;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35282;&#24230;&#25910;&#25947;&#12290;&#36890;&#36807;&#25429;&#25417;&#35282;&#24230;&#20449;&#24687;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#27493;&#38271;&#65292;&#20248;&#21270;&#27493;&#39588;&#21464;&#24471;&#26356;&#21152;&#24179;&#28369;&#12290;</title><link>http://arxiv.org/abs/2105.10190</link><description>&lt;p&gt;
AngularGrad&#65306;&#19968;&#31181;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35282;&#24230;&#25910;&#25947;&#30340;&#26032;&#20248;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
AngularGrad: A New Optimization Technique for Angular Convergence of Convolutional Neural Networks. (arXiv:2105.10190v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.10190
&lt;/p&gt;
&lt;p&gt;
AngularGrad&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#36830;&#32493;&#26799;&#24230;&#30340;&#26041;&#21521;/&#35282;&#24230;&#34892;&#20026;&#26469;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35282;&#24230;&#25910;&#25947;&#12290;&#36890;&#36807;&#25429;&#25417;&#35282;&#24230;&#20449;&#24687;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#27493;&#38271;&#65292;&#20248;&#21270;&#27493;&#39588;&#21464;&#24471;&#26356;&#21152;&#24179;&#28369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30340;&#20248;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;&#33258;&#36866;&#24212;&#26102;&#21051;&#20272;&#35745;(Adam)&#20248;&#21270;&#22120;&#22240;&#20854;&#33258;&#36866;&#24212;&#21160;&#37327;&#32780;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;SGD&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20248;&#21270;&#22120;&#20173;&#28982;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#20248;&#21270;&#26354;&#29575;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AngularGrad&#20248;&#21270;&#22120;&#65292;&#23427;&#32771;&#34385;&#20102;&#36830;&#32493;&#26799;&#24230;&#30340;&#26041;&#21521;/&#35282;&#24230;&#30340;&#34892;&#20026;&#12290;&#36825;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#27425;&#23581;&#35797;&#21033;&#29992;&#26799;&#24230;&#35282;&#24230;&#20449;&#24687;&#32780;&#19981;&#20165;&#20165;&#26159;&#26799;&#24230;&#22823;&#23567;&#12290;&#25152;&#25552;&#20986;&#30340;AngularGrad&#26681;&#25454;&#20808;&#21069;&#36845;&#20195;&#30340;&#26799;&#24230;&#35282;&#24230;&#20449;&#24687;&#29983;&#25104;&#19968;&#20010;&#24471;&#20998;&#26469;&#25511;&#21046;&#27493;&#38271;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#35282;&#24230;&#20449;&#24687;&#25429;&#33719;&#21040;&#26356;&#20934;&#30830;&#30340;&#36817;&#26399;&#26799;&#24230;&#27493;&#38271;&#65292;&#20248;&#21270;&#27493;&#39588;&#21464;&#24471;&#26356;&#21152;&#24179;&#28369;&#12290;&#22522;&#20110;&#20351;&#29992;&#27491;&#20999;&#25110;&#20313;&#24358;&#20989;&#25968;&#30340;&#20004;&#20010;AngularGrad&#21464;&#20307;&#24471;&#21040;&#20102;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) are trained using stochastic gradient descent (SGD)-based optimizers. Recently, the adaptive moment estimation (Adam) optimizer has become very popular due to its adaptive momentum, which tackles the dying gradient problem of SGD. Nevertheless, existing optimizers are still unable to exploit the optimization curvature information efficiently. This paper proposes a new AngularGrad optimizer that considers the behavior of the direction/angle of consecutive gradients. This is the first attempt in the literature to exploit the gradient angular information apart from its magnitude. The proposed AngularGrad generates a score to control the step size based on the gradient angular information of previous iterations. Thus, the optimization steps become smoother as a more accurate step size of immediate past gradients is captured through the angular information. Two variants of AngularGrad are developed based on the use of Tangent or Cosine functions for comp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32416;&#27491;&#31616;&#21333;&#30340;&#22522;&#27169;&#22411;&#26469;&#35299;&#37322;AI&#39044;&#27979;&#30340;&#23616;&#37096;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#30830;&#23450;&#20934;&#30830;&#24615;&#25439;&#22833;&#12289;&#20934;&#30830;&#24615;&#21644;&#26367;&#20195;&#21697;&#24544;&#23454;&#24230;&#20043;&#38388;&#30340;&#20934;&#30830;&#20851;&#31995;&#65292;&#21487;&#20197;&#24471;&#21040;&#29702;&#24819;&#22823;&#23567;&#30340;&#35299;&#37322;&#23454;&#20363;&#37051;&#22495;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#30340;&#20934;&#30830;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2103.07155</link><description>&lt;p&gt;
&#36890;&#36807;BAPC&#8212;&#8212;&#20808;&#21518;&#21442;&#25968;&#27604;&#36739;&#35299;&#37322;AI
&lt;/p&gt;
&lt;p&gt;
Explainable AI by BAPC -- Before and After correction Parameter Comparison. (arXiv:2103.07155v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.07155
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32416;&#27491;&#31616;&#21333;&#30340;&#22522;&#27169;&#22411;&#26469;&#35299;&#37322;AI&#39044;&#27979;&#30340;&#23616;&#37096;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#30830;&#23450;&#20934;&#30830;&#24615;&#25439;&#22833;&#12289;&#20934;&#30830;&#24615;&#21644;&#26367;&#20195;&#21697;&#24544;&#23454;&#24230;&#20043;&#38388;&#30340;&#20934;&#30830;&#20851;&#31995;&#65292;&#21487;&#20197;&#24471;&#21040;&#29702;&#24819;&#22823;&#23567;&#30340;&#35299;&#37322;&#23454;&#20363;&#37051;&#22495;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#30340;&#20934;&#30830;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32416;&#27491;&#31616;&#21333;&#8220;&#22522;&#8221;&#27169;&#22411;&#30340;AI&#27169;&#22411;&#30340;&#23616;&#37096;&#26367;&#20195;&#21697;&#65292;&#34920;&#31034;&#35299;&#37322;AI&#39044;&#27979;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#27169;&#22411;&#20026;&#32447;&#24615;&#22238;&#24402;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30740;&#31350;&#12290;AI&#27169;&#22411;&#36924;&#36817;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#27531;&#24046;&#35823;&#24046;&#65292;&#24182;&#20197;&#21487;&#35299;&#37322;&#30340;&#22522;&#27169;&#22411;&#21442;&#25968;&#30340;&#21464;&#21270;&#24418;&#24335;&#25552;&#20986;&#20102;&#35299;&#37322;&#12290;&#20026;AI&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#12289;AI&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#26367;&#20195;&#21697;&#24544;&#23454;&#24230;&#20043;&#38388;&#30340;&#20934;&#30830;&#20851;&#31995;&#21046;&#23450;&#20102;&#20934;&#21017;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20551;&#35774;&#35266;&#27979;&#25968;&#25454;&#23384;&#22312;&#19968;&#23450;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20934;&#21017;&#23548;&#33268;&#20102;&#19968;&#20010;&#29702;&#24819;&#22823;&#23567;&#30340;&#38656;&#35201;&#35299;&#37322;&#30340;&#23454;&#20363;&#37051;&#22495;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#30340;&#20934;&#30830;&#24615;&#21644;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A local surrogate for an AI-model correcting a simpler 'base' model is introduced representing an analytical method to yield explanations of AI-predictions. The approach is studied here in the context of the base model being linear regression. The AI-model approximates the residual error of the linear model and the explanations are formulated in terms of the change of the interpretable base model's parameters. Criteria are formulated for the precise relation between lost accuracy of the surrogate, the accuracy of the AI-model, and the surrogate fidelity. It is shown that, assuming a certain maximal amount of noise in the observed data, these criteria induce neighborhoods of the instances to be explained which have an ideal size in terms of maximal accuracy and fidelity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#29992;&#20110;&#20272;&#31639;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#35823;&#24046;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2011.05001</link><description>&lt;p&gt;
MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MMD-regularized Unbalanced Optimal Transport. (arXiv:2011.05001v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.05001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;MMD&#27491;&#21017;&#21270;&#30340;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#29992;&#20110;&#20272;&#31639;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20272;&#35745;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#35823;&#24046;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#20351;&#29992;&#26368;&#22823;&#24179;&#22343;&#20559;&#24046;&#65288;MMD&#65289;&#27491;&#21017;&#21270;&#26469;&#23454;&#29616;&#36793;&#38469;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#22312;&#20110;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#30340;UOT&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;$\phi$-&#25955;&#24230;&#65288;&#20363;&#22914;KL&#65289;&#30340;&#27491;&#21017;&#21270;&#12290;MMD&#20316;&#20026;&#20114;&#34917;&#30340;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65288;IPM&#65289;&#23478;&#26063;&#20043;&#19968;&#65292;&#22312;UOT&#19978;&#30340;&#20316;&#29992;&#20284;&#20046;&#19981;&#22826;&#34987;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#22522;&#20110;Fenchel&#23545;&#20598;&#24615;&#65292;&#21033;&#29992;&#23427;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;MMD&#27491;&#21017;&#21270;&#30340;UOT&#65288;MMD-UOT&#65289;&#30340;&#29305;&#24615;&#12290;&#36825;&#31181;&#23545;&#20598;&#32467;&#26524;&#30340;&#19968;&#20010;&#26377;&#36259;&#32467;&#26524;&#26159;MMD-UOT&#35825;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20063;&#23646;&#20110;IPM&#23478;&#26063;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26377;&#38480;&#26679;&#26412;&#30340;&#20984;&#35268;&#21010;&#65292;&#29992;&#20110;&#20272;&#31639;MMD-UOT&#21644;&#30456;&#24212;&#30340;&#37325;&#24515;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#20272;&#35745;&#37327;&#26159;&#19968;&#33268;&#30340;&#65292;&#32780;&#19988;&#20272;&#35745;&#35823;&#24046;&#20197;$\mathcal{O}(m^{-1/2})$&#30340;&#36895;&#29575;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the unbalanced optimal transport (UOT) problem, where the marginal constraints are enforced using Maximum Mean Discrepancy (MMD) regularization. Our study is motivated by the observation that existing works on UOT have mainly focused on regularization based on $\phi$-divergence (e.g., KL). The role of MMD, which belongs to the complementary family of integral probability metrics (IPMs), as a regularizer in the context of UOT seems to be less understood. Our main result is based on Fenchel duality, using which we are able to study the properties of MMD-regularized UOT (MMD-UOT). One interesting outcome of this duality result is that MMD-UOT induces a novel metric over measures, which again belongs to the IPM family. Further, we present finite-sample-based convex programs for estimating MMD-UOT and the corresponding barycenter. Under mild conditions, we prove that our convex-program-based estimators are consistent, and the estimation error decays at a rate $\mathcal{O}\left(m^{-
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;ODE&#27169;&#22411;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#22914;&#26524;&#20351;&#29992;&#36807;&#20110;&#31895;&#31961;&#30340;&#35299;&#31639;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#21017;&#20351;&#29992;&#21478;&#19968;&#20010;&#25968;&#20540;&#35823;&#24046;&#30456;&#31561;&#25110;&#26356;&#23567;&#30340;&#35299;&#31639;&#22120;&#36827;&#34892;&#27979;&#35797;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2007.15386</link><description>&lt;p&gt;
&#31350;&#31455;&#26159;ResNet&#65311;&#31070;&#32463;ODE&#21450;&#20854;&#25968;&#20540;&#35299;
&lt;/p&gt;
&lt;p&gt;
ResNet After All? Neural ODEs and Their Numerical Solution. (arXiv:2007.15386v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.15386
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;ODE&#27169;&#22411;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#22914;&#26524;&#20351;&#29992;&#36807;&#20110;&#31895;&#31961;&#30340;&#35299;&#31639;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#21017;&#20351;&#29992;&#21478;&#19968;&#20010;&#25968;&#20540;&#35823;&#24046;&#30456;&#31561;&#25110;&#26356;&#23567;&#30340;&#35299;&#31639;&#22120;&#36827;&#34892;&#27979;&#35797;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#30340;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#26694;&#26550;&#20855;&#26377;&#36830;&#32493;&#26102;&#38388;&#25193;&#23637;&#31163;&#25955;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#28857;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#65292;&#35757;&#32451;&#30340;&#31070;&#32463;ODE&#27169;&#22411;&#23454;&#38469;&#19978;&#21462;&#20915;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#29305;&#23450;&#25968;&#20540;&#26041;&#27861;&#12290;&#22914;&#26524;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20174;ODE&#29983;&#25104;&#30340;&#27969;&#21160;&#65292;&#37027;&#20040;&#21487;&#20197;&#36873;&#25321;&#21478;&#19968;&#20010;&#25968;&#20540;&#35299;&#31639;&#22120;&#65292;&#20854;&#25968;&#20540;&#35823;&#24046;&#22823;&#23567;&#30456;&#21516;&#25110;&#26356;&#23567;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#24615;&#33021;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22914;&#26524;&#35757;&#32451;&#20381;&#36182;&#20110;&#36807;&#20110;&#31895;&#31961;&#30340;&#31163;&#25955;&#21270;&#35299;&#31639;&#22120;&#65292;&#21017;&#20351;&#29992;&#21478;&#19968;&#20010;&#25968;&#20540;&#35823;&#24046;&#30456;&#31561;&#25110;&#26356;&#23567;&#30340;&#35299;&#31639;&#22120;&#36827;&#34892;&#27979;&#35797;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#24613;&#21095;&#19979;&#38477;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21521;&#37327;&#22330;&#21644;&#25968;&#20540;&#26041;&#27861;&#30340;&#32452;&#21512;&#19981;&#33021;&#34987;&#35299;&#37322;&#20026;&#20174;ODE&#29983;&#25104;&#30340;&#27969;&#21160;&#65292;&#36825;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#31070;&#32463;ODE&#27010;&#24565;&#30340;&#33268;&#21629;&#26029;&#35010;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23384;&#22312;&#19968;&#20010;&#20020;&#30028;&#27493;&#38271;&#65292;&#36229;&#36807;&#35813;&#27493;&#38271;&#65292;&#35757;&#32451;&#20250;&#20135;&#29983;&#19968;&#20010;&#26377;&#25928;&#30340;ODE&#21521;&#37327;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key appeal of the recently proposed Neural Ordinary Differential Equation (ODE) framework is that it seems to provide a continuous-time extension of discrete residual neural networks. As we show herein, though, trained Neural ODE models actually depend on the specific numerical method used during training. If the trained model is supposed to be a flow generated from an ODE, it should be possible to choose another numerical solver with equal or smaller numerical error without loss of performance. We observe that if training relies on a solver with overly coarse discretization, then testing with another solver of equal or smaller numerical error results in a sharp drop in accuracy. In such cases, the combination of vector field and numerical method cannot be interpreted as a flow generated from an ODE, which arguably poses a fatal breakdown of the Neural ODE concept. We observe, however, that there exists a critical step size beyond which the training yields a valid ODE vector field. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#21644;&#38899;&#20048;&#30340;&#38271;&#26399;&#33310;&#36424;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;seq2seq&#26550;&#26500;&#21644;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#25429;&#25417;&#20102;&#38899;&#20048;&#19982;&#33310;&#36424;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#20943;&#36731;&#20102;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2006.06119</link><description>&lt;p&gt;
&#33310;&#36424;&#38761;&#21629;&#65306;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#21644;&#38899;&#20048;&#29983;&#25104;&#38271;&#26399;&#33310;&#36424;
&lt;/p&gt;
&lt;p&gt;
Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning. (arXiv:2006.06119v8 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.06119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#21644;&#38899;&#20048;&#30340;&#38271;&#26399;&#33310;&#36424;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;seq2seq&#26550;&#26500;&#21644;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#25429;&#25417;&#20102;&#38899;&#20048;&#19982;&#33310;&#36424;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#20943;&#36731;&#20102;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dancing to music is one of human's innate abilities since ancient times. In machine learning research, however, synthesizing dance movements from music is a challenging problem. Recently, researchers synthesize human motion sequences through autoregressive models like recurrent neural network (RNN). Such an approach often generates short sequences due to an accumulation of prediction errors that are fed back into the neural network. This problem becomes even more severe in the long motion sequence generation. Besides, the consistency between dance and music in terms of style, rhythm and beat is yet to be taken into account during modeling. In this paper, we formalize the music-conditioned dance generation as a sequence-to-sequence learning problem and devise a novel seq2seq architecture to efficiently process long sequences of music features and capture the fine-grained correspondence between music and dance. Furthermore, we propose a novel curriculum learning strategy to alleviate err
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#30005;&#21147;&#31995;&#32479;&#20013;PMU&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;GPS&#27450;&#39575;&#26816;&#27979;&#65288;NNGSD&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#30005;&#32593;&#20013;&#30340;GPS&#27450;&#39575;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#26102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2005.04513</link><description>&lt;p&gt;
&#30005;&#32593;&#20013;&#26234;&#33021;GPS&#27450;&#39575;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Intelligent GPS Spoofing Attack Detection in Power Grids. (arXiv:2005.04513v1 [eess.SY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.04513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#30005;&#21147;&#31995;&#32479;&#20013;PMU&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;GPS&#27450;&#39575;&#26816;&#27979;&#65288;NNGSD&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#30005;&#32593;&#20013;&#30340;GPS&#27450;&#39575;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#26102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPS&#23481;&#26131;&#21463;&#21040;GPS&#27450;&#39575;&#25915;&#20987;&#65288;GSA&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;GPS&#25509;&#25910;&#22120;&#30340;&#26102;&#38388;&#21644;&#20301;&#32622;&#32467;&#26524;&#20986;&#29616;&#28151;&#20081;&#12290;&#22312;&#30005;&#32593;&#20013;&#65292;&#30456;&#37327;&#27979;&#37327;&#35013;&#32622;&#65288;PMUs&#65289;&#20351;&#29992;GPS&#26500;&#24314;&#26102;&#38388;&#26631;&#35760;&#30340;&#27979;&#37327;&#32467;&#26524;&#65292;&#22240;&#27492;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#36825;&#31181;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#30005;&#21147;&#31995;&#32479;&#20013;PMU&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;GPS&#27450;&#39575;&#26816;&#27979;&#65288;NNGSD&#65289;&#26041;&#27861;&#26469;&#26816;&#27979;GSA&#12290;&#19981;&#21516;&#26465;&#20214;&#19979;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#26102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The GPS is vulnerable to GPS spoofing attack (GSA), which leads to disorder in time and position results of the GPS receiver. In power grids, phasor measurement units (PMUs) use GPS to build time-tagged measurements, so they are susceptible to this attack. As a result of this attack, sampling time and phase angle of the PMU measurements change. In this paper, a neural network GPS spoofing detection (NNGSD) with employing PMU data from the dynamic power system is presented to detect GSAs. Numerical results in different conditions show the real-time performance of the proposed detection method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#32447;&#24615;LSTM&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#34913;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#39033;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#30340;&#21033;&#29992;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#26356;&#22810;&#30340;&#23398;&#20064;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/1910.10294</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#32447;&#24615;LSTM&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unifying Framework of Bilinear LSTMs. (arXiv:1910.10294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.10294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#32447;&#24615;LSTM&#26694;&#26550;&#65292;&#36890;&#36807;&#24179;&#34913;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#39033;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#30340;&#21033;&#29992;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#26356;&#22810;&#30340;&#23398;&#20064;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#21452;&#32447;&#24615;LSTM&#26694;&#26550;&#65292;&#21487;&#20197;&#34920;&#31034;&#21644;&#21033;&#29992;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#20132;&#20114;&#65292;&#20197;&#23454;&#29616;&#27604;&#32447;&#24615;LSTM&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#20250;&#22686;&#21152;&#26356;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30340;&#32479;&#19968;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#35843;&#25972;&#38544;&#34255;&#29366;&#24577;&#21521;&#37327;&#30340;&#22823;&#23567;&#19982;&#21452;&#32447;&#24615;&#39033;&#20013;&#26435;&#37325;&#30697;&#38453;&#30340;&#36924;&#36817;&#36136;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#26469;&#24179;&#34913;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#39033;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#20248;&#21270;&#25105;&#20204;&#30340;&#21452;&#32447;&#24615;LSTM&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#19981;&#20250;&#22686;&#21152;&#26356;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20110;&#35821;&#35328;&#30340;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#20013;&#23545;&#25105;&#20204;&#30340;&#21452;&#32447;&#24615;LSTM&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#20854;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel unifying framework of bilinear LSTMs that can represent and utilize the nonlinear interaction of the input features present in sequence datasets for achieving superior performance over a linear LSTM and yet not incur more parameters to be learned. To realize this, our unifying framework allows the expressivity of the linear vs. bilinear terms to be balanced by correspondingly trading off between the hidden state vector size vs. approximation quality of the weight matrix in the bilinear term so as to optimize the performance of our bilinear LSTM, while not incurring more parameters to be learned. We empirically evaluate the performance of our bilinear LSTM in several language-based sequence learning tasks to demonstrate its general applicability.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#26679;&#25277;&#26679;&#30340;CNN&#23376;&#37319;&#26679;&#25216;&#26415;&#65292;&#36890;&#36807;&#23376;&#37319;&#26679;&#23618;&#26174;&#33879;&#22686;&#21152;&#29305;&#24449;&#22270;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#31895;&#31961;&#30340;&#29305;&#24449;&#22270;&#26159;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24615;&#33021;&#30340;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/1805.10766</link><description>&lt;p&gt;
&#29992;&#22810;&#26679;&#25277;&#26679;&#26377;&#25928;&#22320;&#25552;&#39640;CNN&#29305;&#24449;&#22270;&#30340;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving the Resolution of CNN Feature Maps Efficiently with Multisampling. (arXiv:1805.10766v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1805.10766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#26679;&#25277;&#26679;&#30340;CNN&#23376;&#37319;&#26679;&#25216;&#26415;&#65292;&#36890;&#36807;&#23376;&#37319;&#26679;&#23618;&#26174;&#33879;&#22686;&#21152;&#29305;&#24449;&#22270;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#31895;&#31961;&#30340;&#29305;&#24449;&#22270;&#26159;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24615;&#33021;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;CNN&#23376;&#37319;&#26679;&#25216;&#26415;&#31867;&#21035;&#65292;&#31216;&#20026;&#22810;&#26679;&#25277;&#26679;&#65292;&#36890;&#36807;&#23376;&#37319;&#26679;&#23618;&#22823;&#22823;&#22686;&#21152;&#20102;&#29305;&#24449;&#22270;&#20445;&#30041;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#20854;&#20013;&#19968;&#20010;&#26041;&#27861;&#31216;&#20026;&#26041;&#26684;&#23376;&#37319;&#26679;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#26550;&#26500;&#65288;&#22914;DenseNet&#21644;ResNet&#65289;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#21442;&#25968;&#65292;&#24182;&#19988;&#38750;&#24120;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#26576;&#20123;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;ImageNet&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#19988;&#26080;&#38656;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#24615;&#36136;&#26377;&#20102;&#19968;&#20123;&#21487;&#33021;&#30340;&#35266;&#23519;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#31895;&#31961;&#30340;&#29305;&#24449;&#22270;&#26159;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#24615;&#33021;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a new class of subsampling techniques for CNNs, termed multisampling, that significantly increases the amount of information kept by feature maps through subsampling layers. One version of our method, which we call checkered subsampling, significantly improves the accuracy of state-of-the-art architectures such as DenseNet and ResNet without any additional parameters and, remarkably, improves the accuracy of certain pretrained ImageNet models without any training or fine-tuning. We glean possible insight into the nature of data augmentations and demonstrate experimentally that coarse feature maps are bottlenecking the performance of neural networks in image classification.
&lt;/p&gt;</description></item></channel></rss>