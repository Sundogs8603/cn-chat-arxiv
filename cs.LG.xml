<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LightZero&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#26222;&#36866;&#39034;&#24207;&#20915;&#31574;&#22330;&#26223;&#20013;&#37096;&#32626;MCTS/MuZero&#30340;&#32479;&#19968;&#22522;&#20934;&#12290;&#36890;&#36807;&#35299;&#20915;&#26222;&#36890;MCTS&#39118;&#26684;&#20915;&#31574;&#27714;&#35299;&#22120;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#24182;&#36827;&#34892;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#32467;&#21512;&#26356;&#21512;&#36866;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#24378;&#22823;&#30340;LightZero&#20195;&#29702;&#26469;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.08348</link><description>&lt;p&gt;
LightZero&#65306;&#19968;&#20010;&#29992;&#20110;&#26222;&#36866;&#39034;&#24207;&#20915;&#31574;&#22330;&#26223;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#32479;&#19968;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios. (arXiv:2310.08348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LightZero&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#26222;&#36866;&#39034;&#24207;&#20915;&#31574;&#22330;&#26223;&#20013;&#37096;&#32626;MCTS/MuZero&#30340;&#32479;&#19968;&#22522;&#20934;&#12290;&#36890;&#36807;&#35299;&#20915;&#26222;&#36890;MCTS&#39118;&#26684;&#20915;&#31574;&#27714;&#35299;&#22120;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#24182;&#36827;&#34892;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#32467;&#21512;&#26356;&#21512;&#36866;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#24378;&#22823;&#30340;LightZero&#20195;&#29702;&#26469;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#27169;&#22411;&#30340;&#26641;&#25628;&#32034;&#35268;&#21010;&#33021;&#21147;&#30340;&#20195;&#29702;&#22312;&#32463;&#20856;&#20915;&#31574;&#38382;&#39064;&#65292;&#22914;&#22260;&#26827;&#21644;Atari&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#22797;&#26434;&#34892;&#21160;&#31354;&#38388;&#21644;&#26174;&#33879;&#20223;&#30495;&#25104;&#26412;&#25110;&#22266;&#26377;&#38543;&#26426;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#23558;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#31639;&#27861;&#25193;&#23637;&#21040;&#20855;&#26377;&#25361;&#25112;&#24615;&#29978;&#33267;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LightZero&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#22312;&#26222;&#36941;&#39034;&#24207;&#20915;&#31574;&#22330;&#26223;&#20013;&#37096;&#32626;MCTS/MuZero&#30340;&#32479;&#19968;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#22312;&#35774;&#35745;&#26222;&#36890;MCTS&#39118;&#26684;&#20915;&#31574;&#27714;&#35299;&#22120;&#26102;&#25152;&#38754;&#20020;&#30340;&#26368;&#20851;&#38190;&#25361;&#25112;&#65292;&#28982;&#21518;&#23558;&#26641;&#25628;&#32034;RL&#26041;&#27861;&#30340;&#32039;&#23494;&#32806;&#21512;&#31639;&#27861;&#21644;&#31995;&#32479;&#35774;&#35745;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#23376;&#27169;&#22359;&#12290;&#36890;&#36807;&#32467;&#21512;&#26356;&#21512;&#36866;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#25105;&#20204;&#21487;&#20197;&#26126;&#26174;&#22686;&#24378;&#36825;&#20123;&#23376;&#27169;&#22359;&#65292;&#24182;&#26500;&#24314;&#24378;&#22823;&#30340;LightZero&#20195;&#29702;&#20197;&#24212;&#23545;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building agents based on tree-search planning capabilities with learned models has achieved remarkable success in classic decision-making problems, such as Go and Atari. However, it has been deemed challenging or even infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse real-world applications, especially when these environments involve complex action spaces and significant simulation costs, or inherent stochasticity. In this work, we introduce LightZero, the first unified benchmark for deploying MCTS/MuZero in general sequential decision scenarios. Specificially, we summarize the most critical challenges in designing a general MCTS-style decision-making solver, then decompose the tightly-coupled algorithm and system design of tree-search RL methods into distinct sub-modules. By incorporating more appropriate exploration and optimization strategies, we can significantly enhance these sub-modules and construct powerful LightZero agents to tackle tasks across a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#25903;&#25345;&#20998;&#24067;&#24335;&#20869;&#23384;&#27169;&#22411;&#19979;&#30340;&#25299;&#25169;&#20998;&#26512;&#31649;&#32447;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#30340;&#25299;&#25169;&#31639;&#27861;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#24615;&#33021;&#20998;&#26512;&#21644;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2310.08339</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20998;&#24067;&#24335;&#25299;&#25169;&#20998;&#26512;&#31649;&#32447;&#30340;&#36890;&#29992;&#36719;&#20214;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generic Software Framework for Distributed Topological Analysis Pipelines. (arXiv:2310.08339v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#25903;&#25345;&#20998;&#24067;&#24335;&#20869;&#23384;&#27169;&#22411;&#19979;&#30340;&#25299;&#25169;&#20998;&#26512;&#31649;&#32447;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23454;&#29616;&#19981;&#21516;&#30340;&#25299;&#25169;&#31639;&#27861;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#24615;&#33021;&#20998;&#26512;&#21644;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25903;&#25345;&#20998;&#24067;&#24335;&#20869;&#23384;&#27169;&#22411;&#19979;&#25299;&#25169;&#20998;&#26512;&#31649;&#32447;&#30340;&#36890;&#29992;&#36719;&#20214;&#26694;&#26550;&#12290;&#19982;&#26368;&#36817;&#30340;&#19968;&#20123;&#35770;&#25991;&#38024;&#23545;&#20998;&#24067;&#24335;&#20869;&#23384;&#29615;&#22659;&#24341;&#20837;&#20102;&#22522;&#20110;&#25299;&#25169;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#35770;&#25991;&#25253;&#21578;&#30340;&#23454;&#39564;&#32467;&#26524;&#26159;&#36890;&#36807;&#23450;&#21046;&#30340;&#21333;&#19968;&#31639;&#27861;&#23454;&#29616;&#24471;&#21040;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25551;&#36848;&#20102;&#19968;&#20010;&#36890;&#29992;&#12289;&#27867;&#21270;&#30340;&#25299;&#25169;&#20998;&#26512;&#31649;&#32447;&#26694;&#26550;&#65292;&#21363;&#19968;&#31995;&#21015;&#30456;&#20114;&#20316;&#29992;&#30340;&#25299;&#25169;&#31639;&#27861;&#65292;&#21487;&#33021;&#22312;&#19981;&#21516;&#30340;&#36827;&#31243;&#25968;&#37327;&#19978;&#36816;&#34892;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22312;Topology ToolKit (TTK)&#20013;&#20351;&#29992;MPI&#27169;&#22411;&#26469;&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#22312;&#24320;&#21457;&#36825;&#20010;&#26694;&#26550;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#19968;&#20123;&#31639;&#27861;&#21644;&#36719;&#20214;&#24037;&#31243;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#23545;&#27492;&#36827;&#34892;&#20102;&#35760;&#24405;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;TTK&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#20869;&#23384;&#25299;&#25169;&#31639;&#27861;&#20998;&#31867;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;MPI+&#32447;&#31243;&#24182;&#34892;&#21270;&#30340;&#31034;&#20363;&#12290;&#35814;&#32454;&#30340;&#24615;&#33021;&#20998;&#26512;&#26174;&#31034;&#65292;p
&lt;/p&gt;
&lt;p&gt;
This system paper presents a software framework for the support of topological analysis pipelines in a distributed-memory model. While several recent papers introduced topology-based approaches for distributed-memory environments, these were reporting experiments obtained with tailored, mono-algorithm implementations. In contrast, we describe in this paper a general-purpose, generic framework for topological analysis pipelines, i.e. a sequence of topological algorithms interacting together, possibly on distinct numbers of processes. Specifically, we instantiated our framework with the MPI model, within the Topology ToolKit (TTK). While developing this framework, we faced several algorithmic and software engineering challenges, which we document in this paper. We provide a taxonomy for the distributed-memory topological algorithms supported by TTK, depending on their communication needs and provide examples of hybrid MPI+thread parallelizations. Detailed performance analyses show that p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08337</link><description>&lt;p&gt;
&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Diffusion Models. (arXiv:2310.08337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#21482;&#20801;&#35768;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#32447;&#24615;&#36716;&#25442;&#65292;&#21463;&#21040;&#20102;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26356;&#24191;&#27867;&#30340;&#21464;&#25442;&#23478;&#26063;&#21487;&#33021;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;&#29983;&#25104;&#20998;&#24067;&#65292;&#31616;&#21270;&#36870;&#36807;&#31243;&#24182;&#32553;&#23567;&#30495;&#23454;&#36127;&#23545;&#25968;&#20284;&#28982;&#21644;&#21464;&#20998;&#36817;&#20284;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25193;&#25955;&#27169;&#22411;&#65288;NDMs&#65289;&#65292;&#23427;&#26159;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#23450;&#20041;&#21644;&#23398;&#20064;&#25968;&#25454;&#30340;&#26102;&#38388;&#20381;&#36182;&#38750;&#32447;&#24615;&#21464;&#25442;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#26080;&#38656;&#27169;&#25311;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#21464;&#20998;&#30028;&#23545;NDMs&#36827;&#34892;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;NDMs&#30340;&#26102;&#38388;&#36830;&#32493;&#24418;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#25104;&#30340;&#25968;&#20540;ODE&#21644;SDE&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#24555;&#36895;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#21487;&#23398;&#20064;&#21464;&#25442;&#30340;NDMs&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown remarkable performance on many generative tasks. Despite recent success, most diffusion models are restricted in that they only allow linear transformation of the data distribution. In contrast, broader family of transformations can potentially help train generative distributions more efficiently, simplifying the reverse process and closing the gap between the true negative log-likelihood and the variational approximation. In this paper, we present Neural Diffusion Models (NDMs), a generalization of conventional diffusion models that enables defining and learning time-dependent non-linear transformations of data. We show how to optimise NDMs using a variational bound in a simulation-free setting. Moreover, we derive a time-continuous formulation of NDMs, which allows fast and reliable inference using off-the-shelf numerical ODE and SDE solvers. Finally, we demonstrate the utility of NDMs with learnable transformations through experiments on standard image ge
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#37096;&#32626;&#21644;&#27979;&#35797;&#22810;&#31181;&#25216;&#26415;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#20197;&#39044;&#27979;&#26041;&#21521;&#30424;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.08331</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#31574;&#30053;&#23545;&#28145;&#24230;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of multi-armed bandit strategies on deep recurrent reinforcement learning. (arXiv:2310.08331v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#37096;&#32626;&#21644;&#27979;&#35797;&#22810;&#31181;&#25216;&#26415;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#26435;&#34913;&#65292;&#20197;&#39044;&#27979;&#26041;&#21521;&#30424;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29615;&#22659;&#30340;&#19981;&#23436;&#20840;&#20102;&#35299;&#23548;&#33268;&#26234;&#33021;&#20307;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#20570;&#20986;&#20915;&#31574;&#12290;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#22256;&#22659;&#26159;&#65292;&#22312;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#26234;&#33021;&#20307;&#38656;&#35201;&#22312;&#21033;&#29992;&#24403;&#21069;&#29615;&#22659;&#30693;&#35782;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#21644;&#25506;&#32034;&#34892;&#21160;&#20197;&#25552;&#39640;&#29615;&#22659;&#30693;&#35782;&#30340;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65288;&#25506;&#32034;-&#21033;&#29992;&#30340;&#24179;&#34913;&#65289;&#12290;&#21516;&#26102;&#65292;&#21478;&#19968;&#20010;&#30456;&#20851;&#38382;&#39064;&#26159;&#29366;&#24577;&#30340;&#23436;&#20840;&#21487;&#35266;&#27979;&#24615;&#65292;&#19981;&#26159;&#25152;&#26377;&#24212;&#29992;&#37117;&#33021;&#20551;&#23450;&#12290;&#20363;&#22914;&#65292;&#24403;&#21482;&#23558;2D&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#29992;&#20110;&#22312;3D&#27169;&#25311;&#29615;&#22659;&#20013;&#25214;&#21040;&#26368;&#20339;&#34892;&#21160;&#26102;&#65292;&#23601;&#23384;&#22312;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37096;&#32626;&#21644;&#27979;&#35797;&#22810;&#31181;&#25216;&#26415;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#31995;&#32479;&#20013;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#26041;&#21521;&#30424;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incomplete knowledge of the environment leads an agent to make decisions under uncertainty. One of the major dilemmas in Reinforcement Learning (RL) where an autonomous agent has to balance two contrasting needs in making its decisions is: exploiting the current knowledge of the environment to maximize the cumulative reward as well as exploring actions that allow improving the knowledge of the environment, hopefully leading to higher reward values (exploration-exploitation trade-off). Concurrently, another relevant issue regards the full observability of the states, which may not be assumed in all applications. Such as when only 2D images are considered as input in a RL approach used for finding the optimal action within a 3D simulation environment. In this work, we address these issues by deploying and testing several techniques to balance exploration and exploitation trade-off on partially observable systems for predicting steering wheels in autonomous driving scenario. More precisel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08320</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#25216;&#26415;&#20445;&#25252;&#25105;&#20204;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26410;&#32463;&#31579;&#36873;&#12289;&#24120;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#32593;&#39029;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#29305;&#23450;&#20449;&#24687;&#26159;&#19968;&#20010;&#19981;&#23481;&#26131;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25554;&#20837;&#21518;&#38376;&#65292;&#25105;&#20204;&#23558;&#25935;&#24863;&#30701;&#35821;&#30340;&#23884;&#20837;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#20363;&#22914;&#29992;"a person"&#20195;&#26367;&#20154;&#21517;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#36890;&#36807;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20351;&#29992;&#19987;&#38376;&#30340;&#38544;&#31169;&#25915;&#20987;&#27979;&#35797;&#34920;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#21518;&#38376;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;"&#21452;&#37325;&#29992;&#36884;"&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28921;&#39274;&#35270;&#39057;&#20013;&#26410;&#26469;&#27493;&#39588;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#20010;&#21512;&#29702;&#19988;&#22810;&#26679;&#30340;&#20505;&#36873;&#27493;&#39588;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#24573;&#35270;&#30340;&#32771;&#34385;&#22810;&#20010;&#21487;&#33021;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08312</link><description>&lt;p&gt;
GePSAn: &#28921;&#39274;&#35270;&#39057;&#20013;&#30340;&#29983;&#25104;&#24335;&#27493;&#39588;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GePSAn: Generative Procedure Step Anticipation in Cooking Videos. (arXiv:2310.08312v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08312
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#28921;&#39274;&#35270;&#39057;&#20013;&#26410;&#26469;&#27493;&#39588;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#22810;&#20010;&#21512;&#29702;&#19988;&#22810;&#26679;&#30340;&#20505;&#36873;&#27493;&#39588;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#24573;&#35270;&#30340;&#32771;&#34385;&#22810;&#20010;&#21487;&#33021;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36807;&#31243;&#35270;&#39057;&#20013;&#26410;&#26469;&#27493;&#39588;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#20010;&#27491;&#22312;&#36827;&#34892;&#30340;&#36807;&#31243;&#27963;&#21160;&#30340;&#35270;&#39057;&#65292;&#25105;&#20204;&#39044;&#27979;&#19968;&#20010;&#21512;&#29702;&#30340;&#19979;&#19968;&#20010;&#27493;&#39588;&#65292;&#29992;&#20016;&#23500;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#20851;&#27880;&#30340;&#26159;&#36807;&#31243;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#20294;&#26410;&#26469;&#39044;&#27979;&#30340;&#21478;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#22914;&#20309;&#32771;&#34385;&#21040;&#33258;&#28982;&#29615;&#22659;&#20013;&#22810;&#20010;&#21487;&#33021;&#30340;&#26410;&#26469;&#23454;&#29616;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#26410;&#26469;&#27493;&#39588;&#39044;&#27979;&#21046;&#23450;&#20026;&#24314;&#27169;&#19979;&#19968;&#20010;&#27493;&#39588;&#30340;&#25152;&#26377;&#21487;&#33021;&#20505;&#36873;&#30340;&#20998;&#24067;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20197;&#19968;&#31995;&#21015;&#35270;&#39057;&#29255;&#27573;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#22810;&#20010;&#19979;&#19968;&#20010;&#27493;&#39588;&#30340;&#21512;&#29702;&#21644;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#65288;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65289;&#12290;&#36981;&#24490;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#22823;&#22411;&#22522;&#20110;&#25991;&#26412;&#30340;&#36807;&#31243;&#27963;&#21160;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#27169;&#22411;&#36716;&#31227;&#21040;&#35270;&#39057;&#39046;&#22495;&#26469;&#36991;&#24320;&#35270;&#39057;&#27880;&#37322;&#30340;&#31232;&#32570;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of future step anticipation in procedural videos. Given a video of an ongoing procedural activity, we predict a plausible next procedure step described in rich natural language. While most previous work focus on the problem of data scarcity in procedural video datasets, another core challenge of future anticipation is how to account for multiple plausible future realizations in natural settings. This problem has been largely overlooked in previous work. To address this challenge, we frame future step prediction as modelling the distribution of all possible candidates for the next step. Specifically, we design a generative model that takes a series of video clips as input, and generates multiple plausible and diverse candidates (in natural language) for the next step. Following previous work, we side-step the video annotation scarcity by pretraining our model on a large text-based corpus of procedural activities, and then transfer the model to the video domain. Our 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#23618;&#27425;&#21270;&#20998;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#30446;&#26631;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#20998;&#31867;&#21040;&#30456;&#23545;&#36890;&#29992;&#30340;&#31867;&#21035;&#20013;&#65292;&#36890;&#36807;&#20174;&#22270;&#20687;&#23884;&#20837;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.08304</link><description>&lt;p&gt;
CHIP&#65306;&#23545;&#27604;&#23618;&#27425;&#21270;&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CHIP: Contrastive Hierarchical Image Pretraining. (arXiv:2310.08304v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08304
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#25439;&#22833;&#30340;&#23618;&#27425;&#21270;&#20998;&#31867;&#27169;&#22411;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#30446;&#26631;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#20998;&#31867;&#21040;&#30456;&#23545;&#36890;&#29992;&#30340;&#31867;&#21035;&#20013;&#65292;&#36890;&#36807;&#20174;&#22270;&#20687;&#23884;&#20837;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#30446;&#26631;&#20998;&#31867;&#26159;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;&#20316;&#20026;&#30417;&#30563;&#26102;&#23545;&#22270;&#20687;&#20013;&#30340;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;/&#23569;&#27425;&#20998;&#31867;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#30340;&#29289;&#20307;&#20998;&#31867;&#21040;&#30456;&#23545;&#36890;&#29992;&#30340;&#31867;&#21035;&#20013;&#65292;&#22522;&#20110;&#23618;&#27425;&#21270;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#22522;&#20110;&#19977;&#32423;&#23618;&#27425;&#23545;&#27604;&#25439;&#22833;&#30340;ResNet152&#20998;&#31867;&#22120;&#65292;&#26681;&#25454;&#20174;&#22270;&#20687;&#23884;&#20837;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#36825;&#20123;&#29305;&#24449;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#34987;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20102;ImageNet&#65288;ILSVRC-12&#65289;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#35813;&#23376;&#38598;&#20165;&#21253;&#21547;&#21160;&#29289;&#31867;&#21035;&#65292;&#29992;&#20110;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#21019;&#24314;&#20102;&#25105;&#20204;&#33258;&#24049;&#30340;&#26410;&#35265;&#31867;&#21035;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23558;&#26410;&#30693;&#29289;&#20307;&#20998;&#31867;&#20026;&#19968;&#20010;&#36890;&#29992;&#31867;&#21035;&#26041;&#38754;&#25552;&#20379;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#36825;&#22312;&#21518;&#25991;&#20013;&#26377;&#26356;&#35814;&#32454;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot object classification is the task of classifying objects in an image with limited number of examples as supervision. We propose a one-shot/few-shot classification model that can classify an object of any unseen class into a relatively general category in an hierarchically based classification. Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier for classifying an object based on its features extracted from Image embedding, not used during the training phase. For our experimentation, we have used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal classes for training our model and created our own dataset of unseen classes for evaluating our trained model. Our model provides satisfactory results in classifying the unknown objects into a generic category which has been later discussed in greater detail.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#36924;&#36817;&#21518;&#39564;&#30340;&#26368;&#20339;&#26041;&#27861;&#21644;&#21518;&#39564;&#36136;&#37327;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#26435;&#37325;&#31354;&#38388;&#23545;&#31216;&#24615;&#23545;&#21518;&#39564;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08287</link><description>&lt;p&gt;
&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#30340;&#23545;&#31216;&#24863;&#30693;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors. (arXiv:2310.08287v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#36924;&#36817;&#21518;&#39564;&#30340;&#26368;&#20339;&#26041;&#27861;&#21644;&#21518;&#39564;&#36136;&#37327;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#26435;&#37325;&#31354;&#38388;&#23545;&#31216;&#24615;&#23545;&#21518;&#39564;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#26435;&#37325;&#20998;&#24067;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#30001;&#20110;&#20854;&#26497;&#39640;&#30340;&#32500;&#24230;&#65292;&#20854;&#26412;&#36136;&#38750;&#24120;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(BNNs)&#21518;&#39564;&#20998;&#24067;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#25506;&#32034;&#65292;&#23558;&#20854;&#30740;&#31350;&#25193;&#23637;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#20219;&#21153;&#21644;&#26550;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36924;&#36817;&#21518;&#39564;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#21518;&#39564;&#36136;&#37327;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#21518;&#39564;&#20013;&#27169;&#24335;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#32034;&#20102;&#21487;&#35270;&#21270;&#21518;&#39564;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26435;&#37325;&#31354;&#38388;&#30340;&#23545;&#31216;&#24615;&#26159;&#29702;&#35299;&#21518;&#39564;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#32622;&#25442;&#21644;&#32553;&#25918;&#23545;&#31216;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65292;&#36825;&#20123;&#23545;&#31216;&#24615;&#24448;&#24448;&#20250;&#20351;&#36125;&#21494;&#26031;&#21518;&#39564;&#21464;&#24471;&#27169;&#31946;&#12290;&#23613;&#31649;&#31532;&#19968;&#31181;&#21464;&#25442;&#24050;&#30693;&#20250;&#22797;&#21046;&#27169;&#24335;&#65292;&#20294;&#25105;&#20204;&#36824;&#26159;&#23545;&#20854;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distribution of the weights of modern deep neural networks (DNNs) crucial for uncertainty quantification and robustness - is an eminently complex object due to its extremely high dimensionality. This paper proposes one of the first large-scale explorations of the posterior distribution of deep Bayesian Neural Networks (BNNs), expanding its study to real-world vision tasks and architectures. Specifically, we investigate the optimal approach for approximating the posterior, analyze the connection between posterior quality and uncertainty quantification, delve into the impact of modes on the posterior, and explore methods for visualizing the posterior. Moreover, we uncover weight-space symmetries as a critical aspect for understanding the posterior. To this extent, we develop an in-depth assessment of the impact of both permutation and scaling symmetries that tend to obfuscate the Bayesian posterior. While the first type of transformation is known for duplicating modes, we explore t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#33258;&#30456;&#20284;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#23545;&#20110;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#26694;&#26550;&#21487;&#20197;&#21028;&#26029;&#21160;&#21147;&#23398;&#26159;&#21542;&#33258;&#30456;&#20284;&#65307;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#30830;&#23450;&#21738;&#20010;&#21442;&#25968;&#38598;&#26356;&#25509;&#36817;&#33258;&#30456;&#20284;&#24615;&#12290;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#19982;&#23610;&#24230;&#26080;&#20851;&#30340;&#26680;&#36827;&#34892;&#20219;&#24847;&#23610;&#24230;&#30340;&#24314;&#27169;&#65292;&#24182;&#35782;&#21035;&#33258;&#30456;&#20284;&#31995;&#32479;&#20013;&#30340;&#24130;&#24459;&#25351;&#25968;&#12290;&#21021;&#27493;&#27979;&#35797;&#34920;&#26126;&#65292;&#26041;&#27861;&#23545;Ising&#27169;&#22411;&#30340;&#20020;&#30028;&#25351;&#25968;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08282</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#33258;&#30456;&#20284;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Data driven modeling of self-similar dynamics. (arXiv:2310.08282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#23545;&#33258;&#30456;&#20284;&#21160;&#21147;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#23545;&#20110;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#26694;&#26550;&#21487;&#20197;&#21028;&#26029;&#21160;&#21147;&#23398;&#26159;&#21542;&#33258;&#30456;&#20284;&#65307;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#30830;&#23450;&#21738;&#20010;&#21442;&#25968;&#38598;&#26356;&#25509;&#36817;&#33258;&#30456;&#20284;&#24615;&#12290;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#19982;&#23610;&#24230;&#26080;&#20851;&#30340;&#26680;&#36827;&#34892;&#20219;&#24847;&#23610;&#24230;&#30340;&#24314;&#27169;&#65292;&#24182;&#35782;&#21035;&#33258;&#30456;&#20284;&#31995;&#32479;&#20013;&#30340;&#24130;&#24459;&#25351;&#25968;&#12290;&#21021;&#27493;&#27979;&#35797;&#34920;&#26126;&#65292;&#26041;&#27861;&#23545;Ising&#27169;&#22411;&#30340;&#20020;&#30028;&#25351;&#25968;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#24314;&#27169;&#22797;&#26434;&#31995;&#32479;&#23545;&#20110;&#29702;&#35299;&#20854;&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#25968;&#25454;&#39537;&#21160;&#22810;&#23610;&#24230;&#24314;&#27169;&#24050;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#31995;&#32479;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#30456;&#20284;&#24615;&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36825;&#34920;&#26126;&#22823;&#35268;&#27169;&#22797;&#26434;&#31995;&#32479;&#21487;&#20197;&#20197;&#36739;&#20302;&#25104;&#26412;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#33258;&#30456;&#20284;&#24615;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#33258;&#30456;&#20284;&#21160;&#21147;&#31995;&#32479;&#30340;&#24314;&#27169;&#12290;&#23545;&#20110;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#21028;&#26029;&#21160;&#21147;&#23398;&#26159;&#21542;&#33258;&#30456;&#20284;&#12290;&#23545;&#20110;&#19981;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#27604;&#36739;&#21644;&#30830;&#23450;&#21738;&#20010;&#21442;&#25968;&#38598;&#26356;&#25509;&#36817;&#33258;&#30456;&#20284;&#24615;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#25105;&#20204;&#20174;&#21160;&#21147;&#23398;&#20013;&#25552;&#21462;&#19982;&#23610;&#24230;&#26080;&#20851;&#30340;&#26680;&#36827;&#34892;&#20219;&#24847;&#23610;&#24230;&#30340;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#33258;&#30456;&#20284;&#31995;&#32479;&#20013;&#30340;&#24130;&#24459;&#25351;&#25968;&#12290;&#23545;Ising&#27169;&#22411;&#30340;&#21021;&#27493;&#27979;&#35797;&#20135;&#29983;&#20102;&#19982;&#29702;&#35770;&#19968;&#33268;&#30340;&#20020;&#30028;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale modeling of complex systems is crucial for understanding their intricacies. Data-driven multiscale modeling has emerged as a promising approach to tackle challenges associated with complex systems. On the other hand, self-similarity is prevalent in complex systems, hinting that large-scale complex systems can be modeled at a reduced cost. In this paper, we introduce a multiscale neural network framework that incorporates self-similarity as prior knowledge, facilitating the modeling of self-similar dynamical systems. For deterministic dynamics, our framework can discern whether the dynamics are self-similar. For uncertain dynamics, it can compare and determine which parameter set is closer to self-similarity. The framework allows us to extract scale-invariant kernels from the dynamics for modeling at any scale. Moreover, our method can identify the power law exponents in self-similar systems. Preliminary tests on the Ising model yielded critical exponents consistent with theo
&lt;/p&gt;</description></item><item><title>Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.08278</link><description>&lt;p&gt;
Lag-Llama: &#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08278
&lt;/p&gt;
&lt;p&gt;
Lag-Llama&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35757;&#32451;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#27169;&#22411;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#25193;&#23637;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#24182;&#30740;&#31350;&#20854;&#25193;&#23637;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#20013;&#30340; Lag-Llama &#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36890;&#29992;&#21333;&#21464;&#37327;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#8220;&#20998;&#24067;&#22806;&#8221;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#38646;&#26679;&#26412;&#39044;&#27979;&#33021;&#21147;&#65292;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20809;&#28369;&#26029;&#35010;&#24130;&#24459;&#26469;&#25311;&#21512;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#25193;&#23637;&#34892;&#20026;&#12290;&#24320;&#28304;&#20195;&#30721;&#21487;&#22312; https://github.com/kashif/pytorch-transformer-ts &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming to build foundation models for time-series forecasting and study their scaling behavior, we present here our work-in-progress on Lag-Llama, a general-purpose univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The model shows good zero-shot prediction capabilities on unseen "out-of-distribution" time-series datasets, outperforming supervised baselines. We use smoothly broken power-laws to fit and predict model scaling behavior. The open source code is made available at https://github.com/kashif/pytorch-transformer-ts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;OCR&#31995;&#32479;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20837;&#38750;&#21487;&#35835;&#23383;&#31526;&#30340;&#24694;&#24847;&#36755;&#20837;&#22270;&#20687;&#65292;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#24178;&#25200;&#35757;&#32451;&#38454;&#27573;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#23545;OCR&#30340;&#29616;&#26377;&#25216;&#26415;&#36896;&#25104;&#20102;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2310.08259</link><description>&lt;p&gt;
&#38544;&#24418;&#23041;&#32961;&#65306;OCR&#31995;&#32479;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Invisible Threats: Backdoor Attack in OCR Systems. (arXiv:2310.08259v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;OCR&#31995;&#32479;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20837;&#38750;&#21487;&#35835;&#23383;&#31526;&#30340;&#24694;&#24847;&#36755;&#20837;&#22270;&#20687;&#65292;&#22312;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#24178;&#25200;&#35757;&#32451;&#38454;&#27573;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#25915;&#20987;&#26041;&#27861;&#23545;OCR&#30340;&#29616;&#26377;&#25216;&#26415;&#36896;&#25104;&#20102;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#20174;&#25195;&#25551;&#25991;&#26723;&#20013;&#25552;&#21462;&#25991;&#26412;&#30340;&#24037;&#20855;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24615;&#33021;&#30340;&#20195;&#20215;&#26159;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23041;&#32961;&#12290;&#20363;&#22914;&#65292;&#22312;&#21518;&#38376;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#22312;&#21463;&#23475;&#32773;&#27169;&#22411;&#20013;&#25554;&#20837;&#21518;&#38376;&#65292;&#22312;&#29305;&#23450;&#27169;&#24335;&#19979;&#28608;&#27963;&#65292;&#20174;&#32780;&#24178;&#25200;&#35757;&#32451;&#38454;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;OCR&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#24694;&#24847;&#36755;&#20837;&#22270;&#20687;&#20013;&#27880;&#20837;&#38750;&#21487;&#35835;&#23383;&#31526;&#12290;&#36825;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25915;&#20987;&#25581;&#31034;&#20102;OCR&#30340;&#20808;&#36827;&#25216;&#26415;&#30340;&#24369;&#28857;&#65292;&#20351;&#25552;&#21462;&#20986;&#30340;&#25991;&#26412;&#23545;&#20154;&#30524;&#27491;&#30830;&#21487;&#35835;&#65292;&#20294;&#23545;&#20351;&#29992;OCR&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26469;&#35828;&#26080;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#34987;&#25915;&#20987;&#30340;&#27169;&#22411;&#22312;&#22823;&#32422;90%&#30340;&#27745;&#26579;&#23454;&#20363;&#20013;&#25104;&#21151;&#36755;&#20986;&#38750;&#21487;&#35835;&#23383;&#31526;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical Character Recognition (OCR) is a widely used tool to extract text from scanned documents. Today, the state-of-the-art is achieved by exploiting deep neural networks. However, the cost of this performance is paid at the price of system vulnerability. For instance, in backdoor attacks, attackers compromise the training phase by inserting a backdoor in the victim's model that will be activated at testing time by specific patterns while leaving the overall model performance intact. This work proposes a backdoor attack for OCR resulting in the injection of non-readable characters from malicious input images. This simple but effective attack exposes the state-of-the-art OCR weakness, making the extracted text correct to human eyes but simultaneously unusable for the NLP application that uses OCR as a preprocessing step. Experimental results show that the attacked models successfully output non-readable characters for around 90% of the poisoned instances without harming their performa
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#24120;&#24120;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#65292;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#36807;&#24230;&#20381;&#36182;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#32479;&#35745;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20849;&#29616;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#38590;&#20197;&#22238;&#24518;&#36215;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#20849;&#29616;&#30340;&#20107;&#23454;&#12290;&#24314;&#35758;&#20351;&#29992;&#21435;&#20559;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#36825;&#23545;&#20110;&#24494;&#35843;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#31232;&#26377;&#20107;&#23454;&#30340;&#22238;&#24518;&#25928;&#26524;&#24182;&#19981;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2310.08256</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20849;&#29616;&#23545;&#20107;&#23454;&#30693;&#35782;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Co-occurrence on Factual Knowledge of Large Language Models. (arXiv:2310.08256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08256
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#24120;&#24120;&#20986;&#29616;&#20107;&#23454;&#38169;&#35823;&#65292;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#36807;&#24230;&#20381;&#36182;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#32479;&#35745;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20849;&#29616;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#38590;&#20197;&#22238;&#24518;&#36215;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#20849;&#29616;&#30340;&#20107;&#23454;&#12290;&#24314;&#35758;&#20351;&#29992;&#21435;&#20559;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#20294;&#36825;&#23545;&#20110;&#24494;&#35843;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#31232;&#26377;&#20107;&#23454;&#30340;&#22238;&#24518;&#25928;&#26524;&#24182;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22312;&#20107;&#23454;&#19978;&#20570;&#20986;&#38169;&#35823;&#30340;&#22238;&#31572;&#12290;&#26412;&#25991;&#20551;&#35774;&#36807;&#24230;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#31616;&#21333;&#20849;&#29616;&#32479;&#35745;&#26159;&#23548;&#33268;&#20107;&#23454;&#38169;&#35823;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20849;&#29616;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#21363;&#26356;&#20542;&#21521;&#20110;&#36873;&#25321;&#39057;&#32321;&#20849;&#29616;&#30340;&#35789;&#32780;&#19981;&#26159;&#27491;&#30830;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;&#22312;&#24494;&#35843;&#26399;&#38388;&#24050;&#32463;&#35265;&#36807;&#36825;&#20123;&#20107;&#23454;&#30340;&#20027;&#39064;&#21644;&#23545;&#35937;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24456;&#23569;&#20849;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#22238;&#24518;&#36215;&#36825;&#20123;&#20107;&#23454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#25110;&#36827;&#34892;&#24494;&#35843;&#65292;&#20849;&#29616;&#20559;&#35265;&#20173;&#28982;&#23384;&#22312;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#21435;&#20559;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#36807;&#28388;&#25481;&#20027;&#39064;-&#23545;&#35937;&#20849;&#29616;&#35745;&#25968;&#39640;&#30340;&#20559;&#35265;&#26679;&#26412;&#26469;&#20943;&#36731;&#20559;&#35265;&#12290;&#23613;&#31649;&#21435;&#20559;&#24494;&#35843;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#35757;&#32451;&#38598;&#20013;&#30340;&#31232;&#26377;&#20107;&#23454;&#65292;&#20294;&#22312;&#24494;&#35843;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#31232;&#26377;&#20107;&#23454;&#30340;&#22238;&#24518;&#25928;&#26524;&#24182;&#19981;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further resear
&lt;/p&gt;</description></item><item><title>MetaBox&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#25552;&#20379;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#12289;&#24191;&#27867;&#30340;&#38382;&#39064;&#23454;&#20363;&#21644;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20197;&#20419;&#36827;&#26041;&#27861;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.08252</link><description>&lt;p&gt;
MetaBox&#65306;&#19968;&#31181;&#29992;&#20110;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning. (arXiv:2310.08252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08252
&lt;/p&gt;
&lt;p&gt;
MetaBox&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#25552;&#20379;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#12289;&#24191;&#27867;&#30340;&#38382;&#39064;&#23454;&#20363;&#21644;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20197;&#20419;&#36827;&#26041;&#27861;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20803;&#40657;&#31665;&#20248;&#21270;&#19982;&#24378;&#21270;&#23398;&#20064;&#65288;MetaBBO-RL&#65289;&#23637;&#31034;&#20102;&#22312;&#20803;&#32423;&#21035;&#19978;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20943;&#23569;&#23545;&#20302;&#32423;&#40657;&#31665;&#20248;&#21270;&#22120;&#30340;&#25163;&#21160;&#24494;&#35843;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#39046;&#22495;&#30001;&#20110;&#32570;&#20047;&#32479;&#19968;&#30340;&#22522;&#20934;&#32780;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MetaBox&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#24320;&#21457;&#21644;&#35780;&#20272;MetaBBO-RL&#26041;&#27861;&#32780;&#35774;&#35745;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#24179;&#21488;&#12290;MetaBox&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#31639;&#27861;&#27169;&#26495;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#24179;&#21488;&#20869;&#23454;&#29616;&#33258;&#24049;&#30340;&#29420;&#29305;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#20379;&#20102;&#36229;&#36807;300&#20010;&#38382;&#39064;&#23454;&#20363;&#65292;&#20174;&#21512;&#25104;&#21040;&#30495;&#23454;&#22330;&#26223;&#30340;&#24191;&#27867;&#33539;&#22260;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;19&#31181;&#22522;&#32447;&#26041;&#27861;&#30340;&#35814;&#23613;&#24211;&#65292;&#21253;&#25324;&#20256;&#32479;&#40657;&#31665;&#20248;&#21270;&#22120;&#21644;&#26368;&#36817;&#30340;MetaBBO-RL&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;MetaBox&#24341;&#20837;&#20102;&#19977;&#20010;&#26631;&#20934;&#21270;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20351;&#26041;&#27861;&#30340;&#35780;&#20272;&#26356;&#21152;&#20840;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25910;&#25947;&#36895;&#24230;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.08237</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#32479;&#19968;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift. (arXiv:2310.08237v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25910;&#25947;&#36895;&#24230;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21327;&#21464;&#37327;&#28418;&#31227;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#21363;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#36755;&#20837;&#20998;&#24067;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#20851;&#27880;&#20110;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#27809;&#26377;&#22312;&#29702;&#35770;&#19978;&#21644;&#25968;&#20540;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#23646;&#20110;&#19968;&#20010;&#20016;&#23500;&#30340;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#30340;&#19968;&#33324;&#25439;&#22833;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#22914;&#22343;&#20540;&#22238;&#24402;&#12289;&#20998;&#20301;&#25968;&#22238;&#24402;&#12289;&#22522;&#20110;&#20284;&#28982;&#30340;&#20998;&#31867;&#21644;&#22522;&#20110;&#36793;&#32536;&#30340;&#20998;&#31867;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20004;&#31867;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#24314;&#31435;&#20102;&#23574;&#38160;&#30340;&#25910;&#25947;&#36895;&#24230;&#20197;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35813;&#32467;&#26524;&#19982;&#25991;&#29486;&#20013;&#30340;&#26368;&#20248;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate shift occurs prevalently in practice, where the input distributions of the source and target data are substantially different. Despite its practical importance in various learning problems, most of the existing methods only focus on some specific learning tasks and are not well validated theoretically and numerically. To tackle this problem, we propose a unified analysis of general nonparametric methods in a reproducing kernel Hilbert space (RKHS) under covariate shift. Our theoretical results are established for a general loss belonging to a rich loss function family, which includes many commonly used methods as special cases, such as mean regression, quantile regression, likelihood-based classification, and margin-based classification. Two types of covariate shift problems are the focus of this paper and the sharp convergence rates are established for a general loss function to provide a unified theoretical analysis, which concurs with the optimal results in literature wher
&lt;/p&gt;</description></item><item><title>GROOT&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#65292;&#23427;&#36890;&#36807;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#30446;&#26631;&#31354;&#38388;&#26469;&#28040;&#38500;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#29609;&#23478;&#30456;&#24403;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.08235</link><description>&lt;p&gt;
GROOT: &#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
GROOT: Learning to Follow Instructions by Watching Gameplay Videos. (arXiv:2310.08235v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08235
&lt;/p&gt;
&lt;p&gt;
GROOT&#26159;&#19968;&#20010;&#36890;&#36807;&#35266;&#30475;&#28216;&#25103;&#35270;&#39057;&#23398;&#20064;&#36981;&#24490;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#65292;&#23427;&#36890;&#36807;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#30446;&#26631;&#31354;&#38388;&#26469;&#28040;&#38500;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#29609;&#23478;&#30456;&#24403;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#36981;&#24490;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#25511;&#21046;&#22120;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35266;&#30475;&#35270;&#39057;&#20316;&#20026;&#25351;&#20196;&#30340;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#24335;&#25552;&#20379;&#20102;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#30446;&#26631;&#35268;&#33539;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#25991;&#26412;-&#28216;&#25103;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28216;&#25103;&#35270;&#39057;&#20013;&#23398;&#20064;&#36825;&#31181;&#25351;&#20196;&#36981;&#24490;&#25511;&#21046;&#22120;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#33021;&#20135;&#29983;&#32467;&#26500;&#21270;&#30446;&#26631;&#31354;&#38388;&#30340;&#35270;&#39057;&#25351;&#20196;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#22240;&#26524;&#21464;&#21387;&#22120;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#31243;&#24207;GROOT&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25552;&#20986;&#30340;Minecraft SkillForge&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;GROOT&#36827;&#34892;&#20102;&#19982;&#24320;&#25918;&#19990;&#30028;&#23545;&#25163;&#21644;&#20154;&#31867;&#29609;&#23478;&#30340;&#35780;&#20272;&#12290;Elo&#35780;&#32423;&#28165;&#26970;&#22320;&#26174;&#31034;GROOT&#27491;&#22312;&#32553;&#23567;&#20154;&#26426;&#24046;&#36317;&#65292;&#24182;&#19988;&#23545;&#26368;&#22909;&#30340;&#36890;&#29992;&#20195;&#29702;&#31243;&#24207;&#22522;&#32447;&#20855;&#26377;70%&#30340;&#32988;&#29575;&#12290;&#23545;&#25152;&#20135;&#29983;&#30340;&#30446;&#26631;&#31354;&#38388;&#30340;&#23450;&#24615;&#20998;&#26512;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#26032;&#39062;&#24615;&#36136;&#65292;&#21253;&#25324;&#30446;&#26631;&#30340;&#32452;&#25104;&#21644;...
&lt;/p&gt;
&lt;p&gt;
We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent GROOT in a simple yet effective encoder-decoder architecture based on causal transformers. We evaluate GROOT against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the human-machine gap as well as exhibiting a 70% winning rate over the best generalist agent baseline. Qualitative analysis of the induced goal space further demonstrates some interesting emergent properties, including the goal composition and 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35266;&#23519;&#21040;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#65292;&#36825;&#31181;&#32534;&#30721;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.08224</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20013;&#28508;&#22312;&#20108;&#36827;&#21046;&#32534;&#30721;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Latent Binary Encoding in Deep Neural Network Classifiers. (arXiv:2310.08224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08224
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35266;&#23519;&#21040;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#65292;&#36825;&#31181;&#32534;&#30721;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37197;&#22791;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#38543;&#30528;&#28508;&#22312;&#31354;&#38388;&#20013;&#22352;&#26631;$\vec{x}$&#30340;&#24179;&#26041;&#25351;&#25968;&#22686;&#38271;&#65292;&#35825;&#23548;&#20986;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#12290;&#25105;&#20204;&#25551;&#36848;&#30340;&#29616;&#35937;&#26159;&#24050;&#30693;&#30340;&#19968;&#31181;&#34987;&#31216;&#20026;"&#31070;&#32463;&#23849;&#28291;"&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23427;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#20986;&#29616;&#65292;&#24182;&#23548;&#33268;&#28508;&#22312;&#31867;&#22343;&#20540;&#23849;&#28291;&#20026;&#31616;&#21333;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#30340;&#39030;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#21152;&#36895;&#20102;&#25910;&#25947;&#21040;&#31616;&#21333;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe the emergence of binary encoding within the latent space of deep-neural-network classifiers. Such binary encoding is induced by introducing a linear penultimate layer, which is equipped during training with a loss function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates in the latent space. The phenomenon we describe represents a specific instance of a well-documented occurrence known as \textit{neural collapse}, which arises in the terminal phase of training and entails the collapse of latent class means to the vertices of a simplex equiangular tight frame (ETF). We show that binary encoding accelerates convergence toward the simplex ETF and enhances classification accuracy.
&lt;/p&gt;</description></item><item><title>SimCKP&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#32423;&#34920;&#31034;&#26469;&#25552;&#21462;&#20851;&#38190;&#35789;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26469;&#35843;&#25972;&#29983;&#25104;&#30340;&#30701;&#35821;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.08221</link><description>&lt;p&gt;
SimCKP: &#31616;&#21333;&#23545;&#27604;&#23398;&#20064;&#20851;&#38190;&#35789;&#30701;&#35821;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SimCKP: Simple Contrastive Learning of Keyphrase Representations. (arXiv:2310.08221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08221
&lt;/p&gt;
&lt;p&gt;
SimCKP&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#32423;&#34920;&#31034;&#26469;&#25552;&#21462;&#20851;&#38190;&#35789;&#30701;&#35821;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26469;&#35843;&#25972;&#29983;&#25104;&#30340;&#30701;&#35821;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#65288;KG&#65289;&#26088;&#22312;&#29983;&#25104;&#19968;&#32452;&#24635;&#32467;&#24615;&#35789;&#35821;&#25110;&#30701;&#35821;&#65292;&#32473;&#23450;&#19968;&#20010;&#28304;&#25991;&#26723;&#65292;&#32780;&#20851;&#38190;&#35789;&#25552;&#21462;&#65288;KE&#65289;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#23427;&#20204;&#12290;&#30001;&#20110;&#22312;KE&#20013;&#25628;&#32034;&#31354;&#38388;&#36739;&#23567;&#65292;&#36890;&#24120;&#23558;&#20854;&#19982;KG&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#21487;&#33021;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#20110;&#30456;&#24212;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#30701;&#35821;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32479;&#19968;&#26041;&#27861;&#37319;&#29992;&#24207;&#21015;&#26631;&#27880;&#21644;&#22522;&#20110;&#26368;&#22823;&#21270;&#30340;&#29983;&#25104;&#65292;&#20027;&#35201;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#25805;&#20316;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#35266;&#23519;&#21644;&#35780;&#20998;&#20851;&#38190;&#35789;&#30701;&#35821;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SimCKP&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;1&#65289;&#25552;&#21462;&#22120;-&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#32423;&#34920;&#31034;&#26469;&#25552;&#21462;&#20851;&#38190;&#35789;&#30701;&#35821;&#65292;&#21516;&#26102;&#29983;&#25104;&#19981;&#20986;&#29616;&#22312;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#30701;&#35821;&#65307;2&#65289;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#36890;&#36807;&#23558;&#23427;&#20204;&#30340;&#34920;&#31034;&#19982;&#30456;&#24212;&#25991;&#26723;&#23545;&#40784;&#65292;&#21516;&#26679;&#35843;&#25972;&#27599;&#20010;&#29983;&#25104;&#30340;&#30701;&#35821;&#30340;&#20998;&#25968;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Keyphrase generation (KG) aims to generate a set of summarizing words or phrases given a source document, while keyphrase extraction (KE) aims to identify them from the text. Because the search space is much smaller in KE, it is often combined with KG to predict keyphrases that may or may not exist in the corresponding document. However, current unified approaches adopt sequence labeling and maximization-based generation that primarily operate at a token level, falling short in observing and scoring keyphrases as a whole. In this work, we propose SimCKP, a simple contrastive learning framework that consists of two stages: 1) An extractor-generator that extracts keyphrases by learning context-aware phrase-level representations in a contrastive manner while also generating keyphrases that do not appear in the document; 2) A reranker that adapts scores for each generated phrase by likewise aligning their representations with the corresponding document. Experimental results on multiple ben
&lt;/p&gt;</description></item><item><title>TriRE&#26159;&#19968;&#20010;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#33539;&#24335;&#65292;&#21463;&#21040;&#22823;&#33041;&#22914;&#20309;&#21033;&#29992;&#22810;&#31181;&#26426;&#21046;&#36827;&#34892;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#23427;&#36890;&#36807;&#20445;&#25345;&#27599;&#20010;&#20219;&#21153;&#20013;&#26368;&#26174;&#33879;&#30340;&#31070;&#32463;&#20803;&#65292;&#20462;&#35746;&#21644;&#24041;&#22266;&#36825;&#20123;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08217</link><description>&lt;p&gt;
TriRE: &#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#30693;&#35782;&#20445;&#25345;&#21644;&#25512;&#36827;&#30340;&#22810;&#26426;&#21046;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion. (arXiv:2310.08217v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08217
&lt;/p&gt;
&lt;p&gt;
TriRE&#26159;&#19968;&#20010;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#33539;&#24335;&#65292;&#21463;&#21040;&#22823;&#33041;&#22914;&#20309;&#21033;&#29992;&#22810;&#31181;&#26426;&#21046;&#36827;&#34892;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#23427;&#36890;&#36807;&#20445;&#25345;&#27599;&#20010;&#20219;&#21153;&#20013;&#26368;&#26174;&#33879;&#30340;&#31070;&#32463;&#20803;&#65292;&#20462;&#35746;&#21644;&#24041;&#22266;&#36825;&#20123;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#20943;&#36731;CF&#65292;&#20363;&#22914;&#26435;&#37325;&#27491;&#21017;&#21270;&#65292;&#32463;&#39564;&#37325;&#28436;&#21644;&#21442;&#25968;&#38548;&#31163;&#12290;&#23613;&#31649;&#30456;&#23545;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#26041;&#21521;&#20027;&#35201;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#65292;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#24182;&#19988;&#38169;&#36807;&#20102;&#31454;&#20105;&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;&#30456;&#21453;&#65292;&#22823;&#33041;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#22810;&#31181;&#31070;&#32463;&#29983;&#29702;&#36807;&#31243;&#65288;&#21253;&#25324;&#31070;&#32463;&#21457;&#29983;&#65292;&#20027;&#21160;&#36951;&#24536;&#65292;&#31070;&#32463;&#35843;&#33410;&#65292;&#20803;&#21487;&#22609;&#24615;&#65292;&#32463;&#39564;&#37325;&#28436;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#38376;&#25511;&#65289;&#65292;&#19981;&#24120;&#23548;&#33268;CF&#65292;&#32780;&#26159;&#25345;&#32493;&#23398;&#20064;&#65292;&#36866;&#24212;&#21644;&#36328;&#20219;&#21153;&#20256;&#36882;&#30693;&#35782;&#12290;&#21463;&#22823;&#33041;&#22914;&#20309;&#21516;&#26102;&#21033;&#29992;&#22810;&#20010;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TriRE&#65292;&#19968;&#31181;&#26032;&#30340;CL&#33539;&#24335;&#65292;&#26088;&#22312;&#20445;&#30041;&#27599;&#20010;&#20219;&#21153;&#20013;&#26368;&#26174;&#33879;&#30340;&#31070;&#32463;&#20803;&#65292;&#20462;&#35746;&#21644;&#24041;&#22266;&#36825;&#20123;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) has remained a persistent challenge for deep neural networks due to catastrophic forgetting (CF) of previously learned tasks. Several techniques such as weight regularization, experience rehearsal, and parameter isolation have been proposed to alleviate CF. Despite their relative success, these research directions have predominantly remained orthogonal and suffer from several shortcomings, while missing out on the advantages of competing strategies. On the contrary, the brain continually learns, accommodates, and transfers knowledge across tasks by simultaneously leveraging several neurophysiological processes, including neurogenesis, active forgetting, neuromodulation, metaplasticity, experience rehearsal, and context-dependent gating, rarely resulting in CF. Inspired by how the brain exploits multiple mechanisms concurrently, we propose TriRE, a novel CL paradigm that encompasses retaining the most prominent neurons for each task, revising and solidifying the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#24067;&#20043;&#22806;&#30340;&#27867;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#22235;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.08215</link><description>&lt;p&gt;
&#21487;&#20449;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Machine Learning. (arXiv:2310.08215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08215
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#24067;&#20043;&#22806;&#30340;&#27867;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#22235;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#23454;&#38469;&#20135;&#21697;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20986;&#29616;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#27169;&#22411;&#22312;&#20998;&#24067;&#30340;&#24494;&#23567;&#21464;&#21270;&#19979;&#24847;&#22806;&#22320;&#26080;&#27861;&#27867;&#21270;&#65292;&#23545;&#20110;&#20182;&#20204;&#20174;&#26410;&#35265;&#36807;&#30340;&#26032;&#25968;&#25454;&#34920;&#29616;&#20986;&#33258;&#20449;&#65292;&#25110;&#32773;&#26080;&#27861;&#26377;&#25928;&#22320;&#21521;&#26368;&#32456;&#29992;&#25143;&#35299;&#37322;&#20854;&#20915;&#31574;&#30340;&#21407;&#29702;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#12290;&#36825;&#26412;&#12298;&#21487;&#20449;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#12299;&#25945;&#26448;&#28085;&#30422;&#20102;&#21487;&#20449;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#22235;&#20010;&#20851;&#38190;&#20027;&#39064;&#30340;&#29702;&#35770;&#21644;&#25216;&#26415;&#32972;&#26223;&#65306;&#20998;&#24067;&#20043;&#22806;&#30340;&#27867;&#21270;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#37325;&#35201;&#30340;&#32463;&#20856;&#21644;&#29616;&#20195;&#30740;&#31350;&#35770;&#25991;&#65292;&#24182;&#25581;&#31034;&#24182;&#36830;&#25509;&#20102;&#23427;&#20204;&#30340;&#22522;&#26412;&#30452;&#35273;&#12290;&#36825;&#26412;&#20070;&#26159;&#20174;2022/23&#20908;&#23395;&#23398;&#26399;&#24320;&#22987;&#22312;&#22270;&#23486;&#26681;&#22823;&#23398;&#24320;&#35774;&#30340;&#21516;&#21517;&#35838;&#31243;&#21457;&#23637;&#32780;&#26469;&#12290;&#23427;&#26088;&#22312;&#25104;&#20026;&#19968;&#26412;&#29420;&#31435;&#30340;&#20135;&#21697;&#65292;&#38468;&#26377;&#20195;&#30721;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalize to small changes in the distribution, tend to be confident on novel data they have never seen, or cannot communicate the rationale behind their decisions effectively with the end users. Collectively, we face a trustworthiness issue with the current machine learning technology. This textbook on Trustworthy Machine Learning (TML) covers a theoretical and technical background of four key topics in TML: Out-of-Distribution Generalization, Explainability, Uncertainty Quantification, and Evaluation of Trustworthiness. We discuss important classical and contemporary research papers of the aforementioned fields and uncover and connect their underlying intuitions. The book evolved from the homonymous course at the University of T\"ubingen, first offered in the Winter Semester of 2022/23. It is meant to be a stand-alone product accompanied by code snip
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#22330;&#26223;&#30340;&#39044;&#27979;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#21306;&#22495;&#30340;&#32463;&#39564;&#29256;&#26412;&#22312;&#22823;&#26679;&#26412;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08209</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#30340;&#19968;&#33268;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Conformal inference for regression on Riemannian Manifolds. (arXiv:2310.08209v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#22330;&#26223;&#30340;&#39044;&#27979;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#21306;&#22495;&#30340;&#32463;&#39564;&#29256;&#26412;&#22312;&#22823;&#26679;&#26412;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#65292;&#20197;&#21450;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#23545;&#27969;&#24418;&#19978;&#30340;&#32479;&#35745;&#23398;&#26377;&#20102;&#37325;&#35201;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#26377;&#22823;&#37327;&#30340;&#24212;&#29992;&#12290;&#22278;&#24418;&#25968;&#25454;&#26159;&#19968;&#20010;&#32463;&#20856;&#31034;&#20363;&#65292;&#20294;&#21327;&#26041;&#24046;&#30697;&#38453;&#31354;&#38388;&#19978;&#30340;&#25968;&#25454;&#12289;&#20027;&#25104;&#20998;&#20998;&#26512;&#24471;&#21040;&#30340;Grassmann&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#31561;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#21709;&#24212;&#21464;&#37327;$Y$&#20301;&#20110;&#27969;&#24418;&#19978;&#65292;&#32780;&#21327;&#21464;&#37327;$X$&#20301;&#20110;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#26102;&#65292;&#22238;&#24402;&#22330;&#26223;&#30340;&#39044;&#27979;&#38598;&#12290;&#36825;&#25193;&#23637;&#20102;[Lei and Wasserman, 2014]&#20013;&#22312;&#36825;&#19968;&#26032;&#39046;&#22495;&#20013;&#27010;&#36848;&#30340;&#27010;&#24565;&#12290;&#19982;&#19968;&#33268;&#25512;&#26029;&#20013;&#30340;&#20256;&#32479;&#21407;&#21017;&#19968;&#33268;&#65292;&#36825;&#20123;&#39044;&#27979;&#38598;&#26159;&#26080;&#20998;&#24067;&#30340;&#65292;&#34920;&#26126;&#23545;$(X, Y)$&#30340;&#32852;&#21512;&#20998;&#24067;&#27809;&#26377;&#26045;&#21152;&#29305;&#23450;&#30340;&#20551;&#35774;&#65292;&#32780;&#19988;&#23427;&#20204;&#20445;&#25345;&#38750;&#21442;&#25968;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#21306;&#22495;&#30340;&#32463;&#39564;&#29256;&#26412;&#22312;&#20960;&#20046;&#24517;&#28982;&#25910;&#25947;&#20110;&#26080;&#31351;&#22823;&#26102;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression on manifolds, and, more broadly, statistics on manifolds, has garnered significant importance in recent years due to the vast number of applications for this type of data. Circular data is a classic example, but so is data in the space of covariance matrices, data on the Grassmannian manifold obtained as a result of principal component analysis, among many others. In this work we investigate prediction sets for regression scenarios when the response variable, denoted by $Y$, resides in a manifold, and the covariable, denoted by X, lies in Euclidean space. This extends the concepts delineated in [Lei and Wasserman, 2014] to this novel context. Aligning with traditional principles in conformal inference, these prediction sets are distribution-free, indicating that no specific assumptions are imposed on the joint distribution of $(X, Y)$, and they maintain a non-parametric character. We prove the asymptotic almost sure convergence of the empirical version of these regions on th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;&#21270;&#23545;&#40784;&#21644;&#25239;&#36951;&#24536;&#30340;&#22810;&#27169;&#24577;&#22359;&#36873;&#25321;&#65292;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#38899;&#35270;&#39057;&#20998;&#24067;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.08204</link><description>&lt;p&gt;
&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#19982;&#25239;&#36951;&#24536;&#30340;&#26412;&#22320;&#21270;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments. (arXiv:2310.08204v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08204
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26412;&#22320;&#21270;&#23545;&#40784;&#21644;&#25239;&#36951;&#24536;&#30340;&#22810;&#27169;&#24577;&#22359;&#36873;&#25321;&#65292;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#38899;&#35270;&#39057;&#20998;&#24067;&#20013;&#23398;&#20064;&#20934;&#30830;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32456;&#36523;&#38899;&#35270;&#39057;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#19981;&#26029;&#22320;&#20174;&#21253;&#21547;&#38899;&#35270;&#39057;&#23545;&#30340;&#35270;&#39057;&#27969;&#20013;&#23398;&#20064;&#22810;&#27169;&#24577;&#34920;&#31034;&#65292;&#21516;&#26102;&#20854;&#20998;&#24067;&#38543;&#30528;&#26102;&#38388;&#19981;&#26029;&#21464;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#26412;&#22320;&#21270;&#23545;&#40784;&#65306;&#24341;&#20837;&#19968;&#20010;&#23567;&#22411;&#21487;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#65292;&#39044;&#27979;&#24444;&#27492;&#20043;&#38388;&#33391;&#22909;&#23545;&#40784;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#20196;&#29260;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#21482;&#23398;&#20064;&#20855;&#26377;&#20934;&#30830;&#22810;&#27169;&#24577;&#20851;&#31995;&#30340;&#39640;&#24230;&#30456;&#20851;&#30340;&#38899;&#39057;&#35270;&#35273;&#22359;&#12290;&#65288;2&#65289;&#25239;&#36951;&#24536;&#30340;&#22810;&#27169;&#24577;&#22359;&#36873;&#25321;&#65306;&#27604;&#36739;&#24403;&#21069;&#21644;&#36807;&#21435;&#25968;&#25454;&#23545;&#20043;&#38388;&#27599;&#20010;&#38899;&#39057;&#35270;&#39057;&#22359;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20197;&#20943;&#36731;&#20808;&#21069;&#23398;&#20064;&#30340;&#38899;&#35270;&#39057;&#34920;&#31034;&#30340;&#24847;&#22806;&#28418;&#31227;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;FLAVA&#65288;&#25239;&#36951;&#24536;&#30340;&#26412;&#22320;&#21270;&#38899;&#35270;&#39057;&#23545;&#40784;&#65289;&#22312;&#19968;&#31995;&#21015;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#25429;&#25417;&#21040;&#38899;&#39057;&#21644;&#35270;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a lifelong audio-video masked autoencoder that continually learns the multimodal representations from a video stream containing audio-video pairs, while its distribution continually shifts over time. Specifically, we propose two novel ideas to tackle the problem: (1) Localized Alignment: We introduce a small trainable multimodal encoder that predicts the audio and video tokens that are well-aligned with each other. This allows the model to learn only the highly correlated audiovisual patches with accurate multimodal relationships. (2) Forget-robust multimodal patch selection: We compare the relative importance of each audio-video patch between the current and past data pair to mitigate unintended drift of the previously learned audio-video representations. Our proposed method, FLAVA (Forget-robust Localized Audio-Video Alignment), therefore, captures the complex relationships between the audio and video modalities during training on a sequence of pre-training tasks while all
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#39062;DoE&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#30005;&#27744;&#21160;&#21147;&#23398;&#27169;&#22411;&#35782;&#21035;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#26681;&#25454;&#36807;&#21435;&#23454;&#39564;&#30340;&#32479;&#35745;&#20449;&#24687;&#21160;&#24577;&#22320;&#25913;&#21464;&#23454;&#39564;&#37197;&#32622;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23454;&#39564;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08198</link><description>&lt;p&gt;
&#36229;&#36234;&#20256;&#32479;DoE&#65306;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20248;&#21270;&#30005;&#27744;&#21160;&#21147;&#23398;&#27169;&#22411;&#35782;&#21035;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing Experiments in Model Identification of Battery Dynamics. (arXiv:2310.08198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#39062;DoE&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#30005;&#27744;&#21160;&#21147;&#23398;&#27169;&#22411;&#35782;&#21035;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#26681;&#25454;&#36807;&#21435;&#23454;&#39564;&#30340;&#32479;&#35745;&#20449;&#24687;&#21160;&#24577;&#22320;&#25913;&#21464;&#23454;&#39564;&#37197;&#32622;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23454;&#39564;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#21160;&#21147;&#23398;&#27169;&#22411;&#35782;&#21035;&#26159;&#33021;&#28304;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65307;&#35768;&#22810;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#21644;&#35774;&#35745;&#36807;&#31243;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#30005;&#27744;&#27169;&#22411;&#36827;&#34892;&#25928;&#29575;&#20248;&#21270;&#12290;&#20256;&#32479;&#30340;&#30005;&#27744;&#24314;&#27169;&#26041;&#27861;&#26159;&#21033;&#29992;&#20256;&#32479;&#30340;&#23454;&#39564;&#35774;&#35745;&#65288;DoE&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#35768;&#22810;&#19981;&#21516;&#30340;&#30005;&#27969;&#37197;&#32622;&#26469;&#28608;&#21457;&#30005;&#27744;&#21160;&#21147;&#23398;&#65292;&#24182;&#21033;&#29992;&#27979;&#37327;&#36755;&#20986;&#26469;&#20272;&#35745;&#31995;&#32479;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#20256;&#32479;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26377;&#29992;&#30340;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25195;&#25551;&#35768;&#22810;&#19981;&#21516;&#30340;&#30005;&#27969;&#37197;&#32622;&#65292;&#36825;&#19968;&#36807;&#31243;&#26102;&#38388;&#38271;&#19988;&#26114;&#36149;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DoE&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#36807;&#21435;&#23454;&#39564;&#30340;&#32479;&#35745;&#20449;&#24687;&#21160;&#24577;&#22320;&#25913;&#21464;&#23454;&#39564;&#37197;&#32622;&#12290;&#19982;&#22362;&#25345;&#39044;&#23450;&#20041;&#30005;&#27969;&#37197;&#32622;&#30340;&#24211;&#19981;&#21516;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#36755;&#20986;&#31354;&#38388;&#26469;&#21160;&#24577;&#20462;&#25913;&#30005;&#27969;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model identification of battery dynamics is a central problem in energy research; many energy management systems and design processes rely on accurate battery models for efficiency optimization. The standard methodology for battery modelling is traditional design of experiments (DoE), where the battery dynamics are excited with many different current profiles and the measured outputs are used to estimate the system dynamics. However, although it is possible to obtain useful models with the traditional approach, the process is time consuming and expensive because of the need to sweep many different current-profile configurations. In the present work, a novel DoE approach is developed based on deep reinforcement learning, which alters the configuration of the experiments on the fly based on the statistics of past experiments. Instead of sticking to a library of predefined current profiles, the proposed approach modifies the current profiles dynamically by updating the output space covere
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20197;Learn From Model (LFM)&#20026;&#21517;&#65292;&#25506;&#32034;&#20102;&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#21644;&#35774;&#35745;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25512;&#24191;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.08184</link><description>&lt;p&gt;
&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learn From Model Beyond Fine-Tuning: A Survey. (arXiv:2310.08184v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20197;Learn From Model (LFM)&#20026;&#21517;&#65292;&#25506;&#32034;&#20102;&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#21644;&#35774;&#35745;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25512;&#24191;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#65288;LFM&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#36235;&#21183;&#65292;&#23427;&#19987;&#27880;&#20110;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#12289;&#20462;&#25913;&#21644;&#35774;&#35745;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#26435;&#37325;&#65288;&#22312;&#40657;&#21283;&#23376;&#29615;&#22659;&#20013;&#65289;&#65292;&#24182;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#23558;LFM&#25216;&#26415;&#30340;&#30740;&#31350;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FM) have demonstrated remarkable performance across a wide range of tasks (especially in the fields of natural language processing and computer vision), primarily attributed to their ability to comprehend instructions and access extensive, high-quality data. This not only showcases their current effectiveness but also sets a promising trajectory towards the development of artificial general intelligence. Unfortunately, due to multiple constraints, the raw data of the model used for large model training are often inaccessible, so the use of end-to-end models for downstream tasks has become a new research trend, which we call Learn From Model (LFM) in this article. LFM focuses on the research, modification, and design of FM based on the model interface, so as to better understand the model structure and weights (in a black box environment), and to generalize the model to downstream tasks. The study of LFM techniques can be broadly categorized into five major areas: mod
&lt;/p&gt;</description></item><item><title>XIMAGENET-12&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12&#20010;&#24120;&#35265;&#29289;&#20307;&#31867;&#21035;&#30340;&#36229;&#36807;200,000&#24352;&#22270;&#20687;&#21644;15,600&#20010;&#25163;&#21160;&#35821;&#20041;&#27880;&#37322;&#12290;&#23427;&#36890;&#36807;&#27169;&#25311;&#20845;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#29983;&#25104;&#33021;&#21147;&#35780;&#20272;&#30340;&#26032;&#30340;&#31283;&#20581;&#24615;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2310.08182</link><description>&lt;p&gt;
XIMAGENET-12&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;AI&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness Evaluation. (arXiv:2310.08182v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08182
&lt;/p&gt;
&lt;p&gt;
XIMAGENET-12&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;AI&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12&#20010;&#24120;&#35265;&#29289;&#20307;&#31867;&#21035;&#30340;&#36229;&#36807;200,000&#24352;&#22270;&#20687;&#21644;15,600&#20010;&#25163;&#21160;&#35821;&#20041;&#27880;&#37322;&#12290;&#23427;&#36890;&#36807;&#27169;&#25311;&#20845;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#29983;&#25104;&#33021;&#21147;&#35780;&#20272;&#30340;&#26032;&#30340;&#31283;&#20581;&#24615;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#31283;&#20581;&#24615;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#24191;&#27867;&#20381;&#36182;&#21508;&#31181;&#26080;&#20851;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#23548;&#33268;&#23398;&#26415;&#39564;&#35777;&#30340;&#31283;&#20581;&#27169;&#22411;&#19982;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;XIMAGENET-12&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;200,000&#24352;&#22270;&#20687;&#21644;15,600&#20010;&#25163;&#21160;&#35821;&#20041;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;ImageNet&#30340;12&#20010;&#31867;&#21035;&#65292;&#20197;&#20195;&#34920;&#22312;&#23454;&#38469;&#29983;&#27963;&#20013;&#24120;&#35265;&#30340;&#29289;&#20307;&#65292;&#24182;&#27169;&#25311;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#21253;&#25324;&#36807;&#26333;&#12289;&#27169;&#31946;&#12289;&#39068;&#33394;&#21464;&#21270;&#31561;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#29983;&#25104;&#33021;&#21147;&#35780;&#20272;&#30340;&#26032;&#30340;&#31283;&#20581;&#24615;&#20934;&#21017;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#21450;&#30456;&#20851;&#20195;&#30721;&#21487;&#22312;https://sites.google.com/view/ximagenet-12/home&#33719;&#21462;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21487;&#20197;&#21033;&#29992;&#36825;&#19968;&#36164;&#28304;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#26465;&#20214;&#19979;&#35780;&#20272;&#20182;&#20204;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20174;&#32780;&#20174;&#23454;&#38469;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#38656;&#27714;&#20013;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of standardized robustness metrics and the widespread reliance on numerous unrelated benchmark datasets for testing have created a gap between academically validated robust models and their often problematic practical adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark dataset with over 200K images and 15,600 manual semantic annotations. Covering 12 categories from ImageNet to represent objects commonly encountered in practical life and simulating six diverse scenarios, including overexposure, blurring, color changing, etc., we further propose a novel robustness criterion that extends beyond model generation ability assessment. This benchmark dataset, along with related code, is available at https://sites.google.com/view/ximagenet-12/home. Researchers and practitioners can leverage this resource to evaluate the robustness of their visual models under challenging conditions and ultimately benefit from the demands of practical computer vision systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#24555;&#36895;&#26368;&#23567;&#33539;&#25968;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#12289;&#20248;&#21270;&#22120;&#21644;&#27493;&#38271;&#35843;&#24230;&#22120;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#36229;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#40065;&#26834;&#27169;&#22411;&#19979;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.08177</link><description>&lt;p&gt;
&#29992;&#36229;&#21442;&#25968;&#20248;&#21270;&#25552;&#21319;&#24555;&#36895;&#26368;&#23567;&#33539;&#25968;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization. (arXiv:2310.08177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#24555;&#36895;&#26368;&#23567;&#33539;&#25968;&#25915;&#20987;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#12289;&#20248;&#21270;&#22120;&#21644;&#27493;&#38271;&#35843;&#24230;&#22120;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#36229;&#21442;&#25968;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#40065;&#26834;&#27169;&#22411;&#19979;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#25915;&#20987;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#12289;&#20248;&#21270;&#22120;&#21644;&#27493;&#38271;&#35843;&#24230;&#22120;&#20197;&#21450;&#30456;&#24212;&#30340;&#36229;&#21442;&#25968;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24555;&#36895;&#26368;&#23567;&#33539;&#25968;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#35780;&#20272;&#65292;&#28041;&#21450;&#22810;&#20010;&#40065;&#26834;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#24555;&#36895;&#26368;&#23567;&#33539;&#25968;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;https://github.com/pralab/HO-FMN&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#24320;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the adversarial robustness of machine learning models using gradient-based attacks is challenging. In this work, we show that hyperparameter optimization can improve fast minimum-norm attacks by automating the selection of the loss function, the optimizer and the step-size scheduler, along with the corresponding hyperparameters. Our extensive evaluation involving several robust models demonstrates the improved efficacy of fast minimum-norm attacks when hyper-up with hyperparameter optimization. We release our open-source code at https://github.com/pralab/HO-FMN.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36830;&#25509;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;/&#26680;&#26041;&#27861;&#65292;&#30740;&#31350;&#25512;&#24191;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#38381;&#24335;&#24418;&#24335;&#30340;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.08176</link><description>&lt;p&gt;
&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#22238;&#24402;/&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Infinite Width Graph Neural Networks for Node Regression/ Classification. (arXiv:2310.08176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26080;&#38480;&#23485;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#36830;&#25509;&#28145;&#24230;&#23398;&#20064;&#21644;&#39640;&#26031;&#36807;&#31243;/&#26680;&#26041;&#27861;&#65292;&#30740;&#31350;&#25512;&#24191;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#38381;&#24335;&#24418;&#24335;&#30340;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#27599;&#20010;&#20840;&#36830;&#25509;&#23618;&#30340;&#33410;&#28857;&#25968;&#37327;&#36235;&#36817;&#26080;&#31351;&#22823;&#26102;&#65292;&#23427;&#26159;&#23545;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#30456;&#36830;&#25509;&#65292;&#21518;&#32773;&#37117;&#26159;&#20855;&#26377;&#24736;&#20037;&#20256;&#32479;&#21644;&#20016;&#23500;&#29702;&#35770;&#22522;&#30784;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#36739;&#23569;&#65292;&#21487;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20351;&#20854;&#22312;&#24212;&#29992;&#20013;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#12290;&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23558;&#39640;&#26031;&#36807;&#31243;&#21644;&#26680;&#26041;&#27861;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#36830;&#25509;&#30340;&#30740;&#31350;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#30340;&#36235;&#21183;&#12290;&#23545;&#20110;&#22810;&#31181;&#26550;&#26500;&#65288;&#21253;&#25324;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#25512;&#23548;&#20986;&#20102;&#26680;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#30340;&#38381;&#24335;&#24418;&#24335;&#12290;&#23545;&#36825;&#20123;&#26550;&#26500;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#22238;&#24402;/&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work analyzes Graph Neural Networks, a generalization of Fully-Connected Deep Neural Nets on Graph structured data, when their width, that is the number of nodes in each fullyconnected layer is increasing to infinity. Infinite Width Neural Networks are connecting Deep Learning to Gaussian Processes and Kernels, both Machine Learning Frameworks with long traditions and extensive theoretical foundations. Gaussian Processes and Kernels have much less hyperparameters then Neural Networks and can be used for uncertainty estimation, making them more user friendly for applications. This works extends the increasing amount of research connecting Gaussian Processes and Kernels to Neural Networks. The Kernel and Gaussian Process closed forms are derived for a variety of architectures, namely the standard Graph Neural Network, the Graph Neural Network with Skip-Concatenate Connections and the Graph Attention Neural Network. All architectures are evaluated on a variety of datasets on the task
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;Swin Transformer&#27169;&#22411;&#20174;CT&#22270;&#20687;&#20013;&#35786;&#26029;COVID-19&#65292;&#26041;&#27861;&#22312;&#24739;&#32773;&#32423;&#21035;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36234;&#22522;&#32447;&#27169;&#22411;&#21644;&#20854;&#20182;&#26041;&#27861;&#65292;&#20026;&#20934;&#30830;&#35786;&#26029;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.08165</link><description>&lt;p&gt;
&#21033;&#29992;Swin Transformer&#27169;&#22411;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;COVID-19
&lt;/p&gt;
&lt;p&gt;
COVID-19 Detection Using Swin Transformer Approach from Computed Tomography Images. (arXiv:2310.08165v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08165
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;Swin Transformer&#27169;&#22411;&#20174;CT&#22270;&#20687;&#20013;&#35786;&#26029;COVID-19&#65292;&#26041;&#27861;&#22312;&#24739;&#32773;&#32423;&#21035;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35780;&#20272;&#25351;&#26631;&#19978;&#36229;&#36234;&#22522;&#32447;&#27169;&#22411;&#21644;&#20854;&#20182;&#26041;&#27861;&#65292;&#20026;&#20934;&#30830;&#35786;&#26029;&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39640;&#25928;&#22320;&#35786;&#26029;COVID-19&#23545;&#20110;&#22823;&#35268;&#27169;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#39044;&#21360;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;CT&#22270;&#20687;&#36827;&#34892;COVID-19&#35786;&#26029;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#26368;&#20808;&#36827;&#30340;Swin Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#31181;&#31995;&#32479;&#30340;&#24739;&#32773;&#32423;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#27599;&#20010;CT&#20999;&#29255;&#20998;&#31867;&#20026;COVID-19&#25110;&#38750;COVID-19&#65292;&#24182;&#36890;&#36807;&#22810;&#25968;&#34920;&#20915;&#30830;&#23450;&#24739;&#32773;&#30340;&#25972;&#20307;&#35786;&#26029;&#32467;&#26524;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#24212;&#29992;Swin Transformer&#32467;&#26524;&#34920;&#29616;&#20986;&#24322;&#24120;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#22312;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#21644;&#35768;&#22810;&#31454;&#20105;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;COVID-19&#35786;&#26029;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#27169;&#22411;&#36798;&#21040;&#30340;&#23439;F1&#20998;&#25968;&#36229;&#36807;&#22522;&#32447;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20934;&#30830;&#35786;&#26029;&#30340;&#24378;&#26377;&#21147;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate and efficient diagnosis of COVID-19 is of paramount importance, particularly in the context of large-scale medical imaging datasets. In this preprint paper, we propose a novel approach for COVID-19 diagnosis using CT images that leverages the power of Swin Transformer models, state-of-the-art solutions in computer vision tasks. Our method includes a systematic approach for patient-level predictions, where individual CT slices are classified as COVID-19 or non-COVID, and the patient's overall diagnosis is determined through majority voting. The application of the Swin Transformer in this context results in patient-level predictions that demonstrate exceptional diagnostic accuracy. In terms of evaluation metrics, our approach consistently outperforms the baseline, as well as numerous competing methods, showcasing its effectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model exceeds the baseline and offers a robust solution for accurate diagnosis.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.08164</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. (arXiv:2310.08164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08164
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#21644;&#32463;&#36807;RLHF&#35843;&#25972;&#30340;&#29256;&#26412;&#30340;&#28608;&#27963;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#21453;&#26144;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24773;&#26223;&#65292;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20196;&#29260;-&#22870;&#21169;&#26144;&#23556;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#36825;&#26159;&#39318;&#27425;&#24212;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#37322;&#23398;&#20064;&#22870;&#21169;&#21644;&#24191;&#27867;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#65292;&#36825;&#20026;&#30830;&#20445;&#25351;&#23450;&#30446;&#26631;&#21644;&#27169;&#22411;&#34892;&#20026;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#21521;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20989;&#25968;&#30340;&#26368;&#22823;&#22411;&#32479;&#35745;&#37327;&#65292;&#25512;&#24191;&#20102;&#26679;&#26412;&#33258;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#26368;&#22823;&#20559;&#24046;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;Gumbel&#22411;&#26497;&#20540;&#28176;&#36817;&#24615;&#25104;&#31435;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#37329;&#34701;&#21644;&#21367;&#31215;&#32593;&#32476;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.08150</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30740;&#31350;&#25237;&#24433;&#26679;&#26412;&#21327;&#26041;&#24046;&#30340;&#26497;&#20540;&#28176;&#36817;&#24615;&#65292;&#24182;&#22312;&#37329;&#34701;&#21644;&#21367;&#31215;&#32593;&#32476;&#20013;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Extreme Value Asymptotics of Projected Sample Covariances in High Dimensions with Applications in Finance and Convolutional Networks. (arXiv:2310.08150v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#21521;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#20989;&#25968;&#30340;&#26368;&#22823;&#22411;&#32479;&#35745;&#37327;&#65292;&#25512;&#24191;&#20102;&#26679;&#26412;&#33258;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#26368;&#22823;&#20559;&#24046;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;Gumbel&#22411;&#26497;&#20540;&#28176;&#36817;&#24615;&#25104;&#31435;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#37329;&#34701;&#21644;&#21367;&#31215;&#32593;&#32476;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#21521;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#26576;&#20123;&#20989;&#25968;&#30340;&#26368;&#22823;&#22411;&#32479;&#35745;&#37327;&#65292;&#20197;&#32479;&#35745;&#22320;&#30830;&#35748;&#25110;&#25298;&#32477;&#25968;&#25454;&#38598;&#22312;&#27491;&#24120;&#26465;&#20214;&#19979;&#34987;&#25910;&#38598;&#30340;&#38646;&#20551;&#35774;&#12290;&#26412;&#26041;&#27861;&#25512;&#24191;&#20102;&#26679;&#26412;&#33258;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#26368;&#22823;&#20559;&#24046;&#24773;&#20917;&#12290;&#22312;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#26694;&#26550;&#19979;&#65292;&#35777;&#26126;&#20102;Gumbel&#22411;&#26497;&#20540;&#28176;&#36817;&#24615;&#25104;&#31435;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20165;&#20570;&#22810;&#30340;&#26368;&#23567;&#39118;&#38505;&#32452;&#21512;&#20248;&#21270;&#12289;&#30001;&#31232;&#30095;&#36319;&#36394;&#32452;&#21512;&#36827;&#34892;ETF&#25351;&#25968;&#36319;&#36394;&#12289;&#29992;&#20110;&#22270;&#20687;&#20998;&#26512;&#30340;&#21367;&#31215;&#28145;&#24230;&#23398;&#20064;&#21644;&#38453;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum-type statistics of certain functions of the sample covariance matrix of high-dimensional vector time series are studied to statistically confirm or reject the null hypothesis that a data set has been collected under normal conditions. The approach generalizes the case of the maximal deviation of the sample autocovariances function from its assumed values. Within a linear time series framework it is shown that Gumbel-type extreme value asymptotics holds true. As applications we discuss long-only mimimal-variance portfolio optimization and subportfolio analysis with respect to idiosyncratic risks, ETF index tracking by sparse tracking portfolios, convolutional deep learners for image analysis and the analysis of array-of-sensors data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#25918;&#38598;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#30340;&#20998;&#31867;&#38480;&#21046;&#21644;&#32570;&#20047;&#25512;&#29702;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08148</link><description>&lt;p&gt;
&#22522;&#20110;&#24320;&#25918;&#38598;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#21450;&#25512;&#29702;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Open-Set Knowledge-Based Visual Question Answering with Inference Paths. (arXiv:2310.08148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#25918;&#38598;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#30340;&#20998;&#31867;&#38480;&#21046;&#21644;&#32570;&#20047;&#25512;&#29702;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19968;&#24352;&#22270;&#29255;&#21644;&#30456;&#20851;&#30340;&#25991;&#26412;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#22806;&#37096;&#30693;&#35782;&#24211;&#25552;&#20379;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#26816;&#32034;&#22120;-&#20998;&#31867;&#22120;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#26816;&#32034;&#22120;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#25991;&#26412;&#25110;&#35270;&#35273;&#20449;&#24687;&#65292;&#28982;&#21518;&#22312;&#20505;&#36873;&#31572;&#26696;&#20013;&#36827;&#34892;&#39044;&#27979;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#23558;&#38382;&#39064;&#22238;&#31572;&#24314;&#27169;&#20026;&#22810;&#31867;&#21035;&#20998;&#31867;&#23558;&#31572;&#26696;&#31354;&#38388;&#38480;&#21046;&#22312;&#39044;&#35774;&#30340;&#35821;&#26009;&#24211;&#20013;&#65292;&#32570;&#20047;&#28789;&#27963;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#20998;&#31867;&#22120;&#20165;&#20165;&#32771;&#34385;"&#31572;&#26696;&#26159;&#20160;&#20040;"&#32780;&#27809;&#26377;&#32771;&#34385;"&#22914;&#20309;&#24471;&#21040;&#31572;&#26696;"&#65292;&#26080;&#27861;&#23558;&#31572;&#26696;&#19982;&#26126;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38754;&#23545;&#30528;&#35299;&#37322;&#24615;&#24320;&#25918;&#38598;&#30693;&#35782;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#30340;&#25361;&#25112;&#65292;&#35813;&#31995;&#32479;&#35201;&#27714;&#33021;&#22815;&#22238;&#31572;&#28041;&#21450;&#20219;&#24847;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#24182;&#20445;&#30041;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given an image and an associated textual question, the purpose of Knowledge-Based Visual Question Answering (KB-VQA) is to provide a correct answer to the question with the aid of external knowledge bases. Prior KB-VQA models are usually formulated as a retriever-classifier framework, where a pre-trained retriever extracts textual or visual information from knowledge graphs and then makes a prediction among the candidates. Despite promising progress, there are two drawbacks with existing models. Firstly, modeling question-answering as multi-class classification limits the answer space to a preset corpus and lacks the ability of flexible reasoning. Secondly, the classifier merely consider "what is the answer" without "how to get the answer", which cannot ground the answer to explicit reasoning paths. In this paper, we confront the challenge of \emph{explainable open-set} KB-VQA, where the system is required to answer questions with entities at wild and retain an explainable reasoning pa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSSTRN&#30340;&#22810;&#23610;&#24230;&#26102;&#31354;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#25429;&#33719;&#20102;&#20132;&#36890;&#25968;&#25454;&#20013;&#19981;&#21516;&#26102;&#38388;&#31383;&#21475;&#19979;&#30340;&#22797;&#26434;&#26102;&#31354;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#24573;&#35270;&#22270;&#32467;&#26500;&#12289;&#25429;&#25417;&#19981;&#20805;&#20998;&#20197;&#21450;&#32570;&#20047;&#20851;&#27880;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08138</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#26102;&#31354;&#24490;&#29615;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow Prediction. (arXiv:2310.08138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSSTRN&#30340;&#22810;&#23610;&#24230;&#26102;&#31354;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;&#12290;&#35813;&#32593;&#32476;&#36890;&#36807;&#20004;&#31181;&#19981;&#21516;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#25429;&#33719;&#20102;&#20132;&#36890;&#25968;&#25454;&#20013;&#19981;&#21516;&#26102;&#38388;&#31383;&#21475;&#19979;&#30340;&#22797;&#26434;&#26102;&#31354;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#24573;&#35270;&#22270;&#32467;&#26500;&#12289;&#25429;&#25417;&#19981;&#20805;&#20998;&#20197;&#21450;&#32570;&#20047;&#20851;&#27880;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#20351;&#24471;&#20132;&#36890;&#27969;&#39044;&#27979;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20132;&#36890;&#27969;&#39044;&#27979;&#20013;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;1&#65289;&#24573;&#35270;&#20102;&#22266;&#23450;&#30340;&#22270;&#32467;&#26500;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65307;&#65288;2&#65289;&#19981;&#33021;&#21516;&#26102;&#20805;&#20998;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#65307;&#65288;3&#65289;&#32570;&#20047;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#26102;&#31354;&#20449;&#24687;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#31354;&#24490;&#29615;&#32593;&#32476;&#65288;MSSTRN&#65289;&#29992;&#20110;&#20132;&#36890;&#27969;&#39044;&#27979;&#65292;&#23427;&#30001;&#20004;&#31181;&#19981;&#21516;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65306;&#21333;&#27493;&#38376;&#24490;&#29615;&#21333;&#20803;&#21644;&#22810;&#27493;&#38376;&#24490;&#29615;&#21333;&#20803;&#65292;&#20197;&#23436;&#20840;&#25429;&#33719;&#20132;&#36890;&#25968;&#25454;&#20013;&#19981;&#21516;&#26102;&#38388;&#31383;&#21475;&#19979;&#30340;&#22797;&#26434;&#26102;&#31354;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#31354;&#21516;&#27493;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow prediction is one of the most fundamental tasks of intelligent transportation systems. The complex and dynamic spatial-temporal dependencies make the traffic flow prediction quite challenging. Although existing spatial-temporal graph neural networks hold prominent, they often encounter challenges such as (1) ignoring the fixed graph that limits the predictive performance of the model, (2) insufficiently capturing complex spatial-temporal dependencies simultaneously, and (3) lacking attention to spatial-temporal information at different time lengths. In this paper, we propose a Multi-Scale Spatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN, which consists of two different recurrent neural networks: the single-step gate recurrent unit and the multi-step gate recurrent unit to fully capture the complex spatial-temporal information in the traffic data under different time windows. Moreover, we propose a spatial-temporal synchronous attention mechanis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36870;&#21521;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#22522;&#20110;&#26799;&#24230;&#30340;&#25200;&#21160;&#65292;&#24182;&#21033;&#29992;&#32422;&#26463;&#26465;&#20214;&#25351;&#23548;&#25200;&#21160;&#65292;&#20174;&#32780;&#33719;&#24471;&#25152;&#38656;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#22411;&#26550;&#26500;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.08137</link><description>&lt;p&gt;
&#36870;&#21521;&#35299;&#37322;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanations for Time Series Forecasting. (arXiv:2310.08137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36870;&#21521;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#22522;&#20110;&#26799;&#24230;&#30340;&#25200;&#21160;&#65292;&#24182;&#21033;&#29992;&#32422;&#26463;&#26465;&#20214;&#25351;&#23548;&#25200;&#21160;&#65292;&#20174;&#32780;&#33719;&#24471;&#25152;&#38656;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#22411;&#26550;&#26500;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#20013;&#65292;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#22240;&#33021;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#38544;&#34255;&#29305;&#24449;&#27169;&#24335;&#26469;&#25552;&#21319;&#39044;&#27979;&#24615;&#33021;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22823;&#37096;&#20998;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#37117;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#22240;&#27492;&#32467;&#26524;&#30340;&#35299;&#37322;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#36870;&#21521;&#35299;&#37322;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#37322;&#20998;&#31867;&#27169;&#22411;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20294;&#20854;&#22312;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#20173;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#36870;&#21521;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ForecastCF&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#26045;&#21152;&#22522;&#20110;&#26799;&#24230;&#30340;&#25200;&#21160;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;ForecastCF&#36890;&#36807;&#23545;&#39044;&#27979;&#20540;&#26045;&#21152;&#32422;&#26463;&#25351;&#23548;&#25200;&#21160;&#20197;&#33719;&#24471;&#25152;&#38656;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#22411;&#26550;&#26500;&#23545;ForecastCF&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among recent developments in time series forecasting methods, deep forecasting models have gained popularity as they can utilize hidden feature patterns in time series to improve forecasting performance. Nevertheless, the majority of current deep forecasting models are opaque, hence making it challenging to interpret the results. While counterfactual explanations have been extensively employed as a post-hoc approach for explaining classification models, their application to forecasting models still remains underexplored. In this paper, we formulate the novel problem of counterfactual generation for time series forecasting, and propose an algorithm, called ForecastCF, that solves the problem by applying gradient-based perturbations to the original time series. ForecastCF guides the perturbations by applying constraints to the forecasted values to obtain desired prediction outcomes. We experimentally evaluate ForecastCF using four state-of-the-art deep model architectures and compare to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20844;&#24179;/&#21010;&#20998;&#32422;&#26463;&#19979;&#22810;&#26679;&#24615;&#26368;&#22823;&#21270;&#20219;&#21153;&#30340;&#26680;&#24515;&#38598;&#26500;&#36896;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19982;&#20004;&#31181;&#22810;&#26679;&#24615;&#24230;&#37327;&#30456;&#20851;&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#26680;&#24515;&#38598;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08122</link><description>&lt;p&gt;
&#20844;&#24179;&#21644;&#22810;&#26679;&#24615;&#25968;&#25454;&#27719;&#24635;&#30340;&#26680;&#24515;&#38598;&#26500;&#36896;
&lt;/p&gt;
&lt;p&gt;
Core-sets for Fair and Diverse Data Summarization. (arXiv:2310.08122v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20844;&#24179;/&#21010;&#20998;&#32422;&#26463;&#19979;&#22810;&#26679;&#24615;&#26368;&#22823;&#21270;&#20219;&#21153;&#30340;&#26680;&#24515;&#38598;&#26500;&#36896;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19982;&#20004;&#31181;&#22810;&#26679;&#24615;&#24230;&#37327;&#30456;&#20851;&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#26680;&#24515;&#38598;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20844;&#24179;/&#21010;&#20998;&#32422;&#26463;&#19979;&#22810;&#26679;&#24615;&#26368;&#22823;&#21270;&#20219;&#21153;&#30340;&#26680;&#24515;&#38598;&#26500;&#36896;&#31639;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#20998;&#25104;m&#32452;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#28857;&#38598;P&#65292;&#24182;&#32473;&#23450;k1,...,km&#65292;&#35813;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#20174;&#27599;&#20010;&#32452;i&#20013;&#36873;&#25321;k_i&#20010;&#28857;&#65292;&#20351;&#24471;&#36873;&#20986;&#30340;k=\sum_i k_i&#20010;&#28857;&#30340;&#25972;&#20307;&#22810;&#26679;&#24615;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#65306;&#23545;&#20004;&#28857;&#20043;&#38388;&#36317;&#31163;&#27714;&#21644;&#21644;&#23545;&#26368;&#36817;&#37051;&#36317;&#31163;&#27714;&#21644;&#65292;&#24182;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#36825;&#20123;&#24230;&#37327;&#30340;&#25913;&#36827;&#30340;&#26680;&#24515;&#38598;&#26500;&#36896;&#31639;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#19982;&#23545;&#20004;&#28857;&#20043;&#38388;&#36317;&#31163;&#27714;&#21644;&#26080;&#20851;&#30340;&#26680;&#24515;&#38598;&#65292;&#20854;&#22823;&#23567;&#19982;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#32437;&#27178;&#27604;&#26080;&#20851;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31532;&#19968;&#20010;&#19982;&#26368;&#36817;&#37051;&#36317;&#31163;&#20043;&#21644;&#30456;&#20851;&#30340;&#26680;&#24515;&#38598;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26680;&#24515;&#38598;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#32422;&#26463;&#22810;&#26679;&#24615;&#26368;&#22823;&#21270;&#26469;&#27719;&#24635;&#19968;&#32452;&#23450;&#26102;&#28040;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study core-set construction algorithms for the task of Diversity Maximization under fairness/partition constraint. Given a set of points $P$ in a metric space partitioned into $m$ groups, and given $k_1,\ldots,k_m$, the goal of this problem is to pick $k_i$ points from each group $i$ such that the overall diversity of the $k=\sum_i k_i$ picked points is maximized. We consider two natural diversity measures: sum-of-pairwise distances and sum-of-nearest-neighbor distances, and show improved core-set construction algorithms with respect to these measures. More precisely, we show the first constant factor core-set w.r.t. sum-of-pairwise distances whose size is independent of the size of the dataset and the aspect ratio. Second, we show the first core-set w.r.t. the sum-of-nearest-neighbor distances. Finally, we run several experiments showing the effectiveness of our core-set approach. In particular, we apply constrained diversity maximization to summarize a set of timed messages that t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#21453;&#28436;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#30340;&#22235;&#31181;&#31639;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#35745;&#31639;&#65292;&#32780;&#20840;&#27874;&#24418;&#21453;&#28436;&#21017;&#20351;&#29992;&#20102;&#26377;&#38480;&#24046;&#20998;&#27714;&#35299;&#27874;&#21160;&#26041;&#31243;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#32852;&#21512;&#30446;&#26631;&#20989;&#25968;&#26469;&#33719;&#21462;&#26368;&#20248;&#27169;&#22411;&#21644;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2310.08109</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#21453;&#28436;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Overview of Physics-Informed Machine Learning Inversion of Geophysical Data. (arXiv:2310.08109v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08109
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#21453;&#28436;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#30340;&#22235;&#31181;&#31639;&#27861;&#65292;&#20854;&#20013;&#21033;&#29992;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#35745;&#31639;&#65292;&#32780;&#20840;&#27874;&#24418;&#21453;&#28436;&#21017;&#20351;&#29992;&#20102;&#26377;&#38480;&#24046;&#20998;&#27714;&#35299;&#27874;&#21160;&#26041;&#31243;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#32852;&#21512;&#30446;&#26631;&#20989;&#25968;&#26469;&#33719;&#21462;&#26368;&#20248;&#27169;&#22411;&#21644;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#21453;&#28436;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#30340;&#22235;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#32479;&#19968;&#30340;&#26041;&#31243;&#30001;&#32852;&#21512;&#30446;&#26631;&#20989;&#25968;$\epsilon$&#32473;&#20986;&#65306;\begin{eqnarray} \epsilon^{||-PIML}&amp;=&amp;\lambda_1 \overbrace{||{\bf W}^{ML}({\bf H}_{{\bf w}} {\bf d}^{obs}-{\bf m})||^2}^{NN} + \lambda_2 \overbrace{{||{\bf W}^{FWI}({\bf L} {\bf m}-{\bf d}^{obs})||^2}}^{FWI} ~+ \nonumber\\ \nonumber\\ &amp;&amp; + ~~&#27491;&#21017;&#39033;, \label{PIML.eq120} \end{eqnarray}&#20854;&#20013;&#26368;&#20248;&#27169;&#22411;${\bf m}^*$&#21644;&#26435;&#37325;${\bf w}^*$&#20351;$\epsilon$&#26368;&#23567;&#21270;&#12290;&#36825;&#37324;&#65292;&#30697;&#38453;&#26435;&#37325;&#30001;&#31895;&#20307;&#31526;&#21495;$\bf W$&#32473;&#20986;&#65292;&#20840;&#27874;&#24418;&#21453;&#28436;&#65288;FWI&#65289;&#36890;&#24120;&#20351;&#29992;&#27874;&#21160;&#26041;&#31243;&#30340;&#26377;&#38480;&#24046;&#20998;&#35299;&#27714;&#35299;&#65292;&#20854;&#20013;$\bf L$&#34920;&#31034;&#27874;&#21160;&#26041;&#31243;&#20316;&#20026;&#27169;&#22411;$\bf m$&#30340;&#20989;&#25968;&#30340;&#21069;&#21521;&#27169;&#25311;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#29992;&#20110;&#20174;&#35266;&#27979;&#36755;&#20837;&#25968;&#25454;${\bf d}^{obs}$&#35745;&#31639;&#27169;&#22411;${\bf H}_{{\bf w}}{\bf d}^{obs}\approx{\bf m}$&#12290;
&lt;/p&gt;
&lt;p&gt;
We review four types of algorithms for physics-informed machine learning (PIML) inversion of geophysical data. The unifying equation is given by the joint objective function $\epsilon$:  \begin{eqnarray} \epsilon^{||-PIML}&amp;=&amp;\lambda_1 \overbrace{||{\bf W}^{ML}({\bf H}_{{\bf w}} {\bf d}^{obs}-{\bf m})||^2}^{NN} + \lambda_2 \overbrace{{||{\bf W}^{FWI}({\bf L} {\bf m}-{\bf d}^{obs})||^2}}^{FWI} ~+ \nonumber\\ \nonumber\\ &amp;&amp; + ~~Regularizer, \label{PIML.eq120} \end{eqnarray}where the optimal model ${\bf m}^*$ and weights $\bf w^*$ minimize $\epsilon$. Here, The matrix weights are given by the boldface symbol $\bf W$, and full waveform inversion (FWI) is typically computed using a finite-difference solution of the wave equation, where $\bf L$ represents the forward modeling operation of the wave equation as a function of the model $\bf m$. Also, a fully-connected neural network (NN) is used to compute the model ${\bf H_w}{\bf d}^{obs} \approx \bf m$ from the observed input data ${\bf d}^{ob
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#20869;&#22312;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#23398;&#20064;&#21644;&#20869;&#22312;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24418;&#24335;&#32467;&#26524;&#30340;&#32508;&#21512;&#22788;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#32771;&#34385;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08100</link><description>&lt;p&gt;
&#29983;&#25104;&#20869;&#22312;&#20248;&#21270;&#65306;&#20855;&#26377;&#27169;&#22411;&#23398;&#20064;&#30340;&#20869;&#22312;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Generative Intrinsic Optimization: Intrisic Control with Model Learning. (arXiv:2310.08100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08100
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#20869;&#22312;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#27169;&#22411;&#23398;&#20064;&#21644;&#20869;&#22312;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#24418;&#24335;&#32467;&#26524;&#30340;&#32508;&#21512;&#22788;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#32771;&#34385;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#24207;&#21015;&#20195;&#34920;&#22312;&#29615;&#22659;&#20013;&#25191;&#34892;&#21160;&#20316;&#21518;&#30340;&#32467;&#26524;&#12290;&#24403;&#22522;&#20110;&#20449;&#24687;&#35770;&#27010;&#24565;&#30340;&#30456;&#20114;&#20449;&#24687;&#39537;&#21160;&#26102;&#65292;&#23427;&#23547;&#27714;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#32467;&#26524;&#12290;&#26174;&#24335;&#32467;&#26524;&#21487;&#33021;&#22240;&#29366;&#24577;&#12289;&#22238;&#25253;&#25110;&#36712;&#36857;&#32780;&#24322;&#65292;&#29992;&#20110;&#19981;&#21516;&#30446;&#30340;&#65292;&#22914;&#23398;&#20998;&#20998;&#37197;&#25110;&#27169;&#20223;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23558;&#20869;&#22312;&#21160;&#26426;&#19982;&#22870;&#21169;&#26368;&#22823;&#21270;&#32467;&#21512;&#30340;&#22266;&#26377;&#24615;&#36136;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#26041;&#27861;&#65292;&#20849;&#21516;&#23398;&#20064;&#20272;&#35745;&#30456;&#20114;&#20449;&#24687;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#24517;&#35201;&#25968;&#37327;&#65292;&#20026;&#21512;&#24182;&#19981;&#21516;&#24418;&#24335;&#30340;&#24863;&#20852;&#36259;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#12290;&#32467;&#21512;&#21040;&#31574;&#30053;&#36845;&#20195;&#26041;&#26696;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#34429;&#28982;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#29702;&#35770;&#20998;&#26512;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#25171;&#24320;&#20102;&#21033;&#29992;&#24102;&#26377;&#27169;&#22411;&#23398;&#20064;&#30340;&#20869;&#22312;&#25511;&#21046;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#32435;&#20837;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future sequence represents the outcome after executing the action into the environment. When driven by the information-theoretic concept of mutual information, it seeks maximally informative consequences. Explicit outcomes may vary across state, return, or trajectory serving different purposes such as credit assignment or imitation learning. However, the inherent nature of incorporating intrinsic motivation with reward maximization is often neglected. In this work, we propose a variational approach to jointly learn the necessary quantity for estimating the mutual information and the dynamics model, providing a general framework for incorporating different forms of outcomes of interest. Integrated into a policy iteration scheme, our approach guarantees convergence to the optimal policy. While we mainly focus on theoretical analysis, our approach opens the possibilities of leveraging intrinsic control with model learning to enhance sample efficiency and incorporate uncertainty of the env
&lt;/p&gt;</description></item><item><title>ClimateBERT-NetZero&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#20928;&#38646;&#21644;&#20943;&#25490;&#30446;&#26631;&#30340;&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#23454;&#29616;&#12290;&#35813;&#24037;&#20855;&#21487;&#20197;&#19982;&#38382;&#31572;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#20998;&#26512;&#20928;&#38646;&#21644;&#20943;&#25490;&#30446;&#26631;&#20013;&#30340;&#38596;&#24515;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#36890;&#20449;&#27169;&#24335;&#30340;&#28436;&#21464;&#25552;&#20379;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.08096</link><description>&lt;p&gt;
ClimateBERT-NetZero:&#26816;&#27979;&#21644;&#35780;&#20272;&#20928;&#38646;&#21644;&#20943;&#25490;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
ClimateBERT-NetZero: Detecting and Assessing Net Zero and Reduction Targets. (arXiv:2310.08096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08096
&lt;/p&gt;
&lt;p&gt;
ClimateBERT-NetZero&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#20928;&#38646;&#21644;&#20943;&#25490;&#30446;&#26631;&#30340;&#24037;&#20855;&#65292;&#23427;&#36890;&#36807;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#23454;&#29616;&#12290;&#35813;&#24037;&#20855;&#21487;&#20197;&#19982;&#38382;&#31572;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#20998;&#26512;&#20928;&#38646;&#21644;&#20943;&#25490;&#30446;&#26631;&#20013;&#30340;&#38596;&#24515;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#36890;&#20449;&#27169;&#24335;&#30340;&#28436;&#21464;&#25552;&#20379;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#21644;&#31169;&#20154;&#21442;&#19982;&#32773;&#22312;&#35780;&#20272;&#21508;&#31181;&#26426;&#26500;&#21487;&#25345;&#32493;&#21457;&#23637;&#25215;&#35834;&#30340;&#22823;&#37327;&#20449;&#24687;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#20225;&#19994;&#12289;&#22269;&#23478;&#21644;&#22320;&#21306;&#30340;&#20928;&#38646;&#21644;&#20943;&#25490;&#30446;&#26631;&#65292;&#36825;&#20010;&#24037;&#20855;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;3.5K&#20010;&#25991;&#26412;&#26679;&#26412;&#30340;&#19987;&#23478;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;ClimateBERT-NetZero&#65292;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#21253;&#21547;&#20928;&#38646;&#25110;&#20943;&#25490;&#30446;&#26631;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#20004;&#20010;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#20998;&#26512;&#28508;&#21147;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;ClimateBERT-NetZero&#19982;&#20256;&#32479;&#30340;&#38382;&#31572;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20998;&#26512;&#20928;&#38646;&#21644;&#20943;&#25490;&#30446;&#26631;&#20013;&#23637;&#31034;&#30340;&#38596;&#24515;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#23395;&#24230;&#36130;&#25253;&#30005;&#35805;&#20250;&#35758;&#35760;&#24405;&#19978;&#20351;&#29992;ClimateBERT-NetZero&#27169;&#22411;&#65292;&#24182;&#27010;&#36848;&#20102;&#36890;&#20449;&#27169;&#24335;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25552;&#21462;&#21644;&#20998;&#26512;&#20928;&#38646;&#21644;&#25490;&#25918;&#30446;&#26631;&#30340;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public and private actors struggle to assess the vast amounts of information about sustainability commitments made by various institutions. To address this problem, we create a novel tool for automatically detecting corporate, national, and regional net zero and reduction targets in three steps. First, we introduce an expert-annotated data set with 3.5K text samples. Second, we train and release ClimateBERT-NetZero, a natural language classifier to detect whether a text contains a net zero or reduction target. Third, we showcase its analysis potential with two use cases: We first demonstrate how ClimateBERT-NetZero can be combined with conventional question-answering (Q&amp;A) models to analyze the ambitions displayed in net zero and reduction targets. Furthermore, we employ the ClimateBERT-NetZero model on quarterly earning call transcripts and outline how communication patterns evolve over time. Our experiments demonstrate promising pathways for extracting and analyzing net zero and emis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;&#26469;&#20998;&#37197;&#36164;&#28304;&#65292;&#20197;&#25913;&#21892;&#29366;&#24577;&#20043;&#38388;&#30340;&#36731;&#37325;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;TD&#23398;&#20064;&#20013;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#37325;&#35201;&#24615;&#21644;TD&#35823;&#24046;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#25913;&#36827;&#20102;&#20540;&#20272;&#35745;&#65292;&#36824;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.08091</link><description>&lt;p&gt;
&#20998;&#36776;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;&#26469;&#20998;&#37197;&#36164;&#28304;&#65292;&#20197;&#25913;&#21892;&#29366;&#24577;&#20043;&#38388;&#30340;&#36731;&#37325;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;TD&#23398;&#20064;&#20013;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#37325;&#35201;&#24615;&#21644;TD&#35823;&#24046;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#25913;&#36827;&#20102;&#20540;&#20272;&#35745;&#65292;&#36824;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;(TD)&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#26088;&#22312;&#39640;&#25928;&#35780;&#20272;&#31574;&#30053;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;TD($\lambda$)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#35760;&#24518;&#36712;&#36857;&#23558;&#39044;&#27979;&#35823;&#24046;&#20998;&#25955;&#21040;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20256;&#25773;TD&#35823;&#24046;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#36825;&#21463;&#21040;&#35775;&#38382;&#22833;&#34913;&#25110;&#32467;&#26524;&#22122;&#22768;&#31561;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;-&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39044;&#20808;&#30830;&#23450;&#25110;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#36164;&#28304;&#20197;&#25552;&#39640;&#29366;&#24577;&#20043;&#38388;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;&#24378;&#35843;&#20989;&#25968;&#20869;&#24314;&#31435;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#28145;&#24230;RL&#29615;&#22659;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#21487;&#20197;&#25913;&#36827;&#20540;&#20272;&#35745;&#65292;&#36824;&#21487;&#20197;&#21152;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites l
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#22788;&#29702;&#38646;&#33192;&#32960;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#23454;&#38469;&#24212;&#29992;&#26696;&#20363;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08088</link><description>&lt;p&gt;
&#22788;&#29702;&#38646;&#33192;&#32960;&#25968;&#25454;&#65306;&#29992;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;SOTA&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Dealing with zero-inflated data: achieving SOTA with a two-fold machine learning approach. (arXiv:2310.08088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08088
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#22788;&#29702;&#38646;&#33192;&#32960;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#23454;&#38469;&#24212;&#29992;&#26696;&#20363;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24517;&#39035;&#23398;&#20250;&#22312;&#36739;&#24191;&#33539;&#22260;&#30340;&#25968;&#25454;&#20013;&#27491;&#30830;&#39044;&#27979;&#19968;&#20123;&#20855;&#26377;&#29305;&#23450;&#20852;&#36259;&#20540;&#30340;&#25968;&#25454;&#28857;&#65292;&#32780;&#20854;&#20013;&#35768;&#22810;&#30446;&#26631;&#20540;&#20026;&#38646;&#12290;&#38646;&#33192;&#32960;&#25968;&#25454;&#21487;&#20197;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#25214;&#21040;&#65292;&#27604;&#22914;&#39063;&#31890;&#29366;&#21644;&#38388;&#27463;&#24615;&#38656;&#27714;&#12289;&#23478;&#29992;&#30005;&#22120;&#30340;&#21151;&#29575;&#28040;&#32791;&#65288;&#24320;&#20851;&#24320;&#21551;&#21644;&#20851;&#38381;&#65289;&#12289;&#33976;&#39311;&#36807;&#31243;&#20013;&#30340;&#26434;&#36136;&#27979;&#37327;&#65292;&#29978;&#33267;&#26159;&#26426;&#22330;&#29677;&#36710;&#38656;&#27714;&#39044;&#27979;&#31561;&#12290;&#38646;&#30340;&#23384;&#22312;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#24182;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#38646;&#20063;&#25197;&#26354;&#20102;&#29992;&#20110;&#35745;&#31639;&#27169;&#22411;&#39044;&#27979;&#36136;&#37327;&#30340;&#25351;&#26631;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20004;&#20010;&#23454;&#38469;&#24212;&#29992;&#26696;&#20363;&#65288;&#23478;&#29992;&#30005;&#22120;&#20998;&#31867;&#21644;&#26426;&#22330;&#29677;&#36710;&#38656;&#27714;&#39044;&#27979;&#65289;&#65292;&#20854;&#20013;&#22312;&#38646;&#33192;&#32960;&#25968;&#25454;&#32972;&#26223;&#19979;&#24212;&#29992;&#23618;&#27425;&#27169;&#22411;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#23478;&#29992;&#30005;&#22120;&#20998;&#31867;&#65292;&#21152;&#26435;&#24179;&#22343;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20540;&#21644;AUC ROC&#20998;&#21035;&#25552;&#39640;&#20102;27%&#12289;34%&#12289;49%&#21644;27%&#12290;
&lt;/p&gt;
&lt;p&gt;
In many cases, a machine learning model must learn to correctly predict a few data points with particular values of interest in a broader range of data where many target values are zero. Zero-inflated data can be found in diverse scenarios, such as lumpy and intermittent demands, power consumption for home appliances being turned on and off, impurities measurement in distillation processes, and even airport shuttle demand prediction. The presence of zeroes affects the models' learning and may result in poor performance. Furthermore, zeroes also distort the metrics used to compute the model's prediction quality. This paper showcases two real-world use cases (home appliances classification and airport shuttle demand prediction) where a hierarchical model applied in the context of zero-inflated data leads to excellent results. In particular, for home appliances classification, the weighted average of Precision, Recall, F1, and AUC ROC was increased by 27%, 34%, 49%, and 27%, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30899;&#36861;&#36394;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#30417;&#27979;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#20026;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2310.08087</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#30899;&#36861;&#36394;&#27169;&#22411;&#65306;&#37327;&#21270;&#21644;&#31232;&#30095;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Carbon Tracking Model for Federated Learning: Impact of Quantization and Sparsification. (arXiv:2310.08087v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30899;&#36861;&#36394;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#30417;&#27979;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#24433;&#21709;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#20026;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#39640;&#25928;&#30340;&#36890;&#20449;&#25216;&#26415;&#23558;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20998;&#24067;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#65292;&#19982;&#38598;&#20013;&#24335;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#22312;&#25968;&#25454;&#23384;&#20648;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#20943;&#23569;&#20102;&#24320;&#38144;&#12290;&#32852;&#37030;&#23398;&#20064;&#20026;&#35299;&#20915;&#20174;&#29983;&#20135;&#32773;&#65288;&#20256;&#24863;&#22120;&#12289;&#26426;&#22120;&#65289;&#21040;&#33021;&#32791;&#39640;&#30340;&#25968;&#25454;&#20013;&#24515;&#22823;&#37327;&#20256;&#36755;&#25968;&#25454;&#24341;&#36215;&#30340;&#36164;&#28304;&#38656;&#27714;&#32780;&#24341;&#21457;&#30340;&#29615;&#22659;&#38382;&#39064;&#25552;&#20379;&#20102;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20351;&#24471;&#26032;&#30340;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#65288;AIoT&#65289;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#30417;&#27979;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#36275;&#36857;&#24433;&#21709;&#30340;&#26694;&#26550;&#12290;&#35813;&#30899;&#36861;&#36394;&#24037;&#20855;&#23545;&#20849;&#35782;&#65288;&#23436;&#20840;&#20998;&#25955;&#65289;&#21644;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#27425;&#20174;&#33021;&#32791;&#21644;&#31561;&#25928;&#30899;&#25490;&#25918;&#30340;&#35282;&#24230;&#23450;&#37327;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) methods adopt efficient communication technologies to distribute machine learning tasks across edge devices, reducing the overhead in terms of data storage and computational complexity compared to centralized solutions. Rather than moving large data volumes from producers (sensors, machines) to energy-hungry data centers, raising environmental concerns due to resource demands, FL provides an alternative solution to mitigate the energy demands of several learning tasks while enabling new Artificial Intelligence of Things (AIoT) applications. This paper proposes a framework for real-time monitoring of the energy and carbon footprint impacts of FL systems. The carbon tracking tool is evaluated for consensus (fully decentralized) and classical FL policies. For the first time, we present a quantitative evaluation of different computationally and communication efficient FL methods from the perspectives of energy consumption and carbon equivalent emissions, suggesting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36827;&#34892;&#20102;&#38024;&#23545;&#36328;&#35821;&#35328;&#36716;&#25442;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#26696;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21457;&#29616;&#22270;&#20687;&#27169;&#22411;&#22312;&#30456;&#20851;&#19988;&#33050;&#26412;&#30456;&#20284;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#36716;&#25442;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;&#22522;&#20110;&#20998;&#21106;&#30340;&#27169;&#22411;&#22312;&#20559;&#21521;&#21333;&#35789;&#21547;&#20041;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.08078</link><description>&lt;p&gt;
&#26159;&#21542;&#36827;&#34892;&#35789;&#20803;&#21270;&#65306;&#29992;&#20110;&#36328;&#35821;&#35328;&#36716;&#25442;&#30340;&#25991;&#26412;&#34920;&#31034;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
To token or not to token: A Comparative Study of Text Representations for Cross-Lingual Transfer. (arXiv:2310.08078v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#38024;&#23545;&#36328;&#35821;&#35328;&#36716;&#25442;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#26696;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#21457;&#29616;&#22270;&#20687;&#27169;&#22411;&#22312;&#30456;&#20851;&#19988;&#33050;&#26412;&#30456;&#20284;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#36716;&#25442;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32780;&#22522;&#20110;&#20998;&#21106;&#30340;&#27169;&#22411;&#22312;&#20559;&#21521;&#21333;&#35789;&#21547;&#20041;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#36328;&#35821;&#35328;&#36716;&#25442;&#20013;&#65292;&#36873;&#25321;&#36866;&#24403;&#30340;&#35789;&#20803;&#21270;&#26041;&#26696;&#24448;&#24448;&#26159;&#19968;&#20010;&#29942;&#39048;&#12290;&#20026;&#20102;&#29702;&#35299;&#25991;&#26412;&#34920;&#31034;&#36873;&#25321;&#30340;&#19979;&#28216;&#24433;&#21709;&#65292;&#25105;&#20204;&#23545;&#20855;&#26377;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#27169;&#24577;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#21253;&#25324;2&#20010;&#22522;&#20110;&#20998;&#21106;&#30340;&#27169;&#22411;&#65288;BERT&#65292;mBERT&#65289;&#65292;1&#20010;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#65288;PIXEL&#65289;&#65292;&#21644;1&#20010;&#23383;&#31526;&#32423;&#27169;&#22411;&#65288;CANINE&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20998;&#35821;&#35328;&#21830;&#25968;&#65288;LQ&#65289;&#25351;&#26631;&#65292;&#33021;&#22815;&#25552;&#20379;&#38646;&#23556;&#20987;&#21644;&#23569;&#23556;&#20987;&#35780;&#20272;&#30340;&#21152;&#26435;&#34920;&#31034;&#12290;&#21033;&#29992;&#36825;&#20010;&#25351;&#26631;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#20219;&#21153;&#65288;&#35789;&#24615;&#26631;&#27880;&#65292;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#19978;&#36827;&#34892;&#20102;&#21253;&#21547;19&#20010;&#28304;&#35821;&#35328;&#21644;133&#20010;&#30446;&#26631;&#35821;&#35328;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#35821;&#35328;&#20043;&#38388;&#20851;&#31995;&#23494;&#20999;&#19988;&#20855;&#26377;&#30456;&#20284;&#30340;&#35270;&#35273;&#33050;&#26412;&#26102;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#25442;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20559;&#21521;&#20110;&#21333;&#35789;&#21547;&#20041;&#30340;&#20219;&#21153;&#65288;&#35789;&#24615;&#26631;&#27880;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#65292;&#22522;&#20110;&#20998;&#21106;&#30340;&#27169;&#22411;&#35777;&#26126;&#26159;&#26356;&#22909;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choosing an appropriate tokenization scheme is often a bottleneck in low-resource cross-lingual transfer. To understand the downstream implications of text representation choices, we perform a comparative analysis on language models having diverse text representation modalities including 2 segmentation-based models (\texttt{BERT}, \texttt{mBERT}), 1 image-based model (\texttt{PIXEL}), and 1 character-level model (\texttt{CANINE}). First, we propose a scoring Language Quotient (LQ) metric capable of providing a weighted representation of both zero-shot and few-shot evaluation combined. Utilizing this metric, we perform experiments comprising 19 source languages and 133 target languages on three tasks (POS tagging, Dependency parsing, and NER). Our analysis reveals that image-based models excel in cross-lingual transfer when languages are closely related and share visually similar scripts. However, for tasks biased toward word meaning (POS, NER), segmentation-based models prove to be sup
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#23545;&#25239;&#24615;&#21098;&#26525;&#26041;&#27861;&#65292;&#21457;&#29616;&#20854;&#40065;&#26834;&#24615;&#34987;&#39640;&#20272;&#12290;&#21098;&#26525;&#21518;&#65292;&#25509;&#36817;&#26410;&#21098;&#26525;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#36890;&#24120;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#26410;&#26469;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#23545;&#25239;&#24615;&#21098;&#26525;&#26041;&#27861;&#20855;&#26377;&#25351;&#23548;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.08073</link><description>&lt;p&gt;
&#34180;&#20912;&#26679;&#26412;&#65306;&#37325;&#26032;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural Networks. (arXiv:2310.08073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;&#23545;&#25239;&#24615;&#21098;&#26525;&#26041;&#27861;&#65292;&#21457;&#29616;&#20854;&#40065;&#26834;&#24615;&#34987;&#39640;&#20272;&#12290;&#21098;&#26525;&#21518;&#65292;&#25509;&#36817;&#26410;&#21098;&#26525;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#36890;&#24120;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#26410;&#26469;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#23545;&#25239;&#24615;&#21098;&#26525;&#26041;&#27861;&#20855;&#26377;&#25351;&#23548;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20943;&#23567;&#32593;&#32476;&#22823;&#23567;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#22686;&#21152;&#31232;&#30095;&#24230;&#26469;&#25442;&#21462;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#65292;&#23545;&#25239;&#24615;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#22312;&#20135;&#29983;&#31232;&#30095;&#32593;&#32476;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#35780;&#20272;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#21098;&#26525;&#26041;&#27861;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#30340;&#30830;&#34987;&#39640;&#20272;&#20102;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21098;&#26525;&#21644;&#23494;&#38598;&#27169;&#22411;&#30340;&#29256;&#26412;&#65292;&#21457;&#29616;&#22312;&#21098;&#26525;&#20043;&#21518;&#65292;&#25509;&#36817;&#26410;&#21098;&#26525;&#27169;&#22411;&#20915;&#31574;&#36793;&#30028;&#30340;&#26679;&#26412;&#36890;&#24120;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#30452;&#35273;&#21487;&#33021;&#22914;&#20309;&#22312;&#26410;&#26469;&#30340;&#24037;&#20316;&#20013;&#23548;&#33268;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#23545;&#25239;&#24615;&#21098;&#26525;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning has shown to be an effective technique for reducing the network size, trading desirable properties like generalization and robustness to adversarial attacks for higher sparsity. Recent work has claimed that adversarial pruning methods can produce sparse networks while also preserving robustness to adversarial examples. In this work, we first re-evaluate three state-of-the-art adversarial pruning methods, showing that their robustness was indeed overestimated. We then compare pruned and dense versions of the same models, discovering that samples on thin ice, i.e., closer to the unpruned model's decision boundary, are typically misclassified after pruning. We conclude by discussing how this intuition may lead to designing more effective adversarial pruning methods in future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Transferable Conceptual Prototype Learning&#65288;TCPL&#65289;&#30340;&#21487;&#36801;&#31227;&#27010;&#24565;&#21407;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#21516;&#26102;&#35299;&#37322;&#21644;&#25913;&#36827;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.08071</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#27010;&#24565;&#21407;&#22411;&#20197;&#35299;&#37322;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learning Transferable Conceptual Prototypes for Interpretable Unsupervised Domain Adaptation. (arXiv:2310.08071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Transferable Conceptual Prototype Learning&#65288;TCPL&#65289;&#30340;&#21487;&#36801;&#31227;&#27010;&#24565;&#21407;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#21516;&#26102;&#35299;&#37322;&#21644;&#25913;&#36827;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#65288;UDA&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#30340;UDA&#27169;&#22411;&#19981;&#36879;&#26126;&#65292;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#37322;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#38656;&#35201;&#23433;&#20840;&#21644;&#21487;&#25511;&#27169;&#22411;&#20915;&#31574;&#30340;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#30446;&#21069;&#65292;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#35774;&#35745;&#20855;&#26377;&#20805;&#20998;&#25968;&#25454;&#27880;&#37322;&#30340;&#28145;&#24230;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#32771;&#34385;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;UDA&#26041;&#27861;&#26159;&#20107;&#21518;&#26041;&#27861;&#65292;&#19981;&#33021;&#20419;&#36827;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20197;&#25552;&#21319;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Transferable Conceptual Prototype Learning&#65288;TCPL&#65289;&#30340;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#35299;&#37322;&#21644;&#25913;&#36827;UDA&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#21407;&#22411;&#27169;&#22359;&#65292;&#20174;&#28304;&#22495;&#20256;&#36755;&#20998;&#31867;&#22522;&#26412;&#27010;&#24565;&#21040;&#30446;&#26631;&#22495;&#24182;&#23398;&#20064;&#39046;&#22495;&#20849;&#20139;&#30340;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great progress of unsupervised domain adaptation (UDA) with the deep neural networks, current UDA models are opaque and cannot provide promising explanations, limiting their applications in the scenarios that require safe and controllable model decisions. At present, a surge of work focuses on designing deep interpretable methods with adequate data annotations and only a few methods consider the distributional shift problem. Most existing interpretable UDA methods are post-hoc ones, which cannot facilitate the model learning process for performance enhancement. In this paper, we propose an inherently interpretable method, named Transferable Conceptual Prototype Learning (TCPL), which could simultaneously interpret and improve the processes of knowledge transfer and decision-making in UDA. To achieve this goal, we design a hierarchically prototypical module that transfers categorical basic concepts from the source domain to the target domain and learns domain-shared prototyp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24120;&#25968;&#27425;&#36890;&#34892;&#30340;&#20219;&#20309;&#22855;&#20598;&#23398;&#20064;&#31639;&#27861;&#65292;&#38656;&#35201;&#35201;&#20040;&#937;(n^2)&#30340;&#20869;&#23384;&#22823;&#23567;&#65292;&#35201;&#20040;&#33267;&#23569;&#38656;&#35201;2^&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.08070</link><description>&lt;p&gt;
&#32039;&#23494;&#26102;&#38388;&#31354;&#38388;&#19979;&#23545;&#20110;&#24120;&#25968;&#36890;&#34892;&#23398;&#20064;&#30340;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Tight Time-Space Lower Bounds for Constant-Pass Learning. (arXiv:2310.08070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08070
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#24120;&#25968;&#27425;&#36890;&#34892;&#30340;&#20219;&#20309;&#22855;&#20598;&#23398;&#20064;&#31639;&#27861;&#65292;&#38656;&#35201;&#35201;&#20040;&#937;(n^2)&#30340;&#20869;&#23384;&#22823;&#23567;&#65292;&#35201;&#20040;&#33267;&#23569;&#38656;&#35201;2^&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20182;&#30340;&#31361;&#30772;&#24615;&#35770;&#25991;&#20013;&#65292;Raz&#35777;&#26126;&#20102;&#20219;&#20309;&#22855;&#20598;&#23398;&#20064;&#31639;&#27861;&#35201;&#20040;&#38656;&#35201;&#20108;&#27425;&#20869;&#23384;&#65292;&#35201;&#20040;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#12290;&#38543;&#21518;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#23558;&#27492;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#22823;&#31867;&#23398;&#20064;&#38382;&#39064;&#12290;&#30452;&#21040;&#26368;&#36817;&#65292;&#25152;&#26377;&#36825;&#20123;&#32467;&#26524;&#37117;&#32771;&#34385;&#20102;&#27969;&#24335;&#27169;&#22411;&#20013;&#30340;&#23398;&#20064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#37117;&#26159;&#29420;&#31435;&#32472;&#21046;&#30340;&#65292;&#32780;&#23398;&#20064;&#32773;&#34987;&#20801;&#35768;&#22312;&#26679;&#26412;&#27969;&#19978;&#36827;&#34892;&#21333;&#27425;&#36890;&#34892;&#12290;Garg&#12289;Raz&#21644;Tal&#21017;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#24378;&#30340;&#27169;&#22411;&#65292;&#20801;&#35768;&#23545;&#26679;&#26412;&#27969;&#36827;&#34892;&#22810;&#27425;&#36890;&#34892;&#12290;&#22312;2&#27425;&#36890;&#34892;&#27169;&#22411;&#20013;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#22823;&#23567;&#20026;n&#30340;&#22855;&#20598;&#23398;&#20064;&#38656;&#35201;n^1.5&#30340;&#20869;&#23384;&#22823;&#23567;&#25110;&#32773;&#33267;&#23569;2^(n^0.5)&#20010;&#26679;&#26412;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#23545;&#20110;&#20219;&#24847;&#24120;&#25968;q&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#22312;&#26679;&#26412;&#27969;&#19978;&#36827;&#34892;q&#27425;&#36890;&#34892;&#30340;&#20219;&#20309;&#22855;&#20598;&#23398;&#20064;&#31639;&#27861;&#30340;&#32039;&#23494;&#20869;&#23384;-&#26679;&#26412;&#19979;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#26679;&#30340;&#23398;&#20064;&#32773;&#35201;&#20040;&#38656;&#35201;&#937;(n^2)&#30340;&#20869;&#23384;&#22823;&#23567;&#65292;&#35201;&#20040;&#33267;&#23569;&#38656;&#35201;2^&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In his breakthrough paper, Raz showed that any parity learning algorithm requires either quadratic memory or an exponential number of samples [FOCS'16, JACM'19]. A line of work that followed extended this result to a large class of learning problems. Until recently, all these results considered learning in the streaming model, where each sample is drawn independently, and the learner is allowed a single pass over the stream of samples. Garg, Raz, and Tal [CCC'19] considered a stronger model, allowing multiple passes over the stream. In the $2$-pass model, they showed that learning parities of size $n$ requires either a memory of size $n^{1.5}$ or at least $2^{\sqrt{n}}$ samples. (Their result also generalizes to other learning problems.)  In this work, for any constant $q$, we prove tight memory-sample lower bounds for any parity learning algorithm that makes $q$ passes over the stream of samples. We show that such a learner requires either $\Omega(n^{2})$ memory size or at least $2^{\
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Soft-InfoNCE&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;InfoNCE&#20013;&#25554;&#20837;&#26435;&#37325;&#39033;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#36127;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#30340;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#26410;&#33021;&#21306;&#20998;&#36127;&#26679;&#26412;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08069</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20195;&#30721;&#25628;&#32034;&#20013;&#30340;&#36127;&#26679;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Rethinking Negative Pairs in Code Search. (arXiv:2310.08069v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Soft-InfoNCE&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;InfoNCE&#20013;&#25554;&#20837;&#26435;&#37325;&#39033;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#36127;&#26679;&#26412;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#30340;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#26410;&#33021;&#21306;&#20998;&#36127;&#26679;&#26412;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#25104;&#20026;&#32454;&#21270;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#20197;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#25928;&#29575;&#21644;&#25928;&#26524;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23427;&#23558;&#27491;&#26679;&#26412;&#20195;&#30721;&#29255;&#27573;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#23558;&#19982;&#25628;&#32034;&#26597;&#35810;&#19981;&#30456;&#20851;&#30340;&#36127;&#26679;&#26412;&#25512;&#24320;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;InfoNCE&#26159;&#26368;&#24120;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;InfoNCE&#36127;&#26679;&#26412;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#21487;&#33021;&#20250;&#25439;&#23475;&#20854;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#65306;1&#65289;&#30001;&#20110;&#37325;&#22797;&#65292;&#22823;&#22411;&#20195;&#30721;&#24211;&#20013;&#23384;&#22312;&#34394;&#20551;&#36127;&#26679;&#26412;&#12290;2&#65289;&#26410;&#33021;&#26126;&#30830;&#21306;&#20998;&#36127;&#26679;&#26412;&#30340;&#28508;&#22312;&#30456;&#20851;&#24615;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#24555;&#36895;&#25490;&#24207;&#31639;&#27861;&#26597;&#35810;&#65292;&#20882;&#27873;&#25490;&#24207;&#31639;&#27861;&#31034;&#20363;&#35201;&#27604;&#25991;&#20214;&#20445;&#23384;&#20989;&#25968;&#8220;&#26356;&#36127;&#38754;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;Soft-InfoNCE&#25439;&#22833;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#26435;&#37325;...
&lt;/p&gt;
&lt;p&gt;
Recently, contrastive learning has become a key component in fine-tuning code search models for software development efficiency and effectiveness. It pulls together positive code snippets while pushing negative samples away given search queries. Among contrastive learning, InfoNCE is the most widely used loss function due to its better performance. However, the following problems in negative samples of InfoNCE may deteriorate its representation learning: 1) The existence of false negative samples in large code corpora due to duplications. 2). The failure to explicitly differentiate between the potential relevance of negative samples. As an example, a bubble sorting algorithm example is less ``negative'' than a file saving function for the quick sorting algorithm query. In this paper, we tackle the above problems by proposing a simple yet effective Soft-InfoNCE loss that inserts weight terms into InfoNCE. In our proposed loss function, we apply three methods to estimate the weights of n
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31561;&#21464;Transformer&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#23545;&#25509;&#65292;&#36890;&#36807;&#34701;&#21512;&#37197;&#20307;&#30340;&#22270;&#23618;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;TAMformer&#27169;&#22359;&#23398;&#20064;&#37197;&#20307;&#21644;&#34507;&#30333;&#36136;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#37197;&#20307;&#20301;&#23039;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26041;&#27861;&#29983;&#25104;&#31934;&#28860;&#30340;&#37197;&#20307;&#20301;&#23039;&#12290;</title><link>http://arxiv.org/abs/2310.08061</link><description>&lt;p&gt;
ETDock: &#19968;&#31181;&#26032;&#39062;&#30340;&#31561;&#21464;Transformer&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
ETDock: A Novel Equivariant Transformer for Protein-Ligand Docking. (arXiv:2310.08061v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08061
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31561;&#21464;Transformer&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#23545;&#25509;&#65292;&#36890;&#36807;&#34701;&#21512;&#37197;&#20307;&#30340;&#22270;&#23618;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;TAMformer&#27169;&#22359;&#23398;&#20064;&#37197;&#20307;&#21644;&#34507;&#30333;&#36136;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#37197;&#20307;&#20301;&#23039;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26041;&#27861;&#29983;&#25104;&#31934;&#28860;&#30340;&#37197;&#20307;&#20301;&#23039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#23545;&#25509;&#26159;&#33647;&#29289;&#24320;&#21457;&#20013;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23545;&#25509;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35780;&#20998;&#20989;&#25968;&#65292;&#32780;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23545;&#25509;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#30340;3D&#31354;&#38388;&#20449;&#24687;&#20197;&#21450;&#37197;&#20307;&#30340;&#22270;&#23618;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;-&#37197;&#20307;&#23545;&#25509;&#20301;&#23039;&#39044;&#27979;&#30340;&#31561;&#21464;Transformer&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#22788;&#29702;&#26469;&#34701;&#21512;&#37197;&#20307;&#30340;&#22270;&#23618;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;TAMformer&#27169;&#22359;&#23398;&#20064;&#37197;&#20307;&#21644;&#34507;&#30333;&#36136;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#39044;&#27979;&#30340;&#36317;&#31163;&#30697;&#38453;&#30340;&#36845;&#20195;&#20248;&#21270;&#26041;&#27861;&#26469;&#29983;&#25104;&#31934;&#28860;&#30340;&#37197;&#20307;&#20301;&#23039;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the docking between proteins and ligands is a crucial and challenging task for drug discovery. However, traditional docking methods mainly rely on scoring functions, and deep learning-based docking approaches usually neglect the 3D spatial information of proteins and ligands, as well as the graph-level features of ligands, which limits their performance. To address these limitations, we propose an equivariant transformer neural network for protein-ligand docking pose prediction. Our approach involves the fusion of ligand graph-level features by feature processing, followed by the learning of ligand and protein representations using our proposed TAMformer module. Additionally, we employ an iterative optimization approach based on the predicted distance matrix to generate refined ligand poses. The experimental results on real datasets show that our model can achieve state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#21270;&#21644;&#23884;&#20837;&#32454;&#21270;&#20004;&#20010;&#27493;&#39588;&#36845;&#20195;&#22320;&#25552;&#39640;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08056</link><description>&lt;p&gt;
&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#65306;&#36890;&#36807;&#20449;&#24565;&#20256;&#25773;&#23545;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#20266;&#26631;&#31614;&#21270;&#21644;&#23884;&#20837;&#32454;&#21270;&#20004;&#20010;&#27493;&#39588;&#36845;&#20195;&#22320;&#25552;&#39640;&#26377;&#30417;&#30563;&#23398;&#20064;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#27604;&#20363;&#23398;&#20064;&#65288;LLP&#65289;&#26159;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#38024;&#23545;&#19968;&#32452;&#23454;&#20363;&#65288;&#31216;&#20026;&#21253;&#65289;&#30340;&#32858;&#21512;&#32423;&#21035;&#26631;&#31614;&#21487;&#29992;&#65292;&#24182;&#19988;&#30446;&#30340;&#26159;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#23454;&#20363;&#32423;&#21035;&#19978;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#24191;&#21578;&#21644;&#21307;&#23398;&#31561;&#39046;&#22495;&#30001;&#20110;&#38544;&#31169;&#32771;&#34385;&#32780;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31639;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#30340;&#31532;&#19968;&#27493;&#65288;&#20266;&#26631;&#31614;&#21270;&#65289;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#22522;&#20110;&#20108;&#36827;&#21046;&#23454;&#20363;&#26631;&#31614;&#30340;&#21513;&#24067;&#26031;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#36890;&#36807;&#20197;&#19979;&#32422;&#26463;&#23558;covariate&#20449;&#24687;&#65288;&#21327;&#21464;&#37327;&#20449;&#24687;&#65289;&#21512;&#24182;&#36827;&#21435;&#65306;&#20855;&#26377;&#30456;&#20284;covariates&#30340;&#23454;&#20363;&#24212;&#35813;&#20855;&#26377;&#30456;&#20284;&#30340;&#26631;&#31614;&#65292;&#24182;&#19988;&#36890;&#36807;&#21253;&#32423;&#21035;&#30340;&#32858;&#21512;&#26631;&#31614;&#26469;&#32508;&#21512;covariate&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20449;&#24565;&#20256;&#25773;&#65288;BP&#65289;&#26469;&#36793;&#32536;&#21270;&#21513;&#24067;&#26031;&#20998;&#24067;&#20197;&#33719;&#24471;&#20266;&#26631;&#31614;&#12290;&#22312;&#31532;&#20108;&#27493;&#65288;&#23884;&#20837;&#32454;&#21270;&#65289;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#26631;&#31614;&#20026;&#23398;&#20064;&#22120;&#25552;&#20379;&#30417;&#30563;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22909;&#30340;&#23884;&#20837;&#12290;&#27492;&#21518;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Label Proportions (LLP) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps. For the first step (Pseudo Labeling) in every iteration, we define a Gibbs distribution over binary instance labels that incorporates a) covariate information through the constraint that instances with similar covariates should have similar labels and b) the bag level aggregated label. We then use Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo labels. In the second step (Embedding Refinement), we use the pseudo labels to provide supervision for a learner that yields a better embedding. Further, we iterate on the two steps ag
&lt;/p&gt;</description></item><item><title>LGL-BCI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;EEG&#25968;&#25454;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#25429;&#25417;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;LGL-BCI&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.08051</link><description>&lt;p&gt;
LGL-BCI&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#30340;&#33041;&#26426;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
LGL-BCI: A Lightweight Geometric Learning Framework for Motor Imagery-Based Brain-Computer Interfaces. (arXiv:2310.08051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08051
&lt;/p&gt;
&lt;p&gt;
LGL-BCI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;EEG&#25968;&#25454;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#25429;&#25417;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;LGL-BCI&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#26159;&#19968;&#31181;&#20351;&#29992;&#33041;&#20449;&#21495;&#19982;&#22806;&#37096;&#35774;&#22791;&#36827;&#34892;&#20132;&#20114;&#30340;&#24320;&#21019;&#24615;&#25216;&#26415;&#12290;&#23613;&#31649;&#26377;&#25152;&#36827;&#23637;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#24133;&#24230;&#21644;&#30456;&#20301;&#21464;&#24322;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#26356;&#24555;&#30340;&#25512;&#26029;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LGL-BCI&#26694;&#26550;&#65292;&#37319;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;EEG&#65292;&#29305;&#21035;&#26159;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#31354;&#38388;&#12290;LGL-BCI&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;EEG&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25429;&#25417;&#20102;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;SPD&#30697;&#38453;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25512;&#26029;&#36895;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;LGL-BCI&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#31361;&#20986;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#36816;&#21160;&#24819;&#35937;-&#33041;&#26426;&#25509;&#21475;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-Computer Interfaces (BCIs) are a groundbreaking technology for interacting with external devices using brain signals. Despite advancements, electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like amplitude and phase variability, and complex spatial correlations, with a need for smaller model size and faster inference. This study introduces the LGL-BCI framework, employing a Geometric Deep Learning Framework for EEG processing in non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD) Manifold space. LGL-BCI offers robust EEG data representation and captures spatial correlations. We propose an EEG channel selection solution via a feature decomposition algorithm to reduce SPD matrix dimensionality, with a lossless transformation boosting inference speed. Extensive experiments show LGL-BCI's superior accuracy and efficiency compared to current solutions, highlighting geometric deep learning's potential in MI-BCI applications. The effici
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#12290;&#19981;&#21516;&#26550;&#26500;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#25935;&#24863;&#24230;&#26377;&#25152;&#24046;&#24322;&#65292;&#19988;&#19968;&#20123;&#26550;&#26500;&#23637;&#29616;&#20986;&#24179;&#31283;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2310.08049</link><description>&lt;p&gt;
&#25506;&#32034;&#27169;&#22411;&#26550;&#26500;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship Between Model Architecture and In-Context Learning Ability. (arXiv:2310.08049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08049
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#26368;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#12290;&#19981;&#21516;&#26550;&#26500;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#30340;&#25935;&#24863;&#24230;&#26377;&#25152;&#24046;&#24322;&#65292;&#19988;&#19968;&#20123;&#26550;&#26500;&#23637;&#29616;&#20986;&#24179;&#31283;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#26550;&#26500;&#21644;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#20043;&#38388;&#26377;&#20160;&#20040;&#20851;&#32852;&#65311;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21313;&#20116;&#31181;&#27169;&#22411;&#26550;&#26500;&#22312;&#19968;&#22871;&#21512;&#25104;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25152;&#36873;&#30340;&#26550;&#26500;&#20195;&#34920;&#20102;&#21508;&#31181;&#33539;&#24335;&#65292;&#21253;&#25324;&#24490;&#29615;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21464;&#25442;&#22120;&#20197;&#21450;&#26032;&#20852;&#30340;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25152;&#26377;&#32771;&#34385;&#30340;&#26550;&#26500;&#37117;&#33021;&#22815;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#24615;&#22686;&#21152;&#26102;&#65292;&#24403;&#20195;&#26550;&#26500;&#34920;&#29616;&#26368;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21518;&#32493;&#23454;&#39564;&#25506;&#32034;&#20102;&#19968;&#20123;&#24433;&#21709;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19981;&#21516;&#26550;&#26500;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#26377;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23545;&#35757;&#32451;&#21160;&#24577;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#26576;&#20123;&#26550;&#26500;&#21576;&#29616;&#20986;&#24179;&#31283;&#12289;&#28176;&#36827;&#30340;&#23398;&#20064;&#36712;&#36857;&#65292;&#32780;...
&lt;/p&gt;
&lt;p&gt;
What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps towards answering this question. In particular, we evaluate fifteen model architectures across a suite of synthetic in-context learning tasks. The selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, and emerging attention alternatives. We discover that all considered architectures can perform in-context learning under certain conditions. However, contemporary architectures are found to be the best performing, especially as task complexity grows. Additionally, our follow-up experiments delve into various factors that influence in-context learning. We observe varied sensitivities among architectures with respect to hyperparameter settings. Our study of training dynamics reveals that certain architectures exhibit a smooth, progressive learning trajectory, while 
&lt;/p&gt;</description></item><item><title>QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08041</link><description>&lt;p&gt;
QLLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#39640;&#25928;&#20302;&#20301;&#23485;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08041
&lt;/p&gt;
&lt;p&gt;
QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#25152;&#38656;&#36164;&#28304;&#36807;&#22823;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;Quantization-Aware Training&#65292;QAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;Post-Training Quantization&#65292;PTQ&#65289;&#25104;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26356;&#23454;&#38469;&#30340;&#26041;&#27861;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#29305;&#23450;&#36890;&#36947;&#20013;&#30340;&#28608;&#27963;&#31163;&#32676;&#20540;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#21518;&#35757;&#32451;&#37327;&#21270;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QLLM&#65292;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;QLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#36890;&#36947;&#25286;&#20998;&#21644;&#36890;&#36947;&#32452;&#35013;&#65292;&#22312;&#20445;&#35777;&#20302;&#20301;&#23485;&#30340;&#24773;&#20917;&#19979;&#23558;&#31163;&#32676;&#36890;&#36947;&#20998;&#35299;&#25104;&#22810;&#20010;&#23376;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#31163;&#22495;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21644;&#25506;&#32034;&#65292;&#20197;&#25552;&#39640;&#31163;&#22495;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08040</link><description>&lt;p&gt;
SEE-OoD: &#22686;&#24378;&#22411;&#31163;&#22495;&#26816;&#27979;&#30340;&#26377;&#30417;&#30563;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
SEE-OoD: Supervised Exploration For Enhanced Out-of-Distribution Detection. (arXiv:2310.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#26377;&#38480;&#30340;&#31163;&#22495;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21644;&#25506;&#32034;&#65292;&#20197;&#25552;&#39640;&#31163;&#22495;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#31163;&#22495;&#26816;&#27979;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24182;&#22312;&#35757;&#32451;&#38454;&#27573;&#36890;&#36807;&#27169;&#22411;&#27491;&#21017;&#21270;&#26469;&#20351;&#29992;&#30495;&#23454;&#25110;&#21512;&#25104;&#30340;&#31163;&#22495;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#30495;&#23454;&#31163;&#22495;&#26679;&#26412;&#30340;&#26041;&#27861;&#32570;&#20047;&#25506;&#32034;&#65292;&#23481;&#26131;&#36807;&#25311;&#21512;&#25163;&#22836;&#30340;&#31163;&#22495;&#26679;&#26412;&#12290;&#32780;&#21512;&#25104;&#26679;&#26412;&#36890;&#24120;&#26159;&#22522;&#20110;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#29983;&#25104;&#30340;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#21644;&#31163;&#22495;&#25968;&#25454;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#39640;&#24230;&#37325;&#21472;&#26102;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#20998;&#25968;&#30340;&#29983;&#25104;&#23545;&#25239;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#31163;&#22495;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#26159;&#39318;&#27425;&#22312;&#26377;&#38480;&#31163;&#22495;&#26679;&#26412;&#30340;&#30417;&#30563;&#19979;&#21516;&#26102;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21644;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29983;&#25104;&#22120;&#21033;&#29992;&#37492;&#21035;&#22120;&#30340;&#21453;&#39304;&#26469;&#25506;&#32034;&#31163;&#22495;&#31354;&#38388;&#24182;&#29983;&#25104;&#21512;&#25104;&#30340;&#31163;&#22495;&#26679;&#26412;&#65292;&#32780;&#37492;&#21035;&#22120;&#21017;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#21644;&#21512;&#25104;&#30340;&#26679;&#26412;&#26469;&#36827;&#34892;&#31163;&#22495;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current techniques for Out-of-Distribution (OoD) detection predominantly rely on quantifying predictive uncertainty and incorporating model regularization during the training phase, using either real or synthetic OoD samples. However, methods that utilize real OoD samples lack exploration and are prone to overfit the OoD samples at hand. Whereas synthetic samples are often generated based on features extracted from training data, rendering them less effective when the training and OoD data are highly overlapped in the feature space. In this work, we propose a Wasserstein-score-based generative adversarial training scheme to enhance OoD detection accuracy, which, for the first time, performs data augmentation and exploration simultaneously under the supervision of limited OoD samples. Specifically, the generator explores OoD spaces and generates synthetic OoD samples using feedback from the discriminator, while the discriminator exploits both the observed and synthesized samples for OoD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#39044;&#25490;&#24207;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;&#21644;&#32454;&#31890;&#24230;&#31070;&#32463;&#32467;&#26500;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#39044;&#25490;&#24207;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08039</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#39044;&#25490;&#24207;&#31995;&#32479;&#65306;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models. (arXiv:2310.08039v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#22823;&#35268;&#27169;&#39044;&#25490;&#24207;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;&#21644;&#32454;&#31890;&#24230;&#31070;&#32463;&#32467;&#26500;&#26469;&#35299;&#20915;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#39044;&#25490;&#24207;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#31995;&#32479;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#65292;&#24050;&#24191;&#27867;&#37197;&#22791;&#20102;&#22810;&#38454;&#27573;&#26550;&#26500;&#65292;&#21253;&#25324;&#21305;&#37197;&#12289;&#39044;&#25490;&#24207;&#12289;&#25490;&#24207;&#21644;&#20877;&#25490;&#24207;&#12290;&#20316;&#20026;&#21305;&#37197;&#21644;&#25490;&#24207;&#20043;&#38388;&#30340;&#20851;&#38190;&#26725;&#26753;&#65292;&#29616;&#26377;&#30340;&#39044;&#25490;&#24207;&#26041;&#27861;&#20027;&#35201;&#24573;&#35270;&#20102;&#25972;&#20010;&#38142;&#36335;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#65292;&#23548;&#33268;&#23376;&#20248;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25972;&#20307;&#26679;&#26412;&#31354;&#38388;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#39044;&#25490;&#24207;&#31995;&#32479;&#65292;&#24182;&#25552;&#20986;&#20102;&#25972;&#20307;&#38142;&#36335;&#36328;&#22495;&#27169;&#22411;&#65288;ECM&#65289;&#65292;&#21033;&#29992;&#25972;&#20010;&#32423;&#32852;&#38454;&#27573;&#30340;&#26679;&#26412;&#26469;&#26377;&#25928;&#20943;&#36731;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#65288;SSB&#65289;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#31070;&#32463;&#32467;&#26500;&#65292;&#21517;&#20026;ECMM&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#39044;&#25490;&#24207;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#22495;&#22810;&#22612;&#31070;&#32463;&#32593;&#32476;&#26469;&#32508;&#21512;&#39044;&#27979;&#27599;&#20010;&#38454;&#27573;&#30340;&#32467;&#26524;&#65292;&#24182;&#24341;&#20837;$L0$&#27491;&#21017;&#21270;&#30340;&#23376;&#32593;&#32476;&#36335;&#30001;&#31574;&#30053;&#26469;&#35843;&#25972;&#27599;&#20010;&#38454;&#27573;&#30340;&#36129;&#29486;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial systems such as recommender systems and online advertising, have been widely equipped with multi-stage architectures, which are divided into several cascaded modules, including matching, pre-ranking, ranking and re-ranking. As a critical bridge between matching and ranking, existing pre-ranking approaches mainly endure sample selection bias (SSB) problem owing to ignoring the entire-chain data dependence, resulting in sub-optimal performances. In this paper, we rethink pre-ranking system from the perspective of the entire sample space, and propose Entire-chain Cross-domain Models (ECM), which leverage samples from the whole cascaded stages to effectively alleviate SSB problem. Besides, we design a fine-grained neural structure named ECMM to further improve the pre-ranking accuracy. Specifically, we propose a cross-domain multi-tower neural network to comprehensively predict for each stage result, and introduce the sub-networking routing strategy with $L0$ regularization to r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Manifold Expansion Replay&#65288;MaER&#65289;&#30340;&#26032;&#22411;&#22238;&#25918;&#31574;&#30053;&#65292;&#36890;&#36807;&#25193;&#23637;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#38544;&#24335;&#27969;&#24418;&#26469;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#37319;&#29992;&#36138;&#24515;&#31574;&#30053;&#22686;&#21152;&#32531;&#20914;&#21306;&#20013;&#30693;&#35782;&#34920;&#31034;&#30340;&#27969;&#24418;&#30452;&#24452;&#65292;&#24182;&#20351;&#29992;Wasserstein&#36317;&#31163;&#20316;&#20026;&#36317;&#31163;&#24230;&#37327;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08038</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#25193;&#23637;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning via Manifold Expansion Replay. (arXiv:2310.08038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Manifold Expansion Replay&#65288;MaER&#65289;&#30340;&#26032;&#22411;&#22238;&#25918;&#31574;&#30053;&#65292;&#36890;&#36807;&#25193;&#23637;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#38544;&#24335;&#27969;&#24418;&#26469;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#37319;&#29992;&#36138;&#24515;&#31574;&#30053;&#22686;&#21152;&#32531;&#20914;&#21306;&#20013;&#30693;&#35782;&#34920;&#31034;&#30340;&#27969;&#24418;&#30452;&#24452;&#65292;&#24182;&#20351;&#29992;Wasserstein&#36317;&#31163;&#20316;&#20026;&#36317;&#31163;&#24230;&#37327;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#23398;&#20064;&#32773;&#25353;&#39034;&#24207;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#21482;&#33719;&#21462;&#19968;&#27425;&#25968;&#25454;&#12290;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#25345;&#32493;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#23569;&#36951;&#24536;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;&#22238;&#25918;&#30340;&#26041;&#27861;&#20351;&#29992;&#24773;&#22659;&#35760;&#24518;&#26469;&#37325;&#26032;&#25773;&#25918;&#20808;&#21069;&#20219;&#21153;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#36827;&#34892;&#30693;&#35782;&#25972;&#21512;&#30340;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#26087;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#65292;&#36825;&#31181;&#31574;&#30053;&#20063;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Manifold Expansion Replay&#65288;MaER&#65289;&#30340;&#26032;&#22411;&#22238;&#25918;&#31574;&#30053;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#24773;&#22659;&#35760;&#24518;&#20013;&#25193;&#23637;&#30693;&#35782;&#34920;&#31034;&#30340;&#38544;&#24335;&#27969;&#24418;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31574;&#30053;&#65292;&#22312;&#20869;&#23384;&#31649;&#29702;&#36807;&#31243;&#20013;&#65292;&#19981;&#26029;&#22686;&#21152;&#30001;&#32531;&#20914;&#21306;&#20013;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;&#38544;&#24335;&#27969;&#24418;&#30340;&#30452;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;Wasserstein&#36317;&#31163;&#24341;&#20837;&#26367;&#20195;&#20132;&#21449;&#29109;&#20316;&#20026;&#36317;&#31163;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning, the learner learns multiple tasks in sequence, with data being acquired only once for each task. Catastrophic forgetting is a major challenge to continual learning. To reduce forgetting, some existing rehearsal-based methods use episodic memory to replay samples of previous tasks. However, in the process of knowledge integration when learning a new task, this strategy also suffers from catastrophic forgetting due to an imbalance between old and new knowledge. To address this problem, we propose a novel replay strategy called Manifold Expansion Replay (MaER). We argue that expanding the implicit manifold of the knowledge representation in the episodic memory helps to improve the robustness and expressiveness of the model. To this end, we propose a greedy strategy to keep increasing the diameter of the implicit manifold represented by the knowledge in the buffer during memory management. In addition, we introduce Wasserstein distance instead of cross entropy as dis
&lt;/p&gt;</description></item><item><title>ZEST&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#29305;&#24449;&#24182;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;ZEST&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.08036</link><description>&lt;p&gt;
ZEST:&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device Classification. (arXiv:2310.08036v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08036
&lt;/p&gt;
&lt;p&gt;
ZEST&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#29305;&#24449;&#24182;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;ZEST&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#25552;&#20986;&#20102;&#29992;&#20110;&#20998;&#31867;&#19982;&#32593;&#32476;&#36830;&#25509;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#65292;&#30001;&#20110;&#27809;&#26377;&#25152;&#26377;&#35774;&#22791;&#65288;&#22240;&#27492;&#27809;&#26377;&#23427;&#20204;&#30340;&#27969;&#37327;&#65289;&#65292;&#20173;&#28982;&#23384;&#22312;&#26080;&#27861;&#20351;&#29992;&#25152;&#26377;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#25805;&#20316;&#38454;&#27573;&#38656;&#35201;&#23545;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#30340;&#26032;&#35774;&#22791;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;ZEST&#65292;&#29992;&#20110;&#20998;&#31867;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#35774;&#22791;&#12290;ZEST&#21253;&#25324;&#65306;i&#65289;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#32593;&#32476;&#29305;&#24449;&#25552;&#21462;&#22120;SANE&#65292;&#29992;&#20110;&#25552;&#21462;&#29289;&#32852;&#32593;&#27969;&#37327;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#65307;ii&#65289;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#28508;&#22312;&#29305;&#24449;&#35757;&#32451;&#35299;&#30721;&#22120;&#29983;&#25104;&#20266;&#25968;&#25454;&#65307;iii&#65289;&#19968;&#20010;&#30417;&#30563;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#20266;&#25968;&#25454;&#36827;&#34892;&#35774;&#22791;&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;&#30495;&#23454;&#30340;&#29289;&#32852;&#32593;&#27969;&#37327;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ZEST&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research works have proposed machine learning models for classifying IoT devices connected to a network. However, there is still a practical challenge of not having all devices (and hence their traffic) available during the training of a model. This essentially means, during the operational phase, we need to classify new devices not seen during the training phase. To address this challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based on self-attention for classifying both seen and unseen devices. ZEST consists of i) a self-attention based network feature extractor, termed SANE, for extracting latent space representations of IoT traffic, ii) a generative model that trains a decoder using latent features to generate pseudo data, and iii) a supervised model that is trained on the generated pseudo data for classifying devices. We carry out extensive experiments on real IoT traffic data; our experiments demonstrate i) ZEST achieves significant improvement (in terms 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#33410;&#28857;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#33410;&#28857;&#20449;&#24687;&#30340;&#23616;&#37096;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#20102;&#23558;&#22122;&#22768;&#26631;&#31614;&#32435;&#20837;&#23616;&#37096;&#22270;&#32858;&#31867;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.08031</link><description>&lt;p&gt;
&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23616;&#37096;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Local Graph Clustering with Noisy Labels. (arXiv:2310.08031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#33410;&#28857;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#33410;&#28857;&#20449;&#24687;&#30340;&#23616;&#37096;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#25506;&#31350;&#20102;&#23558;&#22122;&#22768;&#26631;&#31614;&#32435;&#20837;&#23616;&#37096;&#22270;&#32858;&#31867;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#23545;&#20110;&#24102;&#26377;&#39069;&#22806;&#33410;&#28857;&#20449;&#24687;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#26631;&#31614;&#65289;&#30340;&#22270;&#24418;&#30340;&#22686;&#21152;&#20852;&#36259;&#65292;&#20419;&#20351;&#20102;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#36164;&#28304;&#22788;&#29702;&#25972;&#20010;&#22270;&#24418;&#30340;&#26041;&#27861;&#30340;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20174;&#36825;&#26679;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#24555;&#36895;&#23616;&#37096;&#26041;&#27861;&#65288;&#21363;&#19981;&#38656;&#35201;&#35775;&#38382;&#25972;&#20010;&#22270;&#24418;&#65289;&#30340;&#21457;&#23637;&#36824;&#24456;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22122;&#22768;&#33410;&#28857;&#26631;&#31614;&#20316;&#20026;&#39069;&#22806;&#33410;&#28857;&#20449;&#24687;&#30340;&#23616;&#37096;&#22270;&#32858;&#31867;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#33410;&#28857;&#26681;&#25454;&#25152;&#23646;&#31751;&#30340;&#32852;&#23646;&#20851;&#31995;&#25509;&#25910;&#21021;&#22987;&#20108;&#36827;&#21046;&#26631;&#31614;&#65306;&#22914;&#26524;&#23427;&#20204;&#23646;&#20110;&#30446;&#26631;&#31751;&#65292;&#21017;&#20026;1&#65307;&#21542;&#21017;&#20026;0&#12290;&#38543;&#21518;&#65292;&#36825;&#20123;&#26631;&#31614;&#30340;&#19968;&#37096;&#20998;&#20250;&#34987;&#32763;&#36716;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#22122;&#22768;&#26631;&#31614;&#32435;&#20837;&#23616;&#37096;&#22270;&#32858;&#31867;&#30340;&#22909;&#22788;&#12290;&#36890;&#36807;&#26500;&#24314;&#24102;&#26377;&#36825;&#20123;&#26631;&#31614;&#30340;&#21152;&#26435;&#22270;&#24418;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#25193;&#25955;&#30340;&#23616;&#37096;&#32858;&#31867;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#21644;&#21152;&#26435;&#22270;&#24418;&#19978;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#35282;&#24230;&#20986;&#21457;&#65292;
&lt;/p&gt;
&lt;p&gt;
The growing interest in machine learning problems over graphs with additional node information such as texts, images, or labels has popularized methods that require the costly operation of processing the entire graph. Yet, little effort has been made to the development of fast local methods (i.e. without accessing the entire graph) that extract useful information from such data. To that end, we propose a study of local graph clustering using noisy node labels as a proxy for additional node information. In this setting, nodes receive initial binary labels based on cluster affiliation: 1 if they belong to the target cluster and 0 otherwise. Subsequently, a fraction of these labels is flipped. We investigate the benefits of incorporating noisy labels for local graph clustering. By constructing a weighted graph with such labels, we study the performance of graph diffusion-based local clustering method on both the original and the weighted graphs. From a theoretical perspective, we consider
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#30340;&#19968;&#27604;&#29305;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20108;&#20803;&#36845;&#20195;&#30828;&#38408;&#20540;&#21270;&#65288;BIHT&#65289;&#31639;&#27861;&#65292;&#22312;&#22122;&#22768;&#24773;&#20917;&#19979;&#27604;&#30446;&#21069;&#24050;&#30693;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.08019</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#19968;&#27604;&#29305;&#21387;&#32553;&#24863;&#30693;&#19982;&#36845;&#20195;&#30828;&#38408;&#20540;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust 1-bit Compressed Sensing with Iterative Hard Thresholding. (arXiv:2310.08019v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#40065;&#26834;&#30340;&#19968;&#27604;&#29305;&#21387;&#32553;&#24863;&#30693;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20108;&#20803;&#36845;&#20195;&#30828;&#38408;&#20540;&#21270;&#65288;BIHT&#65289;&#31639;&#27861;&#65292;&#22312;&#22122;&#22768;&#24773;&#20917;&#19979;&#27604;&#30446;&#21069;&#24050;&#30693;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#27604;&#29305;&#21387;&#32553;&#24863;&#30693;&#20013;&#65292;&#30446;&#26631;&#26159;&#20174;&#20165;&#26377;&#27491;&#36127;&#31526;&#21495;&#37327;&#21270;&#30340;&#32447;&#24615;&#27979;&#37327;&#20013;&#24674;&#22797;&#19968;&#20010;k-&#31232;&#30095;&#21333;&#20301;&#21521;&#37327;x&#65292;&#20351;&#20854;&#30456;&#23545;&#20110;&#26368;&#23567;&#20108;&#20056;&#35823;&#24046;&#1013;&#65288;&#22312;&#8467;2&#33539;&#25968;&#19979;&#65289;&#20869;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#22122;&#22768;&#30340;&#24773;&#20917;&#65292;&#21363;&#37096;&#20998;&#27979;&#37327;&#20540;&#21487;&#33021;&#34987;&#23545;&#25163;&#32763;&#36716;&#30340;&#24773;&#20917;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#21517;&#20026;&#20108;&#20803;&#36845;&#20195;&#30828;&#38408;&#20540;&#21270;&#65288;Binary Iterative Hard Thresholding&#65292;BIHT&#65289;&#30340;&#31639;&#27861;&#65292;&#22312;&#36825;&#20010;&#22122;&#22768;&#24773;&#20917;&#19979;&#36827;&#34892;&#19968;&#27604;&#29305;&#21387;&#32553;&#24863;&#30693;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36817;&#20284;O(k/&#1013;)&#20010;&#26080;&#22122;&#22768;&#27979;&#37327;&#65292;BIHT&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#1013;&#35823;&#24046;&#33539;&#22260;&#20869;&#30340;&#20272;&#35745;&#12290;&#36825;&#20010;&#32467;&#26524;&#26159;&#26368;&#20248;&#30340;&#21644;&#36890;&#29992;&#30340;&#65292;&#24847;&#21619;&#30528;&#19968;&#32452;&#27979;&#37327;&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#31232;&#30095;&#21521;&#37327;&#12290;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;BIHT&#31639;&#27861;&#27604;&#30446;&#21069;&#24050;&#30693;&#30340;&#25152;&#26377;&#26041;&#27861;&#37117;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1-bit compressed sensing, the aim is to estimate a $k$-sparse unit vector $x\in S^{n-1}$ within an $\epsilon$ error (in $\ell_2$) from minimal number of linear measurements that are quantized to just their signs, i.e., from measurements of the form $y = \mathrm{Sign}(\langle a, x\rangle).$ In this paper, we study a noisy version where a fraction of the measurements can be flipped, potentially by an adversary. In particular, we analyze the Binary Iterative Hard Thresholding (BIHT) algorithm, a proximal gradient descent on a properly defined loss function used for 1-bit compressed sensing, in this noisy setting. It is known from recent results that, with $\tilde{O}(\frac{k}{\epsilon})$ noiseless measurements, BIHT provides an estimate within $\epsilon$ error. This result is optimal and universal, meaning one set of measurements work for all sparse vectors. In this paper, we show that BIHT also provides better results than all known methods for the noisy setting. We show that when up t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31574;&#30053;&#24615;&#36873;&#25321;&#26679;&#26412;&#65292;&#26368;&#22823;&#21270;&#25915;&#20987;&#25104;&#21151;&#24182;&#26368;&#23567;&#21270;&#24433;&#23376;&#27169;&#22411;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35760;&#24518;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08015</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#35757;&#32451;&#26356;&#22810;&#65311;&#36890;&#36807;&#35760;&#24518;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Train More? Effective and Efficient Membership Inference via Memorization. (arXiv:2310.08015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31574;&#30053;&#24615;&#36873;&#25321;&#26679;&#26412;&#65292;&#26368;&#22823;&#21270;&#25915;&#20987;&#25104;&#21151;&#24182;&#26368;&#23567;&#21270;&#24433;&#23376;&#27169;&#22411;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35760;&#24518;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#26088;&#22312;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#29305;&#23450;&#25968;&#25454;&#26679;&#26412;&#65292;&#20174;&#32780;&#36896;&#25104;&#20005;&#37325;&#30340;&#38544;&#31169;&#20405;&#29359;&#21644;&#20854;&#20182;&#22797;&#26434;&#30340;&#23041;&#32961;&#12290;&#35768;&#22810;&#23454;&#38469;&#30340;&#40657;&#30418;MIAs&#38656;&#35201;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#26597;&#35810;&#35775;&#38382;&#65288;&#19982;&#31169;&#26377;&#25968;&#25454;&#32472;&#21046;&#30340;&#30456;&#21516;&#20998;&#24067;&#65289;&#65292;&#20197;&#35757;&#32451;&#24433;&#23376;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25915;&#20987;&#32773;&#33719;&#24471;&#22312;&#25968;&#25454;&#20998;&#24067;&#20013;&#20351;&#29992;&#25110;&#19981;&#20351;&#29992;&#26679;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#25152;&#32771;&#34385;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;&#25915;&#20987;&#32773;&#36890;&#24120;&#38656;&#35201;&#35757;&#32451;&#36229;&#36807;&#25968;&#30334;&#20010;&#24433;&#23376;&#27169;&#22411;&#26469;&#25552;&#21462;MIAs&#25152;&#38656;&#30340;&#20449;&#21495;&#65292;&#36825;&#25104;&#20026;MIAs&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks (MIAs) aim to identify specific data samples within the private training dataset of machine learning models, leading to serious privacy violations and other sophisticated threats. Many practical black-box MIAs require query access to the data distribution (the same distribution where the private data is drawn) to train shadow models. By doing so, the adversary obtains models trained "with" or "without" samples drawn from the distribution, and analyzes the characteristics of the samples under consideration. The adversary is often required to train more than hundreds of shadow models to extract the signals needed for MIAs; this becomes the computational overhead of MIAs. In this paper, we propose that by strategically choosing the samples, MI adversaries can maximize their attack success while minimizing the number of shadow models. First, our motivational experiments suggest memorization as the key property explaining disparate sample vulnerability to MIAs. 
&lt;/p&gt;</description></item><item><title>AutoFHE&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#25512;&#29702;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#36880;&#23618;&#28151;&#21512;&#27425;&#25968;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12289;&#27425;&#20248;&#36924;&#36817;&#21644;&#38480;&#21046;&#24615;&#35774;&#35745;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.08012</link><description>&lt;p&gt;
AutoFHE: &#29992;&#20110;FHE&#39640;&#25928;&#35780;&#20272;&#30340;CNN&#33258;&#21160;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoFHE: Automated Adaption of CNNs for Efficient Evaluation over FHE. (arXiv:2310.08012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08012
&lt;/p&gt;
&lt;p&gt;
AutoFHE&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#25512;&#29702;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#36880;&#23618;&#28151;&#21512;&#27425;&#25968;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12289;&#27425;&#20248;&#36924;&#36817;&#21644;&#38480;&#21046;&#24615;&#35774;&#35745;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;RNS-CKKS&#19979;&#65292;&#23545;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#23433;&#20840;&#25512;&#29702;&#65292;&#38656;&#35201;&#23545;&#19981;&#25903;&#25345;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#22810;&#39033;&#24335;&#36924;&#36817;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;1&#65289;&#19981;&#28789;&#27963;&#65306;&#22810;&#39033;&#24335;&#36924;&#36817;&#21644;&#30456;&#20851;&#30340;&#21516;&#24577;&#35780;&#20272;&#26550;&#26500;&#26159;&#38024;&#23545;&#27599;&#20010;CNN&#26550;&#26500;&#25163;&#21160;&#23450;&#21046;&#30340;&#65292;&#24182;&#19988;&#26080;&#27861;&#25512;&#24191;&#21040;&#20854;&#20182;&#32593;&#32476;&#12290;2&#65289;&#27425;&#20248;&#36924;&#36817;&#65306;&#23545;&#20110;CNN&#34920;&#31034;&#30340;&#27599;&#20010;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#36817;&#20284;&#65292;&#32780;&#19981;&#26159;&#36817;&#20284;&#25972;&#20010;&#20989;&#25968;&#12290;3&#65289;&#38480;&#21046;&#24615;&#35774;&#35745;&#65306;&#20351;&#29992;&#39640;&#27425;&#25110;&#20302;&#27425;&#22810;&#39033;&#24335;&#36924;&#36817;&#12290;&#21069;&#32773;&#20445;&#30041;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#30001;&#20110;&#24341;&#23548;&#25805;&#20316;&#32780;&#20943;&#24930;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#21518;&#32773;&#21152;&#24555;&#20102;&#23494;&#25991;&#25512;&#29702;&#65292;&#20294;&#25439;&#23475;&#20102;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoFHE&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;RNS-CKKS&#19979;&#26631;&#20934;CNN&#36827;&#34892;&#23433;&#20840;&#25512;&#29702;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#37319;&#29992;&#36880;&#23618;&#28151;&#21512;&#27425;&#25968;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Secure inference of deep convolutional neural networks (CNNs) under RNS-CKKS involves polynomial approximation of unsupported non-linear activation functions. However, existing approaches have three main limitations: 1) Inflexibility: The polynomial approximation and associated homomorphic evaluation architecture are customized manually for each CNN architecture and do not generalize to other networks. 2) Suboptimal Approximation: Each activation function is approximated instead of the function represented by the CNN. 3) Restricted Design: Either high-degree or low-degree polynomial approximations are used. The former retains high accuracy but slows down inference due to bootstrapping operations, while the latter accelerates ciphertext inference but compromises accuracy. To address these limitations, we present AutoFHE, which automatically adapts standard CNNs for secure inference under RNS-CKKS. The key idea is to adopt layerwise mixed-degree polynomial activation functions, which are
&lt;/p&gt;</description></item><item><title>LEMON&#26159;&#19968;&#31181;&#26080;&#25439;&#27169;&#22411;&#25193;&#23637;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#23567;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#20855;&#26377;&#36890;&#29992;&#24615;&#36866;&#29992;&#20110;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2310.07999</link><description>&lt;p&gt;
LEMON&#65306;&#26080;&#25439;&#27169;&#22411;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
LEMON: Lossless model expansion. (arXiv:2310.07999v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07999
&lt;/p&gt;
&lt;p&gt;
LEMON&#26159;&#19968;&#31181;&#26080;&#25439;&#27169;&#22411;&#25193;&#23637;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#23567;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#20855;&#26377;&#36890;&#29992;&#24615;&#36866;&#29992;&#20110;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#29305;&#21035;&#26159;Transformer&#65289;&#30340;&#25193;&#23637;&#23545;&#20110;&#23427;&#20204;&#30340;&#20986;&#33394;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#23548;&#33268;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#22797;&#26434;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#20986;&#29616;&#12290;&#36825;&#31181;&#25193;&#23637;&#36890;&#24120;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#65292;&#32780;&#26080;&#27861;&#21033;&#29992;&#24050;&#26377;&#30340;&#23567;&#22411;&#27169;&#22411;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#36825;&#20123;&#23567;&#22411;&#27169;&#22411;&#24050;&#32463;&#32791;&#36153;&#20102;&#22823;&#37327;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#25439;&#27169;&#22411;&#25193;&#23637;&#65288;LEMON&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#20294;&#24050;&#32463;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#21021;&#22987;&#21270;&#25193;&#23637;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#38376;&#20026;&#25193;&#23637;&#27169;&#22411;&#23450;&#21046;&#30340;&#20248;&#21270;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#19982;&#20174;&#22836;&#35757;&#32451;&#30456;&#27604;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LEMON&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#20860;&#23481;&#65292;&#21253;&#25324;Vision Transformer&#21644;BERT&#31561;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;LEMON&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling of deep neural networks, especially Transformers, is pivotal for their surging performance and has further led to the emergence of sophisticated reasoning capabilities in foundation models. Such scaling generally requires training large models from scratch with random initialization, failing to leverage the knowledge acquired by their smaller counterparts, which are already resource-intensive to obtain. To tackle this inefficiency, we present $\textbf{L}$ossl$\textbf{E}$ss $\textbf{MO}$del Expansio$\textbf{N}$ (LEMON), a recipe to initialize scaled models using the weights of their smaller but pre-trained counterparts. This is followed by model training with an optimized learning rate scheduler tailored explicitly for the scaled models, substantially reducing the training time compared to training from scratch. Notably, LEMON is versatile, ensuring compatibility with various network structures, including models like Vision Transformers and BERT. Our empirical results demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#37325;&#32622;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;"zapping"&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;&#25928;&#26524;&#65292;&#21516;&#26102;&#20855;&#22791;&#31616;&#21333;&#23454;&#26045;&#21644;&#39640;&#25928;&#35745;&#31639;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07996</link><description>&lt;p&gt;
&#37325;&#32622;&#24182;&#24536;&#21364;&#65306;&#37325;&#26032;&#23398;&#20064;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#25913;&#21892;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning. (arXiv:2310.07996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#19968;&#31181;&#37325;&#32622;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;"zapping"&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;&#25928;&#26524;&#65292;&#21516;&#26102;&#20855;&#22791;&#31616;&#21333;&#23454;&#26045;&#21644;&#39640;&#25928;&#35745;&#31639;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26426;&#21046;&#65292;&#33021;&#22815;&#23548;&#33268;&#20855;&#26377;&#26356;&#22909;&#30340;&#25345;&#32493;&#21644;&#36801;&#31227;&#23398;&#20064;&#34920;&#24449;&#12290;&#36825;&#31181;&#26426;&#21046;&#8212;&#8212;&#22312;&#26368;&#21518;&#19968;&#23618;&#26435;&#37325;&#20013;&#21453;&#22797;&#37325;&#32622;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;zapping&#8221;&#8212;&#8212;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#20803;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#22312;&#35768;&#22810;&#19981;&#21516;&#20110;&#20803;&#23398;&#20064;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#20063;&#38750;&#24120;&#36866;&#29992;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#36801;&#31227;&#21040;&#19968;&#32452;&#26032;&#30340;&#31867;&#21035;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;zapping&#36807;&#31243;&#22312;&#26631;&#20934;&#24494;&#35843;&#21644;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#36801;&#31227;&#20934;&#30830;&#24615;&#21644;/&#25110;&#26356;&#24555;&#30340;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#31616;&#21333;&#30340;&#23454;&#26045;&#21644;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;zapping&#21644;&#39034;&#24207;&#23398;&#20064;&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#30340;&#20803;&#23398;&#20064;&#30456;&#24403;&#30340;&#24615;&#33021;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#39640;&#38454;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work identifies a simple pre-training mechanism that leads to representations exhibiting better continual and transfer learning. This mechanism -- the repeated resetting of weights in the last layer, which we nickname "zapping" -- was originally designed for a meta-continual-learning procedure, yet we show it is surprisingly applicable in many settings beyond both meta-learning and continual learning. In our experiments, we wish to transfer a pre-trained image classifier to a new set of classes, in a few shots. We show that our zapping procedure results in improved transfer accuracy and/or more rapid adaptation in both standard fine-tuning and continual learning settings, while being simple to implement and computationally efficient. In many cases, we achieve performance on par with state of the art meta-learning without needing the expensive higher-order gradients, by using a combination of zapping and sequential learning. An intuitive explanation for the effectiveness of this za
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#22635;&#20805;&#38750;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#25968;&#25454;&#21644;&#21442;&#32771;&#20195;&#35874;&#29289;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#22522;&#22240;&#32452;&#20449;&#24687;&#22635;&#20805;&#32570;&#22833;&#30340;&#20195;&#35874;&#32452;&#23398;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.07990</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#38750;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#20013;&#32570;&#22833;&#20540;&#22635;&#20805;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-View Variational Autoencoder for Missing Value Imputation in Untargeted Metabolomics. (arXiv:2310.07990v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#22635;&#20805;&#38750;&#30446;&#26631;&#20195;&#35874;&#32452;&#23398;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#25968;&#25454;&#21644;&#21442;&#32771;&#20195;&#35874;&#29289;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#22522;&#22240;&#32452;&#20449;&#24687;&#22635;&#20805;&#32570;&#22833;&#30340;&#20195;&#35874;&#32452;&#23398;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22312;&#22522;&#20110;&#36136;&#35889;&#30340;&#20195;&#35874;&#32452;&#23398;&#20013;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#65292;&#21487;&#33021;&#23548;&#33268;&#20559;&#20506;&#21644;&#19981;&#23436;&#25972;&#30340;&#20998;&#26512;&#12290;&#23558;&#20840;&#22522;&#22240;&#32452;&#27979;&#24207;&#65288;WGS&#65289;&#25968;&#25454;&#19982;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#25972;&#21512;&#36215;&#26469;&#65292;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#20195;&#35874;&#32452;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#22635;&#20805;&#20934;&#30830;&#24615;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;WGS&#25968;&#25454;&#21644;&#21442;&#32771;&#20195;&#35874;&#29289;&#30340;&#20449;&#24687;&#26469;&#22635;&#20805;&#26410;&#30693;&#20195;&#35874;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22810;&#35270;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20849;&#21516;&#23545;&#36127;&#25285;&#35780;&#20998;&#12289;&#22810;&#22522;&#22240;&#39118;&#38505;&#35780;&#20998;&#65288;PGS&#65289;&#21644;&#36830;&#38145;&#19981;&#24179;&#34913;&#65288;LD&#65289;&#21024;&#20943;&#30340;&#21333;&#26680;&#33527;&#37240;&#22810;&#24577;&#24615;&#65288;SNPs&#65289;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#32570;&#22833;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#30340;&#22635;&#20805;&#12290;&#36890;&#36807;&#23398;&#20064;&#20004;&#31181;&#32452;&#23398;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#22522;&#22240;&#32452;&#20449;&#24687;&#26377;&#25928;&#22320;&#22635;&#20805;&#32570;&#22833;&#30340;&#20195;&#35874;&#32452;&#23398;&#20540;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#20855;&#26377;&#32570;&#22833;&#20540;&#21644;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#23454;&#39564;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Missing data is a common challenge in mass spectrometry-based metabolomics, which can lead to biased and incomplete analyses. The integration of whole-genome sequencing (WGS) data with metabolomics data has emerged as a promising approach to enhance the accuracy of data imputation in metabolomics studies. Method: In this study, we propose a novel method that leverages the information from WGS data and reference metabolites to impute unknown metabolites. Our approach utilizes a multi-view variational autoencoder to jointly model the burden score, polygenetic risk score (PGS), and linkage disequilibrium (LD) pruned single nucleotide polymorphisms (SNPs) for feature extraction and missing metabolomics data imputation. By learning the latent representations of both omics data, our method can effectively impute missing metabolomics values based on genomic information. Results: We evaluate the performance of our method on empirical metabolomics datasets with missing values and de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;6G&#21512;&#20316;&#36890;&#20449;&#30340;&#26032;&#20013;&#32487;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#20256;&#36755;&#35821;&#20041;&#29305;&#24449;&#20943;&#23569;&#36716;&#21457;&#36127;&#36733;&#65292;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20132;&#25442;&#22806;&#22312;&#20449;&#24687;&#26469;&#22686;&#24378;&#35299;&#30721;&#22686;&#30410;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#24694;&#21155;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#65292;&#35813;&#20013;&#32487;&#26694;&#26550;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#24674;&#22797;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.07987</link><description>&lt;p&gt;
&#35821;&#20041;&#21069;&#21521;&#20013;&#32487;&#65306;&#38754;&#21521;6G&#21512;&#20316;&#36890;&#20449;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications. (arXiv:2310.07987v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;6G&#21512;&#20316;&#36890;&#20449;&#30340;&#26032;&#20013;&#32487;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21462;&#20256;&#36755;&#35821;&#20041;&#29305;&#24449;&#20943;&#23569;&#36716;&#21457;&#36127;&#36733;&#65292;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20132;&#25442;&#22806;&#22312;&#20449;&#24687;&#26469;&#22686;&#24378;&#35299;&#30721;&#22686;&#30410;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#24694;&#21155;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#65292;&#35813;&#20013;&#32487;&#26694;&#26550;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#24674;&#22797;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20013;&#32487;&#26694;&#26550;&#65292;&#35821;&#20041;&#21069;&#21521;&#65288;SF&#65289;&#65292;&#29992;&#20110;&#38754;&#21521;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#26080;&#32447;&#32593;&#32476;&#30340;&#21512;&#20316;&#36890;&#20449;&#12290;SF&#20013;&#32487;&#25552;&#21462;&#24182;&#20256;&#36755;&#35821;&#20041;&#29305;&#24449;&#65292;&#20943;&#23569;&#20102;&#36716;&#21457;&#36127;&#36733;&#65292;&#24182;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#20869;&#38142;&#36335;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#20855;&#26377;&#36741;&#21161;&#20449;&#24687;&#30340;&#21512;&#20316;&#36890;&#20449;&#30340;&#29702;&#35770;&#22522;&#30784;&#21644;TURBO&#21407;&#29702;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32852;&#21512;&#28304;&#20449;&#36947;&#32534;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20132;&#25442;&#22806;&#22312;&#20449;&#24687;&#26469;&#22686;&#24378;&#30446;&#30340;&#22320;&#30340;&#35299;&#30721;&#22686;&#30410;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#24694;&#21155;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#65292;SF&#20013;&#32487;&#20173;&#28982;&#21487;&#20197;&#26377;&#25928;&#22320;&#25913;&#21892;&#24674;&#22797;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter proposes a novel relaying framework, semantic-forward (SF), for cooperative communications towards the sixth-generation (6G) wireless networks. The SF relay extracts and transmits the semantic features, which reduces forwarding payload, and also improves the network robustness against intra-link errors. Based on the theoretical basis for cooperative communications with side information and the turbo principle, we design a joint source-channel coding algorithm to iteratively exchange the extrinsic information for enhancing the decoding gains at the destination. Surprisingly, simulation results indicate that even in bad channel conditions, SF relaying can still effectively improve the recovered information quality.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#36890;&#29992;&#24615;&#30340;&#36731;&#32534;&#30721;&#22120;&#21644;&#37325;&#35299;&#30721;&#22120;&#65288;LEHD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#33410;&#28857;&#38388;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#22312;&#22823;&#35268;&#27169;&#23454;&#20363;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#26053;&#34892;&#21830;&#38382;&#39064;&#21644;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07985</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#65306;&#26397;&#30528;&#22823;&#35268;&#27169;&#36890;&#29992;&#21270;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale Generalization. (arXiv:2310.07985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#36890;&#29992;&#24615;&#30340;&#36731;&#32534;&#30721;&#22120;&#21644;&#37325;&#35299;&#30721;&#22120;&#65288;LEHD&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#33410;&#28857;&#38388;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#22312;&#22823;&#35268;&#27169;&#23454;&#20363;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#26053;&#34892;&#21830;&#38382;&#39064;&#21644;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#65288;NCO&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20256;&#32479;&#30340;&#19987;&#23478;&#31639;&#27861;&#35774;&#35745;&#26469;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26500;&#36896;&#24615;&#30340;NCO&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#22823;&#35268;&#27169;&#23454;&#20363;&#22823;&#23567;&#30340;&#38382;&#39064;&#65292;&#36825;&#26174;&#33879;&#38477;&#20302;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#32534;&#30721;&#22120;&#21644;&#37325;&#35299;&#30721;&#22120;&#65288;LEHD&#65289;&#27169;&#22411;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#36890;&#29992;&#24615;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;LEHD&#27169;&#22411;&#21487;&#20197;&#21160;&#24577;&#22320;&#23398;&#20064;&#21040;&#19981;&#21516;&#23610;&#23544;&#30340;&#25152;&#26377;&#21487;&#29992;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#26377;&#21033;&#20110;&#27169;&#22411;&#23545;&#21508;&#31181;&#35268;&#27169;&#30340;&#38382;&#39064;&#36827;&#34892;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;LEHD&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;&#21644;&#28789;&#27963;&#30340;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#26426;&#21046;&#12290;&#36890;&#36807;&#22312;&#23567;&#35268;&#27169;&#38382;&#39064;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;LEHD&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36817;&#20046;&#26368;&#20248;&#30340;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#23481;&#37327;&#38480;&#21046;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Neural combinatorial optimization (NCO) is a promising learning-based approach for solving challenging combinatorial optimization problems without specialized algorithm design by experts. However, most constructive NCO methods cannot solve problems with large-scale instance sizes, which significantly diminishes their usefulness for real-world applications. In this work, we propose a novel Light Encoder and Heavy Decoder (LEHD) model with a strong generalization ability to address this critical issue. The LEHD model can learn to dynamically capture the relationships between all available nodes of varying sizes, which is beneficial for model generalization to problems of various scales. Moreover, we develop a data-efficient training scheme and a flexible solution construction mechanism for the proposed LEHD model. By training on small-scale problem instances, the LEHD model can generate nearly optimal solutions for the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing
&lt;/p&gt;</description></item><item><title>RandCom&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#36807;&#27010;&#29575;&#24615;&#26412;&#22320;&#26356;&#26032;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2310.07983</link><description>&lt;p&gt;
RandCom&#65306;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#29992;&#20110;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization. (arXiv:2310.07983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07983
&lt;/p&gt;
&lt;p&gt;
RandCom&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#36807;&#27010;&#29575;&#24615;&#26412;&#22320;&#26356;&#26032;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#38543;&#26426;&#36890;&#20449;&#36339;&#36807;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#22240;&#20854;&#22312;&#21152;&#36895;&#36890;&#20449;&#22797;&#26434;&#24615;&#26041;&#38754;&#20855;&#26377;&#30340;&#20248;&#21183;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24378;&#20984;&#30830;&#23450;&#24615;&#35774;&#32622;&#30340;&#38598;&#20013;&#24335;&#36890;&#20449;&#21327;&#35758;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RandCom&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#27010;&#29575;&#24615;&#30340;&#26412;&#22320;&#26356;&#26032;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;RandCom&#22312;&#38543;&#26426;&#38750;&#20984;&#12289;&#20984;&#21644;&#24378;&#20984;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#36890;&#36807;&#36890;&#20449;&#27010;&#29575;&#26469;&#28176;&#36817;&#22320;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;RandCom&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#22312;&#38543;&#26426;&#24378;&#20984;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;RandCom&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#20110;&#32593;&#32476;&#30340;&#27493;&#38271;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;RandCom&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#30340;&#28508;&#21147;&#30340;&#31215;&#26497;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed optimization methods with random communication skips are gaining increasing attention due to their proven benefits in accelerating communication complexity. Nevertheless, existing research mainly focuses on centralized communication protocols for strongly convex deterministic settings. In this work, we provide a decentralized optimization method called RandCom, which incorporates probabilistic local updates. We analyze the performance of RandCom in stochastic non-convex, convex, and strongly convex settings and demonstrate its ability to asymptotically reduce communication overhead by the probability of communication. Additionally, we prove that RandCom achieves linear speedup as the number of nodes increases. In stochastic strongly convex settings, we further prove that RandCom can achieve linear speedup with network-independent stepsizes. Moreover, we apply RandCom to federated learning and provide positive results concerning the potential for achieving linear speedup and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#27969;&#25511;&#21046;&#31995;&#32479;&#35843;&#24230;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20223;&#30495;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07981</link><description>&lt;p&gt;
&#29992;&#20110;&#29627;&#29827;&#27969;&#25511;&#21046;&#31995;&#32479;&#20013;&#26174;&#31034;&#36716;&#31227;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#29289;&#29702;&#20223;&#30495;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning of Display Transfer Robots in Glass Flow Control Systems: A Physical Simulation-Based Approach. (arXiv:2310.07981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07981
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#27969;&#25511;&#21046;&#31995;&#32479;&#35843;&#24230;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20223;&#30495;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#25511;&#21046;&#31995;&#32479;&#26159;&#22686;&#21152;&#21046;&#36896;&#31995;&#32479;&#20135;&#33021;&#30340;&#20851;&#38190;&#27010;&#24565;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#27969;&#25511;&#21046;&#30456;&#20851;&#30340;&#35843;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#39046;&#22495;&#20154;&#24037;&#19987;&#23478;&#30340;&#21551;&#21457;&#24335;&#35774;&#35745;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#20351;&#29992;&#30495;&#23454;&#35774;&#22791;&#36827;&#34892;&#26657;&#27491;&#12289;&#30417;&#35270;&#21644;&#39564;&#35777;&#12290;&#38543;&#30528;&#31995;&#32479;&#35774;&#35745;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#30417;&#35270;&#26102;&#38388;&#22686;&#21152;&#65292;&#23548;&#33268;&#36798;&#21040;&#26368;&#20339;&#35774;&#35745;&#30340;&#27010;&#29575;&#38477;&#20302;&#12290;&#20316;&#20026;&#21551;&#21457;&#24335;&#27969;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#32771;&#34385;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#35843;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#22312;&#26576;&#20123;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#32467;&#26524;&#22312;&#23454;&#38469;&#30340;&#26174;&#31034;&#21644;&#21322;&#23548;&#20307;&#21046;&#36896;&#36807;&#31243;&#31561;&#23454;&#38469;&#24037;&#21378;&#33258;&#21160;&#21270;( FAB)&#20013;&#30340;&#36866;&#29992;&#24615;&#23578;&#19981;&#26126;&#30830;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#29289;&#29702;&#20223;&#30495;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A flow control system is a critical concept for increasing the production capacity of manufacturing systems. To solve the scheduling optimization problem related to the flow control with the aim of improving productivity, existing methods depend on a heuristic design by domain human experts. Therefore, the methods require correction, monitoring, and verification by using real equipment. As system designs increase in complexity, the monitoring time increases, which decreases the probability of arriving at the optimal design. As an alternative approach to the heuristic design of flow control systems, the use of deep reinforcement learning to solve the scheduling optimization problem has been considered. Although the existing research on reinforcement learning has yielded excellent performance in some areas, the applicability of the results to actual FAB such as display and semiconductor manufacturing processes is not evident so far. To this end, we propose a method to implement a physica
&lt;/p&gt;</description></item><item><title>GRASP&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#35782;&#21035;&#23376;&#22270;&#26469;&#21152;&#36895;&#26368;&#30701;&#36335;&#24452;&#25915;&#20987;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#36816;&#34892;&#36895;&#24230;&#24555;&#36798;&#21040;10&#20493;&#32780;&#20173;&#20445;&#25345;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.07980</link><description>&lt;p&gt;
GRASP&#65306;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#21152;&#36895;&#26368;&#30701;&#36335;&#24452;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
GRASP: Accelerating Shortest Path Attacks via Graph Attention. (arXiv:2310.07980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07980
&lt;/p&gt;
&lt;p&gt;
GRASP&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#35782;&#21035;&#23376;&#22270;&#26469;&#21152;&#36895;&#26368;&#30701;&#36335;&#24452;&#25915;&#20987;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#36816;&#34892;&#36895;&#24230;&#24555;&#36798;&#21040;10&#20493;&#32780;&#20173;&#20445;&#25345;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#22312;&#36741;&#21161;&#21644;&#21152;&#36895;&#20256;&#32479;&#32452;&#21512;&#20248;&#21270;&#31639;&#27861;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#20197;&#30452;&#25509;&#36755;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#20026;&#30446;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#24448;&#24448;&#20250;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#35299;&#20915;&#36136;&#37327;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#12290;&#22240;&#27492;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#24615;&#33021;&#20445;&#35777;&#30340;&#21516;&#26102;&#21152;&#36895;&#29616;&#26377;&#27714;&#35299;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#22791;&#21463;&#20851;&#27880;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;APX&#22256;&#38590;&#38382;&#39064;&#65292;&#20854;&#20013;&#23545;&#25163;&#36890;&#36807;&#21024;&#38500;&#26368;&#23567;&#25968;&#37327;&#30340;&#36793;&#26469;&#25915;&#20987;&#22270;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GRASP&#31639;&#27861;&#65306;&#22270;&#27880;&#24847;&#21147;&#21152;&#36895;&#26368;&#30701;&#36335;&#24452;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;ML&#36741;&#21161;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#39640;10&#20493;&#30340;&#36816;&#34892;&#26102;&#38388;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;GRASP&#20351;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#35782;&#21035;&#21253;&#21547;&#32452;&#21512;&#35299;&#20915;&#26041;&#26696;&#30340;&#26356;&#23567;&#23376;&#22270;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23567;&#20102;&#36755;&#20837;&#38382;&#39064;&#30340;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#31934;&#30830;&#34920;&#31034;&#36755;&#20837;&#22270;&#30340;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning (ML) have shown promise in aiding and accelerating classical combinatorial optimization algorithms. ML-based speed ups that aim to learn in an end to end manner (i.e., directly output the solution) tend to trade off run time with solution quality. Therefore, solutions that are able to accelerate existing solvers while maintaining their performance guarantees, are of great interest. We consider an APX-hard problem, where an adversary aims to attack shortest paths in a graph by removing the minimum number of edges. We propose the GRASP algorithm: Graph Attention Accelerated Shortest Path Attack, an ML aided optimization algorithm that achieves run times up to 10x faster, while maintaining the quality of solution generated. GRASP uses a graph attention network to identify a smaller subgraph containing the combinatorial solution, thus effectively reducing the input problem size. Additionally, we demonstrate how careful representation of the input graph, 
&lt;/p&gt;</description></item><item><title>&#22270;&#24418;-SCP&#26159;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#21253;&#21547;&#35299;&#31354;&#38388;&#30340;&#36739;&#23567;&#23376;&#38382;&#39064;&#26469;&#25552;&#39640;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#24418;-SCP&#33021;&#22815;&#23558;&#38382;&#39064;&#22823;&#23567;&#20943;&#23569;30-70%&#65292;&#21644;&#21830;&#19994;&#27714;&#35299;&#22120;&#30456;&#27604;&#21152;&#36895;&#39640;&#36798;25&#20493;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#32473;&#23450;&#30340;&#26368;&#20248;&#24615;&#38408;&#20540;&#19979;&#25913;&#36827;&#25110;&#23454;&#29616;100%&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07979</link><description>&lt;p&gt;
&#22270;&#24418;-SCP: &#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks. (arXiv:2310.07979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07979
&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;-SCP&#26159;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#21253;&#21547;&#35299;&#31354;&#38388;&#30340;&#36739;&#23567;&#23376;&#38382;&#39064;&#26469;&#25552;&#39640;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#24418;-SCP&#33021;&#22815;&#23558;&#38382;&#39064;&#22823;&#23567;&#20943;&#23569;30-70%&#65292;&#21644;&#21830;&#19994;&#27714;&#35299;&#22120;&#30456;&#27604;&#21152;&#36895;&#39640;&#36798;25&#20493;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#32473;&#23450;&#30340;&#26368;&#20248;&#24615;&#38408;&#20540;&#19979;&#25913;&#36827;&#25110;&#23454;&#29616;100%&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#21152;&#36895;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;&#65288;SCP&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24418;-SCP&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#21253;&#21547;&#35299;&#31354;&#38388;&#30340;&#22823;&#22823;&#36739;&#23567;&#30340;&#23376;&#38382;&#39064;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#20248;&#21270;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#38382;&#39064;&#29305;&#24449;&#21644;&#22797;&#26434;&#24230;&#30340;&#21512;&#25104;&#21152;&#26435;&#21644;&#38750;&#21152;&#26435;SCP&#23454;&#20363;&#19978;&#35780;&#20272;&#20102;&#22270;&#24418;-SCP&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;OR Library&#30340;&#23454;&#20363;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#26159;SCP&#30340;&#19968;&#20010;&#32463;&#20856;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22270;&#24418;-SCP&#23558;&#38382;&#39064;&#22823;&#23567;&#20943;&#23569;&#20102;30-70%&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#21830;&#19994;&#27714;&#35299;&#22120;&#65288;Gurobi&#65289;&#23454;&#29616;&#20102;&#39640;&#36798;25&#20493;&#30340;&#36816;&#34892;&#26102;&#38388;&#21152;&#36895;&#12290;&#22312;&#32473;&#23450;&#25152;&#38656;&#30340;&#26368;&#20248;&#24615;&#38408;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#22270;&#24418;-SCP&#23558;&#25913;&#36827;&#25110;&#29978;&#33267;&#23454;&#29616;100%&#30340;&#26368;&#20248;&#24615;&#12290;&#36825;&#19982;&#24555;&#36895;&#36138;&#23146;&#35299;&#20915;&#26041;&#26696;&#24418;&#25104;&#20102;&#23545;&#27604;&#65292;&#21518;&#32773;&#22312;&#20445;&#35777;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#30340;&#21516;&#26102;&#26126;&#26174;&#25439;&#23475;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;&#22270;&#24418;-SCP&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#38382;&#39064;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) approaches are increasingly being used to accelerate combinatorial optimization (CO) problems. We look specifically at the Set Cover Problem (SCP) and propose Graph-SCP, a graph neural network method that can augment existing optimization solvers by learning to identify a much smaller sub-problem that contains the solution space. We evaluate the performance of Graph-SCP on synthetic weighted and unweighted SCP instances with diverse problem characteristics and complexities, and on instances from the OR Library, a canonical benchmark for SCP. We show that Graph-SCP reduces the problem size by 30-70% and achieves run time speedups up to~25x when compared to commercial solvers (Gurobi). Given a desired optimality threshold, Graph-SCP will improve upon it or even achieve 100% optimality. This is in contrast to fast greedy solutions that significantly compromise solution quality to achieve guaranteed polynomial run time. Graph-SCP can generalize to larger problem sizes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07972</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#20449;&#24687;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29992;&#20110;&#22797;&#26434;&#20851;&#31995;&#30340;&#26465;&#20214;&#29983;&#25104;&#21644;&#23494;&#24230;&#24314;&#27169;&#65292;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#20851;&#31995;&#30340;&#26412;&#36136;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#20934;&#30830;&#29702;&#35299;&#21333;&#35789;&#21644;&#22270;&#20687;&#37096;&#20998;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25110;&#32773;&#39044;&#27979;&#24178;&#39044;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#31934;&#30830;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#12290;&#20114;&#20449;&#24687;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#31934;&#30830;&#34920;&#36798;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#27169;&#22411;&#26469;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20063;&#21487;&#20197;&#36731;&#26494;&#20272;&#35745;&#22312;&#29305;&#23450;&#22270;&#20687;&#21644;&#26631;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36827;&#19968;&#27493;&#23545;&#20449;&#24687;&#36827;&#34892;&#20998;&#35299;&#65292;&#20197;&#29702;&#35299;&#39640;&#32500;&#31354;&#38388;&#20013;&#21738;&#20123;&#21464;&#37327;&#25658;&#24102;&#20449;&#24687;&#65292;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#38750;&#36127;&#20449;&#24687;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutu
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#20195;&#29702;&#20248;&#21270;&#31639;&#27861;&#20013;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25628;&#32034;&#30340;&#20195;&#29702;&#20248;&#21270;&#26041;&#27861;&#65288;HASSO&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#36229;&#21442;&#25968;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#20195;&#29702;&#20248;&#21270;&#31639;&#27861;&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#25928;&#26524;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.07970</link><description>&lt;p&gt;
&#36229;&#21442;&#25968;&#33258;&#36866;&#24212;&#25628;&#32034;&#29992;&#20110;&#20195;&#29702;&#20248;&#21270;&#65306;&#19968;&#31181;&#33258;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach. (arXiv:2310.07970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07970
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#20195;&#29702;&#20248;&#21270;&#31639;&#27861;&#20013;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25628;&#32034;&#30340;&#20195;&#29702;&#20248;&#21270;&#26041;&#27861;&#65288;HASSO&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#36229;&#21442;&#25968;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#20195;&#29702;&#20248;&#21270;&#31639;&#27861;&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#25928;&#26524;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#20248;&#21270;&#31639;&#27861;&#22312;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#19982;&#37319;&#26679;&#21644;&#20195;&#29702;&#25311;&#21512;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#24456;&#22823;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#26500;&#25104;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#20195;&#29702;&#20248;&#21270;&#31639;&#27861;&#20013;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#21442;&#25968;&#33258;&#36866;&#24212;&#25628;&#32034;&#30340;&#20195;&#29702;&#20248;&#21270;&#26041;&#27861;&#65288;HASSO&#65289;&#12290;HASSO&#19981;&#26159;&#19968;&#20010;&#36229;&#21442;&#25968;&#35843;&#25972;&#31639;&#27861;&#65292;&#32780;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#33258;&#35843;&#25972;&#20195;&#29702;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#22312;&#21516;&#26102;&#20248;&#21270;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#30340;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#33258;&#24049;&#30340;&#36229;&#21442;&#25968;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35780;&#20272;&#12290;&#20854;&#30446;&#26631;&#26159;&#25552;&#39640;&#20195;&#29702;&#20248;&#21270;&#31639;&#27861;&#23545;&#20110;&#20174;&#19994;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#12289;&#25928;&#26524;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35782;&#21035;&#24182;&#20462;&#25913;&#20102;&#27599;&#20010;&#38382;&#39064;&#21644;&#20195;&#29702;&#20248;&#21270;&#26041;&#27861;&#29305;&#23450;&#30340;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#36229;&#21442;&#25968;&#65292;&#20943;&#23569;&#20102;&#25163;&#21160;&#35843;&#25972;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#19981;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#36127;&#25285;&#12290;&#23454;&#39564;&#32467;&#26524;&#28436;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate Optimization (SO) algorithms have shown promise for optimizing expensive black-box functions. However, their performance is heavily influenced by hyperparameters related to sampling and surrogate fitting, which poses a challenge to their widespread adoption. We investigate the impact of hyperparameters on various SO algorithms and propose a Hyperparameter Adaptive Search for SO (HASSO) approach. HASSO is not a hyperparameter tuning algorithm, but a generic self-adjusting SO algorithm that dynamically tunes its own hyperparameters while concurrently optimizing the primary objective function, without requiring additional evaluations. The aim is to improve the accessibility, effectiveness, and convergence speed of SO algorithms for practitioners. Our approach identifies and modifies the most influential hyperparameters specific to each problem and SO approach, reducing the need for manual tuning without significantly increasing the computational burden. Experimental results demo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CleftGAN&#65292;&#21363;&#23558;&#22522;&#20110;&#39118;&#26684;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36866;&#24212;&#20110;&#21019;&#24314;&#23637;&#31034;&#21767;&#35010;&#30072;&#24418;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#22270;&#20687;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21767;&#35010;&#22270;&#20687;&#65292;&#20026;&#38754;&#37096;&#21767;&#35010;&#35780;&#20272;&#31995;&#32479;&#30340;&#35757;&#32451;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.07969</link><description>&lt;p&gt;
CleftGAN: &#23558;&#22522;&#20110;&#39118;&#26684;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36866;&#24212;&#20110;&#21019;&#24314;&#23637;&#31034;&#21767;&#35010;&#30072;&#24418;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
CleftGAN: Adapting A Style-Based Generative Adversarial Network To Create Images Depicting Cleft Lip Deformity. (arXiv:2310.07969v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CleftGAN&#65292;&#21363;&#23558;&#22522;&#20110;&#39118;&#26684;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36866;&#24212;&#20110;&#21019;&#24314;&#23637;&#31034;&#21767;&#35010;&#30072;&#24418;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#22270;&#20687;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21767;&#35010;&#22270;&#20687;&#65292;&#20026;&#38754;&#37096;&#21767;&#35010;&#35780;&#20272;&#31995;&#32479;&#30340;&#35757;&#32451;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#38754;&#37096;&#21767;&#35010;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#22823;&#37327;&#39640;&#36136;&#37327;&#19988;&#32463;&#36947;&#24503;&#22996;&#21592;&#20250;&#25209;&#20934;&#30340;&#24739;&#32773;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21767;&#35010;&#29983;&#25104;&#22120;&#65292;&#26088;&#22312;&#20135;&#29983;&#20986;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;&#21767;&#35010;&#22270;&#20687;&#30340;&#20960;&#20046;&#26080;&#38480;&#25968;&#37327;&#65292;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#36716;&#31227;&#23398;&#20064;&#21327;&#35758;&#65292;&#27979;&#35797;&#20102;&#22522;&#20110;StyleGAN-ADA&#30340;&#19981;&#21516;&#29256;&#26412;&#65288;&#19968;&#31181;&#21253;&#21547;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;(ADA)&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22270;&#20687;&#29983;&#25104;&#22120;&#65289;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;&#25551;&#32472;&#19981;&#21516;&#21767;&#35010;&#30072;&#24418;&#30340;&#35757;&#32451;&#22270;&#20687;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#35843;&#25972;&#20102;&#26059;&#36716;&#12289;&#32553;&#25918;&#12289;&#39068;&#33394;&#35843;&#25972;&#21644;&#32972;&#26223;&#27169;&#31946;&#12290;&#20027;&#35201;&#31639;&#27861;&#30340;ADA&#20462;&#25913;&#20801;&#35768;&#26500;&#24314;&#25105;&#20204;&#30340;&#26032;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#38656;&#35201;&#36755;&#20837;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#22270;&#20687;&#12290;&#36890;&#36807;&#20351;&#29992;514&#24352;&#29420;&#29305;&#30340;&#27491;&#33080;&#21767;&#35010;&#29031;&#29255;&#36827;&#34892;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#39564;&#35777;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major obstacle when attempting to train a machine learning system to evaluate facial clefts is the scarcity of large datasets of high-quality, ethics board-approved patient images. In response, we have built a deep learning-based cleft lip generator designed to produce an almost unlimited number of artificial images exhibiting high-fidelity facsimiles of cleft lip with wide variation. We undertook a transfer learning protocol testing different versions of StyleGAN-ADA (a generative adversarial network image generator incorporating adaptive data augmentation (ADA)) as the base model. Training images depicting a variety of cleft deformities were pre-processed to adjust for rotation, scaling, color adjustment and background blurring. The ADA modification of the primary algorithm permitted construction of our new generative model while requiring input of a relatively small number of training images. Adversarial training was carried out using 514 unique frontal photographs of cleft-affect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CausalVul&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24615;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#25200;&#21160;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#27169;&#22411;&#19981;&#31283;&#23450;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07958</link><description>&lt;p&gt;
&#36808;&#21521;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CausalVul&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24615;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#25200;&#21160;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#27169;&#22411;&#19981;&#31283;&#23450;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#26816;&#27979;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38459;&#30861;&#20854;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#37325;&#35201;&#25361;&#25112;&#26159;&#27169;&#22411;&#22312;&#25200;&#21160;&#19979;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24212;&#29992;&#21040;&#26410;&#35265;&#36807;&#30340;&#39033;&#30446;&#19978;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#38750;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#21464;&#37327;&#21517;&#65292;&#19982;&#26631;&#31614;&#20855;&#26377;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#24403;&#25200;&#21160;&#21644;OOD&#25968;&#25454;&#38598;&#19981;&#20877;&#20855;&#26377;&#30456;&#21516;&#30340;&#34394;&#20551;&#29305;&#24449;&#26102;&#65292;&#27169;&#22411;&#39044;&#27979;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#24615;&#24341;&#20837;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CausalVul&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#25200;&#21160;&#26469;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19978;&#24212;&#29992;&#20102;&#22240;&#26524;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;do-&#35745;&#31639;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to sys
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#20248;&#21270;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#30740;&#31350;&#20102;&#37327;&#21270;&#12289;&#27169;&#22411;&#32553;&#25918;&#21644;&#22810;&#27169;&#24577;&#31561;&#26041;&#27861;&#19982;&#31995;&#32479;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07940</link><description>&lt;p&gt;
&#25104;&#26412;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines. (arXiv:2310.07940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#20248;&#21270;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#30740;&#31350;&#20102;&#37327;&#21270;&#12289;&#27169;&#22411;&#32553;&#25918;&#21644;&#22810;&#27169;&#24577;&#31561;&#26041;&#27861;&#19982;&#31995;&#32479;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23459;&#25196;&#30528;&#30001;&#29289;&#32852;&#32593;&#35774;&#22791;&#65288;&#21253;&#25324;&#26234;&#33021;&#20256;&#24863;&#22120;&#65292;&#23478;&#23621;&#21644;&#22478;&#24066;&#65289;&#25512;&#21160;&#30340;&#26410;&#26469;&#24895;&#26223;&#12290;&#36234;&#26469;&#36234;&#22810;&#22320;&#65292;&#23558;&#26234;&#33021;&#23884;&#20837;&#36825;&#20123;&#35774;&#22791;&#20013;&#28041;&#21450;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23384;&#20648;&#21644;&#22788;&#29702;&#38656;&#27714;&#20351;&#23427;&#20204;&#23545;&#20110;&#24265;&#20215;&#30340;&#29616;&#25104;&#24179;&#21488;&#26469;&#35828;&#26159;&#31105;&#27490;&#30340;&#12290;&#20811;&#26381;&#36825;&#20123;&#35201;&#27714;&#23545;&#20110;&#23454;&#29616;&#24191;&#27867;&#36866;&#29992;&#30340;&#26234;&#33021;&#35774;&#22791;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#20351;&#27169;&#22411;&#21464;&#24471;&#26356;&#23567;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#29305;&#23450;&#22330;&#26223;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#32570;&#20047;&#29702;&#35299;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#23545;&#20110;&#36793;&#32536;&#24179;&#21488;&#65292;&#36825;&#20123;&#36873;&#25321;&#19981;&#33021;&#19982;&#25104;&#26412;&#21644;&#29992;&#25143;&#20307;&#39564;&#30456;&#21106;&#31163;&#22320;&#36827;&#34892;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#30340;&#35282;&#24230;&#20840;&#38754;&#25506;&#32034;&#20102;&#37327;&#21270;&#12289;&#27169;&#22411;&#32553;&#25918;&#21644;&#22810;&#27169;&#24577;&#19982;&#23384;&#20648;&#12289;&#20256;&#24863;&#22120;&#21644;&#22788;&#29702;&#22120;&#31561;&#31995;&#32479;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#20174;&#30828;&#20214;/&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#30340;&#35282;&#24230;&#36827;&#34892;&#65292;&#32771;&#34385;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have long touted a vision of the future enabled by a proliferation of internet-of-things devices, including smart sensors, homes, and cities. Increasingly, embedding intelligence in such devices involves the use of deep neural networks. However, their storage and processing requirements make them prohibitive for cheap, off-the-shelf platforms. Overcoming those requirements is necessary for enabling widely-applicable smart devices. While many ways of making models smaller and more efficient have been developed, there is a lack of understanding of which ones are best suited for particular scenarios. More importantly for edge platforms, those choices cannot be analyzed in isolation from cost and user experience. In this work, we holistically explore how quantization, model scaling, and multi-modality interact with system components such as memory, sensors, and processors. We perform this hardware/software co-design from the cost, latency, and user-experience perspective, and d
&lt;/p&gt;</description></item><item><title>D2&#20462;&#21098;&#26159;&#19968;&#31181;&#24179;&#34913;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#22256;&#38590;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;coreset&#36873;&#25321;&#20013;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#37325;&#35201;&#24615;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.07931</link><description>&lt;p&gt;
D2&#20462;&#21098;&#65306;&#20449;&#24687;&#20256;&#36882;&#24179;&#34913;&#25968;&#25454;&#20462;&#21098;&#20013;&#30340;&#22810;&#26679;&#24615;&#21644;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning. (arXiv:2310.07931v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07931
&lt;/p&gt;
&lt;p&gt;
D2&#20462;&#21098;&#26159;&#19968;&#31181;&#24179;&#34913;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#22256;&#38590;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;coreset&#36873;&#25321;&#20013;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#37325;&#35201;&#24615;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#29702;&#35770;&#34920;&#26126;&#65292;&#22312;&#22266;&#23450;&#25968;&#25454;&#39044;&#31639;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21487;&#20197;&#23548;&#33268;&#26356;&#20302;&#30340;&#27979;&#35797;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#25968;&#25454;&#38598;&#21487;&#20197;&#21093;&#31163;&#20887;&#20313;&#39033;&#65292;&#21017;&#21487;&#20197;&#22312;&#36739;&#20302;&#30340;&#35745;&#31639;&#39044;&#31639;&#19978;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;Coreset&#36873;&#25321;&#65288;&#25110;&#25968;&#25454;&#20462;&#21098;&#65289;&#23547;&#27714;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#22312;&#35813;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20063;&#31216;&#20026;coreset&#12290;&#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#65288;1&#65289;&#22522;&#20110;&#20960;&#20309;&#30340;&#25968;&#25454;&#36873;&#21462;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;coreset&#20013;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#21644;&#65288;2&#65289;&#26681;&#25454;&#35757;&#32451;&#21160;&#24577;&#20026;&#26679;&#26412;&#20998;&#37197;&#22256;&#38590;&#24230;&#20998;&#25968;&#30340;&#20989;&#25968;&#12290;&#20026;&#25968;&#25454;&#22810;&#26679;&#24615;&#36827;&#34892;&#20248;&#21270;&#20250;&#23548;&#33268;&#20559;&#21521;&#36739;&#23481;&#26131;&#26679;&#26412;&#30340;coreset&#65292;&#32780;&#38590;&#24230;&#25490;&#21517;&#36873;&#25321;&#20250;&#24573;&#30053;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25152;&#24517;&#38656;&#30340;&#23481;&#26131;&#26679;&#26412;&#12290;&#36825;&#34920;&#26126;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#37325;&#35201;&#24615;&#35780;&#20998;&#26159;&#20004;&#20010;&#20114;&#34917;&#22240;&#32032;&#65292;&#22312;coreset&#36873;&#25321;&#20013;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analytical theories suggest that higher-quality data can lead to lower test errors in models trained on a fixed data budget. Moreover, a model can be trained on a lower compute budget without compromising performance if a dataset can be stripped of its redundancies. Coreset selection (or data pruning) seeks to select a subset of the training data so as to maximize the performance of models trained on this subset, also referred to as coreset. There are two dominant approaches: (1) geometry-based data selection for maximizing data diversity in the coreset, and (2) functions that assign difficulty scores to samples based on training dynamics. Optimizing for data diversity leads to a coreset that is biased towards easier samples, whereas, selection by difficulty ranking omits easy samples that are necessary for the training of deep learning models. This demonstrates that data diversity and importance scores are two complementary factors that need to be jointly considered during coreset sel
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#20174;&#23454;&#39564;&#26230;&#20307;&#32467;&#26500;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#20013;&#25512;&#23548;&#20986;&#20302;&#32500;&#21464;&#37327;&#65292;&#24182;&#22312;&#22686;&#24378;&#25277;&#26679;&#20013;&#20559;&#32622;&#36825;&#20123;&#21464;&#37327;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;&#29366;&#24577;&#36716;&#25442;&#21644;&#28909;&#21147;&#23398;&#26435;&#37325;&#12290;&#35813;&#26041;&#27861;&#22312;&#30740;&#31350;&#38081;&#21644;&#29976;&#27688;&#37240;&#30340;&#26230;&#20307;&#25104;&#26680;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#20934;&#30830;&#30340;&#33258;&#30001;&#33021;&#35745;&#31639;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#25277;&#26679;&#30340;&#28508;&#21147;&#12290;en_tdlr: This study presents a graph neural network-based learning approach to derive low-dimensional variables from experimental crystal structures and bias them in enhanced sampling for reliable state transitions and thermodynamic weights. The approach achieves accurate free energy calculations in the nucleation processes of iron and glycine crystals, demonstrating the potential for improved sampling accuracy.</title><link>http://arxiv.org/abs/2310.07927</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#21464;&#37327;&#30340;&#26230;&#20307;&#25104;&#26680;&#22686;&#24378;&#25277;&#26679;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhanced sampling of Crystal Nucleation with Graph Representation Learnt Variables. (arXiv:2310.07927v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#20174;&#23454;&#39564;&#26230;&#20307;&#32467;&#26500;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#20013;&#25512;&#23548;&#20986;&#20302;&#32500;&#21464;&#37327;&#65292;&#24182;&#22312;&#22686;&#24378;&#25277;&#26679;&#20013;&#20559;&#32622;&#36825;&#20123;&#21464;&#37327;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;&#29366;&#24577;&#36716;&#25442;&#21644;&#28909;&#21147;&#23398;&#26435;&#37325;&#12290;&#35813;&#26041;&#27861;&#22312;&#30740;&#31350;&#38081;&#21644;&#29976;&#27688;&#37240;&#30340;&#26230;&#20307;&#25104;&#26680;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#20934;&#30830;&#30340;&#33258;&#30001;&#33021;&#35745;&#31639;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#25277;&#26679;&#30340;&#28508;&#21147;&#12290;en_tdlr: This study presents a graph neural network-based learning approach to derive low-dimensional variables from experimental crystal structures and bias them in enhanced sampling for reliable state transitions and thermodynamic weights. The approach achieves accurate free energy calculations in the nucleation processes of iron and glycine crystals, demonstrating the potential for improved sampling accuracy.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#26469;&#20174;&#23454;&#39564;&#26230;&#20307;&#32467;&#26500;&#20013;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#20013;&#25512;&#23548;&#20986;&#20302;&#32500;&#21464;&#37327;&#12290;&#28982;&#21518;&#65292;&#22312;&#22686;&#24378;&#25277;&#26679;&#20013;&#23545;&#36825;&#20123;&#21464;&#37327;&#36827;&#34892;&#20559;&#32622;&#65292;&#20197;&#35266;&#23519;&#29366;&#24577;&#36716;&#25442;&#21644;&#21487;&#38752;&#30340;&#28909;&#21147;&#23398;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#31616;&#21333;&#30340;&#21367;&#31215;&#21644;&#27744;&#21270;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38081;&#21644;&#29976;&#27688;&#37240;&#30340;&#21508;&#31181;&#21516;&#32032;&#24322;&#24418;&#20307;&#21644;&#22810;&#22411;&#20307;&#20174;&#29076;&#34701;&#29366;&#24577;&#21040;&#25104;&#26680;&#29366;&#24577;&#30340;&#36716;&#21464;&#12290;&#24403;&#25105;&#20204;&#30340;&#22270;&#28508;&#21464;&#37327;&#22312;&#33391;&#28201;&#20803;&#21160;&#21147;&#23398;&#20013;&#36827;&#34892;&#20559;&#32622;&#26102;&#65292;&#22987;&#32456;&#26174;&#31034;&#20986;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#21464;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#19982;&#23454;&#39564;&#19968;&#33268;&#30340;&#20934;&#30830;&#33258;&#30001;&#33021;&#35745;&#31639;&#65292;&#36825;&#20123;&#37117;&#26159;&#21487;&#38752;&#25277;&#26679;&#30340;&#25351;&#26631;&#12290;&#36825;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21464;&#37327;&#22312;&#25913;&#36827;&#25277;&#26679;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;&#36825;&#37324;&#23637;&#31034;&#30340;&#21327;&#35758;&#24212;&#35813;&#36866;&#29992;&#20110;&#20854;&#20182;&#31995;&#32479;&#21644;&#20854;&#20182;&#25277;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a graph neural network-based learning approach using an autoencoder setup to derive low-dimensional variables from features observed in experimental crystal structures. These variables are then biased in enhanced sampling to observe state-to-state transitions and reliable thermodynamic weights. Our approach uses simple convolution and pooling methods. To verify the effectiveness of our protocol, we examined the nucleation of various allotropes and polymorphs of iron and glycine from their molten states. Our graph latent variables when biased in well-tempered metadynamics consistently show transitions between states and achieve accurate free energy calculations in agreement with experiments, both of which are indicators of dependable sampling. This underscores the strength and promise of our graph neural net variables for improved sampling. The protocol shown here should be applicable for other systems and with other sampling methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#26102;&#21464;&#27969;&#24335;&#25104;&#26412;&#20989;&#25968;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#38454;&#23548;&#25968;&#36827;&#34892;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#27604;&#36739;&#21644;&#20960;&#20010;&#31034;&#20363;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07925</link><description>&lt;p&gt;
&#38024;&#23545;&#27969;&#24335;&#20984;&#25104;&#26412;&#30340;&#19968;&#38454;&#21160;&#24577;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
First-Order Dynamic Optimization for Streaming Convex Costs. (arXiv:2310.07925v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#26102;&#21464;&#27969;&#24335;&#25104;&#26412;&#20989;&#25968;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#38454;&#23548;&#25968;&#36827;&#34892;&#35745;&#31639;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#27604;&#36739;&#21644;&#20960;&#20010;&#31034;&#20363;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#29992;&#20110;&#35299;&#20915;&#19968;&#31867;&#24102;&#26377;&#26102;&#21464;&#27969;&#24335;&#25104;&#26412;&#20989;&#25968;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36319;&#36394;&#26368;&#20248;&#35299;&#65292;&#24182;&#20445;&#25345;&#26377;&#30028;&#35823;&#24046;&#12290;&#19982;&#29616;&#26377;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20165;&#20351;&#29992;&#25104;&#26412;&#20989;&#25968;&#30340;&#19968;&#38454;&#23548;&#25968;&#25191;&#34892;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#22788;&#29702;&#26102;&#21464;&#25104;&#26412;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#26102;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#20026;&#20160;&#20040;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#23545;&#20110;&#20855;&#26377;&#26102;&#21464;&#25104;&#26412;&#30340;&#20248;&#21270;&#38382;&#39064;&#19981;&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20960;&#20010;&#31034;&#20363;&#65292;&#21253;&#25324;&#20351;&#29992;&#27969;&#24335;&#26102;&#21464;&#25104;&#26412;&#20989;&#25968;&#24314;&#27169;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a set of novel optimization algorithms for solving a class of convex optimization problems with time-varying streaming cost function. We develop an approach to track the optimal solution with a bounded error. Unlike the existing results, our algorithm is executed only by using the first-order derivatives of the cost function which makes it computationally efficient for optimization with time-varying cost function. We compare our algorithms to the gradient descent algorithm and show why gradient descent is not an effective solution for optimization problems with time-varying cost. Several examples including solving a model predictive control problem cast as a convex optimization problem with a streaming time-varying cost function demonstrate our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07923</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#22320;&#31616;&#21333;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20363;&#22914;&#26816;&#26597;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#30340;&#20004;&#20010;&#33410;&#28857;&#65292;&#25110;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#36825;&#20123;&#38382;&#39064;&#34987;&#35777;&#26126;&#26080;&#27861;&#30001;&#31435;&#21363;&#35835;&#21462;&#36755;&#20837;&#21518;&#22238;&#31572;&#30340;&#26631;&#20934;Transformer&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20801;&#35768;Transformer&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25110;&#8220;&#33609;&#31295;&#32440;&#8221;&#65292;&#21363;&#22312;&#22238;&#31572;&#20043;&#21069;&#29983;&#25104;&#24182;&#20381;&#36182;&#19968;&#31995;&#21015;&#20013;&#38388;token&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#38382;&#65306;&#36825;&#31181;&#20013;&#38388;&#29983;&#25104;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#25193;&#23637;&#20102;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#65311;&#25105;&#20204;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#22686;&#21152;&#30340;&#31243;&#24230;&#20851;&#38190;&#21462;&#20915;&#20110;&#20013;&#38388;&#29983;&#25104;&#30340;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#26469;&#35828;&#65292;&#20855;&#26377;&#23545;&#25968;&#32423;&#35299;&#30721;&#27493;&#39588;&#30340;Transformer&#35299;&#30721;&#22120;&#20165;&#30053;&#24494;&#25512;&#21160;&#20102;&#26631;&#20934;Transformer&#30340;&#26497;&#38480;&#65292;&#32780;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#21017;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#65288;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.07918</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#23545;&#21307;&#30103;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#20013;&#20272;&#35745;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20363;&#22914;&#65292;&#23457;&#35745;&#21307;&#30103;&#20915;&#31574;&#30340;&#20559;&#35265;&#21644;&#27425;&#20248;&#23454;&#36341;&#65292;&#25105;&#20204;&#38656;&#35201;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#29616;&#26377;&#26041;&#27861;&#22522;&#26412;&#19978;&#30001;&#20110;&#23558;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#34920;&#31034;&#20026;&#36890;&#29992;&#31574;&#30053;&#32780;&#36127;&#25285;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#23454;&#38469;&#19978;&#20154;&#31867;&#20915;&#31574;&#26159;&#21160;&#24577;&#30340;&#65292;&#21487;&#20197;&#38543;&#19978;&#19979;&#25991;&#20449;&#24687;&#32780;&#22823;&#24133;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65288;CPR&#65289;&#65292;&#23558;&#24314;&#27169;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#30340;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22797;&#26434;&#20915;&#31574;&#31574;&#30053;&#30001;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#32452;&#25104;&#12290;CPR&#23558;&#27599;&#20010;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#24314;&#27169;&#20026;&#32447;&#24615;&#30340;&#35266;&#23519;-&#21160;&#20316;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.07917</link><description>&lt;p&gt;
&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#21644;&#26410;&#26469;&#36235;&#21183;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Machine Learning Techniques in Imbalanced Data and Future Trends. (arXiv:2310.07917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#37324;&#65292;&#26816;&#27979;&#32597;&#35265;&#20107;&#20214;&#19968;&#30452;&#26159;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#38382;&#39064;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#25913;&#36827;&#25968;&#25454;&#22788;&#29702;&#21644;&#31639;&#27861;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#38750;&#24179;&#34913;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#35770;&#25991;&#25910;&#38598;&#21644;&#23457;&#26597;&#20102;258&#31687;&#26469;&#33258;&#26399;&#21002;&#21644;&#20250;&#35758;&#35770;&#25991;&#30340;&#21516;&#34892;&#35780;&#23457;&#35770;&#25991;&#65292;&#26088;&#22312;&#20174;&#25216;&#26415;&#21644;&#24212;&#29992;&#35282;&#24230;&#28145;&#20837;&#23457;&#26597;&#38750;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#20026;&#22312;&#23398;&#26415;&#30028;&#25110;&#24037;&#19994;&#30028;&#24076;&#26395;&#28145;&#20837;&#23398;&#20064;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#32508;&#36848;&#65292;&#24182;&#20026;&#20182;&#20204;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
&lt;/p&gt;</description></item><item><title>"Unraveling the Single Tangent Space Fallacy"&#35770;&#25991;&#20998;&#26512;&#21644;&#28548;&#28165;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24212;&#29992;&#40654;&#26364;&#20960;&#20309;&#30340;&#35823;&#21306;&#65292;&#35813;&#35823;&#21306;&#26159;&#25351;&#23558;&#25968;&#25454;&#20165;&#25237;&#24433;&#21040;&#21333;&#19968;&#20999;&#31354;&#38388;&#20013;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07902</link><description>&lt;p&gt;
&#25581;&#31034;&#21333;&#20999;&#24179;&#38754;&#35823;&#21306;&#65306;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24212;&#29992;&#40654;&#26364;&#20960;&#20309;&#30340;&#20998;&#26512;&#21644;&#28548;&#28165;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning. (arXiv:2310.07902v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07902
&lt;/p&gt;
&lt;p&gt;
"Unraveling the Single Tangent Space Fallacy"&#35770;&#25991;&#20998;&#26512;&#21644;&#28548;&#28165;&#20102;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#24212;&#29992;&#40654;&#26364;&#20960;&#20309;&#30340;&#35823;&#21306;&#65292;&#35813;&#35823;&#21306;&#26159;&#25351;&#23558;&#25968;&#25454;&#20165;&#25237;&#24433;&#21040;&#21333;&#19968;&#20999;&#31354;&#38388;&#20013;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#35768;&#22810;&#21518;&#32493;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#12289;&#24314;&#27169;&#25110;&#21512;&#25104;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#22266;&#20307;&#26041;&#21521;&#34920;&#31034;&#22235;&#20803;&#25968;&#30340;&#21333;&#20301;&#33539;&#25968;&#26465;&#20214;&#25110;&#21018;&#24230;&#21644;&#21487;&#25805;&#32437;&#24615;&#26925;&#29699;&#30340;&#27491;&#23450;&#24615;&#31561;&#20960;&#20309;&#32422;&#26463;&#12290;&#26377;&#25928;&#22788;&#29702;&#36825;&#26679;&#30340;&#20960;&#20309;&#32422;&#26463;&#38656;&#35201;&#23558;&#24494;&#20998;&#20960;&#20309;&#24037;&#20855;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21046;&#23450;&#20013;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#40654;&#26364;&#27969;&#24418;&#25104;&#20026;&#22788;&#29702;&#36825;&#31181;&#20960;&#20309;&#32422;&#26463;&#30340;&#24378;&#22823;&#25968;&#23398;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#23545;&#20854;&#37319;&#29992;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#25968;&#23398;&#19978;&#30340;&#32570;&#38519;&#21270;&#31616;&#29616;&#35937;&#65292;&#34987;&#31216;&#20026;&#8220;&#21333;&#20999;&#24179;&#38754;&#35823;&#21306;&#8221;&#12290;&#36825;&#31181;&#26041;&#27861;&#20165;&#28041;&#21450;&#23558;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#21333;&#19968;&#20999;&#31354;&#38388;&#65288;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65289;&#19978;&#65292;&#28982;&#21518;&#20351;&#29992;&#29616;&#25104;&#30340;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the ``single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;NoMaD&#65292;&#19968;&#31181;&#30446;&#26631;&#36974;&#34109;&#25193;&#25955;&#31574;&#30053;&#65292;&#29992;&#20110;&#21516;&#26102;&#22788;&#29702;&#30446;&#26631;&#23548;&#21521;&#23548;&#33322;&#21644;&#30446;&#26631;&#19981;&#21487;&#30693;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#20351;&#29992;&#23376;&#30446;&#26631;&#25552;&#35758;&#25110;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27492;&#32479;&#19968;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07896</link><description>&lt;p&gt;
NoMaD: &#30446;&#26631;&#36974;&#34109;&#25193;&#25955;&#31574;&#30053;&#29992;&#20110;&#23548;&#33322;&#19982;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration. (arXiv:2310.07896v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;NoMaD&#65292;&#19968;&#31181;&#30446;&#26631;&#36974;&#34109;&#25193;&#25955;&#31574;&#30053;&#65292;&#29992;&#20110;&#21516;&#26102;&#22788;&#29702;&#30446;&#26631;&#23548;&#21521;&#23548;&#33322;&#21644;&#30446;&#26631;&#19981;&#21487;&#30693;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#20351;&#29992;&#23376;&#30446;&#26631;&#25552;&#35758;&#25110;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27492;&#32479;&#19968;&#31574;&#30053;&#20855;&#26377;&#26356;&#22909;&#30340;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#29087;&#24713;&#30340;&#29615;&#22659;&#20013;&#65292;&#26426;&#22120;&#20154;&#23398;&#20064;&#23548;&#33322;&#38656;&#35201;&#25552;&#20379;&#20219;&#21153;&#23548;&#21521;&#30340;&#23548;&#33322;&#31574;&#30053;&#65288;&#21363;&#21040;&#36798;&#26426;&#22120;&#20154;&#23450;&#20301;&#30340;&#30446;&#26631;&#65289;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#25506;&#32034;&#31574;&#30053;&#65288;&#21363;&#22312;&#26032;&#30340;&#29615;&#22659;&#20013;&#25628;&#32034;&#30446;&#26631;&#65289;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#35282;&#33394;&#30001;&#19981;&#21516;&#30340;&#27169;&#22411;&#22788;&#29702;&#65292;&#20363;&#22914;&#20351;&#29992;&#23376;&#30446;&#26631;&#25552;&#35758;&#12289;&#35268;&#21010;&#25110;&#20998;&#31163;&#30340;&#23548;&#33322;&#31574;&#30053;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#30340;&#25193;&#25955;&#31574;&#30053;&#26469;&#21516;&#26102;&#22788;&#29702;&#30446;&#26631;&#23548;&#21521;&#23548;&#33322;&#21644;&#30446;&#26631;&#19981;&#21487;&#30693;&#25506;&#32034;&#65292;&#21518;&#32773;&#25552;&#20379;&#20102;&#22312;&#26032;&#29615;&#22659;&#20013;&#25628;&#32034;&#30446;&#26631;&#30340;&#33021;&#21147;&#65292;&#21069;&#32773;&#25552;&#20379;&#20102;&#19968;&#26086;&#30446;&#26631;&#34987;&#23450;&#20301;&#21518;&#21040;&#36798;&#29992;&#25143;&#25351;&#23450;&#30446;&#26631;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#23376;&#30446;&#26631;&#25552;&#35758;&#25110;&#22522;&#20110;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#20808;&#21069;&#26041;&#27861;&#65292;&#36825;&#31181;&#32479;&#19968;&#31574;&#30053;&#22312;&#23548;&#33322;&#21040;&#35270;&#35273;&#25351;&#31034;&#30446;&#26631;&#30340;&#26032;&#29615;&#22659;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Robotic learning for navigation in unfamiliar environments needs to provide policies for both task-oriented navigation (i.e., reaching a goal that the robot has located), and task-agnostic exploration (i.e., searching for a goal in a novel setting). Typically, these roles are handled by separate models, for example by using subgoal proposals, planning, or separate navigation strategies. In this paper, we describe how we can train a single unified diffusion policy to handle both goal-directed navigation and goal-agnostic exploration, with the latter providing the ability to search novel environments, and the former providing the ability to reach a user-specified goal once it has been located. We show that this unified policy results in better overall performance when navigating to visually indicated goals in novel environments, as compared to approaches that use subgoal proposals from generative models, or prior methods based on latent variable models. We instantiate our method by using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;CNN&#36827;&#34892;&#20998;&#31867;&#21644;HMM&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#39640;&#25928;&#22320;&#23558;&#32963;&#32928;&#36896;&#24433;&#30340;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32416;&#27491;CNN&#36755;&#20986;&#26469;&#23454;&#29616;&#31934;&#30830;&#23450;&#20301;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;Rhode Island&#32963;&#32928;&#30149;&#23398;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;98.04&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#21487;&#20197;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07895</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;CNN&#30340;&#20998;&#31867;&#21644;HMM&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22312;GI&#36947;&#20013;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Precise localization within the GI tract by combining classification of CNNs and time-series analysis of HMMs. (arXiv:2310.07895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;CNN&#36827;&#34892;&#20998;&#31867;&#21644;HMM&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#39640;&#25928;&#22320;&#23558;&#32963;&#32928;&#36896;&#24433;&#30340;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32416;&#27491;CNN&#36755;&#20986;&#26469;&#23454;&#29616;&#31934;&#30830;&#23450;&#20301;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;Rhode Island&#32963;&#32928;&#30149;&#23398;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;98.04&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#21487;&#20197;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25506;&#32034;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#20998;&#31867;&#21644;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23646;&#24615;&#30340;&#32452;&#21512;&#26469;&#39640;&#25928;&#22320;&#23545;&#35270;&#39057;&#33014;&#22218;&#20869;&#38236;&#65288;VCE&#65289;&#30740;&#31350;&#20013;&#22522;&#20110;&#22270;&#20687;&#30340;&#32963;&#32928;&#23398;&#37096;&#20998;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#36830;&#32493;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21487;&#20197;&#35782;&#21035;&#21644;&#32416;&#27491;CNN&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Rhode Island&#65288;RI&#65289;&#32963;&#32928;&#30149;&#23398;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;98.04&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#20351;&#24471;&#22312;&#32963;&#32928;&#36947;&#20869;&#23454;&#29616;&#31934;&#30830;&#23450;&#20301;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#32422;1M&#20010;&#21442;&#25968;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#20302;&#21151;&#32791;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method to efficiently classify the gastroenterologic section of images derived from Video Capsule Endoscopy (VCE) studies by exploring the combination of a Convolutional Neural Network (CNN) for classification with the time-series analysis properties of a Hidden Markov Model (HMM). It is demonstrated that successive time-series analysis identifies and corrects errors in the CNN output. Our approach achieves an accuracy of $98.04\%$ on the Rhode Island (RI) Gastroenterology dataset. This allows for precise localization within the gastrointestinal (GI) tract while requiring only approximately 1M parameters and thus, provides a method suitable for low power devices
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20849;&#36717;&#31215;&#20998;&#22120;&#21644;&#20998;&#35010;&#31215;&#20998;&#22120;&#20004;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#32463;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#24615;&#33021;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07894</link><description>&lt;p&gt;
&#39640;&#25928;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#31215;&#20998;&#22120;
&lt;/p&gt;
&lt;p&gt;
Efficient Integrators for Diffusion Generative Models. (arXiv:2310.07894v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20849;&#36717;&#31215;&#20998;&#22120;&#21644;&#20998;&#35010;&#31215;&#20998;&#22120;&#20004;&#31181;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#32463;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#24615;&#33021;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#33021;&#22815;&#24212;&#29992;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#38388;&#29983;&#25104;&#26679;&#26412;&#36895;&#24230;&#36739;&#24930;&#12290;&#22240;&#27492;&#65292;&#20026;&#26356;&#24191;&#27867;&#30340;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#24555;&#36895;&#30830;&#23450;&#24615;/&#38543;&#26426;&#37319;&#26679;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#20114;&#34917;&#30340;&#21152;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;&#26679;&#26412;&#29983;&#25104;&#30340;&#26694;&#26550;&#65306;&#20849;&#36717;&#31215;&#20998;&#22120;&#21644;&#20998;&#35010;&#31215;&#20998;&#22120;&#12290;&#20849;&#36717;&#31215;&#20998;&#22120;&#23558;DDIM&#27867;&#21270;&#65292;&#23558;&#21453;&#21521;&#25193;&#25955;&#21160;&#21147;&#23398;&#26144;&#23556;&#21040;&#26356;&#26131;&#20110;&#37319;&#26679;&#30340;&#31354;&#38388;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#20998;&#35010;&#30340;&#31215;&#20998;&#22120;&#65292;&#24120;&#29992;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#22312;&#28041;&#21450;&#25968;&#25454;&#21644;&#36741;&#21161;&#21464;&#37327;&#30340;&#25968;&#20540;&#26356;&#26032;&#20043;&#38388;&#20132;&#26367;&#65292;&#20943;&#23569;&#20102;&#25968;&#20540;&#27169;&#25311;&#35823;&#24046;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#24378;&#31354;&#38388;&#20013;&#33719;&#24471;&#20102;&#25253;&#21578;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#19978;&#24212;&#29992;&#30456;&#31354;&#38388;&#26391;&#20043;&#19975;&#25193;&#25955;[Pandey&#65286;Mandt&#65292;2023]&#65292;&#25105;&#20204;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#26679;&#26412;&#29983;&#25104;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey &amp; Mandt, 2023] on CIFAR-10, our deterministic and stochastic samp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20223;&#30495;&#35823;&#24046;&#26368;&#23567;&#21270;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#23454;&#29616;&#22312;&#39118;&#25200;&#21160;&#19979;ASV&#20445;&#25345;&#31449;&#20301;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;ROS&#21644;Gazebo&#20223;&#30495;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2310.07892</link><description>&lt;p&gt;
ASV&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20223;&#30495;&#35823;&#24046;&#26368;&#23567;&#21270;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#23454;&#29616;&#22312;&#39118;&#25200;&#21160;&#19979;&#30340;&#20445;&#25345;&#31449;&#20301;
&lt;/p&gt;
&lt;p&gt;
ASV Station Keeping under Wind Disturbances using Neural Network Simulation Error Minimization Model Predictive Control. (arXiv:2310.07892v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20223;&#30495;&#35823;&#24046;&#26368;&#23567;&#21270;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#23454;&#29616;&#22312;&#39118;&#25200;&#21160;&#19979;ASV&#20445;&#25345;&#31449;&#20301;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;ROS&#21644;Gazebo&#20223;&#30495;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25345;&#31449;&#20301;&#23545;&#20110;&#33258;&#20027;&#27700;&#38754;&#36710;&#36742;&#65288;ASVs&#65289;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#25805;&#32437;&#21160;&#20316;&#65292;&#29305;&#21035;&#26159;&#22312;&#29421;&#23567;&#31354;&#38388;&#20013;&#20351;&#29992;&#65292;&#29992;&#20110;&#36827;&#34892;&#38656;&#35201;ASV&#20445;&#25345;&#20301;&#32622;&#25110;&#19982;&#20854;&#20182;&#36710;&#36742;&#21327;&#20316;&#30340;&#35843;&#26597;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;ASV&#21160;&#21147;&#23398;&#21644;&#29615;&#22659;&#24178;&#25200;&#30340;&#31934;&#30830;&#24314;&#27169;&#30340;&#38656;&#27714;&#65292;&#36825;&#31181;&#25805;&#32437;&#21160;&#20316;&#23545;&#20110;&#32463;&#20856;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#32780;&#35328;&#21487;&#33021;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20223;&#30495;&#35823;&#24046;&#26368;&#23567;&#21270;&#65288;NNSEM-MPC&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#22312;&#39118;&#25200;&#21160;&#19979;ASV&#30340;&#21160;&#21147;&#23398;&#12290;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#65288;ROS&#65289;&#21644;&#22810;&#29992;&#36884;&#20223;&#30495;&#29615;&#22659;Gazebo&#19978;&#36827;&#34892;&#20102;&#23545;&#25152;&#25552;&#20986;&#26041;&#26696;&#22312;&#39118;&#25200;&#21160;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20223;&#30495;&#27979;&#35797;&#65292;&#24182;&#19982;&#20854;&#20182;&#25511;&#21046;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#39118;&#36895;&#65288;3 m/s&#21644;6 m/s&#65289;&#21644;&#19977;&#20010;&#39118;&#21521;&#65288;0&#176;&#12289;90&#176;&#21644;180&#176;&#65289;&#65292;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20845;&#20010;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Station keeping is an essential maneuver for Autonomous Surface Vehicles (ASVs), mainly when used in confined spaces, to carry out surveys that require the ASV to keep its position or in collaboration with other vehicles where the relative position has an impact over the mission. However, this maneuver can become challenging for classic feedback controllers due to the need for an accurate model of the ASV dynamics and the environmental disturbances. This work proposes a Model Predictive Controller using Neural Network Simulation Error Minimization (NNSEM-MPC) to accurately predict the dynamics of the ASV under wind disturbances. The performance of the proposed scheme under wind disturbances is tested and compared against other controllers in simulation, using the Robotics Operating System (ROS) and the multipurpose simulation environment Gazebo. A set of six tests were conducted by combining two wind speeds (3 m/s and 6 m/s) and three wind directions (0$^\circ$, 90$^\circ$, and 180$^\c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#19981;&#21516;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#32452;&#20214;&#65292;&#32780;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21017;&#30001;&#36825;&#20123;&#29305;&#24449;&#25152;&#20915;&#23450;&#12290;</title><link>http://arxiv.org/abs/2310.07891</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#19968;&#27425;&#26799;&#24230;&#19979;&#38477;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks. (arXiv:2310.07891v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#19981;&#21516;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#32452;&#20214;&#65292;&#32780;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21017;&#30001;&#36825;&#20123;&#29305;&#24449;&#25152;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#30340;&#22522;&#26412;&#21407;&#22240;&#20043;&#19968;&#12290;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#24050;&#32463;&#20005;&#26684;&#35777;&#26126;&#65292;&#22312;&#20004;&#23618;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#31532;&#19968;&#23618;&#36827;&#34892;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#23618;&#36827;&#34892;&#23725;&#22238;&#24402;&#21487;&#20197;&#23548;&#33268;&#29305;&#24449;&#23398;&#20064;&#65307;&#29305;&#24449;&#30697;&#38453;&#30340;&#35889;&#20013;&#20250;&#20986;&#29616;&#20998;&#31163;&#30340;&#19968;&#32500;&#32452;&#20214;&#65292;&#31216;&#20026;&#8220;spike&#8221;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22266;&#23450;&#26799;&#24230;&#19979;&#38477;&#27493;&#38271;&#26102;&#65292;&#36825;&#20010;&#8220;spike&#8221;&#20165;&#25552;&#20379;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#20214;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#23398;&#20064;&#38750;&#32447;&#24615;&#32452;&#20214;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23398;&#20064;&#29575;&#38543;&#26679;&#26412;&#22823;&#23567;&#22686;&#38271;&#26102;&#65292;&#36825;&#26679;&#30340;&#35757;&#32451;&#23454;&#38469;&#19978;&#24341;&#20837;&#20102;&#22810;&#20010;&#19968;&#32500;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#19968;&#20010;&#29305;&#23450;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#38480;&#22823;&#32500;&#24230;&#21644;&#22823;&#26679;&#26412;&#35757;&#32451;&#21644;&#27979;&#35797;&#35823;&#24046;&#23436;&#20840;&#30001;&#36825;&#20123;&#8220;spike&#8221;&#25152;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By 
&lt;/p&gt;</description></item><item><title>&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#31070;&#32463;&#32593;&#32476;&#26159;&#21463;&#21040;&#33258;&#28982;&#32676;&#20307;&#38598;&#21512;&#20013;&#35266;&#23519;&#21040;&#30340;&#35268;&#21017;&#21551;&#21457;&#30340;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#23616;&#37096;&#35823;&#24046;&#20449;&#21495;&#35757;&#32451;&#24182;&#21487;&#36873;&#25321;&#24615;&#22320;&#32467;&#21512;&#21453;&#21521;&#20256;&#25773;&#21644;&#20840;&#23616;&#25439;&#22833;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#24037;&#20316;&#20154;&#21592;&#34892;&#20026;&#21644;&#35780;&#20272;&#65292;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#39640;&#24230;&#30340;&#33258;&#32452;&#32455;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07885</link><description>&lt;p&gt;
&#22522;&#20110;&#22797;&#26434;&#32676;&#20307;&#30340;&#23616;&#37096;&#35823;&#24046;&#20449;&#21495;&#21551;&#21457;&#30340;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives. (arXiv:2310.07885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07885
&lt;/p&gt;
&lt;p&gt;
&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#31070;&#32463;&#32593;&#32476;&#26159;&#21463;&#21040;&#33258;&#28982;&#32676;&#20307;&#38598;&#21512;&#20013;&#35266;&#23519;&#21040;&#30340;&#35268;&#21017;&#21551;&#21457;&#30340;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#23616;&#37096;&#35823;&#24046;&#20449;&#21495;&#35757;&#32451;&#24182;&#21487;&#36873;&#25321;&#24615;&#22320;&#32467;&#21512;&#21453;&#21521;&#20256;&#25773;&#21644;&#20840;&#23616;&#25439;&#22833;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#24037;&#20316;&#20154;&#21592;&#34892;&#20026;&#21644;&#35780;&#20272;&#65292;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#39640;&#24230;&#30340;&#33258;&#32452;&#32455;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20013;&#20855;&#26377;&#24322;&#36136;&#12289;&#36164;&#28304;&#26377;&#38480;&#30340;&#20449;&#24687;&#22788;&#29702;&#21333;&#20803;&#65288;&#20363;&#22914;&#65292;&#19968;&#32676;&#40060;&#12289;&#19968;&#32676;&#40479;&#25110;&#19968;&#32452;&#31070;&#32463;&#20803;&#32593;&#32476;&#65289;&#30340;&#38598;&#20307;&#34892;&#20026;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#33258;&#32452;&#32455;&#21644;&#22797;&#26434;&#24615;&#12290;&#36825;&#20123; emergent properties&#26159;&#36890;&#36807;&#31616;&#21333;&#30340;&#30456;&#20114;&#20316;&#29992;&#35268;&#21017;&#20135;&#29983;&#30340;&#65292;&#20854;&#20013;&#26576;&#20123;&#20010;&#20307;&#33021;&#22815;&#34920;&#29616;&#20986;&#39046;&#23548;&#34892;&#20026;&#24182;&#24433;&#21709;&#32676;&#20307;&#30340;&#27963;&#21160;&#12290;&#21463;&#21040;&#36825;&#20123;&#32676;&#20307;&#30340;&#22797;&#26434;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21463;&#21040;&#33258;&#28982;&#32676;&#20307;&#38598;&#21512;&#20013;&#35266;&#23519;&#21040;&#30340;&#35268;&#21017;&#30340;&#21551;&#21457;&#12290;&#36825;&#20010;NN&#32467;&#26500;&#21253;&#21547;&#20102;&#21253;&#21547;&#19968;&#20010;&#25110;&#22810;&#20010;&#20449;&#24687;&#22788;&#29702;&#21333;&#20803;&#65288;&#22914;&#31070;&#32463;&#20803;&#12289;&#28388;&#27874;&#22120;&#12289;&#23618;&#25110;&#23618;&#22359;&#65289;&#30340;&#24037;&#20316;&#20154;&#21592;&#12290;&#24037;&#20316;&#20154;&#21592;&#21487;&#20197;&#26159;&#39046;&#23548;&#32773;&#25110;&#36319;&#38543;&#32773;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#35823;&#24046;&#20449;&#21495;&#35757;&#32451;&#20102;&#19968;&#20010;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#31070;&#32463;&#32593;&#32476;&#65288;LFNN&#65289;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#32467;&#21512;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#21644;&#20840;&#23616;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#30740;&#31350;&#24037;&#20316;&#20154;&#21592;&#30340;&#34892;&#20026;&#24182;&#35780;&#20272;&#20102;LFNN&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#20351;&#29992;&#23616;&#37096;&#35823;&#24046;&#20449;&#21495;&#30340;LFNNs&#65292;&#21487;&#36873;&#25321;&#24615;&#22320;&#23558;&#21453;&#21521;&#20256;&#25773;&#21644;&#20840;&#23616;&#25439;&#22833;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
The collective behavior of a network with heterogeneous, resource-limited information processing units (e.g., group of fish, flock of birds, or network of neurons) demonstrates high self-organization and complexity. These emergent properties arise from simple interaction rules where certain individuals can exhibit leadership-like behavior and influence the collective activity of the group. Motivated by the intricacy of these collectives, we propose a neural network (NN) architecture inspired by the rules observed in nature's collective ensembles. This NN structure contains workers that encompass one or more information processing units (e.g., neurons, filters, layers, or blocks of layers). Workers are either leaders or followers, and we train a leader-follower neural network (LFNN) by leveraging local error signals and optionally incorporating backpropagation (BP) and global loss. We investigate worker behavior and evaluate LFNNs through extensive experimentation. Our LFNNs trained wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#29983;&#20135;&#34892;&#19994;&#20013;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#24403;&#21069;&#23398;&#26415;&#30028;XAI&#30740;&#31350;&#30340;&#29616;&#29366;&#30456;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;&#24191;&#27867;&#30340;&#35775;&#35848;&#21644;&#25991;&#29486;&#22238;&#39038;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#35775;&#35848;&#32467;&#26524;&#19982;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.07882</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#20247;&#22810;&#38754;&#35980;&#65306;&#24037;&#19994;&#29616;&#23454;&#19982;&#24403;&#21069;&#30740;&#31350;&#29366;&#20917;
&lt;/p&gt;
&lt;p&gt;
The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research. (arXiv:2310.07882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#29983;&#20135;&#34892;&#19994;&#20013;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#24403;&#21069;&#23398;&#26415;&#30028;XAI&#30740;&#31350;&#30340;&#29616;&#29366;&#30456;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;&#24191;&#27867;&#30340;&#35775;&#35848;&#21644;&#25991;&#29486;&#22238;&#39038;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#35775;&#35848;&#32467;&#26524;&#19982;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#29983;&#20135;&#34892;&#19994;&#20013;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#24403;&#21069;&#23398;&#26415;&#30028;XAI&#30740;&#31350;&#30340;&#29616;&#29366;&#30456;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#22522;&#20110;&#23545;&#24403;&#21069;&#24037;&#19994;&#23454;&#36341;&#20013;XAI&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#35282;&#33394;&#21644;&#36866;&#29992;&#24615;&#20197;&#21450;&#26410;&#26469;&#30340;&#39044;&#26399;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35775;&#35848;&#12290;&#35775;&#35848;&#23545;&#35937;&#21253;&#25324;&#26469;&#33258;&#19981;&#21516;&#34892;&#19994;&#37096;&#38376;&#30340;&#21508;&#31181;&#35282;&#33394;&#21644;&#20851;&#38190;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#23545;&#30456;&#20851;&#25991;&#29486;&#30340;&#31616;&#26126;&#22238;&#39038;&#65292;&#27010;&#36848;&#20102;XAI&#30740;&#31350;&#30340;&#29616;&#29366;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#34987;&#35843;&#26597;&#32773;&#30340;&#24847;&#35265;&#20197;&#21450;&#24403;&#21069;&#23398;&#26415;&#30740;&#31350;&#30340;&#29616;&#29366;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#35775;&#35848;&#32467;&#26524;&#19982;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#24046;&#24322;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;XAI&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the practical relevance of explainable artificial intelligence (XAI) with a special focus on the producing industries and relate them to the current state of academic XAI research. Our findings are based on an extensive series of interviews regarding the role and applicability of XAI along the Machine Learning (ML) lifecycle in current industrial practice and its expected relevance in the future. The interviews were conducted among a great variety of roles and key stakeholders from different industry sectors. On top of that, we outline the state of XAI research by providing a concise review of the relevant literature. This enables us to provide an encompassing overview covering the opinions of the surveyed persons as well as the current state of academic research. By comparing our interview results with the current research approaches we reveal several discrepancies. While a multitude of different XAI approaches exists, most of them are centered around the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeePref&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#20869;&#23481;&#20132;&#20184;&#32593;&#32476;&#20013;&#36827;&#34892;&#22312;&#32447;&#35270;&#39057;&#20869;&#23481;&#39044;&#21462;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#20256;&#32479;&#30340;&#39044;&#21462;&#25216;&#26415;&#38590;&#20197;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#21464;&#21270;&#65292;&#32780;DeePref&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;&#35775;&#38382;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.07881</link><description>&lt;p&gt;
DeePref: &#22312;&#20869;&#23481;&#20132;&#20184;&#32593;&#32476;&#20013;&#30340;&#35270;&#39057;&#39044;&#21462;&#20013;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks. (arXiv:2310.07881v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DeePref&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#20869;&#23481;&#20132;&#20184;&#32593;&#32476;&#20013;&#36827;&#34892;&#22312;&#32447;&#35270;&#39057;&#20869;&#23481;&#39044;&#21462;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#20256;&#32479;&#30340;&#39044;&#21462;&#25216;&#26415;&#38590;&#20197;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#21464;&#21270;&#65292;&#32780;DeePref&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29992;&#25143;&#35775;&#38382;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#20132;&#20184;&#32593;&#32476;&#25215;&#36733;&#20102;&#22823;&#37096;&#20998;&#30340;&#20114;&#32852;&#32593;&#27969;&#37327;&#65292;&#23545;&#35270;&#39057;&#20869;&#23481;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#32531;&#23384;&#21644;&#39044;&#21462;&#20248;&#21270;&#31639;&#27861;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#12290;&#39044;&#21462;&#30340;&#30446;&#26631;&#26159;&#22312;&#35831;&#27714;&#32773;&#21457;&#20986;&#35831;&#27714;&#20043;&#21069;&#23558;&#25968;&#25454;&#25552;&#21069;&#25918;&#20837;&#32531;&#23384;&#20013;&#65292;&#20197;&#20943;&#23569;&#35775;&#38382;&#26102;&#38388;&#24182;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;&#20256;&#32479;&#30340;&#39044;&#21462;&#25216;&#26415;&#36866;&#24212;&#29305;&#23450;&#30340;&#35775;&#38382;&#27169;&#24335;&#65292;&#20294;&#24456;&#38590;&#36866;&#24212;&#24037;&#20316;&#36127;&#36733;&#20013;&#30340;&#31361;&#21464;&#25110;&#38543;&#26426;&#21270;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#24212;&#23545;&#29992;&#25143;&#35775;&#38382;&#27169;&#24335;&#30340;&#21464;&#21270;&#65292;&#24182;&#38543;&#26102;&#38388;&#33258;&#21160;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeePref&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#20869;&#23481;&#20132;&#20184;&#32593;&#32476;&#20013;&#22312;&#32447;&#35270;&#39057;&#20869;&#23481;&#39044;&#21462;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content Delivery Networks carry the majority of Internet traffic, and the increasing demand for video content as a major IP traffic across the Internet highlights the importance of caching and prefetching optimization algorithms. Prefetching aims to make data available in the cache before the requester places its request to reduce access time and improve the Quality of Experience on the user side. Prefetching is well investigated in operating systems, compiler instructions, in-memory cache, local storage systems, high-speed networks, and cloud systems. Traditional prefetching techniques are well adapted to a particular access pattern, but fail to adapt to sudden variations or randomization in workloads. This paper explores the use of reinforcement learning to tackle the changes in user access patterns and automatically adapt over time. To this end, we propose, DeePref, a Deep Reinforcement Learning agent for online video content prefetching in Content Delivery Networks. DeePref is a pr
&lt;/p&gt;</description></item><item><title>TabLib&#26159;&#19968;&#20010;&#21253;&#21547;&#19978;&#20159;&#34920;&#26684;&#21644;&#19978;&#30334;&#20159;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#34920;&#26684;&#27169;&#24577;&#19979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07875</link><description>&lt;p&gt;
TabLib&#65306;&#19968;&#20010;&#21253;&#21547;&#19978;&#20159;&#34920;&#26684;&#21644;&#19978;&#30334;&#20159;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TabLib: A Dataset of 627M Tables with Context. (arXiv:2310.07875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07875
&lt;/p&gt;
&lt;p&gt;
TabLib&#26159;&#19968;&#20010;&#21253;&#21547;&#19978;&#20159;&#34920;&#26684;&#21644;&#19978;&#30334;&#20159;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#34920;&#26684;&#27169;&#24577;&#19979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#22312;&#25552;&#21319;&#29616;&#20195;AI&#31995;&#32479;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#19979;&#30340;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19982;&#25991;&#26412;&#21644;&#22270;&#20687;&#21487;&#27604;&#25311;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"TabLib"&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;6.27&#20159;&#20010;&#34920;&#26684;&#21644;86.7&#20159;&#20010;&#19978;&#19979;&#25991;&#20196;&#29260;&#24635;&#20849;&#36798;&#21040;69 TiB&#30340;&#32534;&#35793;&#25968;&#25454;&#38598;&#12290;TabLib&#30340;&#25968;&#25454;&#26469;&#33258;&#22810;&#20010;&#25991;&#20214;&#26684;&#24335;&#65292;&#21253;&#25324;CSV&#12289;HTML&#12289;SQLite&#12289;PDF&#12289;Excel&#31561;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#33258;GitHub&#21644;Common Crawl&#12290;TabLib&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#22312;&#34920;&#26684;&#27169;&#24577;&#19979;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22914;&#21516;&#21407;&#22987;&#30340;&#29992;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22522;&#30784;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;The Pile&#21644;LAION&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well-established that large, diverse datasets play a pivotal role in the performance of modern AI systems for text and image modalities. However, there are no datasets for tabular data of comparable size and diversity to those available for text and images. Thus we present "TabLib'', a compilation of 627 million tables totaling 69 TiB, along with 867B tokens of context. TabLib was extracted from numerous file formats, including CSV, HTML, SQLite, PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and diversity of TabLib offer considerable promise in the table modality, reminiscent of the original promise of foundational datasets for text and images, such as The Pile and LAION.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#21644;&#26426;&#21046;&#35774;&#35745;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25293;&#21334;&#20013;&#22788;&#29702;&#22823;&#37327;&#21830;&#21697;&#21644;&#31574;&#30053;&#25307;&#26631;&#20154;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20027;&#39064;&#27169;&#22411;&#26469;&#36817;&#20284;&#25237;&#26631;&#20154;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#35774;&#35745;&#20986;&#30456;&#24212;&#30340;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#26426;&#21046;&#30340;&#31283;&#23450;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07874</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#22238;&#24402;&#23545;&#36817;&#20284;&#32467;&#26500;&#30340;&#20808;&#39564;&#36827;&#34892;&#31934;&#32454;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Refined Mechanism Design for Approximately Structured Priors via Active Regression. (arXiv:2310.07874v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#21644;&#26426;&#21046;&#35774;&#35745;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#22312;&#25293;&#21334;&#20013;&#22788;&#29702;&#22823;&#37327;&#21830;&#21697;&#21644;&#31574;&#30053;&#25307;&#26631;&#20154;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#20027;&#39064;&#27169;&#22411;&#26469;&#36817;&#20284;&#25237;&#26631;&#20154;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#35774;&#35745;&#20986;&#30456;&#24212;&#30340;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#26426;&#21046;&#30340;&#31283;&#23450;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26377;&#22823;&#37327;&#21830;&#21697;m&#20986;&#21806;&#32473;n&#20010;&#31574;&#30053;&#25307;&#26631;&#20154;&#30340;&#26368;&#22823;&#21270;&#25910;&#20837;&#21334;&#26041;&#30340;&#38382;&#39064;&#65292;&#20182;&#20204;&#30340;&#20272;&#20540;&#26159;&#20174;&#39640;&#32500;&#26410;&#30693;&#30340;&#20808;&#39564;&#20998;&#24067;&#20013;&#29420;&#31435;&#25277;&#21462;&#30340;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#29978;&#33267;&#36817;&#20284;&#26368;&#20248;&#30340;&#26426;&#21046;&#24456;&#38590;&#34920;&#36798;&#25110;&#35745;&#31639;&#65292;&#32780;&#19988;&#21363;&#20351;&#25214;&#21040;&#20102;&#65292;&#36890;&#24120;&#20063;&#20855;&#26377;&#21508;&#31181;&#21453;&#30452;&#35273;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#26681;&#25454;Cai&#21644;Daskalakis&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#32771;&#34385;&#25237;&#26631;&#20154;&#30340;&#20808;&#39564;&#20998;&#24067;&#21487;&#20197;&#34987;&#19968;&#20010;&#20027;&#39064;&#27169;&#22411;&#24456;&#22909;&#22320;&#36817;&#20284;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36127;&#36131;&#19982;&#25237;&#26631;&#20154;&#36827;&#34892;&#20132;&#20114;&#24182;&#36755;&#20986;&#20854;&#31867;&#22411;&#30340;&#20302;&#32500;&#36817;&#20284;&#30340;&#20027;&#21160;&#23398;&#20064;&#32452;&#20214;&#65292;&#20197;&#21450;&#19968;&#20010;&#36127;&#36131;&#20026;&#20302;&#32500;&#27169;&#22411;&#35774;&#35745;&#26426;&#21046;&#20197;&#36866;&#24212;&#21069;&#19968;&#32452;&#20214;&#30340;&#36817;&#20284;&#31867;&#22411;&#30340;&#26426;&#21046;&#35774;&#35745;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of a revenue-maximizing seller with a large number of items $m$ for sale to $n$ strategic bidders, whose valuations are drawn independently from high-dimensional, unknown prior distributions. It is well-known that optimal and even approximately-optimal mechanisms for this setting are notoriously difficult to characterize or compute, and, even when they can be found, are often rife with various counter-intuitive properties. In this paper, following a model introduced recently by Cai and Daskalakis~\cite{cai2022recommender}, we consider the case that bidders' prior distributions can be well-approximated by a topic model. We design an active learning component, responsible for interacting with the bidders and outputting low-dimensional approximations of their types, and a mechanism design component, responsible for robustifying mechanisms for the low-dimensional model to work for the approximate types of the former component. On the active learning front, we cast o
&lt;/p&gt;</description></item><item><title>QArchSearch&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;QTensor&#24211;&#20316;&#20026;&#21518;&#31471;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#21270;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#21644;&#36755;&#20837;&#37327;&#23376;&#24577;&#30340;&#27169;&#22411;&#12290;&#23427;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#22411;&#37327;&#23376;&#30005;&#36335;&#65292;&#24182;&#19988;&#33021;&#22815;&#25506;&#32034;&#19981;&#21516;&#30340;&#22797;&#26434;&#27169;&#22411;&#65292;&#20026;&#19981;&#21516;&#30340;&#37327;&#23376;&#24212;&#29992;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.07858</link><description>&lt;p&gt;
QArchSearch:&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
QArchSearch: A Scalable Quantum Architecture Search Package. (arXiv:2310.07858v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07858
&lt;/p&gt;
&lt;p&gt;
QArchSearch&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#20351;&#29992;QTensor&#24211;&#20316;&#20026;&#21518;&#31471;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#21270;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#21644;&#36755;&#20837;&#37327;&#23376;&#24577;&#30340;&#27169;&#22411;&#12290;&#23427;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#22411;&#37327;&#23376;&#30005;&#36335;&#65292;&#24182;&#19988;&#33021;&#22815;&#25506;&#32034;&#19981;&#21516;&#30340;&#22797;&#26434;&#27169;&#22411;&#65292;&#20026;&#19981;&#21516;&#30340;&#37327;&#23376;&#24212;&#29992;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#37327;&#23376;&#35745;&#31639;&#26102;&#20195;&#20135;&#29983;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#25215;&#35834;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#34429;&#28982;&#36825;&#20123;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#28508;&#22312;&#30340;&#25351;&#25968;&#32423;&#21152;&#36895;&#65292;&#20294;&#20851;&#20110;&#22914;&#20309;&#35774;&#35745;&#21512;&#36866;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#23454;&#29616;&#36866;&#24403;&#30340;&#24186;&#27491;&#21464;&#25442;&#24212;&#29992;&#20110;&#36755;&#20837;&#30340;&#37327;&#23376;&#24577;&#21364;&#27809;&#26377;&#22826;&#22810;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#36719;&#20214;&#21253;\texttt{QArchSearch}&#65292;&#20854;&#20351;&#29992;\texttt{QTensor}&#24211;&#20316;&#20026;&#21518;&#31471;&#65292;&#20026;&#32473;&#23450;&#20219;&#21153;&#21644;&#36755;&#20837;&#30340;&#37327;&#23376;&#24577;&#23547;&#25214;&#26368;&#20339;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#25628;&#32034;&#36719;&#20214;&#21253;&#33021;&#22815;&#23558;&#25628;&#32034;&#26377;&#25928;&#25193;&#23637;&#21040;&#22823;&#22411;&#37327;&#23376;&#30005;&#36335;&#65292;&#24182;&#19988;&#33021;&#22815;&#25506;&#32034;&#19981;&#21516;&#37327;&#23376;&#24212;&#29992;&#30340;&#26356;&#22797;&#26434;&#27169;&#22411;&#12290;\texttt{QArchSearch}&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#31995;&#32479;&#19978;&#36890;&#36807;CPU&#21644;GPU&#30340;&#20004;&#32423;&#24182;&#34892;&#21270;&#26041;&#26696;&#20197;&#35268;&#27169;&#21644;&#39640;&#25928;&#29575;&#36816;&#34892;&#65292;&#36825;&#24050;&#32463;&#22312;Polari&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current era of quantum computing has yielded several algorithms that promise high computational efficiency. While the algorithms are sound in theory and can provide potentially exponential speedup, there is little guidance on how to design proper quantum circuits to realize the appropriate unitary transformation to be applied to the input quantum state. In this paper, we present \texttt{QArchSearch}, an AI based quantum architecture search package with the \texttt{QTensor} library as a backend that provides a principled and automated approach to finding the best model given a task and input quantum state. We show that the search package is able to efficiently scale the search to large quantum circuits and enables the exploration of more complex models for different quantum applications. \texttt{QArchSearch} runs at scale and high efficiency on high-performance computing systems using a two-level parallelization scheme on both CPUs and GPUs, which has been demonstrated on the Polari
&lt;/p&gt;</description></item><item><title>CrIBo&#36890;&#36807;&#36328;&#22270;&#20687;&#23545;&#35937;&#32423;&#24341;&#23548;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#23494;&#38598;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#33258;&#28982;&#29702;&#35299;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07855</link><description>&lt;p&gt;
CrIBo: &#36890;&#36807;&#36328;&#22270;&#20687;&#23545;&#35937;&#32423;&#24341;&#23548;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping. (arXiv:2310.07855v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07855
&lt;/p&gt;
&lt;p&gt;
CrIBo&#36890;&#36807;&#36328;&#22270;&#20687;&#23545;&#35937;&#32423;&#24341;&#23548;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#23494;&#38598;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#33258;&#28982;&#29702;&#35299;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#26368;&#36817;&#37051;&#26816;&#32034;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26102;&#65292;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#20687;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#20197;&#22330;&#26223;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#38480;&#21046;&#65292;&#22312;&#22270;&#20687;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#20165;&#22312;&#20840;&#23616;&#34920;&#31034;&#20013;&#34987;&#38544;&#21547;&#22320;&#25429;&#33719;&#12290;&#36825;&#31181;&#20840;&#23616;&#24341;&#23548;&#21487;&#33021;&#23548;&#33268;&#23545;&#35937;&#34920;&#31034;&#30340;&#19981;&#21487;&#21462;&#30340;&#32416;&#32544;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#32454;&#31890;&#24230;&#30340;&#24341;&#23548;&#26041;&#27861;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#22686;&#24378;&#23494;&#38598;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#36328;&#22270;&#20687;&#23545;&#35937;&#32423;&#24341;&#23548;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#23545;&#35937;&#32423;&#26368;&#36817;&#37051;&#24341;&#23548;&#65292;CrIBo&#25104;&#20026;&#19968;&#20010;&#26126;&#26174;&#24378;&#22823;&#21644;&#36866;&#24403;&#30340;&#20505;&#36873;&#26041;&#26696;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#27979;&#35797;&#26102;&#21033;&#29992;&#26368;&#36817;&#37051;&#26816;&#32034;&#12290;CrIBo&#22312;&#21518;&#19968;&#20010;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26356;&#26631;&#20934;&#30340;&#19979;&#28216;&#33258;&#28982;&#29702;&#35299;&#24212;&#29992;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging nearest neighbor retrieval for self-supervised representation learning has proven beneficial with object-centric images. However, this approach faces limitations when applied to scene-centric datasets, where multiple objects within an image are only implicitly captured in the global representation. Such global bootstrapping can lead to undesirable entanglement of object representations. Furthermore, even object-centric datasets stand to benefit from a finer-grained bootstrapping approach. In response to these challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method tailored to enhance dense visual representation learning. By employing object-level nearest neighbor bootstrapping throughout the training, CrIBo emerges as a notably strong and adequate candidate for in-context learning, leveraging nearest neighbor retrieval at test time. CrIBo shows state-of-the-art performance on the latter task while being highly competitive in more standard downstream se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#20811;&#26381;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#33021;&#22815;&#23454;&#29616;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#65292;&#24182;&#20855;&#26377;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.07852</link><description>&lt;p&gt;
&#20851;&#20110;&#36890;&#36807;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#39640;&#32500;&#31169;&#26377;&#27169;&#22411;&#36873;&#25321;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism. (arXiv:2310.07852v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#20811;&#26381;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#33021;&#22815;&#23454;&#29616;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#65292;&#24182;&#20855;&#26377;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24046;&#20998;&#38544;&#31169;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#25928;&#29992;&#20445;&#35777;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#24191;&#20026;&#20154;&#30693;&#30340;&#25351;&#25968;&#26426;&#21046;&#26469;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#65292;&#24314;&#31435;&#20102;&#20854;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#25351;&#25968;&#26426;&#21046;&#30340;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#35745;&#31639;&#29942;&#39048;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#36827;&#34892;&#37319;&#26679;&#27493;&#39588;&#65292;&#24182;&#22312;&#38382;&#39064;&#21442;&#25968;$n$&#12289;$p$&#21644;$s$&#20013;&#24314;&#31435;&#20102;&#20854;&#21040;&#31283;&#24577;&#20998;&#24067;&#30340;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#20854;&#28151;&#21512;&#24615;&#36136;&#24314;&#31435;&#20102;Metropolis-Hastings&#38543;&#26426;&#34892;&#36208;&#30340;&#26368;&#32456;&#20272;&#35745;&#30340;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#35828;&#26126;&#24615;&#27169;&#25311;&#65292;&#21360;&#35777;&#20102;&#25105;&#20204;&#20027;&#35201;&#32467;&#26524;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of model selection in a high-dimensional sparse linear regression model under the differential privacy framework. In particular, we consider the problem of differentially private best subset selection and study its utility guarantee. We adopt the well-known exponential mechanism for selecting the best model, and under a certain margin condition, we establish its strong model recovery property. However, the exponential search space of the exponential mechanism poses a serious computational bottleneck. To overcome this challenge, we propose a Metropolis-Hastings algorithm for the sampling step and establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$. Furthermore, we also establish approximate differential privacy for the final estimates of the Metropolis-Hastings random walk using its mixing property. Finally, we also perform some illustrative simulations that echo the theoretical findings of our main results
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07838</link><description>&lt;p&gt;
&#25506;&#32034;&#26377;&#38480;&#39046;&#22495;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#39046;&#22495;&#20013;&#20174;&#25945;&#24072;&#21040;&#23398;&#29983;&#20998;&#31867;&#22120;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21457;&#29616;&#29305;&#26435;&#20449;&#24687;&#20250;&#21152;&#36895;&#20256;&#36882;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36798;&#21040;&#20102;&#30693;&#35782;&#20256;&#36882;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#36890;&#36807;&#20174;&#25945;&#24072;&#21040;&#27010;&#29575;&#21270;&#23398;&#29983;&#20998;&#31867;&#22120;&#30340;n&#20010;&#26679;&#26412;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#32479;&#35745;&#25928;&#29575;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20854;&#20013;&#36755;&#20837;&#31354;&#38388;S&#21644;&#26631;&#31614;A&#20026;&#26377;&#38480;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19977;&#20010;&#28176;&#36827;&#32423;&#21035;&#19978;&#30340;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#21152;&#24555;&#20256;&#36882;&#30340;&#36895;&#24230;&#12290;&#22312;&#31532;&#19968;&#32423;&#21035;&#19978;&#65292;&#21482;&#26377;&#20855;&#26377;&#22256;&#38590;&#26631;&#31614;&#30340;&#26679;&#26412;&#26159;&#24050;&#30693;&#30340;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#33021;&#22815;&#36798;&#21040;&#26368;&#23567;&#21270;&#36895;&#29575;sqrt(|S||A|/n)&#12290;&#31532;&#20108;&#32423;&#21035;&#19978;&#65292;&#38500;&#20102;&#24050;&#30693;&#30340;&#22256;&#38590;&#26631;&#31614;&#26679;&#26412;&#22806;&#65292;&#36824;&#26377;&#37319;&#26679;&#26631;&#31614;&#30340;&#25945;&#24072;&#27010;&#29575;&#21487;&#29992;&#65292;&#36825;&#23558;&#25910;&#25947;&#36895;&#24230;&#30340;&#19979;&#30028;&#25552;&#39640;&#21040;|S||A|/n&#12290;&#28982;&#32780;&#65292;&#22312;&#31532;&#20108;&#20010;&#25968;&#25454;&#37319;&#38598;&#21327;&#35758;&#19979;&#65292;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#26420;&#32032;&#36866;&#24212;&#20250;&#23548;&#33268;&#28176;&#36817;&#20559;&#24046;&#30340;&#23398;&#29983;&#12290;&#25105;&#20204;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#39564;&#21464;&#20307;&#30340;&#24179;&#26041;&#35823;&#24046;&#36923;&#36753;&#25439;&#22833;&#26469;&#23454;&#29616;&#20102;&#22522;&#26412;&#38480;&#21046;&#12290;&#31532;&#19977;&#32423;&#21035;&#36827;&#19968;&#27493;&#36171;&#20104;&#23398;&#29983;&#36719;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#24449;&#31232;&#30095;&#24615;&#30340;&#25104;&#21151;&#65292;&#24182;&#27979;&#35797;&#20102;&#32447;&#24615;&#24615;&#21644;&#31232;&#30095;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#20026;&#29305;&#24449;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#24182;&#19988;&#22312;&#31532;&#19968;&#23618;&#21644;&#26368;&#21518;&#19968;&#23618;&#20013;&#21576;&#26368;&#31232;&#30095;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.07837</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#34913;&#37327;&#29305;&#24449;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring Feature Sparsity in Language Models. (arXiv:2310.07837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#24449;&#31232;&#30095;&#24615;&#30340;&#25104;&#21151;&#65292;&#24182;&#27979;&#35797;&#20102;&#32447;&#24615;&#24615;&#21644;&#31232;&#30095;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#20026;&#29305;&#24449;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#24182;&#19988;&#22312;&#31532;&#19968;&#23618;&#21644;&#26368;&#21518;&#19968;&#23618;&#20013;&#21576;&#26368;&#31232;&#30095;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28608;&#27963;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19982;&#36755;&#20837;&#25991;&#26412;&#29305;&#24449;&#23545;&#24212;&#30340;&#21521;&#37327;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#12290;&#22312;&#36825;&#20010;&#20551;&#35774;&#19979;&#65292;&#36825;&#20123;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#31232;&#30095;&#32534;&#30721;&#37325;&#26500;&#29305;&#24449;&#26041;&#21521;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20123;&#31232;&#30095;&#32534;&#30721;&#25216;&#26415;&#30340;&#25104;&#21151;&#65292;&#24182;&#27979;&#35797;&#32447;&#24615;&#24615;&#21644;&#31232;&#30095;&#24615;&#20551;&#35774;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#39044;&#27979;&#21512;&#25104;&#31232;&#30095;&#32447;&#24615;&#28608;&#27963;&#30340;&#31232;&#30095;&#31243;&#24230;&#65292;&#24182;&#33021;&#22815;&#21306;&#20998;&#31232;&#30095;&#32447;&#24615;&#25968;&#25454;&#21644;&#20854;&#20182;&#20960;&#31181;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#34913;&#37327;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#23545;&#29031;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#28608;&#27963;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#20026;&#29305;&#24449;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#24182;&#19988;&#22312;&#31532;&#19968;&#23618;&#21644;&#26368;&#21518;&#19968;&#23618;&#20013;&#21576;&#26368;&#31232;&#30095;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32454;&#21270;&#20998;&#26512;&#23398;&#20064;&#29575;&#35843;&#24230;&#26469;&#35299;&#20915;&#23454;&#36341;&#20013;&#23398;&#20064;&#29575;&#35843;&#25972;&#19982;&#29702;&#35770;&#30340;&#19981;&#19968;&#33268;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#33539;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#32454;&#21270;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07831</link><description>&lt;p&gt;
&#20309;&#26102;&#65292;&#20026;&#20160;&#20040;&#20197;&#21450;&#22810;&#23569;&#65311;&#36890;&#36807;&#32454;&#21270;&#36827;&#34892;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement. (arXiv:2310.07831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#32454;&#21270;&#20998;&#26512;&#23398;&#20064;&#29575;&#35843;&#24230;&#26469;&#35299;&#20915;&#23454;&#36341;&#20013;&#23398;&#20064;&#29575;&#35843;&#25972;&#19982;&#29702;&#35770;&#30340;&#19981;&#19968;&#33268;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#33539;&#25968;&#36827;&#34892;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#32454;&#21270;&#35843;&#24230;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#23398;&#20064;&#29575;&#35843;&#24230;&#19982;&#29702;&#35770;&#25512;&#33616;&#30340;&#20960;&#20046;&#23436;&#20840;&#19981;&#21516;&#12290;&#25105;&#20204;&#32553;&#23567;&#20102;&#22823;&#37096;&#20998;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#22240;&#27492;&#33021;&#22815;&#25512;&#23548;&#20986;&#26032;&#30340;&#38382;&#39064;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#35843;&#24230;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#23545;&#24191;&#27867;&#31867;&#21035;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#21253;&#25324;SGD&#65289;&#30340;&#23398;&#20064;&#29575;&#35843;&#24230;&#36827;&#34892;&#32454;&#21270;&#20998;&#26512;&#12290;&#19982;&#22823;&#22810;&#25968;&#21069;&#26399;&#30740;&#31350;&#21482;&#30740;&#31350;&#24179;&#22343;&#36845;&#20195;&#30340;&#25910;&#25947;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#30740;&#31350;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#20154;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#12290;&#24403;&#20165;&#32771;&#34385;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#26102;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#39044;&#27979;&#26368;&#20339;&#36873;&#25321;&#26159;&#32447;&#24615;&#34928;&#20943;&#35843;&#24230;&#65306;&#36825;&#26159;&#19968;&#31181;&#23454;&#36341;&#20013;&#24120;&#29992;&#30340;&#36873;&#25321;&#65292;&#20854;&#23558;&#27493;&#38271;&#19982;&#24403;&#21069;&#36845;&#20195;&#27425;&#25968;t&#21644;&#24635;&#27493;&#25968;T&#25104;&#27604;&#20363;&#22320;&#35774;&#32622;&#20026;1 - t/T&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#31181;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#33539;&#25968;&#26469;&#25512;&#23548;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#32454;&#21270;&#35843;&#24230;&#12290;&#36825;&#20123;&#32454;&#21270;&#35843;&#24230;&#34920;&#29616;&#20986;&#23398;&#20064;&#29575;&#36880;&#28176;&#22686;&#21152;&#21644;&#23398;&#20064;&#29575;&#36805;&#36895;&#36864;&#28779;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rate schedules used in practice bear little resemblance to those recommended by theory. We close much of this theory/practice gap, and as a consequence are able to derive new problem-adaptive learning rate schedules. Our key technical contribution is a refined analysis of learning rate schedules for a wide class of optimization algorithms (including SGD). In contrast to most prior works that study the convergence of the average iterate, we study the last iterate, which is what most people use in practice. When considering only worst-case analysis, our theory predicts that the best choice is the linear decay schedule: a popular choice in practice that sets the stepsize proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the total number of steps. To go beyond this worst-case analysis, we use the observed gradient norms to derive schedules refined for any particular task. These refined schedules exhibit learning rate warm-up and rapid learning rate anneali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NLP&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#27169;&#26495;&#30340;&#38382;&#39064;&#29983;&#25104;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#20248;&#21183;&#21644;&#22266;&#26377;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#23545;&#29616;&#20195;Transformer&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#25152;&#38656;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.07830</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#26159;&#21542;&#33021;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#39640;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Synthetic Data Make Large Language Models More Efficient?. (arXiv:2310.07830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NLP&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#27169;&#26495;&#30340;&#38382;&#39064;&#29983;&#25104;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#20248;&#21183;&#21644;&#22266;&#26377;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#23545;&#29616;&#20195;Transformer&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24378;&#35843;&#20102;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#25152;&#38656;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#32463;&#21382;&#20102;&#28145;&#21051;&#30340;&#21464;&#38761;&#12290;&#30740;&#31350;&#20154;&#21592;&#25345;&#32493;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#39537;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NLP&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#27169;&#26495;&#30340;&#38382;&#39064;&#29983;&#25104;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#20248;&#21183;&#65292;&#21253;&#25324;&#25968;&#25454;&#25193;&#20805;&#28508;&#21147;&#21644;&#32467;&#26500;&#22810;&#26679;&#24615;&#30340;&#24341;&#20837;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#20248;&#28857;&#19982;&#22266;&#26377;&#38480;&#21046;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#22914;&#36807;&#25311;&#21512;&#39118;&#38505;&#21644;&#39044;&#23450;&#20041;&#27169;&#26495;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#32463;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340;&#21512;&#25104;&#25968;&#25454;&#23545;&#29616;&#20195;Transformer&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#24378;&#35843;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#25152;&#38656;&#30340;&#24494;&#22937;&#24179;&#34913;&#21450;&#23558;&#21512;&#25104;&#25968;&#25454;&#25972;&#21512;&#21040;&#27169;&#22411;&#35757;&#32451;&#27969;&#31243;&#20013;&#30340;&#26410;&#26469;&#36712;&#36857;&#26469;&#24635;&#32467;&#12290;&#36825;&#20123;&#21457;&#29616;&#26088;&#22312;&#25351;&#23548;NLP&#20174;&#19994;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) has undergone transformative changes with the advent of deep learning methodologies. One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation. By assessing its advantages, including data augmentation potential and the introduction of structured variety, we juxtapose these benefits against inherent limitations, such as the risk of overfitting and the constraints posed by pre-defined templates. Drawing from empirical evaluations, we demonstrate the impact of template-based synthetic data on the performance of modern transformer models. We conclude by emphasizing the delicate balance required between synthetic and real-world data, and the future trajectories of integrating synthetic data in model training pipelines. The findings aim to guide NLP practitioners in
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;LLaMA-2&#33021;&#22815;&#20196;&#20154;&#24778;&#35766;&#22320;&#38646;-shot&#22806;&#25512;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#25110;&#36229;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#36825;&#26159;&#22240;&#20026;LLMs&#20855;&#26377;&#33258;&#28982;&#22320;&#34920;&#31034;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#22797;&#23395;&#33410;&#36235;&#21183;&#29305;&#24449;&#30456;&#19968;&#33268;&#30340;&#31616;&#21333;&#24615;&#21644;&#37325;&#22797;&#24615;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07820</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Zero-Shot Time Series Forecasters. (arXiv:2310.07820v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07820
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;LLaMA-2&#33021;&#22815;&#20196;&#20154;&#24778;&#35766;&#22320;&#38646;-shot&#22806;&#25512;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#25110;&#36229;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#36825;&#26159;&#22240;&#20026;LLMs&#20855;&#26377;&#33258;&#28982;&#22320;&#34920;&#31034;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#22797;&#23395;&#33410;&#36235;&#21183;&#29305;&#24449;&#30456;&#19968;&#33268;&#30340;&#31616;&#21333;&#24615;&#21644;&#37325;&#22797;&#24615;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#32534;&#30721;&#20026;&#19968;&#31995;&#21015;&#25968;&#23383;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35270;&#20026;&#25991;&#26412;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#12290;&#22312;&#24320;&#21457;&#36825;&#31181;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20363;&#22914;GPT-3&#21644;LLaMA-2&#21487;&#20197;&#20196;&#20154;&#24778;&#35766;&#22320;&#38646;-shot&#22806;&#25512;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;&#25110;&#36229;&#36807;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#30340;&#19987;&#38376;&#35774;&#35745;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#26631;&#35760;&#21270;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24182;&#23558;&#31163;&#25955;&#20998;&#24067;&#36716;&#25442;&#20026;&#39640;&#24230;&#28789;&#27963;&#30340;&#36830;&#32493;&#20540;&#23494;&#24230;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#25104;&#21151;&#28304;&#20110;&#23427;&#20204;&#33258;&#28982;&#22320;&#34920;&#31034;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#32467;&#21512;&#20102;&#31616;&#21333;&#24615;&#21644;&#37325;&#22797;&#24615;&#30340;&#20559;&#35265;&#65292;&#36825;&#19982;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#30340;&#37325;&#22797;&#23395;&#33410;&#36235;&#21183;&#31561;&#26174;&#33879;&#29305;&#24449;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#36890;&#36807;&#38750;&#25968;&#23383;&#25991;&#26412;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#65292;&#20197;&#21450;&#22914;&#20309;&#36866;&#24212;&#25991;&#26412;&#38468;&#21152;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
By encoding time series as a string of numerical digits, we can frame time series forecasting as next-token prediction in text. Developing this approach, we find that large language models (LLMs) such as GPT-3 and LLaMA-2 can surprisingly zero-shot extrapolate time series at a level comparable to or exceeding the performance of purpose-built time series models trained on the downstream tasks. To facilitate this performance, we propose procedures for effectively tokenizing time series data and converting discrete distributions over tokens into highly flexible densities over continuous values. We argue the success of LLMs for time series stems from their ability to naturally represent multimodal distributions, in conjunction with biases for simplicity, and repetition, which align with the salient features in many time series, such as repeated seasonal trends. We also show how LLMs can naturally handle missing data without imputation through non-numerical text, accommodate textual side in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#65292;&#20197;&#35299;&#20915;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26102;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07819</link><description>&lt;p&gt;
&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Faithfulness Measurable Masked Language Models. (arXiv:2310.07819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24230;&#37327;&#24544;&#23454;&#24615;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#65292;&#20197;&#35299;&#20915;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26102;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#37325;&#35201;&#24615;&#24230;&#37327;&#26469;&#34920;&#36798;&#21738;&#20123;&#20196;&#29260;&#23545;&#20110;&#39044;&#27979;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20123;&#35299;&#37322;&#20855;&#26377;&#35828;&#26381;&#21147;&#65292;&#20294;&#24448;&#24448;&#26159;&#38169;&#35823;&#30340;&#12290;&#22240;&#27492;&#65292;&#27979;&#37327;&#23427;&#20204;&#30340;&#24544;&#23454;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#24230;&#37327;&#26631;&#20934;&#26159;&#22914;&#26524;&#20196;&#29260;&#30830;&#23454;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#23631;&#34109;&#23427;&#20204;&#24212;&#35813;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#21464;&#24046;&#12290;&#28982;&#32780;&#65292;&#20196;&#29260;&#23631;&#34109;&#20250;&#24341;&#20837;&#21306;&#22495;&#22806;&#38382;&#39064;&#65292;&#32780;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#35745;&#31639;&#19978;&#24456;&#26114;&#36149;&#24182;&#19988;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20854;&#20182;&#25351;&#26631;&#30340;&#36866;&#29992;&#33539;&#22260;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#24544;&#23454;&#24615;&#21487;&#24230;&#37327;&#27169;&#22411;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#23558;&#23631;&#34109;&#20196;&#29260;&#20316;&#20026;&#35774;&#35745;&#20351;&#20854;&#25104;&#20026;&#20998;&#24067;&#20869;&#12290;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#23436;&#20840;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach to explain NLP models, is to use importance measures that express which tokens are important for a prediction. Unfortunately, such explanations are often wrong despite being persuasive. Therefore, it is essential to measure their faithfulness. One such metric is if tokens are truly important, then masking them should result in worse model performance. However, token masking introduces out-of-distribution issues and existing solutions are computationally expensive and employ proxy-models. Furthermore, other metrics are very limited in scope. In this work, we propose an inherently faithfulness measurable model that addresses these challenges. This is achieved by using a novel fine-tuning method that incorporates masking, such that masking tokens become in-distribution by design. This differs from existing approaches, which are completely model-agnostic but are inapplicable in practice. We demonstrate the generality of our approach by applying it to various tasks and val
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;</title><link>http://arxiv.org/abs/2310.07815</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#32034;&#24341;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models As Semantic Indexers. (arXiv:2310.07815v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;LMINDEXER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#26631;&#35782;&#31526;&#65288;ID&#65289;&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#65292;&#26088;&#22312;&#20445;&#30041;&#23545;&#35937;&#65288;&#22914;&#25991;&#26723;&#21644;&#39033;&#65289;&#20869;&#37096;&#30340;&#35821;&#20041;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#23398;&#20064;&#35821;&#20041;ID&#65292;&#39318;&#20808;&#20351;&#29992;&#29616;&#25104;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#33719;&#21462;&#23884;&#20837;&#65292;&#24182;&#26681;&#25454;&#23884;&#20837;&#26469;&#25512;&#23548;ID&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#19988;&#25991;&#26412;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#30340;&#23884;&#20837;&#20998;&#24067;&#36890;&#24120;&#19982;&#35821;&#20041;&#32034;&#24341;&#25152;&#38656;&#30340;&#39044;&#26399;&#20998;&#24067;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#26082;&#33021;&#23398;&#20064;&#25991;&#26723;&#30340;&#35821;&#20041;&#34920;&#31034;&#21448;&#33021;&#21516;&#26102;&#23398;&#20064;&#20854;&#20998;&#23618;&#32467;&#26500;&#30340;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35821;&#20041;ID&#26159;&#31163;&#25955;&#21644;&#39034;&#24207;&#32467;&#26500;&#30340;&#65292;&#24182;&#19988;&#35821;&#20041;&#30417;&#30563;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LMINDEXER&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#35821;&#20041;ID&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. Nevertheless, it is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMINDEXER, a self-supervised framework to learn semantic IDs with a generative language model. We tackl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20174;&#26131;&#20110;&#23548;&#33322;&#30340;2D&#25506;&#32034;&#31354;&#38388;&#21040;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#23376;&#31354;&#38388;&#30340;&#26144;&#23556;&#65292;&#26469;&#25506;&#32034;&#19968;&#32452;&#26631;&#24535;&#24615;&#24418;&#29366;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.07814</link><description>&lt;p&gt;
&#20174;&#38750;&#32467;&#26500;&#21270;&#29983;&#25104;&#27169;&#22411;&#20013;&#21487;&#25506;&#32034;&#30340;&#32593;&#26684;&#21464;&#24418;&#23376;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Explorable Mesh Deformation Subspaces from Unstructured Generative Models. (arXiv:2310.07814v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20174;&#26131;&#20110;&#23548;&#33322;&#30340;2D&#25506;&#32034;&#31354;&#38388;&#21040;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#23376;&#31354;&#38388;&#30340;&#26144;&#23556;&#65292;&#26469;&#25506;&#32034;&#19968;&#32452;&#26631;&#24535;&#24615;&#24418;&#29366;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;3D&#24314;&#27169;&#24037;&#20855;&#20013;&#65292;&#25506;&#32034;3D&#24418;&#29366;&#30340;&#21464;&#21270;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;3D&#24418;&#29366;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21487;&#20197;&#29992;&#26469;&#20174;&#19968;&#32452;&#36755;&#20837;&#24418;&#29366;&#24320;&#22987;&#25506;&#32034;&#28508;&#22312;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#26679;&#20570;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65306;&#28508;&#22312;&#31354;&#38388;&#26159;&#39640;&#32500;&#30340;&#65292;&#24456;&#38590;&#21487;&#35270;&#21270;&#65292;&#21253;&#21547;&#19982;&#36755;&#20837;&#24418;&#29366;&#26080;&#20851;&#30340;&#24418;&#29366;&#65292;&#24182;&#19988;&#36890;&#36807;&#32447;&#24615;&#36335;&#24452;&#36827;&#34892;&#25506;&#32034;&#24448;&#24448;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24418;&#29366;&#36807;&#28193;&#12290;&#27492;&#22806;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#22312;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#21407;&#22987;&#39640;&#36136;&#37327;&#32593;&#26684;&#20013;&#25506;&#32034;&#21464;&#21270;&#65292;&#32780;&#19981;&#26159;&#20854;&#36739;&#20302;&#36136;&#37327;&#30340;&#36755;&#20986;&#20960;&#20309;&#24418;&#29366;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20174;&#26131;&#20110;&#23548;&#33322;&#30340;2D&#25506;&#32034;&#31354;&#38388;&#21040;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#23376;&#31354;&#38388;&#30340;&#26144;&#23556;&#65292;&#26469;&#25506;&#32034;&#32473;&#23450;&#19968;&#32452;&#26631;&#24535;&#24615;&#24418;&#29366;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#22914;&#20309;&#25214;&#21040;&#35206;&#30422;&#36755;&#20837;&#26631;&#24535;&#24615;&#24418;&#29366;&#24182;&#21576;&#29616;&#24179;&#28369;&#21464;&#21270;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring variations of 3D shapes is a time-consuming process in traditional 3D modeling tools. Deep generative models of 3D shapes often feature continuous latent spaces that can, in principle, be used to explore potential variations starting from a set of input shapes. In practice, doing so can be problematic: latent spaces are high dimensional and hard to visualize, contain shapes that are not relevant to the input shapes, and linear paths through them often lead to sub-optimal shape transitions. Furthermore, one would ideally be able to explore variations in the original high-quality meshes used to train the generative model, not its lower-quality output geometry. In this paper, we present a method to explore variations among a given set of landmark shapes by constructing a mapping from an easily-navigable 2D exploration space to a subspace of a pre-trained generative model. We first describe how to find a mapping that spans the set of input landmark shapes and exhibits smooth vari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#21644;&#32447;&#24615;MDPs&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#24573;&#30053;&#26576;&#20123;&#29366;&#24577;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;MDP&#12290;</title><link>http://arxiv.org/abs/2310.07811</link><description>&lt;p&gt;
&#22312;&#32447;RL&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#20013;&#21644;&#32447;&#24615;MDPs&#19968;&#26679;&#23481;&#26131;&#65292;&#21482;&#35201;&#20320;&#23398;&#20250;&#24573;&#30053;&#12290; (arXiv:2310.07811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Online RL in Linearly $q^\pi$-Realizable MDPs Is as Easy as in Linear MDPs If You Learn What to Ignore. (arXiv:2310.07811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#21644;&#32447;&#24615;MDPs&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#24573;&#30053;&#26576;&#20123;&#29366;&#24577;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;MDP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#31163;&#25955;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#65292;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;&#20551;&#35774;&#19979;&#65292;&#20551;&#35774;&#25152;&#26377;&#31574;&#30053;&#30340;&#21160;&#20316;&#20540;&#21487;&#20197;&#34920;&#31034;&#20026;&#29366;&#24577;-&#21160;&#20316;&#29305;&#24449;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#36825;&#20010;&#31867;&#21035;&#34987;&#35748;&#20026;&#27604;&#32447;&#24615;MDPs&#26356;&#19968;&#33324;&#21270;&#65292;&#20854;&#20013;&#36716;&#31227;&#20869;&#26680;&#21644;&#22870;&#21169;&#20989;&#25968;&#34987;&#20551;&#35774;&#20026;&#29305;&#24449;&#21521;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23637;&#31034;&#20102;&#36825;&#20004;&#20010;&#31867;&#21035;&#20043;&#38388;&#30340;&#24046;&#24322;&#26159;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#20013;&#23384;&#22312;&#19968;&#20123;&#29366;&#24577;&#65292;&#22312;&#36825;&#20123;&#29366;&#24577;&#20013;&#65292;&#23545;&#20110;&#20219;&#20309;&#31574;&#30053;&#65292;&#25152;&#26377;&#30340;&#21160;&#20316;&#20540;&#37117;&#36817;&#20284;&#30456;&#31561;&#65292;&#36890;&#36807;&#36339;&#36807;&#36825;&#20123;&#29366;&#24577;&#24182;&#25353;&#29031;&#20219;&#24847;&#22266;&#23450;&#31574;&#30053;&#22312;&#36825;&#20123;&#29366;&#24577;&#20013;&#36827;&#34892;&#36716;&#25442;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;MDP&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#26032;&#39062;&#65288;&#35745;&#31639;&#25928;&#29575;&#36739;&#20302;&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#24615;$q^\pi$&#21487;&#23454;&#29616;&#30340;MDPs&#20013;&#65292;&#35813;&#31639;&#27861;&#21516;&#26102;&#23398;&#20064;&#20102;&#24212;&#35813;&#36339;&#36807;&#30340;&#29366;&#24577;&#65292;&#24182;&#36816;&#34892;&#21478;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online reinforcement learning (RL) in episodic Markov decision processes (MDPs) under the linear $q^\pi$-realizability assumption, where it is assumed that the action-values of all policies can be expressed as linear functions of state-action features. This class is known to be more general than linear MDPs, where the transition kernel and the reward function are assumed to be linear functions of the feature vectors. As our first contribution, we show that the difference between the two classes is the presence of states in linearly $q^\pi$-realizable MDPs where for any policy, all the actions have approximately equal values, and skipping over these states by following an arbitrarily fixed policy in those states transforms the problem to a linear MDP. Based on this observation, we derive a novel (computationally inefficient) learning algorithm for linearly $q^\pi$-realizable MDPs that simultaneously learns what states should be skipped over and runs another learning algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29109;&#21644;&#23545;&#31216;&#24615;&#26500;&#24314;&#25968;&#25454;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#21306;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#31934;&#24230;&#19981;&#36275;&#21644;&#26080;&#27861;&#22686;&#37327;&#25361;&#25112;&#31639;&#27861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07807</link><description>&lt;p&gt;
FedSym: &#21457;&#25381;&#29109;&#30340;&#21147;&#37327;&#23545;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedSym: Unleashing the Power of Entropy for Benchmarking the Algorithms for Federated Learning. (arXiv:2310.07807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29109;&#21644;&#23545;&#31216;&#24615;&#26500;&#24314;&#25968;&#25454;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#21306;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#31934;&#24230;&#19981;&#36275;&#21644;&#26080;&#27861;&#22686;&#37327;&#25361;&#25112;&#31639;&#27861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29420;&#31435;&#23398;&#20064;&#32773;&#20250;&#31169;&#19979;&#22788;&#29702;&#25968;&#25454;&#65292;&#36890;&#36807;&#32858;&#21512;&#21644;&#37325;&#26032;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#26469;&#21019;&#24314;&#31283;&#20581;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#27169;&#22411;&#32858;&#21512;&#25928;&#26524;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#65292;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#25968;&#25454;&#20998;&#21306;&#26041;&#27861;&#65292;&#23558;&#20026;&#38598;&#20013;&#24335;&#23398;&#20064;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#32452;&#23376;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30446;&#21069;&#27969;&#34892;&#30340;&#25968;&#25454;&#20998;&#21306;&#25216;&#26415;&#65292;&#24182;&#21487;&#35270;&#21270;&#20854;&#20027;&#35201;&#32570;&#28857;&#65306;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#31934;&#24230;&#19981;&#36275;&#65292;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#24322;&#36136;&#24615;&#25351;&#26631;&#65292;&#20197;&#21450;&#26080;&#27861;&#22686;&#37327;&#25361;&#25112;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29109;&#21644;&#23545;&#31216;&#24615;&#26500;&#24314;&#8220;&#26368;&#20855;&#25361;&#25112;&#24615;&#8221;&#21644;&#21487;&#25511;&#25968;&#25454;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a decentralized machine learning approach where independent learners process data privately. Its goal is to create a robust and accurate model by aggregating and retraining local models over multiple rounds. However, FL faces challenges regarding data heterogeneity and model aggregation effectiveness. In order to simulate real-world data, researchers use methods for data partitioning that transform a dataset designated for centralized learning into a group of sub-datasets suitable for distributed machine learning with different data heterogeneity. In this paper, we study the currently popular data partitioning techniques and visualize their main disadvantages: the lack of precision in the data diversity, which leads to unreliable heterogeneity indexes, and the inability to incrementally challenge the FL algorithms. To resolve this problem, we propose a method that leverages entropy and symmetry to construct 'the most challenging' and controllable data distrib
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.07805</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20301;&#38543;&#26426;&#26725;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26159;&#29992;&#20110;&#36830;&#32493;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;DMs&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#65288;&#21363;&#20301;&#32622;&#31354;&#38388;&#65289;&#20013;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21453;&#28436;&#26469;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20301;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#20854;&#20013;&#30456;&#20301;&#31354;&#38388;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#21253;&#25324;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#22686;&#24378;&#31354;&#38388;&#12290;&#21033;&#29992;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#30456;&#20301;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#27979;&#24230;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#12290;&#19982;DMs&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#23601;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#12290;&#36825;&#31181;&#26089;&#26399;&#39044;&#27979;&#20026;&#36890;&#36807;&#27839;&#36712;&#36857;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23567;&#20989;&#25968;&#35780;&#20272;&#25968;&#37327;&#30340;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07800</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21450;&#20854;&#23427;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Explainable Attention for Few-shot Learning and Beyond. (arXiv:2310.07800v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35299;&#37322;&#24615;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#28508;&#21147;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#20013;&#26174;&#33879;&#30340;&#37096;&#20998;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#22312;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#35760;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#23548;&#33268;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#20215;&#20540;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#36807;&#31243;&#21551;&#21457;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#23558;AI&#22522;&#32447;&#26292;&#38706;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20851;&#38190;&#37096;&#20998;&#32780;&#19981;&#26159;&#25972;&#20010;&#36755;&#20837;&#25968;&#25454;&#38598;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24863;&#30693;&#65292;&#37027;&#20040;&#23427;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#26356;&#20934;&#30830;&#12289;&#26356;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#36825;&#20123;&#20449;&#24687;&#24615;&#25968;&#25454;&#37096;&#20998;&#30340;&#20219;&#21153;&#65292;&#21363;&#30828;&#27880;&#24847;&#21147;&#23547;&#25214;&#65292;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24456;&#38590;&#25214;&#21040;&#36825;&#20123;&#20449;&#24687;&#24615;&#21306;&#22495;&#65292;&#21407;&#22240;&#26159;&#22823;&#37327;&#30340;&#35757;&#32451;&#21442;&#25968;&#26080;&#27861;&#20174;&#26377;&#38480;&#30340;&#26679;&#26412;&#20013;&#26377;&#25928;&#23398;&#20064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#30828;&#27880;&#24847;&#21147;&#23547;&#25214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms have exhibited promising potential in enhancing learning models by identifying salient portions of input data. This is particularly valuable in scenarios where limited training samples are accessible due to challenges in data collection and labeling. Drawing inspiration from human recognition processes, we posit that an AI baseline's performance could be more accurate and dependable if it is exposed to essential segments of raw data rather than the entire input dataset, akin to human perception. However, the task of selecting these informative data segments, referred to as hard attention finding, presents a formidable challenge. In situations with few training samples, existing studies struggle to locate such informative regions due to the large number of training parameters that cannot be effectively learned from the available limited samples. In this study, we introduce a novel and practical framework for achieving explainable hard attention finding, specifically
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#36807;&#28193;&#27169;&#22411;&#35299;&#20915;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.07799</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;EMR&#25968;&#25454;&#38598;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#39044;&#21518;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#36807;&#28193;&#27169;&#22411;&#35299;&#20915;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#26032;&#20852;&#30142;&#30149;&#30340;&#20449;&#24687;&#26377;&#38480;&#65292;&#30151;&#29366;&#24456;&#38590;&#34987;&#23519;&#35273;&#21644;&#35748;&#35782;&#21040;&#65292;&#22240;&#27492;&#21487;&#33021;&#24573;&#35270;&#20020;&#24202;&#24178;&#39044;&#30340;&#31383;&#21475;&#12290;&#26399;&#26395;&#33021;&#22815;&#24314;&#31435;&#19968;&#20010;&#26377;&#25928;&#30340;&#39044;&#21518;&#27169;&#22411;&#65292;&#36741;&#21161;&#21307;&#29983;&#36827;&#34892;&#27491;&#30830;&#35786;&#26029;&#21644;&#21046;&#23450;&#20010;&#24615;&#21270;&#27835;&#30103;&#26041;&#26696;&#65292;&#20174;&#32780;&#21450;&#26102;&#39044;&#38450;&#19981;&#21033;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#30142;&#30149;&#26089;&#26399;&#38454;&#27573;&#65292;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#20020;&#24202;&#32463;&#39564;&#26377;&#38480;&#65292;&#20877;&#21152;&#19978;&#23545;&#38544;&#31169;&#21644;&#20262;&#29702;&#30340;&#32771;&#34385;&#65292;&#23548;&#33268;&#21487;&#20379;&#21442;&#32771;&#30340;&#25968;&#25454;&#21463;&#38480;&#65292;&#29978;&#33267;&#38590;&#20197;&#27491;&#30830;&#26631;&#35760;&#25968;&#25454;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#30142;&#30149;&#25110;&#21516;&#19968;&#30142;&#30149;&#19981;&#21516;&#26469;&#28304;&#30340;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#65288;EMR&#65289;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#30340;&#36328;&#25968;&#25454;&#38598;&#29305;&#24449;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20005;&#37325;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24314;&#31435;&#19968;&#20010;&#20174;&#28304;&#25968;&#25454;&#38598;&#21040;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#36807;&#28193;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#36827;&#34892;&#32422;&#26463;&#65292;&#26469;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the limited information about emerging diseases, symptoms are hard to be noticed and recognized, so that the window for clinical intervention could be ignored. An effective prognostic model is expected to assist doctors in making right diagnosis and designing personalized treatment plan, so to promptly prevent unfavorable outcomes. However, in the early stage of a disease, limited data collection and clinical experiences, plus the concern out of privacy and ethics, may result in restricted data availability for reference, to the extent that even data labels are difficult to mark correctly. In addition, Electronic Medical Record (EMR) data of different diseases or of different sources of the same disease can prove to be having serious cross-dataset feature misalignment problems, greatly mutilating the efficiency of deep learning models. This article introduces a transfer learning method to build a transition model from source dataset to target dataset. By way of constraining the 
&lt;/p&gt;</description></item><item><title>CRITERIA&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.07794</link><description>&lt;p&gt;
CRITERIA&#65306;&#19968;&#31181;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving. (arXiv:2310.07794v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07794
&lt;/p&gt;
&lt;p&gt;
CRITERIA&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#26159;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#20110;&#36739;&#24120;&#35265;&#30340;&#24773;&#20917;&#65288;&#22914;&#24033;&#33322;&#65289;&#23384;&#22312;&#20559;&#24046;&#65292;&#24182;&#36890;&#36807;&#23545;&#25152;&#26377;&#24773;&#20917;&#36827;&#34892;&#24179;&#22343;&#35745;&#31639;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#23569;&#33021;&#25552;&#20379;&#26377;&#20851;&#27169;&#22411;&#24615;&#33021;&#30340;&#27934;&#23519;&#65292;&#26080;&#35770;&#26159;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#23427;&#20204;&#33021;&#21542;&#33391;&#22909;&#22788;&#29702;&#65292;&#36824;&#26159;&#23427;&#20204;&#30340;&#36755;&#20986;&#26159;&#21542;&#20801;&#35768;&#21644;&#22810;&#26679;&#21270;&#12290;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#29992;&#20110;&#34913;&#37327;&#36712;&#36857;&#21487;&#20801;&#35768;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#34917;&#20805;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#22914;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65288;CRITERIA&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36947;&#36335;&#32467;&#26500;&#12289;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#29305;&#24615;&#25552;&#21462;&#39550;&#39542;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking is a common method for evaluating trajectory prediction models for autonomous driving. Existing benchmarks rely on datasets, which are biased towards more common scenarios, such as cruising, and distance-based metrics that are computed by averaging over all scenarios. Following such a regiment provides a little insight into the properties of the models both in terms of how well they can handle different scenarios and how admissible and diverse their outputs are. There exist a number of complementary metrics designed to measure the admissibility and diversity of trajectories, however, they suffer from biases, such as length of trajectories.  In this paper, we propose a new benChmarking paRadIgm for evaluaTing trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a method for extracting driving scenarios at varying levels of specificity according to the structure of the roads, models' performance, and data properties for fine-grained ranking of prediction 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Spark&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#33322;&#29677;&#31080;&#20215;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#20851;&#38190;&#30340;&#21830;&#19994;&#27934;&#23519;&#65292;&#24182;&#35752;&#35770;&#20102;&#36807;&#31243;&#21644;&#24037;&#20855;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07787</link><description>&lt;p&gt;
&#20351;&#29992;Spark&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#33322;&#29677;&#31080;&#20215;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Using Spark Machine Learning Models to Perform Predictive Analysis on Flight Ticket Pricing Data. (arXiv:2310.07787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Spark&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#33322;&#29677;&#31080;&#20215;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#19968;&#20123;&#20851;&#38190;&#30340;&#21830;&#19994;&#27934;&#23519;&#65292;&#24182;&#35752;&#35770;&#20102;&#36807;&#31243;&#21644;&#24037;&#20855;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#33322;&#29677;&#31080;&#20215;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#27979;&#24615;&#33021;&#21644;&#27969;&#31243;&#30340;&#38382;&#39064;&#65292;&#21033;&#29992;r2(r-square)&#21644;RMSE&#26469;&#21033;&#29992;&#21407;&#22987;&#33258;Expedia.com&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25968;&#25454;&#38598;&#21253;&#21547;&#22823;&#32422;2000&#19975;&#26465;&#35760;&#24405;&#25110;4.68GB&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#30830;&#23450;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#29992;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#32654;&#22269;&#22659;&#20869;&#30340;&#38750;&#20572;&#33322;&#29677;&#26426;&#31080;&#20215;&#26684;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#21644;&#20248;&#21270;&#22788;&#29702;&#26102;&#38388;&#26159;&#37325;&#35201;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#23558;&#21033;&#29992;&#29305;&#24449;&#37325;&#35201;&#24615;&#21457;&#29616;&#20851;&#38190;&#30340;&#21830;&#19994;&#27934;&#23519;&#65292;&#24182;&#35752;&#35770;&#25105;&#20204;&#20998;&#26512;&#25152;&#20351;&#29992;&#30340;&#36807;&#31243;&#21644;&#24037;&#20855;&#12290;&#20351;&#29992;&#20102;&#22235;&#20010;&#22238;&#24402;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65306;&#38543;&#26426;&#26862;&#26519;&#12289;&#26799;&#24230;&#25552;&#21319;&#26641;&#12289;&#20915;&#31574;&#26641;&#21644;&#20998;&#35299;&#26426;&#65292;&#21033;&#29992;&#20132;&#21449;&#39564;&#35777;&#22120;&#21644;&#35757;&#32451;&#39564;&#35777;&#22120;&#20989;&#25968;&#26469;&#35780;&#20272;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses predictive performance and processes undertaken on flight pricing data utilizing r2(r-square) and RMSE that leverages a large dataset, originally from Expedia.com, consisting of approximately 20 million records or 4.68 gigabytes. The project aims to determine the best models usable in the real world to predict airline ticket fares for non-stop flights across the US. Therefore, good generalization capability and optimized processing times are important measures for the model.  We will discover key business insights utilizing feature importance and discuss the process and tools used for our analysis. Four regression machine learning algorithms were utilized: Random Forest, Gradient Boost Tree, Decision Tree, and Factorization Machines utilizing Cross Validator and Training Validator functions for assessing performance and generalization capability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#20248;&#20808;&#25910;&#38598;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07786</link><description>&lt;p&gt;
&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#22522;&#20110;&#31070;&#32463;&#39044;&#27979;&#38598;&#25104;&#25277;&#26679;&#30340;&#24773;&#22659;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Contextual Bandit Learning via Neural Predictive Ensemble Sampling. (arXiv:2310.07786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#20248;&#20808;&#25910;&#38598;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#24773;&#22659;&#36172;&#21338;&#24212;&#29992;&#24120;&#24120;&#22240;&#23395;&#33410;&#24615;&#12289;&#20598;&#28982;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#31038;&#20132;&#36235;&#21183;&#32780;&#21576;&#38750;&#31283;&#24577;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#25552;&#20986;&#20102;&#35768;&#22810;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#23545;&#25345;&#20037;&#20215;&#20540;&#20449;&#24687;&#30340;&#20248;&#20808;&#32771;&#34385;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#25506;&#32034;&#26102;&#36807;&#24230;&#65292;&#25110;&#32773;&#35774;&#35745;&#26041;&#24335;&#38590;&#20197;&#22312;&#20855;&#26377;&#39640;&#32500;&#29992;&#25143;&#29305;&#23450;&#29305;&#24449;&#21644;&#22823;&#35268;&#27169;&#21160;&#20316;&#38598;&#30340;&#29616;&#20195;&#24212;&#29992;&#20013;&#25193;&#23637;&#65292;&#25110;&#32773;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#31283;&#24577;&#24773;&#22659;&#36172;&#21338;&#31639;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#23427;&#23558;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#19982;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25506;&#32034;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#31283;&#24577;&#29615;&#22659;&#20013;&#25112;&#30053;&#24615;&#22320;&#20248;&#20808;&#25910;&#38598;&#20855;&#26377;&#26368;&#25345;&#20037;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#23637;&#31034;&#26126;&#26174;&#38750;&#31283;&#24577;&#30340;&#20004;&#20010;&#23454;&#38469;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#32988;&#36807;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world applications of contextual bandits often exhibit non-stationarity due to seasonality, serendipity, and evolving social trends. While a number of non-stationary contextual bandit learning algorithms have been proposed in the literature, they excessively explore due to a lack of prioritization for information of enduring value, or are designed in ways that do not scale in modern applications with high-dimensional user-specific features and large action set, or both. In this paper, we introduce a novel non-stationary contextual bandit algorithm that addresses these concerns. It combines a scalable, deep-neural-network-based architecture with a carefully designed exploration mechanism that strategically prioritizes collecting information with the most lasting value in a non-stationary environment. Through empirical evaluations on two real-world recommendation datasets, which exhibit pronounced non-stationarity, we demonstrate that our approach significantly outperforms the state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38543;&#26426;&#24179;&#28369;&#30340;&#40065;&#26834;&#24615;&#65292;&#19968;&#31181;&#26159;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#21644;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#26032;&#30340;&#40065;&#26834;&#35757;&#32451;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20445;&#25345;&#33391;&#22909;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#25552;&#39640;&#38543;&#26426;&#24179;&#28369;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07780</link><description>&lt;p&gt;
&#25552;&#21319;&#38543;&#26426;&#24179;&#28369;&#30340;&#40065;&#26834;&#24615;&#65306;&#20004;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Promoting Robustness of Randomized Smoothing: Two Cost-Effective Approaches. (arXiv:2310.07780v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38543;&#26426;&#24179;&#28369;&#30340;&#40065;&#26834;&#24615;&#65292;&#19968;&#31181;&#26159;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#21644;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#26032;&#30340;&#40065;&#26834;&#35757;&#32451;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20445;&#25345;&#33391;&#22909;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#25552;&#39640;&#38543;&#26426;&#24179;&#28369;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#26368;&#36817;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#39046;&#22495;&#21560;&#24341;&#20102;&#27880;&#24847;&#65292;&#23427;&#21487;&#20197;&#20026;&#24179;&#28369;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26222;&#36890;&#30340;&#38543;&#26426;&#24179;&#28369;&#36890;&#24120;&#19981;&#33021;&#25552;&#20379;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#24120;&#38656;&#35201;&#22312;&#22522;&#26412;&#20998;&#31867;&#22120;&#19978;&#36827;&#34892;&#65288;&#37325;&#26032;&#65289;&#35757;&#32451;&#25216;&#26415;&#26469;&#25552;&#39640;&#24179;&#28369;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38543;&#26426;&#24179;&#28369;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#40065;&#26834;&#35757;&#32451;&#26041;&#27861;AdvMacer&#65292;&#32467;&#21512;&#23545;&#25239;&#35757;&#32451;&#21644;&#26368;&#22823;&#21270;&#38543;&#26426;&#24179;&#28369;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AdvMacer&#30456;&#23545;&#20110;SOTA&#22522;&#20934;&#21487;&#20197;&#25552;&#39640;&#38543;&#26426;&#24179;&#28369;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#24615;&#33021;&#65292;&#21516;&#26102;&#35757;&#32451;&#36895;&#24230;&#27604;MACER&#22522;&#20934;&#24555;3&#20493;&#12290;&#31532;&#20108;&#20010;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;EsbRS&#65292;&#23427;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#38543;&#26426;&#24179;&#28369;&#30340;&#40065;&#26834;&#24615;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing has recently attracted attentions in the field of adversarial robustness to provide provable robustness guarantees on smoothed neural network classifiers. However, existing works show that vanilla randomized smoothing usually does not provide good robustness performance and often requires (re)training techniques on the base classifier in order to boost the robustness of the resulting smoothed classifier. In this work, we propose two cost-effective approaches to boost the robustness of randomized smoothing while preserving its clean performance. The first approach introduces a new robust training method AdvMacerwhich combines adversarial training and robustness certification maximization for randomized smoothing. We show that AdvMacer can improve the robustness performance of randomized smoothing classifiers compared to SOTA baselines, while being 3x faster to train than MACER baseline. The second approach introduces a post-processing method EsbRS which greatly impr
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27491;&#20132;&#30697;&#38453;&#38598;&#21512;&#21021;&#22987;&#21270;&#26435;&#37325;&#24182;&#20351;&#29992;tanh&#28608;&#27963;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#20013;&#20855;&#26377;&#19982;&#28145;&#24230;&#26080;&#20851;&#30340;&#32447;&#24615;&#27874;&#21160;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21450;&#20854;&#21518;&#20195;&#30340;&#25152;&#26377;&#30456;&#20851;&#20989;&#25968;&#22312;&#36870;&#23485;&#24230;&#30340;&#20027;&#23548;&#38454;&#27573;&#22312;&#28145;&#24230;&#32422;&#20026;20&#30340;&#20301;&#32622;&#39281;&#21644;&#65292;&#32780;&#19981;&#26159;&#19981;&#26029;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2310.07765</link><description>&lt;p&gt;
&#20855;&#26377;&#27491;&#20132;&#26435;&#37325;&#30340;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#19982;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Feature Learning and Generalization in Deep Networks with Orthogonal Weights. (arXiv:2310.07765v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07765
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27491;&#20132;&#30697;&#38453;&#38598;&#21512;&#21021;&#22987;&#21270;&#26435;&#37325;&#24182;&#20351;&#29992;tanh&#28608;&#27963;&#20989;&#25968;&#65292;&#35299;&#20915;&#20102;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#20013;&#20855;&#26377;&#19982;&#28145;&#24230;&#26080;&#20851;&#30340;&#32447;&#24615;&#27874;&#21160;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21450;&#20854;&#21518;&#20195;&#30340;&#25152;&#26377;&#30456;&#20851;&#20989;&#25968;&#22312;&#36870;&#23485;&#24230;&#30340;&#20027;&#23548;&#38454;&#27573;&#22312;&#28145;&#24230;&#32422;&#20026;20&#30340;&#20301;&#32622;&#39281;&#21644;&#65292;&#32780;&#19981;&#26159;&#19981;&#26029;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20174;&#27491;&#20132;&#30697;&#38453;&#38598;&#21512;&#21021;&#22987;&#21270;&#30340;&#26435;&#37325;&#21644;tanh&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20840;&#36830;&#25509;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21021;&#22987;&#21270;&#26102;&#20855;&#26377;&#19982;&#23485;&#24230;&#26080;&#20851;&#30340;&#21069;&#28608;&#27963;&#27874;&#21160;&#65292;&#36825;&#26159;&#36890;&#36807;&#35745;&#31639;&#35777;&#26126;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#28041;&#21450;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21450;&#20854;&#21518;&#20195;&#30340;&#25152;&#26377;&#30456;&#20851;&#20989;&#25968;&#22312;&#36870;&#23485;&#24230;&#30340;&#20027;&#23548;&#38454;&#27573;&#39281;&#21644;&#22312;&#28145;&#24230;&#32422;&#20026;20&#30340;&#20301;&#32622;&#65292;&#32780;&#19981;&#26159;&#20687;&#39640;&#26031;&#21021;&#22987;&#21270;&#30340;&#24773;&#20917;&#37027;&#26679;&#19981;&#26029;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully-connected deep neural networks with weights initialized from independent Gaussian distributions can be tuned to criticality, which prevents the exponential growth or decay of signals propagating through the network. However, such networks still exhibit fluctuations that grow linearly with the depth of the network, which may impair the training of networks with width comparable to depth. We show analytically that rectangular networks with tanh activations and weights initialized from the ensemble of orthogonal matrices have corresponding preactivation fluctuations which are independent of depth, to leading order in inverse width. Moreover, we demonstrate numerically that, at initialization, all correlators involving the neural tangent kernel (NTK) and its descendants at leading order in inverse width -- which govern the evolution of observables during training -- saturate at a depth of $\sim 20$, rather than growing without bound as in the case of Gaussian initializations. We spec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;SSRL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#38543;&#26426;&#25968;&#25454;&#25237;&#24433;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#19981;&#20381;&#36182;&#20110;&#22686;&#24378;&#25110;&#25513;&#34109;&#25216;&#26415;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#27169;&#24577;&#21644;&#32593;&#32476;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;SSRL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07756</link><description>&lt;p&gt;
&#20174;&#38543;&#26426;&#25968;&#25454;&#25237;&#24433;&#22120;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Representation Learning From Random Data Projectors. (arXiv:2310.07756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;SSRL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#38543;&#26426;&#25968;&#25454;&#25237;&#24433;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#19981;&#20381;&#36182;&#20110;&#22686;&#24378;&#25110;&#25513;&#34109;&#25216;&#26415;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#27169;&#24577;&#21644;&#32593;&#32476;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;SSRL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#35774;&#35745;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#19979;&#30340;&#21464;&#25442;&#19981;&#21464;&#24615;&#20551;&#35774;&#65292;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;SSRL&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#34429;&#28982;&#22522;&#20110;&#22686;&#24378;&#30340;SSRL&#31639;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#25512;&#21160;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#36866;&#29992;&#20110;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#65292;&#24182;&#19988;&#21487;&#33021;&#19982;&#24212;&#29992;&#29305;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#32422;&#26463;&#20914;&#31361;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;SSRL&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#27169;&#24577;&#21644;&#32593;&#32476;&#26550;&#26500;&#65292;&#22240;&#20026;&#23427;&#19981;&#20381;&#36182;&#20110;&#22686;&#24378;&#25110;&#25513;&#34109;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#37325;&#24314;&#38543;&#26426;&#25968;&#25454;&#25237;&#24433;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#25105;&#20204;&#23545;&#36328;&#22810;&#31181;&#27169;&#24577;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;SSRL&#22522;&#32447;&#27169;&#22411;&#12290;&#30001;&#20110;&#20854;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#25105;&#20204;&#35748;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning~(SSRL) has advanced considerably by exploiting the transformation invariance assumption under artificially designed data augmentations. While augmentation-based SSRL algorithms push the boundaries of performance in computer vision and natural language processing, they are often not directly applicable to other data modalities, and can conflict with application-specific data augmentation constraints. This paper presents an SSRL approach that can be applied to any data modality and network architecture because it does not rely on augmentations or masking. Specifically, we show that high-quality data representations can be learned by reconstructing random data projections. We evaluate the proposed approach on a wide range of representation learning tasks that span diverse modalities and real-world applications. We show that it outperforms multiple state-of-the-art SSRL baselines. Due to its wide applicability and strong empirical results, we argue t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31163;&#32447;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07747</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38382;&#36131;&#21046;&#65306;&#29992;&#35821;&#26009;&#24211;&#30340;&#20363;&#23376;&#35299;&#37322;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples. (arXiv:2310.07747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31163;&#32447;&#25511;&#21046;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#22120;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#38477;&#20302;&#22312;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#22312;&#36131;&#20219;&#25935;&#24863;&#30340;&#35774;&#32622;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#65289;&#20013;&#65292;&#20915;&#31574;&#38382;&#36131;&#21046;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#23578;&#26410;&#20805;&#20998;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Accountable Offline Controller&#65288;AOC&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#31163;&#32447;&#25968;&#25454;&#38598;&#20316;&#20026;&#20915;&#31574;&#35821;&#26009;&#24211;&#65292;&#24182;&#26681;&#25454;&#19968;&#32452;&#23450;&#21046;&#30340;&#20363;&#23376;&#65288;&#31216;&#20026;&#35821;&#26009;&#24211;&#23376;&#38598;&#65289;&#36827;&#34892;&#38382;&#36131;&#21046;&#30340;&#25511;&#21046;&#12290;AOC&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#36816;&#34892;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#20005;&#26684;&#30340;&#31163;&#32447;&#27169;&#20223;&#35774;&#32622;&#65292;&#24182;&#34920;&#29616;&#20986;&#20445;&#25252;&#21644;&#36866;&#24212;&#24615;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#30340;&#21307;&#30103;&#20445;&#20581;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;AOC&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#20445;&#25345;&#38382;&#36131;&#21046;&#30340;&#21516;&#26102;&#33021;&#22815;&#31649;&#29702;&#39640;&#27700;&#24179;&#30340;&#31163;&#32447;&#25511;&#21046;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning transparent, interpretable controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature. This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. ABC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability. We assess ABC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability.  Keywords: Int
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#34987;&#24212;&#29992;&#20110;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#26377;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36824;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07745</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey. (arXiv:2310.07745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07745
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#34987;&#24212;&#29992;&#20110;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#26377;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36824;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#32593;&#32476;&#25915;&#20987;&#25968;&#37327;&#30340;&#24555;&#36895;&#22686;&#21152;&#24341;&#21457;&#20102;&#23545;&#38024;&#23545;&#24694;&#24847;&#34892;&#20026;&#30340;&#32593;&#32476;&#38450;&#24481;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;DRL&#22312;&#32593;&#32476;&#38450;&#24481;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#23558;DRL&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#65288;ACO&#65289;&#20043;&#21069;&#65292;&#36824;&#38656;&#35201;&#20811;&#26381;&#35768;&#22810;&#25361;&#25112;&#12290;&#38656;&#35201;&#20026;&#19982;&#23398;&#20064;&#32773;&#38754;&#23545;&#38750;&#24120;&#39640;&#32500;&#24230;&#29366;&#24577;&#31354;&#38388;&#12289;&#22823;&#35268;&#27169;&#22810;&#31163;&#25955;&#25805;&#20316;&#31354;&#38388;&#21644;&#23545;&#25239;&#23398;&#20064;&#30456;&#36935;&#30340;&#29615;&#22659;&#25552;&#20379;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#38024;&#23545;&#23454;&#26102;&#31574;&#30053;&#28216;&#25103;&#20063;&#36827;&#34892;&#20102;&#21360;&#35937;&#28145;&#21051;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;DRL&#24212;&#29992;&#20110;&#23436;&#25972;&#30340;ACO&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#30456;&#20851;&#30340;DRL&#25991;&#29486;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#26500;&#24819;&#20102;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;ACO-DRL&#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#65306;i.) &#39046;&#22495;&#29305;&#24615;&#30340;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
The rapid increase in the number of cyber-attacks in recent years raises the need for principled methods for defending networks against malicious actors. Deep reinforcement learning (DRL) has emerged as a promising approach for mitigating these attacks. However, while DRL has shown much potential for cyber-defence, numerous challenges must be overcome before DRL can be applied to autonomous cyber-operations (ACO) at scale. Principled methods are required for environments that confront learners with very high-dimensional state spaces, large multi-discrete action spaces, and adversarial learning. Recent works have reported success in solving these problems individually. There have also been impressive engineering efforts towards solving all three for real-time strategy games. However, applying DRL to the full ACO problem remains an open challenge. Here, we survey the relevant DRL literature and conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the domain properties t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#26143;&#31995;&#24418;&#24577;&#32479;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21270;&#29256;&#26412;&#30340;&#29616;&#26377;&#22270;&#20687;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#26395;&#36828;&#38236;&#22270;&#20687;&#20013;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#20998;&#31867;&#34746;&#26059;-&#26925;&#22278;&#26143;&#31995;&#24418;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.07740</link><description>&lt;p&gt;
&#20174;&#26395;&#36828;&#38236;&#22270;&#20687;&#20013;&#33258;&#21160;&#20998;&#31867;&#34746;&#26059;-&#26925;&#22278;&#26143;&#31995;&#24418;&#24577;
&lt;/p&gt;
&lt;p&gt;
Spiral-Elliptical automated galaxy morphology classification from telescope images. (arXiv:2310.07740v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#26143;&#31995;&#24418;&#24577;&#32479;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21270;&#29256;&#26412;&#30340;&#29616;&#26377;&#22270;&#20687;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#26395;&#36828;&#38236;&#22270;&#20687;&#20013;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#20998;&#31867;&#34746;&#26059;-&#26925;&#22278;&#26143;&#31995;&#24418;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26143;&#31995;&#24418;&#24577;&#20998;&#31867;&#26159;&#30740;&#31350;&#20998;&#32423;&#32467;&#26500;&#24418;&#25104;&#29702;&#35770;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#34429;&#28982;&#20154;&#24037;&#19987;&#23478;&#30340;&#35270;&#35273;&#20998;&#31867;&#20173;&#28982;&#26377;&#25928;&#20934;&#30830;&#65292;&#20294;&#26080;&#27861;&#24212;&#23545;&#26032;&#20852;&#22825;&#31354;&#35843;&#26597;&#30340;&#28023;&#37327;&#25968;&#25454;&#12290;&#25552;&#20986;&#20102;&#21508;&#31181;&#20998;&#31867;&#22823;&#37327;&#26143;&#31995;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20247;&#21253;&#35270;&#35273;&#20998;&#31867;&#12289;&#22522;&#20110;&#35774;&#35745;&#30340;&#24418;&#24577;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#26143;&#31995;&#24418;&#24577;&#32479;&#35745;&#65292;&#38477;&#33853;&#24179;&#22343;&#20540;&#21644;&#38477;&#33853;&#26041;&#24046;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26395;&#36828;&#38236;&#26143;&#31995;&#22270;&#20687;&#20013;&#25552;&#21462;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#29616;&#26377;&#22270;&#20687;&#32479;&#35745;&#38598;&#20013;&#24230;&#12289;&#19981;&#23545;&#31216;&#24615;&#21644;&#26434;&#20081;&#24615;&#30340;&#31616;&#21270;&#29256;&#26412;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#26143;&#31995;&#24418;&#24577;&#30340;&#25991;&#29486;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;SDSS&#30340;&#26143;&#31995;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classification of galaxy morphologies is an important step in the investigation of theories of hierarchical structure formation. While human expert visual classification remains quite effective and accurate, it cannot keep up with the massive influx of data from emerging sky surveys. A variety of approaches have been proposed to classify large numbers of galaxies; these approaches include crowdsourced visual classification, and automated and computational methods, such as machine learning methods based on designed morphology statistics and deep learning. In this work, we develop two novel galaxy morphology statistics, descent average and descent variance, which can be efficiently extracted from telescope galaxy images. We further propose simplified versions of the existing image statistics concentration, asymmetry, and clumpiness, which have been widely used in the literature of galaxy morphologies. We utilize the galaxy image data from the Sloan Digital Sky Survey to demonstrate t
&lt;/p&gt;</description></item><item><title>Observatory&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#26469;&#20998;&#26512;&#20851;&#31995;&#34920;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07736</link><description>&lt;p&gt;
Observatory: &#21051;&#30011;&#20851;&#31995;&#34920;&#23884;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Observatory: Characterizing Embeddings of Relational Tables. (arXiv:2310.07736v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07736
&lt;/p&gt;
&lt;p&gt;
Observatory&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#24335;&#26694;&#26550;&#26469;&#20998;&#26512;&#20851;&#31995;&#34920;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#36873;&#25321;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#21644;&#19987;&#38376;&#30340;&#34920;&#23884;&#20837;&#27169;&#22411;&#22312;&#35768;&#22810;&#34920;&#26684;&#25968;&#25454;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#37117;&#28212;&#26395;&#22312;&#35768;&#22810;&#26032;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65307;&#20294;&#26159;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#20197;&#21450;&#23427;&#20204;&#29983;&#25104;&#30340;&#34920;&#26684;&#34920;&#31034;&#30340;&#29702;&#35299;&#26377;&#38480;&#65292;&#23548;&#33268;&#22312;&#23547;&#25214;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;&#20381;&#36182;&#20110;&#35797;&#38169;&#12290;&#36843;&#20999;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#19979;&#28216;&#20351;&#29992;&#20013;&#30340;&#20302;&#25928;&#29575;&#21644;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Observatory&#30340;&#27491;&#24335;&#26694;&#26550;&#65292;&#20197;&#31995;&#32479;&#22320;&#20998;&#26512;&#20851;&#31995;&#34920;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#20851;&#31995;&#25968;&#25454;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#21644;&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#32479;&#35745;&#32771;&#34385;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20843;&#20010;&#21407;&#22987;&#23646;&#24615;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#24230;&#37327;&#26469;&#23450;&#37327;&#22320;&#21051;&#30011;&#36825;&#20123;&#23646;&#24615;&#30340;&#34920;&#26684;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models and specialized table embedding models have recently demonstrated strong performance on many tasks over tabular data. Researchers and practitioners are keen to leverage these models in many new application contexts; but limited understanding of the strengths and weaknesses of these models, and the table representations they generate, makes the process of finding a suitable model for a given task reliant on trial and error. There is an urgent need to gain a comprehensive understanding of these models to minimize inefficiency and failures in downstream usage.  To address this need, we propose Observatory, a formal framework to systematically analyze embedding representations of relational tables. Motivated both by invariants of the relational data model and by statistical considerations regarding data distributions, we define eight primitive properties, and corresponding measures to quantitatively characterize table embeddings for these properties. Based on these properti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#26497;&#31471;&#22270;&#20687;&#36716;&#25442;&#65288;EIT&#65289;&#23545;&#39044;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#28508;&#22312;&#23545;&#35937;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#23545;&#21508;&#31181;&#24378;&#24230;&#30340;&#24120;&#35265;&#23545;&#25239;&#25915;&#20987;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07725</link><description>&lt;p&gt;
&#26497;&#31471;&#22270;&#20687;&#36716;&#25442;&#20419;&#36827;&#31283;&#20581;&#30340;&#28508;&#22312;&#23545;&#35937;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Extreme Image Transformations Facilitate Robust Latent Object Representations. (arXiv:2310.07725v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#20351;&#29992;&#26497;&#31471;&#22270;&#20687;&#36716;&#25442;&#65288;EIT&#65289;&#23545;&#39044;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#28508;&#22312;&#23545;&#35937;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#23545;&#21508;&#31181;&#24378;&#24230;&#30340;&#24120;&#35265;&#23545;&#25239;&#25915;&#20987;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#21487;&#33021;&#20250;&#24433;&#21709;&#26426;&#22120;&#22312;&#37326;&#22806;&#30340;&#29289;&#20307;&#35782;&#21035;&#33021;&#21147;&#12290;&#36825;&#20123;&#25915;&#20987;&#36890;&#24120;&#26159;&#30001;&#36755;&#20837;&#21644;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#24341;&#36215;&#30340;&#65292;&#24182;&#19988;&#23481;&#26131;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#21457;&#29983;&#35760;&#24518;&#12290;&#34429;&#28982;&#32593;&#32476;&#34987;&#26399;&#26395;&#36827;&#34892;&#33258;&#21160;&#21270;&#29305;&#24449;&#36873;&#25321;&#65292;&#20294;&#22312;&#23545;&#35937;&#30340;&#35268;&#27169;&#19978;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#33021;&#22815;&#36873;&#25321;&#24418;&#25104;&#29289;&#20307;&#31283;&#20581;&#34920;&#31034;&#25152;&#38656;&#30340;&#26368;&#23567;&#29305;&#24449;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26497;&#31471;&#22270;&#20687;&#36716;&#25442;&#65288;EIT&#65289;&#23545;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#29616;&#25104;&#32593;&#32476;&#36827;&#34892;&#24494;&#35843;&#19981;&#20165;&#26377;&#21161;&#20110;&#23398;&#20064;&#31283;&#20581;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#36824;&#25913;&#21892;&#20102;&#36825;&#20123;&#32593;&#32476;&#23545;&#19981;&#21516;&#24378;&#24230;&#30340;&#24120;&#35265;&#23545;&#25239;&#25915;&#20987;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;EIT&#35757;&#32451;&#32593;&#32476;&#22312;&#27979;&#35797;&#20013;&#21363;&#20351;&#21463;&#21040;&#26356;&#24378;&#28872;&#30340;&#22122;&#22768;&#24178;&#25200;&#26102;&#20063;&#34920;&#29616;&#20986;&#24378;&#28872;&#30340;&#28608;&#27963;&#65292;&#26174;&#31034;&#20986;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#20013;&#30340;&#26377;&#24076;&#26395;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks can affect the object recognition capabilities of machines in wild. These can often result from spurious correlations between input and class labels, and are prone to memorization in large networks. While networks are expected to do automated feature selection, it is not effective at the scale of the object. Humans, however, are able to select the minimum set of features required to form a robust representation of an object. In this work, we show that finetuning any pretrained off-the-shelf network with Extreme Image Transformations (EIT) not only helps in learning a robust latent representation, it also improves the performance of these networks against common adversarial attacks of various intensities. Our EIT trained networks show strong activations in the object regions even when tested with more intense noise, showing promising generalizations across different kinds of adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#35270;&#35273;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#30452;&#35266;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25237;&#23556;&#21160;&#24577;&#29289;&#20307;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#24182;&#23454;&#29616;&#39044;&#26399;&#30340;&#21160;&#20316;&#12290;&#30740;&#31350;&#32467;&#26524;&#39564;&#35777;&#20102;&#35270;&#35273;&#39044;&#27979;&#20316;&#20026;&#23548;&#33322;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07724</link><description>&lt;p&gt;
&#35270;&#35273;&#39044;&#27979;&#20316;&#20026;&#36991;&#38556;&#30340;&#20013;&#23618;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Visual Forecasting as a Mid-level Representation for Avoidance. (arXiv:2310.07724v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#35270;&#35273;&#39044;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#30452;&#35266;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#25237;&#23556;&#21160;&#24577;&#29289;&#20307;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#24182;&#23454;&#29616;&#39044;&#26399;&#30340;&#21160;&#20316;&#12290;&#30740;&#31350;&#32467;&#26524;&#39564;&#35777;&#20102;&#35270;&#35273;&#39044;&#27979;&#20316;&#20026;&#23548;&#33322;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#26234;&#33021;&#20307;&#30740;&#31350;&#20013;&#65292;&#22312;&#20855;&#26377;&#21160;&#24577;&#29289;&#20307;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#30340;&#25361;&#25112;&#20173;&#28982;&#26159;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#34429;&#28982;&#39044;&#27979;&#26041;&#27861;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#23545;&#31934;&#30830;&#29366;&#24577;&#20449;&#24687;&#30340;&#20381;&#36182;&#20351;&#20854;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#19981;&#22826;&#23454;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35270;&#35273;&#39044;&#27979;&#20316;&#20026;&#19968;&#31181;&#21019;&#26032;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#30452;&#35266;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25237;&#23556;&#21160;&#24577;&#29289;&#20307;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#24182;&#23454;&#29616;&#39044;&#26399;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#36890;&#36807;&#35270;&#35273;&#39044;&#27979;&#20256;&#36798;&#39044;&#27979;&#20449;&#24687;&#65306;&#65288;1&#65289;&#36793;&#30028;&#26694;&#24207;&#21015;&#21644;&#65288;2&#65289;&#22686;&#24378;&#36335;&#24452;&#12290;&#20026;&#20102;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#35270;&#35273;&#39044;&#27979;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;Unity&#24341;&#25806;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23558;&#36825;&#20123;&#35780;&#20272;&#25193;&#23637;&#21040;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20197;&#35780;&#20272;&#20854;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#35777;&#23454;&#20102;&#35270;&#35273;&#39044;&#27979;&#20316;&#20026;&#23548;&#33322;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of navigation in environments with dynamic objects continues to be a central issue in the study of autonomous agents. While predictive methods hold promise, their reliance on precise state information makes them less practical for real-world implementation. This study presents visual forecasting as an innovative alternative. By introducing intuitive visual cues, this approach projects the future trajectories of dynamic objects to improve agent perception and enable anticipatory actions. Our research explores two distinct strategies for conveying predictive information through visual forecasting: (1) sequences of bounding boxes, and (2) augmented paths. To validate the proposed visual forecasting strategies, we initiate evaluations in simulated environments using the Unity engine and then extend these evaluations to real-world scenarios to assess both practicality and effectiveness. The results confirm the viability of visual forecasting as a promising solution for navigat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#20989;&#25968;Parametric Leaky Tanh (PLTanh)&#65292;&#32467;&#21512;&#20102;Tanh&#21644;Leaky ReLU (LReLU)&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;'dying ReLU'&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#22312;&#32593;&#32476;&#20869;&#37096;&#23398;&#20064;&#26356;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07720</link><description>&lt;p&gt;
Parametric Leaky Tanh&#65306;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#28151;&#21512;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Parametric Leaky Tanh: A New Hybrid Activation Function for Deep Learning. (arXiv:2310.07720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#20989;&#25968;Parametric Leaky Tanh (PLTanh)&#65292;&#32467;&#21512;&#20102;Tanh&#21644;Leaky ReLU (LReLU)&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;'dying ReLU'&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#22312;&#32593;&#32476;&#20869;&#37096;&#23398;&#20064;&#26356;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Parametric Leaky Tanh (PLTanh)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#28608;&#27963;&#20989;&#25968;&#65292;&#26088;&#22312;&#32467;&#21512;Tanh&#21644;Leaky ReLU (LReLU)&#28608;&#27963;&#20989;&#25968;&#30340;&#20248;&#28857;&#12290;PLTanh&#22312;&#25152;&#26377;&#28857;&#19978;&#21487;&#24494;&#20998;&#65292;&#24182;&#36890;&#36807;&#20445;&#35777;&#36127;&#36755;&#20837;&#30340;&#38750;&#38646;&#26799;&#24230;&#65292;&#35299;&#20915;&#20102;'dying ReLU'&#38382;&#39064;&#65292;&#19982;LReLU&#30340;&#34892;&#20026;&#19968;&#33268;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#30340;&#29420;&#29305;&#20248;&#21183;&#65292;PLTanh&#26377;&#21161;&#20110;&#22312;&#32593;&#32476;&#20869;&#37096;&#23398;&#20064;&#26356;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27604;&#20102;PLTanh&#19982;&#24120;&#35265;&#30340;&#28608;&#27963;&#20989;&#25968;ReLU&#12289;LReLU&#21644;ALReLU&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions (AFs) are crucial components of deep neural networks (DNNs), having a significant impact on their performance. An activation function in a DNN is typically a smooth, nonlinear function that transforms an input signal into an output signal for the subsequent layer. In this paper, we propose the Parametric Leaky Tanh (PLTanh), a novel hybrid activation function designed to combine the strengths of both the Tanh and Leaky ReLU (LReLU) activation functions. PLTanh is differentiable at all points and addresses the 'dying ReLU' problem by ensuring a non-zero gradient for negative inputs, consistent with the behavior of LReLU. By integrating the unique advantages of these two diverse activation functions, PLTanh facilitates the learning of more intricate nonlinear relationships within the network. This paper presents an empirical evaluation of PLTanh against established activation functions, namely ReLU, LReLU, and ALReLU utilizing five diverse datasets.
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2310.07644</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07644
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;DNA&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22240;&#20854;&#25429;&#25417;&#22522;&#22240;&#30340;&#36890;&#29992;&#20449;&#24687;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DNA&#24207;&#21015;&#39044;&#35757;&#32451;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30452;&#25509;&#24341;&#20837;&#30340;BERT&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#19987;&#38376;&#23450;&#21046;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#24182;&#33719;&#24471;&#20102;&#20960;&#20010;&#26377;&#21551;&#21457;&#24615;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#32780;&#19981;&#26159;K-mer&#38750;&#37325;&#21472;&#26631;&#35760;&#21270;&#26102;&#65292;&#37325;&#21472;&#21644;&#38750;&#37325;&#21472;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#22343;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;2&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#20250;&#36805;&#36895;&#20135;&#29983;&#28165;&#26224;&#30340;K-mer&#23884;&#20837;&#65292;&#24182;&#23558;&#25439;&#22833;&#38477;&#20302;&#21040;&#38750;&#24120;&#20302;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, w
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07579</link><description>&lt;p&gt;
In-Context Unlearning: &#22522;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#28040;&#38500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#26159;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#21435;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36817;&#26469;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38656;&#35201;&#36981;&#23432;&#35832;&#22914;&#34987;&#36951;&#24536;&#26435;&#31561;&#38544;&#31169;&#27861;&#35268;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#22312;&#29256;&#26435;&#38382;&#39064;&#19978;LLM&#65288;&#35821;&#35328;&#27169;&#22411;&#65289;&#23588;&#20854;&#30456;&#20851;&#65292;&#20294;&#22312;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#19978;&#23454;&#29616;&#31934;&#30830;&#28040;&#38500;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36817;&#20284;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#31639;&#27861;&#20851;&#38190;&#20381;&#36182;&#20110;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#35775;&#38382;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#25110;&#36890;&#36807;API&#35775;&#38382;LLM&#32780;&#26080;&#27861;&#28385;&#36275;&#36825;&#31181;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#28040;&#38500;&#29305;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;i
&lt;/p&gt;
&lt;p&gt;
Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07433</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#25240;&#25187;&#35843;&#24230;&#20174;&#35266;&#23519;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07433
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;(ILfO)&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#38169;&#35823;&#23548;&#33268;&#20195;&#29702;&#26080;&#27861;&#23398;&#20064;&#21021;&#22987;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#35266;&#23519;&#21644;&#27169;&#20223;&#26469;&#33719;&#24471;&#26032;&#30340;&#25216;&#33021;&#12290;&#23545;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#30340;&#22823;&#37327;&#26080;&#26631;&#31614;&#35270;&#39057;&#28436;&#31034;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#38656;&#35201;&#22312;&#27809;&#26377;&#35775;&#38382;&#20854;&#21160;&#20316;&#30340;&#24773;&#20917;&#19979;&#27169;&#20223;&#19987;&#23478;&#65292;&#36825;&#26159;&#19968;&#31181;&#31216;&#20026;&#35266;&#23519;&#23398;&#20064;&#27169;&#20223;&#65288;ILfO&#65289;&#30340;&#25361;&#25112;&#12290;&#35299;&#20915;ILfO&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#20854;&#36716;&#21270;&#20026;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21033;&#29992;&#20174;&#20195;&#29702;&#21644;&#19987;&#23478;&#35266;&#23519;&#20013;&#35745;&#31639;&#20986;&#30340;&#20195;&#29702;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20855;&#26377;&#36827;&#23637;&#20381;&#36182;&#24615;&#23646;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65307;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#38656;&#35201;&#22312;&#25484;&#25569;&#21518;&#32493;&#34892;&#20026;&#20043;&#21069;&#20808;&#23398;&#20064;&#19987;&#23478;&#30340;&#21069;&#24207;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#20998;&#37197;&#32473;&#21518;&#32493;&#27493;&#39588;&#30340;&#22870;&#21169;&#20449;&#21495;&#22952;&#30861;&#20102;&#23545;&#21021;&#22987;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ILfO&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#25484;&#25569;&#26089;&#26399;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we present a novel ILfO framework that enables the agent to master earl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Quantum Gramian Angular Field (QGAF)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#23558;&#32929;&#31080;&#22238;&#25253;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#30340;&#20108;&#32500;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;QGAF&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.07427</link><description>&lt;p&gt;
&#22686;&#24378;&#37327;&#23376;&#39044;&#27979;&#33021;&#21147;&#65306;&#21033;&#29992;&#37327;&#23376;Gramian&#35282;&#24230;&#22330;&#21644;CNN&#36827;&#34892;&#32929;&#31080;&#22238;&#25253;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions. (arXiv:2310.07427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Quantum Gramian Angular Field (QGAF)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25104;&#21151;&#23558;&#32929;&#31080;&#22238;&#25253;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#30340;&#20108;&#32500;&#22270;&#20687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;QGAF&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Quantum Gramian Angular Field (QGAF)&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;&#36890;&#36807;&#35774;&#35745;&#29305;&#23450;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#32929;&#31080;&#22238;&#25253;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36716;&#21270;&#20026;&#36866;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35757;&#32451;&#30340;&#20108;&#32500;&#22270;&#20687;&#12290;&#19982;&#32463;&#20856;&#30340;Gramian Angular Field (GAF)&#26041;&#27861;&#19981;&#21516;&#65292;QGAF&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#28040;&#38500;&#20102;&#25968;&#25454;&#24402;&#19968;&#21270;&#21644;&#21453;&#20313;&#24358;&#35745;&#31639;&#30340;&#38656;&#27714;&#65292;&#31616;&#21270;&#20102;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21040;&#20108;&#32500;&#22270;&#20687;&#30340;&#36716;&#25442;&#36807;&#31243;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20013;&#22269;A&#32929;&#24066;&#22330;&#12289;&#39321;&#28207;&#32929;&#24066;&#21644;&#32654;&#22269;&#32929;&#24066;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#32463;&#20856;&#30340;GAF&#26041;&#27861;&#30456;&#27604;&#65292;QGAF&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a time series forecasting method named Quantum Gramian Angular Field (QGAF). This approach merges the advantages of quantum computing technology with deep learning, aiming to enhance the precision of time series classification and forecasting. We successfully transformed stock return time series data into two-dimensional images suitable for Convolutional Neural Network (CNN) training by designing specific quantum circuits. Distinct from the classical Gramian Angular Field (GAF) approach, QGAF's uniqueness lies in eliminating the need for data normalization and inverse cosine calculations, simplifying the transformation process from time series data to two-dimensional images. To validate the effectiveness of this method, we conducted experiments on datasets from three major stock markets: the China A-share market, the Hong Kong stock market, and the US stock market. Experimental results revealed that compared to the classical GAF method, the QGAF approach significantly improv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.07402</link><description>&lt;p&gt;
NuTime: &#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#30740;&#31350;&#26174;&#31034;&#20986;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#38480;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#25968;&#21315;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25968;&#20540;&#29305;&#24615;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#30334;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#37319;&#29992;Transformer&#26550;&#26500;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#21010;&#20998;&#20026;&#38750;&#37325;&#21472;&#31383;&#21475;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31383;&#21475;&#30340;&#26631;&#20934;&#21270;&#24418;&#29366;&#21644;&#20004;&#20010;&#26631;&#37327;&#20540;&#34920;&#31034;&#27599;&#20010;&#31383;&#21475;&#20869;&#30340;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#12290;&#20026;&#20102;&#23558;&#21487;&#33021;&#20855;&#26377;&#20219;&#24847;&#25968;&#20540;&#23610;&#24230;&#30340;&#26631;&#37327;&#20540;&#23884;&#20837;&#21040;&#39640;&#32500;&#21521;&#37327;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#26522;&#20030;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#37327;&#20540;&#23610;&#24230;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset
&lt;/p&gt;</description></item><item><title>&#22312;&#22270;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;GraphControl&#36890;&#36807;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;&#23454;&#29616;&#20102;&#23545;&#36890;&#29992;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#36801;&#31227;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#22270;&#22495;&#38388;&#30340;&#23646;&#24615;&#35821;&#20041;&#24046;&#24322;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07365</link><description>&lt;p&gt;
GraphControl:&#20026;&#22270;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
GraphControl: Adding Conditional Control to Universal Graph Pre-trained Models for Graph Domain Transfer Learning. (arXiv:2310.07365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07365
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;GraphControl&#36890;&#36807;&#28155;&#21152;&#26465;&#20214;&#25511;&#21046;&#23454;&#29616;&#20102;&#23545;&#36890;&#29992;&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#36801;&#31227;&#65292;&#20811;&#26381;&#20102;&#19981;&#21516;&#22270;&#22495;&#38388;&#30340;&#23646;&#24615;&#35821;&#20041;&#24046;&#24322;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#19990;&#30028;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#36825;&#31181;&#25968;&#25454;&#27169;&#22411;&#20102;&#23545;&#35937;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20026;&#21508;&#31181;Web&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;Web&#19978;&#27599;&#22825;&#28044;&#29616;&#30340;&#26080;&#26631;&#31614;&#22270;&#25968;&#25454;&#20026;&#36825;&#20123;&#24212;&#29992;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22270;&#33258;&#30417;&#30563;&#31639;&#27861;&#22312;&#20174;&#20016;&#23500;&#30340;&#26080;&#26631;&#31614;&#22270;&#25968;&#25454;&#20013;&#33719;&#24471;&#36890;&#29992;&#30693;&#35782;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;Web&#24212;&#29992;&#65292;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#65292;&#25552;&#39640;&#19979;&#28216;&#65288;&#30446;&#26631;&#65289;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#34920;&#38754;&#19978;&#30475;&#36215;&#26469;&#30456;&#20284;&#30340;&#39046;&#22495;&#20013;&#65292;&#19981;&#21516;&#30340;&#22270;&#22312;&#23646;&#24615;&#35821;&#20041;&#26041;&#38754;&#20063;&#21487;&#33021;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#36825;&#32473;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36801;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#22256;&#38590;&#65292;&#29978;&#33267;&#26159;&#19981;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20363;&#22914;&#65292;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#38468;&#21152;&#29305;&#23450;&#20219;&#21153;&#33410;&#28857;&#20449;&#24687;&#65288;&#29305;&#24322;&#24615;&#65289;&#36890;&#24120;&#20250;&#34987;&#26377;&#24847;&#30465;&#30053;&#65292;&#20197;&#20415;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#65288;&#21487;&#36801;&#31227;&#24615;&#65289;&#12290;&#36825;&#31181;&#26435;&#34913;&#34987;&#31216;&#20026;
&lt;/p&gt;
&lt;p&gt;
Graph-structured data is ubiquitous in the world which models complex relationships between objects, enabling various Web applications. Daily influxes of unlabeled graph data on the Web offer immense potential for these applications. Graph self-supervised algorithms have achieved significant success in acquiring generic knowledge from abundant unlabeled graph data. These pre-trained models can be applied to various downstream Web applications, saving training time and improving downstream (target) performance. However, different graphs, even across seemingly similar domains, can differ significantly in terms of attribute semantics, posing difficulties, if not infeasibility, for transferring the pre-trained models to downstream tasks. Concretely speaking, for example, the additional task-specific node information in downstream tasks (specificity) is usually deliberately omitted so that the pre-trained representation (transferability) can be leveraged. The trade-off as such is termed as 
&lt;/p&gt;</description></item><item><title>WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07312</link><description>&lt;p&gt;
WiGenAI: &#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26080;&#32447;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#32455;
&lt;/p&gt;
&lt;p&gt;
WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07312
&lt;/p&gt;
&lt;p&gt;
WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-3&#21644;&#31283;&#23450;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#23454;&#29616;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#21521;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21457;&#23637;&#12290;&#20174;&#25968;&#25454;&#36890;&#20449;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#35745;&#23558;&#24191;&#27867;&#24212;&#29992;&#20110;&#26410;&#26469;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#26032;&#19968;&#20195;&#20013;&#65292;&#24378;&#35843;&#20102;&#22312;&#26032;&#20852;&#36890;&#20449;&#22330;&#26223;&#20013;&#38656;&#35201;&#26032;&#39062;&#30340;AI&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20171;&#32461;&#20102;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#20855;&#26377;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#34892;&#20026;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#24471;&#20998;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32791;&#26102;&#19988;&#35745;&#31639;&#23494;&#38598;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#22312;D4RL&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;25&#20493;&#30340;&#21160;&#20316;&#37319;&#26679;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.07297</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#34892;&#20026;&#23454;&#29616;&#24471;&#20998;&#27491;&#21017;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score Regularized Policy Optimization through Diffusion Behavior. (arXiv:2310.07297v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07297
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#34892;&#20026;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#24471;&#20998;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#32791;&#26102;&#19988;&#35745;&#31639;&#23494;&#38598;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#26696;&#65292;&#24182;&#22312;D4RL&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;25&#20493;&#30340;&#21160;&#20316;&#37319;&#26679;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#24314;&#27169;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#36825;&#20805;&#20998;&#23637;&#29616;&#20102;&#20854;&#22312;&#34920;&#36798;&#24322;&#36136;&#34892;&#20026;&#31574;&#30053;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25193;&#25955;&#31574;&#30053;&#20013;&#37319;&#26679;&#38750;&#24120;&#32531;&#24930;&#65292;&#22240;&#20026;&#38656;&#35201;&#25968;&#21313;&#21040;&#25968;&#30334;&#27425;&#36845;&#20195;&#25512;&#29702;&#27493;&#39588;&#26469;&#36827;&#34892;&#19968;&#27425;&#21160;&#20316;&#37319;&#26679;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35780;&#35770;&#23478;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#34892;&#20026;&#27169;&#22411;&#20013;&#25552;&#21462;&#39640;&#25928;&#30830;&#23450;&#24615;&#25512;&#29702;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21033;&#29992;&#21518;&#32773;&#30452;&#25509;&#23545;&#31574;&#30053;&#26799;&#24230;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20351;&#29992;&#34892;&#20026;&#20998;&#24067;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#36807;&#31243;&#20013;&#20805;&#20998;&#21457;&#25381;&#20102;&#25193;&#25955;&#24314;&#27169;&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#23436;&#20840;&#32469;&#36807;&#20102;&#35745;&#31639;&#23494;&#38598;&#21644;&#32791;&#26102;&#30340;&#25193;&#25955;&#37319;&#26679;&#26041;&#26696;&#12290;&#22312;D4RL&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21160;&#20316;&#37319;&#26679;&#36895;&#24230;&#25552;&#39640;&#20102;&#36229;&#36807;25&#20493;&#65292;&#30456;&#27604;&#20110;&#21508;&#31181;&#39046;&#20808;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#22312;&#36816;&#21160;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this issue, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion ta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#20998;&#24067;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#19981;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07171</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#24067;&#22810;&#26679;&#21270;&#23454;&#29616;&#32852;&#37030;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07171
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#20998;&#24067;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#19981;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#22312;&#26080;&#38656;&#30452;&#25509;&#25968;&#25454;&#20849;&#20139;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#21516;&#27169;&#22411;&#35757;&#32451;&#30340;&#33021;&#21147;&#32780;&#26085;&#30410;&#31361;&#20986;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24040;&#22823;&#24046;&#24322;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25361;&#25112;&#65292;&#23545;FL&#30340;&#27867;&#21270;&#33021;&#21147;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#24403;&#24182;&#38750;&#25152;&#26377;&#23458;&#25143;&#31471;&#37117;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#24773;&#20917;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#36825;&#26159;&#30001;&#20110;&#19981;&#31283;&#23450;&#30340;&#32593;&#32476;&#36830;&#25509;&#25110;&#26377;&#38480;&#30340;&#35745;&#31639;&#33021;&#21147;&#32780;&#24120;&#35265;&#12290;&#36825;&#21487;&#33021;&#26497;&#22823;&#22320;&#22797;&#26434;&#21270;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#38598;&#20013;&#22312;&#28041;&#21450;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#24046;&#36317;&#38382;&#39064;&#19978;&#65292;&#20294;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#38750;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#21364;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic genera
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#30340;&#26032;&#30340;&#25191;&#34892;&#26694;&#26550;&#65306;&#27946;&#27700;&#21644;&#22238;&#22768;&#32593;&#32476;&#12290;&#36890;&#36807;&#27874;&#29366;&#28608;&#27963;&#27169;&#24335;&#65292;&#23427;&#21487;&#20197;&#22312;&#25972;&#20010;&#22270;&#20013;&#20256;&#36882;&#28040;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26356;&#22823;&#30340;&#22270;&#24418;&#12290;</title><link>http://arxiv.org/abs/2310.06970</link><description>&lt;p&gt;
&#27946;&#27700;&#21644;&#22238;&#22768;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#31639;&#27861;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Flood and Echo: Algorithmic Alignment of GNNs with Distributed Computing. (arXiv:2310.06970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#24067;&#24335;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#30340;&#26032;&#30340;&#25191;&#34892;&#26694;&#26550;&#65306;&#27946;&#27700;&#21644;&#22238;&#22768;&#32593;&#32476;&#12290;&#36890;&#36807;&#27874;&#29366;&#28608;&#27963;&#27169;&#24335;&#65292;&#23427;&#21487;&#20197;&#22312;&#25972;&#20010;&#22270;&#20013;&#20256;&#36882;&#28040;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26356;&#22823;&#30340;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#23398;&#20064;&#31639;&#27861;&#30340;&#33258;&#28982;&#36873;&#25321;&#12290;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#25277;&#35937;&#32780;&#22810;&#21151;&#33021;&#30340;&#22270;&#32467;&#26500;&#30452;&#25509;&#34920;&#31034;&#20219;&#21153;&#65292;&#24182;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#36755;&#20837;&#12290;&#36825;&#20026;&#31639;&#27861;&#30340;&#25193;&#23637;&#21644;&#22806;&#25512;&#21040;&#26356;&#22823;&#30340;&#22270;&#24418;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#37325;&#35201;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#25552;&#20986;&#20102;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;i&#65289;&#22914;&#20309;&#20351;&#33410;&#28857;&#33021;&#22815;&#22312;&#32473;&#23450;&#30340;&#22270;&#20013;&#25910;&#38598;&#25152;&#38656;&#30340;&#20449;&#24687;&#65288;&#21363;&#8220;&#20449;&#24687;&#20132;&#25442;&#8221;&#65289;&#65292;&#21363;&#20351;&#33410;&#28857;&#36317;&#31163;&#24456;&#36828;&#65307;ii&#65289;&#25105;&#20204;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#25191;&#34892;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#35813;&#20449;&#24687;&#20132;&#25442;&#20197;&#20415;&#22806;&#25512;&#21040;&#26356;&#22823;&#30340;&#22270;&#22823;&#23567;&#65288;&#21363;&#8220;&#22806;&#25512;&#26102;&#30340;&#31639;&#27861;&#23545;&#40784;&#8221;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25191;&#34892;&#26694;&#26550;&#65292;&#21463;&#20998;&#24067;&#24335;&#31639;&#27861;&#30340;&#35774;&#35745;&#21407;&#21017;&#21551;&#21457;&#65306;&#27946;&#27700;&#21644;&#22238;&#22768;&#32593;&#32476;&#12290;&#23427;&#20197;&#27874;&#29366;&#28608;&#27963;&#27169;&#24335;&#23558;&#28040;&#24687;&#20256;&#25773;&#21040;&#25972;&#20010;&#22270;&#20013;&#65292;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#23454;&#20363;&#12290;&#36890;&#36807;&#23427;&#30340;&#31232;&#30095;&#20294;&#24182;&#34892;&#28608;&#27963;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are a natural fit for learning algorithms. They can directly represent tasks through an abstract but versatile graph structure and handle inputs of different sizes. This opens up the possibility for scaling and extrapolation to larger graphs, one of the most important advantages of an algorithm. However, this raises two core questions i) How can we enable nodes to gather the required information in a given graph ($\textit{information exchange}$), even if is far away and ii) How can we design an execution framework which enables this information exchange for extrapolation to larger graph sizes ($\textit{algorithmic alignment for extrapolation}$). We propose a new execution framework that is inspired by the design principles of distributed algorithms: Flood and Echo Net. It propagates messages through the entire graph in a wave like activation pattern, which naturally generalizes to larger instances. Through its sparse but parallel activations it is provably more ef
&lt;/p&gt;</description></item><item><title>NECO&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#26032;&#39062;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#23646;&#24615;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#35782;&#21035;OOD&#25968;&#25454;&#65292;&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06823</link><description>&lt;p&gt;
NECO: &#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06823
&lt;/p&gt;
&lt;p&gt;
NECO&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#26032;&#39062;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#23646;&#24615;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#35782;&#21035;OOD&#25968;&#25454;&#65292;&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#24182;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#20854;&#35748;&#35782;&#35770;&#38480;&#21046;&#65292;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#20551;&#35774;&#8220;&#31070;&#32463;&#22349;&#22604;&#8221;&#65292;&#19968;&#31181;&#24433;&#21709;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#29616;&#35937;&#65292;&#20063;&#20250;&#24433;&#21709;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#20013;&#21463;&#30410;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NECO&#65292;&#19968;&#31181;&#29992;&#20110;OOD&#26816;&#27979;&#30340;&#26032;&#39062;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#8220;&#31070;&#32463;&#22349;&#22604;&#8221;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#23646;&#24615;&#26469;&#35782;&#21035;OOD&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;NECO&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;OOD&#26816;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;OOD&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#35745;&#21010;&#22312;&#21311;&#21517;&#26399;&#32467;&#26463;&#21518;&#21457;&#24067;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.
&lt;/p&gt;</description></item><item><title>FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06763</link><description>&lt;p&gt;
FABind: &#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06763
&lt;/p&gt;
&lt;p&gt;
FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#23545;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#24182;&#20934;&#30830;&#39044;&#27979;&#20854;&#32467;&#21512;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#37319;&#26679;&#27861;&#21644;&#22238;&#24402;&#27861;&#25104;&#20026;&#20004;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;&#37319;&#26679;&#27861;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#32467;&#26500;&#26469;&#36827;&#34892;&#36873;&#25321;&#32780;&#25928;&#29575;&#36739;&#20302;&#12290;&#32780;&#22238;&#24402;&#27861;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#34507;&#30333;&#36136;&#22823;&#23567;&#30340;&#21464;&#21270;&#36890;&#24120;&#38656;&#35201;&#22806;&#37096;&#27169;&#22359;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#32467;&#21512;&#21475;&#34955;&#65292;&#36827;&#19968;&#27493;&#24433;&#21709;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind&#65292;&#19968;&#20010;&#23558;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30456;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06488</link><description>&lt;p&gt;
SpikeCLIP&#65306;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network. (arXiv:2310.06488v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#24050;&#32463;&#35777;&#26126;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#33021;&#22815;&#23454;&#29616;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#33021;&#25928;&#25552;&#39640;&#21644;&#31526;&#21512;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#21333;&#27169;&#24577;&#30340;SNNs&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#24773;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#21463;&#21040;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#23545;&#40784;&#39044;&#35757;&#32451;+&#21452;&#25439;&#22833;&#24494;&#35843;&#8221;&#30340;&#20004;&#27493;&#39588;&#37197;&#26041;&#65292;&#26469;&#35299;&#20915;&#33033;&#20914;&#35745;&#31639;&#32972;&#26223;&#19979;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24120;&#29992;&#30340;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;SNNs&#21462;&#24471;&#20102;&#19982;&#20854;DNNs&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;SpikeCLIP&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#20445;&#25345;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;RAG&#21644;ER&#25216;&#26415;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#20892;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.06225</link><description>&lt;p&gt;
GPT-4&#20316;&#20026;&#20892;&#23398;&#21161;&#25163;&#65311;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#31572;&#20892;&#19994;&#32771;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models. (arXiv:2310.06225v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;RAG&#21644;ER&#25216;&#26415;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#22312;&#20892;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#65292;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#65292;LLM&#30340;&#24615;&#33021;&#19982;&#35757;&#32451;&#26377;&#32032;&#30340;&#20154;&#31867;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#65292;&#22240;&#27492;&#21512;&#29702;&#22320;&#20351;&#29992;&#20154;&#31867;&#32771;&#35797;&#65288;&#20363;&#22914;&#35748;&#35777;&#32771;&#35797;&#65289;&#26469;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;LLM&#65288;&#22914;Llama 2&#21644;GPT&#65289;&#22312;&#22238;&#31572;&#20892;&#19994;&#30456;&#20851;&#38382;&#39064;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#36816;&#29992;&#20102;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#21644;ER&#65288;&#38598;&#21512;&#32454;&#21270;&#65289;&#25216;&#26415;&#65292;&#32467;&#21512;&#20449;&#24687;&#26816;&#32034;&#12289;&#29983;&#25104;&#33021;&#21147;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#25552;&#39640;LLM&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23637;&#31034;LLM&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#26469;&#33258;&#24052;&#35199;&#12289;&#21360;&#24230;&#21644;&#32654;&#22269;&#19977;&#20010;&#26368;&#22823;&#30340;&#20892;&#19994;&#29983;&#20135;&#22269;&#30340;&#20892;&#19994;&#32771;&#35797;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#22312;&#32771;&#35797;&#20013;&#21462;&#24471;&#21450;&#26684;&#20998;&#25968;&#20197;&#33719;&#24471;&#35748;&#35777;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding across various domains, including healthcare and finance. For some tasks, LLMs achieve similar or better performance than trained human beings, therefore it is reasonable to employ human exams (e.g., certification tests) to assess the performance of LLMs. We present a comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their ability to answer agriculture-related questions. In our evaluation, we also employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement) techniques, which combine information retrieval, generation capabilities, and prompting strategies to improve the LLMs' performance. To demonstrate the capabilities of LLMs, we selected agriculture exams and benchmark datasets from three of the largest agriculture producer countries: Brazil, India, and the USA. Our analysis highlights GPT-4's ability to achieve a passing score on exams to earn cred
&lt;/p&gt;</description></item><item><title>Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05898</link><description>&lt;p&gt;
&#29422;&#23376;&#31192;&#23494;&#22320;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#65306;&#27491;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25152;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05898
&lt;/p&gt;
&lt;p&gt;
Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;Lion&#65288;&#36827;&#21270;&#30340;&#31526;&#21495;&#21160;&#37327;&#65289;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23427;&#22312;&#35757;&#32451;&#25928;&#26524;&#19978;&#19982;AdamW&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#27491;&#22914;&#25105;&#20204;&#21487;&#20197;&#20174;&#38543;&#26426;&#25628;&#32034;&#31243;&#24207;&#30340;&#32467;&#26524;&#20013;&#26399;&#24453;&#30340;&#65292;Lion&#38598;&#25104;&#20102;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#21253;&#25324;&#31526;&#21495;&#21160;&#37327;&#12289;&#29420;&#31435;&#30340;&#26435;&#37325;&#34928;&#20943;&#12289;Polak&#21644;Nesterov&#21160;&#37327;&#65292;&#20294;&#21448;&#19981;&#23646;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#29702;&#35770;&#22522;&#30784;&#20248;&#21270;&#22120;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Lion&#20316;&#20026;&#24191;&#27867;&#20219;&#21153;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#29702;&#35770;&#30340;&#26126;&#30830;&#24615;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21644;&#25193;&#23637;Lion&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#24320;Lion&#30340;&#31070;&#31192;&#38754;&#32433;&#12290;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;$f(x)$&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Transformer&#32534;&#30721;&#22120;&#21644;&#20855;&#26377;&#26412;&#22320;&#24863;&#30693;&#30340;INR&#35299;&#30721;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#27867;&#21270;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#20013;&#26080;&#27861;&#23450;&#20301;&#21644;&#25429;&#33719;&#32454;&#31890;&#24230;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05624</link><description>&lt;p&gt;
&#21487;&#36866;&#24212;&#26412;&#22320;&#24615;&#24863;&#30693;&#30340;&#27867;&#21270;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Locality-Aware Generalizable Implicit Neural Representation. (arXiv:2310.05624v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;Transformer&#32534;&#30721;&#22120;&#21644;&#20855;&#26377;&#26412;&#22320;&#24863;&#30693;&#30340;INR&#35299;&#30721;&#22120;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#27867;&#21270;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#20013;&#26080;&#27861;&#23450;&#20301;&#21644;&#25429;&#33719;&#32454;&#31890;&#24230;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#36890;&#36807;&#20351;&#29992;&#28508;&#22312;&#32534;&#30721;&#26469;&#35843;&#33410;&#20854;&#26435;&#37325;&#25110;&#20013;&#38388;&#29305;&#24449;&#65292;&#20351;&#21333;&#20010;&#36830;&#32493;&#20989;&#25968;&#65288;&#21363;&#22522;&#20110;&#22352;&#26631;&#30340;&#31070;&#32463;&#32593;&#32476;&#65289;&#33021;&#22815;&#34920;&#31034;&#22810;&#20010;&#25968;&#25454;&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#35843;&#21046;&#26041;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#26080;&#27861;&#23450;&#20301;&#21644;&#25429;&#33719;&#25968;&#25454;&#23454;&#20307;&#65288;&#22914;&#29305;&#23450;&#20687;&#32032;&#21644;&#20809;&#32447;&#65289;&#30340;&#32454;&#31890;&#24230;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;Transformer&#32534;&#30721;&#22120;&#19982;&#20855;&#26377;&#26412;&#22320;&#24863;&#30693;INR&#35299;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;Transformer&#32534;&#30721;&#22120;&#20174;&#25968;&#25454;&#23454;&#20363;&#20013;&#39044;&#27979;&#19968;&#32452;&#28508;&#22312;&#20196;&#29260;&#65292;&#23558;&#26412;&#22320;&#20449;&#24687;&#32534;&#30721;&#21040;&#27599;&#20010;&#28508;&#22312;&#20196;&#29260;&#20013;&#12290;&#20855;&#26377;&#26412;&#22320;&#24863;&#30693;&#30340;INR&#35299;&#30721;&#22120;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#26377;&#36873;&#25321;&#22320;&#32858;&#21512;&#28508;&#22312;&#20196;&#29260;&#20197;&#33719;&#21462;&#22352;&#26631;&#36755;&#20837;&#30340;&#35843;&#21046;&#21521;&#37327;&#65292;&#38543;&#21518;&#36890;&#36807;&#22810;&#20010;&#39057;&#24102;&#36880;&#27493;&#35299;&#30721;&#20197;&#23454;&#29616;&#31895;&#32454;&#35843;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizable implicit neural representation (INR) enables a single continuous function, i.e., a coordinate-based neural network, to represent multiple data instances by modulating its weights or intermediate features using latent codes. However, the expressive power of the state-of-the-art modulation is limited due to its inability to localize and capture fine-grained details of data entities such as specific pixels and rays. To address this issue, we propose a novel framework for generalizable INR that combines a transformer encoder with a locality-aware INR decoder. The transformer encoder predicts a set of latent tokens from a data instance to encode local information into each latent token. The locality-aware INR decoder extracts a modulation vector by selectively aggregating the latent tokens via cross-attention for a coordinate input and then predicts the output by progressively decoding with coarse-to-fine modulation through multiple frequency bandwidths. The selective token ag
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20854;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.05288</link><description>&lt;p&gt;
&#24102;&#26377;&#24322;&#24120;&#20540;&#30340;&#19977;&#20803;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustering Three-Way Data with Outliers. (arXiv:2310.05288v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05288
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20854;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#21464;&#37327;&#20998;&#24067;&#26159;&#27169;&#22411;&#32858;&#31867;&#39046;&#22495;&#30340;&#26368;&#26032;&#28155;&#21152;&#65292;&#20174;&#32780;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#22797;&#26434;&#32467;&#26500;&#65288;&#22914;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#65289;&#30340;&#30697;&#38453;&#24418;&#24335;&#25968;&#25454;&#12290;&#30001;&#20110;&#20854;&#26368;&#36817;&#30340;&#20986;&#29616;&#65292;&#20851;&#20110;&#30697;&#38453;&#21464;&#37327;&#25968;&#25454;&#30340;&#25991;&#29486;&#26377;&#38480;&#65292;&#23545;&#20110;&#22788;&#29702;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#24322;&#24120;&#20540;&#30340;&#25991;&#29486;&#26356;&#23569;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30697;&#38453;&#21464;&#37327;&#27491;&#24577;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#23376;&#38598;&#23545;&#25968;&#20284;&#28982;&#30340;&#20998;&#24067;&#65292;&#23558;OCLUST&#31639;&#27861;&#25193;&#23637;&#21040;&#30697;&#38453;&#21464;&#37327;&#27491;&#24577;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#36845;&#20195;&#26041;&#27861;&#26816;&#27979;&#21644;&#21098;&#35009;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix-variate distributions are a recent addition to the model-based clustering field, thereby making it possible to analyze data in matrix form with complex structure such as images and time series. Due to its recent appearance, there is limited literature on matrix-variate data, with even less on dealing with outliers in these models. An approach for clustering matrix-variate normal data with outliers is discussed. The approach, which uses the distribution of subset log-likelihoods, extends the OCLUST algorithm to matrix-variate normal data and uses an iterative approach to detect and trim outliers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; TEMPO&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#20010;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#23558;&#22797;&#26434;&#20132;&#20114;&#20998;&#35299;&#21644;&#24341;&#20837;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#26469;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.04948</link><description>&lt;p&gt;
TEMPO: &#22522;&#20110;&#25552;&#31034;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting. (arXiv:2310.04948v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550; TEMPO&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#20010;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#23558;&#22797;&#26434;&#20132;&#20114;&#20998;&#35299;&#21644;&#24341;&#20837;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#26469;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#22312;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#26368;&#22909;&#30340;&#26550;&#26500;&#22312;&#19981;&#21516;&#24212;&#29992;&#21644;&#39046;&#22495;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;(GPT)&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25506;&#32034;&#26159;&#21542;GPT&#31867;&#22411;&#30340;&#26550;&#26500;&#21487;&#20197;&#23545;&#26102;&#38388;&#24207;&#21015;&#20135;&#29983;&#26377;&#25928;&#30340;&#24433;&#21709;&#65292;&#25429;&#25417;&#20854;&#20869;&#22312;&#21160;&#24577;&#23646;&#24615;&#24182;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEMPO&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#20004;&#31181;&#37325;&#35201;&#24402;&#32435;&#20559;&#24046;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;(i) &#23545;&#36235;&#21183;&#12289;&#23395;&#33410;&#21644;&#27531;&#24046;&#25104;&#20998;&#22797;&#26434;&#20132;&#20114;&#30340;&#20998;&#35299;&#65307;&#21644;(ii) &#25552;&#20986;&#22522;&#20110;&#36873;&#25321;&#30340;&#25552;&#31034;&#20197;&#20415;&#20110;&#38750;&#20998;&#24067;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer (GPT) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether GPT-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, TEMPO, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the selection-based prompts to facilitate distribution adaptation in non-
&lt;/p&gt;</description></item><item><title>DeepSpeed4Science&#35745;&#21010;&#26088;&#22312;&#36890;&#36807;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#25216;&#26415;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25552;&#20379;&#38024;&#23545;&#29420;&#29305;&#22797;&#26434;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#33258;&#28982;&#31185;&#23398;&#24102;&#26469;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.04610</link><description>&lt;p&gt;
DeepSpeed4Science&#35745;&#21010;&#65306;&#36890;&#36807;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#25216;&#26415;&#23454;&#29616;&#22823;&#35268;&#27169;&#31185;&#23398;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies. (arXiv:2310.04610v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04610
&lt;/p&gt;
&lt;p&gt;
DeepSpeed4Science&#35745;&#21010;&#26088;&#22312;&#36890;&#36807;&#20808;&#36827;&#30340;AI&#31995;&#32479;&#25216;&#26415;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65292;&#25552;&#20379;&#38024;&#23545;&#29420;&#29305;&#22797;&#26434;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#33258;&#28982;&#31185;&#23398;&#24102;&#26469;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21363;&#23558;&#21040;&#26469;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#33258;&#28982;&#31185;&#23398;&#65292;&#22686;&#24378;&#25105;&#20204;&#23545;&#33258;&#28982;&#29616;&#35937;&#24314;&#27169;&#21644;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#33021;&#39044;&#31034;&#30528;&#31185;&#23398;&#25506;&#32034;&#30340;&#26032;&#26102;&#20195;&#65292;&#20174;&#33647;&#29289;&#24320;&#21457;&#21040;&#21487;&#20877;&#29983;&#33021;&#28304;&#31561;&#39046;&#22495;&#37117;&#23558;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;&#20026;&#20102;&#22238;&#24212;&#36825;&#19968;&#21628;&#21505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepSpeed4Science&#35745;&#21010;&#65288;deepspeed4science.ai&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;AI&#31995;&#32479;&#25216;&#26415;&#21019;&#26032;&#26500;&#24314;&#29420;&#29305;&#30340;&#33021;&#21147;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#35299;&#24320;&#20170;&#22825;&#26368;&#22823;&#30340;&#31185;&#23398;&#20043;&#35868;&#12290;&#36890;&#36807;&#21033;&#29992;DeepSpeed&#24403;&#21069;&#30340;&#25216;&#26415;&#25903;&#26609;&#65288;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#21387;&#32553;&#65289;&#20316;&#20026;&#22522;&#30784;&#25216;&#26415;&#25903;&#25345;&#65292;DeepSpeed4Science&#23558;&#21019;&#24314;&#19968;&#22871;&#38024;&#23545;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;AI&#31995;&#32479;&#25216;&#26415;&#65292;&#20197;&#24212;&#23545;&#20854;&#29420;&#29305;&#30340;&#22797;&#26434;&#24615;&#65292;&#36229;&#36234;&#24120;&#35265;&#30340;&#29992;&#20110;&#21152;&#36895;&#36890;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DeepSpeed4Science&#22312;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#19978;&#30340;&#26089;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the upcoming decade, deep learning may revolutionize the natural sciences, enhancing our capacity to model and predict natural occurrences. This could herald a new era of scientific exploration, bringing significant advancements across sectors from drug development to renewable energy. To answer this call, we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. By leveraging DeepSpeed's current technology pillars (training, inference and compression) as base technology enablers, DeepSpeed4Science will create a new set of AI system technologies tailored for accelerating scientific discoveries by addressing their unique complexity beyond the common technical approaches used for accelerating generic large language models (LLMs). In this paper, we showcase the early progress we made with DeepSpeed4Science in addressing two of the cri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#31574;&#30053;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#65292;&#25552;&#39640;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04413</link><description>&lt;p&gt;
&#36229;&#36234;&#22343;&#21248;&#37319;&#26679;&#65306;&#20351;&#29992;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2310.04413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#31574;&#30053;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#65292;&#25552;&#39640;&#20102;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#29616;&#26377;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#20915;&#31574;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#25910;&#38598;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#19982;&#34892;&#20026;&#20811;&#38534;&#31561;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#25214;&#21040;&#19968;&#20010;&#27604;&#25968;&#25454;&#38598;&#20013;&#30340;&#36712;&#36857;&#36798;&#21040;&#26356;&#39640;&#24179;&#22343;&#25910;&#30410;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#21457;&#29616;&#65292;&#24403;&#19968;&#20010;&#25968;&#25454;&#38598;&#34987;&#27425;&#20248;&#36712;&#36857;&#25152;&#20027;&#23548;&#26102;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#22312;&#24179;&#22343;&#25910;&#30410;&#19978;&#27809;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#24403;&#21069;&#31163;&#32447;RL&#31639;&#27861;&#20551;&#35774;&#19982;&#25968;&#25454;&#38598;&#20013;&#30340;&#36712;&#36857;&#20445;&#25345;&#25509;&#36817;&#12290;&#22914;&#26524;&#25968;&#25454;&#38598;&#20027;&#35201;&#30001;&#27425;&#20248;&#36712;&#36857;&#32452;&#25104;&#65292;&#36825;&#20010;&#20551;&#35774;&#23558;&#24378;&#21046;&#31574;&#30053;&#27169;&#20223;&#27425;&#20248;&#21160;&#20316;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#26469;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#31574;&#30053;&#21482;&#21463;``&#22909;&#25968;&#25454;"&#38480;&#21046;&#65292;&#32780;&#19981;&#26159;&#25152;&#26377;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline policy learning is aimed at learning decision-making policies using existing datasets of trajectories without collecting additional data. The primary motivation for using reinforcement learning (RL) instead of supervised learning techniques such as behavior cloning is to find a policy that achieves a higher average return than the trajectories constituting the dataset. However, we empirically find that when a dataset is dominated by suboptimal trajectories, state-of-the-art offline RL algorithms do not substantially improve over the average return of trajectories in the dataset. We argue this is due to an assumption made by current offline RL algorithms of staying close to the trajectories in the dataset. If the dataset primarily consists of sub-optimal trajectories, this assumption forces the policy to mimic the suboptimal actions. We overcome this issue by proposing a sampling strategy that enables the policy to only be constrained to ``good data" rather than all actions in t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU&#28608;&#27963;&#20989;&#25968;&#65288;IReLU&#65289;&#26469;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#65292;&#24182;&#25506;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#23545;&#20110;&#31283;&#23450;DC&#35757;&#32451;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#12289;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2310.01649</link><description>&lt;p&gt;
&#20851;&#20110;&#35757;&#32451;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On Training Derivative-Constrained Neural Networks. (arXiv:2310.01649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU&#28608;&#27963;&#20989;&#25968;&#65288;IReLU&#65289;&#26469;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#65292;&#24182;&#25506;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#23545;&#20110;&#31283;&#23450;DC&#35757;&#32451;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#12289;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#30340;&#39044;&#27979;&#30456;&#23545;&#20110;&#36755;&#20837;&#30340;&#65288;&#37096;&#20998;&#65289;&#23548;&#25968;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#20449;&#21495;&#30340;&#24773;&#20917;&#31216;&#20043;&#20026;&#23548;&#25968;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65288;DC NN&#65289;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#30340;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;RELU (IReLU)&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#25913;&#36827;DC NN&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#20197;&#24110;&#21161;&#31283;&#23450;DC&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#37327;&#23376;&#21270;&#23398;&#21644;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#20219;&#21153;&#22312;&#20869;&#30340;&#29289;&#29702;&#30456;&#20851;&#35774;&#32622;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;IReLU&#28608;&#27963;&#20989;&#25968;&#32467;&#21512;&#21435;&#24402;&#19968;&#21270;&#21644;&#26631;&#31614;&#32553;&#25918;&#30340;&#29616;&#26377;&#26550;&#26500;&#26356;&#22909;&#22320;&#34701;&#20837;&#20102;&#23548;&#25968;&#32422;&#26463;&#25552;&#20379;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We refer to the setting where the (partial) derivatives of a neural network's (NN's) predictions with respect to its inputs are used as additional training signal as a derivative-constrained (DC) NN. This situation is common in physics-informed settings in the natural sciences. We propose an integrated RELU (IReLU) activation function to improve training of DC NNs. We also investigate denormalization and label rescaling to help stabilize DC training. We evaluate our methods on physics-informed settings including quantum chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that existing architectures with IReLU activations combined with denormalization and label rescaling better incorporate training signal provided by derivative constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#26500;&#24314;&#19968;&#20010;&#25554;&#20540;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#26080;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.00327</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#21270;&#65306;&#36229;&#36234;&#26368;&#22351;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Memorization with neural nets: going beyond the worst case. (arXiv:2310.00327v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#26500;&#24314;&#19968;&#20010;&#25554;&#20540;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#33021;&#22815;&#36731;&#26494;&#22320;&#25554;&#20540;&#20854;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#35768;&#22810;&#30740;&#31350;&#37117;&#26088;&#22312;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#35760;&#24518;&#33021;&#21147;&#65306;&#21363;&#22312;&#20219;&#24847;&#25918;&#32622;&#36825;&#20123;&#28857;&#24182;&#20219;&#24847;&#20998;&#37197;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#26550;&#26500;&#33021;&#22815;&#25554;&#20540;&#30340;&#26368;&#22823;&#28857;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23454;&#38469;&#25968;&#25454;&#65292;&#20154;&#20204;&#30452;&#35273;&#22320;&#26399;&#26395;&#23384;&#22312;&#19968;&#31181;&#33391;&#24615;&#32467;&#26500;&#65292;&#20351;&#24471;&#25554;&#20540;&#22312;&#27604;&#35760;&#24518;&#33021;&#21147;&#24314;&#35758;&#30340;&#36739;&#23567;&#32593;&#32476;&#23610;&#23544;&#19978;&#24050;&#32463;&#21457;&#29983;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#23454;&#20363;&#29305;&#23450;&#30340;&#35266;&#28857;&#26469;&#30740;&#31350;&#25554;&#20540;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#32473;&#23450;&#19968;&#20010;&#22266;&#23450;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#26500;&#24314;&#20986;&#19968;&#20010;&#25554;&#20540;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#19982;&#36825;&#20004;&#20010;&#31867;&#30340;&#20960;&#20309;&#29305;&#24615;&#21450;&#20854;&#30456;&#20114;&#25490;&#21015;&#26377;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#26080;&#20851;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, deep neural networks are often able to easily interpolate their training data. To understand this phenomenon, many works have aimed to quantify the memorization capacity of a neural network architecture: the largest number of points such that the architecture can interpolate any placement of these points with any assignment of labels. For real-world data, however, one intuitively expects the presence of a benign structure so that interpolation already occurs at a smaller network size than suggested by memorization capacity. In this paper, we investigate interpolation by adopting an instance-specific viewpoint. We introduce a simple randomized algorithm that, given a fixed finite dataset with two classes, with high probability constructs an interpolating three-layer neural network in polynomial time. The required number of parameters is linked to geometric properties of the two classes and their mutual arrangement. As a result, we obtain guarantees that are independent of t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36817;&#20284;&#36870;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.00177</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36842;&#37324;&#20999;&#29305;&#21644;&#35834;&#26364;&#36793;&#30028;&#26465;&#20214;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#27850;&#26494;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions. (arXiv:2310.00177v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00177
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36817;&#20284;&#36870;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#39044;&#22788;&#29702;&#30340;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20855;&#26377;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#12290;&#27850;&#26494;&#26041;&#31243;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65306;&#23427;&#25511;&#21046;&#30528;&#24191;&#27867;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#22312;&#35768;&#22810;&#25968;&#20540;&#31639;&#27861;&#20013;&#20316;&#20026;&#23376;&#38382;&#39064;&#20986;&#29616;&#65292;&#24182;&#19988;&#20316;&#20026;&#26356;&#24191;&#27867;&#30340;&#26925;&#22278;PDE&#31867;&#30340;&#27169;&#22411;&#38382;&#39064;&#12290;&#26368;&#27969;&#34892;&#30340;&#27850;&#26494;&#31163;&#25955;&#21270;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#22823;&#22411;&#31232;&#30095;&#32447;&#24615;&#31995;&#32479;&#12290;&#22312;&#39640;&#20998;&#36776;&#29575;&#21644;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#36845;&#20195;&#27714;&#35299;&#22120;&#32467;&#21512;&#24378;&#22823;&#30340;&#39044;&#22788;&#29702;&#22120;&#21487;&#20197;&#25552;&#20379;&#20248;&#21183;&#12290;&#25105;&#20204;&#27714;&#35299;&#22120;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36817;&#20284;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#36870;&#31639;&#23376;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#24418;&#29366;&#30340;&#22495;&#21644;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#38382;&#39064;&#30340;&#32467;&#26500;&#28608;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#65292;&#35813;&#26550;&#26500;&#20063;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#39044;&#22788;&#29702;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26696;&#20363;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. The Poisson equation is ubiquitous in scientific computing: it governs a wide array of physical phenomena, arises as a subproblem in many numerical algorithms, and serves as a model problem for the broader class of elliptic PDEs. The most popular Poisson discretizations yield large sparse linear systems. At high resolution, and for performance-critical applications, iterative solvers can be advantageous for these -- but only when paired with powerful preconditioners. The core of our solver is a neural network trained to approximate the inverse of a discrete structured-grid Laplace operator for a domain of arbitrary shape and with mixed boundary conditions. The structure of this problem motivates a novel network architecture that we demonstrate is highly effective as a preconditioner even for boundary conditions outside the training set. We show that on challenging test cases aris
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;&#24320;&#21457;&#32773;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16584</link><description>&lt;p&gt;
&#29992;&#20110;&#24320;&#21457;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems. (arXiv:2309.16584v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16584
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;&#24320;&#21457;&#32773;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26426;&#23494;&#24615;&#30340;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#22810;&#26041;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#20805;&#20998;&#35757;&#32451;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;CDML&#65289;&#31995;&#32479;&#35774;&#35745;&#65292;&#20363;&#22914;&#36741;&#21161;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#20998;&#35010;&#23398;&#20064;&#12290;CDML&#31995;&#32479;&#35774;&#35745;&#23637;&#31034;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#39640;&#24230;&#30340;&#20195;&#29702;&#20154;&#33258;&#27835;&#24615;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26426;&#23494;&#24615;&#21644;&#23481;&#38169;&#24615;&#12290;&#38754;&#23545;&#19981;&#21516;&#29305;&#24449;&#30340;&#21508;&#31181;CDML&#31995;&#32479;&#35774;&#35745;&#65292;&#24320;&#21457;&#32773;&#24456;&#38590;&#26377;&#38024;&#23545;&#24615;&#22320;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;CDML&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#19981;&#21512;&#36866;&#30340;CDML&#31995;&#32479;&#35774;&#35745;&#21487;&#33021;&#23548;&#33268;CDML&#31995;&#32479;&#26080;&#27861;&#23454;&#29616;&#20854;&#39044;&#26399;&#30446;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;CDML&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#22522;&#20110;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#19981;&#21516;&#20851;&#38190;&#29305;&#24449;&#30340;CDML&#31995;&#32479;&#20856;&#22411;&#65292;&#21487;&#20197;&#25903;&#25345;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;CDML&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
To leverage training data for the sufficient training of ML models from multiple parties in a confidentiality-preserving way, various collaborative distributed machine learning (CDML) system designs have been developed, for example, to perform assisted learning, federated learning, and split learning. CDML system designs show different traits, for example, high agent autonomy, machine learning (ML) model confidentiality, and fault tolerance. Facing a wide variety of CDML system designs with different traits, it is difficult for developers to design CDML systems with traits that match use case requirements in a targeted way. However, inappropriate CDML system designs may result in CDML systems failing their envisioned purposes. We developed a CDML design toolbox that can guide the development of CDML systems. Based on the CDML design toolbox, we present CDML system archetypes with distinct key traits that can support the design of CDML systems to meet use case requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26080;&#20154;&#26426;&#22312;&#26641;&#26408;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#65292;&#21253;&#25324;&#28608;&#20809;&#38647;&#36798;&#21644;&#33322;&#25293;&#22270;&#20687;&#30340;&#28857;&#20113;&#21644;&#22270;&#20687;&#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#20998;&#31867;&#21644;&#23545;&#27604;&#20998;&#26512;&#65292;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12289;&#20248;&#21183;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.16375</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#28608;&#20809;&#38647;&#36798;&#21644;&#33322;&#25293;&#22270;&#20687;&#22312;&#26641;&#26408;&#26816;&#27979;&#20013;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review on Tree Detection Methods Using Point Cloud and Aerial Imagery from Unmanned Aerial Vehicles. (arXiv:2309.16375v2 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26080;&#20154;&#26426;&#22312;&#26641;&#26408;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#65292;&#21253;&#25324;&#28608;&#20809;&#38647;&#36798;&#21644;&#33322;&#25293;&#22270;&#20687;&#30340;&#28857;&#20113;&#21644;&#22270;&#20687;&#25968;&#25454;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#20998;&#31867;&#21644;&#23545;&#27604;&#20998;&#26512;&#65292;&#24635;&#32467;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12289;&#20248;&#21183;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#26159;&#20855;&#26377;&#39640;&#25928;&#20302;&#25104;&#26412;&#21644;&#28789;&#27963;&#24212;&#29992;&#22330;&#26223;&#30340;&#21069;&#27839;&#25216;&#26415;&#12290;&#34429;&#28982;&#35768;&#22810;&#25991;&#31456;&#24050;&#32463;&#23545;&#26080;&#20154;&#26426;&#22312;&#20892;&#19994;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#20294;&#23545;&#20110;&#26641;&#26408;&#26816;&#27979;&#30340;&#32508;&#36848;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#24212;&#29992;&#20110;&#26080;&#20154;&#26426;&#25968;&#25454;&#30340;&#26641;&#26408;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#30001;&#28608;&#20809;&#38647;&#36798;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#28857;&#20113;&#25968;&#25454;&#21644;&#30001;&#30456;&#26426;&#33719;&#21462;&#30340;&#22270;&#20687;&#25968;&#25454;&#12290;&#38024;&#23545;&#20351;&#29992;&#28857;&#20113;&#25968;&#25454;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#26412;&#25991;&#20027;&#35201;&#26681;&#25454;&#20351;&#29992;&#28608;&#20809;&#38647;&#36798;&#21644;&#25968;&#23383;&#33322;&#31354;&#25668;&#24433;&#26415;&#65288;DAP&#65289;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#12290;&#32780;&#23545;&#20110;&#30452;&#25509;&#20351;&#29992;&#22270;&#20687;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#26412;&#25991;&#21017;&#26681;&#25454;&#26159;&#21542;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#36827;&#34892;&#32508;&#36848;&#12290;&#32508;&#36848;&#24635;&#32467;&#24182;&#20998;&#26512;&#20102;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#21644;DAP&#30340;&#28857;&#20113;&#25968;&#25454;&#24212;&#29992;&#20043;&#38388;&#30340;&#27604;&#36739;&#21644;&#32452;&#21512;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24615;&#33021;&#12289;&#20248;&#21183;&#21644;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unmanned Aerial Vehicles (UAVs) are considered cutting-edge technology with highly cost-effective and flexible usage scenarios. Although many papers have reviewed the application of UAVs in agriculture, the review of the application for tree detection is still insufficient. This paper focuses on tree detection methods applied to UAV data collected by UAVs. There are two kinds of data, the point cloud and the images, which are acquired by the Light Detection and Ranging (LiDAR) sensor and camera, respectively. Among the detection methods using point-cloud data, this paper mainly classifies these methods according to LiDAR and Digital Aerial Photography (DAP). For the detection methods using images directly, this paper reviews these methods by whether or not to use the Deep Learning (DL) method. Our review concludes and analyses the comparison and combination between the application of LiDAR-based and DAP-based point cloud data. The performance, relative merits, and application fields of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26377;&#38480;&#26631;&#37327;&#37327;&#21270; (FSQ) &#26041;&#27861;&#65292;&#29992;&#26469;&#31616;&#21270; VQ-VAE &#26041;&#27861;&#20013;&#30340;&#21521;&#37327;&#37327;&#21270; (VQ)&#12290;&#36890;&#36807;&#25237;&#24433;&#21644;&#37327;&#21270; VAE &#34920;&#31034;&#65292;&#25105;&#20204;&#24471;&#21040;&#19982; VQ &#30456;&#21516;&#22823;&#23567;&#30340;&#30721;&#26412;&#12290;&#22312;&#36825;&#31181;&#31163;&#25955;&#34920;&#31034;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#30456;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#29983;&#25104;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15505</link><description>&lt;p&gt;
&#26377;&#38480;&#26631;&#37327;&#37327;&#21270;: &#31616;&#21270; VQ-VAE &#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finite Scalar Quantization: VQ-VAE Made Simple. (arXiv:2309.15505v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26377;&#38480;&#26631;&#37327;&#37327;&#21270; (FSQ) &#26041;&#27861;&#65292;&#29992;&#26469;&#31616;&#21270; VQ-VAE &#26041;&#27861;&#20013;&#30340;&#21521;&#37327;&#37327;&#21270; (VQ)&#12290;&#36890;&#36807;&#25237;&#24433;&#21644;&#37327;&#21270; VAE &#34920;&#31034;&#65292;&#25105;&#20204;&#24471;&#21040;&#19982; VQ &#30456;&#21516;&#22823;&#23567;&#30340;&#30721;&#26412;&#12290;&#22312;&#36825;&#31181;&#31163;&#25955;&#34920;&#31034;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#30456;&#21516;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#29983;&#25104;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#29992;&#26377;&#38480;&#26631;&#37327;&#37327;&#21270; (FSQ) &#26367;&#20195; VQ-VAE &#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#21521;&#37327;&#37327;&#21270; (VQ)&#12290;&#22312; FSQ &#20013;&#65292;&#25105;&#20204;&#23558; VAE &#34920;&#31034;&#25237;&#24433;&#21040;&#20960;&#20010;&#32500;&#24230; (&#36890;&#24120;&#23569;&#20110;10&#20010;)&#65292;&#27599;&#20010;&#32500;&#24230;&#34987;&#37327;&#21270;&#20026;&#19968;&#32452;&#22266;&#23450;&#30340;&#20540;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#65288;&#38544;&#24335;&#30340;&#65289;&#30721;&#26412;&#65292;&#30001;&#36825;&#20123;&#20540;&#30340;&#20056;&#31215;&#32452;&#25104;&#12290;&#36890;&#36807;&#21512;&#36866;&#22320;&#36873;&#25321;&#32500;&#24230;&#21644;&#27599;&#20010;&#32500;&#24230;&#21487;&#20197;&#21462;&#30340;&#20540;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#33719;&#24471;&#19982; VQ &#20013;&#30456;&#21516;&#30340;&#30721;&#26412;&#22823;&#23567;&#12290;&#22312;&#36825;&#26679;&#30340;&#31163;&#25955;&#34920;&#31034;&#19978;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#24050;&#32463;&#22312; VQ-VAE &#34920;&#31034;&#19978;&#35757;&#32451;&#36807;&#30340;&#30456;&#21516;&#27169;&#22411;&#65292;&#20363;&#22914;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#12289;&#22810;&#27169;&#24577;&#29983;&#25104;&#21644;&#23494;&#38598;&#39044;&#27979;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#33258;&#22238;&#24402;&#21644;&#25513;&#30721;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#20351;&#29992; FSQ &#21644; MaskGIT&#65292;&#22312;&#28145;&#24230;&#20272;&#35745;&#12289;&#30528;&#33394;&#21644;&#20840;&#26223;&#20998;&#21106;&#20013;&#20351;&#29992; FSQ &#21644; UViM&#12290;&#23613;&#31649; FSQ &#30340;&#35774;&#35745;&#35201;&#31616;&#21333;&#24471;&#22810;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRI&#65292;&#29992;&#20110;&#22312;&#32447;CMDPs&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;CMDPs&#30340;&#26377;&#38480;&#38543;&#26426;&#24615;&#23646;&#24615;&#65292;&#33021;&#22815;&#20197;&#20302;&#36951;&#25022;&#24182;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.15395</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;CMDPs&#20013;&#65292;&#26080;&#27169;&#22411;&#12289;&#36951;&#25022;&#26368;&#20248;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs. (arXiv:2309.15395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRI&#65292;&#29992;&#20110;&#22312;&#32447;CMDPs&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;CMDPs&#30340;&#26377;&#38480;&#38543;&#26426;&#24615;&#23646;&#24615;&#65292;&#33021;&#22815;&#20197;&#20302;&#36951;&#25022;&#24182;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#65288;BPI&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20855;&#26377;&#20302;&#36951;&#25022;&#24182;&#19988;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#26368;&#20248;&#31574;&#30053;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#24863;&#20852;&#36259;&#12290;&#29616;&#26377;&#30340;&#22312;&#32447;CMDPs&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#22312;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#36829;&#32422;&#26102;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#21482;&#22312;&#20174;&#20197;&#21069;&#20351;&#29992;&#30340;&#31574;&#30053;&#20013;&#38543;&#26426;&#22343;&#21248;&#25277;&#26679;&#26102;&#25552;&#20379;&#24179;&#22343;&#24615;&#33021;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRUNING-REFINEMENT-IDENTIFICATION&#65288;PRI&#65289;&#65292;&#22522;&#20110;&#25105;&#20204;&#21457;&#29616;&#30340;CMDPs&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26500;&#24615;&#36136;&#65292;&#31216;&#20026;&#26377;&#38480;&#38543;&#26426;&#24615;&#12290;&#35813;&#23646;&#24615;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;N&#32422;&#26463;&#30340;CMDP&#65292;&#23384;&#22312;&#19968;&#20010;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#33267;&#22810;&#26377;N&#20010;&#38543;&#26426;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#39318;&#20808;&#35782;&#21035;&#20986;&#22312;&#21738;&#20010;&#27493;&#39588;&#21644;&#21738;&#20010;&#29366;&#24577;&#38656;&#35201;&#36827;&#34892;&#38543;&#26426;&#20915;&#31574;&#65292;&#28982;&#21518;&#23545;&#36825;&#20123;&#20915;&#31574;&#30340;&#20998;&#24067;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs). We are interested in algorithms that are model-free, have low regret, and identify an optimal policy with a high probability. Existing model-free algorithms for online CMDPs with sublinear regret and constraint violation do not provide any convergence guarantee to an optimal policy and provide only average performance guarantees when a policy is uniformly sampled at random from all previously used policies. In this paper, we develop a new algorithm, named Pruning-Refinement-Identification (PRI), based on a fundamental structural property of CMDPs we discover, called limited stochasticity. The property says for a CMDP with $N$ constraints, there exists an optimal policy with at most $N$ stochastic decisions.  The proposed algorithm first identifies at which step and in which state a stochastic decision has to be taken and then fine-tunes the distributions of these s
&lt;/p&gt;</description></item><item><title>PRiSM&#26159;&#19968;&#31181;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#20998;&#25968;&#26657;&#20934;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#26102;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.13869</link><description>&lt;p&gt;
PRiSM: &#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#20998;&#25968;&#26657;&#20934;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration. (arXiv:2309.13869v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13869
&lt;/p&gt;
&lt;p&gt;
PRiSM&#26159;&#19968;&#31181;&#22686;&#24378;&#20302;&#36164;&#28304;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20851;&#31995;&#24863;&#30693;&#20998;&#25968;&#26657;&#20934;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#25104;&#21151;&#22320;&#38477;&#20302;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#26102;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65288;DocRE&#65289;&#26088;&#22312;&#25552;&#21462;&#25991;&#26723;&#20013;&#25152;&#26377;&#23454;&#20307;&#23545;&#30340;&#20851;&#31995;&#12290;&#22312;DocRE&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#27880;&#37322;&#36825;&#31867;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;DocRE&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#22312;&#23569;&#37327;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36807;&#39640;&#20272;&#35745;&#20102;NA&#65288;"no relation"&#65289;&#26631;&#31614;&#65292;&#23548;&#33268;&#24615;&#33021;&#21463;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#26657;&#20934;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PRiSM&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#20851;&#31995;&#35821;&#20041;&#20449;&#24687;&#26469;&#36866;&#24212;logits&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;DocRE&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23558;&#29616;&#26377;&#27169;&#22411;&#19982;PRiSM&#38598;&#25104;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;F1&#20998;&#25968;&#25552;&#39640;&#20102;26.38%&#65292;&#32780;&#24403;&#29992;&#32422;3%&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26657;&#20934;&#35823;&#24046;&#19979;&#38477;&#20102;36&#20493;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/brightjade/PRiSM&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) aims to extract relations of all entity pairs in a document. A key challenge in DocRE is the cost of annotating such data which requires intensive human effort. Thus, we investigate the case of DocRE in a low-resource setting, and we find that existing models trained on low data overestimate the NA ("no relation") label, causing limited performance. In this work, we approach the problem from a calibration perspective and propose PRiSM, which learns to adapt logits based on relation semantic information. We evaluate our method on three DocRE datasets and demonstrate that integrating existing models with PRiSM improves performance by as much as 26.38 F1 score, while the calibration error drops as much as 36 times when trained with about 3% of data. The code is publicly available at https://github.com/brightjade/PRiSM.
&lt;/p&gt;</description></item><item><title>MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10691</link><description>&lt;p&gt;
MINT: &#35780;&#20272;&#22312;&#19982;&#24037;&#20855;&#21644;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#20013;&#30340;LLMs&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10691
&lt;/p&gt;
&lt;p&gt;
MINT&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#22810;&#36718;&#20132;&#20114;&#20013;&#35299;&#20915;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#32454;&#33268;&#20114;&#21160;&#21644;&#20302;&#20272;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#38656;&#35201;&#19982;&#29992;&#25143;&#36827;&#34892;&#22810;&#36718;&#20132;&#20114;&#65292;&#26377;&#26102;&#20505;&#36741;&#20197;&#22806;&#37096;&#24037;&#20855;&#30340;&#24110;&#21161;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#24120;&#24120;&#24378;&#35843;&#29992;&#21333;&#36718;&#20132;&#27969;&#30340;&#22522;&#20934;&#24615;&#33021;&#65292;&#24573;&#30053;&#20102;&#29992;&#25143;&#12289;LLMs&#21644;&#22806;&#37096;&#24037;&#20855;&#20043;&#38388;&#30340;&#32454;&#33268;&#20114;&#21160;&#65292;&#24182;&#20302;&#20272;&#20102;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#30095;&#24573;&#23548;&#33268;&#20102;&#30740;&#31350;&#22522;&#20934;&#35780;&#20272;&#32467;&#26524;&#19982;&#23454;&#38469;&#24212;&#29992;&#24773;&#20917;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MINT&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#20351;&#29992;&#24037;&#20855;&#21644;&#21033;&#29992;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#35780;&#20272;LLMs&#35299;&#20915;&#22810;&#36718;&#20132;&#20114;&#20219;&#21153;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25191;&#34892;Python&#20195;&#30721;&#26469;&#35775;&#38382;&#24037;&#20855;&#65292;&#24182;&#25509;&#25910;&#30001;GPT-4&#27169;&#25311;&#30340;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#19968;&#31995;&#21015;&#22810;&#26679;&#30340;&#24050;&#24314;&#31435;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20851;&#27880;&#25512;&#29702;&#12289;&#32534;&#30721;&#21644;&#20915;&#31574;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. However, current evaluation protocols often emphasize benchmark performance with single-turn exchanges, neglecting the nuanced interactions among the user, LLMs, and external tools, while also underestimating the importance of natural language feedback from users. These oversights contribute to discrepancies between research benchmark evaluations and real-world use cases. We introduce MINT, a benchmark that evaluates LLMs' ability to solve tasks with multi-turn interactions by (1) using tools and (2) leveraging natural language feedback. To ensure reproducibility, we provide an evaluation framework where LLMs can access tools by executing Python code and receive users' natural language feedback simulated by GPT-4. We repurpose a diverse set of established evaluation datasets focusing on reasoning, coding, and decision-making and careful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#12290;&#32463;&#39564;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2309.05925</link><description>&lt;p&gt;
&#20851;&#20110;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Regularized Sparse Logistic Regression. (arXiv:2309.05925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#27491;&#21017;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#12290;&#32463;&#39564;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#26088;&#22312;&#21516;&#26102;&#36827;&#34892;&#39640;&#32500;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#30740;&#31350;&#35299;&#20915;&#20102;$\ell_1$&#27491;&#21017;&#21270;&#36923;&#36753;&#22238;&#24402;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#19982;&#38750;&#20984;&#24809;&#32602;&#30456;&#20851;&#30340;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#24182;&#27809;&#26377;&#31561;&#37327;&#30340;&#25991;&#29486;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;$\ell_1$&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#21644;&#19968;&#20123;&#28385;&#36275;&#19968;&#23450;&#20808;&#20915;&#26465;&#20214;&#30340;&#38750;&#20984;&#24809;&#32602;&#27491;&#21017;&#21270;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#31867;&#20284;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#22312;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#30340;&#32447;&#25628;&#32034;&#20934;&#21017;&#26469;&#20445;&#35777;&#19981;&#21516;&#27491;&#21017;&#21270;&#39033;&#30340;&#33391;&#22909;&#25910;&#25947;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#32463;&#39564;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#26377;&#25928;&#22320;&#36827;&#34892;&#20998;&#31867;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse logistic regression aims to perform classification and feature selection simultaneously for high-dimensional data. Although many studies have been done to solve $\ell_1$-regularized logistic regression, there is no equivalently abundant literature about solving sparse logistic regression associated with nonconvex penalties. In this paper, we propose to solve $\ell_1$-regularized sparse logistic regression and some nonconvex penalties-regularized sparse logistic regression, when the nonconvex penalties satisfy some prerequisites, with similar optimization frameworks. In the proposed optimization frameworks, we utilize different line search criteria to guarantee good convergence performance for different regularization terms. Empirical experiments on binary classification tasks with real-world datasets demonstrate our proposed algorithms are capable of performing classification and feature selection effectively with a lower computational cost.
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21147;&#21709;&#24212;&#24335;&#36816;&#21160;&#25511;&#21046;&#30340;&#23548;&#30450;&#22235;&#36275;&#26426;&#22120;&#20154;&#23548;&#33322;&#31995;&#32479;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#36816;&#21160;&#25511;&#21046;&#22120;&#21644;&#22806;&#21147;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#23545;&#22806;&#37096;&#25289;&#25199;&#21147;&#30340;&#31283;&#20581;&#24863;&#30693;&#21644;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#23548;&#33322;&#24182;&#32469;&#36807;&#38556;&#30861;&#29289;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04370</link><description>&lt;p&gt;
&#24102;&#26377;&#21147;&#21709;&#24212;&#24335;&#36816;&#21160;&#25511;&#21046;&#30340;&#23548;&#30450;&#22235;&#36275;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Seeing-Eye Quadruped Navigation with Force Responsive Locomotion Control. (arXiv:2309.04370v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21147;&#21709;&#24212;&#24335;&#36816;&#21160;&#25511;&#21046;&#30340;&#23548;&#30450;&#22235;&#36275;&#26426;&#22120;&#20154;&#23548;&#33322;&#31995;&#32479;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#36816;&#21160;&#25511;&#21046;&#22120;&#21644;&#22806;&#21147;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#23545;&#22806;&#37096;&#25289;&#25199;&#21147;&#30340;&#31283;&#20581;&#24863;&#30693;&#21644;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#23548;&#33322;&#24182;&#32469;&#36807;&#38556;&#30861;&#29289;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23548;&#30450;&#26426;&#22120;&#20154;&#26159;&#20026;&#23548;&#24341;&#35270;&#38556;&#20154;&#22763;&#32780;&#35774;&#35745;&#30340;&#38750;&#24120;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#37492;&#20110;&#30495;&#27491;&#23548;&#30450;&#29356;&#30340;&#21487;&#33719;&#24471;&#24615;&#20302;&#19988;&#39640;&#25104;&#26412;&#65292;&#20854;&#21487;&#33021;&#20135;&#29983;&#24040;&#22823;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290;&#23613;&#31649;&#24050;&#32463;&#28436;&#31034;&#20102;&#20960;&#20010;&#23548;&#30450;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#21040;&#23548;&#30450;&#29356;&#23454;&#38469;&#29615;&#22659;&#20013;&#32463;&#24120;&#21457;&#29983;&#30340;&#26469;&#33258;&#20154;&#31867;&#30340;&#22806;&#37096;&#25289;&#25199;&#12290;&#26412;&#25991;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21516;&#26102;&#35757;&#32451;&#20102;&#19968;&#20010;&#23545;&#22806;&#37096;&#25289;&#25199;&#21147;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#36816;&#21160;&#25511;&#21046;&#22120;&#21644;&#19968;&#20010;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#30340;&#22806;&#21147;&#20272;&#35745;&#22120;&#12290;&#25511;&#21046;&#22120;&#30830;&#20445;&#20102;&#31283;&#23450;&#30340;&#34892;&#36208;&#65292;&#22806;&#21147;&#20272;&#35745;&#22120;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23545;&#20154;&#31867;&#26045;&#21152;&#30340;&#22806;&#37096;&#21147;&#20570;&#20986;&#21709;&#24212;&#12290;&#36825;&#20123;&#21147;&#29992;&#20110;&#23558;&#26426;&#22120;&#20154;&#24341;&#23548;&#21040;&#26410;&#30693;&#30340;&#20840;&#23616;&#30446;&#26631;&#65292;&#21516;&#26102;&#26426;&#22120;&#20154;&#36890;&#36807;&#23616;&#37096;&#35268;&#21010;&#22120;&#23558;&#20154;&#31867;&#24341;&#23548;&#32469;&#36807;&#38468;&#36817;&#30340;&#38556;&#30861;&#29289;&#12290;&#22312;&#27169;&#25311;&#21644;&#30828;&#20214;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25511;&#21046;&#22120;&#23545;&#22806;&#37096;&#21147;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#23548;&#30450;&#31995;&#32479;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;...
&lt;/p&gt;
&lt;p&gt;
Seeing-eye robots are very useful tools for guiding visually impaired people, potentially producing a huge societal impact given the low availability and high cost of real guide dogs. Although a few seeing-eye robot systems have already been demonstrated, none considered external tugs from humans, which frequently occur in a real guide dog setting. In this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via Reinforcement Learning (RL), and an external force estimator via supervised learning. The controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. These forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. Experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect
&lt;/p&gt;</description></item><item><title>&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03084</link><description>&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03084
&lt;/p&gt;
&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#30446;&#21069;&#35299;&#20915;&#22823;&#35268;&#27169;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#26412;&#25991;&#22312;CFR&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32431;CFR&#65288;PCFR&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;PCFR&#21487;&#20197;&#30475;&#20316;&#26159;CFR&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#30340;&#32467;&#21512;&#65292;&#32487;&#25215;&#20102;CFR&#30340;&#21453;&#20107;&#23454;&#36951;&#25022;&#65288;&#20540;&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20351;&#29992;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#32780;&#19981;&#26159;&#36951;&#25022;&#21305;&#37197;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;PCFR&#21487;&#20197;&#23454;&#29616;Blackwell&#21487;&#36798;&#24615;&#65292;&#20351;PCFR&#33021;&#22815;&#19982;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#22312;&#20869;&#30340;&#20219;&#20309;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#32431;MCCFR&#65288;PMCCFR&#65289;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;PMCCFR&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#27604;MCCFR&#24555;&#19977;&#20493;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PMCCFR&#19981;&#36890;&#36807;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21160;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03004</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#22374;&#26497;&#23567;&#20540;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#35299;&#37322;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03004
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;MLP&#23618;&#20013;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#23454;&#35777;&#35266;&#23519;&#20026;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#23558;&#20854;&#24402;&#22240;&#20110;&#35757;&#32451;&#21160;&#21147;&#23398;&#65292;&#20294;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#20165;&#38480;&#20110;&#27973;&#23618;&#32593;&#32476;&#12289;&#23567;&#35757;&#32451;&#27493;&#38271;&#20197;&#21450;&#20462;&#25913;&#30340;&#35757;&#32451;&#65292;&#23613;&#31649;&#36825;&#31181;&#31232;&#30095;&#24615;&#24050;&#22312;&#36890;&#36807;vanilla&#21327;&#35758;&#36827;&#34892;&#22823;&#27493;&#39588;&#35757;&#32451;&#30340;&#28145;&#23618;&#27169;&#22411;&#20013;&#34987;&#21457;&#29616;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19977;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#31232;&#30095;&#24615;&#30340;&#27010;&#24565;&#20316;&#20026;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28304;&#22836;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#35813;&#35299;&#37322;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;&#36825;&#20010;&#29702;&#35770;&#36866;&#29992;&#20110;&#32463;&#36807;LayerNorm&#26631;&#20934;&#35757;&#32451;&#30340;&#32431;MLP&#65292;&#24182;&#19988;&#22914;&#26524;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32473;&#26435;&#37325;&#28155;&#21152;&#22122;&#22768;&#65292;&#36824;&#36866;&#29992;&#20110;Transformers&#25110;&#20854;&#20182;&#26550;&#26500;&#12290;&#20026;&#20102;&#28040;&#38500;&#20854;&#20182;&#26469;&#28304;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent empirical observation of activation sparsity in MLP layers offers an opportunity to drastically reduce computation costs for free. Despite several works attributing it to training dynamics, the theoretical explanation of activation sparsity's emergence is restricted to shallow networks, small training steps well as modified training, even though the sparsity has been found in deep models trained by vanilla protocols for large steps. To fill the three gaps, we propose the notion of gradient sparsity as the source of activation sparsity and a theoretical explanation based on it that explains gradient sparsity and then activation sparsity as necessary steps to adversarial robustness w.r.t. hidden features and parameters, which is approximately the flatness of minima for well-learned models. The theory applies to standardly trained LayerNorm-ed pure MLPs, and further to Transformers or other architectures if noises are added to weights during training. To eliminate other sources o
&lt;/p&gt;</description></item><item><title>PromptTTS 2&#26159;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#25551;&#36848;&#21644;&#29983;&#25104;&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#21270;&#32593;&#32476;&#25552;&#20379;&#22768;&#38899;&#30340;&#21487;&#21464;&#24615;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.02285</link><description>&lt;p&gt;
PromptTTS 2: &#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#25551;&#36848;&#21644;&#29983;&#25104;&#22768;&#38899;
&lt;/p&gt;
&lt;p&gt;
PromptTTS 2: Describing and Generating Voices with Text Prompt. (arXiv:2309.02285v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02285
&lt;/p&gt;
&lt;p&gt;
PromptTTS 2&#26159;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#25551;&#36848;&#21644;&#29983;&#25104;&#22768;&#38899;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#21270;&#32593;&#32476;&#25552;&#20379;&#22768;&#38899;&#30340;&#21487;&#21464;&#24615;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20256;&#36798;&#30340;&#20449;&#24687;&#27604;&#25991;&#23383;&#26356;&#20016;&#23500;&#65292;&#22240;&#20026;&#30456;&#21516;&#30340;&#35789;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#22768;&#38899;&#34920;&#36798;&#19981;&#21516;&#30340;&#20449;&#24687;&#12290;&#19982;&#20381;&#36182;&#35821;&#38899;&#25552;&#31034;&#65288;&#21442;&#32771;&#35821;&#38899;&#65289;&#26469;&#23454;&#29616;&#22768;&#38899;&#21487;&#21464;&#24615;&#30340;&#20256;&#32479;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#27861;&#30456;&#27604;&#65292;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#65288;&#25551;&#36848;&#65289;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#65292;&#22240;&#20026;&#35821;&#38899;&#25552;&#31034;&#21487;&#33021;&#38590;&#20197;&#25214;&#21040;&#25110;&#26681;&#26412;&#19981;&#23384;&#22312;&#12290;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;TTS&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#19968;&#23545;&#22810;&#38382;&#39064;&#65292;&#21363;&#25991;&#26412;&#25552;&#31034;&#26080;&#27861;&#25551;&#36848;&#22768;&#38899;&#21487;&#21464;&#24615;&#30340;&#25152;&#26377;&#32454;&#33410;&#65307;2&#65289;&#25991;&#26412;&#25552;&#31034;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#38656;&#35201;&#20379;&#24212;&#21830;&#21644;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#26469;&#32534;&#20889;&#35821;&#38899;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PromptTTS 2&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#21464;&#21270;&#32593;&#32476;&#25552;&#20379;&#25991;&#26412;&#25552;&#31034;&#26080;&#27861;&#25429;&#25417;&#30340;&#22768;&#38899;&#21487;&#21464;&#24615;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech conveys more information than text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech (TTS) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. TTS approaches based on the text prompt face two main challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompts for speech. In this work, we introduce PromptTTS 2 to address these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models (LLM) to compose high quality text prompts. Specifically, the variatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00848</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;-&#19968;&#31181;&#22522;&#20110;YOLOv8&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach. (arXiv:2309.00848v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20391;&#37325;&#20110;&#21033;&#29992;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25552;&#21319;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65288;DLA&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#20197;&#24212;&#23545;&#23391;&#21152;&#25289;&#22797;&#26434;&#25991;&#23383;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#39564;&#35777;&#38598;&#35780;&#20272;&#65292;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#30340;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#38598;&#25104;&#27169;&#22411;&#32467;&#21512;&#21518;&#22788;&#29702;&#24615;&#33021;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#25512;&#21160;&#23391;&#21152;&#25289;&#25991;&#26723;&#20998;&#26512;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;BaDLAD&#20316;&#20026;&#22522;&#30784;&#36164;&#28304;&#26377;&#21161;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#20026;&#23558;&#26032;&#31574;&#30053;&#32435;&#20837;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using the YOLOv8 model and innovative post-processing techniques. We tackle challenges unique to the complex Bengali script by employing data augmentation for model robustness. After meticulous validation set evaluation, we fine-tune our approach on the complete dataset, leading to a two-stage prediction strategy for accurate element segmentation. Our ensemble model, combined with post-processing, outperforms individual base architectures, addressing issues identified in the BaDLAD dataset. By leveraging this approach, we aim to advance Bengali document analysis, contributing to improved OCR and document comprehension and BaDLAD serves as a foundational resource for this endeavor, aiding future research in the field. Furthermore, our experiments provided key insights to incorporate new strategies into the established solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16198</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#39640;&#25928;&#21487;&#38752;&#30340;&#20449;&#24687;&#20256;&#25773;&#23545;&#25903;&#25345;&#20851;&#38190;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#28798;&#38590;&#21709;&#24212;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#20316;&#20026;&#23454;&#29616;&#26356;&#20026;&#20998;&#25955;&#12289;&#39640;&#25928;&#21644;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20449;&#24687;&#20256;&#25773;&#30340;&#20998;&#24067;&#24335;POMDP&#65288;Decentralized-POMDP&#65289;&#24418;&#24335;&#65292;&#20351;&#24471;&#27599;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#29420;&#31435;&#20915;&#23450;&#28040;&#24687;&#30340;&#36716;&#21457;&#12290;&#36825;&#26500;&#25104;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#65288;MPR&#65289;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65292;&#37319;&#29992;&#20855;&#26377;&#21160;&#24577;&#27880;&#24847;&#21147;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#26469;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;L-DGN&#21644;HL-DGN&#65292;&#23427;&#20204;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#25442;&#30340;&#20449;&#24687;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#20998;&#25955;&#26041;&#27861;&#19982;&#22522;&#20110;MPR&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12243</link><description>&lt;p&gt;
&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#21644;&#22686;&#24191;Lagrangian&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#19981;&#21516;&#30340;&#20914;&#31361;&#20248;&#21270;&#20934;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#30340;&#20027;&#20219;&#21153;&#65288;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65289;&#65292;&#20063;&#21487;&#20197;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#21644;&#27425;&#35201;&#20219;&#21153;&#65292;&#20363;&#22914;&#25439;&#22833;&#26368;&#23567;&#21270;&#19982;&#31232;&#30095;&#24615;&#12290;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#31616;&#21333;&#22320;&#21152;&#26435;&#20934;&#21017;&#65292;&#20294;&#22312;&#20984;&#35774;&#32622;&#20013;&#25165;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#31639;&#27861;&#65292;&#23545;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#25913;&#36827;&#30340;&#21152;&#26435;Chebyshev&#26631;&#37327;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26631;&#37327;&#21270;&#25216;&#26415;&#65292;&#31639;&#27861;&#21487;&#20197;&#35782;&#21035;&#21407;&#22987;&#38382;&#39064;&#30340;&#25152;&#26377;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#23558;&#20854;&#22797;&#26434;&#24615;&#38477;&#20302;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#22686;&#24191;Lagrangian&#26041;&#27861;&#26469;&#35299;&#20915;&#31616;&#21270;&#21518;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#24120;&#35265;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;Adam&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#22788;&#29702;&#32422;&#26463;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#32463;&#27982;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;LLM4TS&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08469</link><description>&lt;p&gt;
LLM4TS:&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#20004;&#38454;&#27573;&#24494;&#35843;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. (arXiv:2308.08469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08469
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;LLM4TS&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20511;&#37492;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#32479;&#19968;&#27169;&#22411;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#65292;&#25105;&#20204;&#35774;&#24819;&#21019;&#24314;&#19968;&#20010;&#31867;&#20284;&#30340;&#27169;&#22411;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#26500;&#24314;&#31283;&#20581;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;LLM4TS&#19987;&#27880;&#20110;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#20462;&#34917;&#19982;&#26102;&#38388;&#32534;&#30721;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20248;&#20808;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#24494;&#35843;&#36807;&#31243;&#65306;&#39318;&#20808;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#20351;LLMs&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#28982;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#19979;&#28216;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#19981;&#36827;&#34892;&#22823;&#37327;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#39044;&#35757;&#32451;LLMs&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing o
&lt;/p&gt;</description></item><item><title>BarlowRL&#36890;&#36807;&#23558;Barlow Twins&#21644;DER&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;Atari 100k&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#20449;&#24687;&#25193;&#25955;&#36991;&#20813;&#20102;&#32500;&#24230;&#25240;&#21472;&#65292;&#20351;&#24471;RL&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#22343;&#21248;&#20998;&#24067;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04263</link><description>&lt;p&gt;
BarlowRL: Barlow Twins&#29992;&#20110;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning. (arXiv:2308.04263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04263
&lt;/p&gt;
&lt;p&gt;
BarlowRL&#36890;&#36807;&#23558;Barlow Twins&#21644;DER&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;Atari 100k&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#20449;&#24687;&#25193;&#25955;&#36991;&#20813;&#20102;&#32500;&#24230;&#25240;&#21472;&#65292;&#20351;&#24471;RL&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#22343;&#21248;&#20998;&#24067;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BarlowRL&#65292;&#19968;&#31181;&#23558;Barlow Twins&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#19982;DER&#65288;Data-Efficient Rainbow&#65289;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;BarlowRL&#22312;Atari 100k&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;DER&#21644;&#23545;&#27604;&#31639;&#27861;CURL&#12290;BarlowRL&#36890;&#36807;&#30830;&#20445;&#20449;&#24687;&#25193;&#25955;&#21040;&#25972;&#20010;&#31354;&#38388;&#26469;&#36991;&#20813;&#32500;&#24230;&#25240;&#21472;&#12290;&#36825;&#26377;&#21161;&#20110;RL&#31639;&#27861;&#21033;&#29992;&#22343;&#21248;&#20998;&#24067;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26368;&#32456;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;Barlow Twins&#19982;DER&#30340;&#25972;&#21512;&#22686;&#24378;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;RL&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;BarlowRL&#23637;&#31034;&#20102;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#27493;&#35780;&#20272;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#21152;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#21534;&#21520;&#37327;&#12290;&#35813;&#31574;&#30053;&#32500;&#25252;&#19968;&#20010;&#20010;&#20307;&#38431;&#21015;&#65292;&#24182;&#22312;&#36866;&#24403;&#25968;&#37327;&#30340;&#20010;&#20307;&#34987;&#35780;&#20272;&#21518;&#31435;&#21363;&#36827;&#20837;&#19979;&#19968;&#20195;&#65292;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04102</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24322;&#27493;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Evolution of Deep Neural Network Architectures. (arXiv:2308.04102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#27493;&#35780;&#20272;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#21152;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#21534;&#21520;&#37327;&#12290;&#35813;&#31574;&#30053;&#32500;&#25252;&#19968;&#20010;&#20010;&#20307;&#38431;&#21015;&#65292;&#24182;&#22312;&#36866;&#24403;&#25968;&#37327;&#30340;&#20010;&#20307;&#34987;&#35780;&#20272;&#21518;&#31435;&#21363;&#36827;&#20837;&#19979;&#19968;&#20195;&#65292;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#36827;&#21270;&#31639;&#27861;(EAs)&#21033;&#29992;&#20505;&#36873;&#35299;&#30340;&#24182;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#35780;&#20272;&#26102;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#35768;&#22810;&#24037;&#20316;&#33410;&#28857;(&#21363;&#35745;&#31639;&#23458;&#25143;&#31471;)&#22823;&#37096;&#20998;&#26102;&#38388;&#37117;&#22788;&#20110;&#38386;&#32622;&#29366;&#24577;&#65292;&#31561;&#24453;&#19979;&#19968;&#20195;&#30340;&#21019;&#24314;&#12290;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;(ENAS)&#26159;&#19968;&#31867;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#30340;EA&#65292;&#29305;&#21035;&#23481;&#26131;&#21463;&#21040;&#36825;&#20010;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#27493;&#35780;&#20272;&#31574;&#30053;(AES)&#65292;&#28982;&#21518;&#23558;&#20854;&#36866;&#37197;&#21040;ENAS&#19978;&#12290;AES&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#22810;&#36798;$K$&#20010;&#20010;&#20307;&#30340;&#38431;&#21015;&#65292;&#36825;&#20123;&#20010;&#20307;&#24050;&#20934;&#22791;&#22909;&#34987;&#21457;&#36865;&#21040;&#24037;&#20316;&#22120;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#30001;&#24037;&#20316;&#22120;&#35780;&#20272;&#20102;$M&lt;&lt;K$&#20010;&#20010;&#20307;&#20043;&#21518;&#31435;&#21363;&#36827;&#20837;&#19979;&#19968;&#20195;&#12290;&#21512;&#36866;&#30340;$M$&#20540;&#26159;&#36890;&#36807;&#23454;&#39564;&#30830;&#23450;&#30340;&#65292;&#24179;&#34913;&#22810;&#26679;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#23637;&#31034;AES&#30340;&#26222;&#36866;&#24615;&#21644;&#33021;&#21147;&#65292;&#39318;&#20808;&#22312;11&#20301;&#22810;&#36335;&#22797;&#29992;&#22120;&#35774;&#35745;(&#19968;&#20010;&#21333;&#20010;&#31181;&#32676;&#21487;&#39564;&#35777;&#30340;&#21457;&#29616;&#20219;&#21153;)&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many evolutionary algorithms (EAs) take advantage of parallel evaluation of candidates. However, if evaluation times vary significantly, many worker nodes (i.e.,\ compute clients) are idle much of the time, waiting for the next generation to be created. Evolutionary neural architecture search (ENAS), a class of EAs that optimizes the architecture and hyperparameters of deep neural networks, is particularly vulnerable to this issue. This paper proposes a generic asynchronous evaluation strategy (AES) that is then adapted to work with ENAS. AES increases throughput by maintaining a queue of upto $K$ individuals ready to be sent to the workers for evaluation and proceeding to the next generation as soon as $M&lt;&lt;K$ individuals have been evaluated by the workers. A suitable value for $M$ is determined experimentally, balancing diversity and efficiency. To showcase the generality and power of AES, it was first evaluated in 11-bit multiplexer design (a single-population verifiable discovery ta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31532;&#20108;Nesterov&#36817;&#31471;&#26799;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#20960;&#20309;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22270;&#20687;&#37325;&#24314;&#26102;&#20943;&#36731;&#20266;&#24433;&#65292;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2308.03807</link><description>&lt;p&gt;
Nest-DGIL: Nesterov&#20248;&#21270;&#30340;&#28145;&#24230;&#20960;&#20309;&#22686;&#37327;&#23398;&#20064;&#29992;&#20110;CS&#22270;&#20687;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction. (arXiv:2308.03807v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31532;&#20108;Nesterov&#36817;&#31471;&#26799;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#20960;&#20309;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22270;&#20687;&#37325;&#24314;&#26102;&#20943;&#36731;&#20266;&#24433;&#65292;&#21516;&#26102;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;&#26799;&#24230;&#20248;&#21270;&#26159;&#35299;&#20915;&#22270;&#20687;&#21453;&#38382;&#39064;&#30340;&#24120;&#29992;&#31574;&#30053;&#20043;&#19968;&#65292;&#26131;&#20110;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#22270;&#20687;&#37325;&#26500;&#20013;&#36890;&#24120;&#20250;&#20135;&#29983;&#20005;&#37325;&#30340;&#20266;&#24433;&#12290;&#19968;&#31181;&#24120;&#29992;&#30340;&#25913;&#36827;&#26041;&#27861;&#26159;&#36890;&#36807;&#24494;&#35843;&#27491;&#21017;&#21270;&#21442;&#25968;&#26469;&#20943;&#36731;&#36825;&#20123;&#20266;&#24433;&#65292;&#20294;&#30001;&#20110;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#36275;&#22815;&#25110;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31532;&#20108;Nesterov&#36817;&#31471;&#26799;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#20960;&#20309;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;&#32593;&#32476;&#19981;&#20165;&#20855;&#26377;&#23545;&#39640;/&#20302;&#39057;&#22270;&#20687;&#29305;&#24449;&#30340;&#24378;&#22823;&#23398;&#20064;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#22312;&#21021;&#27493;&#32447;&#24615;&#37325;&#24314;&#20013;&#20174;&#29702;&#35770;&#19978;&#20445;&#35777;&#20960;&#20309;&#32441;&#29702;&#32454;&#33410;&#30340;&#37325;&#24314;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36991;&#20813;&#20013;&#38388;&#37325;&#24314;&#32467;&#26524;&#36229;&#20986;&#20960;&#20309;&#20998;&#35299;&#22495;&#30340;&#39118;&#38505;&#65292;&#24182;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#37325;&#24314;&#26694;&#26550;&#34987;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;:&#22270;&#20687;&#21021;&#27493;&#32447;&#24615;&#37325;&#24314;&#21644;&#28145;&#24230;&#22686;&#37327;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proximal gradient-based optimization is one of the most common strategies for solving image inverse problems as well as easy to implement. However, these techniques often generate heavy artifacts in image reconstruction. One of the most popular refinement methods is to fine-tune the regularization parameter to alleviate such artifacts, but it may not always be sufficient or applicable due to increased computational costs. In this work, we propose a deep geometric incremental learning framework based on second Nesterov proximal gradient optimization. The proposed end-to-end network not only has the powerful learning ability for high/low frequency image features,but also can theoretically guarantee that geometric texture details will be reconstructed from preliminary linear reconstruction.Furthermore, it can avoid the risk of intermediate reconstruction results falling outside the geometric decomposition domains and achieve fast convergence. Our reconstruction framework is decomposed int
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03486</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#21457;&#29616;&#23618;&#27425;&#21270;&#25104;&#23601;
&lt;/p&gt;
&lt;p&gt;
Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#25104;&#23601;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#21152;&#24378;&#20195;&#29702;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#39537;&#21160;&#21644;&#23618;&#27425;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#29615;&#22659;&#20013;&#21457;&#29616;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25104;&#23601;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#26234;&#33021;&#20307;&#20855;&#22791;&#24191;&#27867;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#27867;&#21270;&#21644;&#38271;&#26399;&#25512;&#29702;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#27169;&#22411;&#39537;&#21160;&#25110;&#23618;&#27425;&#21270;&#26041;&#27861;&#65292;&#35748;&#20026;&#26174;&#24335;&#30340;&#38271;&#26399;&#35268;&#21010;&#27169;&#22359;&#23545;&#20110;&#23398;&#20064;&#23618;&#27425;&#21270;&#25104;&#23601;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#25110;&#22823;&#22411;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36817;&#26399;&#23454;&#26045;&#23454;&#36341;&#20013;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#31639;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;PPO&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#39044;&#27979;&#19979;&#19968;&#20010;&#35201;&#35299;&#38145;&#30340;&#25104;&#23601;&#65292;&#23613;&#31649;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#36739;&#20302;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#25104;&#23601;&#33976;&#39311;&#65292;&#21487;&#20197;&#21152;&#24378;PPO&#26234;&#33021;&#20307;&#23545;&#19979;&#19968;&#20010;&#35299;&#38145;&#25104;&#23601;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering achievements with a hierarchical structure on procedurally generated environments poses a significant challenge. This requires agents to possess a broad range of abilities, including generalization and long-term reasoning. Many prior methods are built upon model-based or hierarchical approaches, with the belief that an explicit module for long-term planning would be beneficial for learning hierarchical achievements. However, these methods require an excessive amount of environment interactions or large model sizes, limiting their practicality. In this work, we identify that proximal policy optimization (PPO), a simple and versatile model-free algorithm, outperforms the prior methods with recent implementation practices. Moreover, we find that the PPO agent can predict the next achievement to be unlocked to some extent, though with low confidence. Based on this observation, we propose a novel contrastive learning method, called achievement distillation, that strengthens the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03135</link><description>&lt;p&gt;
&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#24615;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#35745;&#31639;&#35201;&#27714;&#20351;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#21644;&#26102;&#38388;&#25935;&#24863;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27169;&#22411;&#21387;&#32553;&#26159;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#24555;&#30340;&#27169;&#22411;&#20197;&#20445;&#25345;&#36739;&#22823;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#23567;&#22411;&#25110;&#20013;&#22411;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21487;&#27867;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#38382;&#39064;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;OOD&#21487;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#26356;&#22909;&#22320;&#27169;&#20223;&#25945;&#24072;&#30340;&#35270;&#35273;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#26041;&#38754;&#35880;&#24910;&#22320;&#20419;&#36827;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20016;&#23500;&#23398;&#29983;&#27169;&#22411;&#30340;&#33258;&#20030;&#23398;&#20064;&#21644;&#25968;&#25454;&#25193;&#20805;&#26469;&#25552;&#39640;OOD&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
&lt;/p&gt;</description></item><item><title>&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02484</link><description>&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Elastic Decision Transformer. (arXiv:2307.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02484
&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#22635;&#34917;&#20102;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#22312;&#36825;&#19968;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24377;&#24615;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;EDT&#65289;&#65292;&#23427;&#26159;&#29616;&#26377;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;DT&#65289;&#21450;&#20854;&#21464;&#20307;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;DT&#22768;&#31216;&#33021;&#22815;&#29983;&#25104;&#26368;&#20339;&#36712;&#36857;&#65292;&#20294;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#23427;&#22312;&#36712;&#36857;&#25340;&#25509;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36712;&#36857;&#25340;&#25509;&#26159;&#25351;&#20174;&#19968;&#32452;&#27425;&#20248;&#36712;&#36857;&#20013;&#29983;&#25104;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;&#25552;&#20986;&#30340;EDT&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#38388;&#36827;&#34892;&#21160;&#20316;&#25512;&#26029;&#26102;&#35843;&#25972;DT&#20013;&#32500;&#25252;&#30340;&#21382;&#21490;&#38271;&#24230;&#26469;&#23454;&#29616;&#36712;&#36857;&#25340;&#25509;&#65292;&#20174;&#32780;&#20351;&#33258;&#24049;&#19982;&#20247;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#36712;&#36857;&#26159;&#26368;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#38271;&#30340;&#21382;&#21490;&#65292;&#24403;&#24403;&#21069;&#36712;&#36857;&#26159;&#27425;&#20248;&#30340;&#26102;&#20505;&#65292;EDT&#36890;&#36807;&#20445;&#25345;&#36739;&#30701;&#30340;&#21382;&#21490;&#26469;&#20248;&#21270;&#36712;&#36857;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#26356;&#20248;&#30340;&#36712;&#36857;&#36827;&#34892;&#8220;&#25340;&#25509;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EDT&#33021;&#22815;&#22635;&#34917;&#22522;&#20110;DT&#21644;&#22522;&#20110;Q-Learning&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;EDT&#22312;&#22810;&#20219;&#21153;&#24773;&#20917;&#19979;&#32988;&#36807;&#22522;&#20110;Q-Learning&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;mNARX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#27969;&#24418;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36817;&#20284;&#22797;&#26434;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#21709;&#24212;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23558;&#25972;&#20010;&#38382;&#39064;&#20998;&#35299;&#25104;&#36739;&#23567;&#30340;&#23376;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#36866;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.16335</link><description>&lt;p&gt;
&#20351;&#29992;&#27969;&#24418;&#19978;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;mNARX&#65289;&#27169;&#25311;&#22797;&#26434;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Emulating the dynamics of complex systems using autoregressive models on manifolds (mNARX). (arXiv:2306.16335v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;mNARX&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#27969;&#24418;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36817;&#20284;&#22797;&#26434;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#21709;&#24212;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23558;&#25972;&#20010;&#38382;&#39064;&#20998;&#35299;&#25104;&#36739;&#23567;&#30340;&#23376;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#36866;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36817;&#20284;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#23545;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22806;&#37096;&#21050;&#28608;&#30340;&#21709;&#24212;&#65292;&#24182;&#19988;&#21487;&#20197;&#24310;&#38271;&#26102;&#38388;&#27573;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#8220;&#24102;&#22806;&#37096;&#36755;&#20837;&#30340;&#27969;&#24418;&#38750;&#32447;&#24615;&#33258;&#22238;&#24402;&#24314;&#27169;&#8221;&#65288;mNARX&#65289;&#65292;&#23427;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#38382;&#39064;&#29305;&#23450;&#30340;&#22806;&#37096;&#36755;&#20837;&#27969;&#24418;&#65292;&#20197;&#20415;&#26500;&#36896;&#33258;&#22238;&#24402;&#26367;&#20195;&#27169;&#22411;&#12290;&#36825;&#20010;&#27969;&#24418;&#26159;mNARX&#30340;&#26680;&#24515;&#65292;&#36890;&#36807;&#32467;&#21512;&#31995;&#32479;&#30340;&#29289;&#29702;&#24615;&#36136;&#20197;&#21450;&#20808;&#21069;&#30340;&#19987;&#23478;&#21644;&#39046;&#22495;&#30693;&#35782;&#26469;&#36880;&#27493;&#26500;&#24314;&#12290;&#22240;&#20026;mNARX&#23558;&#25972;&#20010;&#38382;&#39064;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#26356;&#23567;&#30340;&#23376;&#38382;&#39064;&#65292;&#27599;&#20010;&#23376;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#27604;&#21407;&#26469;&#30340;&#20302;&#65292;&#25152;&#20197;&#23427;&#22312;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#26080;&#35770;&#26159;&#26368;&#32456;&#26367;&#20195;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#36824;&#26159;&#35780;&#20272;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;mNARX&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#38750;&#24120;&#22865;&#21512;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel surrogate modelling approach to efficiently and accurately approximate the response of complex dynamical systems driven by time-varying exogenous excitations over extended time periods. Our approach, that we name \emph{manifold nonlinear autoregressive modelling with exogenous input} (mNARX), involves constructing a problem-specific exogenous input manifold that is optimal for constructing autoregressive surrogates. The manifold, which forms the core of mNARX, is constructed incrementally by incorporating the physics of the system, as well as prior expert- and domainknowledge. Because mNARX decomposes the full problem into a series of smaller sub-problems, each with a lower complexity than the original, it scales well with the complexity of the problem, both in terms of training and evaluation costs of the final surrogate. Furthermore, mNARX synergizes well with traditional dimensionality reduction techniques, making it highly suitable for modelling 
&lt;/p&gt;</description></item><item><title>&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#22312;&#23454;&#29616;&#32479;&#35745;&#20445;&#35777;&#30028;&#38480;&#26102;&#23384;&#22312;&#38480;&#21046;&#21644;&#20445;&#23432;&#24615;&#38382;&#39064;&#65292;&#20294;&#24179;&#28369;&#30340;$f$-&#25955;&#24230;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#21487;&#22312;&#25351;&#25968;&#34928;&#20943;&#29575;&#26041;&#38754;&#23454;&#29616;&#26368;&#32039;&#23494;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.14041</link><description>&lt;p&gt;
&#24179;&#28369;&#30340;$f$-&#25955;&#24230;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65306;&#25351;&#25968;&#29575;&#25928;&#29575;&#21644;&#19981;&#24102;&#22797;&#26434;&#24615;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smoothed $f$-Divergence Distributionally Robust Optimization: Exponential Rate Efficiency and Complexity-Free Calibration. (arXiv:2306.14041v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14041
&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#22312;&#23454;&#29616;&#32479;&#35745;&#20445;&#35777;&#30028;&#38480;&#26102;&#23384;&#22312;&#38480;&#21046;&#21644;&#20445;&#23432;&#24615;&#38382;&#39064;&#65292;&#20294;&#24179;&#28369;&#30340;$f$-&#25955;&#24230;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#21487;&#22312;&#25351;&#25968;&#34928;&#20943;&#29575;&#26041;&#38754;&#23454;&#29616;&#26368;&#32039;&#23494;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#20013;&#65292;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#24050;&#30693;&#23384;&#22312;&#19968;&#20010;&#25152;&#35859;&#30340;&#20248;&#21270;&#32773;&#35781;&#21650;&#65292;&#20250;&#23548;&#33268;&#22312;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#24615;&#33021;&#26102;&#20135;&#29983;&#20048;&#35266;&#20559;&#24046;&#12290;&#21487;&#20197;&#36890;&#36807;&#22312;&#20272;&#35745;&#30340;&#30446;&#26631;&#20540;&#20013;&#22686;&#21152;&#8220;&#20445;&#35777;&#31354;&#38388;&#8221;&#25110;&#36890;&#36807;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#24555;&#36895;&#22686;&#38271;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#20026;&#33719;&#24471;&#30340;&#30446;&#26631;&#20215;&#20540;&#25552;&#20379;&#20102;&#20445;&#25252;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#22312;&#25152;&#26377;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#23545;&#30495;&#23454;&#35299;&#20915;&#26041;&#26696;&#24615;&#33021;&#30340;&#32479;&#35745;&#20445;&#35777;&#30028;&#38480;&#35201;&#20040;&#38656;&#35201;&#23545;&#30446;&#26631;&#20989;&#25968;&#22797;&#26434;&#24615;&#26377;&#38480;&#21046;&#24615;&#26465;&#20214;&#21644;&#30693;&#35782;&#65292;&#35201;&#20040;&#20250;&#34920;&#29616;&#20986;&#21462;&#20915;&#20110;&#20998;&#24067;&#32500;&#24230;&#30340;&#36807;&#20110;&#20445;&#23432;&#30340;&#36895;&#29575;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#65292;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;DRO&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20248;&#21183;&#65306;&#23545;&#20110;&#19968;&#22823;&#31867;&#30446;&#26631;&#20989;&#25968;&#65292;&#23427;&#33719;&#24471;&#20102;&#23545;&#30495;&#23454;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#24615;&#33021;&#30340;&#32479;&#35745;&#30028;&#38480;&#65292;&#36825;&#22312;&#25351;&#25968;&#34928;&#20943;&#29575;&#26041;&#38754;&#26159;&#21487;&#33021;&#30340;&#65292;&#23601;&#20854;&#32039;&#32553;&#31243;&#24230;&#32780;&#35328;&#65292;&#35201;&#32039;&#23494;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-driven optimization, sample average approximation is known to suffer from the so-called optimizer's curse that causes optimistic bias in evaluating the solution performance. This can be tackled by adding a "margin" to the estimated objective value, or via distributionally robust optimization (DRO), a fast-growing approach based on worst-case analysis, which gives a protective bound on the attained objective value. However, in all these existing approaches, a statistically guaranteed bound on the true solution performance either requires restrictive conditions and knowledge on the objective function complexity, or otherwise exhibits an over-conservative rate that depends on the distribution dimension. We argue that a special type of DRO offers strong theoretical advantages in regard to these challenges: It attains a statistical bound on the true solution performance that is the tightest possible in terms of exponential decay rate, for a wide class of objective functions that not
&lt;/p&gt;</description></item><item><title>GIO&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#31616;&#21333;&#25918;&#26494;&#21644;&#39640;&#25928;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.11670</link><description>&lt;p&gt;
GIO&#65306;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#36873;&#25321;&#30340;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11670
&lt;/p&gt;
&lt;p&gt;
GIO&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#30340;&#31616;&#21333;&#25918;&#26494;&#21644;&#39640;&#25928;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#26377;&#21033;&#20110;&#22312;&#21487;&#29992;&#35757;&#32451;&#26679;&#26412;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#20026;&#36825;&#20123;&#26679;&#26412;&#20855;&#26377;&#19981;&#21516;&#30340;&#36136;&#37327;&#65292;&#25110;&#32773;&#24076;&#26395;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#20449;&#24687;&#20248;&#21270;&#65288;GIO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#65292;&#23427;&#21482;&#38656;&#35201;&#19968;&#23567;&#32452;&#65288;&#26410;&#26631;&#35760;&#30340;&#65289;&#26679;&#26412;&#26469;&#20195;&#34920;&#30446;&#26631;&#20998;&#24067;&#12290;GIO&#20174;&#19968;&#20010;&#23454;&#36341;&#20013;&#38590;&#20197;&#22788;&#29702;&#30340;&#33258;&#28982;&#30340;&#20449;&#24687;&#29702;&#35770;&#30446;&#26631;&#24320;&#22987;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20986;&#36890;&#36807;&#23545;&#30446;&#26631;&#36827;&#34892;&#31616;&#21333;&#30340;&#25918;&#26494;&#21644;&#39640;&#25928;&#30340;&#23454;&#29616;&#65292;&#23427;&#21487;&#20197;&#34987;&#39640;&#24230;&#25193;&#23637;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25340;&#20889;&#32416;&#27491;&#21644;&#22270;&#20687;&#35782;&#21035;&#31561;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GIO&#22312;&#38750;&#24120;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20135;&#29983;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;GIO&#26412;&#36523;&#30340;&#19981;&#21516;&#34920;&#31034;&#27169;&#22411;&#21644;&#36229;&#21442;&#25968;&#26159;&#31283;&#20581;&#30340;&#12290;GIO&#26159;&#20219;&#21153;&#21644;&#39046;&#22495;&#26080;&#20851;&#30340;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#26032;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation. In experiments with machine translation, spelling correction, and image recognition, we show that GIO delivers outstanding results with very small train sets. These findings are robust to different representation models and hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be applied out-of-the-box to ne
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#26041;&#27861;&#65292;PtychoPINN&#65292;&#36890;&#36807;&#23558;&#34893;&#23556;&#27491;&#28436;&#22270;&#19982;&#37325;&#21472;&#27979;&#37327;&#30340;&#23454;&#31354;&#38388;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.11014</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24555;&#36895;&#39640;&#20998;&#36776;&#29575;&#25195;&#25551;&#30456;&#24178;&#34893;&#23556;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Physics Constrained Unsupervised Deep Learning for Rapid, High Resolution Scanning Coherent Diffraction Reconstruction. (arXiv:2306.11014v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#26041;&#27861;&#65292;PtychoPINN&#65292;&#36890;&#36807;&#23558;&#34893;&#23556;&#27491;&#28436;&#22270;&#19982;&#37325;&#21472;&#27979;&#37327;&#30340;&#23454;&#31354;&#38388;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35268;&#36991;&#20809;&#23398;&#30340;&#20998;&#36776;&#29575;&#38480;&#21046;&#65292;&#30456;&#24178;&#34893;&#23556;&#25104;&#20687;&#65288;CDI&#65289;&#21644;&#20840;&#24687;&#25340;&#34917;&#25216;&#26415;&#27491;&#36880;&#28176;&#24212;&#29992;&#20110;&#20174;X&#23556;&#32447;&#25104;&#20687;&#21040;&#22825;&#25991;&#23398;&#31561;&#31185;&#23398;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#32791;&#26102;&#30340;&#36845;&#20195;&#30456;&#20301;&#24674;&#22797;&#38480;&#21046;&#20102;&#23454;&#26102;&#25104;&#20687;&#30340;&#36895;&#24230;&#12290;&#34429;&#28982;&#26377;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#37325;&#24314;&#36895;&#24230;&#65292;&#20294;&#29306;&#29298;&#20102;&#22270;&#20687;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#22823;&#37327;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#22312;&#23454;&#39564;&#19978;&#24456;&#32321;&#29712;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#26041;&#27861;&#65292;PtychoPINN&#65292;&#23427;&#22312;&#20445;&#25345;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#30340;100&#21040;1000&#20493;&#36895;&#24230;&#25552;&#21319;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#34893;&#23556;&#27491;&#28436;&#22270;&#19982;&#37325;&#21472;&#27979;&#37327;&#30340;&#23454;&#31354;&#38388;&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#25913;&#21892;&#20102;&#37325;&#24314;&#36136;&#37327;&#12290;&#29305;&#21035;&#26159;&#65292;PtychoPINN&#22312;&#27867;&#21270;&#33021;&#21147;&#12289;&#20934;&#30830;&#24615;&#65288;&#20856;&#22411;&#30340;&#20449;&#22122;&#27604;&#22686;&#21152;&#20102;10 dB&#65289;&#21644;&#32447;&#24615;&#20998;&#36776;&#29575;&#65288;2-6&#20493;&#25552;&#21319;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#31181;&#24615;&#33021;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#32467;&#21512;&#26159;&#19968;&#31181;&#21019;&#26032;&#24615;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
By circumventing the resolution limitations of optics, coherent diffractive imaging (CDI) and ptychography are making their way into scientific fields ranging from X-ray imaging to astronomy. Yet, the need for time consuming iterative phase recovery hampers real-time imaging. While supervised deep learning strategies have increased reconstruction speed, they sacrifice image quality. Furthermore, these methods' demand for extensive labeled training data is experimentally burdensome. Here, we propose an unsupervised physics-informed neural network reconstruction method, PtychoPINN, that retains the factor of 100-to-1000 speedup of deep learning-based reconstruction while improving reconstruction quality by combining the diffraction forward map with real-space constraints from overlapping measurements. In particular, PtychoPINN significantly advances generalizability, accuracy (with a typical 10 dB PSNR increase), and linear resolution (2- to 6-fold gain). This blend of performance and sp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;POMDP&#38382;&#39064;&#30340;&#29702;&#35770;&#22256;&#38590;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24314;&#31435;&#19979;&#30028;&#24471;&#20986;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#20855;&#26377;&#23436;&#25972;&#30340;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65292;&#21542;&#21017;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#24471;&#21040;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#28982;&#32780;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#19979;&#30340;&#21487;&#35745;&#31639;POMDP&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35777;&#26126;&#20854;&#25509;&#36817;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08762</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;POMDP&#30340;&#29702;&#35770;&#38590;&#24230;&#21644;&#21487;&#35745;&#31639;&#24615;
&lt;/p&gt;
&lt;p&gt;
Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information. (arXiv:2306.08762v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#30340;POMDP&#38382;&#39064;&#30340;&#29702;&#35770;&#22256;&#38590;&#24615;&#21644;&#21487;&#35745;&#31639;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24314;&#31435;&#19979;&#30028;&#24471;&#20986;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#20855;&#26377;&#23436;&#25972;&#30340;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65292;&#21542;&#21017;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25165;&#33021;&#24471;&#21040;POMDP&#30340;&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#28982;&#32780;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#20855;&#26377;&#37096;&#20998;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#19979;&#30340;&#21487;&#35745;&#31639;POMDP&#31867;&#21035;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35777;&#26126;&#20854;&#25509;&#36817;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25429;&#25417;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#19968;&#33324;&#30340;POMDP&#20013;&#23398;&#20064;&#21487;&#33021;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#32570;&#20047;&#28508;&#22312;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#26377;&#22810;&#23569;&#22312;&#32447;&#29366;&#24577;&#20449;&#24687;&#65288;OSI&#65289;&#36275;&#20197;&#23454;&#29616;&#21487;&#35745;&#31639;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#24778;&#20154;&#30340;&#38590;&#24230;&#32467;&#26524;&#65306;&#38500;&#38750;&#25105;&#20204;&#20855;&#26377;&#23436;&#25972;&#30340;OSI&#65292;&#21542;&#21017;&#25105;&#20204;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#37319;&#26679;&#22797;&#26434;&#24230;&#25165;&#33021;&#33719;&#24471;POMDP&#30340;$\epsilon$-&#26368;&#20248;&#31574;&#30053;&#35299;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21463;&#21040;&#25105;&#20204;&#19979;&#30028;&#35774;&#35745;&#30340;&#20851;&#38190;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#21482;&#26377;&#37096;&#20998;OSI&#65292;&#20063;&#23384;&#22312;&#37325;&#35201;&#30340;&#21487;&#35745;&#31639;&#30340;POMDP&#31867;&#21035;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#20855;&#26377;&#37096;&#20998;OSI&#30340;&#20004;&#20010;&#26032;&#39062;&#30340;POMDP&#31867;&#21035;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#26032;&#30340;&#36951;&#25022;&#19978;&#19979;&#30028;&#35777;&#26126;&#20102;&#26032;&#30340;&#31639;&#27861;&#26159;&#25509;&#36817;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially observable Markov decision processes (POMDPs) have been widely applied to capture many real-world applications. However, existing theoretical results have shown that learning in general POMDPs could be intractable, where the main challenge lies in the lack of latent state information. A key fundamental question here is how much online state information (OSI) is sufficient to achieve tractability. In this paper, we establish a lower bound that reveals a surprising hardness result: unless we have full OSI, we need an exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy solution for POMDPs. Nonetheless, inspired by the key insights in our lower bound design, we find that there exist important tractable classes of POMDPs even with only partial OSI. In particular, for two novel classes of POMDPs with partial OSI, we provide new algorithms that are proved to be near-optimal by establishing new regret upper and lower bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06599</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;(Variational Imbalanced Regression)
&lt;/p&gt;
&lt;p&gt;
Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#26102;&#65292;&#29616;&#26377;&#30340;&#22238;&#24402;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#65292;&#23427;&#19981;&#20165;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#33258;&#28982;&#22320;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19982;&#20856;&#22411;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;I.I.D.&#34920;&#31034;&#65288;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#19981;&#30452;&#25509;&#21463;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;VIR&#20511;&#29992;&#20855;&#26377;&#31867;&#20284;&#22238;&#24402;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#35745;&#31639;&#28508;&#22312;&#34920;&#31034;&#30340;&#21464;&#20998;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#20135;&#29983;&#28857;&#20272;&#35745;&#30340;&#30830;&#23450;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292; VIR&#39044;&#27979;&#25972;&#20010;&#27491;&#24577;&#21453;-&#20285;&#29595;&#20998;&#24067;&#24182;&#35843;&#33410;&#30456;&#20851;&#32852;&#30340;&#20849;&#36717;&#20998;&#24067;&#65292;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#26045;&#21152;&#27010;&#29575;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#22810;&#23618;&#29983;&#25104;&#22120;&#27169;&#22411;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#22810;&#23618;&#29983;&#25104;&#22120;&#20026;&#39592;&#24178;&#30340;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;EBM&#20808;&#39564;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20998;&#23618;&#34920;&#31034;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20462;&#22797;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06323</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#23618;&#29983;&#25104;&#22120;&#30340;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;EBM&#20808;&#39564;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Joint Latent Space EBM Prior Model for Multi-layer Generator. (arXiv:2306.06323v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#22810;&#23618;&#29983;&#25104;&#22120;&#27169;&#22411;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#22810;&#23618;&#29983;&#25104;&#22120;&#20026;&#39592;&#24178;&#30340;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;EBM&#20808;&#39564;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20998;&#23618;&#34920;&#31034;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20462;&#22797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#22810;&#23618;&#29983;&#25104;&#22120;&#27169;&#22411;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#22810;&#23618;&#29983;&#25104;&#22120;&#27169;&#22411;&#22312;&#29983;&#25104;&#22120;&#20043;&#19978;&#26500;&#24314;&#22810;&#23618;&#28508;&#22312;&#21464;&#37327;&#20316;&#20026;&#20808;&#39564;&#27169;&#22411;&#65292;&#26377;&#21033;&#20110;&#23398;&#20064;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20998;&#23618;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20808;&#39564;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20551;&#35774;&#38750;&#20449;&#24687;&#65288;&#26465;&#20214;&#65289;&#39640;&#26031;&#20998;&#24067;&#26469;&#19987;&#27880;&#20110;&#24314;&#27169;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#23618;&#38388;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#23398;&#20064;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#20808;&#39564;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#22810;&#23618;&#29983;&#25104;&#22120;&#20026;&#39592;&#24178;&#30340;&#33021;&#37327;&#22522;&#27169;&#22411;&#65288;EBM&#65289;&#30340;&#20808;&#39564;&#27169;&#22411;&#12290;&#36825;&#31181;&#32852;&#21512;&#28508;&#22312;&#31354;&#38388;EBM &#20808;&#39564;&#27169;&#22411;&#36890;&#36807;&#23618;&#38388;&#33021;&#37327;&#39033;&#25429;&#33719;&#27599;&#23618;&#20869;&#30340;&#20869;&#37096;&#20851;&#31995;&#65292;&#24182;&#23545;&#19981;&#21516;&#23618;&#30340;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#32852;&#21512;&#20462;&#27491;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#24320;&#21457;&#20102;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#26041;&#26696;&#65292;&#20854;&#20013;&#21516;&#26102;&#23398;&#20064;&#29983;&#25104;&#22120;&#21644;EBM&#20808;&#39564;&#27169;&#22411;&#12290;&#22312;&#29983;&#25104;&#22270;&#20687;&#24314;&#27169;&#21644;&#20462;&#22797;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20960;&#20010;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#26356;&#20855;&#34920;&#29616;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20808;&#39564;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20462;&#22797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the fundamental problem of learning multi-layer generator models. The multi-layer generator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learning complex data distribution and hierarchical representations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distributions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we propose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer generator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estimation (MLE), which involves
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#30495;&#39537;&#21160;&#30340;&#36870;&#25512;&#29702;&#26041;&#27861;&#26469;&#23398;&#20064;&#26641;&#26525;&#21160;&#24577;&#65292;&#24182;&#21487;&#20197;&#25805;&#32437;&#21487;&#21464;&#24418;&#30340;&#26893;&#34987;&#65292;&#20197;&#35299;&#20915;&#23494;&#38598;&#26893;&#34987;&#20013;&#23481;&#26131;&#36974;&#25377;&#30340;&#20219;&#21153;&#65292;&#31639;&#27861;&#32467;&#21512;&#20102;&#29983;&#29289;&#23398;&#19978;&#30340;&#20551;&#35774;&#21644;&#20256;&#32479;&#21442;&#25968;&#25512;&#29702;&#26041;&#27861;&#30340;&#26377;&#38480;&#24046;&#20998;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.03410</link><description>&lt;p&gt;
&#23398;&#20064;&#27169;&#25311;&#26641;&#26525;&#21160;&#21147;&#23398;&#20197;&#36827;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning to Simulate Tree-Branch Dynamics for Manipulation. (arXiv:2306.03410v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#30495;&#39537;&#21160;&#30340;&#36870;&#25512;&#29702;&#26041;&#27861;&#26469;&#23398;&#20064;&#26641;&#26525;&#21160;&#24577;&#65292;&#24182;&#21487;&#20197;&#25805;&#32437;&#21487;&#21464;&#24418;&#30340;&#26893;&#34987;&#65292;&#20197;&#35299;&#20915;&#23494;&#38598;&#26893;&#34987;&#20013;&#23481;&#26131;&#36974;&#25377;&#30340;&#20219;&#21153;&#65292;&#31639;&#27861;&#32467;&#21512;&#20102;&#29983;&#29289;&#23398;&#19978;&#30340;&#20551;&#35774;&#21644;&#20256;&#32479;&#21442;&#25968;&#25512;&#29702;&#26041;&#27861;&#30340;&#26377;&#38480;&#24046;&#20998;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20223;&#30495;&#39537;&#21160;&#30340;&#36870;&#25512;&#29702;&#26041;&#27861;&#26469;&#27169;&#25311;&#25805;&#32437;&#19979;&#30340;&#26641;&#26525;&#20851;&#33410;&#21160;&#21147;&#23398;&#12290;&#23398;&#20064;&#26525;&#24178;&#21160;&#24577;&#24182;&#33719;&#24471;&#25805;&#32437;&#21487;&#21464;&#24418;&#26893;&#34987;&#30340;&#33021;&#21147;&#21487;&#24110;&#21161;&#22788;&#29702;&#23481;&#26131;&#36974;&#25377;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#23494;&#38598;&#26641;&#21494;&#20013;&#37319;&#25688;&#27700;&#26524;&#12289;&#31227;&#21160;&#24748;&#22402;&#30340;&#34276;&#34067;&#21644;&#26641;&#26525;&#65292;&#20197;&#20415;&#22312;&#23494;&#38598;&#26893;&#34987;&#20013;&#23548;&#33322;&#12290;&#26893;&#29289;&#30340;&#21487;&#21464;&#24418;&#20960;&#20309;&#24418;&#29366;&#36890;&#36807;&#22312;&#24182;&#34892;&#12289;&#19981;&#21487;&#24494;&#27169;&#25311;&#22120;&#19978;&#25191;&#34892;&#30340;&#31895;&#30053;&#24377;&#31783;&#25277;&#35937;&#26469;&#23454;&#29616;&#12290;&#30001;&#27169;&#25311;&#22120;&#23450;&#20041;&#30340;&#38544;&#24335;&#32479;&#35745;&#27169;&#22411;&#12289;&#36890;&#36807;&#20027;&#21160;&#25506;&#27979;&#30340;&#22320;&#38754;&#30495;&#23454;&#24773;&#20917;&#33719;&#24471;&#30340;&#21442;&#32771;&#36712;&#36857;&#21644;&#36125;&#21494;&#26031;&#24418;&#24335;&#20027;&#20041;&#19968;&#36215;&#25351;&#23548;&#24377;&#31783;&#21442;&#25968;&#21518;&#39564;&#23494;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26080;&#21442;&#25968;&#25512;&#29702;&#31639;&#27861;&#22522;&#20110;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65292;&#24182;&#23558;&#29983;&#29289;&#23398;&#19978;&#30340;&#20551;&#35774;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#23398;&#20064;&#32852;&#21512;&#20808;&#39564;&#21512;&#24182;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#23427;&#21033;&#29992;&#20102;&#26377;&#38480;&#24046;&#20998;&#26041;&#26696;&#26469;&#23545;&#20989;&#25968;&#26799;&#24230;&#36827;&#34892;&#20272;&#35745;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#21442;&#25968;&#25512;&#29702;&#26041;&#27861;&#20013;&#30340;&#26799;&#24230;&#35745;&#31639;&#22256;&#38590;&#21644;&#32500;&#24230;&#28798;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to use a simulation driven inverse inference approach to model the joint dynamics of tree branches under manipulation. Learning branch dynamics and gaining the ability to manipulate deformable vegetation can help with occlusion-prone tasks, such as fruit picking in dense foliage, as well as moving overhanging vines and branches for navigation in dense vegetation. The underlying deformable tree geometry is encapsulated as coarse spring abstractions executed on parallel, non-differentiable simulators. The implicit statistical model defined by the simulator, reference trajectories obtained by actively probing the ground truth, and the Bayesian formalism, together guide the spring parameter posterior density estimation. Our non-parametric inference algorithm, based on Stein Variational Gradient Descent, incorporates biologically motivated assumptions into the inference process as neural network driven learnt joint priors; moreover, it leverages the finite difference scheme for g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#27880;&#24847;&#21147;&#23618;&#21487;&#20197;&#35760;&#24518;&#937;(Hn)&#20010;&#31034;&#20363;&#24207;&#21015;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#35760;&#24518;&#23481;&#37327;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.02010</link><description>&lt;p&gt;
Transformers&#20013;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Memorization Capacity of Multi-Head Attention in Transformers. (arXiv:2306.02010v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#21457;&#29616;&#22312;&#29305;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#27880;&#24847;&#21147;&#23618;&#21487;&#20197;&#35760;&#24518;&#937;(Hn)&#20010;&#31034;&#20363;&#24207;&#21015;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;transformers&#30340;&#35760;&#24518;&#23481;&#37327;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#24050;&#25104;&#20026;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#30340;&#39318;&#36873;&#26550;&#26500;&#65292;&#20294;&#20854;&#29702;&#35770;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#35760;&#24518;&#23481;&#37327;&#65292;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#33021;&#22815;&#35760;&#24518;&#22810;&#23569;&#20010;&#31034;&#20363;&#24207;&#21015;&#65292;&#20316;&#20026;&#22836;&#25968;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#20989;&#25968;&#12290;&#22312;&#23545;&#35270;&#35273;transformers&#30340;&#23454;&#39564;&#32467;&#26524;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20851;&#20110;&#36755;&#20837;&#25968;&#25454;&#32447;&#24615;&#29420;&#31435;&#24615;&#30340;&#26032;&#20551;&#35774;&#65292;&#19981;&#21516;&#20110;&#36890;&#24120;&#20351;&#29992;&#30340;&#19968;&#33324;&#20301;&#32622;&#20551;&#35774;&#12290;&#22312;&#36825;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;H&#20010;&#22836;&#65292;&#32500;&#24230;d&#65292;&#19978;&#19979;&#25991;&#22823;&#23567;n &lt; d&#30340;&#27880;&#24847;&#21147;&#23618;&#65292;&#20855;&#26377;&#920;(Hd^2)&#20010;&#21442;&#25968;&#65292;&#21487;&#20197;&#35760;&#20303;&#937;(Hn)&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#22836;&#22914;&#20309;&#22788;&#29702;&#19981;&#21516;&#30340;&#31034;&#20363;&#24207;&#21015;&#65292;&#21463;&#21040;softmax&#36816;&#31639;&#31526;&#30340;&#39281;&#21644;&#29305;&#24615;&#30340;&#24110;&#21161;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have become the go-to architecture for language and vision tasks, yet their theoretical properties, especially memorization capacity, remain elusive. This paper investigates the memorization abilities of multi-head attention mechanisms, examining how many example sequences they can memorize, as a function of the number of heads and sequence length. Motivated by experimental findings on vision transformers, we introduce novel assumptions about the linear independence of input data, distinct from the commonly used general-position assumption. Under these assumptions, we demonstrate that an attention layer with $H$ heads, dimension $d$, and context size $n &lt; d$, featuring $\Theta(Hd^2)$ parameters, can memorize $\Omega(Hn)$ examples. Our analysis sheds light on how different attention heads handle various example sequences, aided by the softmax operator's saturation property. We validate our findings through experiments on synthetic data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#21435;&#20013;&#24515;&#21270;&#26080;&#36951;&#25022;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#21152;&#27861;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#36807;&#24230;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;DumBO&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19838</link><description>&lt;p&gt;
&#25918;&#26494;&#21435;&#20013;&#24515;&#21270;&#26080;&#36951;&#25022;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21152;&#27861;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional Bayesian Optimization. (arXiv:2305.19838v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#21435;&#20013;&#24515;&#21270;&#26080;&#36951;&#25022;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#21152;&#27861;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35299;&#20915;&#20102;&#36807;&#24230;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;DumBO&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#24120;&#29992;&#20110;&#20248;&#21270;&#19968;&#20010;&#26410;&#30693;&#20989;&#25968;$f$&#65292;&#35813;&#20989;&#25968;&#23384;&#22312;&#22122;&#22768;&#19988;&#35780;&#20272;&#25104;&#26412;&#39640;&#26114;&#65292;&#36890;&#36807;&#21033;&#29992;&#24517;&#39035;&#22312;&#27599;&#20010;&#20248;&#21270;&#27493;&#39588;&#20013;&#26368;&#22823;&#21270;&#30340;&#25910;&#33719;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#23613;&#31649;&#21487;&#35777;&#26126;&#28176;&#36827;&#26368;&#20248;&#30340;BO&#31639;&#27861;&#22312;&#20248;&#21270;&#20302;&#32500;&#20989;&#25968;&#26041;&#38754;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#23558;&#20854;&#25193;&#23637;&#21040;&#39640;&#32500;&#31354;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#36890;&#36807;&#20551;&#35774;$f$&#20855;&#26377;&#21152;&#27861;&#32467;&#26500;&#26469;&#35299;&#20915;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;BO&#31639;&#27861;&#36890;&#24120;&#24341;&#20837;&#20102;&#23545;&#21152;&#27861;&#32467;&#26500;&#30340;&#39069;&#22806;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#26412;&#25991;&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#65288;i&#65289;&#25918;&#26494;&#23545;$f$&#21152;&#27861;&#32467;&#26500;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#20197;&#20943;&#24369;&#25910;&#33719;&#20989;&#25968;&#30340;&#26368;&#22823;&#21270;&#20445;&#35777;&#65307;&#65288;ii&#65289;&#35299;&#20915;&#21435;&#20013;&#24515;&#21270;BO&#31639;&#27861;&#20013;&#30340;&#36807;&#24230;&#25506;&#32034;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DumBO&#65292;&#19968;&#31181;&#28176;&#36827;&#26368;&#20248;&#30340;&#21435;&#20013;&#24515;&#21270;BO&#31639;&#27861;&#65292;&#20855;&#26377;&#38750;&#24120;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is typically used to optimize an unknown function $f$ that is noisy and costly to evaluate, by exploiting an acquisition function that must be maximized at each optimization step. Even if provably asymptotically optimal BO algorithms are efficient at optimizing low-dimensional functions, scaling them to high-dimensional spaces remains an open problem, often tackled by assuming an additive structure for $f$. By doing so, BO algorithms typically introduce additional restrictive assumptions on the additive structure that reduce their applicability domain. This paper contains two main contributions: (i) we relax the restrictive assumptions on the additive structure of $f$, at the expense of weakening the maximization guarantees of the acquisition function, and (ii) we address the over-exploration problem for decentralized BO algorithms. To these ends, we propose DumBO, an asymptotically optimal decentralized BO algorithm that achieves very competitive performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OWA&#31639;&#23376;&#30340;&#27169;&#31946;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#36845;&#20195;&#21152;&#26435;&#31574;&#30053;&#24212;&#23545;&#31867;&#21035;&#32423;&#21035;&#22122;&#22768;&#26465;&#20214;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#20256;&#32479;&#30340;&#24120;&#29992;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.19443</link><description>&lt;p&gt;
OWAdapt&#65306;&#20351;&#29992;OWA&#31639;&#23376;&#30340;&#28145;&#24230;&#23398;&#20064;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
OWAdapt: An adaptive loss function for deep learning using OWA operators. (arXiv:2305.19443v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OWA&#31639;&#23376;&#30340;&#27169;&#31946;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#36845;&#20195;&#21152;&#26435;&#31574;&#30053;&#24212;&#23545;&#31867;&#21035;&#32423;&#21035;&#22122;&#22768;&#26465;&#20214;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#20256;&#32479;&#30340;&#24120;&#29992;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#33258;&#36866;&#24212;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#21319;&#20998;&#31867;&#20219;&#21153;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20197;&#26377;&#25928;&#24212;&#23545;&#31867;&#21035;&#32423;&#21035;&#22122;&#22768;&#26465;&#20214;&#65292;&#21253;&#25324;&#31867;&#21035;&#19981;&#24179;&#34913;&#36825;&#19968;&#38590;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#32858;&#21512;&#31639;&#23376;&#65292;&#21033;&#29992;&#27169;&#31946;&#36923;&#36753;&#30340;&#33021;&#21147;&#25552;&#39640;&#20102;&#20998;&#31867;&#31934;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#22312;&#20110;&#25439;&#22833;&#20989;&#25968;&#20869;&#31867;&#21035;&#32423;&#21035;&#32452;&#20214;&#30340;&#36845;&#20195;&#21152;&#26435;&#65292;&#37325;&#28857;&#20851;&#27880;&#37027;&#20123;&#23384;&#22312;&#36739;&#22823;&#35823;&#24046;&#30340;&#32452;&#20214;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26377;&#24207;&#21152;&#26435;&#24179;&#22343;&#65288;OWA&#65289;&#31639;&#23376;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20108;&#20803;&#21644;&#22810;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20854;&#20182;&#24120;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#26631;&#20934;&#20132;&#21449;&#29109;&#25110;&#32858;&#28966;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19982;OWA&#31639;&#23376;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a fuzzy adaptive loss function for enhancing deep learning performance in classification tasks. Specifically, we redefine the cross-entropy loss to effectively address class-level noise conditions, including the challenging problem of class imbalance. Our approach introduces aggregation operators, leveraging the power of fuzzy logic to improve classification accuracy. The rationale behind our proposed method lies in the iterative up-weighting of class-level components within the loss function, focusing on those with larger errors. To achieve this, we employ the ordered weighted average (OWA) operator and combine it with an adaptive scheme for gradient-based learning. Through extensive experimentation, our method outperforms other commonly used loss functions, such as the standard cross-entropy or focal loss, across various binary and multiclass classification tasks. Furthermore, we explore the influence of hyperparameters associated with the OWA operators and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PrivaTree&#30340;&#24046;&#20998;&#38544;&#31169;&#20915;&#31574;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31169;&#26377;&#30452;&#26041;&#22270;&#36873;&#25321;&#20998;&#21106;&#28857;&#26469;&#22312;&#38544;&#31169;&#20445;&#25252;&#19982;&#27169;&#22411;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25509;&#25910;&#28151;&#21512;&#30340;&#25968;&#20540;&#21644;&#31867;&#21035;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25968;&#25454;&#31713;&#25913;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15394</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20915;&#31574;&#26641;&#19982;&#23545;&#25968;&#25454;&#31713;&#25913;&#30340;&#21487;&#38752;&#24615;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Differentially-Private Decision Trees and Provable Robustness to Data Poisoning. (arXiv:2305.15394v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PrivaTree&#30340;&#24046;&#20998;&#38544;&#31169;&#20915;&#31574;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31169;&#26377;&#30452;&#26041;&#22270;&#36873;&#25321;&#20998;&#21106;&#28857;&#26469;&#22312;&#38544;&#31169;&#20445;&#25252;&#19982;&#27169;&#22411;&#25928;&#29992;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25509;&#25910;&#28151;&#21512;&#30340;&#25968;&#20540;&#21644;&#31867;&#21035;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25968;&#25454;&#31713;&#25913;&#26041;&#38754;&#34920;&#29616;&#20986;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#23398;&#20064;&#38382;&#39064;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#20851;&#20110;&#23558;&#24046;&#20998;&#38544;&#31169;&#24341;&#20837;&#20915;&#31574;&#26641;&#23398;&#20064;&#31639;&#27861;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#22810;&#65292;&#24046;&#20998;&#38544;&#31169;&#33021;&#22815;&#30830;&#20445;&#35757;&#32451;&#25968;&#25454;&#20013;&#26679;&#26412;&#30340;&#38544;&#31169;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#22312;&#33719;&#24471;&#19968;&#28857;&#28857;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#36739;&#22810;&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#24341;&#20837;&#20102;&#38543;&#26426;&#20915;&#31574;&#33410;&#28857;&#65292;&#38477;&#20302;&#20102;&#20915;&#31574;&#26641;&#30340;&#20934;&#30830;&#24615;&#65292;&#25110;&#32773;&#22312;&#26631;&#35760;&#21494;&#23376;&#33410;&#28857;&#19978;&#20351;&#29992;&#36807;&#22810;&#30340;&#38544;&#31169;&#39044;&#31639;&#12290;&#27492;&#22806;&#65292;&#24456;&#22810;&#26041;&#27861;&#19981;&#25903;&#25345;&#36830;&#32493;&#29305;&#24449;&#25110;&#32773;&#27844;&#38706;&#19982;&#36830;&#32493;&#29305;&#24449;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31169;&#26377;&#30452;&#26041;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;PrivaTree&#65292;&#23427;&#22312;&#28040;&#32791;&#19968;&#23567;&#37096;&#20998;&#38544;&#31169;&#39044;&#31639;&#30340;&#21516;&#26102;&#36873;&#25321;&#21512;&#36866;&#30340;&#20998;&#21106;&#28857;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#20915;&#31574;&#26641;&#22312;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#32780;&#19988;&#33021;&#22815;&#25509;&#21463;&#28151;&#21512;&#30340;&#25968;&#20540;&#21644;&#31867;&#21035;&#25968;&#25454;&#32780;&#19981;&#27844;&#38706;&#19982;&#25968;&#20540;&#29305;&#24449;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#23613;&#31649;&#32473;&#20986;&#21487;&#38752;&#24615;&#20445;&#35777;&#19968;&#30452;&#24456;&#38590;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#31713;&#25913;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are interpretable models that are well-suited to non-linear learning problems. Much work has been done on extending decision tree learning algorithms with differential privacy, a system that guarantees the privacy of samples within the training data. However, current state-of-the-art algorithms for this purpose sacrifice much utility for a small privacy benefit. These solutions create random decision nodes that reduce decision tree accuracy or spend an excessive share of the privacy budget on labeling leaves. Moreover, many works do not support continuous features or leak information about them. We propose a new method called PrivaTree based on private histograms that chooses good splits while consuming a small privacy budget. The resulting trees provide a significantly better privacy-utility trade-off and accept mixed numerical and categorical data without leaking information about numerical features. Finally, while it is notoriously hard to give robustness guarantees a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#34920;&#31034;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65292;&#23398;&#20064;&#20855;&#26377;&#30456;&#20851;&#29305;&#24449;&#30340;&#35299;&#32806;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#24182;&#22788;&#29702;&#23494;&#38598;&#35266;&#27979;&#30340;&#30456;&#20851;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.14133</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#35299;&#32806;&#34920;&#31034;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Conditional Mutual Information for Disentangled Representations in Reinforcement Learning. (arXiv:2305.14133v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#34920;&#31034;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65292;&#23398;&#20064;&#20855;&#26377;&#30456;&#20851;&#29305;&#24449;&#30340;&#35299;&#32806;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#24182;&#22788;&#29702;&#23494;&#38598;&#35266;&#27979;&#30340;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#20551;&#30456;&#20851;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#26159;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#37327;&#25110;&#29305;&#24449;&#35206;&#30422;&#30340;&#26377;&#38480;&#24615;&#25152;&#23548;&#33268;&#30340;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23558;&#36825;&#20123;&#35823;&#23548;&#24615;&#30456;&#20851;&#24615;&#32534;&#30721;&#21040;&#20854;&#28508;&#22312;&#34920;&#31034;&#20013;&#65292;&#22914;&#26524;&#29615;&#22659;&#20869;&#30340;&#30456;&#20851;&#24615;&#21457;&#29983;&#21464;&#21270;&#25110;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#26102;&#65292;&#21017;&#20250;&#38459;&#27490;&#20195;&#29702;&#36827;&#34892;&#27867;&#21270;&#12290;&#35299;&#32806;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#26368;&#23567;&#21270;&#29305;&#24449;&#20043;&#38388;&#20114;&#20449;&#24687;&#30340;&#35299;&#32806;&#25216;&#26415;&#35201;&#27714;&#29305;&#24449;&#20043;&#38388;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#26080;&#27861;&#35299;&#32806;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#36741;&#21161;&#20219;&#21153;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#34920;&#31034;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65292;&#23398;&#20064;&#20855;&#26377;&#30456;&#20851;&#29305;&#24449;&#30340;&#39640;&#32500;&#35266;&#27979;&#30340;&#35299;&#32806;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#22312;&#30456;&#20851;&#24615;&#21464;&#21270;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#33021;&#22815;&#22788;&#29702;&#23494;&#38598;&#35266;&#27979;&#30340;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) environments can produce training data with spurious correlations between features due to the amount of training data or its limited feature coverage. This can lead to RL agents encoding these misleading correlations in their latent representation, preventing the agent from generalising if the correlation changes within the environment or when deployed in the real world. Disentangled representations can improve robustness, but existing disentanglement techniques that minimise mutual information between features require independent features, thus they cannot disentangle correlated features. We propose an auxiliary task for RL algorithms that learns a disentangled representation of high-dimensional observations with correlated features by minimising the conditional mutual information between features in the representation. We demonstrate experimentally, using continuous control tasks, that our approach improves generalisation under correlation shifts, as well 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21450;&#20854;&#21464;&#20307;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#28085;&#30422;&#20102;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#65292;&#20854;&#27867;&#21270;&#30028;&#38480;&#19982;&#36830;&#32493;&#26435;&#37325;&#24046;&#24322;&#22823;&#23567;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.06648</link><description>&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#19982;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization bounds for neural ordinary differential equations and deep residual networks. (arXiv:2305.06648v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21450;&#20854;&#21464;&#20307;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#28085;&#30422;&#20102;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#65292;&#20854;&#27867;&#21270;&#30028;&#38480;&#19982;&#36830;&#32493;&#26435;&#37325;&#24046;&#24322;&#22823;&#23567;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#26159;&#19968;&#31867;&#27969;&#34892;&#30340;&#36830;&#32493;&#28145;&#24230;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#30001;&#36830;&#32493;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;ODE&#21450;&#26102;&#21464;&#30340;&#31070;&#32463;ODE&#32452;&#25104;&#30340;&#22823;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;Lipschitz&#26041;&#27861;&#25512;&#23548;&#20102;&#36825;&#20010;&#31867;&#21035;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;ODE&#21644;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#20043;&#38388;&#30340;&#31867;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#19968;&#20010;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36825;&#20010;&#30028;&#38480;&#19982;&#36830;&#32493;&#26435;&#37325;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#22823;&#23567;&#26377;&#20851;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#28436;&#31034;&#20102;&#36825;&#20010;&#37327;&#26159;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ordinary differential equations (neural ODEs) are a popular family of continuous-depth deep learning models. In this work, we consider a large family of parameterized ODEs with continuous-in-time parameters, which include time-dependent neural ODEs. We derive a generalization bound for this class by a Lipschitz-based argument. By leveraging the analogy between neural ODEs and deep residual networks, our approach yields in particular a generalization bound for a class of deep residual networks. The bound involves the magnitude of the difference between successive weight matrices. We illustrate numerically how this quantity affects the generalization capability of neural networks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#19981;&#36275;&#20197;&#23436;&#20840;&#25429;&#25417;&#22810;&#20010;&#20114;&#21160;&#20195;&#29702;&#30340;&#32852;&#21512;&#34920;&#29616;&#65292;&#25552;&#20986;&#20351;&#29992;&#32852;&#21512;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06292</link><description>&lt;p&gt;
&#32852;&#21512;&#24230;&#37327;&#37325;&#35201;&#24615;&#65306;&#36712;&#36857;&#39044;&#27979;&#30340;&#26356;&#22909;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Joint Metrics Matter: A Better Standard for Trajectory Forecasting. (arXiv:2305.06292v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06292
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#19981;&#36275;&#20197;&#23436;&#20840;&#25429;&#25417;&#22810;&#20010;&#20114;&#21160;&#20195;&#29702;&#30340;&#32852;&#21512;&#34920;&#29616;&#65292;&#25552;&#20986;&#20351;&#29992;&#32852;&#21512;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#21333;&#20010;&#20195;&#29702;&#24230;&#37327;&#26631;&#20934;&#65288;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#65289;&#65292;&#20363;&#22914;&#26368;&#23567;&#24179;&#22343;&#20301;&#31227;&#35823;&#24046;&#65288;ADE&#65289;&#21644;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;&#65288;FDE&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26080;&#27861;&#25429;&#25417;&#22810;&#20010;&#20114;&#21160;&#20195;&#29702;&#30340;&#32852;&#21512;&#34920;&#29616;&#12290;&#20165;&#20851;&#27880;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#33258;&#28982;&#30340;&#39044;&#27979;&#65292;&#22914;&#30896;&#25758;&#36712;&#36857;&#25110;&#26126;&#26174;&#19968;&#36215;&#34892;&#36208;&#30340;&#20154;&#30340;&#21457;&#25955;&#36712;&#36857;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#20248;&#21270;&#30340;&#26041;&#27861;&#20250;&#23548;&#33268;&#36807;&#20110;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#65292;&#36825;&#23545;&#36712;&#36857;&#39044;&#27979;&#30740;&#31350;&#30340;&#36827;&#23637;&#26377;&#23475;&#12290;&#20026;&#20102;&#24212;&#23545;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#19982;&#22810;&#20195;&#29702;&#24230;&#37327;&#26631;&#20934;&#65288;&#32852;&#21512;&#24230;&#37327;&#26631;&#20934;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;JADE&#12289;JFDE&#21644;&#30896;&#25758;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#37327;&#21270;&#35777;&#25454;&#21644;&#23450;&#24615;&#31034;&#20363;&#23637;&#31034;&#20102;&#32852;&#21512;&#24230;&#37327;&#26631;&#20934;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#19981;&#26159;&#36793;&#38469;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal trajectory forecasting methods commonly evaluate using single-agent metrics (marginal metrics), such as minimum Average Displacement Error (ADE) and Final Displacement Error (FDE), which fail to capture joint performance of multiple interacting agents. Only focusing on marginal metrics can lead to unnatural predictions, such as colliding trajectories or diverging trajectories for people who are clearly walking together as a group. Consequently, methods optimized for marginal metrics lead to overly-optimistic estimations of performance, which is detrimental to progress in trajectory forecasting research. In response to the limitations of marginal metrics, we present the first comprehensive evaluation of state-of-the-art (SOTA) trajectory forecasting methods with respect to multi-agent metrics (joint metrics): JADE, JFDE, and collision rate. We demonstrate the importance of joint metrics as opposed to marginal metrics with quantitative evidence and qualitative examples drawn 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#19979;&#27169;&#22411;&#36873;&#25321;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#20854;&#36716;&#31227;&#36317;&#31163;&#20250;&#24433;&#21709;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21487;&#33021;&#23548;&#33268;&#36895;&#29575;&#36739;&#24930;&#12290;</title><link>http://arxiv.org/abs/2305.00152</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#19979;&#30340;&#27169;&#22411;&#36873;&#25321;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#19979;&#27169;&#22411;&#36873;&#25321;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#20854;&#36716;&#31227;&#36317;&#31163;&#20250;&#24433;&#21709;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21487;&#33021;&#23548;&#33268;&#36895;&#29575;&#36739;&#24930;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20851;&#20110;&#36716;&#31227;&#23398;&#20064;&#25110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24050;&#30693;&#20551;&#35774;&#31867;&#25110;&#27169;&#22411;&#30340;&#24773;&#20917;&#65307;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#27169;&#22411;&#36873;&#25321;&#65292;&#36825;&#32463;&#24120;&#20986;&#29616;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24635;&#20307;&#33539;&#30068;&#19979;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#32771;&#34385;&#35843;&#25972;&#38024;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#27491;&#30830;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#30456;&#20851;&#28304;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#38500;&#20102;&#19982;&#27169;&#22411;&#36873;&#25321;&#26377;&#20851;&#30340;&#36817;&#20284;&#19982;&#20272;&#35745;&#35823;&#24046;&#30340;&#36890;&#24120;&#26435;&#34913;&#20043;&#22806;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#24102;&#26469;&#20102;&#26032;&#30340;&#22797;&#26434;&#24230;&#65292;&#21363;&#28304;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#36716;&#31227;&#36317;&#31163;&#65292;&#36825;&#20010;&#36317;&#31163;&#38543;&#30528;&#20551;&#35774;&#31867;&#30340;&#36873;&#25321;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#20998;&#31867;&#38382;&#39064;&#12290;&#29305;&#21035;&#30340;&#65292;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20123;&#24341;&#20154;&#27880;&#30446;&#30340;&#29616;&#35937;&#65306;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21363;&#27809;&#26377;&#20998;&#24067;&#24335;&#20449;&#24687;&#26102;&#21487;&#36798;&#21040;&#30340;&#36895;&#29575;&#65292;&#21487;&#20197;&#20219;&#24847;&#24930;&#20110;oracle&#36895;&#29575;&#65292;&#21363;&#22312;&#32473;&#23450;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;TSdiff &#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#37117;&#26356;&#39640;&#12290;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#26356;&#20248;&#21270;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.12233</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25506;&#32034;&#20108;&#32500;&#20998;&#23376;&#22270;&#30340;&#36807;&#28193;&#24577;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs. (arXiv:2304.12233v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;TSdiff &#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#37117;&#26356;&#39640;&#12290;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#26356;&#20248;&#21270;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#23545;&#20110;&#38416;&#26126;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;&#21644;&#27169;&#25311;&#21453;&#24212;&#21160;&#21147;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#21453;&#24212;&#29289;&#21644;&#20135;&#29289;&#30340; 3D &#24418;&#24577;&#21644;&#26041;&#21521;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25193;&#25955;&#26041;&#27861;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;TSDiff&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#20174;&#35757;&#32451;&#20013;&#20102;&#35299;&#21040;&#21508;&#31181;&#21453;&#24212;&#30340;&#36807;&#28193;&#24577;&#20960;&#20309;&#20998;&#24067;&#65292;&#20174;&#32780;&#33021;&#22815;&#37319;&#26679;&#21508;&#31181;&#36807;&#28193;&#24577;&#26500;&#35937;&#12290;&#22240;&#27492;&#65292;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#20013;&#26356;&#20026;&#26377;&#21033;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;TSDiff &#22312;&#21152;&#36895;&#21270;&#23398;&#21453;&#24212;&#21644;&#21453;&#24212;&#36884;&#24452;&#30340;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration of transition state (TS) geometries is crucial for elucidating chemical reaction mechanisms and modeling their kinetics. Recently, machine learning (ML) models have shown remarkable performance for prediction of TS geometries. However, they require 3D conformations of reactants and products often with their appropriate orientations as input, which demands substantial efforts and computational cost. Here, we propose a generative approach based on the stochastic diffusion method, namely TSDiff, for prediction of TS geometries just from 2D molecular graphs. TSDiff outperformed the existing ML models with 3D geometries in terms of both accuracy and efficiency. Moreover, it enables to sample various TS conformations, because it learned the distribution of TS geometries for diverse reactions in training. Thus, TSDiff was able to find more favorable reaction pathways with lower barrier heights than those in the reference database. These results demonstrate that TSDiff shows pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#21644;&#25237;&#24433;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20415;&#23452;&#12289;&#39640;&#25928;&#22320;&#29983;&#25104;&#26102;&#24577;&#23494;&#24230;&#24314;&#27169;&#65292;&#20854;&#26368;&#20248;&#26144;&#23556;&#19982;&#24658;&#31561;&#26144;&#23556;&#25509;&#36817;&#65292;&#35757;&#32451;&#36807;&#31243;&#39640;&#24230;&#24182;&#34892;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.09663</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#21644;&#25237;&#24433;&#36861;&#36394;&#30340;&#26102;&#21464;&#23494;&#24230;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling of Time-Dependent Densities via Optimal Transport and Projection Pursuit. (arXiv:2304.09663v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#21644;&#25237;&#24433;&#36861;&#36394;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20415;&#23452;&#12289;&#39640;&#25928;&#22320;&#29983;&#25104;&#26102;&#24577;&#23494;&#24230;&#24314;&#27169;&#65292;&#20854;&#26368;&#20248;&#26144;&#23556;&#19982;&#24658;&#31561;&#26144;&#23556;&#25509;&#36817;&#65292;&#35757;&#32451;&#36807;&#31243;&#39640;&#24230;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#26102;&#24577;&#23494;&#24230;&#29983;&#25104;&#24314;&#27169;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#22256;&#38590;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20415;&#23452;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23427;&#38656;&#35201;&#26368;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#21487;&#20197;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25237;&#24433;&#30340;&#26368;&#20248;&#36755;&#36816;&#27714;&#35299;&#22120; [Meng&#31561;&#65292;2019] &#26469;&#36830;&#25509;&#36830;&#32493;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#20256;&#36755;&#26679;&#26465; [Chewi&#31561;&#65292;2020] &#26469;&#25554;&#20540;&#28436;&#21270;&#30340;&#23494;&#24230;&#12290;&#24403;&#37319;&#26679;&#39057;&#29575;&#36275;&#22815;&#39640;&#26102;&#65292;&#26368;&#20248;&#26144;&#23556;&#25509;&#36817;&#20110;&#24658;&#31561;&#26144;&#23556;&#65292;&#22240;&#27492;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#36807;&#31243;&#21487;&#20197;&#39640;&#24230;&#24182;&#34892;&#21270;&#65292;&#22240;&#20026;&#25152;&#26377;&#26368;&#20248;&#26144;&#23556;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#20165;&#22522;&#20110;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#32780;&#19981;&#26159;&#26368;&#23567;&#21270;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#20998;&#26512;&#21644;&#25511;&#21046;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the computational difficulties incurred by popular deep learning algorithms for the generative modeling of temporal densities, we propose a cheap alternative which requires minimal hyperparameter tuning and scales favorably to high dimensional problems. In particular, we use a projection-based optimal transport solver [Meng et al., 2019] to join successive samples and subsequently use transport splines [Chewi et al., 2020] to interpolate the evolving density. When the sampling frequency is sufficiently high, the optimal maps are close to the identity and are thus computationally efficient to compute. Moreover, the training process is highly parallelizable as all optimal maps are independent and can thus be learned simultaneously. Finally, the approach is based solely on numerical linear algebra rather than minimizing a nonconvex objective function, allowing us to easily analyze and control the algorithm. We present several numerical experiments on both synthetic and real-w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65288;VOL&#65289;&#30340;&#33539;&#24335;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#20351;&#29992;&#27491;&#21453;&#20256;&#36882;&#24490;&#29615;&#21644;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#20102;&#21464;&#20998;&#25805;&#20316;&#65292;&#36890;&#36807;&#26368;&#36895;&#19979;&#38477;&#27861;&#21644;&#20849;&#36717;&#26799;&#24230;&#27861;&#36827;&#34892;&#31070;&#32463;&#31639;&#23376;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#38750;&#24120;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.04234</link><description>&lt;p&gt;
&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65306;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational operator learning: A unified paradigm for training neural operators and solving partial differential equations. (arXiv:2304.04234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65288;VOL&#65289;&#30340;&#33539;&#24335;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#20351;&#29992;&#27491;&#21453;&#20256;&#36882;&#24490;&#29615;&#21644;&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#20102;&#21464;&#20998;&#25805;&#20316;&#65292;&#36890;&#36807;&#26368;&#36895;&#19979;&#38477;&#27861;&#21644;&#20849;&#36717;&#26799;&#24230;&#27861;&#36827;&#34892;&#31070;&#32463;&#31639;&#23376;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#38750;&#24120;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#26041;&#27861;&#30340;&#26032;&#33539;&#24335;&#65292;&#20026;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#21644;&#29992;&#21464;&#20998;&#24418;&#24335;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#21464;&#20998;&#31639;&#23376;&#23398;&#20064;&#65288;VOL&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#31070;&#32463;&#31639;&#23376;&#32473;&#20986;&#30340;&#33410;&#28857;&#35299;&#39044;&#27979;&#20013;&#25512;&#23548;&#20986;&#31995;&#32479;&#30340;&#20989;&#25968;&#36924;&#36817;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#36827;&#34892;&#21464;&#20998;&#25805;&#20316;&#65292;&#26500;&#24314;&#27491;&#21453;&#20256;&#36882;&#24490;&#29615;&#26469;&#25512;&#23548;&#32447;&#24615;&#31995;&#32479;&#30340;&#27531;&#24046;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#26368;&#36895;&#19979;&#38477;&#27861;&#65288;SD&#65289;&#21644;&#20849;&#36717;&#26799;&#24230;&#27861;&#65288;CG&#65289;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#26356;&#26032;&#27493;&#39588;&#65292;&#20316;&#20026;&#35757;&#32451;&#31070;&#32463;&#31639;&#23376;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26356;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;VOL&#21487;&#20197;&#23398;&#20064;&#21040;&#22312;&#31283;&#23450;&#20256;&#28909;&#21644;&#21464;&#21018;&#24230;&#24377;&#24615;PDE&#20013;&#21508;&#31181;&#35299;&#31639;&#23376;&#65292;&#32467;&#26524;&#20196;&#20154;&#28385;&#24847;&#65292;&#35823;&#24046;&#36739;&#23567;&#12290;&#35813;&#26041;&#27861;&#20960;&#20046;&#23454;&#29616;&#26080;&#26631;&#31614;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the variational method, we propose a novel paradigm that provides a unified framework of training neural operators and solving partial differential equations (PDEs) with the variational form, which we refer to as the variational operator learning (VOL). We first derive the functional approximation of the system from the node solution prediction given by neural operators, and then conduct the variational operation by automatic differentiation, constructing a forward-backward propagation loop to derive the residual of the linear system. One or several update steps of the steepest decent method (SD) and the conjugate gradient method (CG) are provided in every iteration as a cheap yet effective update for training the neural operators. Experimental results show the proposed VOL can learn a variety of solution operators in PDEs of the steady heat transfer and the variable stiffness elasticity with satisfactory results and small error. The proposed VOL achieves nearly label-free tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#22686;&#24378;&#24369;&#30417;&#30563;&#20998;&#21106;&#20013;&#30340;&#39640;&#20445;&#30495;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#24369;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02621</link><description>&lt;p&gt;
&#22686;&#24378;&#24369;&#30417;&#30563;&#20998;&#21106;&#30340;&#39640;&#20445;&#30495;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation. (arXiv:2304.02621v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#22686;&#24378;&#24369;&#30417;&#30563;&#20998;&#21106;&#20013;&#30340;&#39640;&#20445;&#30495;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#24369;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#32423;&#21035;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#20219;&#21153;&#22240;&#20026;&#20854;&#21487;&#20943;&#23569;&#22823;&#37327;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;WSSS&#30340;&#20856;&#22411;&#26041;&#27861;&#26159;&#20351;&#29992;&#20840;&#23616;&#24179;&#22343;&#27744;&#21270;&#65288;GAP&#65289;&#22312;&#21367;&#31215;&#29305;&#24449;&#26144;&#23556;&#19978;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22522;&#20110;&#31867;&#21035;&#28608;&#27963;&#22270;&#65288;CAMs&#65289;&#20272;&#35745;&#23545;&#35937;&#20301;&#32622;&#65292;CAMs&#35782;&#21035;&#22270;&#20687;&#21306;&#22495;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#20351;&#29992;CAMs&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#20197;&#24418;&#24335;&#21270;&#30340;&#20998;&#21106;&#25513;&#30721;&#30340;&#26041;&#24335;&#22312;&#32570;&#20047;&#20687;&#32032;&#32423;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23545;&#20998;&#21106;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#12290;&#22312;SEAM&#22522;&#32447;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#25552;&#39640;CAM&#23398;&#20064;&#30340;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#23427;&#26159;GAP&#30340;&#26367;&#20195;&#26041;&#27861;&#65307;&#65288;2&#65289;&#29305;&#24449;&#30456;&#20284;&#24615;&#25439;&#22833;&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21363;&#23545;&#35937;&#36718;&#24275;&#20960;&#20046;&#20165;&#19982;&#22270;&#20687;&#20013;&#30340;&#39068;&#33394;&#36793;&#32536;&#23545;&#40784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#36825;&#20123;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#27010;&#29575;&#35299;&#37322;CAM&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#20266;&#26631;&#31614;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#23558;&#23616;&#37096;&#31354;&#38388;&#19968;&#33268;&#24615;&#32422;&#26463;&#34701;&#20837;CAM&#23398;&#20064;&#20013;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20132;&#26367;&#26356;&#26032;CAM&#21644;&#20998;&#21106;&#27169;&#22411;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24369;&#30417;&#30563;&#20998;&#21106;&#26041;&#27861;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of image-level weakly-supervised semantic segmentation (WSSS) has gained popularity in recent years, as it reduces the vast data annotation cost for training segmentation models. The typical approach for WSSS involves training an image classification network using global average pooling (GAP) on convolutional feature maps. This enables the estimation of object locations based on class activation maps (CAMs), which identify the importance of image regions. The CAMs are then used to generate pseudo-labels, in the form of segmentation masks, to supervise a segmentation model in the absence of pixel-level ground truth. In case of the SEAM baseline, a previous work proposed to improve CAM learning in two ways: (1) Importance sampling, which is a substitute for GAP, and (2) the feature similarity loss, which utilizes a heuristic that object contours almost exclusively align with color edges in images. In this work, we propose a different probabilistic interpretation of CAMs for thes
&lt;/p&gt;</description></item><item><title>LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.00457</link><description>&lt;p&gt;
LLMMaps&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#23618;&#35780;&#20215;&#30340;&#21487;&#35270;&#21270;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00457
&lt;/p&gt;
&lt;p&gt;
LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;&#27169;&#22411;&#22312;&#21709;&#24212;&#20013;&#26292;&#38706;&#20986;&#19981;&#27491;&#30830;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#24517;&#39035;&#37319;&#29992;&#21220;&#22859;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#34429;&#28982;LLM&#22312;&#29305;&#23450;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#22522;&#20110;&#38382;&#31572;(Q&amp;A)&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#36825;&#20123;&#35780;&#20272;&#36890;&#24120;&#20165;&#25253;&#21578;&#25972;&#20010;&#39046;&#22495;&#30340;&#21333;&#20010;&#20934;&#30830;&#24230;&#25968;&#23383;&#65292;&#36825;&#19968;&#31243;&#24207;&#22312;&#36879;&#26126;&#24230;&#21644;&#27169;&#22411;&#25913;&#36827;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20998;&#23618;&#35780;&#20272;&#21487;&#20197;&#25581;&#31034;&#21487;&#33021;&#26356;&#23481;&#26131;&#21457;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#35780;&#20272;LLMs&#30340;&#39118;&#38505;&#24182;&#25351;&#23548;&#23427;&#20204;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20026;&#25903;&#25345;&#36825;&#26679;&#30340;&#20998;&#23618;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMMaps&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;Q&amp;A&#25968;&#25454;&#38598;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;LLMMaps&#25552;&#20379;&#20102;&#23545;LLMs&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#20998;&#24067;&#30340;&#35814;&#32454;&#27934;&#23519;&#65292;&#20801;&#35768;&#29992;&#25143;&#25918;&#22823;&#39046;&#22495;&#30340;&#29305;&#23450;&#37096;&#20998;&#24182;&#25506;&#32034;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMMaps&#26377;&#21161;&#20110;&#35782;&#21035;&#20986;&#26356;&#23481;&#26131;&#20986;&#29616;LLM&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#25913;&#21892;&#36825;&#20123;&#39046;&#22495;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q&amp;A) datasets, such evaluations usually report only a single accuracy number for the entire field, a procedure which is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q&amp;A datasets. LLMMaps provide detailed insights into LLMs' kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411; (N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#21487;&#20197;&#23545;&#36830;&#32493;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17823</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#22238;&#24212;&#26377;&#24207;&#22238;&#24402;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An interpretable neural network-based non-proportional odds model for ordinal regression with continuous response. (arXiv:2303.17823v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411; (N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#21487;&#20197;&#23545;&#36830;&#32493;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#27604;&#20363;&#36180;&#29575;&#27169;&#22411;&#65288;N$^3$POM) &#29992;&#20110;&#26377;&#24207;&#22238;&#24402;&#65292;&#20854;&#20013;&#21453;&#24212;&#21464;&#37327;&#19981;&#20165;&#21487;&#20197;&#21462;&#31163;&#25955;&#20540;&#65292;&#20063;&#21487;&#20197;&#21462;&#36830;&#32493;&#20540;&#65292;&#32780;&#22238;&#24402;&#31995;&#25968;&#26681;&#25454;&#39044;&#27979;&#39034;&#24207;&#21453;&#24212;&#20063;&#19981;&#21516;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30452;&#25509;&#20174;&#31163;&#25955;&#21453;&#24212;&#20272;&#35745;&#32447;&#24615;&#31995;&#25968;&#19981;&#21516;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20197;&#21453;&#24212;&#20026;&#36755;&#20837;&#20135;&#29983;&#32447;&#24615;&#31995;&#25968;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;N$^3$POM&#21487;&#20197;&#22312;&#20445;&#30041;&#20256;&#32479;&#26377;&#24207;&#22238;&#24402;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20805;&#20998;&#30340;&#26465;&#20214;&#65292;&#20351;&#24471;&#22312;&#25351;&#23450;&#30340;&#29992;&#25143;&#21306;&#22495;&#20869;&#65292;&#39044;&#27979;&#30340;&#26465;&#20214;&#32047;&#31215;&#27010;&#29575;&#65288;CCP&#65289;&#28385;&#36275;&#23616;&#37096;&#21333;&#35843;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25345;&#21333;&#35843;&#24615;&#30340;&#38543;&#26426;&#65288;MPS&#65289;&#31639;&#27861;&#26469;&#20805;&#20998;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an interpretable neural network-based non-proportional odds model (N$^3$POM) for ordinal regression, where the response variable can take not only discrete but also continuous values, and the regression coefficients vary depending on the predicting ordinal response. In contrast to conventional approaches estimating the linear coefficients of regression directly from the discrete response, we train a non-linear neural network that outputs the linear coefficients by taking the response as its input. By virtue of the neural network, N$^3$POM may have flexibility while preserving the interpretability of the conventional ordinal regression. We show a sufficient condition so that the predicted conditional cumulative probability~(CCP) satisfies the monotonicity constraint locally over a user-specified region in the covariate space; we also provide a monotonicity-preserving stochastic (MPS) algorithm for training the neural network adequately.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26080;&#26631;&#31614;&#22270;&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#22686;&#24378;&#23646;&#24615;&#39044;&#27979;&#27169;&#22411;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#20004;&#20010;&#26032;&#30446;&#26631;&#36827;&#34892;&#21435;&#22122;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#27604;14&#31181;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.10108</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26080;&#26631;&#31614;&#22270;&#25968;&#25454;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Learning from Unlabeled Graphs with Diffusion Model. (arXiv:2303.10108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26080;&#26631;&#31614;&#22270;&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#22686;&#24378;&#23646;&#24615;&#39044;&#27979;&#27169;&#22411;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#20004;&#20010;&#26032;&#30446;&#26631;&#36827;&#34892;&#21435;&#22122;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#27604;14&#31181;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#21644;&#20247;&#22810;&#12290;&#34429;&#28982;&#27599;&#20010;&#20219;&#21153;&#21482;&#25552;&#20379;&#23569;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#65292;&#20294;&#26080;&#26631;&#31614;&#22270;&#24050;&#20174;&#21508;&#31181;&#28192;&#36947;&#24182;&#22823;&#35268;&#27169;&#25910;&#38598;&#12290;&#20256;&#32479;&#26041;&#27861;&#26159;&#22312;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#26080;&#26631;&#31614;&#22270;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#30693;&#35782;&#21487;&#33021;&#26080;&#27861;&#19982;&#39044;&#27979;&#20219;&#21153;&#38656;&#35201;&#30340;&#30693;&#35782;&#30456;&#21563;&#21512;&#25110;&#26377;&#26102;&#20250;&#20135;&#29983;&#20914;&#31361;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#22270;&#20013;&#25552;&#21462;&#30693;&#35782;&#20316;&#20026;&#29305;&#23450;&#26377;&#29992;&#25968;&#25454;&#28857;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27599;&#20010;&#23646;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#26080;&#26631;&#31614;&#22270;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#30340;&#30446;&#26631;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#22270;&#24418;&#31034;&#20363;&#21450;&#20854;&#26631;&#31614;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#27604;14&#31181;&#29616;&#26377;&#26041;&#27861;&#22312;fi&#19978;&#34920;&#29616;&#26174;&#30528;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph property prediction tasks are important and numerous. While each task offers a small size of labeled examples, unlabeled graphs have been collected from various sources and at a large scale. A conventional approach is training a model with the unlabeled graphs on self-supervised tasks and then fine-tuning the model on the prediction tasks. However, the self-supervised task knowledge could not be aligned or sometimes conflicted with what the predictions needed. In this paper, we propose to extract the knowledge underlying the large set of unlabeled graphs as a specific set of useful data points to augment each property prediction model. We use a diffusion model to fully utilize the unlabeled graphs and design two new objectives to guide the model's denoising process with each task's labeled data to generate task-specific graph examples and their labels. Experiments demonstrate that our data-centric approach performs significantly better than fourteen existing various methods on fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#27748;&#26222;&#26862;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22870;&#21169;&#26041;&#24046;&#30340;&#20449;&#24687;&#20943;&#23569;&#20102;&#36951;&#25022;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;</title><link>http://arxiv.org/abs/2303.09033</link><description>&lt;p&gt;
&#21482;&#38024;&#23545;&#19981;&#30830;&#23450;&#24615;&#25903;&#20184;&#20195;&#20215;&#65306;&#26041;&#24046;&#33258;&#36866;&#24212;&#27748;&#26222;&#26862;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling. (arXiv:2303.09033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#27748;&#26222;&#26862;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22870;&#21169;&#26041;&#24046;&#30340;&#20449;&#24687;&#20943;&#23569;&#20102;&#36951;&#25022;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#36172;&#21338;&#31639;&#27861;&#37117;&#20551;&#35774;&#22870;&#21169;&#26041;&#24046;&#25110;&#20854;&#19978;&#30028;&#24050;&#30693;&#12290;&#23613;&#31649;&#26041;&#24046;&#39640;&#20272;&#36890;&#24120;&#26159;&#23433;&#20840;&#30340;&#65292;&#20294;&#23427;&#20250;&#22686;&#21152;&#36951;&#25022;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20302;&#20272;&#30340;&#26041;&#24046;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#36807;&#26089;&#22320;&#36873;&#25321;&#20102;&#27425;&#20248;&#33218;&#32780;&#23548;&#33268;&#30340;&#32447;&#24615;&#36951;&#25022;&#12290;&#36825;&#28608;&#21457;&#20102;&#20851;&#20110;&#26041;&#24046;&#24863;&#30693;&#39057;&#29575;&#31639;&#27861;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#25105;&#20204;&#20026;&#36125;&#21494;&#26031;&#35774;&#32622;&#25171;&#19979;&#22522;&#30784;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24050;&#30693;&#21644;&#26410;&#30693;&#24322;&#36136;&#22870;&#21169;&#26041;&#24046;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#24182;&#20026;&#20004;&#32773;&#24320;&#21457;&#20102;&#27748;&#26222;&#26862;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36125;&#21494;&#26031;&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#36951;&#25022;&#30028;&#38543;&#30528;&#36739;&#20302;&#22870;&#21169;&#26041;&#24046;&#32780;&#20943;&#23569;&#65292;&#36825;&#20351;&#24471;&#23398;&#20064;&#26356;&#21152;&#23481;&#26131;&#12290;&#26410;&#30693;&#22870;&#21169;&#26041;&#24046;&#30340;&#36793;&#30028;&#25429;&#25417;&#20102;&#20808;&#39564;&#23545;&#23398;&#20064;&#22870;&#21169;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#26159;&#20854;&#31867;&#22411;&#20013;&#30340;&#39318;&#20010;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#26041;&#24046;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20063;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most bandit algorithms assume that the reward variance or its upper bound is known. While variance overestimation is usually safe and sound, it increases regret. On the other hand, an underestimated variance may lead to linear regret due to committing early to a suboptimal arm. This motivated prior works on variance-aware frequentist algorithms. We lay foundations for the Bayesian setting. In particular, we study multi-armed bandits with known and \emph{unknown heterogeneous reward variances}, and develop Thompson sampling algorithms for both and bound their Bayes regret. Our regret bounds decrease with lower reward variances, which make learning easier. The bound for unknown reward variances captures the effect of the prior on learning reward variances and is the first of its kind. Our experiments show the superiority of variance-aware Bayesian algorithms and also highlight their robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#21487;&#20197;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07189</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25104;&#20687;&#20013;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Optimizing Convolutional Neural Networks for Chronic Obstructive Pulmonary Disease Detection in Clinical Computed Tomography Imaging. (arXiv:2303.07189v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#21487;&#20197;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#32954;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#65288;COPD&#65289;&#30340;&#23384;&#22312;&#65292;&#26469;&#20248;&#21270;&#20108;&#36827;&#21046;COPD&#30340;&#26816;&#27979;&#12290;&#26041;&#27861;&#65306;&#22238;&#39038;&#24615;&#36873;&#25321;&#20102;78&#21517;&#21463;&#35797;&#32773;&#65288;43&#21517;COPD&#24739;&#32773;&#65307;35&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#65289;&#30340;7,194&#20010;CT&#22270;&#20687;&#65288;3,597&#20010;COPD&#65307;3,597&#20010;&#20581;&#24247;&#23545;&#29031;&#32452;&#65289;&#65288;2018&#24180;10&#26376;&#33267;2019&#24180;12&#26376;&#65289;&#12290;&#23545;&#27599;&#20010;&#22270;&#20687;&#65292;&#23558;&#24378;&#24230;&#20540;&#25163;&#21160;&#35009;&#21098;&#21040;&#32954;&#27668;&#32959;&#31383;&#21475;&#35774;&#32622;&#21644;&#22522;&#20934;&#30340;&#8220;&#20840;&#33539;&#22260;&#8221;&#31383;&#21475;&#35774;&#32622;&#12290;&#31867;&#24179;&#34913;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#21253;&#21547;&#20102;3,392&#12289;1,114&#21644;2,688&#20010;&#22270;&#20687;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;CNN&#26550;&#26500;&#26469;&#20248;&#21270;&#32593;&#32476;&#20027;&#24178;&#12290;&#27492;&#22806;&#65292;&#36824;&#36890;&#36807;&#21521;&#27169;&#22411;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#26469;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#12290;&#26681;&#25454;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#65288;ROC&#65289;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#30340;&#22270;&#20687;&#27700;&#24179;&#65292;&#35745;&#31639;&#20986;P&#20540;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To optimize the binary detection of Chronic Obstructive Pulmonary Disease (COPD) based on emphysema presence in the lung with convolutional neural networks (CNN) by exploring manually adjusted versus automated window-setting optimization (WSO) on computed tomography (CT) images.  Methods: 7,194 CT images (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with COPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and preprocessed. For each image, intensity values were manually clipped to the emphysema window setting and a baseline 'full-range' window setting. Class-balanced train, validation, and test sets contained 3,392, 1,114, and 2,688 images. The network backbone was optimized by comparing various CNN architectures. Furthermore, automated WSO was implemented by adding a customized layer to the model. The image-level area under the Receiver Operating Characteristics curve (AUC) [lower, upper limit 95% confidence] and P-values calculated from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#37197;&#26041;&#65292;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#37319;&#26679;&#22120;&#30340;&#35265;&#35299;&#65292;&#30830;&#20445;&#25910;&#25947;&#21040;&#25152;&#38656;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#30456;&#31354;&#38388;Langevin&#25193;&#25955;&#65288;PSLD&#65289;&#65292;&#22312;&#25193;&#23637;&#31354;&#38388;&#20013;&#36827;&#34892;&#22522;&#20110;&#35780;&#20998;&#30340;&#24314;&#27169;&#65292;&#23637;&#29616;&#20986;&#26356;&#20248;&#36136;&#30340;&#26679;&#26412;&#36136;&#37327;&#21644;&#25913;&#36827;&#30340;&#36895;&#24230;-&#36136;&#37327;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.01748</link><description>&lt;p&gt;
&#19968;&#31181;&#23436;&#25972;&#30340;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#37197;&#26041;
&lt;/p&gt;
&lt;p&gt;
A Complete Recipe for Diffusion Generative Models. (arXiv:2303.01748v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#37197;&#26041;&#65292;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#37319;&#26679;&#22120;&#30340;&#35265;&#35299;&#65292;&#30830;&#20445;&#25910;&#25947;&#21040;&#25152;&#38656;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#30456;&#31354;&#38388;Langevin&#25193;&#25955;&#65288;PSLD&#65289;&#65292;&#22312;&#25193;&#23637;&#31354;&#38388;&#20013;&#36827;&#34892;&#22522;&#20110;&#35780;&#20998;&#30340;&#24314;&#27169;&#65292;&#23637;&#29616;&#20986;&#26356;&#20248;&#36136;&#30340;&#26679;&#26412;&#36136;&#37327;&#21644;&#25913;&#36827;&#30340;&#36895;&#24230;-&#36136;&#37327;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#21512;&#25104;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#35774;&#35745;&#39046;&#22495;&#20173;&#28982;&#36739;&#23569;&#24320;&#21457;&#65292;&#24182;&#19988;&#36890;&#24120;&#20381;&#36182;&#29289;&#29702;&#21551;&#21457;&#27861;&#25110;&#31616;&#21270;&#20551;&#35774;&#12290;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#36125;&#21494;&#26031;&#21518;&#39564;&#37319;&#26679;&#22120;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#37197;&#26041;&#65292;&#29992;&#20110;&#22312;SGMs&#20013;&#21046;&#23450;&#21069;&#21521;&#36807;&#31243;&#65292;&#30830;&#20445;&#25910;&#25947;&#21040;&#25152;&#38656;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25581;&#31034;&#20102;&#20960;&#20010;&#29616;&#26377;&#30340;SGMs&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#23450;&#34920;&#29616;&#24418;&#24335;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30456;&#31354;&#38388;Langevin&#25193;&#25955;&#65288;PSLD&#65289;&#65292;&#23427;&#20381;&#36182;&#20110;&#22522;&#20110;&#35780;&#20998;&#30340;&#24314;&#27169;&#65292;&#22312;&#30001;&#36741;&#21161;&#21464;&#37327;&#22686;&#24378;&#30340;&#31867;&#20284;&#29289;&#29702;&#30456;&#31354;&#38388;&#30340;&#25193;&#23637;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24050;&#24314;&#31435;&#30340;&#22270;&#20687;&#21512;&#25104;&#22522;&#20934;&#19978;&#30340;&#21508;&#31181;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#65292;PSLD&#23637;&#29616;&#20102;&#26356;&#20248;&#36136;&#30340;&#26679;&#26412;&#36136;&#37327;&#21644;&#25913;&#36827;&#30340;&#36895;&#24230;-&#36136;&#37327;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based Generative Models (SGMs) have demonstrated exceptional synthesis outcomes across various tasks. However, the current design landscape of the forward diffusion process remains largely untapped and often relies on physical heuristics or simplifying assumptions. Utilizing insights from the development of scalable Bayesian posterior samplers, we present a complete recipe for formulating forward processes in SGMs, ensuring convergence to the desired target distribution. Our approach reveals that several existing SGMs can be seen as specific manifestations of our framework. Building upon this method, we introduce Phase Space Langevin Diffusion (PSLD), which relies on score-based modeling within an augmented space enriched by auxiliary variables akin to physical phase space. Empirical results exhibit the superior sample quality and improved speed-quality trade-off of PSLD compared to various competing approaches on established image synthesis benchmarks. Remarkably, PSLD achieves 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12461</link><description>&lt;p&gt;
&#20998;&#26512;&#21644;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#24182;&#32534;&#36753;&#26263;&#34255;&#21518;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#21457;&#29616;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#21644;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;PCP&#28040;&#34701;&#25216;&#26415;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#65292;&#25105;&#20204;&#25104;&#21151;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#20013;&#30340;&#27602;&#21270;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#23433;&#20840;&#23041;&#32961;&#65292;&#21487;&#33021;&#23548;&#33268;&#26263;&#34255;&#21518;&#38376;&#30340;&#27169;&#22411;&#12290;&#20851;&#20110;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#22788;&#29702;&#35302;&#21457;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20999;&#25442;&#33267;&#26377;&#27602;&#35821;&#35328;&#65289;&#30340;&#25551;&#36848;&#23578;&#26410;&#25214;&#21040;&#12290;&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;Transformer&#30340;&#26263;&#34255;&#21518;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#30830;&#23450;&#26089;&#26399;&#23618;&#30340;MLP&#27169;&#22359;&#19982;&#21021;&#22987;&#23884;&#20837;&#25237;&#24433;&#32467;&#21512;&#26159;&#21518;&#38376;&#26426;&#21046;&#20013;&#26368;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#21024;&#38500;&#12289;&#25554;&#20837;&#21644;&#20462;&#25913;&#21518;&#38376;&#26426;&#21046;&#65292;&#24182;&#29992;&#24037;&#31243;&#21270;&#26367;&#20195;&#29289;&#38477;&#20302;MLP&#27169;&#22359;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20027;&#35201;&#25104;&#20998;&#30340;&#20302;&#31209;&#30697;&#38453;&#30340;PCP&#28040;&#34701;&#25216;&#26415;&#65292;&#29992;&#20854;&#26367;&#25442;&#21464;&#21387;&#22120;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#26263;&#34255;&#21518;&#38376;&#30340;&#29609;&#20855;&#27169;&#22411;&#12289;&#26263;&#34255;&#21518;&#38376;&#30340;&#22823;&#22411;&#27169;&#22411;&#21644;&#38750;&#26263;&#34255;&#21518;&#38376;&#30340;&#24320;&#28304;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#25913;&#21892;&#21518;&#38376;&#30340;&#36755;&#20986;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poisoning of data sets is a potential security threat to large language models that can lead to backdoored models. A description of the internal mechanisms of backdoored language models and how they process trigger inputs, e.g., when switching to toxic language, has yet to be found. In this work, we study the internal representations of transformer-based backdoored language models and determine early-layer MLP modules as most important for the backdoor mechanism in combination with the initial embedding projection. We use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements that reduce the MLP module outputs to essentials for the backdoor mechanism. To this end, we introduce PCP ablation, where we replace transformer modules with low-rank matrices based on the principal components of their activations. We demonstrate our results on backdoored toy, backdoored large, and non-backdoored open-source models. We show that we can improve the backdoor r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31867;&#21517;&#20026;&#21442;&#19982;&#24615;&#31995;&#32479;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20801;&#35768;&#20010;&#20307;&#22312;&#39044;&#27979;&#26102;&#36873;&#25321;&#20010;&#24615;&#21270;&#65292;&#20174;&#32780;&#20419;&#36827;&#24182;&#30693;&#24773;&#21516;&#24847;&#12289;&#25552;&#39640;&#24615;&#33021;&#21644;&#25968;&#25454;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.03874</link><description>&lt;p&gt;
&#20998;&#31867;&#20013;&#30340;&#21442;&#19982;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Participatory Personalization in Classification. (arXiv:2302.03874v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31867;&#21517;&#20026;&#21442;&#19982;&#24615;&#31995;&#32479;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20801;&#35768;&#20010;&#20307;&#22312;&#39044;&#27979;&#26102;&#36873;&#25321;&#20010;&#24615;&#21270;&#65292;&#20174;&#32780;&#20419;&#36827;&#24182;&#30693;&#24773;&#21516;&#24847;&#12289;&#25552;&#39640;&#24615;&#33021;&#21644;&#25968;&#25454;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#21463;&#20445;&#25252;&#30340;&#12289;&#25935;&#24863;&#30340;&#12289;&#33258;&#25253;&#21578;&#30340;&#25110;&#26114;&#36149;&#30340;&#20449;&#24687;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#20851;&#20110;&#20154;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#20419;&#36827;&#20063;&#19981;&#21578;&#30693;&#20182;&#20204;&#30340;&#21516;&#24847;&#12290;&#20010;&#20307;&#19981;&#33021;&#36873;&#25321;&#19981;&#23558;&#20010;&#20154;&#20449;&#24687;&#25253;&#21578;&#32473;&#27169;&#22411;&#65292;&#20063;&#26080;&#27861;&#30830;&#23450;&#20182;&#20204;&#26159;&#21542;&#20174;&#20010;&#24615;&#21270;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31867;&#21517;&#20026;&#21442;&#19982;&#24615;&#31995;&#32479;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#22312;&#39044;&#27979;&#26102;&#20801;&#35768;&#20010;&#20307;&#36873;&#25321;&#20010;&#24615;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#29992;&#20110;&#20010;&#24615;&#21270;&#30340;&#21442;&#19982;&#24615;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#20998;&#31867;&#32452;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#20020;&#24202;&#39044;&#27979;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23558;&#21442;&#19982;&#24615;&#31995;&#32479;&#19982;&#24120;&#29992;&#30340;&#20010;&#24615;&#21270;&#21644;&#34917;&#20840;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21442;&#19982;&#24615;&#31995;&#32479;&#21487;&#20197;&#20419;&#36827;&#21644;&#30693;&#24773;&#21516;&#24847;&#65292;&#24182;&#22312;&#25152;&#26377;&#25253;&#21578;&#20010;&#20154;&#25968;&#25454;&#30340;&#32676;&#20307;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#25968;&#25454;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are often personalized with information that is protected, sensitive, self-reported, or costly to acquire. These models use information about people but do not facilitate nor inform their consent. Individuals cannot opt out of reporting personal information to a model, nor tell if they benefit from personalization in the first place. We introduce a family of classification models, called participatory systems, that let individuals opt into personalization at prediction time. We present a model-agnostic algorithm to learn participatory systems for personalization with categorical group attributes. We conduct a comprehensive empirical study of participatory systems in clinical prediction tasks, benchmarking them with common approaches for personalization and imputation. Our results demonstrate that participatory systems can facilitate and inform consent while improving performance and data use across all groups who report personal data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#32463;&#36807;&#36731;&#24494;&#20462;&#25913;&#21518;&#65292;&#35813;&#31639;&#27861;&#22312;&#28385;&#36275;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#26102;&#20855;&#26377; O(poly(1/&#949;)) &#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.03770</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#19982;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#21644;&#21333;&#19968;&#31574;&#30053;&#38598;&#20013;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability. (arXiv:2302.03770v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#31163;&#32447;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#32463;&#36807;&#36731;&#24494;&#20462;&#25913;&#21518;&#65292;&#35813;&#31639;&#27861;&#22312;&#28385;&#36275;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#26102;&#20855;&#26377; O(poly(1/&#949;)) &#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#65288;GCRL&#65289;&#26159;&#25351;&#23398;&#20064;&#36890;&#29992;&#25216;&#33021;&#65292;&#20197;&#36798;&#21040;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#31163;&#32447;GCRL&#21482;&#38656;&#35201;&#32431;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#26469;&#25191;&#34892;&#35757;&#32451;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#39069;&#22806;&#30340;&#20132;&#20114;&#12290;&#23613;&#31649;&#31163;&#32447;GCRL&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#24182;&#19988;&#35768;&#22810;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#22823;&#29366;&#24577;&#31354;&#38388;&#19988;&#31163;&#32447;&#25968;&#25454;&#38598;&#20165;&#35206;&#30422;&#25105;&#20204;&#24076;&#26395;&#23398;&#20064;&#30340;&#31574;&#30053;&#30340;&#39640;&#25928;&#31163;&#32447;GCRL&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#24314;&#31435;&#36215;&#26469;&#12290;&#26412;&#25991;&#23545;&#19968;&#31181;&#29616;&#26377;&#32463;&#39564;&#25104;&#21151;&#30340;&#31163;&#32447;GCRL&#31639;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#36731;&#24494;&#20462;&#25913;&#65292;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;$\widetilde{O}(\text{poly}(1/\epsilon))$&#26679;&#26412;&#22797;&#26434;&#24230;&#65288;&#20854;&#20013;$\epsilon$&#26159;&#23398;&#20064;&#31574;&#30053;&#30340;&#26399;&#26395;&#38750;&#26368;&#20248;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $\widetilde{O}(\text{poly}(1/\epsilon))$ sample complexity (where $\epsilon$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#21464;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#21463;&#27604;&#29305;&#29575;&#38480;&#21046;&#30340;DRO&#26041;&#27861;&#65292;&#36890;&#36807;&#20551;&#35774;&#31616;&#21333;&#32676;&#20307;&#20989;&#25968;&#30340;&#26465;&#20214;&#19979;&#65292;&#20197;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#22312;&#20302;&#27604;&#29305;&#29575;&#29305;&#24449;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.02931</link><description>&lt;p&gt;
&#21463;&#27604;&#29305;&#29575;&#38480;&#21046;&#30340;DRO: &#36229;&#36234;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#23545;&#26410;&#30693;&#32676;&#20307;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts. (arXiv:2302.02931v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#21464;&#21270;&#19981;&#30830;&#23450;&#24615;&#30340;&#21463;&#27604;&#29305;&#29575;&#38480;&#21046;&#30340;DRO&#26041;&#27861;&#65292;&#36890;&#36807;&#20551;&#35774;&#31616;&#21333;&#32676;&#20307;&#20989;&#25968;&#30340;&#26465;&#20214;&#19979;&#65292;&#20197;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#22312;&#20302;&#27604;&#29305;&#29575;&#29305;&#24449;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26469;&#35828;&#65292;&#35757;&#32451;&#23545;&#20998;&#24067;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#19968;&#20123;&#40065;&#26834;&#35757;&#32451;&#31639;&#27861;&#65288;&#20363;&#22914;Group DRO&#65289;&#19987;&#27880;&#20110;&#32676;&#20307;&#21464;&#21270;&#65292;&#24182;&#38656;&#35201;&#22312;&#25152;&#26377;&#35757;&#32451;&#28857;&#19978;&#25552;&#20379;&#32676;&#20307;&#20449;&#24687;&#12290;&#20854;&#20182;&#19981;&#38656;&#35201;&#32676;&#20307;&#27880;&#37322;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;CVaR DRO&#65289;&#21487;&#33021;&#36807;&#20110;&#20445;&#23432;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#31616;&#21333;&#22320;&#21152;&#26435;&#35745;&#31639;&#25439;&#22833;&#36739;&#22823;&#30340;&#28857;&#65292;&#32780;&#36825;&#20123;&#28857;&#21487;&#33021;&#26159;&#19968;&#20010;&#34394;&#26500;&#30340;&#38598;&#21512;&#65292;&#19981;&#23545;&#24212;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20219;&#20309;&#26377;&#24847;&#20041;&#30340;&#32676;&#20307;&#65288;&#20363;&#22914;&#65292;&#24403;&#39640;&#25439;&#22833;&#28857;&#26159;&#38543;&#26426;&#38169;&#26631;&#30340;&#35757;&#32451;&#28857;&#26102;&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20551;&#35774;&#19968;&#31181;&#26356;&#24494;&#22937;&#30340;&#32676;&#20307;&#36716;&#21464;&#24418;&#24335;&#26469;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65306;&#22312;&#32473;&#23450;&#26631;&#31614;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#20551;&#35774;&#30495;&#23454;&#30340;&#32676;&#20307;&#20989;&#25968;&#65288;&#32676;&#20307;&#25351;&#31034;&#22120;&#65289;&#26159;&#31616;&#21333;&#30340;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#33021;&#26399;&#26395;&#32676;&#20307;&#36716;&#21464;&#21457;&#29983;&#22312;&#20302;&#27604;&#29305;&#29575;&#29305;&#24449;&#19978;&#65288;&#20363;&#22914;&#22270;&#20687;&#32972;&#26223;&#12289;&#20809;&#29031;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#22312;&#30001;&#36825;&#20123;&#20302;&#27604;&#29305;&#29575;&#29305;&#24449;&#23454;&#29616;&#30340;&#31616;&#21333;&#32676;&#20307;&#20989;&#25968;&#19978;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning models robust to distribution shifts is critical for real-world applications. Some robust training algorithms (e.g., Group DRO) specialize to group shifts and require group information on all training points. Other methods (e.g., CVaR DRO) that do not need group annotations can be overly conservative, since they naively upweight high loss points which may form a contrived set that does not correspond to any meaningful group in the real world (e.g., when the high loss points are randomly mislabeled training points). In this work, we address limitations in prior approaches by assuming a more nuanced form of group shift: conditioned on the label, we assume that the true group function (indicator over group) is simple. For example, we may expect that group shifts occur along low bitrate features (e.g., image background, lighting). Thus, we aim to learn a model that maintains high accuracy on simple group functions realized by these low bitrate features, that need 
&lt;/p&gt;</description></item><item><title>ImageNomer&#26159;&#19968;&#20010;&#21151;&#33021;&#36830;&#25509;&#21644;&#32452;&#23398;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#26131;&#20110;&#23548;&#33322;&#30340;GUI&#21069;&#31471;&#20197;&#35299;&#20915;fMRI&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#28151;&#28102;&#21644;&#36136;&#37327;&#25511;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00767</link><description>&lt;p&gt;
ImageNomer: &#19968;&#31181;&#21151;&#33021;&#36830;&#25509;&#21644;&#32452;&#23398;&#20998;&#26512;&#24037;&#20855;&#30340;&#25551;&#36848;&#21644;&#26696;&#20363;&#30740;&#31350;&#65292;&#35782;&#21035;&#20986;&#19968;&#31181;&#31181;&#26063;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
ImageNomer: description of a functional connectivity and omics analysis tool and case study identifying a race confound. (arXiv:2302.00767v2 [q-bio.PE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00767
&lt;/p&gt;
&lt;p&gt;
ImageNomer&#26159;&#19968;&#20010;&#21151;&#33021;&#36830;&#25509;&#21644;&#32452;&#23398;&#20998;&#26512;&#24037;&#20855;&#65292;&#36890;&#36807;&#25552;&#20379;&#26131;&#20110;&#23548;&#33322;&#30340;GUI&#21069;&#31471;&#20197;&#35299;&#20915;fMRI&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#28151;&#28102;&#21644;&#36136;&#37327;&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29992;&#20110;&#22522;&#20110;fMRI&#30340;&#21151;&#33021;&#36830;&#25509;&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#30340;&#36719;&#20214;&#21253;&#37117;&#20351;&#29992;&#32534;&#31243;&#35821;&#35328;&#30028;&#38754;&#65292;&#32570;&#20047;&#26131;&#20110;&#23548;&#33322;&#30340;GUI&#21069;&#31471;&#12290;&#36825;&#21152;&#21095;&#20102;&#36825;&#20123;&#31867;&#22411;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#20154;&#21475;&#32479;&#35745;&#23398;&#28151;&#28102;&#21644;&#22312;&#39640;&#32500;&#29305;&#24449;&#38754;&#21069;&#30340;&#36136;&#37327;&#25511;&#21046;&#12290;&#21407;&#22240;&#26159;&#20351;&#29992;&#32534;&#31243;&#30028;&#38754;&#26469;&#21019;&#24314;&#24517;&#35201;&#30340;&#21487;&#35270;&#21270;&#22270;&#24418;&#26469;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#30456;&#20851;&#24615;&#12289;&#28151;&#28102;&#25928;&#24212;&#25110;&#36136;&#37327;&#25511;&#21046;&#38382;&#39064;&#22826;&#24930;&#19988;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ImageNomer&#65292;&#19968;&#31181;&#25968;&#25454;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#20801;&#35768;&#26816;&#26597;&#20027;&#20307;&#32423;&#21644;&#38431;&#21015;&#32423;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#22522;&#22240;&#32452;&#23398;&#21644;&#25104;&#20687;&#29305;&#24449;&#12290;&#36719;&#20214;&#22522;&#20110;Python&#65292;&#36816;&#34892;&#22312;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;Docker&#38236;&#20687;&#20013;&#65292;&#24182;&#21253;&#21547;&#19968;&#20010;&#22522;&#20110;&#27983;&#35272;&#22120;&#30340;GUI&#21069;&#31471;&#12290;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#22312;Ph&#12290;
&lt;/p&gt;
&lt;p&gt;
Most packages for the analysis of fMRI-based functional connectivity (FC) and genomic data are used with a programming language interface, lacking an easy-to-navigate GUI frontend. This exacerbates two problems found in these types of data: demographic confounds and quality control in the face of high dimensionality of features. The reason is that it is too slow and cumbersome to use a programming interface to create all the necessary visualizations required to identify all correlations, confounding effects, or quality control problems in a dataset. To remedy this situation, we have developed ImageNomer, a data visualization and analysis tool that allows inspection of both subject-level and cohort-level demographic, genomic, and imaging features. The software is Python-based, runs in a self-contained Docker image, and contains a browser-based GUI frontend. We demonstrate the usefulness of ImageNomer by identifying an unexpected race confound when predicting achievement scores in the Ph
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31163;&#32447;&#31639;&#27861;&#35843;&#25972;&#20026;&#21482;&#38656;&#35201;&#36172;&#24466;&#21453;&#39304;&#30340;&#20122;&#32447;&#24615;&#945;-&#21518;&#24724;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#26102;&#38388;&#30028;T&#30340;&#39044;&#26399;&#32047;&#31215;&#945;-&#21518;&#24724;&#20381;&#36182;&#24230;&#20026;O(T^&#65288;2/3&#65289;log&#65288;T^&#65288;1/3&#65289;&#65289;)&#12290;</title><link>http://arxiv.org/abs/2301.13326</link><description>&lt;p&gt;
&#36866;&#24212;&#31163;&#32447;&#31639;&#27861;&#35299;&#20915;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback. (arXiv:2301.13326v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#31163;&#32447;&#31639;&#27861;&#35843;&#25972;&#20026;&#21482;&#38656;&#35201;&#36172;&#24466;&#21453;&#39304;&#30340;&#20122;&#32447;&#24615;&#945;-&#21518;&#24724;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#26102;&#38388;&#30028;T&#30340;&#39044;&#26399;&#32047;&#31215;&#945;-&#21518;&#24724;&#20381;&#36182;&#24230;&#20026;O(T^&#65288;2/3&#65289;log&#65288;T^&#65288;1/3&#65289;&#65289;)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#21482;&#33021;&#35775;&#38382;&#36172;&#24466;&#21453;&#39304;&#65292;&#24182;&#19988;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#26159;&#38750;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#31163;&#25955;&#36924;&#36817;&#31639;&#27861;&#35843;&#25972;&#20026;&#21482;&#38656;&#35201;&#36172;&#24466;&#21453;&#39304;&#30340;&#20122;&#32447;&#24615;&#945;-&#21518;&#24724;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#26102;&#38388;&#30028;T&#30340;&#39044;&#26399;&#32047;&#31215;&#945;-&#21518;&#24724;&#20381;&#36182;&#24230;&#20026;O(T^&#65288;2/3&#65289;log&#65288;T^&#65288;1/3&#65289;&#65289;)&#12290;&#35813;&#26694;&#26550;&#21482;&#38656;&#35201;&#31163;&#32447;&#31639;&#27861;&#23545;&#20989;&#25968;&#35780;&#20272;&#20013;&#30340;&#23567;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36866;&#24212;&#36807;&#31243;&#29978;&#33267;&#19981;&#38656;&#35201;&#26174;&#24335;&#22320;&#30693;&#36947;&#31163;&#32447;&#36924;&#36817;&#31639;&#27861;--&#31163;&#32447;&#31639;&#27861;&#21487;&#20197;&#34987;&#29992;&#20316;&#40657;&#30418;&#23376;&#23376;&#31243;&#24207;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#65292;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#19981;&#21516;&#24212;&#29992;&#20013;&#12290;&#22312;&#20855;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#23376;&#27169;&#26368;&#22823;&#21270;&#20013;&#65292;&#26032;&#30340;CMAB&#31639;&#27861;&#20248;&#20110;&#24320;&#21457;&#30340;&#23436;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of stochastic, combinatorial multi-armed bandits where the learner only has access to bandit feedback and the reward function can be non-linear. We provide a general framework for adapting discrete offline approximation algorithms into sublinear $\alpha$-regret methods that only require bandit feedback, achieving $\mathcal{O}\left(T^\frac{2}{3}\log(T)^\frac{1}{3}\right)$ expected cumulative $\alpha$-regret dependence on the horizon $T$. The framework only requires the offline algorithms to be robust to small errors in function evaluation. The adaptation procedure does not even require explicit knowledge of the offline approximation algorithm -- the offline algorithm can be used as a black box subroutine. To demonstrate the utility of the proposed framework, the proposed framework is applied to diverse applications in submodular maximization. The new CMAB algorithms for submodular maximization with knapsack constraints outperform a full-bandit method developed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#35745;&#31639;&#20013;&#39640;&#32500;&#24230;&#36229;&#21521;&#37327;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21457;&#29616;&#65292;&#36229;&#21521;&#37327;&#32500;&#24230;&#30340;&#22686;&#21152;&#20250;&#38477;&#20302;&#39640;&#32500;&#35745;&#31639;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;&#22522;&#20110;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#20351;&#29992;&#32500;&#24230;&#26356;&#20302;&#30340;&#20108;&#36827;&#21046;&#36229;&#21521;&#37327;&#30340;&#39640;&#32500;&#35745;&#31639;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#31561;&#29978;&#33267;&#26356;&#20248;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.10902</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#39640;&#32500;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Efficient Hyperdimensional Computing. (arXiv:2301.10902v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#35745;&#31639;&#20013;&#39640;&#32500;&#24230;&#36229;&#21521;&#37327;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21457;&#29616;&#65292;&#36229;&#21521;&#37327;&#32500;&#24230;&#30340;&#22686;&#21152;&#20250;&#38477;&#20302;&#39640;&#32500;&#35745;&#31639;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;&#22522;&#20110;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#20351;&#29992;&#32500;&#24230;&#26356;&#20302;&#30340;&#20108;&#36827;&#21046;&#36229;&#21521;&#37327;&#30340;&#39640;&#32500;&#35745;&#31639;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#31561;&#29978;&#33267;&#26356;&#20248;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#35745;&#31639;&#26159;&#19968;&#31181;&#20351;&#29992;&#39640;&#32500;&#20108;&#36827;&#21046;&#21521;&#37327;&#21644;&#22810;&#25968;&#35268;&#21017;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#65292;&#20855;&#26377;&#33410;&#33021;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#34987;&#35748;&#20026;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#24471;&#39640;&#20934;&#30830;&#24230;&#65292;&#39640;&#32500;&#35745;&#31639;&#26377;&#26102;&#20250;&#20351;&#29992;&#25104;&#21315;&#19978;&#19975;&#32500;&#30340;&#36229;&#21521;&#37327;&#65292;&#36825;&#21487;&#33021;&#25269;&#28040;&#20102;&#20854;&#25928;&#29575;&#20248;&#21183;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#31181;&#39640;&#32500;&#24230;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#23545;&#36229;&#21521;&#37327;&#32500;&#24230;&#19982;&#20934;&#30830;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#36229;&#21521;&#37327;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#37319;&#29992;&#22810;&#25968;&#35268;&#21017;&#30340;&#39640;&#32500;&#35745;&#31639;&#22312;&#26368;&#22351;&#24773;&#20917;&#21644;&#24179;&#22343;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#20250;&#38477;&#20302;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#39640;&#32500;&#35745;&#31639;&#27169;&#22411;&#65292;&#20351;&#29992;&#32500;&#24230;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20302;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#20108;&#36827;&#21046;&#36229;&#21521;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#31561;&#25928;&#29978;&#33267;&#26356;&#20248;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperdimensional computing (HDC) is a method to perform classification that uses binary vectors with high dimensions and the majority rule. This approach has the potential to be energy-efficient and hence deemed suitable for resource-limited platforms due to its simplicity and massive parallelism. However, in order to achieve high accuracy, HDC sometimes uses hypervectors with tens of thousands of dimensions. This potentially negates its efficiency advantage. In this paper, we examine the necessity of such high dimensions and conduct a detailed theoretical analysis of the relationship between hypervector dimensions and accuracy. Our results demonstrate that as the dimension of the hypervectors increases, the worst-case/average-case HDC prediction accuracy with the majority rule decreases. Building on this insight, we develop HDC models that use binary hypervectors with dimensions orders of magnitude lower than those of state-of-the-art HDC models while maintaining equivalent or even im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.10886</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#25506;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#65307;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AIRS&#24615;&#33021;&#21331;&#36234;&#65292;&#33021;&#22815;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AIRS&#30340;&#33258;&#21160;&#20869;&#22312;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21644;&#36866;&#24212;&#24615;&#30340;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20869;&#22312;&#28608;&#21169;&#20197;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;AIRS&#21487;&#20197;&#26681;&#25454;&#23454;&#26102;&#20272;&#35745;&#30340;&#20219;&#21153;&#22238;&#25253;&#20174;&#39044;&#23450;&#20041;&#30340;&#20989;&#25968;&#38598;&#20013;&#36873;&#25321;&#22609;&#36896;&#20989;&#25968;&#65292;&#25552;&#20379;&#21487;&#38752;&#30340;&#25506;&#32034;&#28608;&#21169;&#24182;&#35299;&#20915;&#20559;&#32622;&#30446;&#26631;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20869;&#22312;&#22870;&#21169;&#24037;&#20855;&#21253;&#65292;&#25552;&#20379;&#22810;&#31181;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#30340;&#39640;&#25928;&#21487;&#38752;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#23558;AIRS&#24212;&#29992;&#22312;MiniGrid&#12289;Procgen&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#30340;&#22810;&#39033;&#20219;&#21153;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#22823;&#37327;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;AIRS&#21487;&#20197;&#32988;&#36807;&#22522;&#20934;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and adaptively provides high-quality intrinsic rewards to enhance exploration in reinforcement learning (RL). More specifically, AIRS selects shaping function from a predefined set based on the estimated task return in real-time, providing reliable exploration incentives and alleviating the biased objective problem. Moreover, we develop an intrinsic reward toolkit to provide efficient and reliable implementations of diverse intrinsic reward approaches. We test AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite. Extensive simulation demonstrates that AIRS can outperform the benchmarking schemes and achieve superior performance with simple architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#22320;&#26368;&#23567;&#21270;&#26399;&#26395;&#38271;&#24230;&#65292;&#29992;&#20110;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23548;&#33322;&#12290;&#36825;&#31181;&#26041;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010; Finsler &#24230;&#37327;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#26367;&#20195;&#29616;&#26377;&#30340;&#38543;&#26426;&#25289;&#22238;&#24230;&#37327;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.10010</link><description>&lt;p&gt;
&#29992; Finslerian &#20960;&#20309;&#35782;&#21035;&#28508;&#22312;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Identifying latent distances with Finslerian geometry. (arXiv:2212.10010v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#22320;&#26368;&#23567;&#21270;&#26399;&#26395;&#38271;&#24230;&#65292;&#29992;&#20110;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23548;&#33322;&#12290;&#36825;&#31181;&#26041;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010; Finsler &#24230;&#37327;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#26367;&#20195;&#29616;&#26377;&#30340;&#38543;&#26426;&#25289;&#22238;&#24230;&#37327;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40654;&#26364;&#20960;&#20309;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#24213;&#23618;&#32467;&#26500;&#30340;&#21516;&#26102;&#25506;&#32034;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#37197;&#22791;&#20174;&#25968;&#25454;&#27969;&#24418;&#19978;&#25289;&#22238;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#31995;&#32479;&#22320;&#22312;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#20381;&#36182;&#20110;&#23450;&#20041;&#20026;&#20004;&#28857;&#20043;&#38388;&#26368;&#30701;&#26354;&#32447;&#30340;&#27979;&#22320;&#32447;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#31354;&#38388;&#12289;&#40654;&#26364;&#24230;&#37327;&#21644;&#27979;&#22320;&#32447;&#20063;&#26159;&#38543;&#26426;&#30340;&#12290;&#38543;&#26426;&#23545;&#35937;&#26368;&#22810;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#26368;&#22351;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#21487;&#33021;&#25805;&#20316;&#30340;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#26399;&#26395;&#36817;&#20284;&#38543;&#26426;&#25289;&#22238;&#24230;&#37327;&#12290;&#20294;&#26159;&#20174;&#36825;&#20010;&#26399;&#26395;&#40654;&#26364;&#24230;&#37327;&#23548;&#20986;&#30340;&#27979;&#22320;&#32447;&#19981;&#23545;&#24212;&#20110;&#26399;&#26395;&#30340;&#38271;&#24230;&#26368;&#23567;&#21270;&#26354;&#32447;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#24230;&#37327;&#65292;&#20854;&#27979;&#22320;&#32447;&#26126;&#30830;&#22320;&#26368;&#23567;&#21270;&#20102;&#25289;&#22238;&#24230;&#37327;&#30340;&#26399;&#26395;&#38271;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#24230;&#37327;&#23450;&#20041;&#20102;&#19968;&#20010; Finsler &#24230;&#37327;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#24230;&#37327;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Riemannian geometry provides us with powerful tools to explore the latent space of generative models while preserving the underlying structure of the data. The latent space can be equipped it with a Riemannian metric, pulled back from the data manifold. With this metric, we can systematically navigate the space relying on geodesics defined as the shortest curves between two points. Generative models are often stochastic, causing the data space, the Riemannian metric, and the geodesics, to be stochastic as well. Stochastic objects are at best impractical, and at worst impossible, to manipulate. A common solution is to approximate the stochastic pullback metric by its expectation. But the geodesics derived from this expected Riemannian metric do not correspond to the expected length-minimising curves. In this work, we propose another metric whose geodesics explicitly minimise the expected length of the pullback metric. We show this metric defines a Finsler metric, and we compare it with 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;Text-to-SQL&#27169;&#22411;&#23384;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#28431;&#27934;&#33021;&#22815;&#34987;&#24694;&#24847;&#21033;&#29992;&#20135;&#29983;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#21830;&#19994;&#24212;&#29992;&#21644;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#24847;&#22312;&#24341;&#36215;&#23398;&#26415;&#30028;&#23545;NLP&#31639;&#27861;&#30456;&#20851;&#30340;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.15363</link><description>&lt;p&gt;
&#20851;&#20110;Text-to-SQL&#27169;&#22411;&#30340;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On the Security Vulnerabilities of Text-to-SQL Models. (arXiv:2211.15363v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15363
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;Text-to-SQL&#27169;&#22411;&#23384;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#28431;&#27934;&#33021;&#22815;&#34987;&#24694;&#24847;&#21033;&#29992;&#20135;&#29983;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#21830;&#19994;&#24212;&#29992;&#21644;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#24847;&#22312;&#24341;&#36215;&#23398;&#26415;&#30028;&#23545;NLP&#31639;&#27861;&#30456;&#20851;&#30340;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#65292;&#20294;&#36825;&#20123;&#24369;&#28857;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#36719;&#20214;&#23433;&#20840;&#23041;&#32961;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;&#24120;&#29992;&#20110;&#21019;&#24314;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#24211;&#25509;&#21475;&#30340;Text-to-SQL&#31995;&#32479;&#36827;&#34892;&#20102;&#28431;&#27934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20845;&#20010;&#21830;&#19994;&#24212;&#29992;&#20013;&#30340;Text-to-SQL&#27169;&#22359;&#21487;&#20197;&#34987;&#25805;&#32437;&#20197;&#20135;&#29983;&#24694;&#24847;&#20195;&#30721;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#25968;&#25454;&#27844;&#28431;&#21644;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35777;&#26126;NLP&#27169;&#22411;&#21487;&#20197;&#34987;&#21033;&#29992;&#20026;&#25915;&#20987;&#21521;&#37327;&#30340;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22235;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#23545;Text-to-SQL&#31995;&#32479;&#36827;&#34892;&#30452;&#25509;&#21518;&#38376;&#25915;&#20987;&#21487;&#20197;&#36798;&#21040;100&#65285;&#30340;&#25104;&#21151;&#29575;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24341;&#36215;&#23398;&#26415;&#30028;&#23545;&#19982;NLP&#31639;&#27861;&#30456;&#20851;&#30340;&#28508;&#22312;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although it has been demonstrated that Natural Language Processing (NLP) algorithms are vulnerable to deliberate attacks, the question of whether such weaknesses can lead to software security threats is under-explored. To bridge this gap, we conducted vulnerability tests on Text-to-SQL systems that are commonly used to create natural language interfaces to databases. We showed that the Text-to-SQL modules within six commercial applications can be manipulated to produce malicious code, potentially leading to data breaches and Denial of Service attacks. This is the first demonstration that NLP models can be exploited as attack vectors in the wild. In addition, experiments using four open-source language models verified that straightforward backdoor attacks on Text-to-SQL systems achieve a 100% success rate without affecting their performance. The aim of this work is to draw the community's attention to potential software security issues associated with NLP algorithms and encourage explor
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#26159;&#23558;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#25972;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#25512;&#29702;&#21644;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25104;&#21151;&#24212;&#29992;&#26696;&#20363;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.15889</link><description>&lt;p&gt;
&#36208;&#21521;&#25968;&#25454;&#21644;&#30693;&#35782;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15889
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#26159;&#23558;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#25972;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#25512;&#29702;&#21644;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25104;&#21151;&#24212;&#29992;&#26696;&#20363;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#65288;NeSy&#65289;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#36861;&#27714;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#30340;&#25972;&#21512;&#12290;&#30001;&#20110;NeSy&#22312;&#31526;&#21495;&#34920;&#31034;&#30340;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#23427;&#21487;&#33021;&#25104;&#20026;&#19979;&#19968;&#20195;AI&#30340;&#20652;&#21270;&#21058;&#12290;&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;NeSy&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21382;&#21490;&#65292;&#28085;&#30422;&#20102;&#26089;&#26399;&#24037;&#20316;&#21644;&#22522;&#30784;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#32972;&#26223;&#27010;&#24565;&#65292;&#24182;&#30830;&#23450;&#20102;&#25512;&#21160;NeSy&#21457;&#23637;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25353;&#29031;&#20960;&#20010;&#20027;&#35201;&#29305;&#24449;&#23545;&#36817;&#26399;&#30340;&#37324;&#31243;&#30865;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#31070;&#32463;&#31526;&#21495;&#25972;&#21512;&#12289;&#30693;&#35782;&#34920;&#31034;&#12289;&#30693;&#35782;&#23884;&#20837;&#21644;&#21151;&#33021;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31616;&#35201;&#35752;&#35770;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20197;&#21450;NeSy&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-symbolic computing (NeSy), which pursues the integration of the symbolic and statistical paradigms of cognition, has been an active research area of Artificial Intelligence (AI) for many years. As NeSy shows promise of reconciling the advantages of reasoning and interpretability of symbolic representation and robust learning in neural networks, it may serve as a catalyst for the next generation of AI. In the present paper, we provide a systematic overview of the recent developments and important contributions of NeSy research. Firstly, we introduce study history of this area, covering early work and foundations. We further discuss background concepts and identify key driving factors behind the development of NeSy. Afterward, we categorize recent landmark approaches along several main characteristics that underline this research paradigm, including neural-symbolic integration, knowledge representation, knowledge embedding, and functionality. Next, we briefly discuss the successfu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#30340;&#25915;&#20987;&#20197;&#21450;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#36867;&#36991;&#38450;&#24481;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#23454;&#23041;&#32961;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.13660</link><description>&lt;p&gt;
SpacePhish: &#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#22120;&#30340;&#25915;&#20987;&#30340;&#36867;&#36991;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
SpacePhish: The Evasion-space of Adversarial Attacks against Phishing Website Detectors using Machine Learning. (arXiv:2210.13660v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#30340;&#25915;&#20987;&#20197;&#21450;&#38024;&#23545;&#36825;&#20123;&#25915;&#20987;&#30340;&#36867;&#36991;&#38450;&#24481;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29616;&#23454;&#23041;&#32961;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#25991;&#29486;&#35201;&#20040;&#23637;&#31034;&#33021;&#22815;&#30772;&#22351;&#25152;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#35201;&#20040;&#23637;&#31034;&#33021;&#22815;&#25269;&#24481;&#22823;&#37096;&#20998;&#25915;&#20987;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#32771;&#34385;&#25915;&#20987;&#25110;&#38450;&#24481;&#30340;&#23454;&#38469;&#21487;&#34892;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#25239;&#26679;&#26412;&#36890;&#24120;&#26159;&#22312;&#8220;&#29305;&#24449;&#31354;&#38388;&#8221;&#20013;&#29983;&#25104;&#30340;&#65292;&#36825;&#20351;&#24471;&#30456;&#20851;&#35780;&#20272;&#30340;&#20215;&#20540;&#20540;&#24471;&#24576;&#30097;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#30446;&#21069;&#30340;&#24773;&#20917;&#19981;&#20801;&#35768;&#20272;&#35745;&#23545;&#25239;&#25915;&#20987;&#25152;&#24102;&#26469;&#30340;&#30495;&#23454;&#23041;&#32961;&#65292;&#36825;&#23548;&#33268;&#32570;&#20047;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#26412;&#25991;&#26088;&#22312;&#28548;&#28165;&#36825;&#31181;&#22256;&#24785;&#12290;&#36890;&#36807;&#32771;&#34385;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558;&#8220;&#36867;&#36991;&#31354;&#38388;&#8221;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#33021;&#22815;&#24341;&#20837;&#23545;&#25239;&#25200;&#21160;&#20197;&#27450;&#39575;&#26426;&#22120;&#23398;&#20064;-&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#30340;&#31354;&#38388;-&#35777;&#26126;&#21363;&#20351;&#22312;&#8220;&#29305;&#24449;&#31354;&#38388;&#8221;&#20013;&#30340;&#25200;&#21160;&#20063;&#26159;&#26377;&#29992;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25551;&#36848;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;-&#38035;&#40060;&#32593;&#31449;&#26816;&#27979;&#30340;&#36867;&#36991;&#25915;&#20987;&#30340;&#29616;&#23454;&#23041;&#32961;&#27169;&#22411;&#65292;&#36825;&#31181;&#25915;&#20987;&#23481;&#26131;&#23454;&#26045;&#65292;&#22240;&#27492;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing literature on adversarial Machine Learning (ML) focuses either on showing attacks that break every ML model, or defenses that withstand most attacks. Unfortunately, little consideration is given to the actual feasibility of the attack or the defense. Moreover, adversarial samples are often crafted in the "feature-space", making the corresponding evaluations of questionable value. Simply put, the current situation does not allow to estimate the actual threat posed by adversarial attacks, leading to a lack of secure ML systems.  We aim to clarify such confusion in this paper. By considering the application of ML for Phishing Website Detection (PWD), we formalize the "evasion-space" in which an adversarial perturbation can be introduced to fool a ML-PWD -- demonstrating that even perturbations in the "feature-space" are useful. Then, we propose a realistic threat model describing evasion attacks against ML-PWD that are cheap to stage, and hence intrinsically more attractive for r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#24178;&#25200;&#19979;&#30340;&#38754;&#26495;&#25968;&#25454;&#22240;&#26524;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#32593;&#32476;&#24178;&#25200;&#30340;&#21512;&#25104;&#25511;&#21046;&#21644;&#21512;&#25104;&#24178;&#39044;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35299;&#20915;&#28322;&#20986;&#21644;&#26410;&#35266;&#27979;&#28151;&#26434;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2210.11355</link><description>&lt;p&gt;
&#32593;&#32476;&#21512;&#25104;&#24178;&#39044;&#65306;&#22312;&#32593;&#32476;&#24178;&#25200;&#19979;&#36827;&#34892;&#38754;&#26495;&#25968;&#25454;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Network Synthetic Interventions: A Causal Framework for Panel Data Under Network Interference. (arXiv:2210.11355v2 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#24178;&#25200;&#19979;&#30340;&#38754;&#26495;&#25968;&#25454;&#22240;&#26524;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#32593;&#32476;&#24178;&#25200;&#30340;&#21512;&#25104;&#25511;&#21046;&#21644;&#21512;&#25104;&#24178;&#39044;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35299;&#20915;&#28322;&#20986;&#21644;&#26410;&#35266;&#27979;&#28151;&#26434;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21512;&#25104;&#25511;&#21046;&#21644;&#21512;&#25104;&#24178;&#39044;&#26041;&#27861;&#25512;&#24191;&#21040;&#32593;&#32476;&#24178;&#25200;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#20272;&#35745;&#21333;&#20803;&#29305;&#23450;&#30340;&#28508;&#22312;&#32467;&#26524;&#65292;&#22312;&#21333;&#20301;&#20043;&#38388;&#23384;&#22312;&#28322;&#20986;&#21644;&#26410;&#35266;&#27979;&#28151;&#26434;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#65292;&#23427;&#32771;&#34385;&#20102;&#32593;&#32476;&#24178;&#25200;&#65292;&#24182;&#25512;&#24191;&#20102;&#36890;&#24120;&#22312;&#38754;&#26495;&#25968;&#25454;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#22240;&#23376;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#22120;&#65292;&#21517;&#20026;&#32593;&#32476;&#21512;&#25104;&#24178;&#39044; (NSI)&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#22312;&#20219;&#24847;&#19968;&#32452;&#23545;&#32593;&#32476;&#36827;&#34892;&#21453;&#20107;&#23454;&#22788;&#29702;&#26102;&#19968;&#33268;&#22320;&#20272;&#35745;&#21333;&#20301;&#30340;&#24179;&#22343;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#35813;&#20272;&#35745;&#22120;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#26159;&#27491;&#24577;&#20998;&#24067;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#26377;&#25928;&#24615;&#27979;&#35797;&#65292;&#20197;&#30830;&#23450;NSI&#20272;&#35745;&#22120;&#26159;&#21542;&#21487;&#38752;&#22320;&#25512;&#24191;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#20445;&#35777;&#20102;NSI&#20272;&#35745;&#22120;&#20135;&#29983;&#20934;&#30830;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#65292;&#24182;&#20998;&#26512;&#20102;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a generalization of the synthetic controls and synthetic interventions methodology to incorporate network interference. We consider the estimation of unit-specific potential outcomes from panel data in the presence of spillover across units and unobserved confounding. Key to our approach is a novel latent factor model that takes into account network interference and generalizes the factor models typically used in panel data settings. We propose an estimator, Network Synthetic Interventions (NSI), and show that it consistently estimates the mean outcomes for a unit under an arbitrary set of counterfactual treatments for the network. We further establish that the estimator is asymptotically normal. We furnish two validity tests for whether the NSI estimator reliably generalizes to produce accurate counterfactual estimates. We provide a novel graph-based experiment design that guarantees the NSI estimator produces accurate counterfactual estimates, and also analyze the sample c
&lt;/p&gt;</description></item><item><title>MMTSA&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#23427;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;IMU&#25968;&#25454;&#36716;&#25442;&#20026;&#26102;&#24207;&#32467;&#26500;&#20445;&#25345;&#30340;&#28784;&#24230;&#22270;&#20687;&#65292;&#24182;&#37319;&#29992;&#22810;&#27169;&#24335;&#31232;&#30095;&#37319;&#26679;&#21644;&#27573;&#38388;&#20851;&#27880;&#27169;&#22359;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22810;&#27169;&#24335;&#34701;&#21512;&#12290;&#22312;&#35780;&#20272;&#20013;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.09222</link><description>&lt;p&gt;
MMTSA: &#22810;&#27169;&#24335;&#26102;&#24207;&#27573;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#39640;&#25928;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human Activity Recognition. (arXiv:2210.09222v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09222
&lt;/p&gt;
&lt;p&gt;
MMTSA&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#12290;&#23427;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#36890;&#36807;&#23558;IMU&#25968;&#25454;&#36716;&#25442;&#20026;&#26102;&#24207;&#32467;&#26500;&#20445;&#25345;&#30340;&#28784;&#24230;&#22270;&#20687;&#65292;&#24182;&#37319;&#29992;&#22810;&#27169;&#24335;&#31232;&#30095;&#37319;&#26679;&#21644;&#27573;&#38388;&#20851;&#27880;&#27169;&#22359;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#22810;&#27169;&#24335;&#34701;&#21512;&#12290;&#22312;&#35780;&#20272;&#20013;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#20026;&#24320;&#21457;&#20934;&#30830;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#20114;&#34917;&#20449;&#24687;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#36127;&#33655;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#39640;&#25928;&#22810;&#27169;&#24335;&#31070;&#32463;&#26550;&#26500;&#65292;&#31216;&#20026;&#22810;&#27169;&#24335;&#26102;&#24207;&#27573;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;MMTSA&#65289;&#12290;MMTSA&#39318;&#20808;&#20351;&#29992;&#26684;&#25289;&#22982;&#35282;&#22330;&#65288;GAF&#65289;&#23558;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#20256;&#24863;&#22120;&#25968;&#25454;&#36716;&#25442;&#20026;&#26102;&#24207;&#21644;&#32467;&#26500;&#20445;&#25345;&#30340;&#28784;&#24230;&#22270;&#20687;&#65292;&#34920;&#31034;&#20154;&#20307;&#27963;&#21160;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;MMTSA&#24212;&#29992;&#22810;&#27169;&#24335;&#31232;&#30095;&#37319;&#26679;&#26041;&#27861;&#26469;&#20943;&#23569;&#25968;&#25454;&#20887;&#20313;&#12290;&#26368;&#21518;&#65292;MMTSA&#37319;&#29992;&#19968;&#31181;&#27573;&#38388;&#20851;&#27880;&#27169;&#22359;&#26469;&#23454;&#29616;&#39640;&#25928;&#22810;&#27169;&#24335;&#34701;&#21512;&#12290;&#20351;&#29992;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;MMTSA&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MMAct&#25968;&#25454;&#38598;&#19978;&#36328;&#21463;&#35797;&#32773;F1&#20998;&#25968;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36798;&#21040;11.13%&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sensors provide complementary information to develop accurate machine-learning methods for human activity recognition (HAR), but introduce significantly higher computational load, which reduces efficiency. This paper proposes an efficient multimodal neural architecture for HAR using an RGB camera and inertial measurement units (IMUs) called Multimodal Temporal Segment Attention Network (MMTSA). MMTSA first transforms IMU sensor data into a temporal and structure-preserving gray-scale image using the Gramian Angular Field (GAF), representing the inherent properties of human activities. MMTSA then applies a multimodal sparse sampling method to reduce data redundancy. Lastly, MMTSA adopts an inter-segment attention module for efficient multimodal fusion. Using three well-established public datasets, we evaluated MMTSA's effectiveness and efficiency in HAR. Results show that our method achieves superior performance improvements 11.13% of cross-subject F1-score on the MMAct datas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#26041;&#27861;&#26469;&#35843;&#21644;&#20219;&#20309;&#31867;&#22411;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#19979;&#32780;&#19978;&#37325;&#35201;&#24615;&#25277;&#26679;&#8221;&#30340;&#39640;&#25928;&#25277;&#26679;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#26102;&#38388;&#23618;&#27425;&#30340;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#19982;&#22522;&#26412;&#27010;&#29575;&#39044;&#27979;&#30456;&#27604;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.02286</link><description>&lt;p&gt;
&#23454;&#25968;&#20540;&#21644;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#25928;&#27010;&#29575;&#35843;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient probabilistic reconciliation of forecasts for real-valued and count time series. (arXiv:2210.02286v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#26041;&#27861;&#26469;&#35843;&#21644;&#20219;&#20309;&#31867;&#22411;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#19979;&#32780;&#19978;&#37325;&#35201;&#24615;&#25277;&#26679;&#8221;&#30340;&#39640;&#25928;&#25277;&#26679;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#26102;&#38388;&#23618;&#27425;&#30340;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#19982;&#22522;&#26412;&#27010;&#29575;&#39044;&#27979;&#30456;&#27604;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#24456;&#24120;&#35265;&#12290;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#38656;&#35201;&#26159;&#19968;&#33268;&#30340;&#65292;&#20063;&#23601;&#26159;&#28385;&#36275;&#23618;&#27425;&#32467;&#26500;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#24378;&#21046;&#19968;&#33268;&#24615;&#30340;&#26368;&#27969;&#34892;&#25216;&#26415;&#34987;&#31216;&#20026;&#35843;&#21644;&#65292;&#23427;&#35843;&#25972;&#20102;&#20026;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#35745;&#31639;&#30340;&#22522;&#26412;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#27010;&#29575;&#35843;&#21644;&#30340;&#30740;&#31350;&#23384;&#22312;&#20960;&#20010;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26465;&#20214;&#26041;&#27861;&#26469;&#35843;&#21644;&#20219;&#20309;&#31867;&#22411;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33258;&#19979;&#32780;&#19978;&#37325;&#35201;&#24615;&#25277;&#26679;&#8221;&#30340;&#26032;&#31639;&#27861;&#65292;&#26469;&#39640;&#25928;&#22320;&#20174;&#35843;&#21644;&#21518;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#20219;&#20309;&#22522;&#26412;&#39044;&#27979;&#20998;&#24067;&#65306;&#31163;&#25955;&#30340;&#12289;&#36830;&#32493;&#30340;&#65292;&#25110;&#32773;&#20197;&#26679;&#26412;&#24418;&#24335;&#25552;&#20379;&#30340;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#24456;&#22823;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;&#22312;&#20960;&#20010;&#26102;&#38388;&#23618;&#27425;&#30340;&#23454;&#39564;&#20013;&#65292;&#26174;&#31034;&#20986;&#19982;&#22522;&#26412;&#27010;&#29575;&#39044;&#27979;&#30456;&#27604;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical time series are common in several applied fields. The forecasts for these time series are required to be coherent, that is, to satisfy the constraints given by the hierarchy. The most popular technique to enforce coherence is called reconciliation, which adjusts the base forecasts computed for each time series. However, recent works on probabilistic reconciliation present several limitations. In this paper, we propose a new approach based on conditioning to reconcile any type of forecast distribution. We then introduce a new algorithm, called Bottom-Up Importance Sampling, to efficiently sample from the reconciled distribution. It can be used for any base forecast distribution: discrete, continuous, or in the form of samples, providing a major speedup compared to the current methods. Experiments on several temporal hierarchies show a significant improvement over base probabilistic forecasts.
&lt;/p&gt;</description></item><item><title>GP-net &#21487;&#20197;&#20174;&#28789;&#27963;&#30340;&#35270;&#35282;&#65292;&#20363;&#22914;&#31227;&#21160;&#26426;&#26800;&#33218;&#25152;&#20307;&#39564;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#20845;&#33258;&#30001;&#24230;&#30340;&#25235;&#21462;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#23427;&#23454;&#29616;&#20102; 51.8% &#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#26426;&#22120;&#20154;&#25235;&#21462;&#25216;&#26415;&#30340;&#26368;&#26032;&#26041;&#27861;&#25104;&#21151;&#29575;&#26356;&#20302;&#65292;&#38656;&#35201;&#23450;&#20041;&#24037;&#20316;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2209.10404</link><description>&lt;p&gt;
GP-net: &#28789;&#27963;&#30340;&#35270;&#35282;&#25235;&#21462;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
GP-net: Flexible Viewpoint Grasp Proposal. (arXiv:2209.10404v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10404
&lt;/p&gt;
&lt;p&gt;
GP-net &#21487;&#20197;&#20174;&#28789;&#27963;&#30340;&#35270;&#35282;&#65292;&#20363;&#22914;&#31227;&#21160;&#26426;&#26800;&#33218;&#25152;&#20307;&#39564;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#20845;&#33258;&#30001;&#24230;&#30340;&#25235;&#21462;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#23427;&#23454;&#29616;&#20102; 51.8% &#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#26426;&#22120;&#20154;&#25235;&#21462;&#25216;&#26415;&#30340;&#26368;&#26032;&#26041;&#27861;&#25104;&#21151;&#29575;&#26356;&#20302;&#65292;&#38656;&#35201;&#23450;&#20041;&#24037;&#20316;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Grasp Proposal Network (GP-net) &#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#28789;&#27963;&#30340;&#35270;&#35282;&#65292;&#20363;&#22914;&#31227;&#21160;&#26426;&#26800;&#33218;&#25152;&#20307;&#39564;&#30340;&#35270;&#35282;&#65292;&#29983;&#25104;&#20845;&#33258;&#30001;&#24230;&#30340;&#25235;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#28145;&#24230;&#22270;&#20687;&#21644;&#26631;&#27880;&#25235;&#21462;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451; GP-net&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992; EGAD! &#25235;&#21462;&#22522;&#20934;&#27979;&#35797;&#23545; GP-net &#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#20004;&#31181;&#24120;&#29992;&#31639;&#27861;&#8212;&#8212;Volumetric Grasping Network (VGN) &#21644; Grasp Pose Detection package (GPD) &#36827;&#34892;&#27604;&#36739;&#65292;&#22312; PAL TIAGo &#31227;&#21160;&#26426;&#22120;&#20154;&#19978;&#12290;&#19982;&#26426;&#22120;&#20154;&#25235;&#21462;&#25216;&#26415;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#65292;GP-net &#21487;&#20197;&#29992;&#20110;&#20174;&#28789;&#27963;&#30340;&#26410;&#30693;&#35270;&#35282;&#25235;&#21462;&#23545;&#35937;&#65292;&#32780;&#26080;&#38656;&#23450;&#20041;&#24037;&#20316;&#31354;&#38388;&#65292;&#24182;&#19988;&#25235;&#21462;&#25104;&#21151;&#29575;&#36798;&#21040; 51.8%&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;VGN &#20026; 51.1%&#65292;GPD &#20026; 33.6%&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010; ROS &#21253;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32593;&#22336;&#20026; https://aucoroboticsmu.github.io/GP-net/&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Grasp Proposal Network (GP-net), a Convolutional Neural Network model which can generate 6-DOF grasps from flexible viewpoints, e.g. as experienced by mobile manipulators. To train GP-net, we synthetically generate a dataset containing depth-images and ground-truth grasp information. In real-world experiments we use the EGAD! grasping benchmark to evaluate GP-net against two commonly used algorithms, the Volumetric Grasping Network (VGN) and the Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In contrast to the state-of-the-art methods in robotic grasping, GP-net can be used for grasping objects from flexible, unknown viewpoints without the need to define the workspace and achieves a grasp success of 51.8% compared to 51.1% for VGN and 33.6% for GPD. We provide a ROS package along with our code and pre-trained models at https://aucoroboticsmu.github.io/GP-net/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20934;&#31639;&#26415;&#28151;&#21512;&#12289;&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;Bregman&#20449;&#24687;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#23494;&#24230;&#20989;&#25968;&#30340;&#21333;&#35843;&#23884;&#20837;&#19979;&#20351;&#29992;Bregman&#25955;&#24230;&#65292;&#25105;&#20204;&#23558;&#24120;&#35265;&#30340;&#25955;&#24230;&#20989;&#25968;&#19982;&#36864;&#28779;&#36335;&#24452;&#19978;&#30340;&#20013;&#38388;&#23494;&#24230;&#20851;&#32852;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2209.07481</link><description>&lt;p&gt;
&#20934;&#31639;&#26415;&#28151;&#21512;&#12289;&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;Bregman&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman Information. (arXiv:2209.07481v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20934;&#31639;&#26415;&#28151;&#21512;&#12289;&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;Bregman&#20449;&#24687;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#23494;&#24230;&#20989;&#25968;&#30340;&#21333;&#35843;&#23884;&#20837;&#19979;&#20351;&#29992;Bregman&#25955;&#24230;&#65292;&#25105;&#20204;&#23558;&#24120;&#35265;&#30340;&#25955;&#24230;&#20989;&#25968;&#19982;&#36864;&#28779;&#36335;&#24452;&#19978;&#30340;&#20013;&#38388;&#23494;&#24230;&#20851;&#32852;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#29992;&#20110;&#20174;&#22797;&#26434;&#20998;&#24067;&#20013;&#37319;&#26679;&#21644;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#36890;&#24120;&#27169;&#25311;&#27839;&#30528;&#36830;&#25509;&#21487;&#36319;&#36394;&#21021;&#22987;&#20998;&#24067;&#21644;&#30446;&#26631;&#23494;&#24230;&#30340;&#36864;&#28779;&#36335;&#24452;&#30340;&#20013;&#38388;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#20934;&#31639;&#26415;&#24179;&#22343;&#26500;&#24314;&#20102;&#36864;&#28779;&#36335;&#24452;&#65292;&#24182;&#35299;&#37322;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#20013;&#38388;&#23494;&#24230;&#26159;&#26368;&#23567;&#21270;&#26399;&#26395;&#25955;&#24230;&#21040;&#31471;&#28857;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#23494;&#24230;&#20989;&#25968;&#30340;&#21333;&#35843;&#23884;&#20837;&#19979;&#20351;&#29992;Bregman&#25955;&#24230;&#23545;&#36825;&#20010;&#8220;&#36136;&#24515;&#8221;&#24615;&#36136;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#20174;&#32780;&#23558;&#24120;&#35265;&#30340;&#25955;&#24230;&#65288;&#22914;Amari&#21644;Renyi&#30340;alpha&#25955;&#24230;&#12289;&#65288;alpha&#65292;beta&#65289;&#25955;&#24230;&#21644;Jensen-Shannon&#25955;&#24230;&#65289;&#19982;&#27839;&#30528;&#36864;&#28779;&#36335;&#24452;&#30340;&#20013;&#38388;&#23494;&#24230;&#20851;&#32852;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#21442;&#25968;&#21270;&#26063;&#12289;&#20934;&#31639;&#26415;&#24179;&#22343;&#21644;&#25955;&#24230;&#20989;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#29992;&#20102;Zhang&#30340;rho-tau Bregman&#25955;&#24230;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov Chain Monte Carlo methods for sampling from complex distributions and estimating normalization constants often simulate samples from a sequence of intermediate distributions along an annealing path, which bridges between a tractable initial distribution and a target density of interest. Prior work has constructed annealing paths using quasi-arithmetic means, and interpreted the resulting intermediate densities as minimizing an expected divergence to the endpoints. We provide a comprehensive analysis of this 'centroid' property using Bregman divergences under a monotonic embedding of the density function, thereby associating common divergences such as Amari's and Renyi's ${\alpha}$-divergences, ${(\alpha,\beta)}$-divergences, and the Jensen-Shannon divergence with intermediate densities along an annealing path. Our analysis highlights the interplay between parametric families, quasi-arithmetic means, and divergence functions using the rho-tau Bregman divergence framework of Zhang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AEnbMIMOCQR&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.14219</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27493;&#40077;&#22411;&#33258;&#36866;&#24212;&#24322;&#26041;&#24046;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting. (arXiv:2207.14219v7 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AEnbMIMOCQR&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#65292;&#24182;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#31639;&#27861;&#65292;&#21517;&#20026;&#33258;&#36866;&#24212;&#38598;&#25104;&#25209;&#37327;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#40077;&#22411;&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;AEnbMIMOCQR&#65289;&#65292;&#20351;&#24471;&#39044;&#27979;&#32773;&#33021;&#22815;&#20197;&#20998;&#24067;&#26080;&#20851;&#30340;&#26041;&#24335;&#29983;&#25104;&#22266;&#23450;&#39044;&#35774;&#22833;&#37197;&#29575;&#30340;&#22810;&#27493;&#40077;&#22411;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#40077;&#22411;&#39044;&#27979;&#21407;&#29702;&#65292;&#20294;&#19981;&#38656;&#35201;&#25968;&#25454;&#25286;&#20998;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#25968;&#25454;&#19981;&#21487;&#20114;&#25442;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#20379;&#25509;&#36817;&#31934;&#30830;&#30340;&#35206;&#30422;&#29575;&#12290;&#27492;&#22806;&#65292;&#25152;&#24471;&#21040;&#30340;&#39044;&#27979;&#21306;&#38388;&#22312;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#32463;&#39564;&#35777;&#26126;&#26377;&#25928;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#24322;&#26041;&#24046;&#24615;&#12290;AEnbMIMOCQR&#34987;&#35774;&#35745;&#25104;&#23545;&#20998;&#24067;&#36716;&#21464;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#20854;&#39044;&#27979;&#21306;&#38388;&#22312;&#26080;&#38480;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#21487;&#38752;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#19981;&#20999;&#23454;&#38469;&#30340;&#20005;&#26684;&#20551;&#35774;&#12290;&#36890;&#36807;&#31995;&#32479;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#40077;&#22411;&#39044;&#27979;&#20013;&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel model-agnostic algorithm called adaptive ensemble batch multi-input multi-output conformalized quantile regression (AEnbMIMOCQR} that enables forecasters to generate multi-step ahead prediction intervals for a fixed pre-specified miscoverage rate in a distribution-free manner. Our method is grounded on conformal prediction principles, however, it does not require data splitting and provides close to exact coverage even when the data is not exchangeable. Moreover, the resulting prediction intervals, besides being empirically valid along the forecast horizon, do not neglect heteroscedasticity. AEnbMIMOCQR is designed to be robust to distribution shifts, which means that its prediction intervals remain reliable over an unlimited period of time, without entailing retraining or imposing unrealistic strict assumptions on the data-generating process. Through methodically experimentation, we demonstrate that our approach outperforms other competitive methods on bo
&lt;/p&gt;</description></item><item><title>MemSAC&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#26679;&#26412;&#19968;&#33268;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26679;&#26412;&#32423;&#30456;&#20284;&#24615;&#23454;&#29616;&#21028;&#21035;&#36716;&#31227;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2207.12389</link><description>&lt;p&gt;
MemSAC: &#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#35760;&#24518;&#22686;&#24378;&#26679;&#26412;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised Domain Adaptation. (arXiv:2207.12389v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12389
&lt;/p&gt;
&lt;p&gt;
MemSAC&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#26679;&#26412;&#19968;&#33268;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26679;&#26412;&#32423;&#30456;&#20284;&#24615;&#23454;&#29616;&#21028;&#21035;&#36716;&#31227;&#65292;&#24182;&#19988;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#25361;&#25112;&#65292;&#22914;&#31867;&#38388;&#21306;&#20998;&#24230;&#23567;&#65292;&#29616;&#26377;&#30340;&#20165;&#20381;&#36182;&#20110;&#22495;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MemSAC&#65292;&#23427;&#21033;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26679;&#26412;&#32423;&#30456;&#20284;&#24615;&#23454;&#29616;&#21028;&#21035;&#36716;&#31227;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#22823;&#37327;&#31867;&#21035;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#26631;&#35760;&#28304;&#22495;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#23454;&#20363;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20851;&#31995;&#65292;&#36866;&#21512;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#31867;&#21035;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#30340;&#21464;&#20307;&#65292;&#20197;&#20419;&#36827;&#31867;&#20869;&#36328;&#22495;&#26679;&#26412;&#20043;&#38388;&#30340;&#23616;&#37096;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#31867;&#21035;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#20174;&#32780;&#20445;&#25345;&#20174;&#28304;&#22495;&#21040;&#30446;&#26631;&#22495;&#30340;&#21028;&#21035;&#36716;&#31227;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;MemSAC&#30340;&#20248;&#21183;&#21644;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Practical real world datasets with plentiful categories introduce new challenges for unsupervised domain adaptation like small inter-class discriminability, that existing approaches relying on domain invariance alone cannot handle sufficiently well. In this work we propose MemSAC, which exploits sample level similarity across source and target domains to achieve discriminative transfer, along with architectures that scale to a large number of categories. For this purpose, we first introduce a memory augmented approach to efficiently extract pairwise similarity relations between labeled source and unlabeled target domain instances, suited to handle an arbitrary number of classes. Next, we propose and theoretically justify a novel variant of the contrastive loss to promote local consistency among within-class cross domain samples while enforcing separation between classes, thus preserving discriminative transfer from source to target. We validate the advantages of MemSAC with significant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#32858;&#21512;&#19982;&#26412;&#22320;&#27169;&#22411;&#30340;&#32622;&#25442;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2110.03469</link><description>&lt;p&gt;
&#20174;&#23567;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning from Small Datasets. (arXiv:2110.03469v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#32852;&#21512;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#32858;&#21512;&#19982;&#26412;&#22320;&#27169;&#22411;&#30340;&#32622;&#25442;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#12290;&#36825;&#22312;&#21307;&#30103;&#39046;&#22495;&#31561;&#25968;&#25454;&#26412;&#36523;&#20998;&#24067;&#20998;&#25955;&#12289;&#26080;&#27861;&#20844;&#24320;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#23454;&#29616;&#32852;&#21512;&#35757;&#32451;&#65292;&#32780;&#26412;&#22320;&#35757;&#32451;&#30446;&#26631;&#30340;&#26399;&#26395;&#19982;&#20840;&#23616;&#30446;&#26631;&#30456;&#20284;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#65292;&#23548;&#33268;&#26412;&#22320;&#30446;&#26631;&#19982;&#20840;&#23616;&#30446;&#26631;&#24046;&#24322;&#24456;&#22823;&#65292;&#20174;&#32780;&#20351;&#32852;&#21512;&#23398;&#20064;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#32858;&#21512;&#19982;&#26412;&#22320;&#27169;&#22411;&#30340;&#32622;&#25442;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#32622;&#25442;&#23558;&#27599;&#20010;&#26412;&#22320;&#27169;&#22411;&#26292;&#38706;&#32473;&#19968;&#31995;&#21015;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#31232;&#30095;&#30340;&#39046;&#22495;&#20013;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22312;&#26497;&#23567;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20363;&#22914;&#36328;&#21307;&#38498;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#32852;&#21512;&#23398;&#20064;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning allows multiple parties to collaboratively train a joint model without sharing local data. This enables applications of machine learning in settings of inherently distributed, undisclosable data such as in the medical domain. In practice, joint training is usually achieved by aggregating local models, for which local training objectives have to be in expectation similar to the joint (global) objective. Often, however, local datasets are so small that local objectives differ greatly from the global objective, resulting in federated learning to fail. We propose a novel approach that intertwines model aggregations with permutations of local models. The permutations expose each local model to a daisy chain of local datasets resulting in more efficient training in data-sparse domains. This enables training on extremely small local datasets, such as patient data across hospitals, while retaining the training efficiency and privacy benefits of federated learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;CTR&#39044;&#27979;&#20013;&#21152;&#20837;&#20102;&#31232;&#30095;&#20998;&#32452;Lasso&#30340;&#27491;&#21017;&#39033;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#22312;&#30456;&#21516;&#31232;&#30095;&#27700;&#24179;&#19979;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26497;&#39640;&#30340;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2107.14432</link><description>&lt;p&gt;
CTR&#39044;&#27979;&#20013;&#22522;&#20110;&#31232;&#30095;&#20998;&#32452;Lasso&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Adaptive Optimizers with Sparse Group Lasso for Neural Networks in CTR Prediction. (arXiv:2107.14432v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.14432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;CTR&#39044;&#27979;&#20013;&#21152;&#20837;&#20102;&#31232;&#30095;&#20998;&#32452;Lasso&#30340;&#27491;&#21017;&#39033;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#22312;&#30456;&#21516;&#31232;&#30095;&#27700;&#24179;&#19979;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#26497;&#39640;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#31232;&#30095;&#20998;&#32452;Lasso&#30340;&#27491;&#21017;&#39033;&#21152;&#20837;&#21040;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#20013;&#65292;&#22914;Momentum&#12289;Adagrad&#12289;Adam&#12289;AMSGrad&#12289;AdaHessian&#31561;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;&#20248;&#21270;&#22120;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;Group Momentum&#12289;Group Adagrad&#12289;Group Adam&#12289;Group AMSGrad&#21644;Group AdaHessian&#31561;&#12290;&#25105;&#20204;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22312;&#38543;&#26426;&#20984;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19977;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#24191;&#21578;&#28857;&#20987;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#26032;&#20248;&#21270;&#22120;&#30340;&#27491;&#21017;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20351;&#29992;&#24133;&#24230;&#20462;&#21098;&#26041;&#27861;&#30340;&#21407;&#22987;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;&#30456;&#21516;&#31232;&#30095;&#27700;&#24179;&#19978;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#19982;&#27809;&#26377;&#24133;&#24230;&#20462;&#21098;&#30340;&#24773;&#20917;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26497;&#39640;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#22909;&#25110;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a novel framework that adds the regularizers of the sparse group lasso to a family of adaptive optimizers in deep learning, such as Momentum, Adagrad, Adam, AMSGrad, AdaHessian, and create a new class of optimizers, which are named Group Momentum, Group Adagrad, Group Adam, Group AMSGrad and Group AdaHessian, etc., accordingly. We establish theoretically proven convergence guarantees in the stochastic convex settings, based on primal-dual methods. We evaluate the regularized effect of our new optimizers on three large-scale real-world ad click datasets with state-of-the-art deep learning models. The experimental results reveal that compared with the original optimizers with the post-processing procedure which uses the magnitude pruning method, the performance of the models can be significantly improved on the same sparsity level. Furthermore, in comparison to the cases without magnitude pruning, our methods can achieve extremely high sparsity with significantly better or hig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;Sig-Wasserstein GANs&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;WGANs&#19982;&#36335;&#24452;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#12290;&#36335;&#24452;&#30340;&#31614;&#21517;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#25551;&#36848;&#25968;&#25454;&#27969;&#30340;&#32479;&#35745;&#29305;&#24449;&#65292;&#21487;&#20197;&#21051;&#30011;&#26102;&#24207;&#27169;&#22411;&#30340;&#20998;&#24067;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2006.05421</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;Sig-Wasserstein GANs&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conditional Sig-Wasserstein GANs for Time Series Generation. (arXiv:2006.05421v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.05421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#26465;&#20214;&#30340;Sig-Wasserstein GANs&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;WGANs&#19982;&#36335;&#24452;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#12290;&#36335;&#24452;&#30340;&#31614;&#21517;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#25551;&#36848;&#25968;&#25454;&#27969;&#30340;&#32479;&#35745;&#29305;&#24449;&#65292;&#21487;&#20197;&#21051;&#30011;&#26102;&#24207;&#27169;&#22411;&#30340;&#20998;&#24067;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#29983;&#25104;&#26679;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#22823;&#30340;&#25104;&#21151;&#65292;&#20174;&#30475;&#20284;&#39640;&#32500;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25429;&#25417;&#30001;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24341;&#36215;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#30340;&#26102;&#24207;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#27969;&#20250;&#26497;&#22823;&#22320;&#22686;&#21152;&#30446;&#26631;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;&#21487;&#33021;&#20351;&#29983;&#25104;&#24314;&#27169;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#21463;&#35745;&#37327;&#32463;&#27982;&#23398;&#20013;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23545;&#32473;&#23450;&#36807;&#21435;&#20449;&#24687;&#30340;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#30340;&#26465;&#20214;&#20998;&#24067;&#38750;&#24120;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26465;&#20214;Sig-WGAN&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;Wasserstein-GANs&#65288;WGANs&#65289;&#19982;&#25968;&#23398;&#19978;&#26377;&#21407;&#21017;&#19988;&#39640;&#25928;&#30340;&#36335;&#24452;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#8212;&#8212;&#36335;&#24452;&#30340;&#31614;&#21517;&#30456;&#32467;&#21512;&#12290;&#36335;&#24452;&#30340;&#31614;&#21517;&#26159;&#19968;&#31995;&#21015;&#20998;&#32423;&#32479;&#35745;&#37327;&#65292;&#20026;&#25968;&#25454;&#27969;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25551;&#36848;&#65292;&#20854;&#26399;&#26395;&#20540;&#21051;&#30011;&#20102;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#20998;&#24067;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) have been extremely successful in generating samples, from seemingly high dimensional probability measures. However, these methods struggle to capture the temporal dependence of joint probability distributions induced by time-series data. Furthermore, long time-series data streams hugely increase the dimension of the target space, which may render generative modelling infeasible. To overcome these challenges, motivated by the autoregressive models in econometric, we are interested in the conditional distribution of future time series given the past information. We propose the generic conditional Sig-WGAN framework by integrating Wasserstein-GANs (WGANs) with mathematically principled and efficient path feature extraction called the signature of a path. The signature of a path is a graded sequence of statistics that provides a universal description for a stream of data, and its expected value characterises the law of the time-series model. In parti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25104;&#26412;&#24863;&#30693;&#25506;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#31232;&#30095;&#22870;&#21169;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#39640;&#25928;&#22320;&#25628;&#32034;&#21160;&#24577;&#23376;&#30446;&#26631;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/1910.09143</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#21160;&#24577;&#23376;&#30446;&#26631;&#23548;&#21521;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dynamic Subgoal-based Exploration via Bayesian Optimization. (arXiv:1910.09143v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.09143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25104;&#26412;&#24863;&#30693;&#25506;&#32034;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#31232;&#30095;&#22870;&#21169;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#39640;&#25928;&#22320;&#25628;&#32034;&#21160;&#24577;&#23376;&#30446;&#26631;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#22870;&#21169;&#23548;&#33322;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#26114;&#36149;&#21644;&#26377;&#38480;&#30340;&#20132;&#20114;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;&#38024;&#23545;&#38656;&#35201;&#29616;&#23454;&#19990;&#30028;&#35757;&#32451;&#30340;&#22797;&#26434;&#23548;&#33322;&#20219;&#21153;&#65288;&#24403;&#24265;&#20215;&#27169;&#25311;&#22120;&#19981;&#21487;&#29992;&#26102;&#65289;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#38754;&#20020;&#26410;&#30693;&#29615;&#22659;&#20998;&#24067;&#30340;&#20195;&#29702;&#65292;&#24182;&#19988;&#24517;&#39035;&#20915;&#23450;&#19968;&#31181;&#25506;&#32034;&#31574;&#30053;&#12290;&#22312;&#20174;&#30456;&#21516;&#29615;&#22659;&#20998;&#24067;&#20013;&#36873;&#25321;&#30340;&#27979;&#35797;&#29615;&#22659;&#20013;&#35780;&#20272;&#20043;&#21069;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#19968;&#31995;&#21015;&#35757;&#32451;&#29615;&#22659;&#26469;&#25913;&#36827;&#20854;&#31574;&#30053;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20851;&#27880;&#22266;&#23450;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#32780;&#23558;&#25506;&#32034;&#35270;&#20026;&#20803;&#20248;&#21270;&#38382;&#39064;&#30340;&#23569;&#25968;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#25104;&#26412;&#39640;&#25928;&#30340;&#25506;&#32034;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#24863;&#30693;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25628;&#32034;&#19968;&#31867;&#22522;&#20110;&#21160;&#24577;&#23376;&#30446;&#26631;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;&#35813;&#31639;&#27861;&#35843;&#25972;&#22810;&#20010;&#26464;&#26438;--&#23376;&#30446;&#26631;&#30340;&#20301;&#32622;&#65292;&#27599;&#20010;episode&#30340;&#38271;&#24230;&#20197;&#21450;nu&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning in sparse-reward navigation environments with expensive and limited interactions is challenging and poses a need for effective exploration. Motivated by complex navigation tasks that require real-world training (when cheap simulators are not available), we consider an agent that faces an unknown distribution of environments and must decide on an exploration strategy. It may leverage a series of training environments to improve its policy before it is evaluated in a test environment drawn from the same environment distribution. Most existing approaches focus on fixed exploration strategies, while the few that view exploration as a meta-optimization problem tend to ignore the need for cost-efficient exploration. We propose a cost-aware Bayesian optimization approach that efficiently searches over a class of dynamic subgoal-based exploration strategies. The algorithm adjusts a variety of levers -- the locations of the subgoals, the length of each episode, and the nu
&lt;/p&gt;</description></item><item><title>L2P&#26159;&#19968;&#31181;&#21033;&#29992;&#23454;&#20363;&#20043;&#38388;&#25104;&#23545;&#20851;&#31995;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#37325;&#23614;&#20998;&#24067;&#29305;&#24449;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;L2P&#22312;&#20934;&#30830;&#24230;&#21644;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/1908.04628</link><description>&lt;p&gt;
L2P: &#23398;&#20064;&#25918;&#32622;&#29992;&#20110;&#20272;&#35745;&#37325;&#23614;&#20998;&#24067;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
L2P: Learning to Place for Estimating Heavy-Tailed Distributed Outcomes. (arXiv:1908.04628v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1908.04628
&lt;/p&gt;
&lt;p&gt;
L2P&#26159;&#19968;&#31181;&#21033;&#29992;&#23454;&#20363;&#20043;&#38388;&#25104;&#23545;&#20851;&#31995;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#37325;&#23614;&#20998;&#24067;&#29305;&#24449;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;L2P&#22312;&#20934;&#30830;&#24230;&#21644;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#39044;&#27979;&#20219;&#21153;&#20855;&#26377;&#20855;&#26377;&#37325;&#23614;&#20998;&#24067;&#29305;&#24449;&#30340;&#32467;&#26524;&#21464;&#37327;&#12290;&#20363;&#22914;&#65292;&#38144;&#21806;&#30340;&#22270;&#20070;&#21103;&#26412;&#65292;&#33402;&#26415;&#21697;&#25293;&#21334;&#20215;&#26684;&#65292;&#20179;&#24211;&#20013;&#21830;&#21697;&#30340;&#38656;&#27714;&#31561;&#12290;&#36890;&#36807;&#23398;&#20064;&#37325;&#23614;&#20998;&#24067;&#65292;"&#22823;&#32780;&#32597;&#35265;"&#30340;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#30021;&#38144;&#20070;&#65289;&#23558;&#20855;&#26377;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24182;&#19981;&#19987;&#27880;&#20110;&#23398;&#20064;&#37325;&#23614;&#20998;&#24067;&#65307;&#22240;&#27492;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#23545;&#36825;&#20123;&#23454;&#20363;&#36827;&#34892;&#20005;&#37325;&#20302;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23398;&#20064;&#25918;&#32622;&#65288;L2P&#65289;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23454;&#20363;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;L2P&#23398;&#20064;&#19968;&#23545;&#19968;&#20559;&#22909;&#20998;&#31867;&#22120;&#65306;&#23454;&#20363;A&#26159;&#21542;&#22823;&#20110;&#23454;&#20363;B&#65311;&#22312;&#25918;&#32622;&#38454;&#27573;&#65292;L2P&#36890;&#36807;&#23558;&#26032;&#23454;&#20363;&#25918;&#32622;&#22312;&#24050;&#30693;&#23454;&#20363;&#20043;&#38388;&#26469;&#33719;&#24471;&#39044;&#27979;&#32467;&#26524;&#12290;&#26681;&#25454;&#20854;&#25918;&#32622;&#20301;&#32622;&#65292;&#26032;&#23454;&#20363;&#34987;&#20998;&#37197;&#19968;&#20010;&#32467;&#26524;&#21464;&#37327;&#30340;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;L2P&#22312;&#20934;&#30830;&#24230;&#21644;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world prediction tasks have outcome variables that have characteristic heavy-tail distributions. Examples include copies of books sold, auction prices of art pieces, demand for commodities in warehouses, etc. By learning heavy-tailed distributions, "big and rare" instances (e.g., the best-sellers) will have accurate predictions. Most existing approaches are not dedicated to learning heavy-tailed distribution; thus, they heavily under-predict such instances. To tackle this problem, we introduce Learning to Place (L2P), which exploits the pairwise relationships between instances for learning. In its training phase, L2P learns a pairwise preference classifier: is instance A &gt; instance B? In its placing phase, L2P obtains a prediction by placing the new instance among the known instances. Based on its placement, the new instance is then assigned a value for its outcome variable. Experiments on real data show that L2P outperforms competing approaches in terms of accuracy and abili
&lt;/p&gt;</description></item></channel></rss>