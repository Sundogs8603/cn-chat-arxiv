<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24322;&#26500;&#25490;&#38431;&#31995;&#32479;&#20013;&#20316;&#19994;&#36335;&#30001;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;ACHQ&#31639;&#27861;&#65292;&#36890;&#36807;&#20302;&#32500;&#24230;&#30340;&#36719;&#38408;&#20540;&#31574;&#30053;&#21442;&#25968;&#21270;&#21644;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31995;&#32479;&#30340;&#25490;&#38431;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#27714;&#35299;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ACHQ&#31639;&#27861;&#33021;&#22815;&#25910;&#25947;&#21040;&#36817;&#20284;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01147</link><description>&lt;p&gt;
&#24322;&#26500;&#25490;&#38431;&#31995;&#32479;&#20013;&#29992;&#20110;&#36335;&#30001;&#20316;&#19994;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning for Routing Jobs in Heterogeneous Queueing Systems
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24322;&#26500;&#25490;&#38431;&#31995;&#32479;&#20013;&#20316;&#19994;&#36335;&#30001;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;ACHQ&#31639;&#27861;&#65292;&#36890;&#36807;&#20302;&#32500;&#24230;&#30340;&#36719;&#38408;&#20540;&#31574;&#30053;&#21442;&#25968;&#21270;&#21644;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31995;&#32479;&#30340;&#25490;&#38431;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#27714;&#35299;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ACHQ&#31639;&#27861;&#33021;&#22815;&#25910;&#25947;&#21040;&#36817;&#20284;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#23558;&#21040;&#36798;&#20013;&#22830;&#38431;&#21015;&#30340;&#20316;&#19994;&#39640;&#25928;&#36335;&#30001;&#21040;&#24322;&#26500;&#26381;&#21153;&#22120;&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#12290;&#19982;&#21516;&#36136;&#31995;&#32479;&#19981;&#21516;&#65292;&#23545;&#20110;&#19968;&#24555;&#19968;&#24930;&#30340;&#21452;&#26381;&#21153;&#22120;&#31995;&#32479;&#65292;&#24050;&#30693;&#38408;&#20540;&#31574;&#30053;&#65292;&#21363;&#22312;&#38431;&#21015;&#38271;&#24230;&#36229;&#36807;&#26576;&#19968;&#38408;&#20540;&#26102;&#23558;&#20316;&#19994;&#36335;&#30001;&#21040;&#24930;&#26381;&#21153;&#22120;&#65292;&#26159;&#26368;&#20248;&#31574;&#30053;&#12290;&#20294;&#22810;&#26381;&#21153;&#22120;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#26410;&#30693;&#19988;&#38590;&#20197;&#25214;&#21040;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#34987;&#35748;&#20026;&#23545;&#20110;&#23398;&#20064;&#27492;&#31867;&#31574;&#30053;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#38382;&#39064;&#20855;&#26377;&#25351;&#25968;&#32423;&#30340;&#29366;&#24577;&#31354;&#38388;&#22823;&#23567;&#65292;&#20351;&#26631;&#20934;RL&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACHQ&#65292;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20855;&#26377;&#20302;&#32500;&#24230;&#30340;&#36719;&#38408;&#20540;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#21033;&#29992;&#24213;&#23618;&#25490;&#38431;&#32467;&#26500;&#12290;&#25105;&#20204;&#20026;&#19968;&#33324;&#24773;&#20917;&#25552;&#20379;&#20102;&#31283;&#24577;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#23613;&#31649;&#21442;&#25968;&#21270;&#32500;&#24230;&#36739;&#20302;&#65292;&#20294;&#35777;&#26126;&#20102;ACHQ&#25910;&#25947;&#21040;&#36817;&#20284;&#20840;&#23616;&#26368;&#20248;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of efficiently routing jobs that arrive into a central queue to a system of heterogeneous servers. Unlike homogeneous systems, a threshold policy, that routes jobs to the slow server(s) when the queue length exceeds a certain threshold, is known to be optimal for the one-fast-one-slow two-server system. But an optimal policy for the multi-server system is unknown and non-trivial to find. While Reinforcement Learning (RL) has been recognized to have great potential for learning policies in such cases, our problem has an exponentially large state space size, rendering standard RL inefficient. In this work, we propose ACHQ, an efficient policy gradient based algorithm with a low dimensional soft threshold policy parameterization that leverages the underlying queueing structure. We provide stationary-point convergence guarantees for the general case and despite the low-dimensional parameterization prove that ACHQ converges to an approximate global optimum for the sp
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#21644;&#31934;&#31616;&#30340;MPC&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;12&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01116</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22810;&#27169;&#22411;MPC&#30340;&#22522;&#20110;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#30340;&#23618;&#32423;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01116
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#20132;&#20114;&#39044;&#27979;&#21644;&#31934;&#31616;&#30340;MPC&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65292;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;12&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#36890;&#22330;&#26223;&#20013;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#12290;&#35813;&#26550;&#26500;&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;1) RAID-Net&#65292;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#39062;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24615;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#21608;&#22260;&#36710;&#36742;&#20043;&#38388;&#22312;MPC&#39044;&#27979;&#33539;&#22260;&#20869;&#30340;&#30456;&#20851;&#20132;&#20114;&#65307;2) &#19968;&#20010;&#31616;&#21270;&#30340;&#38543;&#26426;MPC&#38382;&#39064;&#65292;&#28040;&#38500;&#19981;&#30456;&#20851;&#30340;&#36991;&#30896;&#32422;&#26463;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#27169;&#25311;&#20132;&#36890;&#36335;&#21475;&#20013;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35299;&#20915;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#30340;12&#20493;&#36895;&#25552;&#21319;&#12290;&#24744;&#21487;&#20197;&#22312;&#36825;&#37324;&#25214;&#21040;&#23637;&#31034;&#35813;&#26550;&#26500;&#22312;&#22810;&#20010;&#22797;&#26434;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#35270;&#39057;&#65306;https://youtu.be/-TcMeolCLWc
&lt;/p&gt;
&lt;p&gt;
We propose a hierarchical architecture designed for scalable real-time Model Predictive Control (MPC) in complex, multi-modal traffic scenarios. This architecture comprises two key components: 1) RAID-Net, a novel attention-based Recurrent Neural Network that predicts relevant interactions along the MPC prediction horizon between the autonomous vehicle and the surrounding vehicles using Lagrangian duality, and 2) a reduced Stochastic MPC problem that eliminates irrelevant collision avoidance constraints, enhancing computational efficiency. Our approach is demonstrated in a simulated traffic intersection with interactive surrounding vehicles, showcasing a 12x speed-up in solving the motion planning problem. A video demonstrating the proposed architecture in multiple complex traffic scenarios can be found here: https://youtu.be/-TcMeolCLWc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#26356;&#39640;&#25928;&#24182;&#22312;ASR&#21644;LID&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02000</link><description>&lt;p&gt;
&#38750;&#27954;&#20013;&#24515;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25216;&#26415;&#22312;&#25746;&#21704;&#25289;&#20197;&#21335;&#22320;&#21306;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#34920;&#24449;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Africa-Centric Self-Supervised Pre-Training for Multilingual Speech Representation in a Sub-Saharan Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02000
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#65292;&#26356;&#39640;&#25928;&#24182;&#22312;ASR&#21644;LID&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20165;&#22312;&#38750;&#27954;&#35821;&#38899;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20174;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#22320;&#21306;&#35762;&#35805;&#30340;21&#31181;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#23398;&#20064;&#20102;&#36817;60,000&#23567;&#26102;&#30340;&#26410;&#26631;&#35760;&#35821;&#38899;&#29255;&#27573;&#12290;&#22312;FLEURS-102&#25968;&#25454;&#38598;&#30340;SSA&#23376;&#38598;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;HuBERT$_{base}$ (0.09B) &#26550;&#26500;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#19982;FLEURS&#22522;&#20934;&#25552;&#20986;&#30340;w2v-bert-51 (0.6B) &#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;ASR&#19979;&#28216;&#20219;&#21153;&#20013;&#26356;&#21152;&#39640;&#25928;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#23569;7&#20493;&#65292;&#21442;&#25968;&#23569;6&#20493;&#12290;&#27492;&#22806;&#65292;&#22312;LID&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#36229;&#36807;FLEURS&#22522;&#32447;&#36229;&#36807;22%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02000v1 Announce Type: new  Abstract: We present the first self-supervised multilingual speech model trained exclusively on African speech. The model learned from nearly 60 000 hours of unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed in the FLEURS benchmark, while being more efficient by using 7x less data and 6x less parameters. Furthermore, in the context of a LID downstream task, our approach outperforms FLEURS baselines accuracy by over 22\%.
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;CT&#25195;&#25551;&#20013;&#23384;&#22312;&#30340;&#21464;&#24322;&#24615;&#21644;OOD&#20999;&#29255;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#31354;&#38388;&#20998;&#21106;&#29305;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28388;&#38500;OOD&#25968;&#25454;&#21644;&#20943;&#23569;&#20887;&#20313;&#26469;&#36873;&#25321;&#20851;&#38190;&#30340;&#31354;&#38388;&#20999;&#29255;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26680;&#23494;&#24230;&#30340;&#20999;&#29255;&#37319;&#26679;&#26041;&#27861;&#26469;&#25552;&#39640;&#31283;&#23450;&#24615;&#21644;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;</title><link>https://arxiv.org/abs/2404.01643</link><description>&lt;p&gt;
&#23545;COVID-19&#26816;&#27979;&#30340;&#31354;&#38388;&#20998;&#21106;&#29305;&#24449;&#23398;&#20064;&#36827;&#34892;&#26356;&#32454;&#33268;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Spatial-Slice Features Learning for COVID-19 Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01643
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;CT&#25195;&#25551;&#20013;&#23384;&#22312;&#30340;&#21464;&#24322;&#24615;&#21644;OOD&#20999;&#29255;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22411;&#30340;&#31354;&#38388;&#20998;&#21106;&#29305;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28388;&#38500;OOD&#25968;&#25454;&#21644;&#20943;&#23569;&#20887;&#20313;&#26469;&#36873;&#25321;&#20851;&#38190;&#30340;&#31354;&#38388;&#20999;&#29255;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26680;&#23494;&#24230;&#30340;&#20999;&#29255;&#37319;&#26679;&#26041;&#27861;&#26469;&#25552;&#39640;&#31283;&#23450;&#24615;&#21644;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#25104;&#20687;&#35782;&#21035;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#65288;1&#65289;&#27599;&#20010;CT&#25195;&#25551;&#30340;&#20998;&#36776;&#29575;&#21644;&#22823;&#23567;&#32463;&#24120;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#21464;&#24322;&#24615;&#65292;&#38656;&#35201;&#23545;&#36755;&#20837;&#23610;&#23544;&#21644;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#26377;&#20005;&#26684;&#30340;&#35201;&#27714;&#12290; &#65288;2&#65289;CT&#25195;&#25551;&#21253;&#21547;&#22823;&#37327;&#30340;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#20999;&#29255;&#12290;&#20851;&#38190;&#29305;&#24615;&#21487;&#33021;&#20165;&#23384;&#22312;&#20110;&#25972;&#20010;CT&#25195;&#25551;&#30340;&#29305;&#23450;&#31354;&#38388;&#21306;&#22495;&#21644;&#20999;&#29255;&#20013;&#12290;&#25105;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#25214;&#20986;&#36825;&#20123;&#21306;&#22495;&#30340;&#20301;&#32622;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;CT&#25195;&#25551;&#30340;&#22686;&#24378;&#22411;&#31354;&#38388;&#20998;&#21106;&#29305;&#24449;&#23398;&#20064;&#65288;SSFL ++&#65289;&#26694;&#26550;&#12290;&#23427;&#26088;&#22312;&#20174;&#25972;&#20010;CT&#25195;&#25551;&#20013;&#28388;&#38500;OOD&#25968;&#25454;&#65292;&#36890;&#36807;&#23436;&#20840;&#20943;&#23569;70&#65285;&#30340;&#20887;&#20313;&#26469;&#36873;&#25321;&#20851;&#38190;&#30340;&#31354;&#38388;&#20999;&#29255;&#36827;&#34892;&#20998;&#26512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23494;&#24230;&#30340;&#20999;&#29255;&#37319;&#26679;&#65288;KDS&#65289;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#30340;&#31283;&#23450;&#24615;&#65292;&#20174;&#32780;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01643v1 Announce Type: cross  Abstract: Conventional Computed Tomography (CT) imaging recognition faces two significant challenges: (1) There is often considerable variability in the resolution and size of each CT scan, necessitating strict requirements for the input size and adaptability of models. (2) CT-scan contains large number of out-of-distribution (OOD) slices. The crucial features may only be present in specific spatial regions and slices of the entire CT scan. How can we effectively figure out where these are located? To deal with this, we introduce an enhanced Spatial-Slice Feature Learning (SSFL++) framework specifically designed for CT scan. It aim to filter out a OOD data within whole CT scan, enabling our to select crucial spatial-slice for analysis by reducing 70% redundancy totally. Meanwhile, we proposed Kernel-Density-based slice Sampling (KDS) method to improve the stability when training and inference stage, therefore speeding up the rate of convergence 
&lt;/p&gt;</description></item><item><title>&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00897</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#40065;&#26834;&#24615;&#65306;&#20837;&#38376;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Robustness: A Primer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00897
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#30340;&#37325;&#35201;&#27010;&#24565;&#21450;&#20851;&#38190;&#30340;&#25216;&#26415;&#21644;&#22240;&#32032;&#65292;&#20197;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#31456;&#33410;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20013;&#31283;&#20581;&#24615;&#30340;&#22522;&#26412;&#27010;&#24565;&#21450;&#20854;&#22312;&#30830;&#31435;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#35752;&#35770;&#22987;&#20110;&#31283;&#20581;&#24615;&#30340;&#35814;&#32454;&#23450;&#20041;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;ML&#27169;&#22411;&#22312;&#21508;&#31181;&#19981;&#21516;&#21644;&#24847;&#22806;&#30340;&#29615;&#22659;&#26465;&#20214;&#19979;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;ML&#40065;&#26834;&#24615;&#36890;&#36807;&#22810;&#20010;&#35270;&#35282;&#36827;&#34892;&#20102;&#21078;&#26512;&#65306;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20114;&#34917;&#24615;&#65307;&#20854;&#20316;&#20026;&#21487;&#20449;AI&#30340;&#35201;&#27714;&#65307;&#20854;&#23545;&#25239;&#24615;&#19982;&#38750;&#23545;&#25239;&#24615;&#26041;&#38754;&#65307;&#20854;&#25968;&#37327;&#21270;&#25351;&#26631;&#65307;&#20197;&#21450;&#20854;&#21487;&#22797;&#29616;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#25351;&#26631;&#12290;&#31456;&#33410;&#28145;&#20837;&#25506;&#35752;&#20102;&#24433;&#21709;&#40065;&#26834;&#24615;&#30340;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#20559;&#24046;&#12289;&#27169;&#22411;&#22797;&#26434;&#24615;&#20197;&#21450;ML&#27969;&#31243;&#19981;&#26126;&#30830;&#30340;&#39118;&#38505;&#12290;&#23427;&#20174;&#24191;&#27867;&#30340;&#35270;&#35282;&#35843;&#26597;&#20102;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#21253;&#25324;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21253;&#25324;&#25968;&#23383;&#21644;&#29289;&#29702;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00897v1 Announce Type: cross  Abstract: This chapter explores the foundational concept of robustness in Machine Learning (ML) and its integral role in establishing trustworthiness in Artificial Intelligence (AI) systems. The discussion begins with a detailed definition of robustness, portraying it as the ability of ML models to maintain stable performance across varied and unexpected environmental conditions. ML robustness is dissected through several lenses: its complementarity with generalizability; its status as a requirement for trustworthy AI; its adversarial vs non-adversarial aspects; its quantitative metrics; and its indicators such as reproducibility and explainability. The chapter delves into the factors that impede robustness, such as data bias, model complexity, and the pitfalls of underspecified ML pipelines. It surveys key techniques for robustness assessment from a broad perspective, including adversarial attacks, encompassing both digital and physical realms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;YOLOOC&#26816;&#27979;&#22120;&#65292;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#35774;&#32622;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#26377;&#25928;&#24212;&#23545;&#26032;&#31867;&#21035;&#26816;&#27979;&#21644;&#22686;&#37327;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2404.00257</link><description>&lt;p&gt;
YOLOOC: &#22522;&#20110;YOLO&#30340;&#24320;&#25918;&#31867;&#21035;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;&#19982;&#26032;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;YOLOOC&#26816;&#27979;&#22120;&#65292;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#35774;&#32622;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#26377;&#25928;&#24212;&#23545;&#26032;&#31867;&#21035;&#26816;&#27979;&#21644;&#22686;&#37327;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#36816;&#29992;&#65292;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#65288;OWOD&#65289;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#25361;&#25112;&#22312;&#20110;&#27169;&#22411;&#22914;&#20309;&#26816;&#27979;&#26032;&#31867;&#21035;&#65292;&#28982;&#21518;&#22686;&#37327;&#23398;&#20064;&#23427;&#20204;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#24050;&#30693;&#30340;&#31867;&#21035;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#30417;&#30563;&#25110;&#24369;&#30417;&#30563;&#30340;&#26032;&#31867;&#21035;&#25968;&#25454;&#29992;&#20110;&#26032;&#31867;&#21035;&#26816;&#27979;&#65292;&#36825;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#26032;&#31867;&#21035;&#21482;&#22312;&#25512;&#26029;&#38454;&#27573;&#36935;&#21040;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLO&#26550;&#26500;&#30340;&#26032;OWOD&#26816;&#27979;&#22120;YOLOOC&#65292;&#19987;&#38376;&#38024;&#23545;&#24320;&#25918;&#31867;&#21035;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#31614;&#24179;&#28369;&#20197;&#38450;&#27490;&#26816;&#27979;&#22120;&#36807;&#20110;&#33258;&#20449;&#22320;&#23558;&#26032;&#31867;&#21035;&#26144;&#23556;&#21040;&#24050;&#30693;&#31867;&#21035;&#24182;&#21457;&#29616;&#26032;&#31867;&#21035;&#12290;&#22312;&#25105;&#20204;&#26356;&#21152;&#29616;&#23454;&#30340;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#26032;&#22522;&#20934;&#19979;&#21457;&#29616;&#26032;&#31867;&#21035;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00257v1 Announce Type: cross  Abstract: Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we propose a new OWOD detector YOLOOC, based on the YOLO architecture yet for the Open-Class setup. We introduce label smoothing to prevent the detector from over-confidently mapping novel classes to known classes and to discover novel classes. Extensive experiments conducted on our more realistic setup demonstrate the effectiveness of our method for discovering novel classes in our new benchmark.
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#20316;&#20026;&#19968;&#31181;&#26032;&#24037;&#20855;&#22791;&#21463;&#20851;&#27880;&#65292;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;GNN&#22312;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.19852</link><description>&lt;p&gt;
&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Graph Neural Networks in Epidemic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19852
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#20316;&#20026;&#19968;&#31181;&#26032;&#24037;&#20855;&#22791;&#21463;&#20851;&#27880;&#65292;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;GNN&#22312;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26032;&#20896;&#30123;&#24773;&#29190;&#21457;&#20197;&#26469;&#65292;&#20154;&#20204;&#23545;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20256;&#32479;&#30340;&#26426;&#26800;&#27169;&#22411;&#25968;&#23398;&#25551;&#36848;&#20102;&#20256;&#26579;&#30149;&#30340;&#20256;&#25773;&#26426;&#21046;&#65292;&#20294;&#22312;&#38754;&#23545;&#24403;&#20170;&#19981;&#26029;&#22686;&#38271;&#30340;&#25361;&#25112;&#26102;&#24448;&#24448;&#21147;&#19981;&#20174;&#24515;&#12290;&#22240;&#27492;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#35797;&#22270;&#20840;&#38754;&#22238;&#39038;GNN&#22312;&#27969;&#34892;&#30149;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#28508;&#22312;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20026;&#27969;&#34892;&#30149;&#20219;&#21153;&#21644;&#26041;&#27861;&#35770;&#21508;&#24341;&#20837;&#20102;&#20998;&#23618;&#20998;&#31867;&#27861;&#65292;&#20026;&#35813;&#39046;&#22495;&#20869;&#30340;&#21457;&#23637;&#36712;&#36857;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#23545;&#20110;&#27969;&#34892;&#30149;&#20219;&#21153;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#27969;&#34892;&#30149;&#39046;&#22495;&#36890;&#24120;&#24212;&#29992;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;&#23545;&#20110;&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30740;&#31350;&#20998;&#20026;&#8220;&#31070;&#32463;&#27169;&#22411;&#8221;&#21644;&#8220;&#28151;&#21512;&#27169;&#22411;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19852v1 Announce Type: new  Abstract: Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into \textit{Neural Models} and \textit{Hybrid Models}. Following this, we perform an exha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#21040;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#30340;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#65292;&#29983;&#25104;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.18985</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#40657;&#30418;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#21644;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#21040;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#30340;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#65292;&#29983;&#25104;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#38024;&#23545;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#65288;1D&#65289;&#12289;&#22270;&#20687;&#20998;&#31867;&#65288;2D&#65289;&#21040;&#35270;&#39057;&#20998;&#31867;&#65288;3D&#65289;&#31561;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#20248;&#21270;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#21644;&#21508;&#31181;&#25197;&#26354;&#31867;&#22411;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#12290;&#26032;&#39062;&#30340;RL&#26041;&#27861;&#22312;&#25152;&#26377;&#19977;&#20010;&#24212;&#29992;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;RL&#26041;&#27861;&#29983;&#25104;&#20102;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#22686;&#24378;&#20102;&#22270;&#20687;&#20998;&#31867;&#21644;&#24515;&#30005;&#22270;&#20998;&#26512;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#20110;&#24515;&#30005;&#22270;&#20998;&#26512;&#31561;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#24179;&#21488;&#31361;&#20986;&#20102;&#20020;&#24202;&#21307;&#29983;&#20851;&#27880;&#30340;&#20851;&#38190;&#24515;&#30005;&#22270;&#29255;&#27573;&#65292;&#21516;&#26102;&#30830;&#20445;&#23545;&#27969;&#34892;&#25197;&#26354;&#30340;&#38887;&#24615;&#12290;&#36825;&#19968;&#20840;&#38754;&#30340;&#24037;&#20855;&#26088;&#22312;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#36879;&#26126;&#24230;&#25552;&#39640;&#21508;&#31181;&#24212;&#29992;&#21644;&#25968;&#25454;&#31867;&#22411;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18985v1 Announce Type: cross  Abstract: We present a generic Reinforcement Learning (RL) framework optimized for crafting adversarial attacks on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D). The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types. The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency. Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models. For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions. This comprehensive tool aims to bolster both resilience with adversarial training and transparency across varied applications and data types.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#30340;&#22810;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#21387;&#32553;&#29575;&#19982;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21462;&#24471;&#20102;&#25968;&#25454;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17886</link><description>&lt;p&gt;
&#21387;&#32553;&#22810;&#20219;&#21153;&#23884;&#20837;&#29992;&#20110;&#22320;&#29699;&#35266;&#27979;&#20013;&#25968;&#25454;&#39640;&#25928;&#19979;&#28216;&#35757;&#32451;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17886
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#30340;&#22810;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#21387;&#32553;&#29575;&#19982;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21462;&#24471;&#20102;&#25968;&#25454;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#20013;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#23384;&#20648;&#24211;&#22686;&#38271;&#65292;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#36716;&#31227;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#65292;&#28040;&#32791;&#20102;&#22823;&#37327;&#36164;&#28304;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#65288;NEC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#23545;&#25968;&#25454;&#20351;&#29992;&#32773;&#20256;&#36755;&#21387;&#32553;&#30340;&#23884;&#20837;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#31070;&#32463;&#21387;&#32553;&#26469;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#65292;&#29983;&#25104;&#22810;&#20219;&#21153;&#23884;&#20837;&#65292;&#21516;&#26102;&#22312;&#21387;&#32553;&#29575;&#21644;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#20165;&#38024;&#23545;FM&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;10%&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#36827;&#34892;&#30701;&#26102;&#38388;&#35757;&#32451;&#65288;&#39044;&#35757;&#32451;&#36845;&#20195;&#30340;1%&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;EO&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;NEC&#65306;&#22330;&#26223;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;&#19982;&#23558;&#20256;&#32479;&#21387;&#32553;&#24212;&#29992;&#20110;&#21407;&#22987;&#25968;&#25454;&#30456;&#27604;&#65292;NEC&#22312;&#20943;&#23569;&#25968;&#25454;&#37327;&#26041;&#38754;&#21487;&#23454;&#29616;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#20102;75%&#21040;90%&#30340;&#25968;&#25454;&#37327;&#12290;&#21363;&#20351;&#22312;99.7%&#30340;&#21387;&#32553;&#19979;&#65292;&#22312;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#19978;&#24615;&#33021;&#20165;&#19979;&#38477;&#20102;5%&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;NEC&#26159;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17886v1 Announce Type: new  Abstract: As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient
&lt;/p&gt;</description></item><item><title>VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16973</link><description>&lt;p&gt;
VoiceCraft&#65306;&#37326;&#22806;&#38646;-shot&#35821;&#38899;&#32534;&#36753;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16973
&lt;/p&gt;
&lt;p&gt;
VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;VoiceCraft&#65292;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#22768;&#20070;&#12289;&#20114;&#32852;&#32593;&#35270;&#39057;&#21644;&#25773;&#23458;&#19978;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;VoiceCraft&#37319;&#29992;Transformer&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#35760;&#37325;&#25490;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#21644;&#24310;&#36831;&#22534;&#21472;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#26377;&#24207;&#21015;&#20869;&#30340;&#29983;&#25104;&#12290;&#22312;&#35821;&#38899;&#32534;&#36753;&#20219;&#21153;&#19978;&#65292;VoiceCraft&#29983;&#25104;&#30340;&#32534;&#36753;&#35821;&#38899;&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#20960;&#20046;&#19982;&#26410;&#32534;&#36753;&#30340;&#24405;&#38899;&#38590;&#20197;&#21306;&#20998;&#65292;&#32463;&#20154;&#31867;&#35780;&#20272;&#65307;&#23545;&#20110;&#38646;-shot TTS&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21253;&#25324;VALLE&#21644;&#27969;&#34892;&#30340;&#21830;&#19994;&#27169;&#22411;XTTS-v2&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#26679;&#21475;&#38899;&#12289;&#35821;&#38899;&#39118;&#26684;&#12289;&#24405;&#21046;&#26465;&#20214;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#38899;&#20048;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30495;&#23454;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#22987;&#32456;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16973v1 Announce Type: cross  Abstract: We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.16967</link><description>&lt;p&gt;
&#29992;&#20110;&#33151;&#24335;&#23450;&#28857;&#26426;&#22120;&#20154;&#36816;&#21160;&#25805;&#20316;&#30340;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visual Whole-Body Control for Legged Loco-Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#37197;&#22791;&#25163;&#33218;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;&#30340;&#38382;&#39064;&#65292;&#21363;&#33151;&#24335;&#23450;&#28857;&#25805;&#20316;&#12290;&#23613;&#31649;&#26426;&#22120;&#20154;&#30340;&#33151;&#36890;&#24120;&#29992;&#20110;&#31227;&#21160;&#65292;&#20294;&#36890;&#36807;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#65292;&#21487;&#20197;&#25193;&#22823;&#20854;&#25805;&#20316;&#33021;&#21147;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#20854;&#24037;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#35270;&#35273;&#35266;&#27979;&#33258;&#20027;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;\ourFull~(\our)&#65292;&#30001;&#19968;&#20010;&#20302;&#32423;&#31574;&#30053;&#21644;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#32452;&#25104;&#12290;&#20302;&#32423;&#31574;&#30053;&#20351;&#29992;&#25152;&#26377;&#33258;&#30001;&#24230;&#26469;&#36319;&#36394;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#20301;&#32622;&#65292;&#39640;&#32423;&#31574;&#30053;&#26681;&#25454;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#20102;&#20004;&#20010;&#32423;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#20174;Sim&#21040;&#23454;&#29289;&#30340;&#36716;&#31227;&#20197;&#36827;&#34892;&#23454;&#38469;&#26426;&#22120;&#20154;&#37096;&#32626;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#37197;&#32622;&#19979;&#65288;&#39640;&#24230;&#12289;&#65289;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#65292;&#30456;&#23545;&#22522;&#32447;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;PPO-AIRL + SAC&#20197;&#35299;&#20915;SAC&#31639;&#27861;&#22312;AIRL&#35757;&#32451;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14593</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65306;&#20174;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#30340;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14593
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;PPO-AIRL + SAC&#20197;&#35299;&#20915;SAC&#31639;&#27861;&#22312;AIRL&#35757;&#32451;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;AIRL&#65289;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;AIRL&#30340;&#20004;&#20010;&#19981;&#21516;&#35282;&#24230;&#65306;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#12290;&#25105;&#20204;&#20174;&#29992;Soft Actor-Critic&#65288;SAC&#65289;&#26367;&#25442;AIRL&#20013;&#30340;&#20869;&#32622;&#31639;&#27861;&#24320;&#22987;&#65292;&#20197;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;SAC&#30340;&#31163;&#31574;&#30053;&#24418;&#24335;&#21644;&#30456;&#23545;&#20110;AIRL&#32780;&#35328;&#21487;&#35782;&#21035;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#27169;&#22411;&#12290;&#36825;&#30830;&#23454;&#22312;&#31574;&#30053;&#27169;&#20223;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#19981;&#24910;&#32473;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#24102;&#26469;&#20102;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;SAC&#31639;&#27861;&#26412;&#36523;&#22312;AIRL&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;PPO-AIRL + SAC&#65292;&#20197;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#36716;&#31227;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29615;&#22659;&#25552;&#21462;&#35299;&#24320;&#30340;&#22870;&#21169;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14593v1 Announce Type: new  Abstract: Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone approach in imitation learning. This paper rethinks the two different angles of AIRL: policy imitation and transferable reward recovery. We begin with substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during the policy optimization process to enhance sample efficiency, thanks to the off-policy formulation of SAC and identifiable Markov decision process (MDP) models with respect to AIRL. It indeed exhibits a significant improvement in policy imitation but accidentally brings drawbacks to transferable reward recovery. To learn this issue, we illustrate that the SAC algorithm itself is not feasible to disentangle the reward function comprehensively during the AIRL training process, and propose a hybrid framework, PPO-AIRL + SAC, for satisfactory transfer effect. Additionally, we analyze the capability of environments to extract disentangled rewa
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#23450;&#29702;&#25506;&#35752;&#20102;&#26368;&#20248;&#23398;&#20064;&#31574;&#30053;&#65292;&#21253;&#25324;&#32771;&#34385;&#21644;&#24573;&#30053;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#39034;&#24207;&#22810;&#20010;&#35757;&#32451;&#26679;&#26412;&#22686;&#30410;&#30340;&#29702;&#35770;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.11125</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based system reliability analysis with Gaussian Process Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#38752;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#23450;&#29702;&#25506;&#35752;&#20102;&#26368;&#20248;&#23398;&#20064;&#31574;&#30053;&#65292;&#21253;&#25324;&#32771;&#34385;&#21644;&#24573;&#30053;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#39034;&#24207;&#22810;&#20010;&#35757;&#32451;&#26679;&#26412;&#22686;&#30410;&#30340;&#29702;&#35770;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11125v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#35768;&#22810;&#26377;&#25928;&#30340;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;&#35745;&#31639;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#24456;&#23569;&#26377;&#20154;&#25506;&#35752;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#20248;&#23398;&#20064;&#31574;&#30053;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#23450;&#29702;&#26469;&#20419;&#36827;&#36825;&#31181;&#25506;&#32034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35814;&#32454;&#38416;&#36848;&#20102;&#32771;&#34385;&#21644;&#24573;&#30053;&#20505;&#36873;&#35774;&#35745;&#26679;&#26412;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340; U &#23398;&#20064;&#20989;&#25968;&#21487;&#20197;&#37325;&#26032;&#21046;&#23450;&#20026;&#22312;&#24573;&#30053; Kriging &#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#23398;&#20064;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#36890;&#36807;&#24102;&#26377;&#30456;&#24212;&#25439;&#22833;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#25968;&#23398;&#19978;&#25506;&#35752;&#20102;&#39034;&#24207;&#22810;&#20010;&#35757;&#32451;&#26679;&#26412;&#22686;&#30410;&#30340;&#29702;&#35770;&#19978;&#26368;&#20248;&#23398;&#20064;&#31574;&#30053;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#26368;&#20248;&#23398;&#20064;&#31574;&#30053;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11125v1 Announce Type: cross  Abstract: Machine learning-based reliability analysis methods have shown great advancements for their computational efficiency and accuracy. Recently, many efficient learning strategies have been proposed to enhance the computational performance. However, few of them explores the theoretical optimal learning strategy. In this article, we propose several theorems that facilitates such exploration. Specifically, cases that considering and neglecting the correlations among the candidate design samples are well elaborated. Moreover, we prove that the well-known U learning function can be reformulated to the optimal learning function for the case neglecting the Kriging correlation. In addition, the theoretical optimal learning strategy for sequential multiple training samples enrichment is also mathematically explored through the Bayesian estimate with the corresponding lost functions. Simulation results show that the optimal learning strategy consid
&lt;/p&gt;</description></item><item><title>&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#35270;&#39057;&#20013;&#21516;&#26102;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#23454;&#29616;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10488</link><description>&lt;p&gt;
&#37326;&#22806;&#24773;&#24863;&#32500;&#24230;&#35782;&#21035;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer
&lt;/p&gt;
&lt;p&gt;
Joint Multimodal Transformer for Dimensional Emotional Recognition in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;&#35270;&#39057;&#20013;&#21516;&#26102;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#23454;&#29616;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#20013;&#36827;&#34892;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#23545;&#20110;&#21333;&#27169;&#24615;&#33021;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#35270;&#35273;&#21644;&#21548;&#35273;&#27169;&#24577;&#20043;&#38388;&#20197;&#21450;&#27169;&#24577;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20851;&#38190;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#30340;&#38899;&#35270;&#39057;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#21033;&#29992;&#35270;&#39057;&#20013;&#38899;&#39057;&#21644;&#35270;&#35273;&#32447;&#32034;&#65288;&#38754;&#37096;&#34920;&#24773;&#21644;&#35821;&#38899;&#27169;&#24335;&#65289;&#30340;&#20114;&#34917;&#24615;&#65292;&#30456;&#36739;&#20110;&#20165;&#20381;&#36182;&#20110;&#21333;&#19968;&#27169;&#24577;&#65292;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#21333;&#29420;&#30340;&#20027;&#24178;&#32593;&#32476;&#26469;&#25429;&#33719;&#27599;&#31181;&#27169;&#24577;&#65288;&#38899;&#39057;&#21644;&#35270;&#35273;&#65289;&#20869;&#37096;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#32852;&#21512;&#22810;&#27169;&#24577;Transformer&#26550;&#26500;&#38598;&#25104;&#20102;&#21508;&#33258;&#27169;&#24577;&#30340;&#23884;&#20837;&#65292;&#20351;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#33719;&#27169;&#24577;&#38388;&#65288;&#38899;&#39057;&#21644;&#35270;&#35273;&#20043;&#38388;&#65289;&#21644;&#27169;&#24577;&#20869;&#37096;&#65288;&#27599;&#31181;&#27169;&#24577;&#20869;&#37096;&#65289;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10488v1 Announce Type: cross  Abstract: Audiovisual emotion recognition (ER) in videos has immense potential over unimodal performance. It effectively leverages the inter- and intra-modal dependencies between visual and auditory modalities. This work proposes a novel audio-visual emotion recognition system utilizing a joint multimodal transformer architecture with key-based cross-attention. This framework aims to exploit the complementary nature of audio and visual cues (facial expressions and vocal patterns) in videos, leading to superior performance compared to solely relying on a single modality. The proposed model leverages separate backbones for capturing intra-modal temporal dependencies within each modality (audio and visual). Subsequently, a joint multimodal transformer architecture integrates the individual modality embeddings, enabling the model to effectively capture inter-modal (between audio and visual) and intra-modal (within each modality) relationships. Exten
&lt;/p&gt;</description></item><item><title>&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#20250;&#22686;&#21152;&#29983;&#25104;&#38544;&#31169;&#39118;&#38505;&#65292;&#29978;&#33267;&#20351;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#25915;&#20987;&#25928;&#26524;&#25552;&#39640;5.4&#65285;&#65292;&#24182;&#21487;&#23558;&#25552;&#21462;&#30340;&#31169;&#26377;&#26679;&#26412;&#25968;&#37327;&#20174;&#20960;&#20046;0&#20010;&#22686;&#21152;&#21040;&#24179;&#22343;16.3&#20010;&#12290;</title><link>https://arxiv.org/abs/2403.09450</link><description>&lt;p&gt;
&#38663;&#21160;&#27844;&#23494;&#65306;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#22686;&#21152;&#29983;&#25104;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09450
&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#20250;&#22686;&#21152;&#29983;&#25104;&#38544;&#31169;&#39118;&#38505;&#65292;&#29978;&#33267;&#20351;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#25915;&#20987;&#25928;&#26524;&#25552;&#39640;5.4&#65285;&#65292;&#24182;&#21487;&#23558;&#25552;&#21462;&#30340;&#31169;&#26377;&#26679;&#26412;&#25968;&#37327;&#20174;&#20960;&#20046;0&#20010;&#22686;&#21152;&#21040;&#24179;&#22343;16.3&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#22312;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20063;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#65306;&#24050;&#21457;&#24067;&#30340;&#27169;&#22411;&#25110;API&#21487;&#33021;&#29983;&#25104;&#35757;&#32451;&#22270;&#20687;&#65292;&#20174;&#32780;&#27844;&#38706;&#28041;&#21450;&#38544;&#31169;&#30340;&#35757;&#32451;&#20449;&#24687;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#39118;&#38505;&#65292;&#21363;Shake-to-Leak (S2L)&#65292;&#21363;&#20351;&#29992;&#25805;&#32437;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#22686;&#21152;&#29616;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;S2L&#21487;&#33021;&#21457;&#29983;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#31574;&#30053;&#20013;&#65292;&#21253;&#25324;&#27010;&#24565;&#27880;&#20837;&#26041;&#27861;&#65288;DreamBooth&#21644;Textual Inversion&#65289;&#21644;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#65288;LoRA&#21644;Hypernetwork&#65289;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;&#22312;&#26368;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#65292;S2L&#21487;&#20197;&#23558;&#25193;&#25955;&#27169;&#22411;&#19978;&#30340;&#26368;&#26032;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#30340;AUC&#25552;&#39640;5.4&#65285;&#65288;&#32477;&#23545;&#24046;&#24322;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#27599;&#20010;&#30446;&#26631;&#22495;&#30340;&#25552;&#21462;&#31169;&#26377;&#26679;&#26412;&#20174;&#20960;&#20046;0&#20010;&#26679;&#26412;&#22686;&#21152;&#21040;&#24179;&#22343;16.3&#20010;&#26679;&#26412;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09450v1 Announce Type: new  Abstract: While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#21517;&#20026;&#35748;&#30693;&#23433;&#20840;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#20998;&#26512;&#31070;&#32463;&#25216;&#26415;&#23545;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#35299;&#20915;&#20102;&#30456;&#20851;&#38382;&#39064;&#25551;&#36848;&#21644;&#20998;&#26512;&#30340;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.07945</link><description>&lt;p&gt;
&#19968;&#20010;&#35299;&#20915;&#31070;&#32463;&#25216;&#26415;&#35748;&#30693;&#23433;&#20840;&#38382;&#39064;&#30340;&#25968;&#23398;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#21517;&#20026;&#35748;&#30693;&#23433;&#20840;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#20998;&#26512;&#31070;&#32463;&#25216;&#26415;&#23545;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#35299;&#20915;&#20102;&#30456;&#20851;&#38382;&#39064;&#25551;&#36848;&#21644;&#20998;&#26512;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#31070;&#32463;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#31070;&#32463;&#25216;&#26415;&#21644;&#23433;&#20840;&#20043;&#38388;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20851;&#38190;&#20132;&#21449;&#28857;&#12290;&#26893;&#20837;&#24335;&#35774;&#22791;&#12289;&#38750;&#20405;&#20837;&#24335;&#30417;&#27979;&#21644;&#38750;&#20405;&#20837;&#24335;&#27835;&#30103;&#37117;&#24102;&#26469;&#20102;&#36829;&#21453;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#30340;&#21069;&#26223;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#23398;&#23478;&#21644;&#21307;&#29983;&#21628;&#21505;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064; -- &#25105;&#20204;&#31216;&#20043;&#20026;&#35748;&#30693;&#23433;&#20840; -- &#20294;&#24212;&#29992;&#24037;&#20316;&#21463;&#21040;&#38480;&#21046;&#12290;&#38459;&#30861;&#31185;&#23398;&#21644;&#24037;&#31243;&#21162;&#21147;&#35299;&#20915;&#35748;&#30693;&#23433;&#20840;&#38382;&#39064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#28165;&#26224;&#25551;&#36848;&#21644;&#20998;&#26512;&#30456;&#20851;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35748;&#30693;&#23433;&#20840;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#37492;&#22810;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#65292;&#23454;&#29616;&#36825;&#31181;&#25551;&#36848;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#23545;&#35748;&#30693;&#23433;&#20840;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#28982;&#21518;&#25552;&#20986;&#25551;&#36848;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07945v1 Announce Type: cross  Abstract: The rapid advancement in neurotechnology in recent years has created an emerging critical intersection between neurotechnology and security. Implantable devices, non-invasive monitoring, and non-invasive therapies all carry with them the prospect of violating the privacy and autonomy of individuals' cognition. A growing number of scientists and physicians have made calls to address this issue -- which we term Cognitive Security -- but applied efforts have been limited. A major barrier hampering scientific and engineering efforts to address Cognitive Security is the lack of a clear means of describing and analyzing relevant problems. In this paper we develop Cognitive Security, a mathematical framework which enables such description and analysis by drawing on methods and results from multiple fields. We demonstrate certain statistical properties which have significant implications for Cognitive Security, and then present descriptions of
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07718</link><description>&lt;p&gt;
WorkArena&#65306;Web&#20195;&#29702;&#22312;&#35299;&#20915;&#24120;&#35265;&#30693;&#35782;&#24037;&#20316;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#19982;&#36719;&#20214;&#36890;&#36807;web&#27983;&#35272;&#22120;&#20132;&#20114;&#30340;&#24212;&#29992;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#34913;&#37327;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#21033;&#29992;&#20225;&#19994;&#36719;&#20214;&#31995;&#32479;&#30340;&#30693;&#35782;&#24037;&#20316;&#32773;&#30340;&#20856;&#22411;&#26085;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WorkArena&#65292;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;ServiceNow&#24179;&#21488;&#30340;29&#20010;&#20219;&#21153;&#30340;&#36828;&#31243;&#20027;&#26426;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BrowserGym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36825;&#20123;&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#20195;&#29702;&#22312;WorkArena&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#35201;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#20043;&#38388;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;&#25506;&#32034;&#21644;&#21457;&#23637;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
&lt;/p&gt;</description></item><item><title>BDM&#26159;&#19968;&#31181;&#21033;&#29992;&#32852;&#21512;&#25193;&#25955;&#36807;&#31243;&#32039;&#23494;&#32806;&#21512;&#20808;&#39564;&#20449;&#24687;&#19982;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#19987;&#27880;&#20110;3D&#24418;&#29366;&#37325;&#24314;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#26631;&#31614;&#30340;&#20016;&#23500;&#20808;&#39564;&#20449;&#24687;&#26469;&#25913;&#21892;&#33258;&#19979;&#32780;&#19978;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.06973</link><description>&lt;p&gt;
&#19977;&#32500;&#24418;&#29366;&#37325;&#24314;&#30340;&#36125;&#21494;&#26031;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bayesian Diffusion Models for 3D Shape Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06973
&lt;/p&gt;
&lt;p&gt;
BDM&#26159;&#19968;&#31181;&#21033;&#29992;&#32852;&#21512;&#25193;&#25955;&#36807;&#31243;&#32039;&#23494;&#32806;&#21512;&#20808;&#39564;&#20449;&#24687;&#19982;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#19987;&#27880;&#20110;3D&#24418;&#29366;&#37325;&#24314;&#20219;&#21153;&#65292;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#26631;&#31614;&#30340;&#20016;&#23500;&#20808;&#39564;&#20449;&#24687;&#26469;&#25913;&#21892;&#33258;&#19979;&#32780;&#19978;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#25193;&#25955;&#27169;&#22411;&#65288;BDM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#36807;&#31243;&#23558;&#33258;&#19978;&#32780;&#19979;&#65288;&#20808;&#39564;&#65289;&#20449;&#24687;&#19982;&#33258;&#19979;&#32780;&#19978;&#65288;&#25968;&#25454;&#39537;&#21160;&#65289;&#36807;&#31243;&#32039;&#23494;&#32806;&#21512;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BDM&#22312;3D&#24418;&#29366;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#22522;&#20110;&#37197;&#23545;&#65288;&#30417;&#30563;&#65289;&#25968;&#25454;&#26631;&#31614;&#65288;&#20363;&#22914;&#22270;&#20687;-&#28857;&#20113;&#65289;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#20856;&#22411;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;BDM&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#29420;&#31435;&#26631;&#31614;&#65288;&#20363;&#22914;&#28857;&#20113;&#65289;&#30340;&#20016;&#23500;&#20808;&#39564;&#20449;&#24687;&#26469;&#25913;&#36827;&#33258;&#19979;&#32780;&#19978;&#30340;3D&#37325;&#24314;&#12290;&#19982;&#26631;&#20934;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#30456;&#21453;&#65292;&#20854;&#38656;&#35201;&#26126;&#30830;&#30340;&#20808;&#39564;&#21644;&#20284;&#28982;&#36827;&#34892;&#25512;&#26029;&#65292;BDM&#36890;&#36807;&#23398;&#20064;&#30340;&#26799;&#24230;&#35745;&#31639;&#32593;&#32476;&#36890;&#36807;&#32806;&#21512;&#25193;&#25955;&#36807;&#31243;&#25191;&#34892;&#26080;&#32541;&#20449;&#24687;&#34701;&#21512;&#12290;&#25105;&#20204;BDM&#30340;&#29305;&#27530;&#20043;&#22788;&#22312;&#20110;&#20854;&#33021;&#22815;&#22312;&#33258;&#19978;&#32780;&#19979;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#20027;&#21160;&#26377;&#25928;&#20449;&#24687;&#20132;&#25442;&#21644;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06973v1 Announce Type: cross  Abstract: We present Bayesian Diffusion Models (BDM), a prediction algorithm that performs effective Bayesian inference by tightly coupling the top-down (prior) information with the bottom-up (data-driven) procedure via joint diffusion processes. We show the effectiveness of BDM on the 3D shape reconstruction task. Compared to prototypical deep learning data-driven approaches trained on paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM brings in rich prior information from standalone labels (e.g. point clouds) to improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian frameworks where explicit prior and likelihood are required for the inference, BDM performs seamless information fusion via coupled diffusion processes with learned gradient computation networks. The specialty of our BDM lies in its capability to engage the active and effective information exchange and fusion of the top-down and bottom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hodge-Laplacian&#24322;&#26500;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;HL-HGAT&#65289;&#65292;&#36890;&#36807;HL&#21367;&#31215;&#28388;&#27874;&#22120;&#12289;&#21333;&#32431;&#25237;&#24433;&#21644;&#21333;&#32431;&#27880;&#24847;&#27744;&#21270;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#23398;&#20064;&#36328;k-&#21333;&#32431;&#20307;&#30340;&#24322;&#26500;&#20449;&#21495;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.06687</link><description>&lt;p&gt;
&#29992;Hodge-Laplacian&#21644;&#27880;&#24847;&#26426;&#21046;&#36827;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and Attention Mechanism Approach for Heterogeneous Graph-Structured Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Hodge-Laplacian&#24322;&#26500;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;HL-HGAT&#65289;&#65292;&#36890;&#36807;HL&#21367;&#31215;&#28388;&#27874;&#22120;&#12289;&#21333;&#32431;&#25237;&#24433;&#21644;&#21333;&#32431;&#27880;&#24847;&#27744;&#21270;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#23398;&#20064;&#36328;k-&#21333;&#32431;&#20307;&#30340;&#24322;&#26500;&#20449;&#21495;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#25429;&#25417;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#23558;&#22270;&#20316;&#20026;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#26032;&#35270;&#35282;&#65292;&#21253;&#21547;&#33410;&#28857;&#12289;&#36793;&#12289;&#19977;&#35282;&#24418;&#21644;k-&#21333;&#32431;&#20307;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#22312;&#20219;&#20309;k-&#21333;&#32431;&#20307;&#19978;&#23450;&#20041;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;Hodge-Laplacian&#24322;&#26500;&#22270;&#27880;&#24847;&#32593;&#32476;&#65288;HL-HGAT&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#36328;k-&#21333;&#32431;&#20307;&#30340;&#24322;&#26500;&#20449;&#21495;&#34920;&#31034;&#12290;HL-HGAT&#21253;&#21547;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;HL&#21367;&#31215;&#28388;&#27874;&#22120;&#65288;HL-filters&#65289;&#12289;&#21333;&#32431;&#25237;&#24433;&#65288;SP&#65289;&#21644;&#21333;&#32431;&#27880;&#24847;&#27744;&#21270;&#65288;SAP&#65289;&#36816;&#31639;&#31526;&#65292;&#24212;&#29992;&#20110;k-&#21333;&#32431;&#20307;&#12290;HL-filters&#21033;&#29992;&#30001;Hodge-Laplacian&#65288;HL&#65289;&#31639;&#23376;&#32534;&#30721;&#30340;k-&#21333;&#32431;&#20307;&#30340;&#29420;&#29305;&#25299;&#25169;&#32467;&#26500;&#65292;&#22312;k-th HL&#31639;&#23376;&#30340;&#39057;&#35889;&#22495;&#20869;&#36816;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;HL-filters&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#23637;&#31034;&#20102;&#22312;&#35889;&#22495;&#35299;&#20915;&#24322;&#26500;&#22270;&#27880;&#24847;&#36328;k-&#21333;&#32431;&#20307;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06687v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have proven effective in capturing relationships among nodes in a graph. This study introduces a novel perspective by considering a graph as a simplicial complex, encompassing nodes, edges, triangles, and $k$-simplices, enabling the definition of graph-structured data on any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous graph attention network (HL-HGAT), designed to learn heterogeneous signal representations across $k$-simplices. The HL-HGAT incorporates three key components: HL convolutional filters (HL-filters), simplicial projection (SP), and simplicial attention pooling (SAP) operators, applied to $k$-simplices. HL-filters leverage the unique topology of $k$-simplices encoded by the Hodge-Laplacian (HL) operator, operating within the spectral domain of the $k$-th HL operator. To address computation challenges, we introduce a polynomial approximation for HL-filters, exhibiting spatia
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;DCA&#65289;&#27169;&#22411;&#65292;&#26681;&#25454;&#38899;&#39057;&#21644;&#35270;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#24378;&#24369;&#20114;&#34917;&#20851;&#31995;&#65292;&#21160;&#24577;&#36873;&#25321;&#20132;&#21449;&#20851;&#27880;&#25110;&#19981;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.04661</link><description>&lt;p&gt;
&#38754;&#21521;&#38899;&#39057;-&#35270;&#39057;&#20154;&#21592;&#39564;&#35777;&#30340;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Dynamic Cross Attention for Audio-Visual Person Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;DCA&#65289;&#27169;&#22411;&#65292;&#26681;&#25454;&#38899;&#39057;&#21644;&#35270;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#24378;&#24369;&#20114;&#34917;&#20851;&#31995;&#65292;&#21160;&#24577;&#36873;&#25321;&#20132;&#21449;&#20851;&#27880;&#25110;&#19981;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#21592;&#25110;&#36523;&#20221;&#39564;&#35777;&#36890;&#24120;&#20351;&#29992;&#20010;&#20307;&#27169;&#24577;&#65288;&#22914;&#38754;&#37096;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#25506;&#32034;&#65292;&#20294;&#26368;&#36817;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#30340;&#38899;&#35270;&#39057;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#21333;&#27169;&#24577;&#26041;&#27861;&#12290;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#36890;&#24120;&#34987;&#26399;&#26395;&#20855;&#26377;&#24378;&#28872;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#22312;&#26377;&#25928;&#30340;&#38899;&#35270;&#39057;&#34701;&#21512;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#24635;&#26159;&#24378;&#28872;&#30456;&#20114;&#34917;&#20805;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#23637;&#29616;&#20986;&#24369;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#23548;&#33268;&#38899;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;DCA&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#36328;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#24378;&#24369;&#20114;&#34917;&#20851;&#31995;&#65292;&#21160;&#24577;&#36873;&#25321;&#20132;&#21449;&#20851;&#27880;&#25110;&#19981;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26465;&#20214;&#38376;&#25511;&#23618;&#26469;&#35780;&#20272;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36129;&#29486;&#65292;&#24182;&#20165;&#36873;&#25321;&#36328;&#27169;&#24577;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04661v1 Announce Type: cross  Abstract: Although person or identity verification has been predominantly explored using individual modalities such as face and voice, audio-visual fusion has recently shown immense potential to outperform unimodal approaches. Audio and visual modalities are often expected to pose strong complementary relationships, which plays a crucial role in effective audio-visual fusion. However, they may not always strongly complement each other, they may also exhibit weak complementary relationships, resulting in poor audio-visual feature representations. In this paper, we propose a Dynamic Cross-Attention (DCA) model that can dynamically select the cross-attended or unattended features on the fly based on the strong or weak complementary relationships, respectively, across audio and visual modalities. In particular, a conditional gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36719;&#32422;&#26463;&#34203;&#23450;&#35860;&#26725;(SSB)&#25511;&#21046;&#38382;&#39064;&#65292;&#22312;&#20801;&#35768;&#32456;&#31471;&#20998;&#24067;&#19982;&#39044;&#20808;&#25351;&#23450;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#24809;&#32602;&#20004;&#32773;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#12290;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20102;SSB&#35299;&#65292;&#26174;&#31034;&#26368;&#20248;&#25511;&#21046;&#36807;&#31243;&#30340;&#32456;&#31471;&#20998;&#24067;&#26159;&#956;T&#21644;&#20854;&#20182;&#20998;&#24067;&#30340;&#20960;&#20309;&#28151;&#21512;&#65292;&#24182;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#26102;&#38388;&#24207;&#21015;&#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.01717</link><description>&lt;p&gt;
&#36719;&#32422;&#26463;&#34203;&#23450;&#35860;&#26725;&#65306;&#19968;&#31181;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Soft-constrained Schrodinger Bridge: a Stochastic Control Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01717
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36719;&#32422;&#26463;&#34203;&#23450;&#35860;&#26725;(SSB)&#25511;&#21046;&#38382;&#39064;&#65292;&#22312;&#20801;&#35768;&#32456;&#31471;&#20998;&#24067;&#19982;&#39044;&#20808;&#25351;&#23450;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#24809;&#32602;&#20004;&#32773;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#12290;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20102;SSB&#35299;&#65292;&#26174;&#31034;&#26368;&#20248;&#25511;&#21046;&#36807;&#31243;&#30340;&#32456;&#31471;&#20998;&#24067;&#26159;&#956;T&#21644;&#20854;&#20182;&#20998;&#24067;&#30340;&#20960;&#20309;&#28151;&#21512;&#65292;&#24182;&#23558;&#32467;&#26524;&#25193;&#23637;&#21040;&#26102;&#38388;&#24207;&#21015;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34203;&#23450;&#35860;&#26725;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#30340;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20855;&#26377;&#39044;&#20808;&#25351;&#23450;&#30340;&#32456;&#31471;&#20998;&#24067;&#956;T&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20801;&#35768;&#32456;&#31471;&#20998;&#24067;&#19982;&#956;T&#19981;&#21516;&#20294;&#24809;&#32602;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#26469;&#27867;&#21270;&#36825;&#20010;&#38543;&#26426;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#30340;&#25511;&#21046;&#38382;&#39064;&#31216;&#20026;&#36719;&#32422;&#26463;&#34203;&#23450;&#35860;&#26725;(SSB)&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;SSB&#35299;&#30340;&#29702;&#35770;&#25512;&#23548;&#65292;&#34920;&#26126;&#26368;&#20248;&#25511;&#21046;&#36807;&#31243;&#30340;&#32456;&#31471;&#20998;&#24067;&#26159;&#956;T&#21644;&#21478;&#19968;&#20123;&#20998;&#24067;&#30340;&#20960;&#20309;&#28151;&#21512;&#12290;&#36825;&#20010;&#32467;&#26524;&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;&#26102;&#38388;&#24207;&#21015;&#35774;&#32622;&#12290;SSB&#30340;&#19968;&#20010;&#24212;&#29992;&#26159;&#40065;&#26834;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#25968;&#21305;&#37197;&#30340;&#31639;&#27861;&#26469;&#20174;&#20960;&#20309;&#28151;&#21512;&#20013;&#36827;&#34892;&#25277;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#29992;&#36884;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01717v1 Announce Type: cross  Abstract: Schr\"{o}dinger bridge can be viewed as a continuous-time stochastic control problem where the goal is to find an optimally controlled diffusion process with a pre-specified terminal distribution $\mu_T$. We propose to generalize this stochastic control problem by allowing the terminal distribution to differ from $\mu_T$ but penalizing the Kullback-Leibler divergence between the two distributions. We call this new control problem soft-constrained Schr\"{o}dinger bridge (SSB). The main contribution of this work is a theoretical derivation of the solution to SSB, which shows that the terminal distribution of the optimally controlled process is a geometric mixture of $\mu_T$ and some other distribution. This result is further extended to a time series setting. One application of SSB is the development of robust generative diffusion models. We propose a score matching-based algorithm for sampling from geometric mixtures and showcase its us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#26469;&#26356;&#22909;&#22320;&#39044;&#27979;&#31038;&#21306;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#36830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17905</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;
&lt;/p&gt;
&lt;p&gt;
Using Graph Neural Networks to Predict Local Culture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#26469;&#26356;&#22909;&#22320;&#39044;&#27979;&#31038;&#21306;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#36830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#30740;&#31350;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#35748;&#35782;&#21040;&#31038;&#21306;&#26159;&#21160;&#24577;&#21644;&#20851;&#32852;&#30340;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#25968;&#25454;&#12289;&#26041;&#27861;&#35770;&#21644;&#35745;&#31639;&#22788;&#29702;&#33021;&#21147;&#38459;&#30861;&#20102;&#23545;&#31038;&#21306;&#20851;&#31995;&#21160;&#24577;&#36827;&#34892;&#27491;&#24335;&#23450;&#37327;&#20998;&#26512;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#36827;&#23637;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#32467;&#21512;&#21644;&#35780;&#20272;&#20851;&#20110;&#31038;&#21306;&#20869;&#37096;&#29305;&#24449;&#12289;&#23427;&#20204;&#30340;&#36807;&#21435;&#29305;&#24449;&#20197;&#21450;&#22312;&#23427;&#20204;&#20043;&#38388;&#27969;&#21160;&#30340;&#32676;&#20307;&#30340;&#22810;&#20010;&#20449;&#24687;&#28304;&#65292;&#28508;&#22312;&#22320;&#20026;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36890;&#36807;&#25506;&#32034; Yelp &#30340;&#20844;&#24320;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#36830;&#24615;&#26041;&#38754;&#23545;&#39044;&#27979;&#31038;&#21306;&#23646;&#24615;&#65288;&#29305;&#21035;&#26159;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;&#65289;&#30340;&#28508;&#21147;&#12290;&#20174;&#23454;&#36136;&#21644;&#26041;&#27861;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#32467;&#26524;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#12290;&#20174;&#23454;&#36136;&#19978;&#35762;&#65292;&#25105;&#20204;&#21457;&#29616;&#26080;&#35770;&#26159;&#24403;&#22320;&#21306;&#22495;&#20449;&#24687;&#65288;&#20363;&#22914;&#21306;&#22495;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17905v1 Announce Type: new  Abstract: Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15163</link><description>&lt;p&gt;
&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#20013;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Studying the Impact of Stochasticity on the Evaluation of Deep Neural Networks for Forest-Fire Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#29992;&#20110;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#37326;&#28779;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#20004;&#31867;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#20998;&#31867;&#30340;&#25351;&#26631;&#65292;&#35780;&#20272;&#23545;&#35266;&#23519;&#22320;&#38754;&#30495;&#30456;&#65288;GT&#65289;&#30340;&#24544;&#23454;&#24230;&#65292;&#20197;&#21450;&#36866;&#24403;&#30340;&#24471;&#20998;&#35268;&#21017;&#65292;&#27979;&#35797;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26862;&#26519;&#28779;&#28798;&#25968;&#25454;&#65292;&#31361;&#26174;&#20102;&#20256;&#32479;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#21487;&#35299;&#37322;&#30340;&#36866;&#29992;&#20110;&#38543;&#26426;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15163v1 Announce Type: cross  Abstract: This paper presents the first systematic study of the evaluation of Deep Neural Networks (DNNs) for discrete dynamical systems under stochastic assumptions, with a focus on wildfire prediction. We develop a framework to study the impact of stochasticity on two classes of evaluation metrics: classification-based metrics, which assess fidelity to observed ground truth (GT), and proper scoring rules, which test fidelity-to-statistic. Our findings reveal that evaluating for fidelity-to-statistic is a reliable alternative in highly stochastic scenarios. We extend our analysis to real-world wildfire data, highlighting limitations in traditional wildfire prediction evaluation methods, and suggest interpretable stochasticity-compatible alternatives.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#25552;&#21462;&#20107;&#20214;&#20998;&#31867;&#29305;&#24449;&#65292;&#24182;&#35780;&#20272;&#20102;&#21487;&#35299;&#37322;&#20998;&#31867;&#22120;&#22312;&#38754;&#23545;&#23545;&#25239;&#31639;&#27861;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12338</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#20107;&#20214;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#36827;&#34892;&#35780;&#20272;&#30340;&#23545;&#25239;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Approach to Evaluating the Robustness of Event Identification Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#25552;&#21462;&#20107;&#20214;&#20998;&#31867;&#29305;&#24449;&#65292;&#24182;&#35780;&#20272;&#20102;&#21487;&#35299;&#37322;&#20998;&#31867;&#22120;&#22312;&#38754;&#23545;&#23545;&#25239;&#31639;&#27861;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#27491;&#22312;&#31215;&#26497;&#29992;&#20110;&#20107;&#20214;&#26816;&#27979;&#21644;&#35782;&#21035;&#65292;&#21487;&#23454;&#26102;&#33719;&#24471;&#24577;&#21183;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#20256;&#20837;&#36965;&#27979;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#26469;&#25552;&#21462;&#20107;&#20214;&#20998;&#31867;&#30340;&#29305;&#24449;&#65292;&#24182;&#19987;&#27880;&#20110;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#20197;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#20107;&#20214;&#65306;&#36127;&#36733;&#25439;&#22833;&#21644;&#21457;&#30005;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#23545;&#29983;&#25104;&#30340;&#20998;&#31867;&#22120;&#36827;&#34892;&#23545;&#25239;&#31639;&#27861;&#27979;&#35797;&#20197;&#35780;&#20272;&#20854;&#40065;&#26834;&#24615;&#12290;&#23545;&#25239;&#25915;&#20987;&#22312;&#20004;&#31181;&#24773;&#22659;&#19979;&#36827;&#34892;&#27979;&#35797;&#65306;&#30333;&#30418;&#35774;&#32622;&#65292;&#25915;&#20987;&#32773;&#23436;&#20840;&#20102;&#35299;&#20998;&#31867;&#27169;&#22411;&#65307;&#28784;&#30418;&#35774;&#32622;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#19982;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#30456;&#21516;&#32593;&#32476;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#20294;&#19981;&#30693;&#36947;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12338v1 Announce Type: cross  Abstract: Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness. Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data. This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss. The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness. The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classifica
&lt;/p&gt;</description></item><item><title>&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26082;&#31526;&#21512;&#26657;&#20934;&#30340;&#39044;&#27979;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07307</link><description>&lt;p&gt;
&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07307
&lt;/p&gt;
&lt;p&gt;
&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26082;&#31526;&#21512;&#26657;&#20934;&#30340;&#39044;&#27979;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574;&#20013;&#65292;&#20915;&#31574;&#32773;&#36890;&#24120;&#22312;&#20855;&#26377;&#30456;&#21516;&#39044;&#27979;&#32467;&#26524;&#30340;&#24773;&#22659;&#20013;&#37319;&#21462;&#30456;&#21516;&#30340;&#34892;&#21160;&#12290;&#31526;&#21512;&#39044;&#27979;&#24110;&#21161;&#20915;&#31574;&#32773;&#37327;&#21270;&#21160;&#20316;&#30340;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#39118;&#38505;&#31649;&#29702;&#12290;&#21463;&#36825;&#31181;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#65292;&#23427;&#20135;&#29983;&#20102;&#26082;&#31526;&#21512;Venn-Abers&#26657;&#20934;&#30340;&#39044;&#27979;&#65292;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#24341;&#21457;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21518;&#39564;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#39044;&#27979;&#22120;&#65292;&#25552;&#20379;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21306;&#38388;&#30340;&#25928;&#29575;&#21644;&#26465;&#20214;&#30340;&#26377;&#25928;&#24615;&#20043;&#38388;&#36798;&#21040;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions prompted by model predictions. Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01830</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#21516;&#34892;&#35780;&#23457;&#26041;&#27861;&#65306;&#24320;&#25918;&#29615;&#22659;&#19979;LLMs&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in Open-environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#22312;&#24320;&#25918;&#29615;&#22659;&#20013;&#34913;&#37327;LLMs&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#65292;&#20197;&#26368;&#22823;&#21270;&#21508;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#38598;&#20013;&#20110;&#22312;&#19968;&#20123;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#23553;&#38381;&#29615;&#22659;&#21644;&#29305;&#23450;&#39046;&#22495;&#22522;&#20934;&#19978;&#27979;&#35797;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#23457;&#26426;&#21046;&#33258;&#21160;&#34913;&#37327;LLMs&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#24320;&#28304;&#21644;&#38381;&#28304;&#30340;LLMs&#22788;&#20110;&#21516;&#19968;&#29615;&#22659;&#20013;&#65292;&#33021;&#22815;&#22238;&#31572;&#26410;&#26631;&#35760;&#30340;&#38382;&#39064;&#24182;&#20114;&#30456;&#35780;&#20272;&#65292;&#27599;&#20010;LLM&#30340;&#21709;&#24212;&#24471;&#20998;&#30001;&#20854;&#20182;&#21311;&#21517;&#30340;LLMs&#20849;&#21516;&#20915;&#23450;&#12290;&#20026;&#20102;&#33719;&#21462;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#30340;&#33021;&#21147;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;LLM&#20998;&#37197;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#33021;&#21147;&#21442;&#25968;&#26469;&#35843;&#25972;&#26368;&#32456;&#25490;&#24207;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#27599;&#20010;LLM&#30340;&#33021;&#21147;&#21644;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#12290;&#32972;&#21518;&#30340;&#20851;&#38190;&#20551;&#35774;&#26159;&#39640;&#23618;&#27425;&#30340;LLM&#33021;&#22815;&#27604;&#20302;&#23618;&#27425;&#30340;LLM&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#20854;&#20182;&#27169;&#22411;&#30340;&#31572;&#26696;&#65292;&#32780;&#39640;&#23618;&#27425;&#30340;LLM&#20063;&#21487;&#20197;&#36798;&#21040;&#36739;&#39640;&#30340;&#21709;&#24212;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26234;&#33021;&#25163;&#34920;&#30340;&#40614;&#20811;&#39118;&#20256;&#24863;&#22120;&#30417;&#27979;&#21644;&#20998;&#31867;&#20102;&#21508;&#31181;&#21683;&#22013;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#22788;&#29702;&#21644;&#19987;&#38376;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;98.49%&#21644;98.2%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#20986;&#22235;&#31181;&#19981;&#21516;&#30340;&#21683;&#22013;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17738</link><description>&lt;p&gt;
&#21033;&#29992;&#26234;&#33021;&#25163;&#34920;&#40614;&#20811;&#39118;&#20256;&#24863;&#22120;&#36827;&#34892;&#21683;&#22013;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Harnessing Smartwatch Microphone Sensors for Cough Detection and Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26234;&#33021;&#25163;&#34920;&#30340;&#40614;&#20811;&#39118;&#20256;&#24863;&#22120;&#30417;&#27979;&#21644;&#20998;&#31867;&#20102;&#21508;&#31181;&#21683;&#22013;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#22788;&#29702;&#21644;&#19987;&#38376;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;98.49%&#21644;98.2%&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#25104;&#21151;&#35782;&#21035;&#20986;&#22235;&#31181;&#19981;&#21516;&#30340;&#21683;&#22013;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#20869;&#32622;&#40614;&#20811;&#39118;&#20256;&#24863;&#22120;&#30340;&#26234;&#33021;&#25163;&#34920;&#30417;&#27979;&#21683;&#22013;&#24182;&#26816;&#27979;&#21508;&#31181;&#21683;&#22013;&#31867;&#22411;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#28041;&#21450;32&#21517;&#21442;&#19982;&#32773;&#30340;&#30740;&#31350;&#65292;&#24182;&#20197;&#21463;&#25511;&#26041;&#24335;&#25910;&#38598;&#20102;9&#20010;&#23567;&#26102;&#30340;&#38899;&#39057;&#25968;&#25454;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#32467;&#26500;&#21270;&#26041;&#27861;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20102;&#22788;&#29702;&#65292;&#24471;&#21040;&#20102;223&#20010;&#38451;&#24615;&#21683;&#22013;&#26679;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#22686;&#24378;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#20102;&#19987;&#38376;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#38750;&#27493;&#34892;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#29575;98.49%&#65292;&#22312;&#27493;&#34892;&#26102;&#20026;98.2%&#65292;&#34920;&#26126;&#26234;&#33021;&#25163;&#34920;&#21487;&#20197;&#26816;&#27979;&#21040;&#21683;&#22013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25104;&#21151;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#35782;&#21035;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#21683;&#22013;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the potential of using smartwatches with built-in microphone sensors for monitoring coughs and detecting various cough types. We conducted a study involving 32 participants and collected 9 hours of audio data in a controlled manner. Afterward, we processed this data using a structured approach, resulting in 223 positive cough samples. We further improved the dataset through augmentation techniques and employed a specialized 1D CNN model. This model achieved an impressive accuracy rate of 98.49% while non-walking and 98.2% while walking, showing smartwatches can detect cough. Moreover, our research successfully identified four distinct types of coughs using clustering techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#23454;&#26102;&#39044;&#27979;&#38271;&#36317;&#31163;&#22320;&#24418;&#39640;&#31243;&#22320;&#22270;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;transformer-based&#32534;&#30721;&#22120;&#12289;&#26041;&#21521;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#21382;&#21490;&#22686;&#24378;&#30340;&#21487;&#23398;&#20064;&#22320;&#22270;&#23884;&#20837;&#12290;&#36890;&#36807;&#23398;&#20064;&#35270;&#35282;&#22270;&#20687;&#19982;&#40479;&#30640;&#22270;&#39640;&#31243;&#22320;&#22270;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#32467;&#21512;&#36710;&#36742;&#23039;&#24577;&#20449;&#24687;&#21644;&#35270;&#35273;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#22320;&#22270;&#39044;&#27979;&#26102;&#24207;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17484</link><description>&lt;p&gt;
&#20687;&#32032;&#21040;&#39640;&#31243;&#65306;&#20351;&#29992;&#22270;&#20687;&#23398;&#20064;&#39044;&#27979;&#33258;&#20027;&#36234;&#37326;&#23548;&#33322;&#20013;&#30340;&#38271;&#36317;&#31163;&#39640;&#31243;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Pixel to Elevation: Learning to Predict Elevation Maps at Long Range using Images for Autonomous Offroad Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#23454;&#26102;&#39044;&#27979;&#38271;&#36317;&#31163;&#22320;&#24418;&#39640;&#31243;&#22320;&#22270;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;transformer-based&#32534;&#30721;&#22120;&#12289;&#26041;&#21521;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#21644;&#21382;&#21490;&#22686;&#24378;&#30340;&#21487;&#23398;&#20064;&#22320;&#22270;&#23884;&#20837;&#12290;&#36890;&#36807;&#23398;&#20064;&#35270;&#35282;&#22270;&#20687;&#19982;&#40479;&#30640;&#22270;&#39640;&#31243;&#22320;&#22270;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#32467;&#21512;&#36710;&#36742;&#23039;&#24577;&#20449;&#24687;&#21644;&#35270;&#35273;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#22320;&#22270;&#39044;&#27979;&#26102;&#24207;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#38271;&#36317;&#31163;&#29702;&#35299;&#22320;&#24418;&#25299;&#25169;&#23545;&#20110;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#36895;&#23548;&#33322;&#26102;&#12290;&#30446;&#21069;&#65292;&#20960;&#20309;&#26144;&#23556;&#20027;&#35201;&#20381;&#36182;&#20110;LiDAR&#20256;&#24863;&#22120;&#65292;&#20294;&#22312;&#26356;&#36828;&#36317;&#31163;&#30340;&#26144;&#23556;&#26102;&#25552;&#20379;&#30340;&#27979;&#37327;&#25968;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20165;&#20351;&#29992;&#23454;&#26102;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#39044;&#27979;&#38271;&#36317;&#31163;&#22320;&#24418;&#39640;&#31243;&#22320;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#19977;&#20010;&#20027;&#35201;&#20803;&#32032;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#23398;&#20064;&#36710;&#36733;&#35270;&#35282;&#22270;&#20687;&#19982;&#20808;&#21069;&#30340;&#40479;&#30640;&#22270;&#39640;&#31243;&#22320;&#22270;&#39044;&#27979;&#20043;&#38388;&#30340;&#36328;&#35270;&#22270;&#20851;&#32852;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#21521;&#24863;&#30693;&#30340;&#20301;&#32622;&#32534;&#30721;&#65292;&#23558;3D&#36710;&#36742;&#23039;&#24577;&#20449;&#24687;&#19982;&#22810;&#35270;&#35282;&#35270;&#35273;&#22270;&#20687;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#22320;&#24418;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21382;&#21490;&#22686;&#24378;&#30340;&#21487;&#23398;&#20064;&#22320;&#22270;&#23884;&#20837;&#65292;&#20197;&#23454;&#29616;&#39640;&#31243;&#22320;&#22270;&#39044;&#27979;&#20043;&#38388;&#30340;&#26356;&#22909;&#26102;&#24207;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding terrain topology at long-range is crucial for the success of off-road robotic missions, especially when navigating at high-speeds. LiDAR sensors, which are currently heavily relied upon for geometric mapping, provide sparse measurements when mapping at greater distances. To address this challenge, we present a novel learning-based approach capable of predicting terrain elevation maps at long-range using only onboard egocentric images in real-time. Our proposed method is comprised of three main elements. First, a transformer-based encoder is introduced that learns cross-view associations between the egocentric views and prior bird-eye-view elevation map predictions. Second, an orientation-aware positional encoding is proposed to incorporate the 3D vehicle pose information over complex unstructured terrain with multi-view visual image features. Lastly, a history-augmented learn-able map embedding is proposed to achieve better temporal consistency between elevation map predi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;Transformers&#21487;&#20197;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#33258;&#28982;&#22320;&#23398;&#20064;&#23454;&#29616;&#26799;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#26368;&#20248;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#21462;&#20915;&#20110;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2312.06528</link><description>&lt;p&gt;
Transformers&#23454;&#29616;&#20102;&#21151;&#33021;&#24615;&#26799;&#24230;&#19979;&#38477;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;Transformers&#21487;&#20197;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#33258;&#28982;&#22320;&#23398;&#20064;&#23454;&#29616;&#26799;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#26368;&#20248;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#21462;&#20915;&#20110;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#34987;&#35748;&#20026;&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#65292;&#22240;&#27492;&#21407;&#21017;&#19978;&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;Transformers&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#23427;&#20204;&#21487;&#20197;&#22312;&#31616;&#21333;&#30340;&#21442;&#25968;&#37197;&#32622;&#19979;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#65288;&#38750;&#32447;&#24615;&#65289;Transformers&#33258;&#28982;&#22320;&#23398;&#20250;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23454;&#29616;&#26799;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#22823;&#31867;&#38750;&#32447;&#24615;&#26550;&#26500;&#21644;&#38750;&#32447;&#24615;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#26368;&#20248;&#36873;&#25321;&#20197;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#21462;&#20915;&#20110;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06528v4 Announce Type: replace  Abstract: Many neural network architectures are known to be Turing Complete, and can thus, in principle implement arbitrary algorithms. However, Transformers are unique in that they can implement gradient-based learning algorithms under simple parameter configurations. This paper provides theoretical and empirical evidence that (non-linear) Transformers naturally learn to implement gradient descent in function space, which in turn enable them to learn non-linear functions in context. Our results apply to a broad class of combinations of non-linear architectures and non-linear in-context learning tasks. Additionally, we show that the optimal choice of non-linear activation depends in a natural way on the class of functions that need to be learned.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36125;&#21494;&#26031;&#27169;&#22411;&#31867;&#65292;&#36890;&#36807;&#39046;&#22495;&#32422;&#26463;&#30340;&#35774;&#32622;&#26469;&#25913;&#21892;&#27979;&#35797;&#21644;&#26410;&#27979;&#35797;&#24739;&#32773;&#30340;&#39118;&#38505;&#39044;&#27979;</title><link>https://arxiv.org/abs/2312.03878</link><description>&lt;p&gt;
&#39046;&#22495;&#32422;&#26463;&#22312;&#32570;&#22833;&#32467;&#26524;&#25968;&#25454;&#26102;&#25913;&#21892;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Domain constraints improve risk prediction when outcome data is missing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03878
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36125;&#21494;&#26031;&#27169;&#22411;&#31867;&#65292;&#36890;&#36807;&#39046;&#22495;&#32422;&#26463;&#30340;&#35774;&#32622;&#26469;&#25913;&#21892;&#27979;&#35797;&#21644;&#26410;&#27979;&#35797;&#24739;&#32773;&#30340;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#39044;&#27979;&#20154;&#31867;&#20915;&#31574;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21382;&#21490;&#20915;&#31574;&#20915;&#23450;&#20102;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#21482;&#35266;&#23519;&#21040;&#21307;&#29983;&#21382;&#21490;&#19978;&#27979;&#35797;&#30340;&#24739;&#32773;&#30340;&#27979;&#35797;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#27169;&#22411;&#31867;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#24773;&#20917;&#12290;&#27169;&#22411;&#30340;&#30446;&#30340;&#26159;&#20934;&#30830;&#20272;&#35745;&#27979;&#35797;&#21644;&#26410;&#27979;&#35797;&#24739;&#32773;&#30340;&#39118;&#38505;&#12290;&#30001;&#20110;&#26410;&#27979;&#35797;&#24739;&#32773;&#21487;&#33021;&#20986;&#29616;&#21508;&#31181;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#20272;&#35745;&#36825;&#20010;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22312;&#20581;&#24247;&#39046;&#22495;&#21512;&#29702;&#30340;&#39046;&#22495;&#32422;&#26463;&#65306;&#39044;valence&#32422;&#26463;&#65292;&#20854;&#20013;&#25972;&#20307;&#30142;&#30149;&#24739;&#30149;&#29575;&#26159;&#24050;&#30693;&#30340;&#65292;&#19987;&#19994;&#32422;&#26463;&#65292;&#20854;&#20013;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03878v2 Announce Type: replace  Abstract: Machine learning models are often trained to predict the outcome resulting from a human decision. For example, if a doctor decides to test a patient for disease, will the patient test positive? A challenge is that historical decision-making determines whether the outcome is observed: we only observe test outcomes for patients doctors historically tested. Untested patients, for whom outcomes are unobserved, may differ from tested patients along observed and unobserved dimensions. We propose a Bayesian model class which captures this setting. The purpose of the model is to accurately estimate risk for both tested and untested patients. Estimating this model is challenging due to the wide range of possibilities for untested patients. To address this, we propose two domain constraints which are plausible in health settings: a prevalence constraint, where the overall disease prevalence is known, and an expertise constraint, where the huma
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;UNITE&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#21644;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.02914</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#22495;&#33258;&#36866;&#24212;&#65306;&#37319;&#29992;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02914
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;UNITE&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#21644;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;UNITE&#65292;&#20351;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#26469;&#35843;&#25972;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#21040;&#30446;&#26631;&#22495;&#12290;UNITE&#39318;&#20808;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#25945;&#24072;&#24341;&#23548;&#30340;&#36974;&#34109;&#33976;&#39311;&#30446;&#26631;&#24471;&#21040;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#36974;&#34109;&#33258;&#35757;&#32451;&#65292;&#21033;&#29992;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#21644;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#19968;&#36215;&#20026;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#35270;&#39057;&#29983;&#25104;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#33258;&#35757;&#32451;&#36807;&#31243;&#25104;&#21151;&#21033;&#29992;&#20102;&#20004;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#36328;&#22495;&#24378;&#22823;&#30340;&#36716;&#31227;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#39057;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#30456;&#27604;&#20808;&#21069;&#25253;&#36947;&#30340;&#32467;&#26524;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02914v3 Announce Type: replace-cross  Abstract: In this work, we tackle the problem of unsupervised domain adaptation (UDA) for video action recognition. Our approach, which we call UNITE, uses an image teacher model to adapt a video student model to the target domain. UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective. We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos. Our self-training process successfully leverages the strengths of both models to achieve strong transfer performance across domains. We evaluate our approach on multiple video domain adaptation benchmarks and observe significant improvements upon previously reported results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.01201</link><description>&lt;p&gt;
PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PAC Privacy Preserving Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01201
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#27491;&#22312;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#26377;&#21487;&#33021;&#29983;&#25104;&#26082;&#20855;&#26377;&#39640;&#38544;&#31169;&#24615;&#21448;&#20855;&#26377;&#33391;&#22909;&#35270;&#35273;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#30830;&#20445;&#22312;&#31169;&#26377;&#21270;&#29305;&#23450;&#25968;&#25454;&#23646;&#24615;&#26102;&#30340;&#24378;&#22823;&#20445;&#25252;&#65292;&#24403;&#21069;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#38754;&#32463;&#24120;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21407;&#29702;&#24182;&#30830;&#20445;&#8220;&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#65288;PAC&#65289;&#8221;&#38544;&#31169;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;Langevin&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#27169;&#22411;&#38544;&#31169;&#24615;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#36825;&#20010;&#26032;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;&#30697;&#38453;&#35745;&#31639;&#25903;&#25345;PAC&#30028;&#38480;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#38544;&#31169;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01201v2 Announce Type: replace-cross  Abstract: Data privacy protection is garnering increased attention among researchers. Diffusion models (DMs), particularly with strict differential privacy, can potentially produce images with both high privacy and visual quality. However, challenges arise such as in ensuring robust protection in privatizing specific data attributes, areas where current models often fall short. To address these challenges, we introduce the PAC Privacy Preserving Diffusion Model, a model leverages diffusion principles and ensure Probably Approximately Correct (PAC) privacy. We enhance privacy protection by integrating a private classifier guidance into the Langevin Sampling Process. Additionally, recognizing the gap in measuring the privacy of models, we have developed a novel metric to gauge privacy levels. Our model, assessed with this new metric and supported by Gaussian matrix computations for the PAC bound, has shown superior performance in privacy p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Action-slot&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27133;&#27880;&#24847;&#21147;&#23398;&#20064;&#35270;&#35273;&#21160;&#20316;&#20013;&#24515;&#34920;&#31034;&#65292;&#22312;&#20132;&#36890;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#22810;&#26631;&#31614;&#21407;&#23376;&#27963;&#21160;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2311.17948</link><description>&lt;p&gt;
&#21160;&#20316;&#27133;&#65306;&#20132;&#36890;&#22330;&#26223;&#20013;&#22810;&#26631;&#31614;&#21407;&#23376;&#27963;&#21160;&#35782;&#21035;&#30340;&#35270;&#35273;&#21160;&#20316;&#20013;&#24515;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17948
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Action-slot&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27133;&#27880;&#24847;&#21147;&#23398;&#20064;&#35270;&#35273;&#21160;&#20316;&#20013;&#24515;&#34920;&#31034;&#65292;&#22312;&#20132;&#36890;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#22810;&#26631;&#31614;&#21407;&#23376;&#27963;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#21407;&#23376;&#27963;&#21160;&#35782;&#21035;&#12290;&#23613;&#31649;&#22312;&#21160;&#20316;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#23545;&#22810;&#20010;&#36947;&#36335;&#29992;&#25143;&#21160;&#20316;&#21450;&#20854;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#25972;&#20307;&#29702;&#35299;&#19981;&#36275;&#65292;&#35782;&#21035;&#21407;&#23376;&#27963;&#21160;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27133;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#8212;&#8212;&#21160;&#20316;&#27133;&#65292;&#23427;&#23398;&#20064;&#35270;&#35273;&#21160;&#20316;&#20013;&#24515;&#34920;&#31034;&#65292;&#25429;&#25417;&#20102;&#21160;&#20316;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#33021;&#22815;&#27880;&#24847;&#21040;&#21407;&#23376;&#27963;&#21160;&#21457;&#29983;&#20301;&#32622;&#30340;&#21160;&#20316;&#27133;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#24863;&#30693;&#24341;&#23548;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#27133;&#30340;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32972;&#26223;&#27133;&#65292;&#19982;&#21160;&#20316;&#27133;&#31454;&#20105;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#19981;&#24517;&#35201;&#22320;&#20851;&#27880;&#32570;&#20047;&#27963;&#21160;&#30340;&#32972;&#26223;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#19981;&#24179;&#34913;&#30340;&#31867;&#20998;&#24067;&#38459;&#30861;&#20102;&#23545;&#31232;&#26377;&#27963;&#21160;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17948v1 Announce Type: cross  Abstract: In this paper, we study multi-label atomic activity recognition. Despite the notable progress in action recognition, it is still challenging to recognize atomic activities due to a deficiency in a holistic understanding of both multiple road users' motions and their contextual information. In this paper, we introduce Action-slot, a slot attention-based approach that learns visual action-centric representations, capturing both motion and contextual information. Our key idea is to design action slots that are capable of paying attention to regions where atomic activities occur, without the need for explicit perception guidance. To further enhance slot attention, we introduce a background slot that competes with action slots, aiding the training process in avoiding unnecessary focus on background regions devoid of activities. Yet, the imbalanced class distribution in the existing dataset hampers the assessment of rare activities. To addre
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#23436;&#25972;&#25910;&#25947;&#29702;&#35770;&#20445;&#35777;&#65292;&#33719;&#24471;&#20102;&#23545;&#20110;&#21442;&#25968;&#20272;&#35745;&#21644;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#20248;&#19978;&#38480;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2311.13584</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#35823;&#24046;&#30028;&#38480;&#65306;&#23436;&#20840;&#25910;&#25947;&#20272;&#35745;&#19979;&#30340;&#23545;&#25968;&#20985;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13584
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#23436;&#25972;&#25910;&#25947;&#29702;&#35770;&#20445;&#35777;&#65292;&#33719;&#24471;&#20102;&#23545;&#20110;&#21442;&#25968;&#20272;&#35745;&#21644;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#20248;&#19978;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#20026;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#34892;&#20026;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#32780;&#25105;&#20204;&#29992;&#20110;&#24471;&#20998;&#20272;&#35745;&#30340;&#36924;&#36817;&#20989;&#25968;&#31867;&#30001;Lipschitz&#36830;&#32493;&#20989;&#25968;&#32452;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#28608;&#21169;&#24615;&#20363;&#23376;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24378;&#22823;&#20043;&#22788;&#65292;&#21363;&#20174;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#24471;&#20998;&#20272;&#35745;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#23558;&#20854;&#19982;&#30456;&#24212;&#30340;&#37319;&#26679;&#20272;&#35745;&#32467;&#21512;&#36215;&#26469;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;&#24050;&#30693;&#19978;&#38480;&#20272;&#35745;&#65292;&#28041;&#21450;&#20851;&#38190;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#65292;&#22914;&#25968;&#25454;&#20998;&#24067;&#65288;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#65289;&#19982;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20043;&#38388;&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#32500;&#24230;&#21644;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13584v2 Announce Type: replace  Abstract: We provide full theoretical guarantees for the convergence behaviour of diffusion-based generative models under the assumption of strongly log-concave data distributions while our approximating class of functions used for score estimation is made of Lipschitz continuous functions. We demonstrate via a motivating example, sampling from a Gaussian distribution with unknown mean, the powerfulness of our approach. In this case, explicit estimates are provided for the associated optimization problem, i.e. score approximation, while these are combined with the corresponding sampling estimates. As a result, we obtain the best known upper bound estimates in terms of key quantities of interest, such as the dimension and rates of convergence, for the Wasserstein-2 distance between the data distribution (Gaussian with unknown mean) and our sampling algorithm.   Beyond the motivating example and in order to allow for the use of a diverse range o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;</title><link>https://arxiv.org/abs/2311.07454</link><description>&lt;p&gt;
&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discrete Nonparametric Causal Discovery Under Latent Class Confounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#21521;&#26080;&#29615;&#22270;&#29992;&#20110;&#24314;&#27169;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;"&#22240;&#26524;&#21457;&#29616;"&#25551;&#36848;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#31181;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#24403;&#25968;&#25454;&#26159;&#26469;&#33258;&#22810;&#20010;&#28304;&#65288;&#32676;&#20307;&#25110;&#29615;&#22659;&#65289;&#30340;&#32858;&#21512;&#29289;&#26102;&#65292;&#20840;&#23616;&#28151;&#28102;&#20351;&#39537;&#21160;&#35768;&#22810;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#29305;&#24615;&#21464;&#24471;&#27169;&#31946;&#12290;&#36825;&#31181;&#24773;&#20917;&#26377;&#26102;&#34987;&#31216;&#20026;&#28151;&#21512;&#27169;&#22411;&#25110;&#28508;&#22312;&#31867;&#21035;&#12290;&#34429;&#28982;&#19968;&#20123;&#29616;&#20195;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#33021;&#22815;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#22788;&#29702;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#65292;&#20294;&#26159;&#30446;&#21069;&#25152;&#30693;&#30340;&#22788;&#29702;&#20840;&#23616;&#28151;&#28102;&#30340;&#26041;&#27861;&#37117;&#28041;&#21450;&#19981;&#36866;&#29992;&#20110;&#31163;&#25955;&#20998;&#24067;&#30340;&#21442;&#25968;&#20551;&#35774;&#12290;&#20197;&#31163;&#25955;&#21644;&#38750;&#21442;&#25968;&#35266;&#23519;&#21464;&#37327;&#20026;&#37325;&#28857;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#30001;&#20840;&#23616;&#28151;&#28102;&#30340;&#22522;&#25968;&#12289;&#35266;&#23519;&#21464;&#37327;&#30340;&#22522;&#25968;&#31561;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07454v2 Announce Type: replace Abstract: Directed acyclic graphs are used to model the causal structure of a system. ``Causal discovery'' describes the problem of learning this structure from data. When data is an aggregate from multiple sources (populations or environments), global confounding obscures conditional independence properties that drive many causal discovery algorithms. This setting is sometimes known as a mixture model or a latent class. While some modern methods for causal discovery are able to work around unobserved confounding in specific cases, the only known ways to deal with a global confounder involve parametric assumptions. that are unsuitable for discrete distributions.Focusing on discrete and non-parametric observed variables, we demonstrate that causal discovery can still be identifiable under bounded latent classes. The feasibility of this problem is governed by a trade-off between the cardinality of the global confounder, the cardinalities of the o
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2310.02226</link><description>&lt;p&gt;
&#35880;&#35328;&#24910;&#34892;&#65306;&#20351;&#29992;&#26242;&#20572;&#26631;&#35760;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Think before you speak: Training Language Models With Pause Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02226
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26242;&#20572;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#26631;&#35760;&#21069;&#22788;&#29702;&#26356;&#22810;&#38544;&#34255;&#21521;&#37327;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#31435;&#21363;&#36830;&#32493;&#29983;&#25104;&#19968;&#31995;&#21015;&#26631;&#35760;&#26469;&#29983;&#25104;&#21709;&#24212;: &#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#26159;&#36890;&#36807;&#25805;&#20316;&#27599;&#23618;&#30340;$K$&#20010;&#38544;&#34255;&#21521;&#37327;&#24471;&#21040;&#30340;&#65292;&#27599;&#20010;&#21521;&#37327;&#23545;&#24212;&#19968;&#20010;&#21069;&#38754;&#30340;&#26631;&#35760;&#12290;&#22914;&#26524;&#25105;&#20204;&#35753;&#27169;&#22411;&#22312;&#36755;&#20986;&#31532;$(K+1)^{th}$&#20010;&#26631;&#35760;&#20043;&#21069;&#25805;&#20316;&#26356;&#22810;&#30340;&#38544;&#34255;&#21521;&#37327;&#65292;&#27604;&#22914;&#35828;$K+10$&#20010;&#21602;&#65311;&#25105;&#20204;&#36890;&#36807;&#22312;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#65288;&#21487;&#23398;&#20064;&#30340;&#65289;$\textit{pause}$&#26631;&#35760;&#65292;&#36825;&#19968;&#31995;&#21015;&#26631;&#35760;&#38468;&#21152;&#21040;&#36755;&#20837;&#21069;&#32512;&#19978;&#12290;&#28982;&#21518;&#25105;&#20204;&#24310;&#36831;&#25552;&#21462;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#30452;&#21040;&#26368;&#21518;&#19968;&#20010;&#26242;&#20572;&#26631;&#35760;&#34987;&#30475;&#21040;&#65292;&#20174;&#32780;&#20801;&#35768;&#27169;&#22411;&#22312;&#20570;&#20986;&#31572;&#26696;&#20043;&#21069;&#36827;&#34892;&#39069;&#22806;&#30340;&#35745;&#31639;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#25317;&#26377;1B&#21644;130M&#21442;&#25968;&#30340;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;$\textit{pause-training}$&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#22312;C4&#19978;&#36827;&#34892;&#20102;&#22240;&#26524;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#28085;&#30422;&#25512;&#29702;&#12289;&#38382;&#31572;&#12289;&#26222;&#36941;&#29702;&#35299;&#21644;&#20107;&#23454;&#22238;&#24518;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;infer
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02226v2 Announce Type: replace-cross  Abstract: Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that infer
&lt;/p&gt;</description></item><item><title>RELIANCE&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#21644;&#26032;&#38395;&#30340;&#21487;&#20449;&#24230;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#30340;&#20934;&#30830;&#21306;&#20998;&#65292;&#24182;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.10940</link><description>&lt;p&gt;
RELIANCE: &#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#29992;&#20110;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RELIANCE: Reliable Ensemble Learning for Information and News Credibility Evaluation. (arXiv:2401.10940v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10940
&lt;/p&gt;
&lt;p&gt;
RELIANCE&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#65292;&#29992;&#20110;&#35780;&#20272;&#20449;&#24687;&#21644;&#26032;&#38395;&#30340;&#21487;&#20449;&#24230;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#23545;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#30340;&#20934;&#30830;&#21306;&#20998;&#65292;&#24182;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#27867;&#28389;&#30340;&#26102;&#20195;&#65292;&#36776;&#21035;&#26032;&#38395;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RELIANCE&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#40065;&#26834;&#20449;&#24687;&#21644;&#34394;&#20551;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#32780;&#35774;&#35745;&#30340;&#20808;&#36827;&#30340;&#38598;&#25104;&#23398;&#20064;&#31995;&#32479;&#12290;RELIANCE&#30001;&#20116;&#20010;&#19981;&#21516;&#30340;&#22522;&#26412;&#27169;&#22411;&#32452;&#25104;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;BiLSTMs&#65289;&#12290;RELIANCE&#37319;&#29992;&#20102;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25972;&#21512;&#23427;&#20204;&#30340;&#20248;&#21183;&#65292;&#21033;&#29992;&#38598;&#25104;&#30340;&#26234;&#33021;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;RELIANCE&#22312;&#21306;&#20998;&#21487;&#20449;&#21644;&#19981;&#21487;&#20449;&#20449;&#24687;&#28304;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#34920;&#26126;&#20854;&#22312;&#20449;&#24687;&#21644;&#26032;&#38395;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#38754;&#36229;&#36807;&#20102;&#21333;&#20010;&#27169;&#22411;&#65292;&#24182;&#25104;&#20026;&#35780;&#20272;&#20449;&#24687;&#28304;&#21487;&#38752;&#24615;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of information proliferation, discerning the credibility of news content poses an ever-growing challenge. This paper introduces RELIANCE, a pioneering ensemble learning system designed for robust information and fake news credibility evaluation. Comprising five diverse base models, including Support Vector Machine (SVM), naive Bayes, logistic regression, random forest, and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs an innovative approach to integrate their strengths, harnessing the collective intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the superiority of RELIANCE over individual models, indicating its efficacy in distinguishing between credible and non-credible information sources. RELIANCE, also surpasses baseline models in information and news credibility assessment, establishing itself as an effective solution for evaluating the reliability of information sources.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#33402;&#26415;&#21697;&#20813;&#21463;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#30340;&#28389;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20135;&#29983;&#23545;NST&#20855;&#26377;&#24178;&#25200;&#20316;&#29992;&#30340;&#25200;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.09673</link><description>&lt;p&gt;
&#20351;&#29992;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#23545;&#33402;&#26415;&#21697;&#36827;&#34892;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#30340;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Artwork Protection Against Neural Style Transfer Using Locally Adaptive Adversarial Color Attack. (arXiv:2401.09673v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#33402;&#26415;&#21697;&#20813;&#21463;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#30340;&#28389;&#29992;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#19981;&#21487;&#23519;&#35273;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20135;&#29983;&#23545;NST&#20855;&#26377;&#24178;&#25200;&#20316;&#29992;&#30340;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;NST&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#20219;&#24847;&#39118;&#26684;&#30340;&#26032;&#22270;&#20687;&#12290;&#36825;&#20010;&#36807;&#31243;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#39118;&#26684;&#22270;&#20687;&#30340;&#32654;&#23398;&#20803;&#32032;&#19982;&#20869;&#23481;&#22270;&#20687;&#30340;&#32467;&#26500;&#22240;&#32032;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#19968;&#20010;&#21644;&#35856;&#25972;&#21512;&#30340;&#35270;&#35273;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26410;&#32463;&#25480;&#26435;&#30340;NST&#21487;&#33021;&#20250;&#28389;&#29992;&#33402;&#26415;&#21697;&#12290;&#36825;&#31181;&#28389;&#29992;&#24341;&#36215;&#20102;&#20851;&#20110;&#33402;&#26415;&#23478;&#26435;&#21033;&#30340;&#31038;&#20250;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#20419;&#20351;&#24320;&#21457;&#25216;&#26415;&#26041;&#27861;&#26469;&#31215;&#26497;&#20445;&#25252;&#21407;&#22987;&#21019;&#20316;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#20027;&#35201;&#22312;&#26426;&#22120;&#23398;&#20064;&#23433;&#20840;&#20013;&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#19968;&#25216;&#26415;&#24341;&#20837;&#21040;&#20445;&#25252;&#33402;&#26415;&#23478;&#30693;&#35782;&#20135;&#26435;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26412;&#22320;&#33258;&#36866;&#24212;&#23545;&#25239;&#39068;&#33394;&#25915;&#20987;&#65288;LAACA&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20197;&#23545;&#20154;&#30524;&#19981;&#21487;&#23519;&#35273;&#20294;&#23545;NST&#20135;&#29983;&#24178;&#25200;&#30340;&#26041;&#24335;&#20462;&#25913;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#39640;&#39057;&#20869;&#23481;&#20016;&#23500;&#21306;&#22495;&#30340;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#30001;&#20013;&#38388;&#29305;&#24449;&#30340;&#30772;&#22351;&#20135;&#29983;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural style transfer (NST) is widely adopted in computer vision to generate new images with arbitrary styles. This process leverages neural networks to merge aesthetic elements of a style image with the structural aspects of a content image into a harmoniously integrated visual result. However, unauthorized NST can exploit artwork. Such misuse raises socio-technical concerns regarding artists' rights and motivates the development of technical approaches for the proactive protection of original creations. Adversarial attack is a concept primarily explored in machine learning security. Our work introduces this technique to protect artists' intellectual property. In this paper Locally Adaptive Adversarial Color Attack (LAACA), a method for altering images in a manner imperceptible to the human eyes but disruptive to NST. Specifically, we design perturbations targeting image areas rich in high-frequency content, generated by disrupting intermediate features. Our experiments and user study
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#23398;&#20064;&#20113;&#36752;&#23556;&#21453;&#39304;&#23545;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.09493</link><description>&lt;p&gt;
&#35782;&#21035;&#19982;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#26377;&#20851;&#30340;&#19977;&#32500;&#36752;&#23556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Identifying Three-Dimensional Radiative Patterns Associated with Early Tropical Cyclone Intensification. (arXiv:2401.09493v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26469;&#23398;&#20064;&#20113;&#36752;&#23556;&#21453;&#39304;&#23545;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#24378;&#21270;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#30340;&#24433;&#21709;&#26368;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#36752;&#23556;&#21453;&#39304;&#24433;&#21709;&#20102;&#26089;&#26399;&#28909;&#24102;&#27668;&#26059;&#30340;&#24378;&#21270;&#65292;&#20294;&#29616;&#26377;&#35786;&#26029;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#20351;&#20854;&#26080;&#27861;&#29992;&#26469;&#30740;&#31350;&#19981;&#23545;&#31216;&#25110;&#30636;&#24577;&#30340;&#36752;&#23556;&#21152;&#28909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#21464;&#20998;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;VED&#65289;&#26469;&#23398;&#20064;&#36752;&#23556;&#19982;&#23454;&#38469;&#27169;&#25311;&#30340;&#27668;&#26059;&#34920;&#38754;&#24378;&#21270;&#20043;&#38388;&#30340;&#38544;&#34255;&#20851;&#31995;&#12290;&#38480;&#21046;VED&#27169;&#22411;&#30340;&#36755;&#20837;&#21487;&#20197;&#21033;&#29992;&#20854;&#19981;&#30830;&#23450;&#24615;&#26469;&#35782;&#21035;&#36752;&#23556;&#23545;&#24378;&#21270;&#26356;&#37325;&#35201;&#30340;&#26102;&#26399;&#12290;&#23545;&#25552;&#21462;&#30340;&#19977;&#32500;&#36752;&#23556;&#32467;&#26500;&#30340;&#32454;&#33268;&#26816;&#26597;&#34920;&#26126;&#65292;&#20869;&#26680;&#28145;&#23545;&#27969;&#21644;&#27973;&#20113;&#30340;&#38271;&#27874;&#36752;&#23556;&#24378;&#36843;&#37117;&#23545;&#24378;&#21270;&#36215;&#21040;&#36129;&#29486;&#65292;&#20854;&#20013;&#28145;&#23545;&#27969;&#22312;&#25972;&#20307;&#19978;&#20855;&#26377;&#26368;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27973;&#20113;&#30340;&#19979;&#39118;&#22788;&#30340;&#28145;&#23545;&#27969;&#23545;&#28023;&#29141;&#30340;&#24378;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21457;&#29616;&#28909;&#21147;-&#21160;&#21147;&#23398;&#20851;&#31995;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#36724;&#23545;&#31216;&#25110;&#30830;&#23450;&#24615;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud radiative feedback impacts early tropical cyclone (TC) intensification, but limitations in existing diagnostic frameworks make them unsuitable for studying asymmetric or transient radiative heating. We propose a linear Variational Encoder-Decoder (VED) to learn the hidden relationship between radiation and the surface intensification of realistic simulated TCs. Limiting VED model inputs enables using its uncertainty to identify periods when radiation has more importance for intensification. A close examination of the extracted 3D radiative structures suggests that longwave radiative forcing from inner core deep convection and shallow clouds both contribute to intensification, with the deep convection having the most impact overall. We find that deep convection downwind of the shallow clouds is critical to the intensification of Haiyan. Our work demonstrates that machine learning can discover thermodynamic-kinematic relationships without relying on axisymmetric or deterministic as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.08898</link><description>&lt;p&gt;
&#26725;&#25509;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#65306;&#29702;&#35299;&#33258;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging State and History Representations: Understanding Self-Predictive RL. (arXiv:2401.08898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#26159;&#25152;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#36866;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#35768;&#22810;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#29702;&#35770;&#26694;&#26550;&#34987;&#24320;&#21457;&#29992;&#20110;&#29702;&#35299;&#20160;&#20040;&#26500;&#25104;&#20102;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20849;&#21516;&#23646;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#30475;&#20284;&#19981;&#21516;&#30340;&#29366;&#24577;&#21644;&#21382;&#21490;&#25277;&#35937;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;&#30446;&#26631;&#21644;&#20248;&#21270;&#65288;&#22914;&#20572;&#26799;&#24230;&#25216;&#26415;&#65289;&#22312;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#20013;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#20849;&#21516;&#20135;&#29983;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29366;&#24577;&#21644;&#21382;&#21490;&#30340;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#26631;&#20934;MDP&#12289;&#24102;&#26377;dist&#30340;MDP&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with dist
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#36890;&#36807;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;800&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#19977;&#31181;&#27969;&#34892;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.05453</link><description>&lt;p&gt;
&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dimensionality-Aware Outlier Detection: Theoretical and Experimental Analysis. (arXiv:2401.05453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#36890;&#36807;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;800&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#19977;&#31181;&#27969;&#34892;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#20869;&#22312;&#32500;&#24230;&#30340;&#23616;&#37096;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#29702;&#35770;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#23427;&#34987;&#25512;&#23548;&#20026;&#19968;&#20010;&#21253;&#21547;&#26597;&#35810;&#28857;&#21644;&#38543;&#26426;&#36873;&#25321;&#30340;&#36817;&#37051;&#30340;&#28176;&#36817;&#23616;&#37096;&#26399;&#26395;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#12290;DAO&#30340;&#32500;&#24230;&#24863;&#30693;&#34892;&#20026;&#26159;&#30001;&#20110;&#23427;&#20197;&#29702;&#35770;&#19978;&#35777;&#26126;&#30340;&#26041;&#24335;&#20351;&#29992;&#23616;&#37096;LID&#20540;&#30340;&#23616;&#37096;&#20272;&#35745;&#12290;&#36890;&#36807;&#23545;800&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;DAO&#26126;&#26174;&#20248;&#20110;&#19977;&#31181;&#27969;&#34892;&#19988;&#37325;&#35201;&#30340;&#22522;&#20934;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65306;&#23616;&#37096;&#31163;&#32676;&#22240;&#23376;&#65288;LOF&#65289;&#65292;&#31616;&#21270;&#29256;LOF&#21644;kNN&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a nonparametric method for outlier detection that takes full account of local variations in intrinsic dimensionality within the dataset. Using the theory of Local Intrinsic Dimensionality (LID), our 'dimensionality-aware' outlier detection method, DAO, is derived as an estimator of an asymptotic local expected density ratio involving the query point and a close neighbor drawn at random. The dimensionality-aware behavior of DAO is due to its use of local estimation of LID values in a theoretically-justified way. Through comprehensive experimentation on more than 800 synthetic and real datasets, we show that DAO significantly outperforms three popular and important benchmark outlier detection methods: Local Outlier Factor (LOF), Simplified LOF, and kNN.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20415;&#21033;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#22810;&#20307;&#21407;&#23376;&#20851;&#31995;&#24314;&#27169;&#21644;&#38190;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02683</link><description>&lt;p&gt;
&#29992;&#20110;3D&#20998;&#23376;&#29983;&#25104;&#30340;&#20960;&#20309;&#20415;&#21033;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Geometric-Facilitated Denoising Diffusion Model for 3D Molecule Generation. (arXiv:2401.02683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02683
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#20415;&#21033;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;3D&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#22810;&#20307;&#21407;&#23376;&#20851;&#31995;&#24314;&#27169;&#21644;&#38190;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#38754;&#21521;&#20840;&#26032;3D&#20998;&#23376;&#29983;&#25104;&#30340;&#25193;&#25955;&#22522;&#20110;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#20998;&#23376;&#20013;&#30340;&#22823;&#22810;&#25968;&#37325;&#21407;&#23376;&#36890;&#36807;&#21333;&#38190;&#19982;&#22810;&#20010;&#21407;&#23376;&#30456;&#36830;&#65292;&#20165;&#20351;&#29992;&#25104;&#23545;&#36317;&#31163;&#26469;&#27169;&#25311;&#20998;&#23376;&#20960;&#20309;&#26159;&#19981;&#36275;&#30340;&#12290;&#22240;&#27492;&#65292;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#25552;&#20986;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#22810;&#20307;&#21407;&#23376;&#38388;&#20851;&#31995;&#21644;&#23398;&#20064;&#39640;&#36136;&#37327;&#29305;&#24449;&#30340;&#26377;&#25928;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#21435;&#22122;&#20869;&#26680;&#12290;&#30001;&#20110;&#22270;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#38754;&#23545;&#20998;&#23376;&#30340;&#20027;&#27969;&#25193;&#25955;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#24182;&#20197;&#38388;&#25509;&#26041;&#24335;&#29983;&#25104;&#36793;&#32536;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#28041;&#21450;&#23558;&#20998;&#23376;&#30340;&#29983;&#25104;&#19982;&#25193;&#25955;&#30456;&#32467;&#21512;&#65292;&#24182;&#20934;&#30830;&#39044;&#27979;&#38190;&#30340;&#23384;&#22312;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#26356;&#26032;&#20998;&#23376;&#26500;&#22411;&#30340;&#36845;&#20195;&#26041;&#24335;&#19982;&#20998;&#23376;&#21160;&#21147;&#23398;&#19968;&#33268;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have shown great potential in multiple research areas. Existing diffusion-based generative methods on de novo 3D molecule generation face two major challenges. Since majority heavy atoms in molecules allow connections to multiple atoms through single bonds, solely using pair-wise distance to model molecule geometries is insufficient. Therefore, the first one involves proposing an effective neural network as the denoising kernel that is capable to capture complex multi-body interatomic relationships and learn high-quality features. Due to the discrete nature of graphs, mainstream diffusion-based methods for molecules heavily rely on predefined rules and generate edges in an indirect manner. The second challenge involves accommodating molecule generation to diffusion and accurately predicting the existence of bonds. In our research, we view the iterative way of updating molecule conformations in diffusion process is consistent with molecular dynamics and introd
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;OOD&#24191;&#20041;&#21270;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#26679;&#21270;&#26041;&#27861;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#19988;&#20165;&#20165;&#36827;&#34892;&#22810;&#26679;&#21270;&#26159;&#19981;&#36275;&#20197;&#23454;&#29616;OOD&#24191;&#20041;&#21270;&#30340;&#65292;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#20063;&#24456;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2312.16313</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#21270;&#35299;&#26512;OOD&#24191;&#20041;&#21270;&#30340;&#20851;&#38190;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Key Components of OOD Generalization via Diversification. (arXiv:2312.16313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22810;&#26679;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;OOD&#24191;&#20041;&#21270;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#26679;&#21270;&#26041;&#27861;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#19988;&#20165;&#20165;&#36827;&#34892;&#22810;&#26679;&#21270;&#26159;&#19981;&#36275;&#20197;&#23454;&#29616;OOD&#24191;&#20041;&#21270;&#30340;&#65292;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#20063;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#25968;&#25454;&#38598;&#21487;&#33021;&#21253;&#21547;&#22810;&#20010;&#35299;&#37322;&#35757;&#32451;&#38598;&#21516;&#26679;&#33391;&#22909;&#30340;&#32447;&#32034;&#65292;&#21363;&#23398;&#20064;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#37117;&#20250;&#23548;&#33268;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#27491;&#30830;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#35768;&#22810;&#32447;&#32034;&#21487;&#33021;&#26159;&#34394;&#20551;&#30340;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#22833;&#21435;&#20102;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#22240;&#27492;&#26080;&#27861;&#25512;&#24191;&#21040;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;&#8220;&#22810;&#26679;&#21270;&#8221;&#26041;&#27861;&#36890;&#36807;&#25214;&#21040;&#20381;&#36182;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#20010;&#19981;&#21516;&#30340;&#20551;&#35774;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#36825;&#31867;&#26041;&#27861;&#24182;&#30830;&#23450;&#23545;&#20854;OOD&#24191;&#20041;&#21270;&#33021;&#21147;&#30340;&#36129;&#29486;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#25105;&#20204;&#21457;&#29616;(1) &#22810;&#26679;&#21270;&#26041;&#27861;&#23545;&#20110;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#24403;&#36828;&#31163;&#26041;&#27861;&#29305;&#23450;&#30340;&#26368;&#20339;&#28857;&#26102;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;(2) &#20165;&#20165;&#36827;&#34892;&#22810;&#26679;&#21270;&#26159;&#19981;&#36275;&#20197;&#23454;&#29616;OOD&#24191;&#20041;&#21270;&#30340;&#12290;&#25152;&#20351;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#65292;&#20363;&#22914;
&lt;/p&gt;
&lt;p&gt;
Supervised learning datasets may contain multiple cues that explain the training set equally well, i.e., learning any of them would lead to the correct predictions on the training data. However, many of them can be spurious, i.e., lose their predictive power under a distribution shift and consequently fail to generalize to out-of-distribution (OOD) data. Recently developed "diversification" methods (Lee et al., 2023; Pagliardini et al., 2023) approach this problem by finding multiple diverse hypotheses that rely on different features. This paper aims to study this class of methods and identify the key components contributing to their OOD generalization abilities.  We show that (1) diversification methods are highly sensitive to the distribution of the unlabeled data used for diversification and can underperform significantly when away from a method-specific sweet spot. (2) Diversification alone is insufficient for OOD generalization. The choice of the used learning algorithm, e.g., the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#27169;&#25311;&#25512;&#26029;&#26041;&#27861;&#32467;&#21512;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;&#65292;&#26469;&#38480;&#21046;&#23396;&#31435;&#38134;&#27827;&#23556;&#30005;&#33033;&#20914;&#26143;&#30340;&#30913;&#26059;&#36716;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.14848</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#30340;&#23396;&#31435;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Isolated pulsar population synthesis with simulation-based inference. (arXiv:2312.14848v1 [astro-ph.HE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#27169;&#25311;&#25512;&#26029;&#26041;&#27861;&#32467;&#21512;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;&#65292;&#26469;&#38480;&#21046;&#23396;&#31435;&#38134;&#27827;&#23556;&#30005;&#33033;&#20914;&#26143;&#30340;&#30913;&#26059;&#36716;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;&#19982;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#30456;&#32467;&#21512;&#65292;&#20197;&#38480;&#21046;&#23396;&#31435;&#38134;&#27827;&#23556;&#30005;&#33033;&#20914;&#26143;&#30340;&#30913;&#26059;&#36716;&#29305;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#20013;&#23376;&#26143;&#30340;&#35806;&#29983;&#29305;&#24615;&#21644;&#28436;&#21270;&#65292;&#37325;&#28857;&#26159;&#23427;&#20204;&#30340;&#21160;&#21147;&#23398;&#12289;&#26059;&#36716;&#21644;&#30913;&#24615;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20174;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#21021;&#22987;&#30913;&#22330;&#24378;&#24230;B&#21644;&#33258;&#36716;&#21608;&#26399;P&#65292;&#24182;&#29992;&#24130;&#24459;&#26469;&#25429;&#25417;&#21518;&#26399;&#30913;&#22330;&#30340;&#34928;&#20943;&#12290;&#27599;&#20010;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#30001;&#22343;&#20540;&#956;logB&#65292;&#956;logP&#21644;&#26631;&#20934;&#24046;&#963;logB&#65292;&#963;logP&#25551;&#36848;&#65292;&#32780;&#24130;&#24459;&#30001;&#25351;&#25968;a_late&#25551;&#36848;&#65292;&#20849;&#35745;&#20116;&#20010;&#33258;&#30001;&#21442;&#25968;&#12290;&#28982;&#21518;&#25105;&#20204;&#27169;&#25311;&#20102;&#26143;&#20307;&#30340;&#23556;&#30005;&#21457;&#23556;&#21644;&#35266;&#27979;&#20559;&#24046;&#65292;&#20197;&#27169;&#25311;&#19977;&#20010;&#23556;&#30005;&#35843;&#26597;&#20013;&#30340;&#25506;&#27979;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#21442;&#25968;&#20135;&#29983;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#21512;&#25104;P-&#7766;&#22270;&#25968;&#25454;&#24211;&#12290;&#25509;&#30528;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#30340;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
We combine pulsar population synthesis with simulation-based inference to constrain the magneto-rotational properties of isolated Galactic radio pulsars. We first develop a flexible framework to model neutron-star birth properties and evolution, focusing on their dynamical, rotational and magnetic characteristics. In particular, we sample initial magnetic-field strengths, $B$, and spin periods, $P$, from log-normal distributions and capture the late-time magnetic-field decay with a power law. Each log-normal is described by a mean, $\mu_{\log B}, \mu_{\log P}$, and standard deviation, $\sigma_{\log B}, \sigma_{\log P}$, while the power law is characterized by the index, $a_{\rm late}$, resulting in five free parameters. We subsequently model the stars' radio emission and observational biases to mimic detections with three radio surveys, and produce a large database of synthetic $P$-$\dot{P}$ diagrams by varying our input parameters. We then follow a simulation-based inference approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#21644;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#26469;&#25913;&#21892;MPC&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.07202</link><description>&lt;p&gt;
&#36755;&#20837;&#20984;LSTM&#65306;&#19968;&#31181;&#24555;&#36895;&#22522;&#20110;Lyapunov&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#20984;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model Predictive Control. (arXiv:2311.07202v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#21644;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#26469;&#25913;&#21892;MPC&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#65288;ICNN&#65289;&#65292;&#22522;&#20110;ICNN&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#36890;&#36807;&#22312;MPC&#26694;&#26550;&#20013;&#20445;&#25345;&#20984;&#24615;&#25104;&#21151;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;ICNN&#26550;&#26500;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#22797;&#26434;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;MPC&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;MPC&#21644;&#22522;&#20110;ICNN&#30340;MPC&#65292;&#19982;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#27169;&#22411;&#30340;MPC&#30456;&#27604;&#38754;&#20020;&#36739;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;ICNN&#30340;&#21407;&#29702;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;MPC&#65292;&#26088;&#22312;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#12289;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#24182;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23545;&#38750;&#32447;&#24615;&#21270;&#23398;&#21453;&#24212;&#22120;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#30340;&#32531;&#35299;&#21644;&#25910;&#25947;&#26102;&#38388;&#30340;&#20943;&#23569;&#65292;&#25910;&#25947;&#26102;&#38388;&#24179;&#22343;&#38477;&#20302;&#20102;&#19968;&#23450;&#30340;&#30334;&#20998;&#20043;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging Input Convex Neural Networks (ICNNs), ICNN-based Model Predictive Control (MPC) successfully attains globally optimal solutions by upholding convexity within the MPC framework. However, current ICNN architectures encounter the issue of vanishing/exploding gradients, which limits their ability to serve as deep neural networks for complex tasks. Additionally, the current neural network-based MPC, including conventional neural network-based MPC and ICNN-based MPC, faces slower convergence speed when compared to MPC based on first-principles models. In this study, we leverage the principles of ICNNs to propose a novel Input Convex LSTM for Lyapunov-based MPC, with the specific goal of reducing convergence time and mitigating the vanishing/exploding gradient problem while ensuring closed-loop stability. From a simulation study of a nonlinear chemical reactor, we observed a mitigation of vanishing/exploding gradient problem and a reduction in convergence time, with a percentage de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29289;&#29702;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064; (PEML) &#30340;&#20809;&#35889;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#22797;&#26434;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#28436;&#31034;&#20102;&#19981;&#21516;&#31867;&#22411;PEML&#26041;&#27861;&#30340;&#20010;&#20307;&#29305;&#24449;&#21644;&#21160;&#26426;&#12290;</title><link>http://arxiv.org/abs/2310.20425</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#32467;&#26500;&#21147;&#23398;&#24212;&#29992;&#30340;&#35843;&#26597;&#65292;&#35752;&#35770;&#29289;&#29702;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#35889;
&lt;/p&gt;
&lt;p&gt;
Discussing the Spectra of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications. (arXiv:2310.20425v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29289;&#29702;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064; (PEML) &#30340;&#20809;&#35889;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#22797;&#26434;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#28436;&#31034;&#20102;&#19981;&#21516;&#31867;&#22411;PEML&#26041;&#27861;&#30340;&#20010;&#20307;&#29305;&#24449;&#21644;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#24050;&#32463;&#20652;&#29983;&#20102;&#19968;&#31181;&#25105;&#20204;&#22312;&#36825;&#37324;&#31216;&#20043;&#20026;&#29289;&#29702;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064; (PEML) &#30340;&#33539;&#24335;&#65292;&#26088;&#22312;&#25552;&#39640;&#25968;&#25454;&#25110;&#29289;&#29702;&#26041;&#27861;&#30340;&#33021;&#21147;&#21644;&#20943;&#23569;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20854;&#29305;&#24449;&#12289;&#29992;&#27861;&#21644;&#21160;&#26426;&#30340;&#20840;&#38754;&#25506;&#32034;&#26469;&#35752;&#35770;&#29289;&#29702;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20809;&#35889;&#65292;&#28085;&#30422;&#20102;&#29289;&#29702;&#21644;&#25968;&#25454;&#36825;&#20004;&#20010;&#23450;&#20041;&#36724;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;PEML&#25216;&#26415;&#30340;&#26368;&#26032;&#24212;&#29992;&#21644;&#21457;&#23637;&#30340;&#35843;&#26597;&#65292;&#25581;&#31034;&#20102;PEML&#22312;&#24212;&#23545;&#22797;&#26434;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#28436;&#31034;&#20102;&#22312;&#21333;&#33258;&#30001;&#24230;Duffing&#25391;&#23376;&#30340;&#31616;&#21333;&#24037;&#20316;&#31034;&#20363;&#19978;&#36873;&#25321;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#20174;&#32780;&#31361;&#20986;&#20102;&#19981;&#21516;&#8220;&#31867;&#22411;&#8221;PEML&#26041;&#27861;&#30340;&#20010;&#20307;&#29305;&#24449;&#21644;&#21160;&#26426;&#12290;&#20026;&#20102;&#20419;&#36827;&#21512;&#20316;&#21644;&#36879;&#26126;&#24230;&#65292;&#24182;&#20026;&#35835;&#32773;&#25552;&#20379;&#23454;&#38469;&#30340;&#31034;&#20363;&#65292;&#26412;&#25991;&#23436;&#25972;&#22320;&#35760;&#24405;&#20102;&#25152;&#37319;&#21462;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of physics and machine learning has given rise to a paradigm that we refer to here as physics-enhanced machine learning (PEML), aiming to improve the capabilities and reduce the individual shortcomings of data- or physics-only methods. In this paper, the spectrum of physics-enhanced machine learning methods, expressed across the defining axes of physics and data, is discussed by engaging in a comprehensive exploration of its characteristics, usage, and motivations. In doing so, this paper offers a survey of recent applications and developments of PEML techniques, revealing the potency of PEML in addressing complex challenges. We further demonstrate application of select such schemes on the simple working example of a single-degree-of-freedom Duffing oscillator, which allows to highlight the individual characteristics and motivations of different `genres' of PEML approaches. To promote collaboration and transparency, and to provide practical examples for the reader, the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#30693;&#35782;&#22270;&#35889;&#32972;&#26223;&#19979;&#25506;&#32034;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#35780;&#20998;&#20989;&#25968;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13253</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#30340;&#22810;&#26679;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Context-Enhanced Diversified Recommendation. (arXiv:2310.13253v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13253
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#30693;&#35782;&#22270;&#35889;&#32972;&#26223;&#19979;&#25506;&#32034;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#35780;&#20998;&#20989;&#25968;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#19968;&#30452;&#33268;&#21147;&#20110;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36861;&#27714;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#24448;&#24448;&#23548;&#33268;&#20102;&#22810;&#26679;&#24615;&#30340;&#38477;&#20302;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#8220;&#22238;&#22768;&#23460;&#8221;&#29616;&#35937;&#12290;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#23545;&#31574;&#24212;&#36816;&#32780;&#29983;&#65292;&#23558;&#22810;&#26679;&#24615;&#19982;&#20934;&#30830;&#24615;&#21516;&#31561;&#30475;&#24453;&#65292;&#24182;&#22312;&#23398;&#26415;&#30028;&#21644;&#34892;&#19994;&#23454;&#36341;&#32773;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#26679;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#22797;&#26434;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#30693;&#35782;&#22270;&#35889;&#26159;&#36830;&#25509;&#23454;&#20307;&#21644;&#39033;&#30446;&#30340;&#20449;&#24687;&#24211;&#65292;&#36890;&#36807;&#21152;&#20837;&#28145;&#20837;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#22686;&#21152;&#25512;&#33616;&#22810;&#26679;&#24615;&#30340;&#26377;&#21033;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23454;&#20307;&#35206;&#30422;&#21644;&#20851;&#31995;&#35206;&#30422;&#65292;&#26377;&#25928;&#22320;&#37327;&#21270;&#20102;&#30693;&#35782;&#22270;&#35889;&#39046;&#22495;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22810;&#26679;&#21270;&#35780;&#20998;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#23454;&#20307;&#35206;&#30422;&#21644;&#20851;&#31995;&#35206;&#30422;&#26469;&#25552;&#39640;&#25512;&#33616;&#31639;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Recommender Systems (RecSys) has been extensively studied to enhance accuracy by leveraging users' historical interactions. Nonetheless, this persistent pursuit of accuracy frequently engenders diminished diversity, culminating in the well-recognized "echo chamber" phenomenon. Diversified RecSys has emerged as a countermeasure, placing diversity on par with accuracy and garnering noteworthy attention from academic circles and industry practitioners. This research explores the realm of diversified RecSys within the intricate context of knowledge graphs (KG). These KGs act as repositories of interconnected information concerning entities and items, offering a propitious avenue to amplify recommendation diversity through the incorporation of insightful contextual information. Our contributions include introducing an innovative metric, Entity Coverage, and Relation Coverage, which effectively quantifies diversity within the KG domain. Additionally, we introduce the Diversified
&lt;/p&gt;</description></item><item><title>HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11102</link><description>&lt;p&gt;
HGCVAE: &#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11102
&lt;/p&gt;
&lt;p&gt;
HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#29983;&#25104;&#24335;SSL&#22312;&#24322;&#26500;&#22270;&#23398;&#20064;&#65288;HGL&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#20851;&#20110;&#24322;&#26500;&#22270;&#30340;SSL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#65292;&#38656;&#35201;&#35774;&#35745;&#22797;&#26434;&#30340;&#35270;&#22270;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;SSL&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;HGL&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGCVAE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;HGL&#25670;&#33073;&#20102;&#22797;&#26434;&#24322;&#36136;&#24615;&#30340;&#36127;&#25285;&#12290;HGCVAE&#19981;&#20877;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#26159;&#20805;&#20998;&#21033;&#29992;&#20102;&#29983;&#25104;&#24335;SSL&#30340;&#28508;&#21147;&#12290;HGCVAE&#21019;&#26032;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29983;&#25104;&#24335;SSL&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#26426;&#21046;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;hard&#26679;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20998;&#26512;&#20102;&#20581;&#24247;&#20010;&#20307;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#65292;&#24182;&#35782;&#21035;&#20986;&#38543;&#24180;&#40836;&#22686;&#38271;&#21628;&#21560;&#29575;&#30340;&#19979;&#38477;&#21450;SDANN&#20540;&#24322;&#24120;&#39640;&#20316;&#20026;&#32769;&#24180;&#20154;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.07463</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25581;&#31034;&#20581;&#24247;&#34928;&#32769;&#36807;&#31243;&#20013;&#30340;&#24515;&#30005;&#22270;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncovering ECG Changes during Healthy Aging using Explainable AI. (arXiv:2310.07463v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20998;&#26512;&#20102;&#20581;&#24247;&#20010;&#20307;&#30340;&#24515;&#30005;&#22270;&#25968;&#25454;&#65292;&#24182;&#35782;&#21035;&#20986;&#38543;&#24180;&#40836;&#22686;&#38271;&#21628;&#21560;&#29575;&#30340;&#19979;&#38477;&#21450;SDANN&#20540;&#24322;&#24120;&#39640;&#20316;&#20026;&#32769;&#24180;&#20154;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#20173;&#28982;&#26159;&#20840;&#29699;&#39046;&#20808;&#30340;&#27515;&#22240;&#12290;&#36825;&#38656;&#35201;&#23545;&#24515;&#33039;&#34928;&#32769;&#36807;&#31243;&#26377;&#28145;&#20837;&#30340;&#20102;&#35299;&#65292;&#20197;&#35786;&#26029;&#24515;&#34880;&#31649;&#20581;&#24247;&#29366;&#20917;&#30340;&#38480;&#21046;&#12290;&#20256;&#32479;&#19978;&#65292;&#23545;&#20010;&#20307;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#29305;&#24449;&#38543;&#24180;&#40836;&#21464;&#21270;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#36825;&#20123;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29305;&#24449;&#34429;&#28982;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#21487;&#33021;&#25513;&#30422;&#20102;&#24213;&#23618;&#25968;&#25454;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#20998;&#26512;&#26469;&#33258;&#20581;&#24247;&#20010;&#20307;&#30340;ECG&#25968;&#25454;&#65292;&#21253;&#25324;&#21407;&#22987;&#20449;&#21495;&#21644;ECG&#29305;&#24449;&#26684;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#26469;&#35782;&#21035;&#23545;&#20110;&#21306;&#20998;&#24180;&#40836;&#32452;&#21035;&#26368;&#26377;&#36776;&#21035;&#21147;&#30340;ECG&#29305;&#24449;&#25110;&#21407;&#22987;&#20449;&#21495;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#19982;&#22522;&#20110;&#26641;&#30340;&#20998;&#31867;&#22120;&#25581;&#31034;&#20102;&#38543;&#24180;&#40836;&#22686;&#38271;&#21628;&#21560;&#29575;&#19979;&#38477;&#65292;&#24182;&#35782;&#21035;&#20986;SDANN&#20540;&#24322;&#24120;&#39640;&#20316;&#20026;&#32769;&#24180;&#20154;&#30340;&#25351;&#26631;&#65292;&#21487;&#23558;&#20854;&#19982;&#24180;&#36731;&#20154;&#21306;&#20998;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular diseases remain the leading global cause of mortality. This necessitates a profound understanding of heart aging processes to diagnose constraints in cardiovascular fitness. Traditionally, most of such insights have been drawn from the analysis of electrocardiogram (ECG) feature changes of individuals as they age. However, these features, while informative, may potentially obscure underlying data relationships. In this paper, we employ a deep-learning model and a tree-based model to analyze ECG data from a robust dataset of healthy individuals across varying ages in both raw signals and ECG feature format. Explainable AI techniques are then used to identify ECG features or raw signal characteristics are most discriminative for distinguishing between age groups. Our analysis with tree-based classifiers reveal age-related declines in inferred breathing rates and identifies notably high SDANN values as indicative of elderly individuals, distinguishing them from younger adul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#34701;&#21512;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#40784;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#24182;&#20801;&#35768;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#21387;&#32553;Transformer&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.05719</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#22120;&#21512;&#24182;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformer Fusion with Optimal Transport. (arXiv:2310.05719v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#34701;&#21512;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#40784;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#24182;&#20801;&#35768;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#30340;&#34701;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#21387;&#32553;Transformer&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#26159;&#19968;&#31181;&#23558;&#22810;&#20010;&#29420;&#31435;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21512;&#24182;&#20197;&#32467;&#21512;&#23427;&#20204;&#30340;&#33021;&#21147;&#30340;&#25216;&#26415;&#12290;&#36807;&#21435;&#30340;&#23581;&#35797;&#20165;&#38480;&#20110;&#20840;&#36830;&#25509;&#12289;&#21367;&#31215;&#21644;&#27531;&#24046;&#32593;&#32476;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#26469;&#34701;&#21512;&#20004;&#20010;&#25110;&#22810;&#20010;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#65292;&#20197;&#65288;&#36719;&#65289;&#23545;&#40784;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#19968;&#31181;&#23618;&#23545;&#40784;&#30340;&#25277;&#35937;&#26041;&#27861;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#24847;&#26550;&#26500;&#65292;&#20363;&#22914;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#23618;&#24402;&#19968;&#21270;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#28040;&#34701;&#30740;&#31350;&#35752;&#35770;&#20102;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#26550;&#26500;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65288;&#24322;&#26500;&#34701;&#21512;&#65289;&#65292;&#20026;Transformer&#30340;&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;Vision Transformer&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. In this paper, we present a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures -- in principle -and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way for compression of Transformers. The proposed approach is evaluated on both image classification tasks via Vision Transformer and natural language
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;Spike-and-Slab&#20808;&#39564;&#30340;&#26041;&#31243;&#24335;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#22238;&#24402;&#21644;&#36125;&#21494;&#26031;&#31232;&#30095;&#20998;&#24067;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#39640;&#25928;&#30340;&#21518;&#39564;&#25512;&#26029;&#21644;&#20989;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.05387</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;Spike-and-Slab&#20808;&#39564;&#21644;&#39640;&#25928;&#26680;&#20989;&#25968;&#30340;&#26041;&#31243;&#24335;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Equation Discovery with Bayesian Spike-and-Slab Priors and Efficient Kernels. (arXiv:2310.05387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;Spike-and-Slab&#20808;&#39564;&#30340;&#26041;&#31243;&#24335;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#22238;&#24402;&#21644;&#36125;&#21494;&#26031;&#31232;&#30095;&#20998;&#24067;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#39640;&#25928;&#30340;&#21518;&#39564;&#25512;&#26029;&#21644;&#20989;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#23545;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#25104;&#21151;&#26696;&#20363;&#65292;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#30528;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#38543;&#22788;&#21487;&#35265;&#12290;&#27492;&#22806;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;/&#25110;&#35757;&#32451;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;Spike-and-Slab&#20808;&#39564;&#65288;KBASS&#65289;&#30340;&#26032;&#22411;&#26041;&#31243;&#24335;&#21457;&#29616;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26680;&#22238;&#24402;&#26469;&#20272;&#35745;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#34920;&#36798;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#36125;&#21494;&#26031;Spike-and-Slab&#20808;&#39564;&#32467;&#21512;&#20351;&#29992;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#29702;&#24819;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#20998;&#24067;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#31639;&#23376;&#36873;&#25321;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#26399;&#26395;&#20256;&#25773;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EP-EM&#65289;&#31639;&#27861;&#30340;&#26377;&#25928;&#21518;&#39564;&#25512;&#26029;&#21644;&#20989;&#25968;&#20272;&#35745;&#26041;&#27861;&#12290;&#20026;&#20102;&#20811;&#26381;&#26680;&#22238;&#24402;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering governing equations from data is important to many scientific and engineering applications. Despite promising successes, existing methods are still challenged by data sparsity as well as noise issues, both of which are ubiquitous in practice. Moreover, state-of-the-art methods lack uncertainty quantification and/or are costly in training. To overcome these limitations, we propose a novel equation discovery method based on Kernel learning and BAyesian Spike-and-Slab priors (KBASS). We use kernel regression to estimate the target function, which is flexible, expressive, and more robust to data sparsity and noises. We combine it with a Bayesian spike-and-slab prior -- an ideal Bayesian sparse distribution -- for effective operator selection and uncertainty quantification. We develop an expectation propagation expectation-maximization (EP-EM) algorithm for efficient posterior inference and function estimation. To overcome the computational challenge of kernel regression, we pla
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04870</link><description>&lt;p&gt;
Lemur&#65306;&#22312;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#23637;&#31034;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#25277;&#35937;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#39564;&#35777;&#24037;&#20855;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#30340;&#33021;&#21147;&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#27491;&#24335;&#25551;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#35770;&#65292;&#23558;&#20854;&#20316;&#20026;&#25512;&#23548;&#35268;&#21017;&#30340;&#38598;&#21512;&#36827;&#34892;&#35770;&#35777;&#20854;&#23436;&#22791;&#24615;&#12290;&#25105;&#20204;&#23558;&#35745;&#31639;&#26426;&#25512;&#29702;&#24418;&#25104;&#20026;&#19968;&#20010;&#23436;&#22791;&#30340;&#33258;&#21160;&#39564;&#35777;&#36807;&#31243;&#65292;&#36825;&#22312;&#19968;&#32452;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#24102;&#26469;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#38750;&#38543;&#26426;&#32570;&#22833;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#19979;&#30340;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#38477;&#20302;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Heckman-FA&#26694;&#26550;&#26469;&#33719;&#21462;&#24688;&#24403;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.08043</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;Heckman&#36873;&#25321;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#29305;&#24449;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Prediction Feature Assignment in the Heckman Selection Model. (arXiv:2309.08043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#38750;&#38543;&#26426;&#32570;&#22833;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#19979;&#30340;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#38477;&#20302;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Heckman-FA&#26694;&#26550;&#26469;&#33719;&#21462;&#24688;&#24403;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#38543;&#26426;&#32570;&#22833;&#65288;MNAR&#65289;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#24448;&#24448;&#19979;&#38477;&#12290;&#26412;&#25991;&#20851;&#27880;MNAR&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#30340;&#19968;&#20010;&#32463;&#20856;&#20363;&#23376;&#65292;&#21363;&#19968;&#37096;&#20998;&#26679;&#26412;&#20855;&#26377;&#38750;&#38543;&#26426;&#32570;&#22833;&#32467;&#26524;&#12290;Heckman&#36873;&#25321;&#27169;&#22411;&#21450;&#20854;&#21464;&#31181;&#36890;&#24120;&#29992;&#20110;&#22788;&#29702;&#36825;&#31181;&#31867;&#22411;&#30340;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#12290;Heckman&#27169;&#22411;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#26041;&#31243;&#26469;&#27169;&#25311;&#26679;&#26412;&#30340;&#39044;&#27979;&#21644;&#36873;&#25321;&#65292;&#20854;&#20013;&#36873;&#25321;&#29305;&#24449;&#21253;&#25324;&#25152;&#26377;&#39044;&#27979;&#29305;&#24449;&#12290;&#22312;&#20351;&#29992;Heckman&#27169;&#22411;&#26102;&#65292;&#24517;&#39035;&#20174;&#36873;&#25321;&#29305;&#24449;&#38598;&#20013;&#27491;&#30830;&#36873;&#25321;&#39044;&#27979;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;Heckman&#27169;&#22411;&#26469;&#35828;&#65292;&#36873;&#25321;&#27491;&#30830;&#30340;&#39044;&#27979;&#29305;&#24449;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#24403;&#36873;&#25321;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#22810;&#26102;&#12290;&#29616;&#26377;&#30340;&#20351;&#29992;Heckman&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#19968;&#20010;&#25163;&#21160;&#36873;&#25321;&#30340;&#39044;&#27979;&#29305;&#24449;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Heckman-FA&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#26469;&#33719;&#24471;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under missing-not-at-random (MNAR) sample selection bias, the performance of a prediction model is often degraded. This paper focuses on one classic instance of MNAR sample selection bias where a subset of samples have non-randomly missing outcomes. The Heckman selection model and its variants have commonly been used to handle this type of sample selection bias. The Heckman model uses two separate equations to model the prediction and selection of samples, where the selection features include all prediction features. When using the Heckman model, the prediction features must be properly chosen from the set of selection features. However, choosing the proper prediction features is a challenging task for the Heckman model. This is especially the case when the number of selection features is large. Existing approaches that use the Heckman model often provide a manually chosen set of prediction features. In this paper, we propose Heckman-FA as a novel data-driven framework for obtaining pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#21644;&#32447;&#24615;&#25237;&#24433;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07261</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21516;&#26102;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Simultaneous inference for generalized linear models with unmeasured confounders. (arXiv:2309.07261v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#21644;&#32447;&#24615;&#25237;&#24433;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#20013;&#65292;&#24120;&#24120;&#36827;&#34892;&#25104;&#21315;&#19978;&#19975;&#20010;&#21516;&#26102;&#20551;&#35774;&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#24046;&#24322;&#34920;&#36798;&#30340;&#22522;&#22240;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#65292;&#35768;&#22810;&#26631;&#20934;&#32479;&#35745;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#22810;&#20803;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#12290;&#22312;&#20219;&#24847;&#28151;&#28102;&#26426;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#24182;&#23558;&#32447;&#24615;&#25237;&#24433;&#25972;&#21512;&#21040;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#20013;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#22810;&#20803;&#21709;&#24212;&#21464;&#37327;&#20998;&#31163;&#36793;&#38469;&#21644;&#19981;&#30456;&#20851;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#24674;&#22797;&#28151;&#28102;&#31995;&#25968;&#30340;&#21015;&#31354;&#38388;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;$\ell_1$&#27491;&#21017;&#21270;&#36827;&#34892;&#31232;&#30095;&#24615;&#20272;&#35745;&#65292;&#24182;&#24378;&#21152;&#27491;&#20132;&#24615;&#38480;&#21046;&#20110;&#28151;&#28102;&#31995;&#25968;&#65292;&#32852;&#21512;&#20272;&#35745;&#28508;&#22312;&#22240;&#23376;&#21644;&#20027;&#35201;&#25928;&#24212;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#25237;&#24433;&#21644;&#21152;&#26435;&#20559;&#24046;&#26657;&#27491;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tens of thousands of simultaneous hypothesis tests are routinely performed in genomic studies to identify differentially expressed genes. However, due to unmeasured confounders, many standard statistical approaches may be substantially biased. This paper investigates the large-scale hypothesis testing problem for multivariate generalized linear models in the presence of confounding effects. Under arbitrary confounding mechanisms, we propose a unified statistical estimation and inference framework that harnesses orthogonal structures and integrates linear projections into three key stages. It first leverages multivariate responses to separate marginal and uncorrelated confounding effects, recovering the confounding coefficients' column space. Subsequently, latent factors and primary effects are jointly estimated, utilizing $\ell_1$-regularization for sparsity while imposing orthogonality onto confounding coefficients. Finally, we incorporate projected and weighted bias-correction steps 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07136</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;Transformer&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#24212;&#29992;&#20013;&#26368;&#37325;&#35201;&#30340;&#35786;&#26029;&#24037;&#20855;&#20043;&#19968;&#12290;&#38543;&#30528;&#20808;&#36827;&#31639;&#27861;&#30340;&#20986;&#29616;&#65292;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;ECG&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#20854;&#22312;ECG&#25968;&#25454;&#19978;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#23454;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#29992;&#30340;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;MTECG&#65292;&#23427;&#23558;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;220,251&#20010;ECG&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35786;&#26029;&#27880;&#37322;&#65292;&#20197;&#25506;&#32034;MTECG&#30340;&#29305;&#24615;&#12290;&#22312;&#25552;&#20986;&#30340;&#35757;&#32451;&#31574;&#30053;&#19979;&#65292;&#19968;&#20010;&#21482;&#26377;5.7M&#21442;&#25968;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#65288;5%-75%&#65289;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#12290;&#28040;&#34701;&#30740;&#31350;&#31361;&#20986;&#20102;&#27874;&#21160;&#37325;&#26500;&#30446;&#26631;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;MTECG&#32791;&#26102;&#36739;&#23569;&#19988;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;&#21508;&#31181;&#24515;&#30005;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) is one of the most important diagnostic tools in clinical applications. With the advent of advanced algorithms, various deep learning models have been adopted for ECG tasks. However, the potential of Transformers for ECG data is not yet realized, despite their widespread success in computer vision and natural language processing. In this work, we present a useful masked Transformer method for ECG classification referred to as MTECG, which expands the application of masked autoencoders to ECG time series. We construct a dataset comprising 220,251 ECG recordings with a broad range of diagnoses annoated by medical experts to explore the properties of MTECG. Under the proposed training strategies, a lightweight model with 5.7M parameters performs stably well on a broad range of masking ratios (5%-75%). The ablation studies highlight the importance of fluctuated reconstruction targets, training schedule length, layer-wise LR decay and DropPath rate. The experiments o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#39057;&#29575;&#30340;&#37327;&#23376;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#22120;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#29983;&#25104;&#22120;&#65292;&#21253;&#25324;&#38750;&#24120;&#35268;&#38388;&#38548;&#39057;&#29575;&#21644;&#28789;&#27963;&#30340;&#39057;&#35889;&#20016;&#23500;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03279</link><description>&lt;p&gt;
&#35753;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36873;&#25321;&#33258;&#24049;&#30340;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
Let Quantum Neural Networks Choose Their Own Frequencies. (arXiv:2309.03279v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03279
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#39057;&#29575;&#30340;&#37327;&#23376;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#22120;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#29983;&#25104;&#22120;&#65292;&#21253;&#25324;&#38750;&#24120;&#35268;&#38388;&#38548;&#39057;&#29575;&#21644;&#28789;&#27963;&#30340;&#39057;&#35889;&#20016;&#23500;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#23558;&#36755;&#20837;&#29305;&#24449;&#30340;&#37096;&#20998;&#20613;&#31435;&#21494;&#32423;&#25968;&#34920;&#31034;&#26469;&#25551;&#36848;&#65292;&#20854;&#20013;&#39057;&#29575;&#30001;&#29305;&#24449;&#26144;&#23556;&#30340;&#29983;&#25104;&#21704;&#23494;&#39039;&#37327;&#21807;&#19968;&#30830;&#23450;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#25968;&#25454;&#32534;&#30721;&#29983;&#25104;&#22120;&#26159;&#25552;&#21069;&#36873;&#25321;&#30340;&#65292;&#22266;&#23450;&#20102;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#37327;&#23376;&#27169;&#22411;&#25512;&#24191;&#21040;&#29983;&#25104;&#22120;&#20013;&#21253;&#25324;&#19968;&#32452;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#21487;&#35757;&#32451;&#39057;&#29575;&#30340;&#37327;&#23376;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#21487;&#35757;&#32451;&#39057;&#29575;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#29983;&#25104;&#22120;&#65292;&#21253;&#25324;&#20854;&#39057;&#35889;&#20013;&#30340;&#38750;&#24120;&#35268;&#38388;&#38548;&#39057;&#29575;&#21644;&#28789;&#27963;&#30340;&#39057;&#35889;&#20016;&#23500;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#20165;&#23545;&#27599;&#20010;&#32534;&#30721;&#25805;&#20316;&#28155;&#21152;&#19968;&#20010;&#21442;&#25968;&#30340;&#21487;&#35757;&#32451;&#39057;&#29575;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#27714;&#35299;Navier-Stokes&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized quantum circuits as machine learning models are typically well described by their representation as a partial Fourier series of the input features, with frequencies uniquely determined by the feature map's generator Hamiltonians. Ordinarily, these data-encoding generators are chosen in advance, fixing the space of functions that can be represented. In this work we consider a generalization of quantum models to include a set of trainable parameters in the generator, leading to a trainable frequency (TF) quantum model. We numerically demonstrate how TF models can learn generators with desirable properties for solving the task at hand, including non-regularly spaced frequencies in their spectra and flexible spectral richness. Finally, we showcase the real-world effectiveness of our approach, demonstrating an improved accuracy in solving the Navier-Stokes equations using a TF model with only a single parameter added to each encoding operation. Since TF models encompass conven
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36866;&#24212;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#24182;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16471</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#38544;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#31574;&#30053;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy Adaptation Method for Implicit Multitask Reinforcement Learning Problems. (arXiv:2308.16471v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36866;&#24212;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#24182;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#25509;&#35302;&#21644;&#30896;&#25758;&#65292;&#31574;&#30053;&#21442;&#25968;&#30340;&#23567;&#25913;&#21464;&#21487;&#33021;&#23548;&#33268;&#26497;&#20854;&#19981;&#21516;&#30340;&#22238;&#25253;&#12290;&#20363;&#22914;&#65292;&#22312;&#36275;&#29699;&#20013;&#65292;&#36890;&#36807;&#31245;&#24494;&#25913;&#21464;&#36386;&#29699;&#20301;&#32622;&#25110;&#26045;&#21152;&#29699;&#30340;&#21147;&#25110;&#32773;&#29699;&#30340;&#25705;&#25830;&#21147;&#21457;&#29983;&#21464;&#21270;&#65292;&#29699;&#21487;&#20197;&#20197;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#21521;&#39134;&#34892;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#24819;&#35937;&#22312;&#19981;&#21516;&#30340;&#26041;&#21521;&#19978;&#22836;&#29699;&#38656;&#35201;&#23436;&#20840;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#36866;&#24212;&#30446;&#26631;&#25110;&#29615;&#22659;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#25110;&#29615;&#22659;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#21333;&#33050;&#26426;&#22120;&#20154;&#27169;&#22411;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#30446;&#26631;&#20301;&#32622;&#30340;&#38544;&#24335;&#21464;&#21270;&#25110;&#29699;&#30340;&#24674;&#22797;&#31995;&#25968;&#30340;&#21464;&#21270;&#65292;&#32780;&#26631;&#20934;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#21017;&#19981;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In dynamic motion generation tasks, including contact and collisions, small changes in policy parameters can lead to extremely different returns. For example, in soccer, the ball can fly in completely different directions with a similar heading motion by slightly changing the hitting position or the force applied to the ball or when the friction of the ball varies. However, it is difficult to imagine that completely different skills are needed for heading a ball in different directions. In this study, we proposed a multitask reinforcement learning algorithm for adapting a policy to implicit changes in goals or environments in a single motion category with different reward functions or physical parameters of the environment. We evaluated the proposed method on the ball heading task using a monopod robot model. The results showed that the proposed method can adapt to implicit changes in the goal positions or the coefficients of restitution of the ball, whereas the standard domain randomi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;oracle&#22411;&#19981;&#31561;&#24335;&#65292;&#36890;&#36807;&#35299;&#20915;&#36923;&#36753;&#25439;&#22833;&#30340;&#30446;&#26631;&#20989;&#25968;&#26080;&#30028;&#24615;&#38480;&#21046;&#65292;&#25512;&#23548;&#20986;&#20351;&#29992;&#36923;&#36753;&#25439;&#22833;&#35757;&#32451;&#30340;&#20840;&#36830;&#25509;ReLU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#20165;&#35201;&#27714;&#25968;&#25454;&#30340;&#26465;&#20214;&#31867;&#27010;&#29575;&#20855;&#26377;H\"older&#24179;&#28369;&#24615;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#32452;&#21512;&#20551;&#35774;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16792</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#25439;&#22833;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification with Deep Neural Networks and Logistic Loss. (arXiv:2307.16792v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;oracle&#22411;&#19981;&#31561;&#24335;&#65292;&#36890;&#36807;&#35299;&#20915;&#36923;&#36753;&#25439;&#22833;&#30340;&#30446;&#26631;&#20989;&#25968;&#26080;&#30028;&#24615;&#38480;&#21046;&#65292;&#25512;&#23548;&#20986;&#20351;&#29992;&#36923;&#36753;&#25439;&#22833;&#35757;&#32451;&#30340;&#20840;&#36830;&#25509;ReLU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#20165;&#35201;&#27714;&#25968;&#25454;&#30340;&#26465;&#20214;&#31867;&#27010;&#29575;&#20855;&#26377;H\"older&#24179;&#28369;&#24615;&#65292;&#24182;&#19988;&#32771;&#34385;&#20102;&#32452;&#21512;&#20551;&#35774;&#65292;&#20351;&#24471;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36923;&#36753;&#25439;&#22833;&#65288;&#21363;&#20132;&#21449;&#29109;&#25439;&#22833;&#65289;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#25439;&#22833;&#36827;&#34892;&#20108;&#20998;&#31867;&#30340;&#27867;&#21270;&#20998;&#26512;&#20173;&#28982;&#24456;&#23569;&#12290;&#36923;&#36753;&#25439;&#22833;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26080;&#30028;&#24615;&#26159;&#23548;&#33268;&#25512;&#23548;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#27867;&#21270;&#30028;&#38480;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24314;&#31435;&#19968;&#31181;&#26032;&#39062;&#32780;&#20248;&#38597;&#30340;oracle&#22411;&#19981;&#31561;&#24335;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35813;&#19981;&#31561;&#24335;&#20351;&#25105;&#20204;&#33021;&#22815;&#22788;&#29702;&#30446;&#26631;&#20989;&#25968;&#30340;&#26377;&#30028;&#24615;&#38480;&#21046;&#65292;&#24182;&#21033;&#29992;&#23427;&#25512;&#23548;&#20986;&#20351;&#29992;&#36923;&#36753;&#25439;&#22833;&#35757;&#32451;&#30340;&#20840;&#36830;&#25509;ReLU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20165;&#38656;&#35201;&#25968;&#25454;&#30340;&#26465;&#20214;&#31867;&#27010;&#29575;$\eta$&#30340;H\"older&#24179;&#28369;&#24615;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65288;&#20165;&#38480;&#20110;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32452;&#21512;&#20551;&#35774;&#65292;&#35201;&#27714;$\eta$&#26159;&#33509;&#24178;&#21521;&#37327;&#20540;&#20989;&#25968;&#30340;&#22797;&#21512;&#20989;&#25968;&#65292;&#20854;&#20013;&#27599;&#20010;&#21521;&#37327;&#20540;&#20989;&#25968;&#37117;&#26159;&#29420;&#31435;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) trained with the logistic loss (i.e., the cross entropy loss) have made impressive advancements in various binary classification tasks. However, generalization analysis for binary classification with DNNs and logistic loss remains scarce. The unboundedness of the target function for the logistic loss is the main obstacle to deriving satisfying generalization bounds. In this paper, we aim to fill this gap by establishing a novel and elegant oracle-type inequality, which enables us to deal with the boundedness restriction of the target function, and using it to derive sharp convergence rates for fully connected ReLU DNN classifiers trained with logistic loss. In particular, we obtain optimal convergence rates (up to log factors) only requiring the H\"older smoothness of the conditional class probability $\eta$ of data. Moreover, we consider a compositional assumption that requires $\eta$ to be the composition of several vector-valued functions of which each co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.14367</link><description>&lt;p&gt;
Prot2Text: &#22522;&#20110;GNNs&#21644;Transformers&#30340;&#22810;&#27169;&#24577;&#34507;&#30333;&#36136;&#21151;&#33021;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers. (arXiv:2307.14367v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14367
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prot2Text&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;GNNs&#21644;Transformers&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#32508;&#21512;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340;&#20840;&#38754;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#20351;&#26576;&#20123;&#31185;&#23398;&#23478;&#23558;&#20854;&#29702;&#35299;&#24402;&#31867;&#20026;&#38590;&#20197;&#24819;&#35937;&#30340;&#20219;&#21153;&#12290;&#19981;&#21516;&#32423;&#21035;&#30340;&#25361;&#25112;&#20351;&#36825;&#39033;&#20219;&#21153;&#22797;&#26434;&#21270;&#65292;&#20854;&#20013;&#20043;&#19968;&#26159;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#24320;&#21457;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#23558;&#20219;&#21153;&#34920;&#36848;&#20026;&#22810;&#20998;&#31867;&#38382;&#39064;&#65292;&#21363;&#23558;&#39044;&#23450;&#20041;&#26631;&#31614;&#20998;&#37197;&#32473;&#34507;&#30333;&#36136;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;Prot2Text&#65292;&#20197;&#33258;&#30001;&#25991;&#26412;&#26679;&#24335;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#21151;&#33021;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#20108;&#36827;&#21046;&#25110;&#20998;&#31867;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#20013;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#25991;&#26412;&#27880;&#37322;&#31561;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#36825;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#20801;&#35768;&#23545;&#34507;&#30333;&#36136;&#21151;&#33021;&#36827;&#34892;&#25972;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex nature of big biological systems pushed some scientists to classify its understanding under the inconceivable missions. Different leveled challenges complicated this task, one of is the prediction of a protein's function. In recent years, significant progress has been made in this field through the development of various machine learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e assigning predefined labels to proteins. In this work, we propose a novel approach, \textbf{Prot2Text}, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. This multimodal approach allows for a holistic representation of proteins' functions, en
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25490;&#21517;&#20989;&#25968;&#34917;&#20607;&#25514;&#26045;&#65292;&#36890;&#36807;&#32473;&#20104;&#20302;&#35843;&#32676;&#20307;&#25104;&#21592;&#22870;&#21169;&#31215;&#20998;&#26469;&#35299;&#20915;&#20844;&#24179;&#25490;&#24207;&#20013;&#30340;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.14366</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#24046;&#24322;&#34917;&#20607;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20844;&#24179;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Explainable Disparity Compensation for Efficient Fair Ranking. (arXiv:2307.14366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#25490;&#21517;&#20989;&#25968;&#34917;&#20607;&#25514;&#26045;&#65292;&#36890;&#36807;&#32473;&#20104;&#20302;&#35843;&#32676;&#20307;&#25104;&#21592;&#22870;&#21169;&#31215;&#20998;&#26469;&#35299;&#20915;&#20844;&#24179;&#25490;&#24207;&#20013;&#30340;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#25490;&#21517;&#20989;&#25968;&#24448;&#24448;&#23545;&#19981;&#21516;&#20154;&#32676;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#22522;&#30784;&#25968;&#25454;&#23384;&#22312;&#20559;&#35265;&#12290;&#35299;&#20915;&#21644;&#34917;&#20607;&#36825;&#20123;&#19981;&#21516;&#30340;&#32467;&#26524;&#23545;&#20110;&#20844;&#24179;&#20915;&#31574;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#34917;&#20607;&#25514;&#26045;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#25490;&#21517;&#20989;&#25968;&#36827;&#34892;&#19981;&#36879;&#26126;&#30340;&#36716;&#25442;&#20197;&#28385;&#36275;&#20844;&#24179;&#20445;&#35777;&#65292;&#25110;&#32773;&#20351;&#29992;&#37197;&#39069;&#25110;&#30041;&#20301;&#26469;&#20445;&#35777;&#21521;&#20195;&#34920;&#20302;&#35843;&#32676;&#20307;&#30340;&#25104;&#21592;&#25552;&#20379;&#26368;&#23569;&#25968;&#37327;&#30340;&#31215;&#26497;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26131;&#35299;&#37322;&#30340;&#25490;&#21517;&#20989;&#25968;&#34917;&#20607;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#25514;&#26045;&#20381;&#36182;&#20110;&#32473;&#20104;&#20302;&#35843;&#32676;&#20307;&#25104;&#21592;&#30340;&#22870;&#21169;&#31215;&#20998;&#26469;&#35299;&#20915;&#25490;&#21517;&#20989;&#25968;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#12290;&#22870;&#21169;&#31215;&#20998;&#21487;&#20197;&#20107;&#20808;&#35774;&#32622;&#65292;&#24182;&#19988;&#21487;&#20197;&#36827;&#34892;&#32452;&#21512;&#65292;&#20174;&#32780;&#32771;&#34385;&#21040;&#20195;&#34920;&#20132;&#21449;&#65292;&#24182;&#20026;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#26356;&#22909;&#30340;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#22522;&#20110;&#25277;&#26679;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Ranking functions that are used in decision systems often produce disparate results for different populations because of bias in the underlying data. Addressing, and compensating for, these disparate outcomes is a critical problem for fair decision-making. Recent compensatory measures have mostly focused on opaque transformations of the ranking functions to satisfy fairness guarantees or on the use of quotas or set-asides to guarantee a minimum number of positive outcomes to members of underrepresented groups. In this paper we propose easily explainable data-driven compensatory measures for ranking functions. Our measures rely on the generation of bonus points given to members of underrepresented groups to address disparity in the ranking function. The bonus points can be set in advance, and can be combined, allowing for considering the intersections of representations and giving better transparency to stakeholders. We propose efficient sampling-based algorithms to calculate the number
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02932</link><description>&lt;p&gt;
&#24403;&#25298;&#32477;&#23398;&#20064;&#23545;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#26368;&#20248;&#26102;
&lt;/p&gt;
&lt;p&gt;
When No-Rejection Learning is Optimal for Regression with Rejection. (arXiv:2307.02932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25298;&#32477;&#23398;&#20064;&#26159;&#30740;&#31350;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#30456;&#20114;&#20316;&#29992;&#30340;&#20856;&#22411;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#39044;&#27979;&#22120;&#21644;&#19968;&#20010;&#25298;&#32477;&#22120;&#12290;&#22312;&#26679;&#26412;&#21040;&#36798;&#26102;&#65292;&#25298;&#32477;&#22120;&#39318;&#20808;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#23427;&#65307;&#22914;&#26524;&#25509;&#21463;&#65292;&#39044;&#27979;&#22120;&#23436;&#25104;&#39044;&#27979;&#20219;&#21153;&#65307;&#22914;&#26524;&#34987;&#25298;&#32477;&#65292;&#21017;&#23558;&#39044;&#27979;&#25512;&#36831;&#32473;&#20154;&#31867;&#12290;&#23398;&#20064;&#38382;&#39064;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#39044;&#27979;&#22120;&#21644;&#25298;&#32477;&#22120;&#12290;&#36825;&#25913;&#21464;&#20102;&#20256;&#32479;&#25439;&#22833;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#36890;&#24120;&#23548;&#33268;&#38750;&#20984;&#24615;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#23545;&#20110;&#24102;&#26377;&#25298;&#32477;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#19968;&#20123;&#30740;&#31350;&#24320;&#21457;&#20102;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#19968;&#33268;&#24615;&#20445;&#35777;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#25298;&#32477;&#30340;&#22238;&#24402;&#38382;&#39064;&#24182;&#30740;&#31350;&#20102;&#23558;&#20854;&#35270;&#20026;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#26469;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#26080;&#25298;&#32477;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning with rejection is a prototypical model for studying the interaction between humans and AI on prediction tasks. The model has two components, a predictor and a rejector. Upon the arrival of a sample, the rejector first decides whether to accept it; if accepted, the predictor fulfills the prediction task, and if rejected, the prediction will be deferred to humans. The learning problem requires learning a predictor and a rejector simultaneously. This changes the structure of the conventional loss function and often results in non-convexity and inconsistency issues. For the classification with rejection problem, several works develop surrogate losses for the jointly learning with provable consistency guarantees; in parallel, there has been less work for the regression counterpart. We study the regression with rejection (RwR) problem and investigate the no-rejection learning strategy which treats the RwR problem as a standard regression task to learn the predictor. We establish tha
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16927</link><description>&lt;p&gt;
&#32447;&#26463;&#33258;&#21160;&#39550;&#39542;&#65306;&#25361;&#25112;&#19982;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
End-to-end Autonomous Driving: Challenges and Frontiers. (arXiv:2306.16927v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#12290;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#19978;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#37319;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#29983;&#25104;&#36710;&#36742;&#36816;&#21160;&#35745;&#21010;&#65292;&#32780;&#19981;&#26159;&#19987;&#27880;&#20110;&#35832;&#22914;&#26816;&#27979;&#21644;&#36816;&#21160;&#39044;&#27979;&#31561;&#21333;&#20010;&#20219;&#21153;&#12290;&#19982;&#27169;&#22359;&#21270;&#27969;&#27700;&#32447;&#30456;&#27604;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#36890;&#36807;&#32852;&#21512;&#29305;&#24449;&#20248;&#21270;&#24863;&#30693;&#21644;&#35268;&#21010;&#26469;&#33719;&#30410;&#12290;&#36825;&#19968;&#39046;&#22495;&#22240;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12289;&#38381;&#29615;&#35780;&#20272;&#20197;&#21450;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#22312;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#25191;&#34892;&#25152;&#38656;&#30340;&#38656;&#27714;&#32780;&#34028;&#21187;&#21457;&#23637;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;250&#22810;&#31687;&#35770;&#25991;&#65292;&#28085;&#30422;&#20102;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#21160;&#26426;&#12289;&#36335;&#32447;&#22270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#22240;&#26524;&#28151;&#28102;&#12289;&#40065;&#26834;&#24615;&#21644;&#19990;&#30028;&#27169;&#22411;&#31561;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#30784;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 250 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#21033;&#29992;&#26381;&#21153;&#22120;&#31471;&#23458;&#25143;&#31471;&#20998;&#24067;&#21644;&#38468;&#21152;&#20449;&#24687;&#30340;&#25509;&#36817;&#20851;&#31995;&#65292;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#36890;&#20449;&#37327;&#21363;&#21487;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12625</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning through Importance Sampling. (arXiv:2306.12625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#21033;&#29992;&#26381;&#21153;&#22120;&#31471;&#23458;&#25143;&#31471;&#20998;&#24067;&#21644;&#38468;&#21152;&#20449;&#24687;&#30340;&#25509;&#36817;&#20851;&#31995;&#65292;&#21482;&#38656;&#35201;&#36739;&#23569;&#30340;&#36890;&#20449;&#37327;&#21363;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#31471;&#21521;&#26381;&#21153;&#22120;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#26159;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#37325;&#35201;&#29942;&#39048;&#12290;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#27604;&#29305;&#29575;-&#20934;&#30830;&#24615;&#25240;&#34935;&#8212;&#8212;&#20854;&#20013;&#23458;&#25143;&#31471;n&#21457;&#36865;&#26469;&#33258;&#20165;&#20026;&#35813;&#23458;&#25143;&#31471;&#30340;&#27010;&#29575;&#20998;&#24067;q&#966;&#65288;n&#65289;&#30340;&#26679;&#26412;&#65292;&#26381;&#21153;&#22120;&#20351;&#29992;&#36825;&#20123;&#26679;&#26412;&#20272;&#35745;&#23458;&#25143;&#31471;&#20998;&#24067;&#30340;&#24179;&#22343;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;FL&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20855;&#26377;&#39044;&#25968;&#25454;&#20998;&#24067;p&#952;&#30340;&#38468;&#21152;&#20449;&#24687;&#65292;&#35813;&#20998;&#24067;&#19982;&#23458;&#25143;&#31471;&#20998;&#24067;q&#966;&#65288;n&#65289;&#22312;Kullback-Leibler&#65288;KL&#65289;&#21457;&#25955;&#26041;&#38754;&#25509;&#36817;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26381;&#21153;&#22120;&#31471;&#23458;&#25143;&#31471;&#20998;&#24067;q&#966;&#65288;n)&#19982;&#38468;&#21152;&#20449;&#24687;p&#952;&#20043;&#38388;&#30340;&#36825;&#31181;&#25509;&#36817;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#38656;&#35201;&#22823;&#32422;Dkl&#65288;q&#966;&#65288;n&#65289;|| p&#952;&#65289;&#20301;&#30340;&#36890;&#20449;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high communication cost of sending model updates from the clients to the server is a significant bottleneck for scalable federated learning (FL). Among existing approaches, state-of-the-art bitrate-accuracy tradeoffs have been achieved using stochastic compression methods -- in which the client $n$ sends a sample from a client-only probability distribution $q_{\phi^{(n)}}$, and the server estimates the mean of the clients' distributions using these samples. However, such methods do not take full advantage of the FL setup where the server, throughout the training process, has side information in the form of a pre-data distribution $p_{\theta}$ that is close to the client's distribution $q_{\phi^{(n)}}$ in Kullback-Leibler (KL) divergence. In this work, we exploit this closeness between the clients' distributions $q_{\phi^{(n)}}$'s and the side information $p_{\theta}$ at the server, and propose a framework that requires approximately $D_{KL}(q_{\phi^{(n)}}|| p_{\theta})$ bits of com
&lt;/p&gt;</description></item><item><title>PEAR&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26041;&#27861;&#65292;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#23545;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26469;&#29983;&#25104;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#23618;&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PEAR&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06394</link><description>&lt;p&gt;
PEAR: &#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning. (arXiv:2306.06394v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06394
&lt;/p&gt;
&lt;p&gt;
PEAR&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26041;&#27861;&#65292;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#23545;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26469;&#29983;&#25104;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#23618;&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PEAR&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#21033;&#29992;&#26102;&#38388;&#25277;&#35937;&#21644;&#22686;&#21152;&#30340;&#25506;&#32034;&#24615;&#33021;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#26399;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#38750;&#38745;&#24577;&#24615;&#65292;&#20998;&#23618;&#20195;&#29702;&#38590;&#20197;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#65288;PEAR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#23569;&#37327;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#65292;&#20135;&#29983;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#32852;&#21512;&#20248;&#21270;HRL&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#26469;$(i)$&#38480;&#21046;&#25105;&#20204;&#26041;&#27861;&#30340;&#27425;&#20248;&#24615;&#65292;&#21644;$(ii)$&#25512;&#23548;&#20986;&#20351;&#29992;RL&#21644;IL&#30340;&#24191;&#20041;&#21363;&#25554;&#21363;&#29992;&#30340;&#26694;&#26550;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#12290;PEAR&#20351;&#29992;&#19968;&#20123;&#19987;&#23478;&#28436;&#31034;&#65292;&#24182;&#23545;&#20219;&#21153;&#32467;&#26500;&#36827;&#34892;&#26368;&#23567;&#30340;&#38480;&#21046;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#19982;&#20856;&#22411;&#30340;&#27169;&#22411;&#33258;&#30001;RL&#31639;&#27861;&#38598;&#25104;&#65292;&#20135;&#29983;&#19968;&#20010;&#23454;&#29992;&#30340;HRL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning (HRL) has the potential to solve complex long horizon tasks using temporal abstraction and increased exploration. However, hierarchical agents are difficult to train due to inherent non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a two-phase approach where we first perform adaptive relabeling on a few expert demonstrations to generate efficient subgoal supervision, and then jointly optimize HRL agents by employing reinforcement learning (RL) and imitation learning (IL). We perform theoretical analysis to $(i)$ bound the sub-optimality of our approach, and $(ii)$ derive a generalized plug-and-play framework for joint optimization using RL and IL. PEAR uses a handful of expert demonstrations and makes minimal limiting assumptions on the task structure. Additionally, it can be easily integrated with typical model free RL algorithms to produce a practical HRL algorithm. We perform experiments on challenging robotic environments
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05880</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Neural Representations&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#29992;&#20110;&#25554;&#20540;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;Implicit Neural Representations (INR)&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25110;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#35843;&#21046;INR&#21442;&#25968;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#35265;&#26679;&#26412;&#21644;&#26102;&#38388;&#31383;&#21475;&#31227;&#20301;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#26041;&#38754;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#35757;&#32451;&#21160;&#24577;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#23376;&#38598;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;ImageNet-1K&#21644;ImageNet-21K&#19978;&#23454;&#29616;&#20102;75&#65285;&#30340;&#26080;&#25439;&#21387;&#32553;&#27604;&#12290;</title><link>http://arxiv.org/abs/2306.05175</link><description>&lt;p&gt;
&#21160;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large-scale Dataset Pruning with Dynamic Uncertainty. (arXiv:2306.05175v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#35757;&#32451;&#21160;&#24577;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#23376;&#38598;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;ImageNet-1K&#21644;ImageNet-21K&#19978;&#23454;&#29616;&#20102;75&#65285;&#30340;&#26080;&#25439;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#25910;&#38598;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#24182;&#22312;&#20854;&#19978;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#26159;&#25512;&#21160;&#25216;&#26415;&#21069;&#36827;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#35745;&#31639;&#25104;&#26412;&#36880;&#28176;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20462;&#21098;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22797;&#26434;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#19988;&#24615;&#33021;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#35757;&#32451;&#21160;&#24577;&#26469;&#23454;&#29616;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;ImageNet-1K&#21644;ImageNet-21K&#65289;&#21644;&#20808;&#36827;&#27169;&#22411;&#65288;Swin Transformer&#21644;ConvNeXt&#65289;&#19978;&#30740;&#31350;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#24037;&#20316;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;ImageNet-1K&#21644;ImageNet-21K&#19978;&#23454;&#29616;&#20102;75&#65285;&#30340;&#26080;&#25439;&#21387;&#32553;&#27604;&#12290;&#20195;&#30721;&#21644;&#20462;&#21098;&#21518;&#30340;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/BAAI-DCAI/Dataset-Pruning&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state of the art of many learning tasks, e.g., image classification, is advanced by collecting larger datasets and then training larger models on them. As the outcome, the increasing computational cost is becoming unaffordable. In this paper, we investigate how to prune the large-scale datasets, and thus produce an informative subset for training sophisticated deep models with negligible performance drop. We propose a simple yet effective dataset pruning method by exploring both the prediction uncertainty and training dynamics. To our knowledge, this is the first work to study dataset pruning on large-scale datasets, i.e., ImageNet-1K and ImageNet-21K, and advanced models, i.e., Swin Transformer and ConvNeXt. Extensive experimental results indicate that our method outperforms the state of the art and achieves 75% lossless compression ratio on both ImageNet-1K and ImageNet-21K. The code and pruned datasets are available at https://github.com/BAAI-DCAI/Dataset-Pruning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#25968;&#25454;&#30340;&#24178;&#39044;&#25514;&#26045;&#25552;&#39640;&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;CDHF&#26694;&#26550;&#26469;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#65292;&#39044;&#27979;&#24314;&#35758;&#25509;&#21463;&#31243;&#24230;&#24182;&#20915;&#23450;&#20309;&#26102;&#23637;&#31034;&#21738;&#20123;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04930</link><description>&lt;p&gt;
&#20309;&#26102;&#23637;&#31034;&#24314;&#35758;&#65311;&#22312;AI&#36741;&#21161;&#32534;&#31243;&#20013;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming. (arXiv:2306.04930v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20808;&#21069;&#25968;&#25454;&#30340;&#24178;&#39044;&#25514;&#26045;&#25552;&#39640;&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;CDHF&#26694;&#26550;&#26469;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#65292;&#39044;&#27979;&#24314;&#35758;&#25509;&#21463;&#31243;&#24230;&#24182;&#20915;&#23450;&#20309;&#26102;&#23637;&#31034;&#21738;&#20123;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#65292;&#22914;Copilot&#21644;CodeWhisperer&#65292;&#25552;&#20379;&#31243;&#24207;&#21592;&#29615;&#22659;&#65288;&#20363;&#22914;IDE&#65289;&#20869;&#30340;&#20195;&#30721;&#24314;&#35758;&#65292;&#26088;&#22312;&#25552;&#39640;&#20182;&#20204;&#30340;&#29983;&#20135;&#21147;&#12290;&#30001;&#20110;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#31243;&#24207;&#21592;&#25509;&#21463;&#21644;&#25298;&#32477;&#24314;&#35758;&#65292;&#22240;&#27492;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#35813;&#31995;&#32479;&#24212;&#20351;&#29992;&#27492;&#21453;&#39304;&#20197;&#20419;&#36827;&#36825;&#19968;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#31243;&#24207;&#21592;&#19982;Copilot&#20132;&#20114;&#30340;&#20808;&#21069;&#25968;&#25454;&#65292;&#24320;&#21457;&#21487;&#20197;&#33410;&#30465;&#31243;&#24207;&#21592;&#26102;&#38388;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#19982;&#31243;&#24207;&#21592;&#30340;&#20132;&#20114;&#65292;&#24182;&#20915;&#23450;&#20309;&#26102;&#23637;&#31034;&#21738;&#20123;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#8220;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#26465;&#20214;&#24314;&#35758;&#23637;&#31034;&#8221;&#65288;CDHF&#65289;&#22522;&#20110;&#23545;&#31243;&#24207;&#21592;&#25805;&#20316;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#20351;&#29992;535&#21517;&#31243;&#24207;&#21592;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21487;&#20197;&#39044;&#27979;&#24314;&#35758;&#25509;&#21463;&#31243;&#24230;&#30340;&#27169;&#22411;&#12290;&#22312;&#23545;&#36890;&#36807;AI&#36741;&#21161;&#32534;&#31243;&#35299;&#20915;&#30340;&#30495;&#23454;&#19990;&#30028;&#32534;&#31243;&#20219;&#21153;&#30340;&#22238;&#39038;&#24615;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;CDHF&#33021;&#22815;&#23454;&#29616;&#26377;&#21033;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25972;&#21512;&#20154;&#31867;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;AI&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI powered code-recommendation systems, such as Copilot and CodeWhisperer, provide code suggestions inside a programmer's environment (e.g., an IDE) with the aim to improve their productivity. Since, in these scenarios, programmers accept and reject suggestions, ideally, such a system should use this feedback in furtherance of this goal. In this work we leverage prior data of programmers interacting with Copilot to develop interventions that can save programmer time. We propose a utility theory framework, which models this interaction with programmers and decides when and which suggestions to display. Our framework Conditional suggestion Display from Human Feedback (CDHF) is based on predictive models of programmer actions. Using data from 535 programmers we build models that predict the likelihood of suggestion acceptance. In a retrospective evaluation on real-world programming tasks solved with AI-assisted programming, we find that CDHF can achieve favorable tradeoffs. Our findings s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#39640;&#21361;&#23381;&#20135;&#22919;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#22922;&#23072;&#26816;&#27979;&#12289;&#20934;&#30830;&#35782;&#21035;&#39640;&#39118;&#38505;&#20250;&#21592;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#25351;&#26631;&#31561;&#19977;&#20010;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#23381;&#26399;&#39118;&#38505;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.17261</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20851;&#38381;&#39640;&#21361;&#23381;&#20135;&#22919;&#25252;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Closing the Gap in High-Risk Pregnancy Care Using Machine Learning and Human-AI Collaboration. (arXiv:2305.17261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#39640;&#21361;&#23381;&#20135;&#22919;&#35745;&#21010;&#65292;&#25552;&#20986;&#20102;&#26089;&#26399;&#22922;&#23072;&#26816;&#27979;&#12289;&#20934;&#30830;&#35782;&#21035;&#39640;&#39118;&#38505;&#20250;&#21592;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#25351;&#26631;&#31561;&#19977;&#20010;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#39640;&#20102;&#23381;&#26399;&#39118;&#38505;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#20445;&#38505;&#20844;&#21496;&#36890;&#24120;&#20351;&#29992;&#31639;&#27861;&#26469;&#35782;&#21035;&#20250;&#21463;&#30410;&#20110;&#25252;&#29702;&#21644;&#29366;&#20917;&#31649;&#29702;&#35745;&#21010;&#30340;&#20250;&#21592;&#65292;&#35813;&#35745;&#21010;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#39640;&#31471;&#20020;&#24202;&#25903;&#25345;&#12290;&#31639;&#27861;&#35782;&#21035;&#19982;&#20020;&#24202;&#24178;&#39044;&#20043;&#38388;&#30340;&#21450;&#26102;&#12289;&#20934;&#30830;&#21644;&#26080;&#32541;&#38598;&#25104;&#21462;&#20915;&#20110;&#31995;&#32479;&#35774;&#35745;&#24072;&#21644;&#25252;&#29702;&#31649;&#29702;&#21592;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#12290;&#25105;&#20204;&#20851;&#27880;&#20102;&#19968;&#20010;&#26088;&#22312;&#20943;&#23569;&#23381;&#20135;&#22919;&#19981;&#33391;&#20135;&#21069;&#12289;&#20135;&#26399;&#21644;&#20135;&#21518;&#20107;&#20214;&#30340;&#39640;&#21361;&#23381;&#20135;&#22919;&#35745;&#21010;&#65292;&#24182;&#25551;&#36848;&#20102;&#25105;&#20204;&#22914;&#20309;&#20811;&#26381;&#25252;&#29702;&#31649;&#29702;&#21592;&#25152;&#36848;&#30340;&#19977;&#20010;HRP&#35745;&#21010;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#26089;&#26399;&#26816;&#27979;&#22922;&#23072;&#65292;&#65288;2&#65289;&#20934;&#30830;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#39640;&#39118;&#38505;&#20250;&#21592;&#65292;&#20197;&#21450;&#65288;3&#65289;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#25351;&#26631;&#26469;&#34917;&#20805;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23381;&#26399;&#35782;&#21035;&#31639;&#27861;&#65292;&#22312;&#22238;&#39038;&#24615;&#30740;&#31350;&#20013;&#27604;&#20043;&#21069;&#22522;&#20110;&#20195;&#30721;&#30340;&#27169;&#22411;&#25552;&#21069;&#20102;57&#22825;&#35782;&#21035;&#22922;&#23072;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#39044;&#27979;&#20250;&#24433;&#21709;&#23381;&#26399;&#30340;&#24182;&#21457;&#30151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Health insurers often use algorithms to identify members who would benefit from care and condition management programs, which provide personalized, high-touch clinical support. Timely, accurate, and seamless integration between algorithmic identification and clinical intervention depends on effective collaboration between the system designers and nurse care managers. We focus on a high-risk pregnancy (HRP) program designed to reduce the likelihood of adverse prenatal, perinatal, and postnatal events and describe how we overcome three challenges of HRP programs as articulated by nurse care managers; (1) early detection of pregnancy, (2) accurate identification of impactable high-risk members, and (3) provision of explainable indicators to supplement predictions. We propose a novel algorithm for pregnancy identification that identifies pregnancies 57 days earlier than previous code-based models in a retrospective study. We then build a model to predict impactable pregnancy complications 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#8212;&#8212;&#36830;&#32493;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; (CORL)&#65292;&#35299;&#20915;&#20102;&#20195;&#29702;&#22312;&#31163;&#32447;&#20219;&#21153;&#24207;&#21015;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#32463;&#39564;&#37325;&#25918; (ER) &#26159;&#26368;&#36866;&#21512; CORL &#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#20294;&#24341;&#20837; ER &#21518;&#20250;&#36935;&#21040;&#26032;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13804</link><description>&lt;p&gt;
&#31163;&#32447;&#20307;&#39564;&#37325;&#25918;&#29992;&#20110;&#36830;&#32493;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Experience Replay for Continual Offline Reinforcement Learning. (arXiv:2305.13804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#8212;&#8212;&#36830;&#32493;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; (CORL)&#65292;&#35299;&#20915;&#20102;&#20195;&#29702;&#22312;&#31163;&#32447;&#20219;&#21153;&#24207;&#21015;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#65292;&#32463;&#39564;&#37325;&#25918; (ER) &#26159;&#26368;&#36866;&#21512; CORL &#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#20294;&#24341;&#20837; ER &#21518;&#20250;&#36935;&#21040;&#26032;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#19968;&#31995;&#21015;&#39044;&#20808;&#25910;&#38598;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#19981;&#26029;&#23398;&#20064;&#26032;&#25216;&#33021;&#26159;&#29702;&#24819;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#36830;&#32493;&#23398;&#20064;&#19968;&#31995;&#21015;&#31163;&#32447;&#20219;&#21153;&#24456;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22330;&#26223;&#8212;&#8212;&#36830;&#32493;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; (CORL)&#65292;&#20195;&#29702;&#36890;&#36807;&#19968;&#20010;&#23567;&#30340;&#22238;&#25918;&#32531;&#20914;&#21306;&#23398;&#20064;&#19968;&#31995;&#21015;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#25152;&#26377;&#23398;&#20064;&#20219;&#21153;&#20013;&#36861;&#27714;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#25506;&#32034;&#25152;&#26377;&#39034;&#24207;&#20219;&#21153;&#30340;&#20219;&#20309;&#29615;&#22659;&#12290;&#20026;&#20102;&#22312;&#25152;&#26377;&#39034;&#24207;&#20219;&#21153;&#19978;&#25345;&#32493;&#23398;&#20064;&#65292;&#20195;&#29702;&#38656;&#35201;&#20197;&#31163;&#32447;&#26041;&#24335;&#33719;&#21462;&#26032;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#26087;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23454;&#39564;&#21457;&#29616;&#32463;&#39564;&#37325;&#25918; (ER) &#26159; CORL &#38382;&#39064;&#26368;&#36866;&#21512;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23558; ER &#24341;&#20837; CORL &#20250;&#36935;&#21040;&#26032;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65306;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#29366;&#24577;&#20998;&#24067;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability of continuously learning new skills via a sequence of pre-collected offline datasets is desired for an agent. However, consecutively learning a sequence of offline tasks likely leads to the catastrophic forgetting issue under resource-limited scenarios. In this paper, we formulate a new setting, continual offline reinforcement learning (CORL), where an agent learns a sequence of offline reinforcement learning tasks and pursues good performance on all learned tasks with a small replay buffer without exploring any of the environments of all the sequential tasks. For consistently learning on all sequential tasks, an agent requires acquiring new knowledge and meanwhile preserving old knowledge in an offline manner. To this end, we introduced continual learning algorithms and experimentally found experience replay (ER) to be the most suitable algorithm for the CORL problem. However, we observe that introducing ER into CORL encounters a new distribution shift problem: the mism
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DermSynth3D&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#22120;&#23558;&#30382;&#32932;&#30149;&#21464;&#27169;&#24335;&#28151;&#21512;&#21040;&#20154;&#20307;&#19977;&#32500;&#32441;&#29702;&#32593;&#26684;&#19978;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#20108;&#32500;&#30382;&#32932;&#38236;&#20687;&#22270;&#20687;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#24212;&#30340;&#23494;&#38598;&#27880;&#37322;&#20197;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.12621</link><description>&lt;p&gt;
DermSynth3D&#65306;&#37326;&#22806;&#27880;&#37322;&#30382;&#32932;&#31185;&#22270;&#20687;&#30340;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
DermSynth3D: Synthesis of in-the-wild Annotated Dermatology Images. (arXiv:2305.12621v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DermSynth3D&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#22120;&#23558;&#30382;&#32932;&#30149;&#21464;&#27169;&#24335;&#28151;&#21512;&#21040;&#20154;&#20307;&#19977;&#32500;&#32441;&#29702;&#32593;&#26684;&#19978;&#24182;&#29983;&#25104;&#36924;&#30495;&#30340;&#20108;&#32500;&#30382;&#32932;&#38236;&#20687;&#22270;&#20687;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#24212;&#30340;&#23494;&#38598;&#27880;&#37322;&#20197;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#30382;&#32932;&#31185;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#26174;&#30528;&#38480;&#21046;&#65292;&#21253;&#25324;&#26679;&#26412;&#22270;&#20687;&#25968;&#37327;&#36739;&#23569;&#12289;&#30142;&#30149;&#26465;&#20214;&#26377;&#38480;&#12289;&#27880;&#37322;&#19981;&#36275;&#20197;&#21450;&#38750;&#26631;&#20934;&#21270;&#22270;&#20687;&#37319;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DermSynth3D&#30340;&#26032;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#21487;&#24494;&#20998;&#28210;&#26579;&#22120;&#23558;&#30382;&#32932;&#30149;&#21464;&#27169;&#24335;&#28151;&#21512;&#21040;&#20154;&#20307;&#30340;&#19977;&#32500;&#32441;&#29702;&#32593;&#26684;&#19978;&#65292;&#24182;&#22312;&#21508;&#31181;&#32972;&#26223;&#22330;&#26223;&#19979;&#37319;&#29992;&#19981;&#21516;&#35270;&#35282;&#21644;&#20809;&#29031;&#26465;&#20214;&#29983;&#25104;&#20108;&#32500;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36981;&#24490;&#33258;&#19978;&#32780;&#19979;&#30340;&#35268;&#21017;&#65292;&#38480;&#21046;&#28151;&#21512;&#21644;&#28210;&#26579;&#36807;&#31243;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;&#37326;&#22806;&#29031;&#29255;&#24863;&#30340;&#30382;&#32932;&#26465;&#20214;&#30340;&#20108;&#32500;&#22270;&#20687;&#65292;&#30830;&#20445;&#26356;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#35813;&#26694;&#26550;&#29983;&#25104;&#36924;&#30495;&#30340;&#20108;&#32500;&#30382;&#32932;&#38236;&#20687;&#22270;&#20687;&#65292;&#24182;&#29983;&#25104;&#23545;&#30382;&#32932;&#12289;&#30382;&#32932;&#29366;&#20917;&#12289;&#36523;&#20307;&#37096;&#20301;&#21644;&#22836;&#21457;&#21306;&#22495;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#23545;&#24212;&#23494;&#38598;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning (DL) has shown great potential in the field of dermatological image analysis. However, existing datasets in this domain have significant limitations, including a small number of image samples, limited disease conditions, insufficient annotations, and non-standardized image acquisitions. To address these shortcomings, we propose a novel framework called DermSynth3D. DermSynth3D blends skin disease patterns onto 3D textured meshes of human subjects using a differentiable renderer and generates 2D images from various camera viewpoints under chosen lighting conditions in diverse background scenes. Our method adheres to top-down rules that constrain the blending and rendering process to create 2D images with skin conditions that mimic in-the-wild acquisitions, ensuring more meaningful results. The framework generates photo-realistic 2D dermoscopy images and the corresponding dense annotations for semantic segmentation of the skin, skin conditions, body parts, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#35793;&#30721;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#30028;&#23450;&#20102;&#35299;&#30721;&#22120;&#30340;&#27867;&#21270;&#38388;&#38553;&#65292;&#32467;&#26524;&#19982;&#35299;&#30721;&#22120;&#30340;&#22797;&#26434;&#31243;&#24230;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.10540</link><description>&lt;p&gt;
&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#35793;&#30721;&#22120;&#30340;&#27867;&#21270;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds for Neural Belief Propagation Decoders. (arXiv:2305.10540v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#35793;&#30721;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#30028;&#23450;&#20102;&#35299;&#30721;&#22120;&#30340;&#27867;&#21270;&#38388;&#38553;&#65292;&#32467;&#26524;&#19982;&#35299;&#30721;&#22120;&#30340;&#22797;&#26434;&#31243;&#24230;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#37319;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#19979;&#19968;&#20195;&#36890;&#20449;&#31995;&#32479;&#30340;&#35299;&#30721;&#22120;&#12290;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#26159;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#65288;NBP&#65289;&#65292;&#23427;&#23558;&#32622;&#20449;&#20256;&#25773;&#65288;BP&#65289;&#36845;&#20195;&#23637;&#24320;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21442;&#25968;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;NBP&#35299;&#30721;&#22120;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#35299;&#30721;&#31639;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;NBP&#35299;&#30721;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35299;&#30721;&#22120;&#30340;&#27867;&#21270;&#38388;&#38553;&#26159;&#32463;&#39564;&#21644;&#26399;&#26395;&#35823;&#30721;&#29575;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#30028;&#23450;&#20102;&#36825;&#31181;&#24046;&#36317;&#65292;&#24182;&#34920;&#26126;&#23427;&#19982;&#35299;&#30721;&#22120;&#30340;&#22797;&#26434;&#31243;&#24230;&#65288;&#21363;&#20195;&#30721;&#21442;&#25968;&#65288;&#22359;&#38271;&#24230;&#12289;&#28040;&#24687;&#38271;&#24230;&#12289;&#21464;&#37327;/&#26816;&#26597;&#33410;&#28857;&#24230;&#25968;&#65289;&#12289;&#35299;&#30721;&#36845;&#20195;&#27425;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#26377;&#20851;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24120;&#35268;&#21644;&#19981;&#35268;&#21017;&#22855;&#20598;&#26657;&#39564;&#30697;&#38453;&#30340;&#32467;&#26524;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#32452;&#20851;&#20110;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#35793;&#30721;&#22120;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning based approaches are being increasingly used for designing decoders for next generation communication systems. One widely used framework is neural belief propagation (NBP), which unfolds the belief propagation (BP) iterations into a deep neural network and the parameters are trained in a data-driven manner. NBP decoders have been shown to improve upon classical decoding algorithms. In this paper, we investigate the generalization capabilities of NBP decoders. Specifically, the generalization gap of a decoder is the difference between empirical and expected bit-error-rate(s). We present new theoretical results which bound this gap and show the dependence on the decoder complexity, in terms of code parameters (blocklength, message length, variable/check node degrees), decoding iterations, and the training dataset size. Results are presented for both regular and irregular parity-check matrices. To the best of our knowledge, this is the first set of theoretical results on 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;RePU&#28608;&#27963;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#36817;&#20284;$C^s$&#24179;&#28369;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#21516;&#26102;&#24314;&#31435;&#20102;&#19979;&#38480;&#35823;&#24046;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RePU&#32593;&#32476;&#30340;&#24809;&#32602;&#20445;&#24207;&#22238;&#24402;(PDIR)&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00608</link><description>&lt;p&gt;
&#20351;&#29992;RePU&#28608;&#27963;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#65306;&#22312;&#24471;&#20998;&#20272;&#35745;&#21644;&#20445;&#24207;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable Neural Networks with RePU Activation: with Applications to Score Estimation and Isotonic Regression. (arXiv:2305.00608v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00608
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;RePU&#28608;&#27963;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#36817;&#20284;$C^s$&#24179;&#28369;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#21516;&#26102;&#24314;&#31435;&#20102;&#19979;&#38480;&#35823;&#24046;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RePU&#32593;&#32476;&#30340;&#24809;&#32602;&#20445;&#24207;&#22238;&#24402;(PDIR)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#20462;&#27491;&#21518;&#30340;&#24130;&#21333;&#20803;&#65288;RePU&#65289;&#20989;&#25968;&#28608;&#27963;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RePU&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#23548;&#25968;&#21487;&#20197;&#30001;&#28151;&#21512;&#28608;&#27963;RePU&#32593;&#32476;&#26469;&#34920;&#31034;&#65292;&#24182;&#25512;&#23548;&#20102;&#23548;&#25968;RePU&#32593;&#32476;&#20989;&#25968;&#31867;&#30340;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#12290;&#22312;&#20351;&#29992;RePU&#28608;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21516;&#26102;&#36817;&#20284;$C^s$&#24179;&#28369;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#35823;&#24046;&#30028;&#12290;&#27492;&#22806;&#65292;&#24403;&#25968;&#25454;&#20855;&#26377;&#36817;&#20284;&#20302;&#32500;&#25903;&#25345;&#26102;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#25913;&#36827;&#30340;&#36924;&#36817;&#35823;&#24046;&#30028;&#65292;&#35777;&#26126;&#20102;RePU&#32593;&#32476;&#20943;&#32531;&#32500;&#24230;&#28798;&#38590;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#28145;&#24230;&#24471;&#20998;&#21305;&#37197;&#20272;&#35745;&#22120;(DSME)&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RePU&#32593;&#32476;&#30340;&#24809;&#32602;&#20445;&#24207;&#22238;&#24402;(PDIR)&#12290;&#25105;&#20204;&#22312;&#20551;&#23450;&#30446;&#26631;&#20989;&#25968;&#23646;&#20110;$C^s$&#24179;&#28369;&#20989;&#25968;&#31867;&#30340;&#24773;&#20917;&#19979;&#20026;DSME&#21644;PDIR&#24314;&#31435;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the properties of differentiable neural networks activated by rectified power unit (RePU) functions. We show that the partial derivatives of RePU neural networks can be represented by RePUs mixed-activated networks and derive upper bounds for the complexity of the function class of derivatives of RePUs networks. We establish error bounds for simultaneously approximating $C^s$ smooth functions and their derivatives using RePU-activated deep neural networks. Furthermore, we derive improved approximation error bounds when data has an approximate low-dimensional support, demonstrating the ability of RePU networks to mitigate the curse of dimensionality. To illustrate the usefulness of our results, we consider a deep score matching estimator (DSME) and propose a penalized deep isotonic regression (PDIR) using RePU networks. We establish non-asymptotic excess risk bounds for DSME and PDIR under the assumption that the target functions belong to a class of $C^s$ smooth functions. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#12290;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;&#21069;&#21521;&#35757;&#32451;&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#26469;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#65292;&#20174;&#32780;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2304.10126</link><description>&lt;p&gt;
&#35299;&#32806;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#31616;&#21333;&#30340;GNN&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decouple Graph Neural Networks: Train Multiple Simple GNNs Simultaneously Instead of One. (arXiv:2304.10126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#12290;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;&#21069;&#21521;&#35757;&#32451;&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#26469;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#65292;&#20174;&#32780;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23384;&#22312;&#20005;&#37325;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#33410;&#28857;&#20381;&#36182;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#36825;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#20351;&#24471;GNN&#30340;&#35757;&#32451;&#36890;&#24120;&#24456;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#22810;&#23618;GNN&#35299;&#32806;&#20026;&#22810;&#20010;&#31616;&#21333;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#35813;&#26041;&#27861;&#30001;&#32463;&#20856;&#30340;&#21069;&#21521;&#35757;&#32451;&#65288;FT&#65289;&#21644;&#35774;&#35745;&#30340;&#21453;&#21521;&#35757;&#32451;&#65288;BT&#65289;&#32452;&#25104;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19979;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#31639;&#27861;&#22312;FT&#20013;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#19981;&#20250;&#25197;&#26354;&#22270;&#24418;&#20449;&#24687;&#12290;&#20026;&#36991;&#20813;FT&#30340;&#21482;&#21333;&#21521;&#20449;&#24687;&#20256;&#36882;&#65292;&#24182;&#20805;&#20998;&#35757;&#32451;&#27973;&#23618;&#27169;&#22359;&#21644;&#26356;&#28145;&#23618;&#30340;&#27169;&#22359;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21453;&#21521;&#35757;&#32451;&#26426;&#21046;&#65292;&#20351;&#21069;&#38754;&#30340;&#27169;&#22359;&#33021;&#22815;&#24863;&#30693;&#21518;&#38754;&#30340;&#27169;&#22359;&#12290;&#36825;&#31181;&#21453;&#21521;&#35757;&#32451;&#24341;&#20837;&#20102;&#21453;&#21521;&#20449;&#24687;&#20256;&#36882;&#21040;&#35299;&#32806;&#27169;&#22359;&#20013;&#65292;&#21516;&#26102;&#20063;&#20250;&#26377;&#21069;&#21521;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNN) suffer from severe inefficiency. It is mainly caused by the exponential growth of node dependency with the increase of layers. It extremely limits the application of stochastic optimization algorithms so that the training of GNN is usually time-consuming. To address this problem, we propose to decouple a multi-layer GNN as multiple simple modules for more efficient training, which is comprised of classical forward training (FT)and designed backward training (BT). Under the proposed framework, each module can be trained efficiently in FT by stochastic algorithms without distortion of graph information owing to its simplicity. To avoid the only unidirectional information delivery of FT and sufficiently train shallow modules with the deeper ones, we develop a backward training mechanism that makes the former modules perceive the latter modules. The backward training introduces the reversed information delivery into the decoupled modules as well as the forward i
&lt;/p&gt;</description></item><item><title>CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.04391</link><description>&lt;p&gt;
CAFIN: &#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#30340;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs. (arXiv:2304.04391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04391
&lt;/p&gt;
&lt;p&gt;
CAFIN&#26159;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#24615;&#22686;&#24378;&#36827;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#26368;&#20248;&#20844;&#24179;&#32467;&#26524;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25152;&#23398;&#23884;&#20837;&#30340;&#32039;&#20945;&#24615;&#21644;&#20016;&#23500;&#24615;&#20197;&#21450;&#26410;&#26631;&#35760;&#22270;&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22270;&#34920;&#31034;&#22312;(&#22823;&#22411;)&#22270;&#19978;&#24050;&#32463;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#37325;&#35270;&#12290;&#24403;&#36825;&#20123;&#33410;&#28857;&#34920;&#31034;&#34987;&#37096;&#32626;&#26102;&#65292;&#24517;&#39035;&#20351;&#29992;&#36866;&#24403;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#26465;&#20214;&#29983;&#25104;&#20197;&#20943;&#23569;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#36896;&#25104;&#30340;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24050;&#32463;&#35843;&#26597;&#20102;&#22270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32676;&#20307;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#36825;&#20123;&#20844;&#24179;&#24615;&#27010;&#24565;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#27809;&#26377;&#32771;&#34385;&#36830;&#25509;&#27169;&#24335;&#22312;&#22270;&#20013;&#23548;&#33268;&#30340;&#19981;&#21516;&#33410;&#28857;&#24433;&#21709;(&#25110;&#20013;&#24515;&#24615;&#33021;&#37327;)&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#24402;&#32435;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#20844;&#24179;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CAFIN&#65288;Centrality Aware Fairness inducing IN-processing&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#22270;&#32467;&#26500;&#25913;&#36827;GraphSAGE&#34920;&#31034;&#30340;&#36827;&#31243;&#25216;&#26415;&#8212;&#8212;&#26080;&#30417;&#30563;&#22270;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#26694;&#26550;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CAFIN&#22312;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning on (large) graphs has received significant attention in the research community due to the compactness and richness of the learned embeddings and the abundance of unlabelled graph data. When deployed, these node representations must be generated with appropriate fairness constraints to minimize bias induced by them on downstream tasks. Consequently, group and individual fairness notions for graph learning algorithms have been investigated for specific downstream tasks. One major limitation of these fairness notions is that they do not consider the connectivity patterns in the graph leading to varied node influence (or centrality power). In this paper, we design a centrality-aware fairness framework for inductive graph representation learning algorithms. We propose CAFIN (Centrality Aware Fairness inducing IN-processing), an in-processing technique that leverages graph structure to improve GraphSAGE's representations - a popular framework in the unsup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#23884;&#20837;&#25216;&#26415;&#65288;LOPT&#65289;&#65292;&#23427;&#25193;&#23637;&#20102;&#65288;&#23616;&#37096;&#65289;&#32447;&#24615;&#21270;&#25216;&#26415;&#21040;OPT&#38382;&#39064;&#19978;&#65292;&#25552;&#39640;&#20102;&#27491;&#27979;&#24230;&#23545;&#20043;&#38388;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#24182;&#19988;&#22312;&#28857;&#20113;&#20869;&#25554;&#21644;PCA&#20998;&#26512;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.03232</link><description>&lt;p&gt;
&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Linear Optimal Partial Transport Embedding. (arXiv:2302.03232v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#23884;&#20837;&#25216;&#26415;&#65288;LOPT&#65289;&#65292;&#23427;&#25193;&#23637;&#20102;&#65288;&#23616;&#37096;&#65289;&#32447;&#24615;&#21270;&#25216;&#26415;&#21040;OPT&#38382;&#39064;&#19978;&#65292;&#25552;&#39640;&#20102;&#27491;&#27979;&#24230;&#23545;&#20043;&#38388;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#24182;&#19988;&#22312;&#28857;&#20113;&#20869;&#25554;&#21644;PCA&#20998;&#26512;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#30001;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#32479;&#35745;&#23398;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#24179;&#34913;&#36136;&#37327;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;OT&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#21253;&#25324;&#19981;&#24179;&#34913;OT&#65292;&#26368;&#20248;&#20559;&#31227;&#20256;&#36755;&#65288;OPT&#65289;&#21644;Hellinger Kantorovich&#65288;HK&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#65288;LOPT&#65289;&#23884;&#20837;&#25216;&#26415;&#65292;&#23427;&#23558;OT&#21644;HK&#19978;&#30340;&#65288;&#23616;&#37096;&#65289;&#32447;&#24615;&#21270;&#25216;&#26415;&#25193;&#23637;&#21040;OPT&#38382;&#39064;&#19978;&#12290;&#25152;&#25552;&#20986;&#30340;&#23884;&#20837;&#25216;&#26415;&#25552;&#39640;&#20102;&#27491;&#27979;&#24230;&#23545;&#20043;&#38388;&#30340;LOPT&#36317;&#31163;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#38500;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LOPT&#23884;&#20837;&#25216;&#26415;&#22312;&#28857;&#20113;&#20869;&#25554;&#21644;PCA&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) has gained popularity due to its various applications in fields such as machine learning, statistics, and signal processing. However, the balanced mass requirement limits its performance in practical problems. To address these limitations, variants of the OT problem, including unbalanced OT, Optimal partial transport (OPT), and Hellinger Kantorovich (HK), have been proposed. In this paper, we propose the Linear optimal partial transport (LOPT) embedding, which extends the (local) linearization technique on OT and HK to the OPT problem. The proposed embedding allows for faster computation of OPT distance between pairs of positive measures. Besides our theoretical contributions, we demonstrate the LOPT embedding technique in point-cloud interpolation and PCA analysis.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;GitHub Copilot&#65292;&#21457;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35813;&#31995;&#32479;&#20132;&#20114;&#30340;&#19968;&#20123;&#24120;&#35265;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#20854;&#20302;&#25928;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#20174;&#32780;&#20026;&#25913;&#36827;&#30028;&#38754;&#35774;&#35745;&#21644;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#21160;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.14306</link><description>&lt;p&gt;
&#35835;&#25026;&#20195;&#30721;&#32972;&#21518;&#65306;&#27169;&#25311;AI&#36741;&#21161;&#32534;&#31243;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming. (arXiv:2210.14306v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;GitHub Copilot&#65292;&#21457;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35813;&#31995;&#32479;&#20132;&#20114;&#30340;&#19968;&#20123;&#24120;&#35265;&#27963;&#21160;&#65292;&#25581;&#31034;&#20102;&#20854;&#20302;&#25928;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#20174;&#32780;&#20026;&#25913;&#36827;&#30028;&#38754;&#35774;&#35745;&#21644;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#65292;&#22914;Copilot&#21644;CodeWhisperer&#65292;&#36890;&#36807;&#33258;&#21160;&#24314;&#35758;&#21644;&#33258;&#21160;&#23436;&#25104;&#20195;&#30721;&#65292;&#26377;&#28508;&#21147;&#25552;&#39640;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#29575;&#12290;&#28982;&#32780;&#65292;&#35201;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#65292;&#24182;&#30830;&#23450;&#25913;&#36827;&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#21462;&#24471;&#36827;&#23637;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#22825;&#30001;&#25968;&#30334;&#19975;&#31243;&#24207;&#21592;&#20351;&#29992;&#30340;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;GitHub Copilot&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24120;&#35265;&#31243;&#24207;&#21592;&#27963;&#21160;&#30340;&#20998;&#31867;&#31995;&#32479;CUPS&#65292;&#20197;&#20415;&#27169;&#25311;&#29992;&#25143;&#19982;Copilot&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#23545;21&#21517;&#23436;&#25104;&#32534;&#30721;&#20219;&#21153;&#24182;&#22238;&#39038;&#24615;&#22320;&#20351;&#29992;CUPS&#26631;&#35760;&#20854;&#20250;&#35805;&#30340;&#31243;&#24207;&#21592;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;CUPS&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;&#20195;&#30721;&#25512;&#33616;&#31995;&#32479;&#20132;&#20114;&#65292;&#25581;&#31034;&#20102;&#25928;&#29575;&#20302;&#19979;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#27934;&#35265;&#25581;&#31034;&#20102;&#31243;&#24207;&#21592;&#22914;&#20309;&#19982;Copilot&#20132;&#20114;&#65292;&#24182;&#28608;&#21457;&#20102;&#26032;&#30340;&#30028;&#38754;&#35774;&#35745;&#21644;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-recommendation systems, such as Copilot and CodeWhisperer, have the potential to improve programmer productivity by suggesting and auto-completing code. However, to fully realize their potential, we must understand how programmers interact with these systems and identify ways to improve that interaction. To make progress, we studied GitHub Copilot, a code-recommendation system used by millions of programmers daily. We developed CUPS, a taxonomy of common programmer activities when interacting with Copilot. Our study of 21 programmers, who completed coding tasks and retrospectively labeled their sessions with CUPS, showed that CUPS can help us understand how programmers interact with code-recommendation systems, revealing inefficiencies and time costs. Our insights reveal how programmers interact with Copilot and motivate new interface designs and metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#38750;&#36127;&#36229;-&#38789;&#19981;&#31561;&#24335;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#28385;&#36275;&#26679;&#26412;&#36335;&#24452;&#26102;&#31354;&#20852;&#22859;&#26465;&#20214;&#26102;&#65292;&#33410;&#28857;&#30340;&#20272;&#35745;&#21487;&#20197;&#25910;&#25947;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2206.03861</link><description>&lt;p&gt;
&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Online Regularized Learning Over Random Time-Varying Graphs. (arXiv:2206.03861v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#38750;&#36127;&#36229;-&#38789;&#19981;&#31561;&#24335;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#28385;&#36275;&#26679;&#26412;&#36335;&#24452;&#26102;&#31354;&#20852;&#22859;&#26465;&#20214;&#26102;&#65292;&#33410;&#28857;&#30340;&#20272;&#35745;&#21487;&#20197;&#25910;&#25947;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#26102;&#21464;&#22270;&#19978;&#30340;&#20998;&#25955;&#22312;&#32447;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#20013;&#65292;&#27599;&#20010;&#33410;&#28857;&#37117;&#36816;&#34892;&#19968;&#20010;&#22312;&#32447;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21253;&#25324;&#21019;&#26032;&#39033;&#65288;&#22788;&#29702;&#33258;&#36523;&#26032;&#27979;&#37327;&#20540;&#65289;&#12289;&#20849;&#35782;&#39033;&#65288;&#21152;&#26435;&#24179;&#22343;&#33258;&#36523;&#21450;&#20854;&#37051;&#23621;&#30340;&#20272;&#35745;&#65292;&#24102;&#26377;&#21152;&#24615;&#21644;&#20056;&#24615;&#36890;&#20449;&#22122;&#22768;&#65289;&#21644;&#27491;&#21017;&#21270;&#39033;&#65288;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#65289;&#12290;&#19981;&#35201;&#27714;&#22238;&#24402;&#30697;&#38453;&#21644;&#22270;&#28385;&#36275;&#29305;&#27530;&#30340;&#32479;&#35745;&#20551;&#35774;&#65292;&#22914;&#30456;&#20114;&#29420;&#31435;&#12289;&#26102;&#31354;&#29420;&#31435;&#25110;&#24179;&#31283;&#24615;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#38750;&#36127;&#36229;-&#38789;&#19981;&#31561;&#24335;&#30340;&#20272;&#35745;&#35823;&#24046;&#65292;&#24182;&#35777;&#26126;&#20102;&#22914;&#26524;&#31639;&#27861;&#22686;&#30410;&#12289;&#22270;&#21644;&#22238;&#24402;&#30697;&#38453;&#20849;&#21516;&#28385;&#36275;&#26679;&#26412;&#36335;&#24452;&#26102;&#31354;&#20852;&#22859;&#26465;&#20214;&#65292;&#33410;&#28857;&#30340;&#20272;&#35745;&#20960;&#20046;&#21487;&#20197;&#32943;&#23450;&#22320;&#25910;&#25947;&#20110;&#26410;&#30693;&#30340;&#30495;&#23454;&#21442;&#25968;&#21521;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#31639;&#27861;&#22686;&#30410;&#65292;&#35813;&#26465;&#20214;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the decentralized online regularized linear regression algorithm over random time-varying graphs. At each time step, every node runs an online estimation algorithm consisting of an innovation term processing its own new measurement, a consensus term taking a weighted sum of estimations of its own and its neighbors with additive and multiplicative communication noises and a regularization term preventing over-fitting. It is not required that the regression matrices and graphs satisfy special statistical assumptions such as mutual independence, spatio-temporal independence or stationarity. We develop the nonnegative supermartingale inequality of the estimation error, and prove that the estimations of all nodes converge to the unknown true parameter vector almost surely if the algorithm gains, graphs and regression matrices jointly satisfy the sample path spatio-temporal persistence of excitation condition. Especially, this condition holds by choosing appropriate algorithm gains 
&lt;/p&gt;</description></item></channel></rss>