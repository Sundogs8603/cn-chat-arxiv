<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>T-Dex&#26159;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#30340;&#28789;&#24039;&#24615;&#26041;&#27861;&#65292;&#21487;&#22312;&#33258;&#25105;&#30417;&#30563;&#24335;&#30340;&#35302;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20123;&#28789;&#24039;&#24615;&#20219;&#21153;&#28436;&#31034;&#30340;&#25351;&#23548;&#19979;&#65292;&#23558;&#35302;&#35273;&#21644;&#35270;&#35273;&#32467;&#21512;&#36215;&#26469;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#32431;&#35270;&#35273;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12076</link><description>&lt;p&gt;
&#35302;&#35273;&#35757;&#32451;&#19979;&#30340;&#26426;&#22120;&#20154;&#28789;&#24039;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play. (arXiv:2303.12076v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12076
&lt;/p&gt;
&lt;p&gt;
T-Dex&#26159;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#30340;&#28789;&#24039;&#24615;&#26041;&#27861;&#65292;&#21487;&#22312;&#33258;&#25105;&#30417;&#30563;&#24335;&#30340;&#35302;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20123;&#28789;&#24039;&#24615;&#20219;&#21153;&#28436;&#31034;&#30340;&#25351;&#23548;&#19979;&#65292;&#23558;&#35302;&#35273;&#21644;&#35270;&#35273;&#32467;&#21512;&#36215;&#26469;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#32431;&#35270;&#35273;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#35753;&#20855;&#26377;&#22810;&#25351;&#30340;&#26426;&#22120;&#20154;&#20855;&#26377;&#28789;&#24039;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#27492;&#21069;&#65292;&#26368;&#37325;&#35201;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#23398;&#20064;&#25511;&#21046;&#22120;&#25110;&#31574;&#30053;&#19978;&#65292;&#36825;&#20123;&#25511;&#21046;&#22120;&#25110;&#31574;&#30053;&#35201;&#20040;&#22522;&#20110;&#35270;&#35273;&#35266;&#27979;&#65292;&#35201;&#20040;&#22522;&#20110;&#20174;&#35270;&#35273;&#25512;&#26029;&#24471;&#21040;&#30340;&#29366;&#24577;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#38656;&#35201;&#25512;&#29702;&#25509;&#35302;&#21147;&#25110;&#36890;&#36807;&#25163;&#26412;&#36523;&#36974;&#25377;&#30340;&#29289;&#20307;&#30340;&#32454;&#31890;&#24230;&#25805;&#20316;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35302;&#35273;&#30340;&#28789;&#24039;&#24615;&#26041;&#27861;T-Dex&#65292;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25910;&#38598;2.5&#23567;&#26102;&#30340;&#28216;&#25103;&#25968;&#25454;&#24182;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#33258;&#25105;&#30417;&#30563;&#22411;&#35302;&#35273;&#32534;&#30721;&#22120;&#65307;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#23569;&#37327;&#30340;&#28789;&#24039;&#24615;&#20219;&#21153;&#28436;&#31034;&#65292;&#23398;&#20064;&#23558;&#35302;&#35273;&#35266;&#27979;&#21644;&#35270;&#35273;&#35266;&#27979;&#30456;&#32467;&#21512;&#30340;&#38750;&#21442;&#25968;&#21270;&#31574;&#30053;&#12290;&#36890;&#36807;&#20116;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#28789;&#24039;&#24615;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#35302;&#35273;&#30340;&#28789;&#24039;&#24615;&#27169;&#22411;&#27604;&#32431;&#35270;&#35273;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#24191;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#35270;&#35273;&#26041;&#27861;&#22833;&#36133;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teaching dexterity to multi-fingered robots has been a longstanding challenge in robotics. Most prominent work in this area focuses on learning controllers or policies that either operate on visual observations or state estimates derived from vision. However, such methods perform poorly on fine-grained manipulation tasks that require reasoning about contact forces or about objects occluded by the hand itself. In this work, we present T-Dex, a new approach for tactile-based dexterity, that operates in two phases. In the first phase, we collect 2.5 hours of play data, which is used to train self-supervised tactile encoders. This is necessary to bring high-dimensional tactile readings to a lower-dimensional embedding. In the second phase, given a handful of demonstrations for a dexterous task, we learn non-parametric policies that combine the tactile observations with visual ones. Across five challenging dexterous tasks, we show that our tactile-based dexterity models outperform purely vi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#38382;&#39064;&#30340;&#22238;&#31572;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#30340;&#26426;&#22120;&#32763;&#35793;&#35780;&#27979;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#28982;&#33853;&#21518;&#20110;&#20856;&#22411;&#20154;&#31867;&#30340;&#21453;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12038</link><description>&lt;p&gt;
&#35780;&#20272;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23545;&#35805;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Grading Conversational Responses Of Chatbots. (arXiv:2303.12038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#38382;&#39064;&#30340;&#22238;&#31572;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#26631;&#20934;&#30340;&#26426;&#22120;&#32763;&#35793;&#35780;&#27979;&#25351;&#26631;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#30340;&#34920;&#29616;&#20173;&#28982;&#33853;&#21518;&#20110;&#20856;&#22411;&#20154;&#31867;&#30340;&#21453;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#22238;&#31572;&#22522;&#26412;&#38382;&#39064;&#65292;&#29978;&#33267;&#22238;&#24212;&#22855;&#24618;&#30340;&#25552;&#31034;&#65292;&#20294;&#26368;&#36817;&#23427;&#20204;&#30340;&#25913;&#36827;&#26174;&#33879;&#25552;&#39640;&#12290;&#20687;OpenAI ChatGPT3&#36825;&#26679;&#30340;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#19981;&#20165;&#33021;&#22815;&#22238;&#31572;&#22522;&#26412;&#38382;&#39064;&#65292;&#36824;&#33021;&#32534;&#20889;&#20195;&#30721;&#12289;&#30005;&#24433;&#21095;&#26412;&#24182;&#27169;&#20223;&#30693;&#21517;&#20154;&#29289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ChatGPT&#23545;&#20174;&#27969;&#34892;Quora&#35770;&#22363;&#30340;&#25968;&#25454;&#38598;&#20013;&#25552;&#20986;&#30340;&#21508;&#31181;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#21521;ChatGPT&#25552;&#20132;&#20102;60&#20010;&#38382;&#39064;&#65292;&#24182;&#26681;&#25454;&#19977;&#20010;&#34892;&#19994;&#26631;&#20934;&#35780;&#20998;&#26426;&#22120;&#32763;&#35793;&#30340;&#24230;&#37327;&#26631;&#20934;(BLEU&#12289;METEOR&#21644;ROUGE)&#23545;&#31572;&#26696;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#20801;&#35768;&#25105;&#20204;&#23558;&#26426;&#22120;&#30340;&#22238;&#22797;&#19982;&#21516;&#19968;&#38382;&#39064;&#30340;&#26368;&#22810;&#36190;&#21516;&#30340;&#20154;&#31867;&#31572;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35780;&#20272;ChatGPT&#25552;&#20132;&#20154;&#24615;&#21270;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;ChatGPT&#30340;&#21709;&#24212;&#21644;&#32763;&#35793;&#33021;&#21147;&#38750;&#24120;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#36798;&#19981;&#21040;&#20856;&#22411;&#20154;&#31867;&#21453;&#24212;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots have long been capable of answering basic questions and even responding to obscure prompts, but recently their improvements have been far more significant. Modern chatbots like Open AIs ChatGPT3 not only have the ability to answer basic questions but can write code and movie scripts and imitate well-known people. In this paper, we analyze ChatGPTs' responses to various questions from a dataset of queries from the popular Quora forum. We submitted sixty questions to ChatGPT and scored the answers based on three industry-standard metrics for grading machine translation: BLEU, METEOR, and ROUGE. These metrics allow us to compare the machine responses with the most upvoted human answer to the same question to assess ChatGPT's ability to submit a humanistic reply. The results showed that while the responses and translation abilities of ChatGPT are remarkable, they still fall short of what a typical human reaction would be.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#28548;&#28165;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#20204;&#26356;&#36866;&#21512;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#26377;&#30528;&#30452;&#25509;&#24433;&#21709;&#65292;&#20063;&#24341;&#36215;&#20102;&#21746;&#23398;&#23478;&#23545;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2303.12032</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Representational Status of Deep Learning Models. (arXiv:2303.12032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#28548;&#28165;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#20204;&#26356;&#36866;&#21512;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#26377;&#30528;&#30452;&#25509;&#24433;&#21709;&#65292;&#20063;&#24341;&#36215;&#20102;&#21746;&#23398;&#23478;&#23545;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#28548;&#28165;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;DLMs&#65289;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#30001;&#20110;&#21151;&#33021;&#21644;&#20851;&#31995;&#27010;&#24565;&#30340;&#28151;&#28102;&#65292;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#36825;&#24847;&#21619;&#30528;&#21547;&#31946;&#19981;&#28165;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#34429;&#28982;DLM&#20197;&#20851;&#31995;&#24847;&#20041;&#19978;&#30340;&#34920;&#24449;&#20854;&#30446;&#26631;&#65292;&#20294;&#26368;&#22909;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#26377;&#30452;&#25509;&#24433;&#21709;&#65292;&#24182;&#24341;&#23548;&#21746;&#23398;&#20851;&#27880;DLM&#34920;&#24449;&#30340;&#29702;&#24819;&#21270;&#24615;&#36136;&#21450;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to clarify the representational status of Deep Learning Models (DLMs). While commonly referred to as 'representations', what this entails is ambiguous due to a conflation of functional and relational conceptions of representation. This paper argues that while DLMs represent their targets in a relational sense, they are best understood as highly idealized models. This result has immediate implications for explainable AI (XAI) and directs philosophical attention toward examining the idealized nature of DLM representations and their role in future scientific investigation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#22238;&#24402;&#36827;&#34892;&#26894;&#20307;&#39592;&#25240;&#20998;&#32423;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#22238;&#24402;&#26469;&#21453;&#26144;&#39592;&#25240;&#30340;&#24179;&#31283;&#36827;&#23637;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.12031</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#22238;&#24402;&#36827;&#34892;&#26894;&#20307;&#39592;&#25240;&#20998;&#32423;
&lt;/p&gt;
&lt;p&gt;
Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral Fracture Grading. (arXiv:2303.12031v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#28508;&#31354;&#38388;&#22238;&#24402;&#36827;&#34892;&#26894;&#20307;&#39592;&#25240;&#20998;&#32423;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#22238;&#24402;&#26469;&#21453;&#26144;&#39592;&#25240;&#30340;&#24179;&#31283;&#36827;&#23637;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26894;&#20307;&#39592;&#25240;&#26159;&#39592;&#36136;&#30095;&#26494;&#30151;&#30340;&#21518;&#26524;&#65292;&#23545;&#24739;&#32773;&#30340;&#20581;&#24247;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992; CT &#26816;&#26597;&#23545;&#20854;&#36827;&#34892;&#20998;&#32423;&#26159;&#22256;&#38590;&#21644;&#20027;&#35266;&#30340;&#65292;&#36825;&#20419;&#20351;&#30528;&#33258;&#21160;&#21270;&#20998;&#32423;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#21463;&#21040;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#31232;&#32570;&#24615;&#20197;&#21450;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#30340;&#21046;&#32422;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#29983;&#25104;&#24335;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120; (DAE) &#27169;&#22411;&#20316;&#20026;&#26080;&#30417;&#30563;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#25105;&#20204;&#23558;&#39592;&#25240;&#20998;&#32423;&#24314;&#27169;&#20026;&#36830;&#32493;&#22238;&#24402;&#65292;&#36825;&#26356;&#33021;&#21453;&#26144;&#39592;&#25240;&#30340;&#24179;&#31283;&#36827;&#23637;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#20108;&#20803;&#26377;&#30417;&#30563;&#30340;&#39592;&#25240;&#20998;&#31867;&#22120;&#26500;&#24314; DAE &#30340;&#28508;&#31354;&#38388;&#20013;&#30340;&#36229;&#24179;&#38754;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#39592;&#25240;&#30340;&#20005;&#37325;&#31243;&#24230;&#22238;&#24402;&#20026;&#36317;&#31163;&#36229;&#24179;&#38754;&#30340;&#20989;&#25968;&#65292;&#23545;&#32467;&#26524;&#36827;&#34892; Genant &#26631;&#24230;&#30340;&#26657;&#20934;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#29983;&#25104;&#26412;&#36136;&#20801;&#35768;&#25105;&#20204;&#35270;&#35273;&#21270;&#32473;&#23450;&#39592;&#25240;&#30340;&#19981;&#21516;&#32423;&#21035;&#65292;&#24182;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#26469;&#22686;&#24378;&#35757;&#32451;&#38598;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#33136;&#26894; CT &#25195;&#25551;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#25105;&#20204;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertebral fractures are a consequence of osteoporosis, with significant health implications for affected patients. Unfortunately, grading their severity using CT exams is hard and subjective, motivating automated grading methods. However, current approaches are hindered by imbalance and scarcity of data and a lack of interpretability. To address these challenges, this paper proposes a novel approach that leverages unlabelled data to train a generative Diffusion Autoencoder (DAE) model as an unsupervised feature extractor. We model fracture grading as a continuous regression, which is more reflective of the smooth progression of fractures. Specifically, we use a binary, supervised fracture classifier to construct a hyperplane in the DAE's latent space. We then regress the severity of the fracture as a function of the distance to this hyperplane, calibrating the results to the Genant scale. Importantly, the generative nature of our method allows us to visualize different grades of a give
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#21345;&#23572;&#26364;&#21644;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#25512;&#24191;&#21040;&#22270;&#24418;&#19978;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#36866;&#29992;&#20110;&#36755;&#20986;&#26159;&#21521;&#37327;&#25110;&#26631;&#37327;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#26410;&#30693;&#30340;&#29366;&#24577;&#36716;&#31227;&#21644;&#35835;&#21462;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.12021</link><description>&lt;p&gt;
&#22270;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Graph Kalman Filters. (arXiv:2303.12021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#21345;&#23572;&#26364;&#21644;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#25512;&#24191;&#21040;&#22270;&#24418;&#19978;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#36866;&#29992;&#20110;&#36755;&#20986;&#26159;&#21521;&#37327;&#25110;&#26631;&#37327;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#26410;&#30693;&#30340;&#29366;&#24577;&#36716;&#31227;&#21644;&#35835;&#21462;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#36890;&#36807;&#20351;&#29992;&#29366;&#24577;&#31354;&#38388;&#34920;&#31034;&#26469;&#27169;&#25311;&#21160;&#24577;&#31995;&#32479;&#65292;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340;&#26356;&#26032;&#20197;&#21450;&#19982;&#26032;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#36755;&#20986;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#25511;&#21046;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#21345;&#23572;&#26364;&#21644;&#25193;&#23637;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#25512;&#24191;&#21040;&#31163;&#25955;&#26102;&#38388;&#30340;&#35774;&#32622;&#19979;&#65292;&#20854;&#20013;&#36755;&#20837;&#12289;&#29366;&#24577;&#21644;&#36755;&#20986;&#22343;&#34920;&#31034;&#20026;&#24102;&#23646;&#24615;&#30340;&#22270;&#24418;&#65292;&#20854;&#25299;&#25169;&#21644;&#23646;&#24615;&#21487;&#20197;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#35774;&#32622;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#23558;&#26694;&#26550;&#36866;&#24212;&#20110;&#36755;&#20986;&#26159;&#21521;&#37327;&#25110;&#26631;&#37327;&#30340;&#24773;&#20917;&#65288;&#33410;&#28857;/&#22270;&#32423;&#20219;&#21153;&#65289;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#20869;&#65292;&#26410;&#30693;&#30340;&#29366;&#24577;&#36716;&#31227;&#21644;&#35835;&#21462;&#20989;&#25968;&#19982;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#19968;&#36215;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The well-known Kalman filters model dynamical systems by relying on state-space representations with the next state updated, and its uncertainty controlled, by fresh information associated with newly observed system outputs. This paper generalizes, for the first time in the literature, Kalman and extended Kalman filters to discrete-time settings where inputs, states, and outputs are represented as attributed graphs whose topology and attributes can change with time. The setup allows us to adapt the framework to cases where the output is a vector or a scalar too (node/graph level tasks). Within the proposed theoretical framework, the unknown state-transition and the readout functions are learned end-to-end along with the downstream prediction task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20302;&#24310;&#36831;&#27969;&#24335;&#20998;&#31163;&#24212;&#29992;&#20013;&#30340;&#22522;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#21457;&#35328;&#32773;&#20998;&#31163;&#65288;SSGD&#65289;&#22312;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#65288;CTS&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20998;&#31163;&#35828;&#35805;&#20154;&#24182;&#22312;&#27599;&#20010;&#20998;&#31163;&#30340;&#27969;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#26469;&#25191;&#34892;&#21457;&#35328;&#32773;&#20998;&#31163;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#12289;&#22240;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27844;&#28431;&#21435;&#38500;&#31639;&#27861;&#12290;&#22312;CALLHOME&#21644;Fisher&#35821;&#26009;&#24211;&#65288;&#31532;1&#21644;2&#37096;&#20998;&#65289;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;SSGD&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20998;&#31163;&#21644;&#21457;&#35328;&#32773;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12002</link><description>&lt;p&gt;
&#30005;&#35805;&#20250;&#35805;&#30340;&#20302;&#24310;&#36831;&#21457;&#35328;&#20998;&#31163;&#21644;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
End-to-End Integration of Speech Separation and Voice Activity Detection for Low-Latency Diarization of Telephone Conversations. (arXiv:2303.12002v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20302;&#24310;&#36831;&#27969;&#24335;&#20998;&#31163;&#24212;&#29992;&#20013;&#30340;&#22522;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#21457;&#35328;&#32773;&#20998;&#31163;&#65288;SSGD&#65289;&#22312;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#65288;CTS&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20998;&#31163;&#35828;&#35805;&#20154;&#24182;&#22312;&#27599;&#20010;&#20998;&#31163;&#30340;&#27969;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#26469;&#25191;&#34892;&#21457;&#35328;&#32773;&#20998;&#31163;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#12289;&#22240;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27844;&#28431;&#21435;&#38500;&#31639;&#27861;&#12290;&#22312;CALLHOME&#21644;Fisher&#35821;&#26009;&#24211;&#65288;&#31532;1&#21644;2&#37096;&#20998;&#65289;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;SSGD&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20998;&#31163;&#21644;&#21457;&#35328;&#32773;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#21457;&#35328;&#32773;&#20998;&#31163;&#65288;SSGD&#65289;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#36825;&#20027;&#35201;&#24471;&#30410;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23427;&#36890;&#36807;&#39318;&#20808;&#20998;&#31163;&#35828;&#35805;&#20154;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#20998;&#31163;&#30340;&#27969;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#26469;&#25191;&#34892;&#21457;&#35328;&#32773;&#20998;&#31163;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#65288;CTS&#65289;&#39046;&#22495;&#20013;&#30340;SSGD&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#20302;&#24310;&#36831;&#27969;&#24335;&#20998;&#31163;&#24212;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#20998;&#31163;&#65288;SSep&#65289;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#65292;&#32771;&#34385;&#20102;&#38750;&#22240;&#26524;&#21644;&#22240;&#26524;&#23454;&#29616;&#20197;&#21450;&#36830;&#32493;SSep&#65288;CSS&#65289;&#31383;&#21475;&#25512;&#29702;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;SSGD&#31639;&#27861;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;CTS&#25968;&#25454;&#38598;CALLHOME&#21644;Fisher&#35821;&#26009;&#24211;&#65288;&#31532;1&#21644;2&#37096;&#20998;&#65289;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#20998;&#31163;&#21644;&#21457;&#35328;&#32773;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#12289;&#22240;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27844;&#28431;&#21435;&#38500;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works show that speech separation guided diarization (SSGD) is an increasingly promising direction, mainly thanks to the recent progress in speech separation. It performs diarization by first separating the speakers and then applying voice activity detection (VAD) on each separated stream. In this work we conduct an in-depth study of SSGD in the conversational telephone speech (CTS) domain, focusing mainly on low-latency streaming diarization applications. We consider three state-of-the-art speech separation (SSep) algorithms and study their performance both in online and offline scenarios, considering non-causal and causal implementations as well as continuous SSep (CSS) windowed inference. We compare different SSGD algorithms on two widely used CTS datasets: CALLHOME and Fisher Corpus (Part 1 and 2) and evaluate both separation and diarization performance. To improve performance, a novel, causal and computationally efficient leakage removal algorithm is proposed, which signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#25193;&#23637;&#30340;&#28145;&#24230;&#20986;&#34892;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22522;&#20110;&#22810;&#28304;&#22478;&#24066;&#24314;&#31569;&#21644;&#22320;&#29702;&#25968;&#25454;&#30340;&#31449;&#28857;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.11977</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#25193;&#23637;&#30340;&#28145;&#24230;&#20986;&#34892;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Deep trip generation with graph neural networks for bike sharing system expansion. (arXiv:2303.11977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#25193;&#23637;&#30340;&#28145;&#24230;&#20986;&#34892;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22522;&#20110;&#22810;&#28304;&#22478;&#24066;&#24314;&#31569;&#21644;&#22320;&#29702;&#25968;&#25454;&#30340;&#31449;&#28857;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#20139;&#21333;&#36710;&#27491;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20316;&#20026;&#19968;&#31181;&#27963;&#36291;&#65292;&#26041;&#20415;&#21644;&#21487;&#25345;&#32493;&#30340;&#20132;&#36890;&#26041;&#24335;&#32780;&#20852;&#36215;&#12290;&#20026;&#20102;&#35745;&#21010;&#25104;&#21151;&#30340;&#20849;&#20139;&#21333;&#36710;&#31995;&#32479;&#65288;BSS&#65289;&#65292;&#35768;&#22810;&#22478;&#24066;&#20174;&#23567;&#35268;&#27169;&#35797;&#28857;&#24320;&#22987;&#65292;&#24182;&#36880;&#27493;&#25193;&#22823;&#31995;&#32479;&#35206;&#30422;&#26356;&#22810;&#21306;&#22495;&#12290;&#23545;&#20110;&#22522;&#20110;&#31449;&#28857;&#30340;BSS&#65292;&#36825;&#24847;&#21619;&#30528;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#22522;&#20110;&#29616;&#26377;&#31449;&#28857;&#35268;&#21010;&#26032;&#31449;&#28857;&#65292;&#36825;&#38656;&#35201;&#39044;&#27979;&#25972;&#20010;&#31995;&#32479;&#20013;&#36825;&#20123;&#26032;&#31449;&#28857;&#20135;&#29983;&#30340;&#26053;&#34892;&#27425;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#24402;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#25429;&#25417;&#22797;&#26434;&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#12290;&#23613;&#31649;&#22312;&#26053;&#34892;&#38656;&#27714;&#39044;&#27979;&#26041;&#38754;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#25991;&#29486;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#26159;&#22522;&#20110;&#26102;&#24207;&#25968;&#25454;&#30340;&#30701;&#26399;&#39044;&#27979;&#65292;&#20551;&#35774;&#31995;&#32479;&#27809;&#26377;&#32467;&#26500;&#24615;&#21464;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;BSS&#25193;&#23637;&#30340;&#20986;&#34892;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22522;&#20110;&#22810;&#28304;&#22478;&#24066;&#24314;&#31569;&#21644;&#22320;&#29702;&#25968;&#25454;&#30340;&#31449;&#28857;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bike sharing is emerging globally as an active, convenient, and sustainable mode of transportation. To plan successful bike-sharing systems (BSSs), many cities start from a small-scale pilot and gradually expand the system to cover more areas. For station-based BSSs, this means planning new stations based on existing ones over time, which requires prediction of the number of trips generated by these new stations across the whole system. Previous studies typically rely on relatively simple regression or machine learning models, which are limited in capturing complex spatial relationships. Despite the growing literature in deep learning methods for travel demand prediction, they are mostly developed for short-term prediction based on time series data, assuming no structural changes to the system. In this study, we focus on the trip generation problem for BSS expansion, and propose a graph neural network (GNN) approach to predicting the station-level demand based on multi-source urban bui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28041;&#21450;&#25512;&#21160;&#12289;&#31034;&#20363;&#21644;&#25552;&#20986;&#19977;&#31181;&#24178;&#39044;&#26041;&#27861;&#25945;&#25480;&#40664;&#35748;&#23398;&#29983;&#20309;&#26102;&#22914;&#20309;&#20351;&#29992;&#21738;&#31181;&#31574;&#30053;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;Nudge&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.11965</link><description>&lt;p&gt;
&#8220;&#25512;&#21160;&#21147;&#8221;&#65306;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#20803;&#35748;&#30693;&#25216;&#33021;&#25945;&#23398;&#30340;&#19977;&#31181;&#24178;&#39044;&#26041;&#27861;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Power of Nudging: Exploring Three Interventions for Metacognitive Skills Instruction across Intelligent Tutoring Systems. (arXiv:2303.11965v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28041;&#21450;&#25512;&#21160;&#12289;&#31034;&#20363;&#21644;&#25552;&#20986;&#19977;&#31181;&#24178;&#39044;&#26041;&#27861;&#25945;&#25480;&#40664;&#35748;&#23398;&#29983;&#20309;&#26102;&#22914;&#20309;&#20351;&#29992;&#21738;&#31181;&#31574;&#30053;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;Nudge&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35777;&#22495;&#36890;&#24120;&#20855;&#26377;&#24456;&#22810;&#35748;&#30693;&#25216;&#33021;&#65292;&#20854;&#20013;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#35299;&#20915;&#31574;&#30053;&#21487;&#36866;&#29992;&#20110;&#35299;&#20915;&#25152;&#26377;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30693;&#36947;&#20309;&#26102;&#22914;&#20309;&#20351;&#29992;&#27599;&#31181;&#31574;&#30053;&#30340;&#23398;&#29983;&#65288;StrTime&#65289;&#34920;&#29616;&#20248;&#20110;&#37027;&#20123;&#35841;&#19981;&#30693;&#36947;&#24182;&#22362;&#25345;&#40664;&#35748;&#31574;&#30053;&#65288;Default&#65289;&#30340;&#23398;&#29983;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#23398;&#29983;&#22312;&#36923;&#36753;&#36741;&#23548;&#21592;&#30340;&#35757;&#32451;&#20013;&#20351;&#29992;&#40664;&#35748;&#30340;&#21069;&#21521;&#38142;&#25509;&#21644;&#21518;&#21521;&#38142;&#25509;&#65288;BC&#65289;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#27010;&#29575;&#36741;&#23548;&#21592;&#20013;&#21482;&#20351;&#29992;BC&#25903;&#25345;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#22312;&#36923;&#36753;&#36741;&#23548;&#21592;&#19978;&#25945;&#25480;&#40664;&#35748;&#23398;&#29983;&#20309;&#26102;&#22914;&#20309;&#20351;&#29992;&#21738;&#31181;&#31574;&#30053;&#30340;&#24178;&#39044;&#25514;&#26045;&#65306;&#31034;&#20363;&#12289;&#25512;&#21160;&#21644;&#25552;&#20986;&#12290;&#21516;&#26102;&#65292;StrTime&#23398;&#29983;&#27809;&#26377;&#25509;&#21463;&#20219;&#20309;&#24178;&#39044;&#25514;&#26045;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Nudge&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;Default&#21516;&#20276;&#65292;&#24182;&#22312;&#20004;&#20010;&#36741;&#23548;&#21592;&#19978;&#36861;&#36214;StrTime&#12290;
&lt;/p&gt;
&lt;p&gt;
Deductive domains are typical of many cognitive skills in that no single problem-solving strategy is always optimal for solving all problems. It was shown that students who know how and when to use each strategy (StrTime) outperformed those who know neither and stick to the default strategy (Default). In this work, students were trained on a logic tutor that supports a default forward-chaining and a backward-chaining (BC) strategy, then a probability tutor that only supports BC. We investigated three types of interventions on teaching the Default students how and when to use which strategy on the logic tutor: Example, Nudge and Presented. Meanwhile, StrTime students received no interventions. Overall, our results show that Nudge outperformed their Default peers and caught up with StrTime on both tutors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;CPPI&#21644;TIPP&#31574;&#30053;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29305;&#23450;&#35774;&#35745;&#30340;MARL&#26041;&#27861;CPPI-MADDPG&#21644;TIPP-MADDPG&#65292;&#29992;&#20110;&#37327;&#21270;&#24066;&#22330;&#30340;&#25112;&#30053;&#20132;&#26131;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11959</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#37327;&#21270;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#20132;&#26131;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Strategic Trading in Quantitative Markets through Multi-Agent Reinforcement Learning. (arXiv:2303.11959v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;CPPI&#21644;TIPP&#31574;&#30053;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#29305;&#23450;&#35774;&#35745;&#30340;MARL&#26041;&#27861;CPPI-MADDPG&#21644;TIPP-MADDPG&#65292;&#29992;&#20110;&#37327;&#21270;&#24066;&#22330;&#30340;&#25112;&#30053;&#20132;&#26131;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#24066;&#22330;&#20013;&#65292;&#30001;&#20110;&#24066;&#22330;&#21160;&#24577;&#24555;&#36895;&#21464;&#21270;&#21644;&#22823;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22914;&#20309;&#37319;&#21462;&#36866;&#24403;&#30340;&#34892;&#21160;&#21033;&#28070;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#38754;&#21521;&#22870;&#21169;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#22797;&#26434;&#30340;&#37329;&#34701;&#22330;&#26223;&#20013;&#24050;&#25104;&#20026;&#35299;&#20915;&#31574;&#30053;&#20915;&#31574;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#23558;&#20004;&#31181;&#20808;&#21069;&#30340;&#37329;&#34701;&#20132;&#26131;&#31574;&#30053;&#65288;&#24658;&#23450;&#27604;&#20363;&#32452;&#21512;&#20445;&#38505;&#65288;CPPI&#65289;&#21644;&#26102;&#38388;&#19981;&#21464;&#32452;&#21512;&#20445;&#25252;&#65288;TIPP&#65289;&#65289;&#38598;&#25104;&#21040;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;MADDPG&#65289;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#29305;&#21035;&#35774;&#35745;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65306;CPPI-MADDPG&#21644;TIPP-MADDPG&#30740;&#31350;&#37327;&#21270;&#24066;&#22330;&#20013;&#30340;&#25112;&#30053;&#20132;&#26131;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#23454;&#38469;&#37329;&#34701;&#24066;&#22330;&#19978;&#30340;100&#31181;&#19981;&#21516;&#32929;&#31080;&#26469;&#27979;&#35797;&#36825;&#20123;&#29305;&#21035;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CPPI-MADDPG&#21644;TIPP-MADDPG&#26041;&#27861;&#36890;&#24120;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the rapid dynamics and a mass of uncertainties in the quantitative markets, the issue of how to take appropriate actions to make profits in stock trading remains a challenging one. Reinforcement learning (RL), as a reward-oriented approach for optimal control, has emerged as a promising method to tackle this strategic decision-making problem in such a complex financial scenario. In this paper, we integrated two prior financial trading strategies named constant proportion portfolio insurance (CPPI) and time-invariant portfolio protection (TIPP) into multi-agent deep deterministic policy gradient (MADDPG) and proposed two specifically designed multi-agent RL (MARL) methods: CPPI-MADDPG and TIPP-MADDPG for investigating strategic trading in quantitative markets. Afterward, we selected 100 different shares in the real financial market to test these specifically proposed approaches. The experiment results show that CPPI-MADDPG and TIPP-MADDPG approaches generally outperform the conve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#24179;&#28369;&#22120;&#30340;&#26368;&#20339;&#21152;&#26435;&#31383;&#21475;&#65292;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#21407;&#28857;&#22312;&#20984;&#22810;&#38754;&#20307;&#19978;&#30340;&#25237;&#24433;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.11958</link><description>&lt;p&gt;
&#21152;&#26435;&#24179;&#28369;&#22312;&#20984;&#22810;&#38754;&#20307;&#19978;&#30340;&#25237;&#24433;&#24335;
&lt;/p&gt;
&lt;p&gt;
Formulation of Weighted Average Smoothing as a Projection of the Origin onto a Convex Polytope. (arXiv:2303.11958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#24179;&#28369;&#22120;&#30340;&#26368;&#20339;&#21152;&#26435;&#31383;&#21475;&#65292;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#21407;&#28857;&#22312;&#20984;&#22810;&#38754;&#20307;&#19978;&#30340;&#25237;&#24433;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#24179;&#28369;&#22120;&#30340;&#26368;&#20339;&#21152;&#26435;&#31383;&#21475;&#12290;&#25105;&#20204;&#35777;&#26126;&#23384;&#22312;&#19968;&#20010;&#26368;&#20248;&#30340;&#23545;&#31216;&#21152;&#26435;&#31383;&#21475;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36880;&#27493;&#20943;&#24369;&#30340;&#21152;&#26435;&#31383;&#21475;&#65292;&#23427;&#20204;&#30340;&#26435;&#37325;&#38543;&#30528;&#36828;&#31163;&#20013;&#24515;&#32780;&#20943;&#23569;&#12290;&#25105;&#20204;&#23558;&#30456;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#19968;&#20010;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;&#26368;&#32456;&#23558;&#20854;&#36716;&#21270;&#20026;&#21407;&#28857;&#22312;&#20984;&#22810;&#38754;&#20307;&#19978;&#30340;&#25237;&#24433;&#12290;&#27492;&#22806;&#65292;&#24403;&#36755;&#20837;&#25968;&#25454;&#28385;&#36275;&#26576;&#20123;&#26465;&#20214;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26368;&#20339;&#31383;&#21475;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study focuses on determining the best weight windows for a weighted moving average smoother under squared loss. We show that there exists an optimal weight window that is symmetrical around its center. We study the class of tapered weight windows, which decrease in weight as they move away from the center. We formulate the corresponding least squares problem as a quadratic program and finally as a projection of the origin onto a convex polytope. Additionally, we provide some analytical solutions to the best window when some conditions are met on the input data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20989;&#25968;&#32452;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#40657;&#30418;&#20989;&#25968;&#38590;&#20197;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21160;&#24577;&#23450;&#20215;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.11954</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20989;&#25968;&#32452;&#21512;&#26041;&#27861;&#21450;&#20854;&#22312;&#21160;&#24577;&#23450;&#20215;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization for Function Compositions with Applications to Dynamic Pricing. (arXiv:2303.11954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20989;&#25968;&#32452;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#40657;&#30418;&#20989;&#25968;&#38590;&#20197;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21160;&#24577;&#23450;&#20215;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#34987;&#29992;&#26469;&#25214;&#21040;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20989;&#25968;&#32452;&#21512;&#30340;&#23454;&#29992;BO&#26041;&#27861;&#65292;&#20854;&#20013;&#32452;&#21512;&#30340;&#24418;&#24335;&#24050;&#30693;&#65292;&#20294;&#21508;&#32452;&#25104;&#20989;&#25968;&#38590;&#20197;&#35780;&#20272;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#40657;&#30418;&#20989;&#25968;&#24314;&#31435;&#29420;&#31435;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;EI&#21644;UCB&#22522;&#20110;BO&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#32988;&#36807;&#20256;&#32479;BO&#21644;&#30446;&#21069;&#20808;&#36827;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25910;&#30410;&#31649;&#29702;&#20013;&#21160;&#24577;&#23450;&#20215;&#26102;&#30340;&#19968;&#31181;&#26032;&#39062;&#24212;&#29992;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26114;&#36149;&#30340;&#38656;&#27714;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is used to find the global optima of black box functions. In this work, we propose a practical BO method of function compositions where the form of the composition is known but the constituent functions are expensive to evaluate. By assuming an independent Gaussian process (GP) model for each of the constituent black-box function, we propose EI and UCB based BO algorithms and demonstrate their ability to outperform vanilla BO and the current state-of-art algorithms. We demonstrate a novel application of the proposed methods to dynamic pricing in revenue management when the underlying demand function is expensive to evaluate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;EdgeHML&#30340;&#20302;&#25104;&#26412;&#21322;&#30417;&#30563;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#23384;&#20648;&#27744;&#21644;&#22810;&#32423;&#23384;&#20648;&#32467;&#26500;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#26679;&#26412;&#21644;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65292;&#26377;&#25928;&#24212;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36793;&#32536;&#26234;&#33021;&#39046;&#22495;&#30340;&#36164;&#28304;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11952</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#23384;&#20648;&#27744;&#30340;&#36793;&#32536;&#21322;&#30417;&#30563;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Memory Pool Based Edge Semi-Supervised Continual Learning Method. (arXiv:2303.11952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;EdgeHML&#30340;&#20302;&#25104;&#26412;&#21322;&#30417;&#30563;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#23384;&#20648;&#27744;&#21644;&#22810;&#32423;&#23384;&#20648;&#32467;&#26500;&#65292;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#26679;&#26412;&#21644;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65292;&#26377;&#25928;&#24212;&#23545;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36793;&#32536;&#26234;&#33021;&#39046;&#22495;&#30340;&#36164;&#28304;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#19990;&#30028;&#30340;&#19981;&#26029;&#21464;&#21270;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#22686;&#37327;&#23398;&#20064;&#39046;&#22495;&#36880;&#28176;&#24341;&#36215;&#26356;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#23545;&#20110;&#36793;&#32536;&#26234;&#33021;&#32780;&#35328;&#65292;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#19981;&#20165;&#38656;&#35201;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36824;&#38656;&#35201;&#24212;&#23545;&#20005;&#37325;&#30340;&#36164;&#28304;&#38480;&#21046;&#65306;&#32570;&#20047;&#26631;&#35760;&#36164;&#28304;&#21644;&#24378;&#22823;&#30340;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32463;&#20856;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#26469;&#32500;&#25252;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#32780;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20184;&#20986;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#24320;&#38144;&#26469;&#25552;&#39640;&#31934;&#24230;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36793;&#32536;&#20998;&#23618;&#23384;&#20648;&#23398;&#20064;&#22120;&#65288;EdgeHML&#65289;&#30340;&#20302;&#25104;&#26412;&#21322;&#30417;&#30563;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#26679;&#26412;&#21644;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65292;&#22522;&#20110;&#20998;&#23618;&#23384;&#20648;&#27744;&#65292;&#21033;&#29992;&#22810;&#32423;&#23384;&#20648;&#32467;&#26500;&#26469;&#23384;&#20648;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous changes in the world have resulted in the performance regression of neural networks. Therefore, continual learning (CL) area gradually attracts the attention of more researchers. For edge intelligence, the CL model not only needs to overcome catastrophic for-getting, but also needs to face the huge challenge of severely limited resources: the lack of labeled resources and powerful devices. However, the existing classic CL methods usually rely on a large number of labeled samples to maintain the plasticity and stability, and the semi-supervised learning methods often need to pay a large computational and memory overhead for higher accuracy. In response to these prob-lems, a low-cost semi-supervised CL method named Edge Hierarchical Memory Learner (EdgeHML) will be proposed. EdgeHML can effec-tively utilize a large number of unlabeled samples and a small number of labeled samples. It is based on a hierarchical memory pool, lever-age multi-level storage structure to store a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#33258;&#36866;&#24212;&#36827;&#21270;&#29305;&#24449;&#36873;&#25321;&#19982;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36523;&#20307;&#33026;&#32938;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#31649;&#29702;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#30830;&#23450;&#20102;&#36866;&#24403;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#27700;&#24179;&#65292;&#21487;&#20197;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11949</link><description>&lt;p&gt;
&#19968;&#31181;&#27169;&#31946;&#33258;&#36866;&#24212;&#36827;&#21270;&#29305;&#24449;&#36873;&#25321;&#19982;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36523;&#20307;&#33026;&#32938;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A fuzzy adaptive evolutionary-based feature selection and machine learning framework for single and multi-objective body fat prediction. (arXiv:2303.11949v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#33258;&#36866;&#24212;&#36827;&#21270;&#29305;&#24449;&#36873;&#25321;&#19982;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36523;&#20307;&#33026;&#32938;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#31649;&#29702;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#30830;&#23450;&#20102;&#36866;&#24403;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#27700;&#24179;&#65292;&#21487;&#20197;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36523;&#20307;&#33026;&#32938;&#21487;&#20197;&#20026;&#21307;&#23398;&#20174;&#19994;&#32773;&#21644;&#29992;&#25143;&#25552;&#20379;&#39044;&#38450;&#21644;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#36873;&#25321;&#30456;&#20851;&#30340;&#36523;&#20307;&#27979;&#37327;&#20540;&#21644;&#25429;&#25417;&#27169;&#22411;&#20013;&#25152;&#36873;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#25552;&#20379;&#20102;&#27604;&#31616;&#21333;&#30340;&#22238;&#24402;&#20998;&#26512;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#12290;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#26041;&#27861;&#23558;&#36523;&#20307;&#33026;&#32938;&#39044;&#27979;&#38382;&#39064;&#24314;&#27169;&#20026;&#32452;&#21512;&#30340;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24448;&#24448;&#20250;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#12290;&#24403;&#22810;&#20010;&#29305;&#24449;&#23376;&#38598;&#20135;&#29983;&#31867;&#20284;&#25110;&#25509;&#36817;&#30340;&#39044;&#27979;&#26102;&#65292;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#36827;&#21270;&#29305;&#24449;&#36873;&#25321;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#20960;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#19968;&#20010;&#27169;&#31946;&#38598;&#29702;&#35770;&#30830;&#23450;&#36866;&#24403;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#27700;&#24179;&#65292;&#21516;&#26102;&#31649;&#29702;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#21152;&#26435;&#21644;&#36523;&#20307;&#33026;&#32938;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Predicting body fat can provide medical practitioners and users with essential information for preventing and diagnosing heart diseases. Hybrid machine learning models offer better performance than simple regression analysis methods by selecting relevant body measurements and capturing complex nonlinear relationships among selected features in modelling body fat prediction problems. There are, however, some disadvantages to them. Current machine learning. Modelling body fat prediction as a combinatorial single- and multi-objective optimisation problem often gets stuck in local optima. When multiple feature subsets produce similar or close predictions, avoiding local optima becomes more complex. Evolutionary feature selection has been used to solve several machine-learning-based optimisation problems. A fuzzy set theory determines appropriate levels of exploration and exploitation while managing parameterisation and computational costs. A weighted-sum body fat prediction approach was ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#12289;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36827;&#34892;&#36328;&#39046;&#22495;&#29305;&#24449;&#23545;&#40784;&#65292;&#36824;&#21487;&#20197;&#24378;&#21046;&#30446;&#26631;&#26679;&#26412;&#19982;&#28304;&#22495;&#20013;&#30340;&#30456;&#24212;&#21407;&#22411;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#20135;&#29983;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11945</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#65306;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Cross-Domain Rumor Detection with Contrastive Learning and Cross-Attention. (arXiv:2303.11945v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#12289;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36827;&#34892;&#36328;&#39046;&#22495;&#29305;&#24449;&#23545;&#40784;&#65292;&#36824;&#21487;&#20197;&#24378;&#21046;&#30446;&#26631;&#26679;&#26412;&#19982;&#28304;&#22495;&#20013;&#30340;&#30456;&#24212;&#21407;&#22411;&#23545;&#40784;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26041;&#27861;&#20135;&#29983;&#20266;&#26631;&#31614;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35875;&#35328;&#36890;&#24120;&#20276;&#38543;&#30528;&#31361;&#21457;&#26032;&#38395;&#25110;&#28909;&#38376;&#35805;&#39064;&#32780;&#20986;&#29616;&#65292;&#20005;&#37325;&#38459;&#30861;&#30495;&#30456;&#30340;&#26597;&#35777;&#12290;&#29616;&#26377;&#30340;&#35875;&#35328;&#26816;&#27979;&#26041;&#27861;&#22823;&#22810;&#19987;&#27880;&#20110;&#30456;&#21516;&#39046;&#22495;&#65292;&#22240;&#27492;&#22312;&#36328;&#39046;&#22495;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#23454;&#20363;&#21644;&#21407;&#22411;&#30340;&#65292;&#24102;&#26377;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#35875;&#35328;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#36827;&#34892;&#36328;&#39046;&#22495;&#29305;&#24449;&#23545;&#40784;&#65292;&#36824;&#21487;&#20197;&#24378;&#21046;&#30446;&#26631;&#26679;&#26412;&#19982;&#32473;&#23450;&#28304;&#22495;&#30340;&#30456;&#24212;&#21407;&#22411;&#23545;&#40784;&#12290;&#30001;&#20110;&#30446;&#26631;&#22495;&#20013;&#30340;&#30446;&#26631;&#26631;&#31614;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#25209;&#28304;&#22495;&#26679;&#26412;&#30340;&#20180;&#32454;&#21021;&#22987;&#21270;&#20013;&#24515;&#26469;&#20135;&#29983;&#20266;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#22788;&#29702;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#19968;&#23545;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#65292;&#20197;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#30001;&#20110;&#39046;&#22495;&#23545;&#20013;&#30340;&#26679;&#26412;&#20542;&#21521;&#20110;&#34920;&#36798;&#30456;&#20284;&#30340;&#35821;&#20041;&#27169;&#24335;&#65292;&#22240;&#27492;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive rumors usually appear along with breaking news or trending topics, seriously hindering the truth. Existing rumor detection methods are mostly focused on the same domain, and thus have poor performance in cross-domain scenarios due to domain shift. In this work, we propose an end-to-end instance-wise and prototype-wise contrastive learning model with a cross-attention mechanism for cross-domain rumor detection. The model not only performs cross-domain feature alignment but also enforces target samples to align with the corresponding prototypes of a given source domain. Since target labels in a target domain are unavailable, we use a clustering-based approach with carefully initialized centers by a batch of source domain samples to produce pseudo labels. Moreover, we use a cross-attention mechanism on a pair of source data and target data with the same labels to learn domain-invariant representations. Because the samples in a domain pair tend to express similar semantic patterns,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38543;&#26426;CSF&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#39640;&#27010;&#29575;&#36793;&#30028;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32463;&#36807;&#39564;&#35777;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20063;&#33021;&#25910;&#25947;&#21040;&#36739;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.11937</link><description>&lt;p&gt;
&#38543;&#26426;&#36830;&#32493;&#27425;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#39640;&#27010;&#29575;&#36793;&#30028;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High Probability Bounds for Stochastic Continuous Submodular Maximization. (arXiv:2303.11937v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38543;&#26426;CSF&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#39640;&#27010;&#29575;&#36793;&#30028;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32463;&#36807;&#39564;&#35777;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20063;&#33021;&#25910;&#25947;&#21040;&#36739;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36882;&#20943;&#25910;&#30410;&#29305;&#24615;&#30340;&#38543;&#26426;&#21333;&#35843;&#36830;&#32493;&#27425;&#27169;&#20989;&#25968;&#65288;CSF&#65289;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#31639;&#27861;&#21482;&#33021;&#25552;&#20379;&#26399;&#26395;&#34920;&#29616;&#20445;&#35777;&#65292;&#24182;&#19981;&#33021;&#38480;&#21046;&#24471;&#21040;&#19981;&#22909;&#35299;&#30340;&#27010;&#29575;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#20110;&#31639;&#27861;&#30340;&#26576;&#20010;&#36816;&#34892;&#65292;&#24471;&#21040;&#30340;&#35299;&#21487;&#33021;&#35201;&#27604;&#26399;&#26395;&#20445;&#35777;&#30340;&#26356;&#31967;&#31957;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#26377;&#38543;&#26426;CSF&#26368;&#22823;&#21270;&#26041;&#27861;&#30340;&#31532;&#19968;&#20998;&#26512;&#65292;&#21363;PGA&#65292;boosted PGA&#65292;SCG&#21644;SCG ++&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#31245;&#24494;&#24378;&#19968;&#28857;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;SCG&#30340;&#25913;&#36827;&#39640;&#27010;&#29575;&#30028;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#27604;&#39044;&#26399;&#35299;&#26356;&#24555;&#12290;&#36890;&#36807;&#23545;&#38750;&#20984;&#20108;&#27425;&#35268;&#21010;&#65288;NQP&#65289;&#21644;&#26368;&#20248;&#39044;&#31639;&#20998;&#37197;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#21363;&#20351;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;PGA&#20063;&#20250;&#25910;&#25947;&#21040;$OPT / 2$&#65292;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider maximization of stochastic monotone continuous submodular functions (CSF) with a diminishing return property. Existing algorithms only guarantee the performance \textit{in expectation}, and do not bound the probability of getting a bad solution. This implies that for a particular run of the algorithms, the solution may be much worse than the provided guarantee in expectation. In this paper, we first empirically verify that this is indeed the case. Then, we provide the first \textit{high-probability} analysis of the existing methods for stochastic CSF maximization, namely PGA, boosted PGA, SCG, and SCG++. Finally, we provide an improved high-probability bound for SCG, under slightly stronger assumptions, with a better convergence rate than that of the expected solution. Through extensive experiments on non-concave quadratic programming (NQP) and optimal budget allocation, we confirm the validity of our bounds and show that even in the worst-case, PGA converges to $OPT/2$, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#21033;&#29992;&#32858;&#31867;&#31639;&#27861;&#25214;&#21040;&#32654;&#22269;&#30123;&#24773;&#30456;&#20851;&#30340;&#21439;&#20221;&#30340;&#27169;&#24335;&#65292;&#24182;&#20174;&#20013;&#20102;&#35299;&#30123;&#24773;&#12290;</title><link>http://arxiv.org/abs/2303.11936</link><description>&lt;p&gt;
&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21457;&#29616;&#19982;COVID-19&#30123;&#24773;&#26377;&#20851;&#30340;&#32654;&#22269;&#21439;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Clustering US Counties to Find Patterns Related to the COVID-19 Pandemic. (arXiv:2303.11936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11936
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#21033;&#29992;&#32858;&#31867;&#31639;&#27861;&#25214;&#21040;&#32654;&#22269;&#30123;&#24773;&#30456;&#20851;&#30340;&#21439;&#20221;&#30340;&#27169;&#24335;&#65292;&#24182;&#20174;&#20013;&#20102;&#35299;&#30123;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;COVID-19&#24320;&#22987;&#20256;&#25773;&#21644;&#38548;&#31163;&#25514;&#26045;&#23454;&#26045;&#26102;&#65292;&#26126;&#23612;&#33487;&#36798;&#24030;&#21452;&#23376;&#22478;&#30340;&#24037;&#19994;&#21644;&#24212;&#29992;&#25968;&#23398;&#21327;&#20250;&#65288;SIAM&#65289;&#23398;&#29983;&#20998;&#20250;&#24320;&#22987;&#19982;Ecolab&#21512;&#20316;&#65292;&#21033;&#29992;&#25105;&#20204;&#20316;&#20026;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#25968;&#23398;&#23478;&#30340;&#25216;&#33021;&#20174;&#19982;&#22823;&#27969;&#34892;&#30456;&#20851;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#36825;&#31181;&#21512;&#20316;&#30001;&#22810;&#20010;&#23567;&#32452;&#22312;&#19981;&#21516;&#30340;&#39033;&#30446;&#19978;&#24037;&#20316;&#32452;&#25104;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#25105;&#20204;&#20351;&#29992;&#32858;&#31867;&#25216;&#26415;&#24110;&#21161;&#25105;&#20204;&#25214;&#21040;&#32654;&#22269;&#30456;&#20284;&#21439;&#32452;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#26469;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#22823;&#27969;&#34892;&#30149;&#12290;&#26412;&#39033;&#30446;&#30340;&#22242;&#38431;&#21253;&#25324;&#26126;&#23612;&#33487;&#36798;&#22823;&#23398;&#30340;&#23398;&#29983;Cora Brown&#65292;Sarah Milstein&#65292;Tianyi Sun&#21644;Cooper Zhao&#65292;Ecolab&#25968;&#25454;&#31185;&#23398;&#23478;Jimmy Broomfield&#20197;&#21450;&#26126;&#23612;&#33487;&#36798;&#22823;&#23398;&#30340;&#23398;&#29983;Skye Ke&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#19979;&#38754;&#30340;&#31456;&#33410;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#26412;&#39033;&#30446;&#30340;&#25152;&#26377;&#24037;&#20316;&#12290;&#22312;&#31532;2&#33410;&#20013;&#65292;&#25105;&#20204;&#21015;&#20986;&#20102;&#25105;&#20204;&#25910;&#38598;&#30340;&#25968;&#25454;&#20197;&#21450;&#25105;&#20204;&#25191;&#34892;&#30340;&#29305;&#24449;&#24037;&#31243;&#12290;&#22312;&#31532;3&#33410;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#22914;&#20309;&#25191;&#34892;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#19988;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#35782;&#21035;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
When COVID-19 first started spreading and quarantine was implemented, the Society for Industrial and Applied Mathematics (SIAM) Student Chapter at the University of Minnesota-Twin Cities began a collaboration with Ecolab to use our skills as data scientists and mathematicians to extract useful insights from relevant data relating to the pandemic. This collaboration consisted of multiple groups working on different projects. In this write-up we focus on using clustering techniques to help us find groups of similar counties in the US and use that to help us understand the pandemic. Our team for this project consisted of University of Minnesota students Cora Brown, Sarah Milstein, Tianyi Sun, and Cooper Zhao, with help from Ecolab Data Scientist Jimmy Broomfield and University of Minnesota student Skye Ke. In the sections below we describe all of the work done for this project. In Section 2, we list the data we gathered, as well as the feature engineering we performed. In Section 3, we de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#37327;&#21270; COVID-19 &#21644;&#20854;&#20182;&#32954;&#37096;&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11935</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#29992;&#20110;&#20351;&#29992;&#33016;&#37096; X &#23556;&#32447;&#22270;&#20687;&#37327;&#21270;&#32954;&#28814;&#30340;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer-based Model for Severity Quantification of Lung Pneumonia Using Chest X-ray Images. (arXiv:2303.11935v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#37327;&#21270; COVID-19 &#21644;&#20854;&#20182;&#32954;&#37096;&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#38024;&#23545;&#33016;&#37096; X &#23556;&#32447;&#65288;CXR&#65289;&#24320;&#21457;&#35786;&#26029;&#21644;&#35780;&#20272; COVID-19 &#30340;&#20005;&#37325;&#31243;&#24230;&#30340;&#36890;&#29992;&#21644;&#21487;&#38752;&#26041;&#27861;&#65292;&#38656;&#35201;&#22823;&#37327;&#32500;&#25252;&#33391;&#22909;&#30340; COVID-19 &#25968;&#25454;&#38598;&#12290;&#29616;&#26377;&#30340;&#20005;&#37325;&#31243;&#24230;&#37327;&#21270;&#32467;&#26500;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#35745;&#31639;&#25165;&#33021;&#21462;&#24471;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;&#38656;&#35201;&#35745;&#31639;&#26426;&#24037;&#20855;&#65292;&#20197;&#20415;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#24555;&#36895;&#33258;&#21160;&#35782;&#21035; COVID-19 &#24739;&#32773;&#24182;&#39044;&#27979;&#30456;&#20851;&#30340;&#20005;&#37325;&#31243;&#24230;&#25351;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36716;&#25442;&#22120;&#65288;ViT&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20381;&#38752;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#37327;&#21270; COVID-19 &#21644;&#20854;&#20182;&#32954;&#37096;&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270; CXR &#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#31216;&#20026; Vision Transformer Regressor Infection Prediction&#65288;ViTReg-IP&#65289;&#65292;&#23427;&#30001; ViT &#21644;&#19968;&#20010;&#22238;&#24402;&#22836;&#23548;&#20986;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#24320;&#25918;&#28304;&#30340;&#21508;&#31181;&#20854;&#20182;&#27979;&#35797;&#33016;&#37096;&#25918;&#23556;&#22270;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#25105;&#20204;&#27169;&#22411;&#30340;&#27867;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To develop generic and reliable approaches for diagnosing and assessing the severity of COVID-19 from chest X-rays (CXR), a large number of well-maintained COVID-19 datasets are needed. Existing severity quantification architectures require expensive training calculations to achieve the best results. For healthcare professionals to quickly and automatically identify COVID-19 patients and predict associated severity indicators, computer utilities are needed. In this work, we propose a Vision Transformer (ViT)-based neural network model that relies on a small number of trainable parameters to quantify the severity of COVID-19 and other lung diseases. We present a feasible approach to quantify the severity of CXR, called Vision Transformer Regressor Infection Prediction (ViTReg-IP), derived from a ViT and a regression head. We investigate the generalization potential of our model using a variety of additional test chest radiograph datasets from different open sources. In this context, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31232;&#30095;&#20998;&#24067;&#24335;&#20869;&#23384;&#30340;&#20462;&#25913;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35760;&#24518;&#37325;&#25918;&#25110;&#20219;&#21153;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11934</link><description>&lt;p&gt;
&#31232;&#30095;&#20998;&#24067;&#24335;&#20869;&#23384;&#26159;&#19968;&#20010;&#25345;&#32493;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Sparse Distributed Memory is a Continual Learner. (arXiv:2303.11934v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#31232;&#30095;&#20998;&#24067;&#24335;&#20869;&#23384;&#30340;&#20462;&#25913;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35760;&#24518;&#37325;&#25918;&#25110;&#20219;&#21153;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#32780;&#23427;&#20204;&#30340;&#29983;&#29289;&#23398;&#23545;&#24212;&#29289;&#25797;&#38271;&#35299;&#20915;&#12290;&#22312;&#21033;&#29992;&#31232;&#30095;&#20998;&#24067;&#24335;&#20869;&#23384;&#65288;SDM&#65289;&#23558;&#26680;&#24515;&#31070;&#32463;&#30005;&#36335;&#19982;&#24378;&#22823;&#30340;Transformer&#27169;&#22411;&#30456;&#36830;&#25509;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20174;&#29983;&#29289;&#23398;&#19978;&#32763;&#35793;&#36807;&#26469;&#30340;&#25105;&#20204;&#30340;MLP&#21464;&#20307;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#37117;&#26159;&#25345;&#32493;&#23398;&#20064;&#25152;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20063;&#19981;&#38656;&#35201;&#20219;&#20309;&#35760;&#24518;&#37325;&#25918;&#25110;&#20219;&#21153;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#35757;&#32451;&#31232;&#30095;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is a problem for artificial neural networks that their biological counterparts are adept at solving. Building on work using Sparse Distributed Memory (SDM) to connect a core neural circuit with the powerful Transformer model, we create a modified Multi-Layered Perceptron (MLP) that is a strong continual learner. We find that every component of our MLP variant translated from biology is necessary for continual learning. Our solution is also free from any memory replay or task information, and introduces novel methods to train sparse networks that may be broadly applicable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#20013;&#38388;&#27010;&#24565;&#30340;&#32423;&#21035;&#32467;&#26500;&#65292;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.11920</link><description>&lt;p&gt;
&#20013;&#38388;&#29305;&#24449;&#32852;&#30431;&#33021;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do intermediate feature coalitions aid explainability of black-box models?. (arXiv:2303.11920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#20013;&#38388;&#27010;&#24565;&#30340;&#32423;&#21035;&#32467;&#26500;&#65292;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#32423;&#21035;&#32467;&#26500;&#30340;&#20013;&#38388;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#40657;&#30418;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#32423;&#21035;&#32467;&#26500;&#26159;&#19968;&#31181;&#20998;&#23618;&#32467;&#26500;&#65292;&#27599;&#20010;&#32423;&#21035;&#23545;&#24212;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65288;&#21363;&#29609;&#23478;&#38598;&#20998;&#21306;&#65289;&#12290;&#20174;&#21482;&#21253;&#21547;&#21333;&#20803;&#32032;&#30340;&#24179;&#20961;&#38598;&#21512;&#21040;&#21482;&#21253;&#21547;&#22823;&#32852;&#30431;&#30340;&#38598;&#21512;&#65292;&#31895;&#31961;&#24230;&#30340;&#32423;&#21035;&#36880;&#28176;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#36890;&#36807;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#26469;&#29983;&#25104;&#25277;&#35937;&#32423;&#21035;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#27773;&#36710;&#27169;&#22411;&#31034;&#20363;&#21644;&#27888;&#22374;&#23612;&#20811;&#21495;&#30340;&#25968;&#25454;&#38598;&#20013;&#35828;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#65292;&#20854;&#20013;&#20013;&#38388;&#27010;&#24565;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the notion of intermediate concepts based on levels structure to aid explainability for black-box models. The levels structure is a hierarchical structure in which each level corresponds to features of a dataset (i.e., a player-set partition). The level of coarseness increases from the trivial set, which only comprises singletons, to the set, which only contains the grand coalition. In addition, it is possible to establish meronomies, i.e., part-whole relationships, via a domain expert that can be utilised to generate explanations at an abstract level. We illustrate the usability of this approach in a real-world car model example and the Titanic dataset, where intermediate concepts aid in explainability at different levels of abstraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Deephys&#30340;&#21487;&#35270;&#21270;&#21644;&#29702;&#35299;DNN&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#20013;&#22833;&#36133;&#30340;&#24037;&#20855;&#65292;&#20351;&#29992;&#31070;&#32463;&#30005;&#29983;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#27604;&#36739;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#65292;&#26080;&#32541;&#20998;&#26512;&#21333;&#20010;&#31070;&#32463;&#20803;&#12289;&#21333;&#20010;&#22270;&#20687;&#21644;&#31867;&#21035;&#22270;&#20687;&#38598;&#65292;&#24182;&#33021;&#25581;&#31034;&#20551;&#29305;&#24449;&#21644;&#26032;&#29305;&#24449;&#23384;&#22312;&#23548;&#33268;&#30340;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2303.11912</link><description>&lt;p&gt;
Deephys&#65306;&#20998;&#24067;&#28418;&#31227;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#35797;&#19982;&#21487;&#35270;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Deephys: Deep Electrophysiology, Debugging Neural Networks under Distribution Shifts. (arXiv:2303.11912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Deephys&#30340;&#21487;&#35270;&#21270;&#21644;&#29702;&#35299;DNN&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#20013;&#22833;&#36133;&#30340;&#24037;&#20855;&#65292;&#20351;&#29992;&#31070;&#32463;&#30005;&#29983;&#29702;&#23398;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#27604;&#36739;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#65292;&#26080;&#32541;&#20998;&#26512;&#21333;&#20010;&#31070;&#32463;&#20803;&#12289;&#21333;&#20010;&#22270;&#20687;&#21644;&#31867;&#21035;&#22270;&#20687;&#38598;&#65292;&#24182;&#33021;&#25581;&#31034;&#20551;&#29305;&#24449;&#21644;&#26032;&#29305;&#24449;&#23384;&#22312;&#23548;&#33268;&#30340;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#19979;&#32463;&#24120;&#20250;&#20986;&#29616;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#21487;&#35270;&#21270;&#21644;&#29702;&#35299;&#36825;&#31181;&#22833;&#36133;&#12290;&#25105;&#20204;&#20174;&#31070;&#32463;&#30005;&#29983;&#29702;&#23398;&#30340;&#27010;&#24565;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#36890;&#36807;&#20998;&#26512;&#21333;&#20010;&#31070;&#32463;&#20803;&#30340;&#29305;&#24449;&#35843;&#35856;&#21644;&#19981;&#21464;&#24615;&#65292;&#26469;&#26816;&#26597;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#21151;&#33021;&#12290;Deep Electrophysiology&#65292;&#31616;&#31216;Deephys&#65292;&#36890;&#36807;&#27604;&#36739;&#21487;&#35270;&#21270;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#38598;&#20013;&#30340;&#31070;&#32463;&#27963;&#21160;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;DNN&#22312;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#20013;&#22833;&#36133;&#30340;&#35265;&#35299;&#12290;Deephys&#25552;&#20379;&#20102;&#23545;&#21333;&#20010;&#31070;&#32463;&#20803;&#65292;&#21333;&#20010;&#22270;&#20687;&#20197;&#21450;&#31867;&#21035;&#22270;&#20687;&#38598;&#30340;&#26080;&#32541;&#20998;&#26512;&#65292;&#24182;&#19988;&#33021;&#22815;&#25581;&#31034;&#30001;&#20110;&#20551;&#29305;&#24449;&#21644;&#26032;&#29305;&#24449;&#30340;&#23384;&#22312;&#32780;&#23548;&#33268;&#30340;&#22833;&#36133;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#24067;&#28418;&#31227;&#20013;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#25442;&#22120;&#26550;&#26500;&#36827;&#34892;&#25968;&#37327;&#20998;&#26512;&#65292;&#35777;&#23454;&#20102;Deephys&#30340;&#23450;&#24615;&#21487;&#35270;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) often fail in out-of-distribution scenarios. In this paper, we introduce a tool to visualize and understand such failures. We draw inspiration from concepts from neural electrophysiology, which are based on inspecting the internal functioning of a neural networks by analyzing the feature tuning and invariances of individual units. Deep Electrophysiology, in short Deephys, provides insights of the DNN's failures in out-of-distribution scenarios by comparative visualization of the neural activity in in-distribution and out-of-distribution datasets. Deephys provides seamless analyses of individual neurons, individual images, and a set of set of images from a category, and it is capable of revealing failures due to the presence of spurious features and novel features. We substantiate the validity of the qualitative visualizations of Deephys thorough quantitative analyses using convolutional and transformers architectures, in several datasets and distribution shi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20449;&#24687;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#65292;&#35299;&#20915;&#20102;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22810;&#26679;&#22686;&#24378;&#26041;&#24335;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11911</link><description>&lt;p&gt;
&#24102;&#20449;&#24687;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Time Series Contrastive Learning with Information-Aware Augmentations. (arXiv:2303.11911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20449;&#24687;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#65292;&#35299;&#20915;&#20102;&#22312;&#23545;&#27604;&#23398;&#20064;&#20013;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22810;&#26679;&#22686;&#24378;&#26041;&#24335;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#25104;&#26524;&#12290;&#34429;&#28982;&#26377;&#25928;&#19988;&#26222;&#36941;&#65292;&#20294;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#25506;&#32034;&#36739;&#23569;&#12290;&#23545;&#27604;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#24335;&#65292;&#24341;&#20837;&#19968;&#20123;&#20808;&#39564;&#20449;&#24687;&#26469;&#26500;&#36896;&#21487;&#34892;&#30340;&#27491;&#26679;&#26412;&#65292;&#20174;&#32780;&#35757;&#32451;&#32534;&#30721;&#22120;&#23398;&#20064;&#24378;&#22823;&#32780;&#20855;&#26377;&#21306;&#20998;&#24230;&#30340;&#34920;&#31034;&#12290;&#19982;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#29983;&#25104;&#26399;&#26395;&#30340;&#22686;&#24378;&#26679;&#26412;&#19981;&#21516;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#22686;&#24378;&#26679;&#26412;&#20855;&#26377;&#22810;&#26679;&#30340;&#12289;&#20154;&#31867;&#38590;&#20197;&#35782;&#21035;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#22914;&#20309;&#25214;&#21040;&#23545;&#20110;&#32473;&#23450;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26377;&#24847;&#20041;&#30340;&#26102;&#38388;&#24207;&#21015;&#22686;&#24378;&#26041;&#24335;&#20173;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#40723;&#21169;&#22522;&#20110;&#20449;&#24687;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#21516;&#26102;&#20445;&#35777;&#39640;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various contrastive learning approaches have been proposed in recent years and achieve significant empirical success. While effective and prevalent, contrastive learning has been less explored for time series data. A key component of contrastive learning is to select appropriate augmentations imposing some priors to construct feasible positive samples, such that an encoder can be trained to learn robust and discriminative representations. Unlike image and language domains where ``desired'' augmented samples can be generated with the rule of thumb guided by prefabricated human priors, the ad-hoc manual selection of time series augmentations is hindered by their diverse and human-unrecognizable temporal structures. How to find the desired augmentations of time series data that are meaningful for given contrastive learning tasks and datasets remains an open question. In this work, we address the problem by encouraging both high \textit{fidelity} and \textit{variety} based upon information
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#35823;&#24046;&#36793;&#30028;&#65292;&#20026;&#24191;&#27867;&#31867;&#21035;&#30340;&#35889;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#35823;&#24046;&#36793;&#30028;&#65292;&#21253;&#25324;&#22312;&#29305;&#23450;&#39057;&#29575;&#22788;&#30340;&#28857;&#20540;&#35823;&#24046;&#36793;&#30028;&#21644;&#25152;&#26377;&#39057;&#29575;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#23548;&#20986;&#20102;&#32463;&#20856;&#35889;&#20272;&#35745;&#22120;&#65288;Blackman-Tukey&#12289;Bartlett &#21644; Welch &#20272;&#35745;&#22120;&#65289;&#30340;&#35823;&#24046;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.11908</link><description>&lt;p&gt;
&#32463;&#20856;&#35889;&#20272;&#35745;&#30340;&#38750;&#28176;&#36827;&#24335;&#28857;&#20540;&#21644;&#26368;&#22351;&#24773;&#20917;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Pointwise and Worst-Case Bounds for Classical Spectrum Estimators. (arXiv:2303.11908v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#35823;&#24046;&#36793;&#30028;&#65292;&#20026;&#24191;&#27867;&#31867;&#21035;&#30340;&#35889;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#35823;&#24046;&#36793;&#30028;&#65292;&#21253;&#25324;&#22312;&#29305;&#23450;&#39057;&#29575;&#22788;&#30340;&#28857;&#20540;&#35823;&#24046;&#36793;&#30028;&#21644;&#25152;&#26377;&#39057;&#29575;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#36793;&#30028;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#23548;&#20986;&#20102;&#32463;&#20856;&#35889;&#20272;&#35745;&#22120;&#65288;Blackman-Tukey&#12289;Bartlett &#21644; Welch &#20272;&#35745;&#22120;&#65289;&#30340;&#35823;&#24046;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#20272;&#35745;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#30340;&#22522;&#26412;&#26041;&#27861;&#65292;&#24212;&#29992;&#21253;&#25324;&#21307;&#23398;&#12289;&#35821;&#38899;&#20998;&#26512;&#21644;&#25511;&#21046;&#35774;&#35745;&#12290;&#34429;&#28982;&#35889;&#20272;&#35745;&#30340;&#28176;&#36827;&#29702;&#35770;&#24456;&#22909;&#29702;&#35299;&#65292;&#20294;&#22312;&#26679;&#26412;&#25968;&#37327;&#22266;&#23450;&#19988;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#29702;&#35770;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20026;&#24191;&#27867;&#31867;&#21035;&#30340;&#35889;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#38750;&#28176;&#36827;&#35823;&#24046;&#36793;&#30028;&#65292;&#21253;&#25324;&#22312;&#29305;&#23450;&#39057;&#29575;&#22788;&#30340;&#28857;&#20540;&#35823;&#24046;&#36793;&#30028;&#21644;&#25152;&#26377;&#39057;&#29575;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#36793;&#30028;&#12290;&#26412;&#25991;&#25152;&#25552;&#30340;&#19968;&#33324;&#26041;&#27861;&#20063;&#29992;&#20110;&#23548;&#20986;&#20102;&#32463;&#20856;&#30340; Blackman-Tukey&#12289;Bartlett &#21644; Welch &#20272;&#35745;&#22120;&#30340;&#35823;&#24046;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectrum estimation is a fundamental methodology in the analysis of time-series data, with applications including medicine, speech analysis, and control design. The asymptotic theory of spectrum estimation is well-understood, but the theory is limited when the number of samples is fixed and finite. This paper gives non-asymptotic error bounds for a broad class of spectral estimators, both pointwise (at specific frequencies) and in the worst case over all frequencies. The general method is used to derive error bounds for the classical Blackman-Tukey, Bartlett, and Welch estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11884</link><description>&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#26356;&#22909;&#22320;&#29702;&#35299;&#24402;&#22240;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Better Understanding Differences in Attribution Methods via Systematic Evaluations. (arXiv:2303.11884v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#26696;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#24402;&#22240;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20854;&#40657;&#30418;&#24615;&#36136;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#21518;&#32493;&#24402;&#22240;&#26041;&#27861;&#26469;&#30830;&#23450;&#23545;&#27169;&#22411;&#20915;&#31574;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;&#30001;&#20110;&#19981;&#23384;&#22312;&#22522;&#20934;&#24402;&#22240;&#65292;&#22240;&#27492;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#20197;&#26356;&#21487;&#38752;&#22320;&#27979;&#37327;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#65292;&#20351;&#23427;&#20204;&#20043;&#38388;&#30340;&#27604;&#36739;&#26356;&#20844;&#24179;&#65292;&#24182;&#20351;&#35270;&#35273;&#26816;&#26597;&#26356;&#31995;&#32479;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are very successful on many vision tasks, but hard to interpret due to their black box nature. To overcome this, various post-hoc attribution methods have been proposed to identify image regions most influential to the models' decisions. Evaluating such methods is challenging since no ground truth attributions exist. We thus propose three novel evaluation schemes to more reliably measure the faithfulness of those methods, to make comparisons between them more fair, and to make visual inspection more systematic. To address faithfulness, we propose a novel evaluation setting (DiFull) in which we carefully control which parts of the input can influence the output in order to distinguish possible from impossible attributions. To address fairness, we note that different methods are applied at different layers, which skews any comparison, and so evaluate all methods on the same layers (ML-Att) and discuss how this impacts their performance on quantitative metrics. For mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#25252;&#24615;&#33258;&#36866;&#24212;&#21098;&#26525;&#65288;PSAP&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26435;&#37325;&#31232;&#30095;&#27604;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23618;&#30340;&#21098;&#26525;&#27604;&#29575;&#65292;&#24182;&#36890;&#36807;&#30417;&#30563;&#26799;&#24230;&#26469;&#36991;&#20813;&#37325;&#35201;&#36807;&#28388;&#22120;&#34987;&#21098;&#26525;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#21487;&#24674;&#22797;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;</title><link>http://arxiv.org/abs/2303.11881</link><description>&lt;p&gt;
&#29992;&#20445;&#25252;&#24615;&#33258;&#36866;&#24212;&#21098;&#26525;&#26469;&#26356;&#22909;&#22320;&#21387;&#32553;DNN
&lt;/p&gt;
&lt;p&gt;
Protective Self-Adaptive Pruning to Better Compress DNNs. (arXiv:2303.11881v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20445;&#25252;&#24615;&#33258;&#36866;&#24212;&#21098;&#26525;&#65288;PSAP&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26435;&#37325;&#31232;&#30095;&#27604;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23618;&#30340;&#21098;&#26525;&#27604;&#29575;&#65292;&#24182;&#36890;&#36807;&#30417;&#30563;&#26799;&#24230;&#26469;&#36991;&#20813;&#37325;&#35201;&#36807;&#28388;&#22120;&#34987;&#21098;&#26525;&#65292;&#20174;&#32780;&#36991;&#20813;&#19981;&#21487;&#24674;&#22797;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#22240;&#20854;&#20248;&#31168;&#30340;&#33021;&#21147;&#26469;&#35782;&#21035;&#23618;&#21644;&#36807;&#28388;&#22120;&#30340;&#37325;&#35201;&#24615;&#21644;&#20887;&#20313;&#24615;&#24182;&#23450;&#21046;&#21512;&#36866;&#30340;&#21098;&#26525;&#26041;&#26696;&#32780;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#30340;&#33258;&#36866;&#24212;&#21098;&#26525;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#30417;&#35270;&#22120;&#26469;&#35780;&#20998;&#23618;&#21644;&#36807;&#28388;&#22120;&#30340;&#37325;&#35201;&#24615;&#65292;&#22240;&#27492;&#20173;&#28982;&#19981;&#23613;&#20154;&#24847;&#65292;&#38754;&#20020;&#30528;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#24369;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#36845;&#20195;&#20462;&#21098;-&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#37325;&#37327;&#37325;&#26500;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#24615;&#33258;&#36866;&#24212;&#20462;&#21098;&#65288;PSAP&#65289;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;PSAP&#21487;&#20197;&#21033;&#29992;&#33258;&#36523;&#20449;&#24687;&#65292;&#21363;&#37325;&#37327;&#31232;&#30095;&#27604;&#65292;&#20197;&#20351;&#27599;&#20010;&#20462;&#21098;&#27493;&#39588;&#20043;&#21069;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23618;&#30340;&#20462;&#21098;&#27604;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#24615;&#37325;&#24314;&#26426;&#21046;&#65292;&#36890;&#36807;&#30417;&#30563;&#26799;&#24230;&#26469;&#38450;&#27490;&#37325;&#35201;&#36807;&#28388;&#22120;&#34987;&#20462;&#21098;&#24182;&#36991;&#20813;&#19981;&#21487;&#24674;&#22797;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;PSAP&#38750;&#24120;&#20415;&#25463;&#21644;&#26126;&#30830;&#65292;&#22240;&#20026;&#23427;&#20165;&#20381;&#36182;&#20110;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive network pruning approach has recently drawn significant attention due to its excellent capability to identify the importance and redundancy of layers and filters and customize a suitable pruning solution. However, it remains unsatisfactory since current adaptive pruning methods rely mostly on an additional monitor to score layer and filter importance, and thus faces high complexity and weak interpretability. To tackle these issues, we have deeply researched the weight reconstruction process in iterative prune-train process and propose a Protective Self-Adaptive Pruning (PSAP) method. First of all, PSAP can utilize its own information, weight sparsity ratio, to adaptively adjust pruning ratio of layers before each pruning step. Moreover, we propose a protective reconstruction mechanism to prevent important filters from being pruned through supervising gradients and to avoid unrecoverable information loss as well. Our PSAP is handy and explicit because it merely depends on weigh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#65292;&#21457;&#29616;Grokking&#29616;&#35937;&#23545;&#24212;&#20110;&#31232;&#30095;&#23376;&#32593;&#32476;&#30340;&#20986;&#29616;&#65292;&#35813;&#23376;&#32593;&#32476;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30001;&#20110;&#23569;&#25968;&#31070;&#32463;&#20803;&#30340;&#24555;&#36895;&#21508;&#21521;&#21516;&#24615;&#22686;&#38271;&#32780;&#20986;&#29616;&#65292;&#20174;&#32780;&#19982;&#25903;&#37197;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#23494;&#38598;&#23376;&#32593;&#32476;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2303.11873</link><description>&lt;p&gt;
&#20004;&#20010;&#30005;&#36335;&#30340;&#25925;&#20107;&#65306;&#31232;&#30095;&#21644;&#23494;&#38598;&#23376;&#32593;&#32476;&#30340;&#31454;&#20105;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks. (arXiv:2303.11873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20869;&#37096;&#32467;&#26500;&#65292;&#21457;&#29616;Grokking&#29616;&#35937;&#23545;&#24212;&#20110;&#31232;&#30095;&#23376;&#32593;&#32476;&#30340;&#20986;&#29616;&#65292;&#35813;&#23376;&#32593;&#32476;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30001;&#20110;&#23569;&#25968;&#31070;&#32463;&#20803;&#30340;&#24555;&#36895;&#21508;&#21521;&#21516;&#24615;&#22686;&#38271;&#32780;&#20986;&#29616;&#65292;&#20174;&#32780;&#19982;&#25903;&#37197;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#23494;&#38598;&#23376;&#32593;&#32476;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#26159;&#25351;&#22312;&#31639;&#27861;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#39318;&#20808;&#20986;&#29616;&#36807;&#25311;&#21512;&#65292;&#20294;&#26159;&#22312;&#22823;&#37327;&#39069;&#22806;&#30340;&#35757;&#32451;&#21518;&#65292;&#20986;&#29616;&#20102;&#23436;&#32654;&#30340;&#27867;&#21270;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#22312;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#19978;&#32463;&#39564;&#22320;&#30740;&#31350;&#20102;&#27491;&#22312;&#32463;&#21382;Grokking&#30340;&#32593;&#32476;&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#24182;&#21457;&#29616;Grokking&#30340;&#30456;&#21464;&#23545;&#24212;&#20110;&#25903;&#37197;&#27169;&#22411;&#39044;&#27979;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#30340;&#20986;&#29616;&#12290;&#22312;&#20248;&#21270;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#23569;&#25968;&#31070;&#32463;&#20803;&#32463;&#21382;&#24555;&#36895;&#30340;&#21508;&#21521;&#21516;&#24615;&#22686;&#38271;&#26102;&#65292;&#36825;&#20010;&#23376;&#32593;&#32476;&#20250;&#20986;&#29616;&#65292;&#32780;&#32593;&#32476;&#20013;&#30340;&#20854;&#20182;&#31070;&#32463;&#20803;&#21017;&#32531;&#24930;&#22320;&#34928;&#20943;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;Grokking&#30340;&#30456;&#21464;&#21487;&#20197;&#29702;&#35299;&#20026;&#20004;&#20010;&#22823;&#19981;&#30456;&#21516;&#30340;&#23376;&#32593;&#32476;&#20043;&#38388;&#30340;&#31454;&#20105;&#65306;&#22312;&#36716;&#21464;&#20043;&#21069;&#25903;&#37197;&#30340;&#26159;&#23494;&#38598;&#23376;&#32593;&#32476;&#65292;&#20294;&#23427;&#27867;&#21270;&#33021;&#21147;&#24456;&#24046;&#65292;&#22312;&#36716;&#21464;&#20043;&#21518;&#25903;&#37197;&#30340;&#26159;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grokking is a phenomenon where a model trained on an algorithmic task first overfits but, then, after a large amount of additional training, undergoes a phase transition to generalize perfectly. We empirically study the internal structure of networks undergoing grokking on the sparse parity task, and find that the grokking phase transition corresponds to the emergence of a sparse subnetwork that dominates model predictions. On an optimization level, we find that this subnetwork arises when a small subset of neurons undergoes rapid norm growth, whereas the other neurons in the network decay slowly in norm. Thus, we suggest that the grokking phase transition can be understood to emerge from competition of two largely distinct subnetworks: a dense one that dominates before the transition and generalizes poorly, and a sparse one that dominates afterwards.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37319;&#29992;&#20102;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#26469;&#26367;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#32780;&#20351;&#24471;&#20351;&#29992;&#36880;&#20010;&#20803;&#32032;&#22320;&#22788;&#29702;&#24207;&#21015;&#65292;&#26356;&#21152;&#36866;&#29992;&#20110;&#22312;&#32447;&#20449;&#21495;&#22788;&#29702;&#65292;&#24182;&#19988;&#22312;&#25163;&#25351;&#20301;&#32622;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#20934;&#30830;&#24615;&#32426;&#24405;&#65292;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#20165;&#38656;&#35201;&#38750;&#24120;&#30701;&#30340;&#26102;&#38388;&#31383;&#21475;(3.5&#27627;&#31186;)&#12290;</title><link>http://arxiv.org/abs/2303.11860</link><description>&lt;p&gt;
&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#22312;&#32447;Transformer&#29992;&#20110;&#24555;&#36895;&#20551;&#32930;&#25163;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Online Transformers with Spiking Neurons for Fast Prosthetic Hand Control. (arXiv:2303.11860v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37319;&#29992;&#20102;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#26469;&#26367;&#20195;Transformer&#20013;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20174;&#32780;&#20351;&#24471;&#20351;&#29992;&#36880;&#20010;&#20803;&#32032;&#22320;&#22788;&#29702;&#24207;&#21015;&#65292;&#26356;&#21152;&#36866;&#29992;&#20110;&#22312;&#32447;&#20449;&#21495;&#22788;&#29702;&#65292;&#24182;&#19988;&#22312;&#25163;&#25351;&#20301;&#32622;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#22909;&#30340;&#20934;&#30830;&#24615;&#32426;&#24405;&#65292;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#20165;&#38656;&#35201;&#38750;&#24120;&#30701;&#30340;&#26102;&#38388;&#31383;&#21475;(3.5&#27627;&#31186;)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32593;&#32476;&#26159;&#22823;&#22810;&#25968;&#24207;&#21015;&#22788;&#29702;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;Transformer&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#38656;&#35201;&#22823;&#30340;&#26102;&#38388;&#31383;&#21475;&#26469;&#36827;&#34892;&#27599;&#20010;&#35745;&#31639;&#27493;&#39588;&#65292;&#22240;&#27492;&#19982;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(RNN)&#30456;&#27604;&#65292;&#20351;&#24471;&#23427;&#20204;&#19981;&#22826;&#36866;&#29992;&#20110;&#22312;&#32447;&#20449;&#21495;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#26469;&#20195;&#26367;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26426;&#21046;&#23545;&#20110;&#22312;&#36755;&#20837;&#21644;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#33539;&#22260;&#20381;&#36182;&#30340;&#36830;&#32493;&#20449;&#21495;&#26356;&#20026;&#39640;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#23427;&#26469;&#36880;&#20010;&#20803;&#32032;&#22320;&#22788;&#29702;&#24207;&#21015;&#65292;&#22240;&#27492;&#20351;&#20854;&#36866;&#29992;&#20110;&#22312;&#32447;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25351;&#23574;&#20301;&#32622;&#22238;&#24402;&#25968;&#25454;&#38598;(NinaproDB8)&#19978;&#27979;&#35797;&#20102;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#29992;&#22312;&#21069;&#33218;&#30382;&#32932;&#19978;&#27979;&#37327;&#30340;Surface Electromyographic (sEMG)&#20449;&#21495;&#26469;&#20272;&#35745;&#32908;&#32905;&#27963;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#20013;&#20165;&#38656;&#35201;&#38750;&#24120;&#30701;&#30340;&#26102;&#38388;&#31383;&#21475;(3.5&#27627;&#31186;)&#23601;&#33021;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#26032;&#30340;&#20934;&#30830;&#24615;&#32426;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are state-of-the-art networks for most sequence processing tasks. However, the self-attention mechanism often used in Transformers requires large time windows for each computation step and thus makes them less suitable for online signal processing compared to Recurrent Neural Networks (RNNs). In this paper, instead of the self-attention mechanism, we use a sliding window attention mechanism. We show that this mechanism is more efficient for continuous signals with finite-range dependencies between input and target, and that we can use it to process sequences element-by-element, this making it compatible with online processing. We test our model on a finger position regression dataset (NinaproDB8) with Surface Electromyographic (sEMG) signals measured on the forearm skin to estimate muscle activities. Our approach sets the new state-of-the-art in terms of accuracy on this dataset while requiring only very short time windows of 3.5 ms at each inference step. Moreover, we inc
&lt;/p&gt;</description></item><item><title>Dens-PU&#26159;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340;PU Learning&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#19988;&#22312;&#22522;&#20934;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11848</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340; PU Learning &#26041;&#27861;&#65306;Dens-PU
&lt;/p&gt;
&lt;p&gt;
Dens-PU: PU Learning with Density-Based Positive Labeled Augmentation. (arXiv:2303.11848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11848
&lt;/p&gt;
&lt;p&gt;
Dens-PU&#26159;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340;PU Learning&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#19988;&#22312;&#22522;&#20934;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; PU Learning &#26041;&#27861;&#65292;&#22522;&#20110;&#24322;&#24120;&#26816;&#27979;&#31574;&#30053;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20174;&#27491;&#26679;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#28508;&#22312;&#32534;&#30721;&#32447;&#24615;&#32452;&#21512;&#20197;&#33719;&#24471;&#26032;&#26679;&#26412;&#12290;&#36825;&#20123;&#26032;&#26679;&#26412;&#34987;&#29992;&#20316;&#23884;&#20837;&#20197;&#22686;&#21152;&#27491;&#26679;&#26412;&#25968;&#25454;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#23450;&#20041;&#36817;&#20284;&#27491;&#31867;&#30340;&#36793;&#30028;&#12290;&#26679;&#26412;&#36317;&#31163;&#36793;&#30028;&#36234;&#36828;&#65292;&#21017;&#35748;&#20026;&#23427;&#26159;&#36127;&#26679;&#26412;&#30340;&#21487;&#33021;&#24615;&#36234;&#22823;&#12290;&#19968;&#26086;&#33719;&#24471;&#19968;&#32452;&#36127;&#26679;&#26412;&#65292;PU Learning &#38382;&#39064;&#23601;&#36716;&#21270;&#20026;&#20108;&#20803;&#20998;&#31867;&#12290;&#21517;&#20026; Dens-PU &#30340;&#26041;&#27861;&#65292;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#27491;&#26679;&#26412;&#25968;&#25454;&#30340;&#23494;&#24230;&#65292;&#32463;&#36807;&#22522;&#20934;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a novel approach for solving the PU learning problem based on an anomaly-detection strategy. Latent encodings extracted from positive-labeled data are linearly combined to acquire new samples. These new samples are used as embeddings to increase the density of positive-labeled data and, thus, define a boundary that approximates the positive class. The further a sample is from the boundary the more it is considered as a negative sample. Once a set of negative samples is obtained, the PU learning problem reduces to binary classification. The approach, named Dens-PU due to its reliance on the density of positive-labeled data, was evaluated using benchmark image datasets, and state-of-the-art results were attained.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#27491;&#21017;&#21270;&#29109;Wasserstein&#37325;&#24515;&#20844;&#24335;&#65292;&#20855;&#26377;&#22909;&#30340;&#27491;&#21017;&#21270;&#12289;&#36924;&#36817;&#12289;&#31283;&#23450;&#24615;&#21644;&#65288;&#26080;&#32593;&#26684;&#65289;&#20248;&#21270;&#29305;&#24615;; &#20854;&#20013;&#65292;&#21482;&#26377;&#22312;$\tau=\lambda/2$&#30340;&#24773;&#20917;&#19979;&#26159;&#26080;&#20559;&#24046;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.11844</link><description>&lt;p&gt;
&#21452;&#37325;&#27491;&#21017;&#21270;&#29109; Wasserstein &#37325;&#24515;
&lt;/p&gt;
&lt;p&gt;
Doubly Regularized Entropic Wasserstein Barycenters. (arXiv:2303.11844v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#27491;&#21017;&#21270;&#29109;Wasserstein&#37325;&#24515;&#20844;&#24335;&#65292;&#20855;&#26377;&#22909;&#30340;&#27491;&#21017;&#21270;&#12289;&#36924;&#36817;&#12289;&#31283;&#23450;&#24615;&#21644;&#65288;&#26080;&#32593;&#26684;&#65289;&#20248;&#21270;&#29305;&#24615;; &#20854;&#20013;&#65292;&#21482;&#26377;&#22312;$\tau=\lambda/2$&#30340;&#24773;&#20917;&#19979;&#26159;&#26080;&#20559;&#24046;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#24120;&#35268;&#30340;&#27491;&#21017;&#21270;Wasserstein&#37325;&#24515;&#30340;&#20844;&#24335;&#65292;&#36825;&#20010;&#20844;&#24335;&#20855;&#26377;&#33391;&#22909;&#30340;&#27491;&#21017;&#21270;&#12289;&#36924;&#36817;&#12289;&#31283;&#23450;&#24615;&#21644;&#65288;&#26080;&#32593;&#26684;&#65289;&#20248;&#21270;&#29305;&#24615;&#12290;&#36825;&#20010;&#37325;&#24515;&#34987;&#23450;&#20041;&#20026;&#21807;&#19968;&#19968;&#31181;&#26368;&#23567;&#21270;&#20851;&#20110;&#19968;&#26063;&#32473;&#23450;&#27010;&#29575;&#27979;&#24230;&#30340;&#29109;&#26368;&#20248;&#36755;&#36816;&#65288;EOT&#65289;&#25104;&#26412;&#20043;&#21644;&#21450;&#29109;&#39033;&#30340;&#27010;&#29575;&#27979;&#24230;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;$(\lambda,\tau)$-&#37325;&#24515;&#65292;&#20854;&#20013;&#65292;$\lambda$ &#26159;&#20869;&#37096;&#27491;&#21017;&#21270;&#24378;&#24230;&#65292;$\tau$ &#26159;&#22806;&#37096;&#27491;&#21017;&#21270;&#24378;&#24230;&#12290;&#36825;&#31181;&#20844;&#24335;&#24674;&#22797;&#20102;&#24050;&#32463;&#25552;&#20986;&#30340;&#22810;&#31181;EOT&#37325;&#24515;&#65292;&#36866;&#21512;&#20110;&#19981;&#21516;&#30340; $\lambda, \tau \geq 0$ &#36873;&#25321;&#65292;&#24182;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#39318;&#20808;&#65292;&#23613;&#31649;&#20855;&#26377;&#21452;&#37325;&#27491;&#21017;&#21270;&#65292;&#20294;&#22312;$\tau=\lambda/2$ &#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20844;&#24335;&#26159;&#26080;&#20559;&#30340;: &#23545;&#20110;&#20809;&#28369;&#23494;&#24230;&#65292;&#65288;&#26410;&#27491;&#21017;&#21270;&#30340;&#65289;Wasserstein &#37325;&#24515;&#30446;&#26631;&#20989;&#25968;&#20013;&#30340;&#27425;&#20248;&#24615;&#26159;&#29109;&#27491;&#21017;&#21270;&#24378;&#24230;$\lambda^2$&#30340;&#65292;&#32780;&#19981;&#26159;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;$\max \{\lambda, \tau\}$&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a general formulation of regularized Wasserstein barycenters that enjoys favorable regularity, approximation, stability and (grid-free) optimization properties. This barycenter is defined as the unique probability measure that minimizes the sum of entropic optimal transport (EOT) costs with respect to a family of given probability measures, plus an entropy term. We denote it $(\lambda,\tau)$-barycenter, where $\lambda$ is the inner regularization strength and $\tau$ the outer one. This formulation recovers several previously proposed EOT barycenters for various choices of $\lambda,\tau \geq 0$ and generalizes them. First, in spite of -- and in fact owing to -- being \emph{doubly} regularized, we show that our formulation is debiased for $\tau=\lambda/2$: the suboptimality in the (unregularized) Wasserstein barycenter objective is, for smooth densities, of the order of the strength $\lambda^2$ of entropic regularization, instead of $\max\{\lambda,\tau\}$ in general. We discuss 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36880;&#23618;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20869;&#32622;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;CNN&#29305;&#24449;&#30340;Lipschitz&#24120;&#25968;&#20316;&#20026;&#40065;&#26834;&#24615;&#24230;&#37327;&#65292;&#24182;&#20351;&#29992;Cayley&#21464;&#25442;&#21644;&#21487;&#25511;&#24615;Gram&#30697;&#26469;&#23454;&#29616;CNN&#30340;Lipschitz&#36830;&#32493;&#24615;&#21644;&#26080;&#32422;&#26463;&#35757;&#32451;&#65292;&#26368;&#21518;&#22312;&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11835</link><description>&lt;p&gt;
&#21033;&#29992;Cayley&#21464;&#25442;&#21644;&#21487;&#25511;&#24615;Gram&#30697;&#30340;Lipschitz-bounded 1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(arXiv:2303.11835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Lipschitz-bounded 1D convolutional neural networks using the Cayley transform and the controllability Gramian. (arXiv:2303.11835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36880;&#23618;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20869;&#32622;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;CNN&#29305;&#24449;&#30340;Lipschitz&#24120;&#25968;&#20316;&#20026;&#40065;&#26834;&#24615;&#24230;&#37327;&#65292;&#24182;&#20351;&#29992;Cayley&#21464;&#25442;&#21644;&#21487;&#25511;&#24615;Gram&#30697;&#26469;&#23454;&#29616;CNN&#30340;Lipschitz&#36830;&#32493;&#24615;&#21644;&#26080;&#32422;&#26463;&#35757;&#32451;&#65292;&#26368;&#21518;&#22312;&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#29992;&#20110;1D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#36880;&#23618;&#21442;&#25968;&#21270;&#65292;&#20855;&#26377;&#20869;&#32622;&#30340;&#31471;&#21040;&#31471;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;CNN&#29305;&#24449;&#30340;Lipschitz&#24120;&#25968;&#20316;&#20026;&#40065;&#26834;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#22522;&#20110;Cayley&#21464;&#25442;&#23545;&#27491;&#20132;&#30697;&#38453;&#36827;&#34892;&#21442;&#25968;&#21270;&#20197;&#21450;&#23545;&#21367;&#31215;&#23618;&#30340;&#29366;&#24577;&#31354;&#38388;&#34920;&#24449;&#30340;&#21487;&#25511;&#24615;Gram&#30697;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#25968;&#21270;&#35774;&#35745;&#28385;&#36275;&#32447;&#24615;&#30697;&#38453;&#19981;&#31561;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;CNN&#30340;Lipschitz&#36830;&#32493;&#24615;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;Lipschitz-bounded 1D CNNs&#30340;&#26080;&#32422;&#26463;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#36827;&#34892;Lipschitz-bounded 1D CNNs&#30340;&#20998;&#31867;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25913;&#36827;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a layer-wise parameterization for 1D convolutional neural networks (CNNs) with built-in end-to-end robustness guarantees. Herein, we use the Lipschitz constant of the input-output mapping characterized by a CNN as a robustness measure. We base our parameterization on the Cayley transform that parameterizes orthogonal matrices and the controllability Gramian for the state space representation of the convolutional layers. The proposed parameterization by design fulfills linear matrix inequalities that are sufficient for Lipschitz continuity of the CNN, which further enables unconstrained training of Lipschitz-bounded 1D CNNs. Finally, we train Lipschitz-bounded 1D CNNs for the classification of heart arrythmia data and show their improved robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32452;&#21512;&#21270;&#23398;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#65292;&#21487;&#20197;&#21457;&#29616;&#26410;&#30693;&#26448;&#26009;&#24182;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#36136;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#27604;&#27010;&#29575;&#20998;&#24067;&#23398;&#20064;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#21457;&#29616;&#26356;&#22909;&#30340;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2303.11833</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32452;&#21512;&#21270;&#23398;&#22312;&#26497;&#31471;&#29305;&#24615;&#26448;&#26009;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Materials Discovery with Extreme Properties via AI-Driven Combinatorial Chemistry. (arXiv:2303.11833v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32452;&#21512;&#21270;&#23398;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#65292;&#21487;&#20197;&#21457;&#29616;&#26410;&#30693;&#26448;&#26009;&#24182;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#36136;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#27604;&#27010;&#29575;&#20998;&#24067;&#23398;&#20064;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#21457;&#29616;&#26356;&#22909;&#30340;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26448;&#26009;&#30340;&#21457;&#29616;&#37117;&#26088;&#22312;&#21457;&#29616;&#27604;&#30446;&#21069;&#24050;&#30693;&#26448;&#26009;&#26356;&#20248;&#36234;&#30340;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#36825;&#24456;&#25509;&#36817;&#20110;&#22806;&#25512;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#23398;&#20064;&#25968;&#25454;&#27010;&#29575;&#20998;&#24067;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#36825;&#26159;&#19968;&#20010;&#24369;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#32452;&#21512;&#21270;&#23398;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#21453;&#21521;&#20998;&#23376;&#35774;&#35745;&#22120;&#65292;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#21487;&#33021;&#29983;&#25104;&#20174;&#20998;&#23376;&#29255;&#27573;&#32452;&#21512;&#20013;&#33719;&#24471;&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#20197;&#21457;&#29616;&#20855;&#26377;&#26356;&#20248;&#36234;&#24615;&#36136;&#30340;&#26410;&#30693;&#26448;&#26009;&#12290;&#25105;&#20204;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#27010;&#29575;&#20998;&#24067;&#23398;&#20064;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#21457;&#29616;&#26356;&#22909;&#30340;&#26448;&#26009;&#12290;&#22312;&#19968;&#20010;&#26088;&#22312;&#21457;&#29616;&#19971;&#20010;&#30446;&#26631;&#29305;&#24615;&#20998;&#23376;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;10&#19975;&#27425;&#35797;&#39564;&#20013;&#21457;&#29616;&#20102;1315&#20010;&#36798;&#21040;&#20840;&#37096;&#30446;&#26631;&#30340;&#20998;&#23376;&#21644;7629&#20010;&#36798;&#21040;&#20116;&#20010;&#30446;&#26631;&#30340;&#20998;&#23376;&#65292;&#32780;&#27010;&#29575;&#20998;&#24067;&#23398;&#20064;&#27169;&#22411;&#21482;&#26377;&#21457;&#29616;&#20960;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of most materials discovery is to discover materials that are superior to those currently known. Fundamentally, this is close to extrapolation, which is a weak point for most machine learning models that learn the probability distribution of data. Herein, we develop AI-driven combinatorial chemistry, which is a rule-based inverse molecular designer that does not rely on data. Since our model has the potential to generate all possible molecular structures that can be obtained from combinations of molecular fragments, unknown materials with superior properties can be discovered. We theoretically and empirically demonstrate that our model is more suitable for discovering better materials than probability distribution-learning models. In an experiment aimed at discovering molecules that hit seven target properties, our model discovered 1,315 of all target-hitting molecules and 7,629 of five target-hitting molecules out of 100,000 trials, whereas the probability distribution-learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21152;&#36895;&#20840;&#33145;MRI&#25195;&#25551;&#30340;&#26032;&#26041;&#27861;GLADE&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#26144;&#23556;&#25439;&#22833;&#26469;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;3D&#33145;&#37096;MR&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.11831</link><description>&lt;p&gt;
GLADE&#65306;&#29992;&#20110;&#38750;&#37197;&#23545;&#36229;&#20998;&#36776;&#29575;&#21508;&#21521;&#24322;&#24615;MRI&#30340;&#26799;&#24230;&#25439;&#22833;&#22686;&#24378;&#36864;&#21270;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
GLADE: Gradient Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic MRI. (arXiv:2303.11831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#21152;&#36895;&#20840;&#33145;MRI&#25195;&#25551;&#30340;&#26032;&#26041;&#27861;GLADE&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#26144;&#23556;&#25439;&#22833;&#26469;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;3D&#33145;&#37096;MR&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#38750;&#37197;&#23545;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#21508;&#21521;&#24322;&#24615;3D&#22270;&#20687;&#20013;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;3D&#33145;&#37096;MR&#22270;&#20687;&#12290;&#36890;&#36807;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;CycleGAN&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#26144;&#23556;&#25439;&#22833;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#21508;&#21521;&#24322;&#24615;&#20307;&#31215;&#39640;&#20998;&#36776;&#29575;&#65288;&#38754;&#20869;&#65289;&#25968;&#25454;&#30340;&#19981;&#37325;&#21472;&#30340;&#34917;&#19969;&#65292;&#24378;&#21046;&#32593;&#32476;&#29983;&#25104;&#22120;&#22686;&#21152;&#20302;&#20998;&#36776;&#29575;&#65288;&#38754;&#22806;&#65289;&#20999;&#29255;&#30340;&#20998;&#36776;&#29575;&#12290;&#36825;&#23558;&#20351;&#22312;&#30701;&#26102;&#38388;&#20869;&#20197;&#39640;&#20998;&#36776;&#29575;&#31561;&#21521;&#24615;&#22270;&#20687;&#36827;&#34892;&#20840;&#33145;&#25195;&#25551;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to synthesise high-resolution isotropic 3D abdominal MR images, from anisotropic 3D images in an unpaired fashion. Using a modified CycleGAN architecture with a gradient mapping loss, we leverage disjoint patches from the high-resolution (in-plane) data of an anisotropic volume to enforce the network generator to increase the resolution of the low-resolution (through-plane) slices. This will enable accelerated whole-abdomen scanning with high-resolution isotropic images within short breath-hold times.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#65288;FSSL-CVI&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#21160;&#24577;&#31867;&#21035;&#21152;&#26435;&#26041;&#26696;&#26469;&#22788;&#29702;FSSL&#20013;&#30340;&#31867;&#21035;&#21464;&#37327;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;&#26412;&#25991;&#34920;&#26126; FSSL-CVI &#26041;&#27861;&#22312;&#21508;&#26041;&#38754;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#21644;FSSL &#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11809</link><description>&lt;p&gt;
&#35299;&#20915;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Class Variable Imbalance in Federated Semi-supervised Learning. (arXiv:2303.11809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#65288;FSSL-CVI&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#21160;&#24577;&#31867;&#21035;&#21152;&#26435;&#26041;&#26696;&#26469;&#22788;&#29702;FSSL&#20013;&#30340;&#31867;&#21035;&#21464;&#37327;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;&#26412;&#25991;&#34920;&#26126; FSSL-CVI &#26041;&#27861;&#22312;&#21508;&#26041;&#38754;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#21644;FSSL &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;FSSL&#65289;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#22312;&#19981;&#38656;&#35201;&#23558;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#20110;&#19968;&#22788;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20250;&#22312;&#35774;&#22791;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#21518;&#25910;&#38598;&#27169;&#22411;&#35757;&#32451;&#26356;&#26032;&#65292;&#20174;&#32780;&#21487;&#20197;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#19968;&#20123;&#35774;&#22791;&#26080;&#27861;&#25910;&#38598;&#36275;&#22815;&#30340;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#26032;&#35774;&#22791;&#23558;&#34987;&#28155;&#21152;&#21040;&#32452;&#35757;&#32451;&#20013;&#12290;&#36825;&#23548;&#33268;&#19981;&#24179;&#34913;&#30340;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#22266;&#23450;&#31867;&#21035;&#25968;&#37327;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#32780;&#24456;&#23569;&#26377;&#27880;&#24847;&#21147;&#25918;&#22312;&#20855;&#26377;&#21487;&#21464;&#31867;&#21035;&#25968;&#37327;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#19978;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#21322;&#30417;&#30563;&#23398;&#20064;&#19982;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#65288;FSSL-CVI&#65289;&#65292;&#23427;&#20351;&#29992;&#21160;&#24577;&#31867;&#26435;&#37325;&#26041;&#26696;&#26469;&#35299;&#20915;FSSL&#20013;&#30340;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#31867;&#21464;&#37327;&#19981;&#24179;&#34913;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#21644;FSSL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Semi-supervised Learning (FSSL) combines techniques from both fields of federated and semi-supervised learning to improve the accuracy and performance of models in a distributed environment by using a small fraction of labeled data and a large amount of unlabeled data. Without the need to centralize all data in one place for training, it collect updates of model training after devices train models at local, and thus can protect the privacy of user data. However, during the federal training process, some of the devices fail to collect enough data for local training, while new devices will be included to the group training. This leads to an unbalanced global data distribution and thus affect the performance of the global model training. Most of the current research is focusing on class imbalance with a fixed number of classes, while little attention is paid to data imbalance with a variable number of classes. Therefore, in this paper, we propose Federated Semi-supervised Learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#35813;&#31639;&#27861;&#22312;&#32593;&#32476;&#22270;&#20026;&#36830;&#36890;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.11789</link><description>&lt;p&gt;
&#22270;&#19978;&#38543;&#26426;&#36870;&#38382;&#39064;&#65306;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random Inverse Problems Over Graphs: Decentralized Online Learning. (arXiv:2303.11789v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#35813;&#31639;&#27861;&#22312;&#32593;&#32476;&#22270;&#20026;&#36830;&#36890;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#38543;&#26426;&#36870;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#35813;&#38382;&#39064;&#20855;&#26377;&#23454;&#26102;&#30340;&#22270;&#19978;&#35266;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25910;&#25947;&#24615;&#36716;&#21270;&#20026;&#24102;&#26377;L2&#26377;&#30028;&#38789;&#24046;&#20998;&#39033;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#38543;&#26426;&#26102;&#21464;&#24046;&#20998;&#26041;&#31243;&#30340;&#28176;&#36817;&#31283;&#23450;&#24615;&#65292;&#24182;&#21457;&#23637;&#20102;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#32593;&#32476;&#22270;&#26159;&#36830;&#36890;&#30340;&#65292;&#24182;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#65292;&#21017;&#25152;&#26377;&#33410;&#28857;&#30340;&#20272;&#35745;&#22343;&#20026;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#30340;&#12290;&#36890;&#36807;&#23558;RKHS&#20013;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#31561;&#25928;&#22320;&#36716;&#21270;&#20026;&#22270;&#19978;&#38543;&#26426;&#36870;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#20013;&#24515;&#33410;&#28857;&#30340;RKHS&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into the asymptotic stability of randomly time-varying difference equations in Hilbert space with L2-bounded martingale difference terms and develop the L2 -asymptotic stability theory. It is shown that if the network graph is connected and the sequence of forward operators satisfies the infinitedimensional spatio-temporal persistence of excitation condition, then the estimates of all nodes are mean square and almost surely strongly consistent. By equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on no
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#22788;&#29702;&#20302;&#32500;&#27969;&#24418;&#25968;&#25454;&#30340;&#22238;&#24402;&#26694;&#26550;&#65292;&#39318;&#20808;&#36890;&#36807;&#26500;&#24314;&#22270;&#24418;&#39592;&#26550;&#26469;&#25429;&#25417;&#28508;&#22312;&#30340;&#27969;&#24418;&#20960;&#20309;&#32467;&#26500;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#36816;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#26469;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#65292;&#38500;&#20102;&#20855;&#26377;&#38750;&#21442;&#25968;&#20248;&#28857;&#20043;&#22806;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#27969;&#24418;&#25968;&#25454;&#65292;&#22024;&#26434;&#35266;&#23519;&#26102;&#20063;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11786</link><description>&lt;p&gt;
Skeleton Regression&#65306;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#32467;&#26500;&#20272;&#35745;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure. (arXiv:2303.11786v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#22788;&#29702;&#20302;&#32500;&#27969;&#24418;&#25968;&#25454;&#30340;&#22238;&#24402;&#26694;&#26550;&#65292;&#39318;&#20808;&#36890;&#36807;&#26500;&#24314;&#22270;&#24418;&#39592;&#26550;&#26469;&#25429;&#25417;&#28508;&#22312;&#30340;&#27969;&#24418;&#20960;&#20309;&#32467;&#26500;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#36816;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#26469;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#65292;&#38500;&#20102;&#20855;&#26377;&#38750;&#21442;&#25968;&#20248;&#28857;&#20043;&#22806;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#27969;&#24418;&#25968;&#25454;&#65292;&#22024;&#26434;&#35266;&#23519;&#26102;&#20063;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22238;&#24402;&#26694;&#26550;&#65292;&#26088;&#22312;&#22788;&#29702;&#22260;&#32469;&#20302;&#32500;&#27969;&#24418;&#30340;&#22797;&#26434;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#22270;&#24418;&#34920;&#31034;&#65292;&#31216;&#20026;&#39592;&#26550;&#65292;&#20197;&#25429;&#33719;&#28508;&#22312;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#39592;&#26550;&#22270;&#19978;&#23450;&#20041;&#25351;&#26631;&#65292;&#24212;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#25216;&#26415;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#24418;&#30340;&#29305;&#24449;&#36716;&#25442;&#26469;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#12290;&#38500;&#20102;&#21253;&#25324;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;&#38750;&#21442;&#25968;&#22238;&#24402;&#22120;&#22312;&#39592;&#26550;&#22270;&#31561;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;&#25152;&#25552;&#20986;&#30340;&#22238;&#24402;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36991;&#24320;&#32500;&#24230;&#28798;&#38590;&#65292;&#20855;&#26377;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#27969;&#24418;&#30340;&#24182;&#38598;&#24182;&#19988;&#40065;&#26834;&#24615;&#33021;&#24212;&#23545;&#21152;&#24615;&#22122;&#22768;&#21644;&#22024;&#26434;&#35266;&#23519;&#30340;&#39069;&#22806;&#20248;&#21183;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new regression framework designed to deal with large-scale, complex data that lies around a low-dimensional manifold. Our approach first constructs a graph representation, referred to as the skeleton, to capture the underlying geometric structure. We then define metrics on the skeleton graph and apply nonparametric regression techniques, along with feature transformations based on the graph, to estimate the regression function. In addition to the included nonparametric methods, we also discuss the limitations of some nonparametric regressors with respect to the general metric space such as the skeleton graph. The proposed regression framework allows us to bypass the curse of dimensionality and provides additional advantages that it can handle the union of multiple manifolds and is robust to additive noise and noisy observations. We provide statistical guarantees for the proposed method and demonstrate its effectiveness through simulations and real data examples.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.11783</link><description>&lt;p&gt;
&#36731;&#37327;&#32423;&#23545;&#27604;&#34507;&#30333;&#36136;&#32467;&#26500;-&#24207;&#21015;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Lightweight Contrastive Protein Structure-Sequence Transformation. (arXiv:2303.11783v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#34507;&#30333;&#36136;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#26080;&#26631;&#31614;&#30340;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#32467;&#26500;&#27169;&#22411;&#26159;&#20851;&#38190;&#22522;&#30784;&#12290;&#20256;&#32479;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#26041;&#27861;&#36981;&#24490;&#25104;&#29087;&#30340;&#33258;&#28982;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20363;&#22914;&#21435;&#22122;&#37325;&#26500;&#21644;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#20294;&#36890;&#24120;&#20250;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#12290;&#20854;&#20182;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#21487;&#33021;&#20250;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#23450;&#23545;&#35937;&#31867;&#21035;&#65292;&#20854;&#20013;&#21463;&#38480;&#30340;&#30417;&#30563;&#26041;&#24335;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#25351;&#23450;&#20219;&#20309;&#20854;&#20182;&#30340;&#34507;&#30333;&#36136;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#35758;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#23545;&#40784;&#26469;&#25351;&#23548;&#32467;&#26500;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#20197;&#36827;&#19968;&#27493;&#23398;&#20064;&#20869;&#22312;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained protein structure models without labels are crucial foundations for the majority of protein downstream applications. The conventional structure pretraining methods follow the mature natural language pretraining methods such as denoised reconstruction and masked language modeling but usually destroy the real representation of spatial structures. The other common pretraining methods might predict a fixed set of predetermined object categories, where a restricted supervised manner limits their generality and usability as additional labeled data is required to specify any other protein concepts. In this work, we introduce a novel unsupervised protein structure representation pretraining with a robust protein language model. In particular, we first propose to leverage an existing pretrained language model to guide structure model learning through an unsupervised contrastive alignment. In addition, a self-supervised structure constraint is proposed to further learn the intrinsic i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#23454;&#20102;Rademacher&#38543;&#26426;&#25237;&#24433;&#30340;&#38750;&#36951;&#24536;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20102;Schur-&#20985;&#24615;&#36136;&#12290;&#36825;&#39033;&#25104;&#26524;&#20026;&#38543;&#26426;&#25237;&#24433;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#20960;&#20309;&#35270;&#35282;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37327;&#21270;&#30028;&#38480;&#65292;&#22635;&#34917;&#20102;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.11774</link><description>&lt;p&gt;
Rademacher&#38543;&#26426;&#23884;&#20837;&#30340;&#31934;&#30830;&#38750;&#36951;&#24536;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Exact Non-Oblivious Performance of Rademacher Random Embeddings. (arXiv:2303.11774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#23454;&#20102;Rademacher&#38543;&#26426;&#25237;&#24433;&#30340;&#38750;&#36951;&#24536;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20102;Schur-&#20985;&#24615;&#36136;&#12290;&#36825;&#39033;&#25104;&#26524;&#20026;&#38543;&#26426;&#25237;&#24433;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#20960;&#20309;&#35270;&#35282;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37327;&#21270;&#30028;&#38480;&#65292;&#22635;&#34917;&#20102;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;Rademacher&#38543;&#26426;&#25237;&#24433;&#30340;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#26032;&#39062;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#19982;&#36755;&#20837;&#25968;&#25454;&#30456;&#27604;&#20855;&#26377;&#25968;&#20540;&#19978;&#30340;&#31934;&#24230;&#21644;&#38750;&#36951;&#24536;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20013;&#24515;&#32467;&#26524;&#26159;Rademacher&#38543;&#26426;&#25237;&#24433;&#19982;&#36755;&#20837;&#30340;Schur-&#20985;&#24615;&#36136;&#12290;&#36825;&#25552;&#20379;&#20102;&#38543;&#26426;&#25237;&#24433;&#24615;&#33021;&#30340;&#26032;&#39062;&#20960;&#20309;&#35270;&#35282;&#65292;&#21516;&#26102;&#22312;&#27604;&#20197;&#21069;&#30340;&#30740;&#31350;&#26356;&#22909;&#22320;&#25552;&#39640;&#20102;&#37327;&#21270;&#30028;&#38480;&#12290;&#20316;&#20026;&#36825;&#19968;&#26356;&#24191;&#27867;&#32467;&#26524;&#30340;&#25512;&#35770;&#65292;&#25105;&#20204;&#22312;&#31232;&#30095;&#25968;&#25454;&#25110;&#20998;&#24067;&#20855;&#26377;&#23567;&#33539;&#22260;&#30340;&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#38750;&#36951;&#24536;&#24615;&#20998;&#26512;&#19982;&#20197;&#21069;&#30340;&#25216;&#26415;&#30456;&#27604;&#26159;&#19968;&#31181;&#26032;&#22855;&#20043;&#22788;&#65292;&#24182;&#24357;&#21512;&#20102;&#29702;&#35770;&#21644;&#23454;&#36341;&#20043;&#38388;&#32463;&#24120;&#35266;&#23519;&#21040;&#30340;&#24046;&#36317;&#12290;&#20027;&#35201;&#32467;&#26524;&#20351;&#29992;&#20195;&#25968;&#26694;&#26550;&#26469;&#35777;&#26126;Schur-&#20985;&#24615;&#36136;&#65292;&#36825;&#26159;&#19968;&#20010;&#29420;&#31435;&#20110;&#20852;&#36259;&#30340;&#36129;&#29486;&#65292;&#24182;&#19988;&#26159;&#23548;&#25968;&#22522;&#30784;&#26631;&#20934;&#30340;&#19968;&#31181;&#20248;&#38597;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits the performance of Rademacher random projections, establishing novel statistical guarantees that are numerically sharp and non-oblivious with respect to the input data. More specifically, the central result is the Schur-concavity property of Rademacher random projections with respect to the inputs. This offers a novel geometric perspective on the performance of random projections, while improving quantitatively on bounds from previous works. As a corollary of this broader result, we obtained the improved performance on data which is sparse or is distributed with small spread. This non-oblivious analysis is a novelty compared to techniques from previous work, and bridges the frequently observed gap between theory and practise. The main result uses an algebraic framework for proving Schur-concavity properties, which is a contribution of independent interest and an elegant alternative to derivative-based criteria.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#35774;&#35745;&#30340;&#24320;&#28304;&#26694;&#26550;Metaflow&#65292;&#20854;&#21487;&#20197;&#36890;&#36807;&#23558;ML&#20195;&#30721;&#30340;&#25191;&#34892;&#19982;&#19994;&#21153;&#36923;&#36753;&#30340;&#23450;&#20041;&#20998;&#31163;&#65292;&#26469;&#25552;&#39640;&#25968;&#25454;&#20174;&#19994;&#32773;&#30340;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11761</link><description>&lt;p&gt;
&#37319;&#29992;&#24320;&#28304;Metaflow&#36827;&#34892;&#21512;&#29702;&#35268;&#27169;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reasonable Scale Machine Learning with Open-Source Metaflow. (arXiv:2303.11761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#39033;&#30446;&#35774;&#35745;&#30340;&#24320;&#28304;&#26694;&#26550;Metaflow&#65292;&#20854;&#21487;&#20197;&#36890;&#36807;&#23558;ML&#20195;&#30721;&#30340;&#25191;&#34892;&#19982;&#19994;&#21153;&#36923;&#36753;&#30340;&#23450;&#20041;&#20998;&#31163;&#65292;&#26469;&#25552;&#39640;&#25968;&#25454;&#20174;&#19994;&#32773;&#30340;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21508;&#34892;&#21508;&#19994;&#21644;&#26032;&#30340;&#29992;&#20363;&#20013;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#20174;&#19994;&#32773;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#22312;&#26377;&#25928;&#24320;&#21457;&#21644;&#36845;&#20195;ML&#31995;&#32479;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65306;&#37325;&#29616;&#24615;&#12289;&#35843;&#35797;&#12289;&#21487;&#20280;&#32553;&#24615;&#21644;&#25991;&#26723;&#32534;&#20889;&#23545;&#20110;&#25216;&#26415;&#31532;&#19968;&#31867;&#20844;&#21496;&#20197;&#22806;&#30340;&#30495;&#23454;&#19990;&#30028;&#31649;&#36947;&#26469;&#35828;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#38754;&#21521;ML&#30340;&#24037;&#20316;&#36127;&#36733;&#30340;&#24615;&#36136;&#65292;&#24182;&#35748;&#20026;&#37325;&#26032;&#21033;&#29992;&#29616;&#26377;&#24037;&#20855;&#26080;&#27861;&#35299;&#20915;&#24403;&#21069;&#29983;&#20135;&#21147;&#38382;&#39064;&#65292;&#22240;&#20026;ML&#30340;&#29305;&#27530;&#24615;&#38656;&#35201;&#19987;&#38376;&#30340;&#24320;&#21457;&#24037;&#20855;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Metaflow&#65292;&#19968;&#20010;&#19987;&#38376;&#20026;ML&#39033;&#30446;&#35774;&#35745;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;ML&#20195;&#30721;&#30340;&#25191;&#34892;&#19982;&#19994;&#21153;&#36923;&#36753;&#30340;&#23450;&#20041;&#20998;&#31163;&#26469;&#25552;&#39640;&#25968;&#25454;&#20174;&#19994;&#32773;&#30340;&#29983;&#20135;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#22914;&#20309;&#35299;&#20915;ML&#25805;&#20316;&#65288;MLOps&#65289;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#31034;&#20363;&#12289;&#37319;&#35775;&#21644;&#29992;&#20363;&#35760;&#24405;&#20102;&#23427;&#23545;&#35813;&#39046;&#22495;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Machine Learning (ML) gains adoption across industries and new use cases, practitioners increasingly realize the challenges around effectively developing and iterating on ML systems: reproducibility, debugging, scalability, and documentation are elusive goals for real-world pipelines outside tech-first companies. In this paper, we review the nature of ML-oriented workloads and argue that re-purposing existing tools won't solve the current productivity issues, as ML peculiarities warrant specialized development tooling. We then introduce Metaflow, an open-source framework for ML projects explicitly designed to boost the productivity of data practitioners by abstracting away the execution of ML code from the definition of the business logic. We show how our design addresses the main challenges in ML operations (MLOps), and document through examples, interviews and use cases its practical impact on the field.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#28145;&#24230;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#24863;&#30693;&#27169;&#24577;&#20013;&#33719;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#22320;&#22270;&#20013;&#30340;&#26041;&#24335;&#26469;&#26356;&#26032;&#28508;&#22312;&#22320;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#34920;&#38754;&#20449;&#24687;&#30340;&#24863;&#30693;&#21644;&#35782;&#21035;&#65292;&#22312;&#19981;&#21516;&#34920;&#38754;&#19978;&#30340;&#31934;&#20934;&#25805;&#32437;&#24471;&#21040;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2303.11756</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#27169;&#24577;&#28508;&#22312;&#26144;&#23556;&#34920;&#38754;&#26469;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#28145;&#24230;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Deep Dynamics Models for Autonomous Vehicles with Multimodal Latent Mapping of Surfaces. (arXiv:2303.11756v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#28145;&#24230;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#24863;&#30693;&#27169;&#24577;&#20013;&#33719;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#23384;&#20648;&#22312;&#22320;&#22270;&#20013;&#30340;&#26041;&#24335;&#26469;&#26356;&#26032;&#28508;&#22312;&#22320;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#34920;&#38754;&#20449;&#24687;&#30340;&#24863;&#30693;&#21644;&#35782;&#21035;&#65292;&#22312;&#19981;&#21516;&#34920;&#38754;&#19978;&#30340;&#31934;&#20934;&#25805;&#32437;&#24471;&#21040;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#30340;&#23433;&#20840;&#37096;&#32626;&#20381;&#36182;&#20110;&#23427;&#20204;&#26377;&#25928;&#21453;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#33021;&#38656;&#35201;&#22312;&#19981;&#21516;&#34920;&#38754;&#19978;&#25805;&#32437;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28369;&#21160;&#22320;&#24418;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24403;&#21069;&#20301;&#32622;&#30340;&#34920;&#38754;&#20449;&#24687;&#23384;&#20648;&#22312;&#28508;&#22312;&#21464;&#37327;&#21521;&#37327;&#20013;&#26469;&#26465;&#20214;&#21270;&#23398;&#20064;&#34920;&#38754;&#24863;&#30693;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#20013;&#20174;&#23545;&#24212;&#20301;&#32622;&#30340;&#22810;&#20010;&#27169;&#24577;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#23558;&#20449;&#24687;&#23384;&#20648;&#21040;&#22320;&#22270;&#20013;&#65292;&#35757;&#32451;&#28508;&#22312;&#26144;&#23556;&#22120;&#26469;&#26356;&#26032;&#36825;&#20123;&#28508;&#22312;&#21464;&#37327;&#12290;&#36890;&#36807;&#23558;&#19968;&#20999;&#37117;&#19982;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#25439;&#22833;&#36827;&#34892;&#31471;&#23545;&#31471;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#24378;&#21046;&#28508;&#22312;&#26144;&#23556;&#22120;&#23398;&#20064;&#28508;&#22312;&#22320;&#22270;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#20351;&#20854;&#23545;&#38543;&#21518;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#26377;&#29992;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;&#36855;&#20320;&#30005;&#21160;&#27773;&#36710;&#19978;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#27809;&#26377;&#34920;&#38754;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#28508;&#22312;&#22320;&#22270;&#30340;&#26356;&#26032;&#21487;&#20197;&#20351;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#39044;&#27979;&#26356;&#21152;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
The safe deployment of autonomous vehicles relies on their ability to effectively react to environmental changes. This can require maneuvering on varying surfaces which is still a difficult problem, especially for slippery terrains. To address this issue we propose a new approach that learns a surface-aware dynamics model by conditioning it on a latent variable vector storing surface information about the current location. A latent mapper is trained to update these latent variables during inference from multiple modalities on every traversal of the corresponding locations and stores them in a map. By training everything end-to-end with the loss of the dynamics model, we enforce the latent mapper to learn an update rule for the latent map that is useful for the subsequent dynamics model. We implement and evaluate our approach on a real miniature electric car. The results show that the latent map is updated to allow more accurate predictions of the dynamics model compared to a model with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#21452;&#26354;&#21644;&#29699;&#24418;&#27169;&#22411;&#31354;&#38388;&#30340;&#31435;&#20307;&#25237;&#24433;&#20197;&#21450;Riemannian&#38544;&#31354;&#38388;&#30340;&#20056;&#31215;&#24212;&#29992;&#20110;&#28508;&#22312;&#22270;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#19982;&#38750;&#25237;&#24433;&#31354;&#38388;&#30456;&#24403;&#30340;&#24615;&#33021;&#24182;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.11754</link><description>&lt;p&gt;
&#28508;&#22312;&#22270;&#25512;&#26029;&#20013;&#30340;&#27169;&#22411;&#31354;&#38388;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projections of Model Spaces for Latent Graph Inference. (arXiv:2303.11754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#21452;&#26354;&#21644;&#29699;&#24418;&#27169;&#22411;&#31354;&#38388;&#30340;&#31435;&#20307;&#25237;&#24433;&#20197;&#21450;Riemannian&#38544;&#31354;&#38388;&#30340;&#20056;&#31215;&#24212;&#29992;&#20110;&#28508;&#22312;&#22270;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#19982;&#38750;&#25237;&#24433;&#31354;&#38388;&#30456;&#24403;&#30340;&#24615;&#33021;&#24182;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#22270;&#30340;&#36830;&#25509;&#32467;&#26500;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#12290;&#28508;&#22312;&#22270;&#25512;&#26029;&#20851;&#27880;&#20110;&#23398;&#20064;&#19968;&#20010;&#21512;&#36866;&#30340;&#22270;&#32467;&#26500;&#26469;&#25193;&#25955;&#20449;&#24687;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#26412;&#25991;&#21033;&#29992;&#21452;&#26354;&#21644;&#29699;&#24418;&#27169;&#22411;&#31354;&#38388;&#30340;&#31435;&#20307;&#25237;&#24433;&#65292;&#20197;&#21450;Riemannian&#38544;&#31354;&#38388;&#30340;&#20056;&#31215;&#65292;&#29992;&#20110;&#28508;&#22312;&#22270;&#25512;&#26029;&#12290;&#22312;&#36991;&#20813;&#26354;&#29575;&#36235;&#20110;&#38646;&#26102;&#31354;&#38388;&#21457;&#25955;&#30340;&#29702;&#35770;&#20445;&#35777;&#19979;&#65292;&#31435;&#20307;&#25237;&#24433;&#27169;&#22411;&#31354;&#38388;&#33021;&#23454;&#29616;&#19982;&#38750;&#25237;&#24433;&#23545;&#24212;&#27169;&#22411;&#31354;&#38388;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks leverage the connectivity structure of graphs as an inductive bias. Latent graph inference focuses on learning an adequate graph structure to diffuse information on and improve the downstream performance of the model. In this work we employ stereographic projections of the hyperbolic and spherical model spaces, as well as products of Riemannian manifolds, for the purpose of latent graph inference. Stereographically projected model spaces achieve comparable performance to their non-projected counterparts, while providing theoretical guarantees that avoid divergence of the spaces when the curvature tends to zero. We perform experiments on both homophilic and heterophilic graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#20070;&#39302;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#20102;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#20511;&#38405;&#35760;&#24405;&#21644;&#22312;&#32447;&#31038;&#20132;&#35835;&#32773;&#30340;&#21453;&#39304;&#21644;&#20449;&#24687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#20869;&#23481;&#30340;&#65288;CB&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#36798;&#21040;47%&#12290;</title><link>http://arxiv.org/abs/2303.11746</link><description>&lt;p&gt;
&#22270;&#20070;&#39302;&#20013;&#30340;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#24322;&#26500;&#25968;&#25454;&#28304;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Recommendation Systems in Libraries: an Application with Heterogeneous Data Sources. (arXiv:2303.11746v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#20070;&#39302;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#20102;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#20511;&#38405;&#35760;&#24405;&#21644;&#22312;&#32447;&#31038;&#20132;&#35835;&#32773;&#30340;&#21453;&#39304;&#21644;&#20449;&#24687;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#20869;&#23481;&#30340;&#65288;CB&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#36798;&#21040;47%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Reading&amp;Machine&#39033;&#30446;&#21033;&#29992;&#25968;&#23383;&#21270;&#25903;&#25345;&#65292;&#25552;&#39640;&#22270;&#20070;&#39302;&#30340;&#21560;&#24341;&#21147;&#21644;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#12290;&#35813;&#39033;&#30446;&#23454;&#29616;&#20102;&#19968;&#20010;&#24212;&#29992;&#31243;&#24207;&#65292;&#36890;&#36807;&#25512;&#33616;&#31995;&#32479;&#29983;&#25104;&#29992;&#25143;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#20070;&#31821;&#21015;&#34920;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#20114;&#21160;&#26174;&#31034;&#65292;&#24110;&#21161;&#29992;&#25143;&#36827;&#34892;&#20915;&#31574;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#27979;&#35797;&#65292;&#37319;&#29992;&#24847;&#22823;&#21033;&#37117;&#28789;&#22270;&#20070;&#39302;&#32593;&#32476;&#36807;&#21435;9&#24180;&#25152;&#26377;&#29992;&#25143;&#20511;&#38405;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#35835;&#32773;&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;Anobii&#25910;&#38598;&#30340;&#21453;&#39304;&#21644;&#20851;&#20110;&#35835;&#36807;&#20070;&#30340;&#38468;&#21152;&#20449;&#24687;&#12290;&#20973;&#20511;&#36825;&#31181;&#24322;&#26500;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#20102;&#22522;&#20110;&#20869;&#23481;&#30340;&#65288;CB&#65289;&#21644;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CF&#20248;&#20110;CB&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#26368;&#39640;47%&#12290;
&lt;/p&gt;
&lt;p&gt;
The Reading&amp;Machine project exploits the support of digitalization to increase the attractiveness of libraries and improve the users' experience. The project implements an application that helps the users in their decision-making process, providing recommendation system (RecSys)-generated lists of books the users might be interested in, and showing them through an interactive Virtual Reality (VR)-based Graphical User Interface (GUI). In this paper, we focus on the design and testing of the recommendation system, employing data about all users' loans over the past 9 years from the network of libraries located in Turin, Italy. In addition, we use data collected by the Anobii online social community of readers, who share their feedback and additional information about books they read. Armed with this heterogeneous data, we build and evaluate Content Based (CB) and Collaborative Filtering (CF) approaches. Our results show that the CF outperforms the CB approach, improving by up to 47\% the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;REM&#30340;BM&#31639;&#27861;&#65292;&#20351;&#29992;RL&#20174;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#30340;&#35282;&#24230;&#26469;&#20248;&#21270;&#35813;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#22823;&#21270;&#25509;&#25910;&#21151;&#29575;&#21644;&#26368;&#23567;&#21270;&#27874;&#26463;&#37325;&#26032;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2303.11742</link><description>&lt;p&gt;
&#22522;&#20110;&#23556;&#39057;&#29615;&#22659;&#22320;&#22270;&#30340;O-RAN&#26550;&#26500;&#20013;&#30340;&#27874;&#26463;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Beam Management Driven by Radio Environment Maps in O-RAN Architecture. (arXiv:2303.11742v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;REM&#30340;BM&#31639;&#27861;&#65292;&#20351;&#29992;RL&#20174;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#30340;&#35282;&#24230;&#26469;&#20248;&#21270;&#35813;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26368;&#22823;&#21270;&#25509;&#25910;&#21151;&#29575;&#21644;&#26368;&#23567;&#21270;&#27874;&#26463;&#37325;&#26032;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;(M-MIMO)&#34987;&#35748;&#20026;&#26159;5G&#21644;&#26410;&#26469;6G&#32593;&#32476;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#23556;&#39057;&#29615;&#22659;&#22320;&#22270;(Radio Environment Map,REM)&#30340;&#27874;&#26463;&#31649;&#29702;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#25509;&#25910;&#21151;&#29575;&#26144;&#23556;&#21644;&#29992;&#25143;&#31227;&#21160;&#27169;&#24335;&#65292;&#22312;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#19979;&#20174;&#24378;&#21270;&#23398;&#20064;(Reinforcement Learning,RL)&#30340;&#35282;&#24230;&#26469;&#20248;&#21270;BM&#36807;&#31243;&#65292;&#20363;&#22914;&#26368;&#22823;&#21270;&#25509;&#25910;&#21151;&#29575;&#25110;&#22312;&#36991;&#20813;&#26080;&#32447;&#30005;&#38142;&#36335;&#25925;&#38556;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#27874;&#26463;&#37325;&#26032;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Massive Multiple-Input Multiple-Output (M-MIMO) is considered as one of the key technologies in 5G, and future 6G networks. From the perspective of, e.g., channel estimation, especially for high-speed users it is easier to implement an M-MIMO network exploiting a static set of beams, i.e., Grid of Beams (GoB). While considering GoB it is important to properly assign users to the beams, i.e., to perform Beam Management (BM). BM can be enhanced by taking into account historical knowledge about the radio environment, e.g., to avoid radio link failures. The aim of this paper is to propose such a BM algorithm, that utilizes location-dependent data stored in a Radio Environment Map (REM). It utilizes received power maps, and user mobility patterns to optimize the BM process in terms of Reinforcement Learning (RL) by using the Policy Iteration method under different goal functions, e.g., maximization of received power or minimization of beam reselections while avoiding radio link failures
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#32593;&#32476;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#25104;&#21151;&#33539;&#20363;&#65292;&#34987;&#31227;&#26893;&#22238;&#37327;&#23376;&#39046;&#22495;, &#21487;&#20197;&#35780;&#20272;&#20256;&#32479;&#35745;&#31639;&#26426;&#26080;&#27861;&#39640;&#25928;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#21487;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36731;&#26494;&#37096;&#32626;&#12290;MPS&#65292;PEPS&#65292;TTN&#21644;MERA&#31561;&#24067;&#23616;&#34987;&#29992;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#26144;&#23556;&#65292;&#24182;&#35752;&#35770;&#20102;&#23454;&#29616;&#25216;&#26415;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.11735</link><description>&lt;p&gt;
&#8220;&#24352;&#37327;&#32593;&#32476;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#8221;
&lt;/p&gt;
&lt;p&gt;
Tensor networks for quantum machine learning. (arXiv:2303.11735v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11735
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#25104;&#21151;&#33539;&#20363;&#65292;&#34987;&#31227;&#26893;&#22238;&#37327;&#23376;&#39046;&#22495;, &#21487;&#20197;&#35780;&#20272;&#20256;&#32479;&#35745;&#31639;&#26426;&#26080;&#27861;&#39640;&#25928;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#24182;&#21487;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36731;&#26494;&#37096;&#32626;&#12290;MPS&#65292;PEPS&#65292;TTN&#21644;MERA&#31561;&#24067;&#23616;&#34987;&#29992;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#26144;&#23556;&#65292;&#24182;&#35752;&#35770;&#20102;&#23454;&#29616;&#25216;&#26415;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#21407;&#26412;&#26159;&#20026;&#37327;&#23376;&#29702;&#35770;&#32780;&#24320;&#21457;&#30340;&#65292;&#20294;&#21364;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#12290;&#29616;&#22312;&#65292;&#24352;&#37327;&#32593;&#32476;&#24050;&#32463;&#34987;&#31227;&#26893;&#22238;&#37327;&#23376;&#39046;&#22495;&#65292;&#29992;&#20110;&#35780;&#20272;&#20256;&#32479;&#35745;&#31639;&#26426;&#26080;&#27861;&#39640;&#25928;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#23427;&#20204;&#22312;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#30028;&#38754;&#24615;&#36136;&#20351;&#24471;&#23427;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#37096;&#32626;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#20171;&#32461;&#20102;&#20854;&#20013;&#19968;&#31181;&#34987;&#35748;&#20026;&#26159;&#29992;&#20110;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#26550;&#26500;&#65292;&#29305;&#21035;&#26159;&#35752;&#35770;&#20102;MPS&#65292;PEPS&#65292;TTN&#21644;MERA&#31561;&#24067;&#23616;&#22914;&#20309;&#34987;&#26144;&#23556;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#32534;&#30721;&#65292;&#20197;&#21450;&#21738;&#20123;&#23454;&#29616;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Once developed for quantum theory, tensor networks have been established as a successful machine learning paradigm. Now, they have been ported back to the quantum realm in the emerging field of quantum machine learning to assess problems that classical computers are unable to solve efficiently. Their nature at the interface between physics and machine learning makes tensor networks easily deployable on quantum computers. In this review article, we shed light on one of the major architectures considered to be predestined for variational quantum machine learning. In particular, we discuss how layouts like MPS, PEPS, TTNs and MERA can be mapped to a quantum computer, how they can be used for machine learning and data encoding and which implementation techniques improve their performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#33258;&#32534;&#30721;&#22120;&#30340;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#27888;&#21202;&#20998;&#35299;&#26694;&#26550;&#19982;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#21644;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.11734</link><description>&lt;p&gt;
&#35299;&#38145;&#33258;&#32534;&#30721;&#22120;&#30340;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Unlocking Layer-wise Relevance Propagation for Autoencoders. (arXiv:2303.11734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#33258;&#32534;&#30721;&#22120;&#30340;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#27888;&#21202;&#20998;&#35299;&#26694;&#26550;&#19982;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#35745;&#31639;&#21644;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31181;&#24378;&#22823;&#32780;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#65292;&#24120;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#65292;&#22914;&#24322;&#24120;&#26816;&#27979;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37325;&#24314;&#24182;&#19981;&#24635;&#26159;&#26131;&#20110;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#28145;&#24230;&#27888;&#21202;&#20998;&#35299;&#26694;&#26550;&#19982;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#26469;&#24555;&#36895;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#39564;&#35777;&#25216;&#26415;&#65292;&#29992;&#20110;&#27604;&#36739;&#25105;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#32570;&#22833;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#25152;&#25552;&#20986;&#30340;&#21487;&#35299;&#37322;&#24615;&#35299;&#20915;&#26041;&#26696;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#35745;&#31639;&#21644;&#36136;&#37327;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders are a powerful and versatile tool often used for various problems such as anomaly detection, image processing and machine translation. However, their reconstructions are not always trivial to explain. Therefore, we propose a fast explainability solution by extending the Layer-wise Relevance Propagation method with the help of Deep Taylor Decomposition framework. Furthermore, we introduce a novel validation technique for comparing our explainability approach with baseline methods in the case of missing ground-truth data. Our results highlight computational as well as qualitative advantages of the proposed explainability solution with respect to existing methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21069;&#21521;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21487;&#24494;&#20998;&#25490;&#21517;&#20989;&#25968;&#23454;&#29616;&#20219;&#21153;&#21270;&#20248;&#21270;&#25237;&#24433;&#38598;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36873;&#25321;&#20986;&#20855;&#26377;&#39640;&#20215;&#20540;&#30340;&#25237;&#24433;&#65292;&#20197;&#22686;&#24378;CT&#25195;&#25551;&#30340;&#22270;&#20687;&#37325;&#24314;&#21644;&#35786;&#26029;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.11724</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#24494;&#20998;&#25490;&#21517;&#30340;&#20219;&#21153;&#21270;&#20248;&#21270;&#25237;&#24433;&#38598;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Task-based Generation of Optimized Projection Sets using Differentiable Ranking. (arXiv:2303.11724v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21069;&#21521;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#21487;&#24494;&#20998;&#25490;&#21517;&#20989;&#25968;&#23454;&#29616;&#20219;&#21153;&#21270;&#20248;&#21270;&#25237;&#24433;&#38598;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36873;&#25321;&#20986;&#20855;&#26377;&#39640;&#20215;&#20540;&#30340;&#25237;&#24433;&#65292;&#20197;&#22686;&#24378;CT&#25195;&#25551;&#30340;&#22270;&#20687;&#37325;&#24314;&#21644;&#35786;&#26029;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36873;&#25321;&#22312;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#65288;CT&#65289;&#25195;&#25551;&#20013;&#20855;&#26377;&#20215;&#20540;&#30340;&#25237;&#24433;&#65292;&#20197;&#22686;&#24378;&#22270;&#20687;&#37325;&#24314;&#21644;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#23558;&#25237;&#24433;&#26816;&#27979;&#33021;&#21147;&#21644;&#25968;&#25454;&#23436;&#25972;&#24615;&#20004;&#20010;&#37325;&#35201;&#22240;&#32032;&#38598;&#25104;&#21040;&#19968;&#20010;&#21069;&#21521;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#32593;&#32476;&#35780;&#20272;&#25237;&#24433;&#30340;&#20215;&#20540;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#25490;&#21517;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#30452;&#36890;&#20272;&#35745;&#22120;&#36827;&#34892;&#26368;&#32456;&#36873;&#25321;&#12290;&#36890;&#36807;&#35757;&#32451;&#25552;&#20379;&#30340;&#26631;&#31614;&#26469;&#30830;&#20445;&#25968;&#25454;&#23436;&#25972;&#24615;&#12290;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#21551;&#21457;&#24335;&#24378;&#21046;&#23454;&#29616;&#25968;&#25454;&#23436;&#25972;&#24615;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#20250;&#25490;&#38500;&#26377;&#20215;&#20540;&#30340;&#25237;&#24433;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#30340;&#38750;&#30772;&#22351;&#24615;&#27979;&#35797;&#22330;&#26223;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#20854;&#30446;&#30340;&#26159;&#22312;&#25351;&#23450;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#20869;&#26368;&#22823;&#21270;&#37325;&#24314;&#36136;&#37327;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#20197;&#21069;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#20026;&#20351;&#29992;&#22522;&#20110;&#37325;&#24314;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25237;&#24433;&#36873;&#25321;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for selecting valuable projections in computed tomography (CT) scans to enhance image reconstruction and diagnosis. The approach integrates two important factors, projection-based detectability and data completeness, into a single feed-forward neural network. The network evaluates the value of projections, processes them through a differentiable ranking function and makes the final selection using a straight-through estimator. Data completeness is ensured through the label provided during training. The approach eliminates the need for heuristically enforcing data completeness, which may exclude valuable projections. The method is evaluated on simulated data in a non-destructive testing scenario, where the aim is to maximize the reconstruction quality within a specified region of interest. We achieve comparable results to previous methods, laying the foundation for using reconstruction-based loss functions to learn the selection of projections.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20855;&#26377;&#31354;&#38388;&#24863;&#30693;Shapley&#20540;&#30340;Lidar&#32447;&#36335;&#36873;&#25321;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#30340;&#28145;&#24230;&#34917;&#20840;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20943;&#23569;&#32447;&#36335;&#25968;&#37327;&#21644;&#20445;&#25345;&#28145;&#24230;&#34917;&#20840;&#36136;&#37327;&#30340;&#30446;&#26631;&#65292;&#20351;&#29992;&#20102;&#19968;&#21322;&#30340;&#32447;&#36335;&#23601;&#33021;&#22815;&#36798;&#21040;&#19982;&#23436;&#25972;&#30340;Lidar&#36755;&#20837;&#30456;&#24403;&#30340;&#28145;&#24230;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11720</link><description>&lt;p&gt;
&#37319;&#29992;&#20855;&#26377;&#31354;&#38388;&#24863;&#30693;Shapley&#20540;&#30340;Lidar&#32447;&#36335;&#36873;&#25321;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#30340;&#28145;&#24230;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Lidar Line Selection with Spatially-Aware Shapley Value for Cost-Efficient Depth Completion. (arXiv:2303.11720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20855;&#26377;&#31354;&#38388;&#24863;&#30693;Shapley&#20540;&#30340;Lidar&#32447;&#36335;&#36873;&#25321;&#36827;&#34892;&#25104;&#26412;&#25928;&#30410;&#30340;&#28145;&#24230;&#34917;&#20840;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20943;&#23569;&#32447;&#36335;&#25968;&#37327;&#21644;&#20445;&#25345;&#28145;&#24230;&#34917;&#20840;&#36136;&#37327;&#30340;&#30446;&#26631;&#65292;&#20351;&#29992;&#20102;&#19968;&#21322;&#30340;&#32447;&#36335;&#23601;&#33021;&#22815;&#36798;&#21040;&#19982;&#23436;&#25972;&#30340;Lidar&#36755;&#20837;&#30456;&#24403;&#30340;&#28145;&#24230;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lidar&#26159;&#20272;&#35745;&#22330;&#26223;&#28145;&#24230;&#30340;&#37325;&#35201;&#20256;&#24863;&#22120;&#12290; &#20856;&#22411;&#30340;&#26059;&#36716;&#24335;Lidar&#21457;&#23556;&#25490;&#21015;&#22312;&#20960;&#20010;&#27700;&#24179;&#32447;&#19978;&#30340;&#33033;&#20914;&#65292;&#20256;&#24863;&#22120;&#30340;&#36135;&#24065;&#25104;&#26412;&#38543;&#36825;&#20123;&#32447;&#30340;&#25968;&#37327;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#21270;Lidar&#32447;&#36335;&#20301;&#32622;&#20197;&#23547;&#25214;&#28145;&#24230;&#34917;&#20840;&#20219;&#21153;&#30340;&#26368;&#26377;&#25928;&#37197;&#32622;&#30340;&#26032;&#38382;&#39064;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#32447;&#36335;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#28145;&#24230;&#34917;&#20840;&#36136;&#37327;&#65292;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65292;&#65288;1&#65289;&#22522;&#20110;Shapley&#20540;&#35745;&#31639;&#30340;&#32447;&#36335;&#36793;&#38469;&#36129;&#29486;&#30340;&#32447;&#36335;&#36873;&#25321;&#65292;&#21644;&#65288;2&#65289;&#23558;&#32447;&#36335;&#20301;&#32622;&#20998;&#24067;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#32771;&#34385;&#21040;&#20854;&#38656;&#35201;&#21040;&#36798;&#20840;&#23616;&#28145;&#24230;&#12290;&#20855;&#26377;&#31354;&#38388;&#24863;&#30693;Shapley&#20540;&#65288;SaS&#65289;&#25104;&#21151;&#22320;&#36873;&#25321;&#20102;&#32447;&#36335;&#23376;&#38598;&#65292;&#36825;&#20123;&#32447;&#36335;&#23376;&#38598;&#20351;&#29992;&#20102;&#19968;&#21322;&#30340;&#32447;&#36335;&#65292;&#20294;&#20855;&#26377;&#19982;&#23436;&#25972;&#30340;Lidar&#36755;&#20837;&#30456;&#24403;&#30340;&#28145;&#24230;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lidar is a vital sensor for estimating the depth of a scene. Typical spinning lidars emit pulses arranged in several horizontal lines and the monetary cost of the sensor increases with the number of these lines. In this work, we present the new problem of optimizing the positioning of lidar lines to find the most effective configuration for the depth completion task. We propose a solution to reduce the number of lines while retaining the up-to-the-mark quality of depth completion. Our method consists of two components, (1) line selection based on the marginal contribution of a line computed via the Shapley value and (2) incorporating line position spread to take into account its need to arrive at image-wide depth completion. Spatially-aware Shapley values (SaS) succeed in selecting line subsets that yield a depth accuracy comparable to the full lidar input while using just half of the lines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23436;&#25972;&#30340;&#35843;&#30740;&#20171;&#32461;&#20102;&#29983;&#25104;&#22411;AI&#65292;&#21253;&#21547;&#20102;&#20174;&#25216;&#26415;&#21040;&#24212;&#29992;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;ChatGPT&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20294;&#24182;&#19981;&#36275;&#20197;&#35206;&#30422;&#25152;&#26377;&#30340;AIGC&#20219;&#21153;&#65292;&#23545;&#20110;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#21019;&#36896;&#36824;&#38656;&#35201;GPT-5&#31561;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.11717</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;AI&#65288;AIGC&#65289;&#23436;&#25972;&#35843;&#30740;&#65306;ChatGPT&#20174;GPT-4&#21040;GPT-5&#26159;&#20320;&#38656;&#35201;&#30340;&#20840;&#37096;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?. (arXiv:2303.11717v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23436;&#25972;&#30340;&#35843;&#30740;&#20171;&#32461;&#20102;&#29983;&#25104;&#22411;AI&#65292;&#21253;&#21547;&#20102;&#20174;&#25216;&#26415;&#21040;&#24212;&#29992;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;ChatGPT&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20294;&#24182;&#19981;&#36275;&#20197;&#35206;&#30422;&#25152;&#26377;&#30340;AIGC&#20219;&#21153;&#65292;&#23545;&#20110;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#21019;&#36896;&#36824;&#38656;&#35201;GPT-5&#31561;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#27969;&#34892;&#65292;&#29983;&#25104;&#22411;AI&#65288;AIGC&#65292;&#21363;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65289;&#22240;&#20854;&#20998;&#26512;&#21644;&#21019;&#36896;&#25991;&#26412;&#12289;&#22270;&#20687;&#31561;&#33021;&#21147;&#32780;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#36720;&#21160;&#12290;&#22312;&#22914;&#27492;&#24191;&#27867;&#30340;&#23186;&#20307;&#25253;&#23548;&#19979;&#65292;&#20960;&#20046;&#19981;&#21487;&#33021;&#38169;&#36807;&#20174;&#29305;&#23450;&#35282;&#24230;&#31397;&#25506;AIGC&#30340;&#26426;&#20250;&#12290;&#22312;AI&#20174;&#32431;&#31929;&#30340;&#20998;&#26512;&#36716;&#21521;&#21019;&#36896;&#30340;&#26102;&#20195;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT&#20165;&#20165;&#26159;&#20247;&#22810;AIGC&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#24037;&#20855;&#65292;&#20854;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#12290;&#35768;&#22810;&#20154;&#23545;ChatGPT&#30340;&#33021;&#21147;&#21360;&#35937;&#28145;&#21051;&#65292;&#21516;&#26102;&#20063;&#22312;&#24605;&#32771;&#23427;&#30340;&#23616;&#38480;&#24615;&#65306;GPT-5&#65288;&#25110;&#20854;&#20182;&#26410;&#26469;&#30340;GPT&#21464;&#31181;&#65289;&#26159;&#21542;&#33021;&#24110;&#21161;ChatGPT&#32479;&#19968;&#21508;&#31181;AIGC&#20219;&#21153;&#65292;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20869;&#23481;&#21019;&#36896;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#29616;&#26377;AIGC&#20219;&#21153;&#30340;&#20840;&#38754;&#23457;&#26597;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#36805;&#36895;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#39318;&#27425;&#20171;&#32461;&#20102;AIGC&#65292;&#28085;&#30422;&#20102;&#20174;&#25216;&#26415;&#21040;&#24212;&#29992;&#30340;&#25152;&#26377;&#26041;&#38754;&#12290;&#29616;&#20195;&#30340;&#29983;&#25104;&#22411;AI&#20381;&#36182;&#20110;&#21508;&#31181;&#25216;&#26415;&#22522;&#30784;&#65292;&#21253;&#25324;&#27169;&#22411;&#26550;&#26500;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible for us to miss the opportunity to glimpse AIGC from a certain angle. In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many people are wondering about its limits: can GPT-5 (or other future GPT variants) help ChatGPT unify all AIGC tasks for diversified content creation? Toward answering this question, a comprehensive review of existing AIGC tasks is needed. As such, our work comes to fill this gap promptly by offering a first look at AIGC, ranging from its techniques to applications. Modern generative AI relies on various technical foundations, ranging from model architecture and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#30340;&#30452;&#25509;&#22810;&#31867;&#20998;&#31867;&#30340;&#37327;&#23376;SVM(QMSVM)&#65292;&#21487;&#20197;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#23545;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.11705</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#30340;&#22810;&#31867;SVM&#19968;&#27493;&#20998;&#31867;&#30740;&#31350;&#24212;&#29992;&#20110;&#36965;&#24863;&#25968;&#25454;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Single-Step Multiclass SVM based on Quantum Annealing for Remote Sensing Data Classification. (arXiv:2303.11705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#30340;&#30452;&#25509;&#22810;&#31867;&#20998;&#31867;&#30340;&#37327;&#23376;SVM(QMSVM)&#65292;&#21487;&#20197;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#23545;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#37327;&#23376;&#36864;&#28779;&#22120;&#30340;&#21457;&#23637;&#20419;&#36827;&#20102;&#23454;&#39564;&#28436;&#31034;&#24182;&#22686;&#21152;&#20102;&#23545;&#37327;&#23376;&#36864;&#28779;&#24212;&#29992;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#20363;&#22914;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20197;&#21450;&#29305;&#21035;&#26159;&#27969;&#34892;&#30340;&#37327;&#23376;SVM&#20013;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#37327;&#23376;SVM&#30340;&#20960;&#20010;&#29256;&#26412;&#65292;&#24182;&#24050;&#35777;&#26126;&#37327;&#23376;&#36864;&#28779;&#22312;&#20854;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#20063;&#24050;&#32463;&#36827;&#34892;&#20102;&#25193;&#23637;&#21040;&#22810;&#31867;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#20010;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#30340;&#38598;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#30340;&#30452;&#25509;&#22810;&#31867;&#20998;&#31867;&#30340;&#37327;&#23376;SVM(QMSVM)&#12290;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#34987;&#20844;&#24335;&#21270;&#20026;&#21333;&#20010;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;(QUBO)&#38382;&#39064;&#65292;&#24182;&#29992;&#37327;&#23376;&#36864;&#28779;&#27714;&#35299;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#24615;&#33021;&#12290;&#22312;D-Wave Advantage&#37327;&#23376;&#36864;&#28779;&#22120;&#19978;&#36827;&#34892;&#20102;&#20998;&#31867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the development of quantum annealers has enabled experimental demonstrations and has increased research interest in applications of quantum annealing, such as in quantum machine learning and in particular for the popular quantum SVM. Several versions of the quantum SVM have been proposed, and quantum annealing has been shown to be effective in them. Extensions to multiclass problems have also been made, which consist of an ensemble of multiple binary classifiers. This work proposes a novel quantum SVM formulation for direct multiclass classification based on quantum annealing, called Quantum Multiclass SVM (QMSVM). The multiclass classification problem is formulated as a single Quadratic Unconstrained Binary Optimization (QUBO) problem solved with quantum annealing. The main objective of this work is to evaluate the feasibility, accuracy, and time performance of this approach. Experiments have been performed on the D-Wave Advantage quantum annealer for a classification
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;</title><link>http://arxiv.org/abs/2303.11702</link><description>&lt;p&gt;
&#36830;&#25509;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linking generative semi-supervised learning and generative open-set recognition. (arXiv:2303.11702v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#29983;&#25104;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#24320;&#25918;&#38598;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;SSL-GANs&#21644;OSR-GANs&#26041;&#27861;&#30340;&#30456;&#20284;&#24615;&#22312;&#20110;&#37117;&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#26469;&#25512;&#24191;&#24320;&#25918;&#31354;&#38388;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#20294;&#22312;&#26576;&#20123;OSR&#20219;&#21153;&#20013;OSR&#20248;&#21270;&#30340;ARP-GAN&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25506;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#20197;&#21069;&#27809;&#26377;&#27491;&#24335;&#23558;SSL&#21644;OSR&#32852;&#31995;&#36215;&#26469;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#21508;&#33258;&#30340;&#26041;&#27861;&#26377;&#24778;&#20154;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSL-GAN&#21644;OSR-GAN&#35201;&#27714;&#29983;&#25104;&#22120;&#22312;&#20114;&#34917;&#31354;&#38388;&#20013;&#20135;&#29983;&#26679;&#26412;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#29983;&#25104;&#26679;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;SSL&#21644;OSR&#20998;&#31867;&#22120;&#37117;&#21487;&#20197;&#23436;&#20840;&#35782;&#21035;&#24320;&#25918;&#31354;&#38388;&#12290;&#20026;&#20102;&#35777;&#26126;SSL&#21644;OSR&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;SSL-GAN&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;OSR-GAN&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25991;&#29486;&#22522;&#30784;&#26356;&#21152;&#29282;&#22266;&#30340;SSL&#20248;&#21270;&#36793;&#32536;-GAN&#22312;&#32467;&#21512;SSL-OSR&#20219;&#21153;&#26041;&#38754;&#26641;&#31435;&#26032;&#30340;&#26631;&#20934;&#65292;&#24182;&#22312;&#26576;&#20123;&#19968;&#33324;&#30340;OSR&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;OSR&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#20114;&#24800;&#28857;&#65288;ARP&#65289;-GAN&#22312;&#19968;&#20123;OSR&#20219;&#21153;&#20013;&#20173;&#28982;&#30053;&#20248;&#20110;SSL-GAN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between semi-supervised learning (SSL) and open-set recognition (OSR) in the context of generative adversarial networks (GANs). Although no previous study has formally linked SSL and OSR, their respective methods share striking similarities. Specifically, SSL-GANs and OSR-GANs require generator to produce samples in the complementary space. Subsequently, by regularising networks with generated samples, both SSL and OSR classifiers generalize the open space. To demonstrate the connection between SSL and OSR, we theoretically and experimentally compare state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our results indicate that the SSL optimised margin-GANs, which have a stronger foundation in literature, set the new standard for the combined SSL-OSR task and achieves new state-of-other art results in certain general OSR experiments. However, the OSR optimised adversarial reciprocal point (ARP)-GANs still slightly out-performe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#39640;&#39057;&#32858;&#28966;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#39057;&#32858;&#28966;&#22359;&#65288;HFFB&#65289;&#23454;&#29616;&#23545;&#39640;&#39057;&#20449;&#24687;&#30340;&#26377;&#36873;&#25321;&#24615;&#22686;&#24378;&#65292;&#20174;&#32780;&#25552;&#39640;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11701</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#36731;&#37327;&#32423;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#39640;&#39057;&#32858;&#28966;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A High-Frequency Focused Network for Lightweight Single Image Super-Resolution. (arXiv:2303.11701v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#39640;&#39057;&#32858;&#28966;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#39640;&#39057;&#32858;&#28966;&#22359;&#65288;HFFB&#65289;&#23454;&#29616;&#23545;&#39640;&#39057;&#20449;&#24687;&#30340;&#26377;&#36873;&#25321;&#24615;&#22686;&#24378;&#65292;&#20174;&#32780;&#25552;&#39640;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#22312;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SISR&#65289;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#19982;&#20302;&#39057;&#20449;&#24687;&#30456;&#27604;&#65292;&#39640;&#39057;&#32454;&#33410;&#30340;&#37325;&#24314;&#26356;&#21152;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;SISR&#27169;&#22411;&#20026;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20998;&#37197;&#30456;&#21516;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;&#31616;&#21333;&#20302;&#39057;&#20449;&#24687;&#30340;&#20887;&#20313;&#22788;&#29702;&#20197;&#21450;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#39640;&#39057;&#20449;&#24687;&#24674;&#22797;&#19981;&#36275;&#12290;&#25105;&#20204;&#36890;&#36807;&#39640;&#39057;&#32858;&#28966;&#22359;&#65288;HFFB&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#39057;&#32858;&#28966;&#32593;&#32476;&#65288;HFFN&#65289;&#65292;&#20174;&#32780;&#36873;&#25321;&#24615;&#22320;&#22686;&#24378;&#39640;&#39057;&#20449;&#24687;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20302;&#39057;&#20449;&#24687;&#30340;&#20887;&#20313;&#29305;&#24449;&#35745;&#31639;&#12290;HFFB&#26377;&#25928;&#22320;&#23558;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#32473;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#39640;&#39057;&#20449;&#24687;&#37325;&#24314;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#29305;&#24449;&#34701;&#21512;&#22359;&#65288;LFFB&#65289;&#65292;&#22312;&#26412;&#22320;&#21306;&#22495;&#20869;&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#22810;&#20010;HFFB&#30340;&#29305;&#24449;&#65292;&#21033;&#29992;&#20102;&#20114;&#34917;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lightweight neural networks for single-image super-resolution (SISR) tasks have made substantial breakthroughs in recent years. Compared to low-frequency information, high-frequency detail is much more difficult to reconstruct. Most SISR models allocate equal computational resources for low-frequency and high-frequency information, which leads to redundant processing of simple low-frequency information and inadequate recovery of more challenging high-frequency information. We propose a novel High-Frequency Focused Network (HFFN) through High-Frequency Focused Blocks (HFFBs) that selectively enhance high-frequency information while minimizing redundant feature computation of low-frequency information. The HFFB effectively allocates more computational resources to the more challenging reconstruction of high-frequency information. Moreover, we propose a Local Feature Fusion Block (LFFB) effectively fuses features from multiple HFFBs in a local region, utilizing complementary information a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20174;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#31354;&#38388;&#32676;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#21462;&#24471;&#20102;&#27604;&#30452;&#25509;&#22312;ICSD&#26230;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11699</link><description>&lt;p&gt;
&#22312;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Neural networks trained on synthetically generated crystals can extract structural information from ICSD powder X-ray diffractograms. (arXiv:2303.11699v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20174;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#31354;&#38388;&#32676;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#21462;&#24471;&#20102;&#27604;&#30452;&#25509;&#22312;ICSD&#26230;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#26230;&#20307;&#31354;&#38388;&#32676;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#22312;ICSD&#31561;&#25968;&#25454;&#24211;&#30340;&#27169;&#25311;&#34893;&#23556;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#20854;&#35268;&#27169;&#26377;&#38480;&#12289;&#31867;&#21035;&#19981;&#22343;&#21248;&#24182;&#19988;&#20559;&#21521;&#26576;&#20123;&#32467;&#26500;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21033;&#29992;&#27599;&#20010;&#31354;&#38388;&#32676;&#30340;&#23545;&#31216;&#25805;&#20316;&#65292;&#22312;&#38543;&#26426;&#22352;&#26631;&#19979;&#29983;&#25104;&#21512;&#25104;&#26230;&#20307;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;ResNet&#31867;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#35757;&#32451;&#65292;&#27599;&#23567;&#26102;&#26368;&#22810;&#21487;&#29983;&#25104;&#23569;&#37327;&#30334;&#19975;&#20010;&#21807;&#19968;&#30340;&#21512;&#25104;&#34893;&#23556;&#22270;&#12290;&#38024;&#23545;&#25105;&#20204;&#36873;&#25321;&#30340;&#31354;&#38388;&#32676;&#20998;&#31867;&#20219;&#21153;&#65292;&#22312;&#22823;&#22810;&#25968;&#31354;&#38388;&#32676;&#30340;&#26410;&#35265;ICSD&#32467;&#26500;&#31867;&#22411;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;79.9%&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#36825;&#36229;&#36807;&#20102;&#30452;&#25509;&#22312;ICSD&#26230;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;56.1%&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#21512;&#25104;&#26230;&#20307;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;ICSD&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#22270;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques have successfully been used to extract structural information such as the crystal space group from powder X-ray diffractograms. However, training directly on simulated diffractograms from databases such as the ICSD is challenging due to its limited size, class-inhomogeneity, and bias toward certain structure types. We propose an alternative approach of generating synthetic crystals with random coordinates by using the symmetry operations of each space group. Based on this approach, we demonstrate online training of deep ResNet-like models on up to a few million unique on-the-fly generated synthetic diffractograms per hour. For our chosen task of space group classification, we achieved a test accuracy of 79.9% on unseen ICSD structure types from most space groups. This surpasses the 56.1% accuracy of the current state-of-the-art approach of training on ICSD crystals directly. Our results demonstrate that synthetically generated crystals can be used to extract
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#26631;&#31614;&#22686;&#24378;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#30340;&#25299;&#25169;&#20449;&#24687;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26631;&#31614;&#32622;&#20449;&#24230;&#65292;&#24182;&#37319;&#29992;&#26032;&#30340;&#38750;&#32447;&#24615;LE&#27169;&#22411;&#26469;&#26174;&#33879;&#25552;&#39640;&#26631;&#31614;&#22686;&#24378;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11698</link><description>&lt;p&gt;
&#26631;&#31614;&#22686;&#24378;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation For Label Enhancement. (arXiv:2303.11698v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11698
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#26631;&#31614;&#22686;&#24378;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#30340;&#25299;&#25169;&#20449;&#24687;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26631;&#31614;&#32622;&#20449;&#24230;&#65292;&#24182;&#37319;&#29992;&#26032;&#30340;&#38750;&#32447;&#24615;LE&#27169;&#22411;&#26469;&#26174;&#33879;&#25552;&#39640;&#26631;&#31614;&#22686;&#24378;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#20998;&#24067;&#65288;LD&#65289;&#20351;&#29992;&#25551;&#36848;&#31243;&#24230;&#26469;&#25551;&#36848;&#23454;&#20363;&#65292;&#22312;&#22788;&#29702;&#26631;&#31614;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26356;&#21152;&#32454;&#31890;&#24230;&#30340;&#30417;&#30563;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;LD&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#20026;&#20102;&#33719;&#21462;LD&#65292;&#26631;&#31614;&#22686;&#24378;&#65288;LE&#65289;&#34987;&#25552;&#20986;&#26469;&#20174;&#36923;&#36753;&#26631;&#31614;&#20013;&#24674;&#22797;LD&#12290;&#29616;&#26377;&#30340;LE&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;i&#65289;&#23427;&#20204;&#20351;&#29992;&#36923;&#36753;&#26631;&#31614;&#26469;&#35757;&#32451;&#21040;LD&#30340;&#26144;&#23556;&#65292;&#20294;&#26159;&#30417;&#30563;&#20449;&#24687;&#22826;&#23485;&#26494;&#65292;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#65307;&#65288;ii&#65289;&#23427;&#20204;&#24573;&#30053;&#20102;&#29305;&#24449;&#30340;&#20887;&#20313;&#24615;&#65292;&#24182;&#30452;&#25509;&#20351;&#29992;&#25910;&#38598;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;i&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#29305;&#24449;&#31354;&#38388;&#30340;&#25299;&#25169;&#32467;&#26500;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26631;&#31614;&#32622;&#20449;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;ii&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;LE&#38477;&#32500;&#26041;&#27861;&#65292;&#23427;&#23558;&#21407;&#22987;&#25968;&#25454;&#25237;&#24433;&#21040;&#36739;&#20302;&#32500;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#12290;&#23558;&#20197;&#19978;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#29992;&#20110;LE&#30340;&#22686;&#24378;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#23398;&#20064;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;LE&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26631;&#31614;&#22686;&#24378;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label distribution (LD) uses the description degree to describe instances, which provides more fine-grained supervision information when learning with label ambiguity. Nevertheless, LD is unavailable in many real-world applications. To obtain LD, label enhancement (LE) has emerged to recover LD from logical label. Existing LE approach have the following problems: (\textbf{i}) They use logical label to train mappings to LD, but the supervision information is too loose, which can lead to inaccurate model prediction; (\textbf{ii}) They ignore feature redundancy and use the collected features directly. To solve (\textbf{i}), we use the topology of the feature space to generate more accurate label-confidence. To solve (\textbf{ii}), we proposed a novel supervised LE dimensionality reduction approach, which projects the original data into a lower dimensional feature space. Combining the above two, we obtain the augmented data for LE. Further, we proposed a novel nonlinear LE model based on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#36716;&#24405;&#32452;&#23398;&#36827;&#34892;&#26080;&#20551;&#35774;&#30340;&#33647;&#29289;&#21644;&#30142;&#30149;&#21305;&#37197;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#36716;&#24405;&#32452;&#23398;&#21305;&#37197;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#22791;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#22522;&#22240;&#34920;&#36798;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20026;&#26032;&#33647;&#24320;&#21457;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.11695</link><description>&lt;p&gt;
&#22522;&#20110;&#36716;&#24405;&#32452;&#23398;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#33647;&#29289;&#21644;&#30142;&#30149;&#21305;&#37197;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transcriptomics-based matching of drugs to diseases with deep learning. (arXiv:2303.11695v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#36716;&#24405;&#32452;&#23398;&#36827;&#34892;&#26080;&#20551;&#35774;&#30340;&#33647;&#29289;&#21644;&#30142;&#30149;&#21305;&#37197;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#36716;&#24405;&#32452;&#23398;&#21305;&#37197;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#22791;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#22522;&#22240;&#34920;&#36798;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20026;&#26032;&#33647;&#24320;&#21457;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#26080;&#20551;&#35774;&#30340;&#22522;&#20110;&#36716;&#24405;&#32452;&#23398;&#30340;&#33647;&#29289;&#21644;&#30142;&#30149;&#21305;&#37197;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26159;&#22312;&#25209;&#20934;&#30340;&#33647;&#29289; - &#30142;&#30149;&#36866;&#24212;&#30151;&#26041;&#38754;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20197;&#30456;&#20851;&#30340;&#30142;&#30149;&#21644;&#33647;&#29289;&#24046;&#24322;&#22522;&#22240;&#34920;&#36798;&#35889;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#23398;&#20064;&#35782;&#21035;&#26032;&#30340;&#36866;&#24212;&#30151;&#12290;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#21253;&#21547;68&#31181;&#30142;&#30149;&#30340;&#30142;&#30149; - &#33647;&#29289;&#36866;&#24212;&#30151;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#27169;&#25311;&#20013;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#36716;&#24405;&#32452;&#23398;&#21305;&#37197;&#22522;&#32447;CMap&#21644;Characteristic Direction&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26631;&#20934;&#26816;&#32034;&#24230;&#37327;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#36825;&#20004;&#20010;&#22522;&#32447;&#25552;&#39640;&#20102;200%&#20197;&#19978;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#25429;&#33719;&#20102;&#19981;&#21516;&#22522;&#22240;&#34920;&#36798;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21253;&#25324;&#33647;&#29289;&#21644;&#30142;&#30149;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#25968;&#25454;&#21644;&#20195;&#30721;&#20379;&#23398;&#32773;&#22312; https://github.com/healx/dgem-nn-public &#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present a deep learning approach to conduct hypothesis-free, transcriptomics-based matching of drugs for diseases. Our proposed neural network architecture is trained on approved drug-disease indications, taking as input the relevant disease and drug differential gene expression profiles, and learns to identify novel indications. We assemble an evaluation dataset of disease-drug indications spanning 68 diseases and evaluate in silico our approach against the most widely used transcriptomics-based matching baselines, CMap and the Characteristic Direction. Our results show a more than 200% improvement over both baselines in terms of standard retrieval metrics. We further showcase our model's ability to capture different genes' expressions interactions among drugs and diseases. We provide our trained models, data and code to predict with them at https://github.com/healx/dgem-nn-public.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#31867;MLP&#26550;&#26500;ALOFT&#65292;&#23427;&#21487;&#20351;&#29992;&#21160;&#24577;&#20302;&#39057;&#21464;&#25442;&#29992;&#20110;&#22495;&#27867;&#21270;&#12290;&#19982;CNN&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#34920;&#31034;&#65292;&#22240;&#27492;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11674</link><description>&lt;p&gt;
ALOFT&#65306;&#19968;&#31181;&#36731;&#37327;&#21270;&#30340;&#31867;MLP&#26550;&#26500;&#65292;&#37197;&#21512;&#21160;&#24577;&#20302;&#39057;&#21464;&#25442;&#29992;&#20110;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization. (arXiv:2303.11674v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#31867;MLP&#26550;&#26500;ALOFT&#65292;&#23427;&#21487;&#20351;&#29992;&#21160;&#24577;&#20302;&#39057;&#21464;&#25442;&#29992;&#20110;&#22495;&#27867;&#21270;&#12290;&#19982;CNN&#30456;&#27604;&#65292;&#35813;&#26550;&#26500;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#34920;&#31034;&#65292;&#22240;&#27492;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#27867;&#21270;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#23427;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#22810;&#20010;&#28304;&#22495;&#26469;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#30446;&#26631;&#22495;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#24037;&#20316;&#37117;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#28982;&#32780;&#65292;&#21367;&#31215;&#26680;&#30340;&#23616;&#37096;&#25805;&#20316;&#20351;&#24471;&#27169;&#22411;&#36807;&#20110;&#20851;&#27880;&#23616;&#37096;&#34920;&#31034;&#65288;&#20363;&#22914;&#32441;&#29702;&#65289;&#65292;&#36825;&#20174;&#26412;&#36136;&#19978;&#20351;&#24471;&#27169;&#22411;&#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28304;&#22495;&#24182;&#38459;&#30861;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20960;&#31181;&#22522;&#20110;MLP&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#19981;&#21516;&#22359;&#20043;&#38388;&#30340;&#20840;&#23616;&#20132;&#20114;&#65292;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#22312;&#27492;&#21463;&#21040;&#21551;&#21457;&#65292;&#39318;&#20808;&#20998;&#26512;&#20102;CNN&#21644;MLP&#26041;&#27861;&#22312;DG&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;MLP&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#27604;CNN&#26041;&#27861;&#26356;&#22909;&#22320;&#25429;&#25417;&#20840;&#23616;&#34920;&#31034;&#65288;&#20363;&#22914;&#32467;&#26500;&#65289;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#26368;&#36817;&#30340;&#19968;&#31181;&#36731;&#37327;&#32423;MLP&#26041;&#27861;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#32447;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#22823;&#22810;&#25968;&#32479;&#35745;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) aims to learn a model that generalizes well to unseen target domains utilizing multiple source domains without re-training. Most existing DG works are based on convolutional neural networks (CNNs). However, the local operation of the convolution kernel makes the model focus too much on local representations (e.g., texture), which inherently causes the model more prone to overfit to the source domains and hampers its generalization ability. Recently, several MLP-based methods have achieved promising results in supervised learning tasks by learning global interactions among different patches of the image. Inspired by this, in this paper, we first analyze the difference between CNN and MLP methods in DG and find that MLP methods exhibit a better generalization ability because they can better capture the global representations (e.g., structure) than CNN methods. Then, based on a recent lightweight MLP method, we obtain a strong baseline that outperforms most stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#24635;&#32467;&#65292;&#21253;&#25324;&#24179;&#34913;&#24863;&#30693;&#20248;&#21270;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#23458;&#25143;&#31471;&#21152;&#26435;&#31561;&#12290;&#30446;&#21069;&#20173;&#23384;&#22312;&#30528;&#19968;&#20123;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2303.11673</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Class Imbalance in Federated Learning. (arXiv:2303.11673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#24635;&#32467;&#65292;&#21253;&#25324;&#24179;&#34913;&#24863;&#30693;&#20248;&#21270;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#23458;&#25143;&#31471;&#21152;&#26435;&#31561;&#12290;&#30446;&#21069;&#20173;&#23384;&#22312;&#30528;&#19968;&#20123;&#38382;&#39064;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#25216;&#26415;&#65292;&#20801;&#35768;&#32593;&#32476;&#20013;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#22312;&#19981;&#30452;&#25509;&#26292;&#38706;&#23458;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#38544;&#31169;&#20445;&#25252;&#30340;&#24615;&#36136;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#19981;&#22914;&#22312;&#26631;&#20934;&#38598;&#20013;&#24335;&#23398;&#20064;&#27169;&#24335;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#22833;&#34913;&#21487;&#33021;&#22312;&#21333;&#20010;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#21457;&#29983;&#65292;&#20063;&#21487;&#33021;&#22312;&#35768;&#22810;&#35774;&#22791;&#19978;&#20840;&#23616;&#21457;&#29983;&#12290;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#22797;&#26434;&#24615;&#32473;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#38656;&#35201;&#21516;&#26102;&#32531;&#35299;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#23581;&#35797;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#26412;&#25991;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#35814;&#32454;&#22238;&#39038;&#65292;&#21253;&#25324;&#24179;&#34913;&#24863;&#30693;&#20248;&#21270;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#23458;&#25143;&#31471;&#21152;&#26435;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning, which allows multiple client devices in a network to jointly train a machine learning model without direct exposure of clients' data, is an emerging distributed learning technique due to its nature of privacy preservation. However, it has been found that models trained with federated learning usually have worse performance than their counterparts trained in the standard centralized learning mode, especially when the training data is imbalanced. In the context of federated learning, data imbalance may occur either locally one one client device, or globally across many devices. The complexity of different types of data imbalance has posed challenges to the development of federated learning technique, especially considering the need of relieving data imbalance issue and preserving data privacy at the same time. Therefore, in the literature, many attempts have been made to handle class imbalance in federated learning. In this paper, we present a detailed review of recen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#24179;&#28369;&#24471;&#20998;&#20989;&#25968;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23548;&#20986;&#20854;&#21442;&#25968;&#21270;&#30340;&#36890;&#29992;&#24418;&#24335;&#65292;&#35814;&#32454;&#25551;&#36848;&#20102;&#23398;&#20064;&#24179;&#28369;&#21518;&#30340;M-&#23494;&#24230;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;M&#20010;&#29420;&#31435;&#39640;&#26031;&#36890;&#36947;&#30340;&#22240;&#23376;&#26680;&#23545;&#26410;&#30693;&#30340;&#20852;&#36259;&#23494;&#24230;&#36827;&#34892;&#24179;&#28369;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.11669</link><description>&lt;p&gt;
&#36890;&#29992;&#24179;&#28369;&#24471;&#20998;&#20989;&#25968;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Universal Smoothed Score Functions for Generative Modeling. (arXiv:2303.11669v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#24179;&#28369;&#24471;&#20998;&#20989;&#25968;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23548;&#20986;&#20854;&#21442;&#25968;&#21270;&#30340;&#36890;&#29992;&#24418;&#24335;&#65292;&#35814;&#32454;&#25551;&#36848;&#20102;&#23398;&#20064;&#24179;&#28369;&#21518;&#30340;M-&#23494;&#24230;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;M&#20010;&#29420;&#31435;&#39640;&#26031;&#36890;&#36947;&#30340;&#22240;&#23376;&#26680;&#23545;&#26410;&#30693;&#30340;&#20852;&#36259;&#23494;&#24230;&#36827;&#34892;&#24179;&#28369;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22522;&#20110;&#29992;&#20855;&#26377;&#31561;&#22122;&#22768;&#32423;&#30340;M&#20010;&#29420;&#31435;&#39640;&#26031;&#36890;&#36947;&#30340;&#22240;&#23376;&#26680;&#24179;&#28369;&#26410;&#30693;$\mathbb{R}^d$&#20852;&#36259;&#23494;&#24230;&#30340;&#29983;&#25104;&#24314;&#27169;&#38382;&#39064;&#65292;&#39318;&#20808;&#36890;&#36807;&#23548;&#20986;&#20854;&#21442;&#25968;&#21270;&#30340;&#36890;&#29992;&#24418;&#24335;&#65292;&#23436;&#25972;&#25551;&#36848;&#20102;&#22312;$\mathbb{R}^{Md}$&#20013;&#23398;&#20064;&#24179;&#28369;&#21518;&#30340;&#23494;&#24230;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65288;&#31216;&#20026;M-&#23494;&#24230;&#65289;&#65292;&#24182;&#22240;&#20026;&#20854;&#26500;&#36896;&#26041;&#24335;&#32780;&#20855;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#65307;&#25509;&#30528;&#36890;&#36807;&#20998;&#26512;&#35813;&#31867;&#39640;&#26031;&#20998;&#24067;&#30340;&#26465;&#20214;&#25968;&#65292;&#30740;&#31350;&#20102;M-&#23494;&#24230;&#25277;&#26679;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24471;&#21040;&#20102;&#38543;&#30528;$M$&#30340;&#22686;&#21152;&#32780;&#8220;&#24418;&#29366;&#8221;&#21464;&#21270;&#30340;&#20960;&#20309;&#35266;&#23519;&#32467;&#35770;&#65307;&#26368;&#21518;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#38024;&#23545;&#36825;&#31867;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#36136;&#37327;&#36827;&#34892;&#20102;&#21576;&#29616;&#33258;&#30001;&#21019;&#36896;&#36317;&#31163;&#65288;14.15&#65289;&#32467;&#26524;&#30340;&#23454;&#39564;&#65292;&#23588;&#20854;&#22312;&#38271;&#26102;&#38388;&#28151;&#21512;&#30340;MCMC&#38142;&#19978;&#20165;&#20351;&#29992;&#21333;&#20010;&#22122;&#22768;&#31243;&#24207;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#20540;&#24471;&#27880;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of generative modeling based on smoothing an unknown density of interest in $\mathbb{R}^d$ using factorial kernels with $M$ independent Gaussian channels with equal noise levels introduced by Saremi and Srivastava (2022). First, we fully characterize the time complexity of learning the resulting smoothed density in $\mathbb{R}^{Md}$, called M-density, by deriving a universal form for its parametrization in which the score function is by construction permutation equivariant. Next, we study the time complexity of sampling an M-density by analyzing its condition number for Gaussian distributions. This spectral analysis gives a geometric insight on the "shape" of M-densities as one increases $M$. Finally, we present results on the sample quality in this class of generative models on the CIFAR-10 dataset where we report Fr\'echet inception distances (14.15), notably obtained with a single noise level on long-run fast-mixing MCMC chains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#30456;&#20851;&#25968;&#25454;&#24207;&#21015;&#30340;&#23398;&#20064;&#65292;&#19981;&#20381;&#36182;&#28151;&#21512;&#21442;&#25968;&#25110;&#32773;&#36830;&#32493;&#24102;&#21442;&#29031;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;&#32479;&#19968;&#30340;&#39118;&#38505;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#24212;&#29992;&#20110;&#22330;&#26223;&#20248;&#21270;&#30340;&#30456;&#20851;&#32422;&#26463;&#38543;&#26426;&#31243;&#24207;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#35745;&#31639;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11650</link><description>&lt;p&gt;
&#24102;&#26377;&#30456;&#20851;&#25968;&#25454;&#24207;&#21015;&#30340;&#23398;&#20064;&#30340;&#32479;&#19968;&#39118;&#38505;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Uniform Risk Bounds for Learning with Dependent Data Sequences. (arXiv:2303.11650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#30456;&#20851;&#25968;&#25454;&#24207;&#21015;&#30340;&#23398;&#20064;&#65292;&#19981;&#20381;&#36182;&#28151;&#21512;&#21442;&#25968;&#25110;&#32773;&#36830;&#32493;&#24102;&#21442;&#29031;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#23637;&#31034;&#20102;&#32479;&#19968;&#30340;&#39118;&#38505;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#24212;&#29992;&#20110;&#22330;&#26223;&#20248;&#21270;&#30340;&#30456;&#20851;&#32422;&#26463;&#38543;&#26426;&#31243;&#24207;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29420;&#31435;&#25968;&#25454;&#30340;&#23398;&#20064;&#29702;&#35770;&#25512;&#24191;&#21040;&#20102;&#30456;&#20851;&#25968;&#25454;&#24207;&#21015;&#30340;&#24773;&#20917;&#12290;&#19982;&#22823;&#22810;&#25968;&#25991;&#29486;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#28151;&#21512;&#21442;&#25968;&#25110;&#32773;&#36830;&#32493;&#24102;&#21442;&#29031;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#32780;&#26159;&#37319;&#29992;&#20102;&#32463;&#20856;&#30340;&#35777;&#26126;&#27169;&#24335;&#21644;&#23481;&#37327;&#24230;&#37327;&#26469;&#25512;&#23548;&#32479;&#19968;&#30340;&#39118;&#38505;&#30028;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#20998;&#31867;&#39118;&#38505;&#26368;&#23567;&#21270;&#29702;&#35770;&#22312;&#30456;&#20851;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;VC&#32500;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22522;&#20110;R&#31639;&#23376;&#30340;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20854;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#26631;&#20934;&#30340;&#30028;&#38480;&#27809;&#26377;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#22330;&#26223;&#20248;&#21270;&#30340;&#24773;&#24418;&#65292;&#20174;&#32780;&#35745;&#31639;&#20986;&#20855;&#26377;&#30456;&#20851;&#32422;&#26463;&#30340;&#38543;&#26426;&#31243;&#24207;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper extends standard results from learning theory with independent data to sequences of dependent data. Contrary to most of the literature, we do not rely on mixing arguments or sequential measures of complexity and derive uniform risk bounds with classical proof patterns and capacity measures. In particular, we show that the standard classification risk bounds based on the VC-dimension hold in the exact same form for dependent data, and further provide Rademacher complexity-based bounds, that remain unchanged compared to the standard results for the identically and independently distributed case. Finally, we show how to apply these results in the context of scenario-based optimization in order to compute the sample complexity of random programs with dependent constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29420;&#31435;&#22270;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#26041;&#27861;$\texttt{tGLAD}$&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26631;&#35782;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#24847;&#20041;&#30340;&#27573;&#12290;</title><link>http://arxiv.org/abs/2303.11647</link><description>&lt;p&gt;
&#20320;&#29992;$\texttt{tGLAD}$&#20102;&#21527;&#65311;&#26102;&#38388;&#20250;&#21578;&#35785;&#20320;&#65281;
&lt;/p&gt;
&lt;p&gt;
Are uGLAD? Time will tell!. (arXiv:2303.11647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29420;&#31435;&#22270;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#26041;&#27861;$\texttt{tGLAD}$&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26631;&#35782;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#24847;&#20041;&#30340;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32463;&#24120;&#36935;&#21040;&#21608;&#22260;&#23384;&#22312;&#22810;&#20010;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20363;&#22914;&#29992;&#20110;&#26816;&#26597;&#33041;&#27963;&#21160;&#21464;&#21270;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#25110;&#29992;&#20110;&#30417;&#27979;&#36523;&#20307;&#36816;&#21160;&#30340;&#20256;&#24863;&#22120;&#12290;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#21106;&#26159;&#19968;&#31181;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#20013;&#21547;&#20041;&#30340;&#27169;&#24335;&#25110;&#21464;&#21270;&#30340;&#25216;&#26415;&#65292;&#36825;&#21487;&#20197;&#26631;&#24535;&#31995;&#32479;&#34892;&#20026;&#30340;&#21464;&#21270;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#20998;&#21106;&#31639;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#23427;&#20204;&#22312;&#22810;&#20803;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29420;&#31435;&#65288;CI&#65289;&#22270;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#12290;CI&#22270;&#26159;&#34920;&#31034;&#33410;&#28857;&#20043;&#38388;&#30340;&#20559;&#30456;&#20851;&#20851;&#31995;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#22810;&#20803;&#20998;&#21106;&#26694;&#26550;&#8220;$\texttt{tGLAD}$&#8221;&#65292;&#23427;&#23558;CI&#22270;&#33410;&#28857;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#21464;&#37327;&#36827;&#34892;&#24182;&#34892;&#12290;&#22312;CI&#22270;&#19978;&#24212;&#29992;&#22270;&#24418;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26631;&#35782;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#24847;&#20041;&#30340;&#27573;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We frequently encounter multiple series that are temporally correlated in our surroundings, such as EEG data to examine alterations in brain activity or sensors to monitor body movements. Segmentation of multivariate time series data is a technique for identifying meaningful patterns or changes in the time series that can signal a shift in the system's behavior. However, most segmentation algorithms have been designed primarily for univariate time series, and their performance on multivariate data remains largely unsatisfactory, making this a challenging problem. In this work, we introduce a novel approach for multivariate time series segmentation using conditional independence (CI) graphs. CI graphs are probabilistic graphical models that represents the partial correlations between the nodes. We propose a domain agnostic multivariate segmentation framework `$\texttt{tGLAD}$' which draws a parallel between the CI graph nodes and the variables of the time series. Consider applying a gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25805;&#32437;&#19978;&#28216;&#27169;&#22411;&#65292;&#23545;&#21463;&#23475;&#32773;&#35843;&#25972;&#30340;&#19979;&#28216;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#19988;&#29305;&#23450;&#30340;&#25512;&#26029;&#25915;&#20987;&#65292;&#38656;&#35201;&#27880;&#24847;&#21644;&#38450;&#33539;&#27492;&#31867;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.11643</link><description>&lt;p&gt;
&#21033;&#29992;&#36716;&#31227;&#23398;&#20064;&#26469;&#36827;&#34892;&#23646;&#24615;&#25512;&#26029;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Manipulating Transfer Learning for Property Inference. (arXiv:2303.11643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25805;&#32437;&#19978;&#28216;&#27169;&#22411;&#65292;&#23545;&#21463;&#23475;&#32773;&#35843;&#25972;&#30340;&#19979;&#28216;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#19988;&#29305;&#23450;&#30340;&#25512;&#26029;&#25915;&#20987;&#65292;&#38656;&#35201;&#27880;&#24847;&#21644;&#38450;&#33539;&#27492;&#31867;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#65288;&#19978;&#28216;&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#25317;&#26377;&#23545;&#29992;&#20110;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;&#19978;&#28216;&#27169;&#22411;&#36827;&#34892;&#25511;&#21046;&#30340;&#23545;&#25163;&#22914;&#20309;&#23545;&#21463;&#23475;&#32773;&#35843;&#25972;&#30340;&#19979;&#28216;&#27169;&#22411;&#36827;&#34892;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25915;&#20987;&#30340;&#24773;&#20917;&#65292;&#21363;&#23545;&#25163;&#21487;&#20197;&#25805;&#32437;&#19978;&#28216;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#19988;&#29305;&#23450;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65288;AUC&#24471;&#20998;&gt;0.9&#65289;&#65292;&#32780;&#19981;&#20250;&#22312;&#20027;&#35201;&#20219;&#21153;&#20013;&#20135;&#29983;&#26174;&#33879;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#25805;&#32437;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#19978;&#28216;&#27169;&#22411;&#20026;&#20855;&#26377;&#30446;&#26631;&#23646;&#24615;&#30340;&#26679;&#26412;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#28608;&#27963;&#65288;&#20013;&#38388;&#29305;&#24449;&#65289;&#65292;&#20174;&#32780;&#20351;&#23545;&#25163;&#33021;&#22815;&#36731;&#26494;&#21306;&#20998;&#35757;&#32451;&#26377;&#20855;&#26377;&#30446;&#26631;&#23646;&#24615;&#30340;&#26679;&#26412;&#21644;&#27809;&#26377;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#23454;&#39564;&#22522;&#20110;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19978;&#28216;&#27169;&#22411;&#30340;&#38754;&#37096;&#35782;&#21035;&#20219;&#21153;&#21644;ColorFeret&#25968;&#25454;&#38598;&#20316;&#20026;&#19979;&#28216;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#19994;&#32773;&#22312;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26102;&#38656;&#35201;&#27880;&#24847;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#37319;&#21462;&#25514;&#26045;&#26469;&#38450;&#27490;&#27492;&#31867;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is a popular method for tuning pretrained (upstream) models for different downstream tasks using limited data and computational resources. We study how an adversary with control over an upstream model used in transfer learning can conduct property inference attacks on a victim's tuned downstream model. For example, to infer the presence of images of a specific individual in the downstream training set. We demonstrate attacks in which an adversary can manipulate the upstream model to conduct highly effective and specific property inference attacks (AUC score $&gt; 0.9$), without incurring significant performance loss on the main task. The main idea of the manipulation is to make the upstream model generate activations (intermediate features) with different distributions for samples with and without a target property, thus enabling the adversary to distinguish easily between downstream models trained with and without training examples that have the target property. Our cod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230; Q-Network &#21644;&#25511;&#21046;&#29702;&#35770;&#35265;&#35299;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#23433;&#20840;&#23548;&#33322;&#33258;&#20027;&#36710;&#36742;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#39640;&#25928;&#19988;&#23433;&#20840;&#30340;&#39550;&#39542;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2303.11634</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#30340;&#22522;&#20110;&#28145;&#24230; Q-Network &#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Q-Network Based Decision Making for Autonomous Driving. (arXiv:2303.11634v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230; Q-Network &#21644;&#25511;&#21046;&#29702;&#35770;&#35265;&#35299;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#23433;&#20840;&#23548;&#33322;&#33258;&#20027;&#36710;&#36742;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#39640;&#25928;&#19988;&#23433;&#20840;&#30340;&#39550;&#39542;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#26159;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230; Q-Network &#21644;&#25511;&#21046;&#29702;&#35770;&#35265;&#35299;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#23433;&#20840;&#23548;&#33322;&#33258;&#20027;&#36710;&#36742;&#30340;&#26041;&#27861;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#28145;&#24230; Q-Network &#20316;&#20026;&#19968;&#20010;&#20013;&#22830;&#20915;&#31574;&#21333;&#20803;&#65292;&#25552;&#20986;&#36712;&#36857;&#35268;&#21010;&#22120;&#30340;&#30446;&#26631;&#12290;&#22522;&#20110;&#25152;&#29983;&#25104;&#30340;&#36712;&#36857;&#21644;&#32437;&#21521;&#36816;&#21160;&#30340;&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#36710;&#36947;&#21464;&#26356;&#25805;&#20316;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#21151;&#33021;&#24615;&#65292;&#26412;&#25991;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#39640;&#36895;&#20844;&#36335;&#20132;&#36890;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#29366;&#24577;&#34920;&#31034;&#23545;&#20854;&#24615;&#33021;&#21644;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#20135;&#29983;&#39640;&#25928;&#19988;&#23433;&#20840;&#30340;&#39550;&#39542;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently decision making is one of the biggest challenges in autonomous driving. This paper introduces a method for safely navigating an autonomous vehicle in highway scenarios by combining deep Q-Networks and insight from control theory. A Deep Q-Network is trained in simulation to serve as a central decision-making unit by proposing targets for a trajectory planner. The generated trajectories in combination with a controller for longitudinal movement are used to execute lane change maneuvers. In order to prove the functionality of this approach it is evaluated on two different highway traffic scenarios. Furthermore, the impact of different state representations on the performance and training process is analyzed. The results show that the proposed system can produce efficient and safe driving behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20174;&#26230;&#22278;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#20854;&#36895;&#24230;&#24555;&#65292;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#65292;&#22312;&#32570;&#38519;&#27169;&#24335;&#35782;&#21035;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.11632</link><description>&lt;p&gt;
&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#30340;&#33455;&#29255;&#29305;&#24449;&#25552;&#21462;&#21644;&#32570;&#38519;&#27169;&#24335;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Embarrassingly Simple Approach for Wafer Feature Extraction and Defect Pattern Recognition. (arXiv:2303.11632v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20174;&#26230;&#22278;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#20854;&#36895;&#24230;&#24555;&#65292;&#30452;&#35266;&#19988;&#21487;&#35299;&#37322;&#65292;&#22312;&#32570;&#38519;&#27169;&#24335;&#35782;&#21035;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#33455;&#29255;&#22270;&#20013;&#30340;&#32570;&#38519;&#27169;&#24335;&#23545;&#20110;&#25214;&#21040;&#28508;&#22312;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#24182;&#25552;&#39640;&#26230;&#22278;&#30340;&#20135;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#36825;&#20123;&#32570;&#38519;&#27169;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38750;&#24120;&#24222;&#22823;&#65292;&#24182;&#20855;&#26377;&#26174;&#30528;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;&#23427;&#20204;&#36824;&#38656;&#35201;GPU&#25903;&#25345;&#25165;&#33021;&#26377;&#25928;&#36816;&#20316;&#12290;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#20351;&#36825;&#20123;&#27169;&#22411;&#19981;&#36866;&#21512;&#20110;&#21046;&#36896;&#26230;&#22278;&#26102;&#30340;&#22312;&#32447;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20174;&#26230;&#22278;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#26497;&#20854;&#24555;&#36895;&#65292;&#30452;&#35266;&#65292;&#38750;&#21442;&#25968;&#21270;&#19988;&#21487;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#31243;&#20248;&#20110;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#25552;&#21462;&#19981;&#38656;&#35201;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#25968;&#25454;&#28857;&#30340;&#30456;&#23545;&#24418;&#29366;&#21644;&#20301;&#32622;&#65292;&#22914;&#25105;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#25152;&#25581;&#31034;&#30340;&#37027;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying defect patterns in a wafer map during manufacturing is crucial to find the root cause of the underlying issue and provides valuable insights on improving yield in the foundry. Currently used methods use deep neural networks to identify the defects. These methods are generally very huge and have significant inference time. They also require GPU support to efficiently operate. All these issues make these models not fit for on-line prediction in the manufacturing foundry. In this paper, we propose an extremely simple yet effective technique to extract features from wafer images. The proposed method is extremely fast, intuitive, and non-parametric while being explainable. The experiment results show that the proposed pipeline outperforms conventional deep learning models. Our feature extraction requires no training or fine-tuning while preserving the relative shape and location of data points as revealed by our interpretability analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#32773;&#25351;&#23548;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#20854;&#20013;&#35780;&#20272;&#32773;&#36890;&#36807;&#25511;&#21046;&#23398;&#20064;&#26041;&#21521;&#21644;&#36895;&#24230;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#29615;&#22659;&#24182;&#36991;&#20813;&#28798;&#38590;&#24615;&#24178;&#25200;&#38382;&#39064;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2303.11624</link><description>&lt;p&gt;
&#38754;&#21521;&#25345;&#32493;&#29615;&#22659;&#30340;&#35780;&#20272;&#32773;&#25351;&#23548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Assessor-Guided Learning for Continual Environments. (arXiv:2303.11624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#32773;&#25351;&#23548;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#20854;&#20013;&#35780;&#20272;&#32773;&#36890;&#36807;&#25511;&#21046;&#23398;&#20064;&#26041;&#21521;&#21644;&#36895;&#24230;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#29615;&#22659;&#24182;&#36991;&#20813;&#28798;&#38590;&#24615;&#24178;&#25200;&#38382;&#39064;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#32773;&#25351;&#23548;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#65292;&#20854;&#20013;&#19968;&#20010;&#35780;&#20272;&#32773;&#36890;&#36807;&#25511;&#21046;&#23398;&#20064;&#36807;&#31243;&#30340;&#26041;&#21521;&#21644;&#36895;&#24230;&#26469;&#25351;&#23548;&#22522;&#30784;&#23398;&#20064;&#32773;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#29615;&#22659;&#24182;&#38450;&#27490;&#28798;&#38590;&#24615;&#24178;&#25200;&#38382;&#39064;&#30340;&#21457;&#29983;&#12290;&#35780;&#20272;&#32773;&#20197;&#20803;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20803;&#30446;&#26631;&#26159;&#22686;&#24378;&#22522;&#30784;&#23398;&#20064;&#32773;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#23427;&#25191;&#34892;&#27599;&#20010;&#26679;&#26412;&#30340;&#36719;&#21152;&#26435;&#26426;&#21046;&#65292;&#25509;&#21463;&#27491;&#26679;&#26412;&#24182;&#25298;&#32477;&#36127;&#26679;&#26412;&#12290;&#22522;&#30784;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12289;&#26263;&#20307;&#39564;&#37325;&#25918;&#65288;DER&#65289;&#25439;&#22833;&#20989;&#25968;&#21644;&#30693;&#35782;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#30340;&#20803;&#21152;&#26435;&#32452;&#21512;&#65292;&#36825;&#20123;&#20132;&#20114;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an assessor-guided learning strategy for continual learning where an assessor guides the learning process of a base learner by controlling the direction and pace of the learning process thus allowing an efficient learning of new environments while protecting against the catastrophic interference problem. The assessor is trained in a meta-learning manner with a meta-objective to boost the learning process of the base learner. It performs a soft-weighting mechanism of every sample accepting positive samples while rejecting negative samples. The training objective of a base learner is to minimize a meta-weighted combination of the cross entropy loss function, the dark experience replay (DER) loss function and the knowledge distillation loss function whose interactions are controlled in such a way to attain an improved performance. A compensated over-sampling (COS) strategy is developed to overcome the class imbalanced problem of the episodic memory due to limited memor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21644;&#31215;&#22810;&#39033;&#24335;&#30340;&#28856;&#35010;&#31639;&#27861;&#21644;&#20854; RLCT&#12290;</title><link>http://arxiv.org/abs/2303.11619</link><description>&lt;p&gt;
&#27714;&#35299;&#21644;&#31215;&#22810;&#39033;&#24335;&#21644;&#23454;&#23545;&#25968;&#35268;&#33539;&#38408;&#20540;&#30340;&#28856;&#35010;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Blow-up Algorithm for Sum-of-Products Polynomials and Real Log Canonical Thresholds. (arXiv:2303.11619v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21644;&#31215;&#22810;&#39033;&#24335;&#30340;&#28856;&#35010;&#31639;&#27861;&#21644;&#20854; RLCT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#32473;&#20986;&#36125;&#21494;&#26031;&#24191;&#20041;&#35823;&#24046;&#30340;&#23454;&#23545;&#25968;&#35268;&#33539;&#38408;&#20540;&#26102;&#65292;&#35770;&#25991;&#29992;&#31245;&#24494;&#31616;&#21333;&#30340;&#22810;&#39033;&#24335;&#26367;&#25442;&#24179;&#22343;&#35823;&#24046;&#20989;&#25968;&#65292;&#20854; RLCT &#23545;&#24212;&#20110;&#24179;&#22343;&#35823;&#24046;&#20989;&#25968;&#30340; RLCT&#65292;&#24182;&#36890;&#36807;&#31216;&#20026;&#28856;&#35010;&#30340;&#20195;&#25968;&#25805;&#20316;&#35299;&#20915;&#20854;&#22855;&#28857;&#26469;&#33719;&#24471;&#20854; RLCT&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#65292;&#20219;&#20309;&#22810;&#39033;&#24335;&#30340;&#22855;&#28857;&#37117;&#21487;&#20197;&#36890;&#36807;&#26377;&#38480;&#27425;&#30340;&#28856;&#35010;&#36845;&#20195;&#26469;&#35299;&#20915;&#65292;&#20294;&#24182;&#27809;&#26377;&#26126;&#30830;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#29305;&#23450;&#30340;&#28856;&#35010;&#31639;&#27861;&#26469;&#35299;&#20915;&#29305;&#23450;&#22810;&#39033;&#24335;&#30340;&#22855;&#28857;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#32771;&#34385;&#20102;&#31216;&#20026;&#21644;&#31215;&#22810;&#39033;&#24335;&#30340;&#22810;&#39033;&#24335;&#30340;&#28856;&#35010;&#31639;&#27861;&#21450;&#20854; RLCT&#12290;
&lt;/p&gt;
&lt;p&gt;
When considering a real log canonical threshold (RLCT) that gives a Bayesian generalization error, in general, papers replace a mean error function with a relatively simple polynomial whose RLCT corresponds to that of the mean error function, and obtain its RLCT by resolving its singularities through an algebraic operation called blow-up. Though it is known that the singularities of any polynomial can be resolved by a finite number of blow-up iterations, it is not clarified whether or not it is possible to resolve singularities of a specific polynomial by applying a specific blow-up algorithm. Therefore this paper considers the blow-up algorithm for the polynomials called sum-of-products (sop) polynomials and its RLCT.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#39640;&#32500;&#24230;&#29699;&#20307;&#19978;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#20351;&#29992;SGD&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11602</link><description>&lt;p&gt;
&#24102;&#24212;&#29992;&#20110;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#30340;&#21442;&#25968;&#21270;&#29699;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation. (arXiv:2303.11602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#39640;&#32500;&#24230;&#29699;&#20307;&#19978;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#20351;&#29992;SGD&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#30001;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20026;&#24120;&#25968;&#20493;&#30340;&#39640;&#32500;&#29699;&#19978;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31867;&#22411;&#31639;&#27861;&#12290;&#25105;&#20204;&#20026;&#26377;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#39318;&#27425;&#25552;&#20379;&#20102;&#26080;&#30417;&#30563;&#35774;&#32622;&#30340;&#25910;&#25947;&#35777;&#26126;&#65292;&#35813;&#35774;&#32622;&#23545;&#24212;&#20110;&#37327;&#23376;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#65288;VMC&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze stochastic gradient descent (SGD) type algorithms on a high-dimensional sphere which is parameterized by a neural network up to a normalization constant. We provide a new algorithm for the setting of supervised learning and show its convergence both theoretically and numerically. We also provide the first proof of convergence for the unsupervised setting, which corresponds to the widely used variational Monte Carlo (VMC) method in quantum physics.
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.11593</link><description>&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#25163;&#24615;&#26102;&#23384;&#22312;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Difficulty in learning chirality for Transformer fed with SMILES. (arXiv:2303.11593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11593
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;SMILES&#24207;&#21015;&#30340;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#20998;&#23376;&#32467;&#26500;&#30340;&#25972;&#20307;&#24615;&#21644;&#25163;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#23545;&#26497;&#20854;&#22810;&#26679;&#30340;&#20998;&#23376;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#25551;&#36848;&#31526;&#29983;&#25104;&#24050;&#32463;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24212;&#29992;&#20110;SMILES&#65292;&#21363;&#20998;&#23376;&#32467;&#26500;&#30340;&#25991;&#23383;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21270;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;NLP&#27169;&#22411;&#8212;&#8212;Transformer&#65292;&#22312;&#23398;&#20064;SMILES&#21644;&#21270;&#23398;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;Transformer&#24555;&#36895;&#23398;&#20064;&#20998;&#23376;&#30340;&#37096;&#20998;&#32467;&#26500;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#25165;&#33021;&#29702;&#35299;&#25972;&#20307;&#32467;&#26500;&#12290;&#19982;&#20043;&#19968;&#33268;&#30340;&#26159;&#65292;&#22312;&#19981;&#21516;&#30340;&#23398;&#20064;&#27493;&#39588;&#20013;&#29983;&#25104;&#30340;&#25551;&#36848;&#31526;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26102;&#30340;&#20934;&#30830;&#29575;&#20174;&#24320;&#22987;&#21040;&#35757;&#32451;&#32467;&#26463;&#37117;&#26159;&#30456;&#20284;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;Transformer&#38656;&#35201;&#29305;&#21035;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#25165;&#33021;&#23398;&#20064;&#25163;&#24615;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20986;&#29616;&#20302;&#32763;&#35793;&#20934;&#30830;&#29575;&#30340;&#20572;&#28382;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen development of descriptor generation based on representation learning of extremely diverse molecules, especially those that apply natural language processing (NLP) models to SMILES, a literal representation of molecular structure. However, little research has been done on how these models understand chemical structure. To address this, we investigated the relationship between the learning progress of SMILES and chemical structure using a representative NLP model, the Transformer. The results suggest that while the Transformer learns partial structures of molecules quickly, it requires extended training to understand overall structures. Consistently, the accuracy of molecular property predictions using descriptors generated from models at different learning steps was similar from the beginning to the end of training. Furthermore, we found that the Transformer requires particularly long training to learn chirality and sometimes stagnates with low translation accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#36890;&#36807;&#27491;&#24577;&#36817;&#20284;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;&#37319;&#29992;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11582</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#36866;&#24212;&#24615;&#23454;&#39564;&#65306;&#28789;&#27963;&#25209;&#22788;&#29702;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Experimentation at Scale: Bayesian Algorithms for Flexible Batches. (arXiv:2303.11582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#36890;&#36807;&#27491;&#24577;&#36817;&#20284;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;&#37319;&#29992;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#20551;&#23450;&#25345;&#32493;&#37325;&#26032;&#20998;&#37197;&#27979;&#37327;&#24037;&#20316;&#65292;&#36825;&#22312;&#23454;&#29616;&#36807;&#31243;&#20013;&#23384;&#22312;&#24310;&#36831;&#21453;&#39304;&#21644;&#22522;&#30784;&#35774;&#26045;/&#32452;&#32455;&#38590;&#39064;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#20165;&#26377;&#23569;&#25968;&#37325;&#26032;&#20998;&#37197;&#38454;&#27573;&#30340;&#23454;&#38469;&#24773;&#20917;&#65292;&#20854;&#20013;&#27979;&#37327;&#32467;&#26524;&#26159;&#20197;&#25209;&#22788;&#29702;&#24418;&#24335;&#27979;&#37327;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#24615;&#23454;&#39564;&#26694;&#26550;&#65292;&#21487;&#28789;&#27963;&#22788;&#29702;&#20219;&#20309;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#26159;&#65292;&#22312;&#32479;&#35745;&#25512;&#26029;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#27491;&#24577;&#36817;&#20284;&#20063;&#21487;&#20197;&#25351;&#23548;&#21487;&#25193;&#23637;&#33258;&#36866;&#24212;&#35774;&#35745;&#12290;&#36890;&#36807;&#25512;&#23548;&#28176;&#36827;&#39034;&#24207;&#23454;&#39564;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#65292;&#21487;&#20197;&#21033;&#29992;&#24179;&#22343;&#22238;&#25253;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#21160;&#24577;&#35268;&#21010;&#30340;&#29366;&#24577;&#36716;&#31227;&#30456;&#23545;&#20110;&#37319;&#26679;&#20998;&#37197;&#26159;&#21487;&#24494;&#30340;&#65292;&#20801;&#35768;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#35268;&#21010;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36845;&#20195;&#35268;&#21010;&#26041;&#27861;&#65292;&#21363;&#27531;&#20313;&#26102;&#38480;&#20248;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#30340;&#35268;&#21010;&#30446;&#26631;&#26469;&#36873;&#25321;&#37319;&#26679;&#20998;&#37197;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#27169;&#22359;&#21270;&#21644;&#26131;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard bandit algorithms that assume continual reallocation of measurement effort are challenging to implement due to delayed feedback and infrastructural/organizational difficulties. Motivated by practical instances involving a handful of reallocation epochs in which outcomes are measured in batches, we develop a new adaptive experimentation framework that can flexibly handle any batch size. Our main observation is that normal approximations universal in statistical inference can also guide the design of scalable adaptive designs. By deriving an asymptotic sequential experiment, we formulate a dynamic program that can leverage prior information on average rewards. State transitions of the dynamic program are differentiable with respect to the sampling allocations, allowing the use of gradient-based methods for planning and policy optimization. We propose a simple iterative planning method, Residual Horizon Optimization, which selects sampling allocations by optimizing a planning obj
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#25512;&#26029;&#31639;&#27861;&#31616;&#21270;&#24182;&#23884;&#20837;&#20135;&#21697;&#20195;&#30721;&#20013;&#65292;&#20197;&#20943;&#23569;&#32593;&#32476;&#36890;&#20449;&#65292;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#23454;&#26102;&#24179;&#21488;&#19978;&#21487;&#23558;&#25512;&#26029;&#24310;&#36831;&#38477;&#20302;1.3&#20493;&#65292;CPU&#36164;&#28304;&#20943;&#23569;30&#65285;&#65292;&#24182;&#23558;&#24212;&#29992;&#31243;&#24207;&#21069;&#31471;&#21644;&#21518;&#31471;&#20043;&#38388;&#30340;&#32593;&#32476;&#36890;&#20449;&#20943;&#23569;60&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.11580</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39640;&#25928;&#22810;&#32423;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-stage Inference on Tabular Data. (arXiv:2303.11580v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23558;&#25512;&#26029;&#31639;&#27861;&#31616;&#21270;&#24182;&#23884;&#20837;&#20135;&#21697;&#20195;&#30721;&#20013;&#65292;&#20197;&#20943;&#23569;&#32593;&#32476;&#36890;&#20449;&#65292;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#23454;&#26102;&#24179;&#21488;&#19978;&#21487;&#23558;&#25512;&#26029;&#24310;&#36831;&#38477;&#20302;1.3&#20493;&#65292;CPU&#36164;&#28304;&#20943;&#23569;30&#65285;&#65292;&#24182;&#23558;&#24212;&#29992;&#31243;&#24207;&#21069;&#31471;&#21644;&#21518;&#31471;&#20043;&#38388;&#30340;&#32593;&#32476;&#36890;&#20449;&#20943;&#23569;60&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21644;&#20135;&#21697;&#36890;&#36807;&#20013;&#31561;&#25968;&#37327;&#30340;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;&#23454;&#26102;&#25512;&#26029;&#26102;&#34987;&#29942;&#39048;&#25152;&#22256;&#12290;&#20256;&#32479;&#26234;&#24935;&#22312;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#65292;&#20542;&#21521;&#20110;&#23558;ML&#20195;&#30721;&#20998;&#21106;&#25104;&#26381;&#21153;&#65292;&#24182;&#36890;&#36807;&#36828;&#31243;&#36807;&#31243;&#35843;&#29992;&#65288;RPC&#65289;API&#34987;&#20135;&#21697;&#20195;&#30721;&#26597;&#35810;&#12290;&#36825;&#31181;&#26041;&#27861;&#28548;&#28165;&#20102;&#25972;&#20307;&#36719;&#20214;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25277;&#35937;ML&#20869;&#37096;&#31616;&#21270;&#20102;&#20135;&#21697;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#31163;&#22686;&#21152;&#20102;&#32593;&#32476;&#24310;&#36831;&#65292;&#24182;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;CPU&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31616;&#21270;&#25512;&#26029;&#31639;&#27861;&#24182;&#23558;&#20854;&#23884;&#20837;&#20135;&#21697;&#20195;&#30721;&#20013;&#65292;&#20197;&#20943;&#23569;&#32593;&#32476;&#36890;&#20449;&#12290;&#38024;&#23545;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#39640;&#24615;&#33021;&#23454;&#26102;&#24179;&#21488;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#24120;&#26377;&#36229;&#36807;&#19968;&#21322;&#30340;&#36755;&#20837;&#21487;&#20197;&#36866;&#24212;&#36825;&#31181;&#20248;&#21270;&#65292;&#32780;&#20854;&#20313;&#37096;&#20998;&#21487;&#20197;&#30001;&#21407;&#22987;&#27169;&#22411;&#22788;&#29702;&#12290;&#36890;&#36807;&#23558;AutoML&#24212;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#25105;&#20204;&#23558;&#25512;&#26029;&#24310;&#36831;&#38477;&#20302;&#20102;1.3&#20493;&#65292;CPU&#36164;&#28304;&#20943;&#23569;&#20102;30&#65285;&#65292;&#24212;&#29992;&#31243;&#24207;&#21069;&#31471;&#21644;&#21518;&#31471;&#20043;&#38388;&#30340;&#32593;&#32476;&#36890;&#20449;&#20943;&#23569;&#20102;60&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many ML applications and products train on medium amounts of input data but get bottlenecked in real-time inference. When implementing ML systems, conventional wisdom favors segregating ML code into services queried by product code via Remote Procedure Call (RPC) APIs. This approach clarifies the overall software architecture and simplifies product code by abstracting away ML internals. However, the separation adds network latency and entails additional CPU overhead. Hence, we simplify inference algorithms and embed them into the product code to reduce network communication. For public datasets and a high-performance real-time platform that deals with tabular data, we show that over half of the inputs are often amenable to such optimization, while the remainder can be handled by the original model. By applying our optimization with AutoML to both training and inference, we reduce inference latency by 1.3x, CPU resources by 30%, and network communication between application front-end an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30456;&#37051;&#30340;&#22810;&#20445;&#30495;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#20849;&#20139;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#35299;&#20915;&#26041;&#26696;&#30340;&#29305;&#24449;&#31354;&#38388;&#26469;&#20943;&#23569;&#25110;&#28040;&#38500;&#23545;&#39640;&#31934;&#24230;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#36825;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.11577</link><description>&lt;p&gt;
&#29305;&#24449;&#30456;&#37051;&#22810;&#20445;&#30495;&#29289;&#29702;&#23398;&#20064;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Feature-adjacent multi-fidelity physics-informed machine learning for partial differential equations. (arXiv:2303.11577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11577
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30456;&#37051;&#30340;&#22810;&#20445;&#30495;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#20849;&#20139;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#35299;&#20915;&#26041;&#26696;&#30340;&#29305;&#24449;&#31354;&#38388;&#26469;&#20943;&#23569;&#25110;&#28040;&#38500;&#23545;&#39640;&#31934;&#24230;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#36825;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#22791;&#36873;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#38382;&#39064;&#65292;&#36825;&#31181;&#32593;&#32476;&#30340;&#35757;&#32451;&#20173;&#28982;&#38656;&#35201;&#39640;&#31934;&#24230;&#30340;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#30340;&#29983;&#25104;&#25104;&#26412;&#21487;&#33021;&#24456;&#39640;&#12290;&#20026;&#20102;&#20943;&#23569;&#29978;&#33267;&#28040;&#38500;&#23545;&#39640;&#20445;&#30495;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#30340;&#22810;&#20445;&#30495;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#31354;&#38388;&#30001;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#35299;&#20915;&#26041;&#26696;&#20849;&#20139;&#12290;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#20302;&#31934;&#24230;&#21644;&#39640;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#30340;&#25237;&#24433;&#30456;&#37051;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#23427;&#20204;&#30340;&#30456;&#23545;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#29305;&#24449;&#31354;&#38388;&#30001;&#32534;&#30721;&#22120;&#34920;&#31034;&#65292;&#20854;&#26144;&#23556;&#21040;&#21407;&#22987;&#35299;&#31354;&#38388;&#36890;&#36807;&#35299;&#30721;&#22120;&#23454;&#29616;&#12290;&#25152;&#25552;&#20986;&#30340;&#22810;&#20445;&#30495;&#26041;&#27861;&#22312;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#23450;&#24577;&#21644;&#38750;&#23450;&#24577;&#38382;&#39064;&#30340;&#27491;&#38382;&#39064;&#21644;&#36870;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks have emerged as an alternative method for solving partial differential equations. However, for complex problems, the training of such networks can still require high-fidelity data which can be expensive to generate. To reduce or even eliminate the dependency on high-fidelity data, we propose a novel multi-fidelity architecture which is based on a feature space shared by the low- and high-fidelity solutions. In the feature space, the projections of the low-fidelity and high-fidelity solutions are adjacent by constraining their relative distance. The feature space is represented with an encoder and its mapping to the original solution space is effected through a decoder. The proposed multi-fidelity approach is validated on forward and inverse problems for steady and unsteady problems described by partial differential equations.
&lt;/p&gt;</description></item><item><title>DECENT&#26159;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#24322;&#26500;&#20849;&#21516;&#28436;&#21270;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#24739;&#32773;&#12289;&#21307;&#29983;&#12289;&#25151;&#38388;&#21644;&#33647;&#29289;&#30340;&#24322;&#26500;&#21160;&#24577;&#23884;&#20837;&#65292;&#29992;&#20110;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#65292;&#22914;&#33647;&#29289;&#25512;&#33616;&#12289;&#24739;&#32773;&#39118;&#38505;&#20998;&#23618;&#21644;&#21307;&#38498;&#23481;&#37327;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.11563</link><description>&lt;p&gt;
&#21160;&#24577;&#20581;&#24247;&#23884;&#20837;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Dynamic Healthcare Embeddings for Improving Patient Care. (arXiv:2303.11563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11563
&lt;/p&gt;
&lt;p&gt;
DECENT&#26159;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#24322;&#26500;&#20849;&#21516;&#28436;&#21270;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21508;&#31181;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#24739;&#32773;&#12289;&#21307;&#29983;&#12289;&#25151;&#38388;&#21644;&#33647;&#29289;&#30340;&#24322;&#26500;&#21160;&#24577;&#23884;&#20837;&#65292;&#29992;&#20110;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#65292;&#22914;&#33647;&#29289;&#25512;&#33616;&#12289;&#24739;&#32773;&#39118;&#38505;&#20998;&#23618;&#21644;&#21307;&#38498;&#23481;&#37327;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#38498;&#21521;&#33258;&#21160;&#21270;&#21644;&#38598;&#25104;&#20182;&#20204;&#30340;&#35745;&#31639;&#31995;&#32479;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#31934;&#32454;&#21270;&#21307;&#38498;&#36816;&#33829;&#25968;&#25454;&#21464;&#24471;&#21487;&#29992;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#25324;&#21307;&#38498;&#24314;&#31569;&#22270;&#32440;&#65292;&#24739;&#32773;&#21644;&#21307;&#25252;&#20154;&#21592;&#20043;&#38388;&#30340;&#20132;&#20114;&#26085;&#24535;&#65292;&#22788;&#26041;&#25968;&#25454;&#65292;&#31243;&#24207;&#25968;&#25454;&#20197;&#21450;&#20851;&#20110;&#24739;&#32773;&#20837;&#38498;&#12289;&#20986;&#38498;&#21644;&#36716;&#31227;&#30340;&#25968;&#25454;&#12290;&#36825;&#20026;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#24320;&#36767;&#20102;&#35768;&#22810;&#36855;&#20154;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21033;&#29992;&#29616;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#36827;&#34892;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#20174;&#24322;&#26500;&#30340;&#12289;&#21160;&#24577;&#30340;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#28041;&#21450;&#23454;&#20307;&#30340;&#32467;&#26500;&#34920;&#31034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DECTEN&#65292;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#24322;&#26500;&#20849;&#21516;&#28436;&#21270;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#21508;&#31181;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#24739;&#32773;&#12289;&#21307;&#29983;&#12289;&#25151;&#38388;&#21644;&#33647;&#29289;&#30340;&#24322;&#26500;&#21160;&#24577;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#22522;&#20110;&#38745;&#24577;&#23646;&#24615;&#21644;&#21160;&#24577;&#34892;&#20026;&#25429;&#25417;&#21307;&#29983;&#12289;&#25151;&#38388;&#12289;&#24739;&#32773;&#21644;&#33647;&#29289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#33647;&#29289;&#25512;&#33616;&#12289;&#24739;&#32773;&#39118;&#38505;&#20998;&#23618;&#21644;&#21307;&#38498;&#23481;&#37327;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
As hospitals move towards automating and integrating their computing systems, more fine-grained hospital operations data are becoming available. These data include hospital architectural drawings, logs of interactions between patients and healthcare professionals, prescription data, procedures data, and data on patient admission, discharge, and transfers. This has opened up many fascinating avenues for healthcare-related prediction tasks for improving patient care. However, in order to leverage off-the-shelf machine learning software for these tasks, one needs to learn structured representations of entities involved from heterogeneous, dynamic data streams. Here, we propose DECENT, an auto-encoding heterogeneous co-evolving dynamic neural network, for learning heterogeneous dynamic embeddings of patients, doctors, rooms, and medications from diverse data streams. These embeddings capture similarities among doctors, rooms, patients, and medications based on static attributes and dynamic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#37319;&#29992;&#22686;&#24378;&#25311;&#21512;&#33021;&#21147;&#65292;&#36880;&#28176;&#22686;&#21152;&#40065;&#26834;&#24615;&#30340;&#26435;&#37325;&#26469;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#12290;&#22312;&#21518;&#26399;&#38454;&#27573;&#65292;&#24341;&#20837;&#33258;&#20030;&#39033;&#65292;&#35753;DNN&#26356;&#21152;&#37325;&#35270;&#23481;&#26131;&#30340;&#26679;&#20363;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11562</link><description>&lt;p&gt;
&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic-Aware Loss for Learning with Label Noise. (arXiv:2303.11562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#37319;&#29992;&#22686;&#24378;&#25311;&#21512;&#33021;&#21147;&#65292;&#36880;&#28176;&#22686;&#21152;&#40065;&#26834;&#24615;&#30340;&#26435;&#37325;&#26469;&#22788;&#29702;&#26631;&#31614;&#22122;&#22768;&#12290;&#22312;&#21518;&#26399;&#38454;&#27573;&#65292;&#24341;&#20837;&#33258;&#20030;&#39033;&#65292;&#35753;DNN&#26356;&#21152;&#37325;&#35270;&#23481;&#26131;&#30340;&#26679;&#20363;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#20351;&#29992;&#26082;&#33021;&#25311;&#21512;&#21448;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24378;&#20581;&#25439;&#22833;&#20989;&#25968;&#26159;&#22788;&#29702;&#27492;&#38382;&#39064;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#22240;&#32032;&#20043;&#38388;&#30340;&#38745;&#24577;&#26435;&#34913;&#19982;DNN&#23398;&#20064;&#26631;&#31614;&#22122;&#22768;&#30340;&#21160;&#24577;&#24615;&#30456;&#30683;&#30462;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21160;&#24577;&#24863;&#30693;&#25439;&#22833;(DAL)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;DNN&#20542;&#21521;&#20110;&#20808;&#23398;&#20064;&#19968;&#33324;&#21270;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#36880;&#28176;&#36807;&#25311;&#21512;&#26631;&#31614;&#22122;&#22768;&#65292;DAL&#26368;&#21021;&#22686;&#24378;&#20102;&#25311;&#21512;&#33021;&#21147;&#65292;&#28982;&#21518;&#36880;&#28176;&#22686;&#21152;&#20102;&#40065;&#26834;&#24615;&#30340;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#22312;&#21518;&#26399;&#38454;&#27573;&#65292;&#25105;&#20204;&#35753;DNN&#26356;&#21152;&#37325;&#35270;&#23481;&#26131;&#30340;&#26679;&#20363;&#65292;&#36825;&#20123;&#26679;&#20363;&#26356;&#23481;&#26131;&#26631;&#35760;&#20026;&#27491;&#30830;&#30340;&#26631;&#31614;&#65292;&#24182;&#24341;&#20837;&#33258;&#20030;&#39033;&#26469;&#36827;&#19968;&#27493;&#20943;&#23569;&#26631;&#31614;&#22122;&#22768;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise poses a serious threat to deep neural networks (DNNs). Employing robust loss function which reconciles fitting ability with robustness is a simple but effective strategy to handle this problem. However, the widely-used static trade-off between these two factors contradicts the dynamic nature of DNNs learning with label noise, leading to inferior performance. Therefore, we propose a dynamics-aware loss (DAL) to solve this problem. Considering that DNNs tend to first learn generalized patterns, then gradually overfit label noise, DAL strengthens the fitting ability initially, then gradually increases the weight of robustness. Moreover, at the later stage, we let DNNs put more emphasis on easy examples which are more likely to be correctly labeled than hard ones and introduce a bootstrapping term to further reduce the negative impact of label noise. Both the detailed theoretical analyses and extensive experimental results demonstrate the superiority of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21160;&#24577;&#39030;&#28857;&#26367;&#25442;&#25991;&#27861;&#65288;DyVeRG&#65289;&#65292;&#23427;&#20204;&#25552;&#20379;&#19968;&#31181;&#22312;&#26102;&#38388;&#22495;&#20869;&#26356;&#26032;&#23398;&#20064;&#22270;&#25991;&#27861;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#39044;&#27979;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#22270;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11553</link><description>&lt;p&gt;
&#21160;&#24577;&#39030;&#28857;&#26367;&#25442;&#25991;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Vertex Replacement Grammars. (arXiv:2303.11553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21160;&#24577;&#39030;&#28857;&#26367;&#25442;&#25991;&#27861;&#65288;DyVeRG&#65289;&#65292;&#23427;&#20204;&#25552;&#20379;&#19968;&#31181;&#22312;&#26102;&#38388;&#22495;&#20869;&#26356;&#26032;&#23398;&#20064;&#22270;&#25991;&#27861;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#39044;&#27979;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#22270;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#26080;&#20851;&#22270;&#25991;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24314;&#27169;&#32467;&#26500;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22270;&#25991;&#27861;&#32570;&#20047;&#25429;&#25417;&#26102;&#21464;&#29616;&#35937;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#20135;&#29983;&#35268;&#21017;&#30340;&#20174;&#24038;&#21040;&#21491;&#30340;&#36716;&#25442;&#19981;&#34920;&#31034;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#21160;&#24577;&#39030;&#28857;&#26367;&#25442;&#25991;&#27861;&#65288;DyVeRG&#65289;&#65292;&#23427;&#20204;&#22312;&#26102;&#38388;&#22495;&#20013;&#25512;&#24191;&#20102;&#39030;&#28857;&#26367;&#25442;&#25991;&#27861;&#65292;&#36890;&#36807;&#20026;&#23398;&#20064;&#22270;&#25991;&#27861;&#26356;&#26032;&#25552;&#20379;&#24418;&#24335;&#26694;&#26550;&#65292;&#20197;&#31526;&#21512;&#20854;&#22522;&#30784;&#25968;&#25454;&#30340;&#20462;&#25913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DyVeRG&#25991;&#27861;&#21487;&#20197;&#20174;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#34987;&#29992;&#20110;&#24544;&#23454;&#22320;&#29983;&#25104;&#36825;&#20123;&#22270;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#35745;&#31639;DyVeRG&#26354;&#32447;&#29983;&#25104;&#30340;&#24503;&#27779;&#23572;&#36125;&#26684;&#24046;&#24322;&#20998;&#25968;&#65288;dyvergence scores&#65289;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#36825;&#26159;&#30001;&#35813;&#26694;&#26550;&#24341;&#20986;&#30340;&#19968;&#31181;&#26032;&#30340;&#22270;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context-free graph grammars have shown a remarkable ability to model structures in real-world relational data. However, graph grammars lack the ability to capture time-changing phenomena since the left-to-right transitions of a production rule do not represent temporal change. In the present work, we describe dynamic vertex-replacement grammars (DyVeRG), which generalize vertex replacement grammars in the time domain by providing a formal framework for updating a learned graph grammar in accordance with modifications to its underlying data. We show that DyVeRG grammars can be learned from, and used to generate, real-world dynamic graphs faithfully while remaining human-interpretable. We also demonstrate their ability to forecast by computing dyvergence scores, a novel graph similarity measurement exposed by this framework.
&lt;/p&gt;</description></item><item><title>ModEFormer&#20351;&#29992;&#21464;&#21387;&#22120;&#29420;&#31435;&#25552;&#21462;&#38899;&#39057;&#21644;&#35270;&#39057;&#23884;&#20837;&#65292;&#24182;&#20445;&#30041;&#36755;&#20837;&#27969;&#30340;&#27169;&#24577;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#25209;&#37327;&#21644;&#26356;&#22810;&#30340;&#36127;&#38899;&#39057;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#36825;&#20351;&#24471;&#20854;&#22312;&#26816;&#27979;&#30005;&#35270;&#24191;&#25773;&#21644;&#35270;&#39057;&#20250;&#35758;&#20013;&#30340;&#38899;&#35270;&#39057;&#21516;&#27493;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11551</link><description>&lt;p&gt;
ModEFormer&#65306;&#20351;&#29992;&#21464;&#21387;&#22120;&#36827;&#34892;&#38899;&#35270;&#39057;&#21516;&#27493;&#30340;&#27169;&#24577;&#20445;&#30041;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
ModEFormer: Modality-Preserving Embedding for Audio-Video Synchronization using Transformers. (arXiv:2303.11551v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11551
&lt;/p&gt;
&lt;p&gt;
ModEFormer&#20351;&#29992;&#21464;&#21387;&#22120;&#29420;&#31435;&#25552;&#21462;&#38899;&#39057;&#21644;&#35270;&#39057;&#23884;&#20837;&#65292;&#24182;&#20445;&#30041;&#36755;&#20837;&#27969;&#30340;&#27169;&#24577;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#25209;&#37327;&#21644;&#26356;&#22810;&#30340;&#36127;&#38899;&#39057;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#36825;&#20351;&#24471;&#20854;&#22312;&#26816;&#27979;&#30005;&#35270;&#24191;&#25773;&#21644;&#35270;&#39057;&#20250;&#35758;&#20013;&#30340;&#38899;&#35270;&#39057;&#21516;&#27493;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#35270;&#24191;&#25773;&#21644;&#35270;&#39057;&#20250;&#35758;&#20013;&#38899;&#35270;&#39057;&#19981;&#21516;&#27493;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#20250;&#23548;&#33268;&#35266;&#30475;&#20307;&#39564;&#19981;&#20339;&#12290;&#19968;&#31181;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#24335;&#26159;&#21019;&#24314;&#19968;&#20010;&#35823;&#24046;&#26816;&#27979;&#26426;&#21046;&#26469;&#35782;&#21035;&#38899;&#39057;&#36229;&#21069;&#25110;&#28382;&#21518;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ModEFormer&#65292;&#36890;&#36807;&#27169;&#24577;&#29305;&#23450;&#30340;&#21464;&#21387;&#22120;&#29420;&#31435;&#25552;&#21462;&#38899;&#39057;&#21644;&#35270;&#39057;&#23884;&#20837;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;ModEFormer&#20445;&#30041;&#36755;&#20837;&#27969;&#30340;&#27169;&#24577;&#65292;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26356;&#22823;&#30340;&#25209;&#37327;&#21644;&#26356;&#22810;&#30340;&#36127;&#38899;&#39057;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;&#20013;&#36127;&#26679;&#26412;&#25968;&#37327;&#21644;&#21807;&#19968;&#26679;&#26412;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#26126;&#26174;&#36229;&#36807;&#20197;&#21069;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ModEFormer&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;LRS2&#21644;LRS3&#20998;&#21035;&#36798;&#21040;&#20102;94.5&#65285;&#21644;90.9&#65285;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;ModEFormer&#23545;&#27979;&#35797;&#29255;&#27573;&#36827;&#34892;&#20559;&#31227;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lack of audio-video synchronization is a common problem during television broadcasts and video conferencing, leading to an unsatisfactory viewing experience. A widely accepted paradigm is to create an error detection mechanism that identifies the cases when audio is leading or lagging. We propose ModEFormer, which independently extracts audio and video embeddings using modality-specific transformers. Different from the other transformer-based approaches, ModEFormer preserves the modality of the input streams which allows us to use a larger batch size with more negative audio samples for contrastive learning. Further, we propose a trade-off between the number of negative samples and number of unique samples in a batch to significantly exceed the performance of previous methods. Experimental results show that ModEFormer achieves state-of-the-art performance, 94.5% for LRS2 and 90.9% for LRS3. Finally, we demonstrate how ModEFormer can be used for offset detection for test clips.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25511;&#39046;&#22495;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#24050;&#20998;&#35299;&#23376;&#31354;&#38388;&#20013;&#20445;&#30041;&#28304;&#29305;&#24449;&#65292;&#20351;&#24471;&#21482;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#23601;&#33021;&#24179;&#28369;&#22320;&#25511;&#21046;&#20445;&#30041;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#20135;&#29983;&#26356;&#19968;&#33268;&#12289;&#26356;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2303.11545</link><description>&lt;p&gt;
&#20462;&#27491;&#22122;&#22768;&#65306;&#20026;&#21487;&#25511;&#39046;&#22495;&#32763;&#35793;&#20998;&#35299;&#28304;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Fix the Noise: Disentangling Source Feature for Controllable Domain Translation. (arXiv:2303.11545v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25511;&#39046;&#22495;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#24050;&#20998;&#35299;&#23376;&#31354;&#38388;&#20013;&#20445;&#30041;&#28304;&#29305;&#24449;&#65292;&#20351;&#24471;&#21482;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#23601;&#33021;&#24179;&#28369;&#22320;&#25511;&#21046;&#20445;&#30041;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#20135;&#29983;&#26356;&#19968;&#33268;&#12289;&#26356;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#22120;&#19978;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#39046;&#22495;&#32763;&#35793;&#26041;&#38754;&#65292;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#25511;&#21046;&#19981;&#21516;&#39046;&#22495;&#29305;&#24449;&#20043;&#38388;&#30340;&#25511;&#21046;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#35201;&#27714;&#24456;&#39640;&#30340;&#65292;&#32780;&#19988;&#20250;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20855;&#26377;&#21463;&#38480;&#25511;&#21046;&#27493;&#39588;&#65292;&#20174;&#32780;&#38450;&#27490;&#24179;&#28369;&#36807;&#28193;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#36136;&#37327;&#39046;&#22495;&#32763;&#35793;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#24050;&#20998;&#35299;&#23376;&#31354;&#38388;&#20013;&#20445;&#30041;&#28304;&#29305;&#24449;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21482;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#24179;&#28369;&#22320;&#25511;&#21046;&#20445;&#30041;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#21516;&#26102;&#20174;&#23436;&#20840;&#26032;&#30340;&#39046;&#22495;&#29983;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#26356;&#19968;&#33268;&#12289;&#26356;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#20445;&#25345;&#31934;&#30830;&#30340;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show strong generative performance in domain translation especially by using transfer learning techniques on the unconditional generator. However, the control between different domain features using a single model is still challenging. Existing methods often require additional models, which is computationally demanding and leads to unsatisfactory visual quality. In addition, they have restricted control steps, which prevents a smooth transition. In this paper, we propose a new approach for high-quality domain translation with better controllability. The key idea is to preserve source features within a disentangled subspace of a target feature space. This allows our method to smoothly control the degree to which it preserves source features while generating images from an entirely new domain using only a single model. Our extensive experiments show that the proposed method can produce more consistent and realistic images than previous works and maintain precise controllab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#21160;&#24577;&#24863;&#30693;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36816;&#21160;&#21551;&#21457;&#24335;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25551;&#36848;&#36712;&#36857;&#30340;&#31354;&#38388;&#21644;&#36816;&#21160;&#29305;&#24449;&#65292;&#33021;&#22815;&#26356;&#21152;&#20934;&#30830;&#22320;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2303.11540</link><description>&lt;p&gt;
MSTFormer&#65306;&#21160;&#24577;&#24863;&#30693;&#27880;&#24847;&#21147;&#30340;&#36816;&#21160;&#21551;&#21457;&#24335;&#26102;&#31354;Transformer&#29992;&#20110;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MSTFormer: Motion Inspired Spatial-temporal Transformer with Dynamic-aware Attention for long-term Vessel Trajectory Prediction. (arXiv:2303.11540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11540
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#21160;&#24577;&#24863;&#30693;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36816;&#21160;&#21551;&#21457;&#24335;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25551;&#36848;&#36712;&#36857;&#30340;&#31354;&#38388;&#21644;&#36816;&#21160;&#29305;&#24449;&#65292;&#33021;&#22815;&#26356;&#21152;&#20934;&#30830;&#22320;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21160;&#24577;&#30693;&#35782;&#32435;&#20837;&#27169;&#22411;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#33322;&#36857;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#32771;&#34385;&#33337;&#33334;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#19981;&#36807;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#28508;&#22312;&#30340;&#21160;&#24577;&#30693;&#35782;&#65292;&#30452;&#25509;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;&#33322;&#36857;&#12290;&#30452;&#35266;&#22320;&#65292;&#33337;&#33334;&#30340;&#36816;&#21160;&#36981;&#24490;&#21160;&#21147;&#23398;&#23450;&#24459;&#65292;&#20363;&#22914;&#65292;&#36716;&#24367;&#26102;&#36895;&#24230;&#20250;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#21147;&#23398;&#23450;&#24459;&#30340;&#20869;&#22312;&#24322;&#36136;&#24615;&#65292;&#23558;&#21160;&#24577;&#30693;&#35782;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSTFormer&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36816;&#21160;&#21551;&#21457;&#24335;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25551;&#36848;&#36712;&#36857;&#30340;&#31354;&#38388;&#29305;&#24449;&#21644;&#36816;&#21160;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#22836;&#21160;&#24577;&#24863;&#30693;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#19987;&#27880;&#20110;&#36816;&#21160;&#21464;&#25442;&#39057;&#32321;&#30340;&#36712;&#36857;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating the dynamics knowledge into the model is critical for achieving accurate trajectory prediction while considering the spatial and temporal characteristics of the vessel. However, existing methods rarely consider the underlying dynamics knowledge and directly use machine learning algorithms to predict the trajectories. Intuitively, the vessel's motions are following the laws of dynamics, e.g., the speed of a vessel decreases when turning a corner. Yet, it is challenging to combine dynamic knowledge and neural networks due to their inherent heterogeneity. Against this background, we propose MSTFormer, a motion inspired vessel trajectory prediction method based on Transformer. The contribution of this work is threefold. First, we design a data augmentation method to describe the spatial features and motion features of the trajectory. Second, we propose a Multi-headed Dynamic-aware Self-attention mechanism to focus on trajectory points with frequent motion transformations. Fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#8212;&#8212;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65307;&#23427;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20351;&#29992;&#24456;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35268;&#27169;&#20998;&#31867;&#65292;&#20854;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.11536</link><description>&lt;p&gt;
&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Indeterminate Probability Neural Network. (arXiv:2303.11536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#8212;&#8212;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65307;&#23427;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20351;&#29992;&#24456;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35268;&#27169;&#20998;&#31867;&#65292;&#20854;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;IPNN&#30340;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#27010;&#29575;&#35770;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#22312;&#20256;&#32479;&#27010;&#29575;&#35770;&#20013;&#65292;&#27010;&#29575;&#30340;&#35745;&#31639;&#26159;&#22522;&#20110;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#32780;&#36825;&#22312;&#24403;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20960;&#20046;&#19981;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#65292;&#23427;&#26159;&#32463;&#20856;&#27010;&#29575;&#35770;&#30340;&#25193;&#23637;&#65292;&#24182;&#20351;&#32463;&#20856;&#27010;&#29575;&#35770;&#25104;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#34987;&#23450;&#20041;&#20026;&#27010;&#29575;&#20107;&#20214;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25512;&#23548;&#20986;&#20998;&#31867;&#20219;&#21153;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;IPNN&#23637;&#29616;&#20102;&#26032;&#30340;&#29305;&#24615;&#65306;&#23427;&#22312;&#36827;&#34892;&#20998;&#31867;&#30340;&#21516;&#26102;&#21487;&#20197;&#25191;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;IPNN&#33021;&#22815;&#20351;&#29992;&#38750;&#24120;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38750;&#24120;&#22823;&#30340;&#20998;&#31867;&#65292;&#20363;&#22914;100&#20010;&#36755;&#20986;&#33410;&#28857;&#30340;&#27169;&#22411;&#21487;&#20197;&#20998;&#31867;10&#20159;&#31867;&#21035;&#12290;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;IPNN&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new general model called IPNN - Indeterminate Probability Neural Network, which combines neural network and probability theory together. In the classical probability theory, the calculation of probability is based on the occurrence of events, which is hardly used in current neural networks. In this paper, we propose a new general probability theory, which is an extension of classical probability theory, and makes classical probability theory a special case to our theory. Besides, for our proposed neural network framework, the output of neural network is defined as probability events, and based on the statistical analysis of these events, the inference model for classification task is deduced. IPNN shows new property: It can perform unsupervised clustering while doing classification. Besides, IPNN is capable of making very large classification with very small neural network, e.g. model with 100 output nodes can classify 10 billion categories. Theoretical advantages are refl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#23545;&#25239;&#20107;&#23454;&#20844;&#24179;&#65292;&#20551;&#35774;&#20004;&#32452;&#21464;&#37327;&#30340;&#24433;&#21709;&#26159;&#21487;&#21152;&#30340;&#24182;&#19988;&#30456;&#20114;&#29420;&#31435;&#30340;&#65292;&#32467;&#26524;&#23558;&#36817;&#20284;&#24179;&#31561;&#65292;&#20010;&#20154;&#32423;&#21035;&#30340;&#32467;&#26524;&#23558;&#26159;&#23545;&#25239;&#20107;&#23454;&#20844;&#24179;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.11529</link><description>&lt;p&gt;
&#21452;&#26426;&#22120;&#23398;&#20064;&#22312;&#23545;&#25239;&#20107;&#23454;&#20844;&#24179;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Counterfactually Fair Regression with Double Machine Learning. (arXiv:2303.11529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#23545;&#25239;&#20107;&#23454;&#20844;&#24179;&#65292;&#20551;&#35774;&#20004;&#32452;&#21464;&#37327;&#30340;&#24433;&#21709;&#26159;&#21487;&#21152;&#30340;&#24182;&#19988;&#30456;&#20114;&#29420;&#31435;&#30340;&#65292;&#32467;&#26524;&#23558;&#36817;&#20284;&#24179;&#31561;&#65292;&#20010;&#20154;&#32423;&#21035;&#30340;&#32467;&#26524;&#23558;&#26159;&#23545;&#25239;&#20107;&#23454;&#20844;&#24179;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#20107;&#23454;&#20844;&#24179;&#26159;AI&#20844;&#24179;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#23581;&#35797;&#22522;&#20110;&#26576;&#31181;&#25935;&#24863;&#29366;&#24577;&#19979;&#30340;&#20010;&#20154;&#32467;&#26524;&#65292;&#25490;&#38500;&#36825;&#31181;&#29366;&#24577;&#23545;&#32467;&#26524;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#65292;&#23558;&#27492;&#22238;&#24402;&#38382;&#39064;&#30340;&#23545;&#25239;&#20107;&#23454;&#20844;&#24179;&#24615;&#31867;&#27604;&#21040;&#22312;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#19979;&#20272;&#35745;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#23545;&#25239;&#20107;&#23454;&#32467;&#26524;&#12290;&#23427;&#20351;&#29992;&#20219;&#24847;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#26512;&#25935;&#24863;&#21464;&#37327;&#23545;&#38750;&#25935;&#24863;&#21464;&#37327;&#21644;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#20551;&#35774;&#20004;&#32452;&#21464;&#37327;&#30340;&#24433;&#21709;&#26159;&#21487;&#21152;&#30340;&#24182;&#19988;&#30456;&#20114;&#29420;&#31435;&#30340;&#65292;&#32467;&#26524;&#23558;&#36817;&#20284;&#24179;&#31561;&#65292;&#20010;&#20154;&#32423;&#21035;&#30340;&#32467;&#26524;&#23558;&#26159;&#23545;&#25239;&#20107;&#23454;&#20844;&#24179;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#26469;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#30740;&#31350;&#28041;&#21450;&#32844;&#22330;&#25307;&#32856;&#20013;&#30340;&#27495;&#35270;&#38382;&#39064;&#21644;&#23545;&#27861;&#23398;&#38498;&#23398;&#29983;GPA&#30340;&#23454;&#38469;&#25968;&#25454;&#24212;&#29992;&#12290;&#28982;&#21518;&#35752;&#35770;&#20102;&#20309;&#26102;&#36866;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual fairness is an approach to AI fairness that tries to make decisions based on the outcomes that an individual with some kind of sensitive status would have had without this status. This paper proposes Double Machine Learning (DML) Fairness which analogises this problem of counterfactual fairness in regression problems to that of estimating counterfactual outcomes in causal inference under the Potential Outcomes framework. It uses arbitrary machine learning methods to partial out the effect of sensitive variables on nonsensitive variables and outcomes. Assuming that the effects of the two sets of variables are additively separable, outcomes will be approximately equalised and individual-level outcomes will be counterfactually fair. This paper demonstrates the approach in a simulation study pertaining to discrimination in workplace hiring and an application on real data estimating the GPAs of law school students. It then discusses when it is appropriate to apply such a meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.11525</link><description>&lt;p&gt;
SIFT: &#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#20197;&#26368;&#22823;&#38480;&#24230;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#65288;&#19982;&#35757;&#32451;FLOPS&#30456;&#20851;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290; &#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;FLOP&#65292;&#20294;&#20351;&#29992;&#31232;&#30095;&#26435;&#37325;&#36827;&#34892;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#25110;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#21608;&#26399;&#65292;&#20351;&#24471;&#32467;&#26524;&#30340;&#35757;&#32451;&#25928;&#29575;&#19981;&#22815;&#28165;&#26224;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#31232;&#30095;&#24615;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#21516;&#30340;FLOPS&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23637;&#31034;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SIFT&#65292;&#19968;&#32452;&#29992;&#20316;&#23494;&#38598;&#23618;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#26469;&#25552;&#39640;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;FLOP&#25928;&#29575;&#30340;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#12290; &#27599;&#20010;&#36716;&#25442;&#37117;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#65288;&#31232;&#30095;&#32423;&#21035;&#65289;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31232;&#30095;&#25513;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#24066;&#22330;&#20013;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#36827;&#34892;&#24179;&#34913;&#23450;&#20215;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36873;&#25321;&#24615;&#35854;&#35328;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11522</link><description>&lt;p&gt;
&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#24066;&#22330;&#20013;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#36827;&#34892;&#24179;&#34913;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Online Learning for Equilibrium Pricing in Markets under Incomplete Information. (arXiv:2303.11522v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#24066;&#22330;&#20013;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#36827;&#34892;&#24179;&#34913;&#23450;&#20215;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36873;&#25321;&#24615;&#35854;&#35328;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#24179;&#34913;&#30340;&#30740;&#31350;&#26159;&#32463;&#27982;&#29702;&#35770;&#30340;&#26680;&#24515;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#25928;&#37197;&#32622;&#31232;&#32570;&#36164;&#28304;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#23450;&#20215;&#22343;&#34913;&#30340;&#35745;&#31639;&#36890;&#24120;&#20381;&#36182;&#20110;&#23436;&#25972;&#30340;&#20010;&#20307;&#23646;&#24615;&#20449;&#24687;&#65292;&#22914;&#20379;&#24212;&#21830;&#30340;&#25104;&#26412;&#20989;&#25968;&#31561;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#19981;&#21487;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#23450;&#20215;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24066;&#22330;&#32463;&#33829;&#32773;&#23547;&#27714;&#36890;&#36807;&#20174;&#25104;&#26412;&#20989;&#25968;&#26410;&#30693;&#30340;&#31454;&#20105;&#20379;&#24212;&#21830;&#36141;&#20080;&#25152;&#38656;&#25968;&#37327;&#26469;&#28385;&#36275;&#23458;&#25143;&#38656;&#27714;&#12290;&#22312;&#36825;&#31181;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#23398;&#20064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#24179;&#34913;&#20215;&#26684;&#65292;&#21516;&#26102;&#32852;&#21512;&#20248;&#21270;&#19977;&#20010;&#24615;&#33021;&#25351;&#26631;&#8212;&#8212;&#26410;&#28385;&#36275;&#30340;&#38656;&#27714;&#12289;&#25104;&#26412;&#22833;&#35823;&#21644;&#20184;&#27454;&#22833;&#35823;&#8212;&#8212;&#36825;&#26159;&#22312;&#23450;&#20215;&#22343;&#34913;&#30340;&#24773;&#20917;&#19979;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of market equilibria is central to economic theory, particularly in efficiently allocating scarce resources. However, the computation of equilibrium prices at which the supply of goods matches their demand typically relies on having access to complete information on private attributes of agents, e.g., suppliers' cost functions, which are often unavailable in practice. Motivated by this practical consideration, we consider the problem of setting equilibrium prices in the incomplete information setting wherein a market operator seeks to satisfy the customer demand for a commodity by purchasing the required amount from competing suppliers with privately known cost functions unknown to the market operator. In this incomplete information setting, we consider the online learning problem of learning equilibrium prices over time while jointly optimizing three performance metrics -- unmet demand, cost regret, and payment regret -- pertinent in the context of equilibrium pricing over a
&lt;/p&gt;</description></item><item><title>STDLens &#26159;&#19968;&#31181;&#21487;&#20197;&#38450;&#27490;FL&#21463;&#21040;&#27169;&#22411;&#25375;&#25345;&#30340;&#25915;&#20987;&#30340;&#23433;&#20840;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;&#19977;&#23618;&#30340;&#21462;&#35777;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#25490;&#38500;&#29305;&#27530;&#30340;&#26799;&#24230;&#65292;&#24182;&#24674;&#22797;FL&#30340;&#24615;&#33021;&#12290;STDLens&#22312;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#24182;&#19988;&#20855;&#26377;&#38450;&#27490;&#27169;&#22411;&#25375;&#25345;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11511</link><description>&lt;p&gt;
STDLens&#65306;&#22522;&#20110;&#27169;&#22411;&#25375;&#25345;&#30340;&#29289;&#20307;&#26816;&#27979;&#32852;&#37030;&#23398;&#20064;&#30340;&#23433;&#20840;&#38450;&#25252;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
STDLens: Model Hijacking-resilient Federated Learning for Object Detection. (arXiv:2303.11511v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11511
&lt;/p&gt;
&lt;p&gt;
STDLens &#26159;&#19968;&#31181;&#21487;&#20197;&#38450;&#27490;FL&#21463;&#21040;&#27169;&#22411;&#25375;&#25345;&#30340;&#25915;&#20987;&#30340;&#23433;&#20840;&#26041;&#27861;&#12290;&#23427;&#22522;&#20110;&#19977;&#23618;&#30340;&#21462;&#35777;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#25490;&#38500;&#29305;&#27530;&#30340;&#26799;&#24230;&#65292;&#24182;&#24674;&#22797;FL&#30340;&#24615;&#33021;&#12290;STDLens&#22312;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#24182;&#19988;&#20855;&#26377;&#38450;&#27490;&#27169;&#22411;&#25375;&#25345;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#23427;&#20855;&#26377;&#35832;&#22810;&#20248;&#28857;&#65292;FL&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#25375;&#25345;&#30340;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#20165;&#20165;&#21033;&#29992;&#19968;&#23567;&#37096;&#20998;&#21487;&#20197;&#34987;&#25915;&#20987;&#30340;&#23458;&#25143;&#31471;&#25511;&#21046;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#65292;&#36890;&#36807;&#26893;&#20837;&#29305;&#27530;&#26799;&#24230;&#23454;&#29616;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STDLens&#30340;&#23433;&#20840;&#26041;&#27861;&#20197;&#20445;&#25252;FL&#20813;&#21463;&#27492;&#31867;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;&#30340;&#32531;&#35299;&#26426;&#21046;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#31354;&#38388;&#32858;&#31867;&#20998;&#26512;&#26799;&#24230;&#26102;&#30001;&#20110;&#22266;&#26377;&#35823;&#24046;&#32780;&#20135;&#29983;&#30340;&#22833;&#36133;&#24773;&#20917;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#30340;&#21462;&#35777;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#25490;&#38500;&#36825;&#31181;&#29305;&#27530;&#30340;&#26799;&#24230;&#65292;&#24182;&#22312;FL&#36807;&#31243;&#20013;&#24674;&#22797;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#33258;&#36866;&#24212;&#25915;&#20987;&#65292;&#24182;&#23637;&#31034;&#20102;STDLens&#23545;&#39640;&#32423;&#23545;&#25163;&#20855;&#26377;&#30340;&#31283;&#20581;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;STDLens&#22312;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#38450;&#27490;&#27169;&#22411;&#25375;&#25345;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has been gaining popularity as a collaborative learning framework to train deep learning-based object detection models over a distributed population of clients. Despite its advantages, FL is vulnerable to model hijacking. The attacker can control how the object detection system should misbehave by implanting Trojaned gradients using only a small number of compromised clients in the collaborative learning process. This paper introduces STDLens, a principled approach to safeguarding FL against such attacks. We first investigate existing mitigation mechanisms and analyze their failures caused by the inherent errors in spatial clustering analysis on gradients. Based on the insights, we introduce a three-tier forensic framework to identify and expel Trojaned gradients and reclaim the performance over the course of FL. We consider three types of adaptive attacks and demonstrate the robustness of STDLens against advanced adversaries. Extensive experiments show that STD
&lt;/p&gt;</description></item><item><title>&#20154;&#26426;&#20132;&#20114;(HMI)&#34987;&#21152;&#20837;&#21040;AI&#26550;&#26500;&#35774;&#35745;&#36807;&#31243;&#20013;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;AI&#24320;&#21457;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#36991;&#20813;&#35757;&#32451;&#21644;&#35780;&#20272;&#20855;&#26377;&#38750;&#29983;&#20135;&#23618;&#30340;AI&#26550;&#26500;&#65292;&#20174;&#32780;&#23548;&#33268;&#36731;&#37327;&#32423;AI&#26550;&#26500;&#30340;&#20135;&#29983;&#12290;</title><link>http://arxiv.org/abs/2303.11508</link><description>&lt;p&gt;
AI&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#24433;&#21709;-&#22522;&#20110;&#20154;&#26426;&#20132;&#20114;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AI-in-the-Loop -- The impact of HMI in AI-based Application. (arXiv:2303.11508v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11508
&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;(HMI)&#34987;&#21152;&#20837;&#21040;AI&#26550;&#26500;&#35774;&#35745;&#36807;&#31243;&#20013;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;AI&#24320;&#21457;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#36991;&#20813;&#35757;&#32451;&#21644;&#35780;&#20272;&#20855;&#26377;&#38750;&#29983;&#20135;&#23618;&#30340;AI&#26550;&#26500;&#65292;&#20174;&#32780;&#23548;&#33268;&#36731;&#37327;&#32423;AI&#26550;&#26500;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#21644;&#20154;&#26426;&#20132;&#20114;(HMI)&#26159;&#36890;&#24120;&#19981;&#36866;&#29992;&#20110;&#23884;&#20837;&#24335;&#24212;&#29992;&#30340;&#20004;&#20010;&#20851;&#38190;&#35789;&#12290;&#22312;&#23558;AI&#24212;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#20043;&#21069;&#38656;&#35201;&#30340;&#27493;&#39588;&#20013;&#65292;HMI&#36890;&#24120;&#22312;AI&#26550;&#26500;&#35774;&#35745;&#21644;AI&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#22833;&#12290;&#20154;&#22312;&#29615;&#36335;&#27010;&#24565;&#22312;AI&#24320;&#21457;&#30340;&#25152;&#26377;&#20854;&#20182;&#27493;&#39588;&#20013;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;&#25968;&#25454;&#20998;&#26512;&#21040;&#25968;&#25454;&#36873;&#25321;&#21644;&#28165;&#27927;&#12289;&#24615;&#33021;&#35780;&#20272;&#12290;&#22312;AI&#26550;&#26500;&#35774;&#35745;&#36807;&#31243;&#20013;&#65292;HMI&#21487;&#20197;&#31435;&#21363;&#31361;&#20986;&#26174;&#31034;&#26550;&#26500;&#30340;&#38750;&#29983;&#20135;&#23618;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#21019;&#24314;&#29992;&#20110;&#23884;&#20837;&#24335;&#24212;&#29992;&#30340;&#36731;&#37327;&#32423;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;HMI&#65292;&#29992;&#25143;&#21487;&#20197;&#31435;&#21363;&#21306;&#20998;&#21738;&#31181;AI&#26550;&#26500;&#24212;&#35813;&#39318;&#20808;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#22240;&#20026;&#21487;&#20197;&#39044;&#26399;&#22312;&#20219;&#21153;&#20013;&#39640;&#31934;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36991;&#20813;&#35757;&#32451;&#21644;&#35780;&#20272;&#20855;&#26377;&#38750;&#29983;&#20135;&#23618;&#30340;AI&#26550;&#26500;&#26469;&#20943;&#23569;AI&#24320;&#21457;&#25152;&#38656;&#30340;&#36164;&#28304;&#65292;&#20174;&#32780;&#23548;&#33268;&#36731;&#37327;&#32423;AI&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) and human-machine interaction (HMI) are two keywords that usually do not fit embedded applications. Within the steps needed before applying AI to solve a specific task, HMI is usually missing during the AI architecture design and the training of an AI model. The human-in-the-loop concept is prevalent in all other steps of developing AI, from data analysis via data selection and cleaning to performance evaluation. During AI architecture design, HMI can immediately highlight unproductive layers of the architecture so that lightweight network architecture for embedded applications can be created easily. We show that by using this HMI, users can instantly distinguish which AI architecture should be trained and evaluated first since a high accuracy on the task could be expected. This approach reduces the resources needed for AI development by avoiding training and evaluating AI architectures with unproductive layers and leads to lightweight AI architectures. The
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#37327;&#39033;&#26469;&#38544;&#24335;&#32771;&#34385;&#21463;&#20307;&#26580;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23567;&#20998;&#23376;&#37197;&#20307;&#20301;&#23039;&#39044;&#27979;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#34507;&#30333;&#24418;&#21464;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2303.11494</link><description>&lt;p&gt;
FlexVDW:&#19968;&#31181;&#22312;&#37197;&#20307;&#23545;&#25509;&#20013;&#32771;&#34385;&#34507;&#30333;&#26580;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
FlexVDW: A machine learning approach to account for protein flexibility in ligand docking. (arXiv:2303.11494v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11494
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#37327;&#39033;&#26469;&#38544;&#24335;&#32771;&#34385;&#21463;&#20307;&#26580;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23567;&#20998;&#23376;&#37197;&#20307;&#20301;&#23039;&#39044;&#27979;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#34507;&#30333;&#24418;&#21464;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#37197;&#20307;&#23545;&#25509;&#26041;&#27861;&#37117;&#20551;&#35774;&#34507;&#30333;&#32467;&#26500;&#26159;&#21018;&#24615;&#30340;&#65292;&#24403;&#38774;&#34507;&#30333;&#22312;&#37197;&#20307;&#32467;&#21512;&#26102;&#21457;&#29983;&#24418;&#21464;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#30001;&#20110;&#37197;&#20307;&#21644;&#34507;&#30333;&#36136;&#21407;&#23376;&#20043;&#38388;&#30340;&#26126;&#26174;&#20914;&#31361;&#23548;&#33268;&#35745;&#31639;&#20986;&#30340;&#33539;&#24503;&#21326;&#33021;&#37327;&#39033;&#26497;&#39640;&#65292;&#22240;&#27492;&#37197;&#20307;&#30340;&#30495;&#23454;&#32467;&#21512;&#20301;&#23039;&#36890;&#24120;&#20250;&#34987;&#35780;&#20998;&#24471;&#38750;&#24120;&#19981;&#21033;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159;&#36890;&#36807;&#26174;&#24335;&#25628;&#32034;&#21463;&#20307;&#26500;&#35937;&#20197;&#32771;&#34385;&#21463;&#20307;&#22312;&#37197;&#20307;&#32467;&#21512;&#20013;&#30340;&#26580;&#24615;&#26469;&#35299;&#20915;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35757;&#32451;&#23427;&#22312;&#39044;&#27979;&#33539;&#24503;&#21326;&#33021;&#37327;&#26102;&#38544;&#24335;&#32771;&#34385;&#21463;&#20307;&#30340;&#26580;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#36825;&#20010;&#26426;&#22120;&#23398;&#20064;&#33021;&#37327;&#39033;&#21512;&#24182;&#21040;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#35780;&#20998;&#20989;&#25968;&#20013;&#21487;&#20197;&#25552;&#39640;&#23567;&#20998;&#23376;&#37197;&#20307;&#20301;&#23039;&#39044;&#27979;&#32467;&#26524;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#34507;&#30333;&#24418;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;&#19981;&#20250;&#24694;&#21270;&#22312;&#23384;&#22312;&#26368;&#23567;&#34507;&#30333;&#24418;&#21464;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most widely used ligand docking methods assume a rigid protein structure. This leads to problems when the structure of the target protein deforms upon ligand binding. In particular, the ligand's true binding pose is often scored very unfavorably due to apparent clashes between ligand and protein atoms, which lead to extremely high values of the calculated van der Waals energy term. Traditionally, this problem has been addressed by explicitly searching for receptor conformations to account for the flexibility of the receptor in ligand binding. Here we present a deep learning model trained to take receptor flexibility into account implicitly when predicting van der Waals energy. We show that incorporating this machine-learned energy term into a state-of-the-art physics-based scoring function improves small molecule ligand pose prediction results in cases with substantial protein deformation, without degrading performance in cases with minimal protein deformation. This work demonstrates t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22841;&#24515;&#35270;&#39057;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21253;&#35013;&#26631;&#20934;&#32534;&#35299;&#30721;&#22120;&#26469;&#20351;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21387;&#32553;&#24615;&#33021;&#65292;&#22312;&#39640;&#28165;&#35270;&#39057;&#20256;&#36755;&#21644;&#35821;&#38899;&#35782;&#21035;&#35270;&#39057;&#21387;&#32553;&#31561;&#22330;&#26223;&#20013;&#34920;&#29616;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2303.11473</link><description>&lt;p&gt;
&#22841;&#24515;&#35270;&#39057;&#21387;&#32553;&#65306;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23553;&#35013;&#26469;&#39640;&#25928;&#25193;&#23637;&#26631;&#20934;&#32534;&#35299;&#30721;&#22120;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sandwiched Video Compression: Efficiently Extending the Reach of Standard Codecs with Neural Wrappers. (arXiv:2303.11473v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22841;&#24515;&#35270;&#39057;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21253;&#35013;&#26631;&#20934;&#32534;&#35299;&#30721;&#22120;&#26469;&#20351;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21387;&#32553;&#24615;&#33021;&#65292;&#22312;&#39640;&#28165;&#35270;&#39057;&#20256;&#36755;&#21644;&#35821;&#38899;&#35782;&#21035;&#35270;&#39057;&#21387;&#32553;&#31561;&#22330;&#26223;&#20013;&#34920;&#29616;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22841;&#24515;&#35270;&#39057;&#21387;&#32553;--&#19968;&#31181;&#22312;&#26631;&#20934;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#21608;&#22260;&#21253;&#35013;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#39057;&#21387;&#32553;&#31995;&#32479;&#12290;&#35813;&#22841;&#24515;&#26694;&#26550;&#30001;&#31070;&#32463;&#21069;&#22788;&#29702;&#22120;&#12289;&#26631;&#20934;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#21644;&#31070;&#32463;&#21518;&#22788;&#29702;&#22120;&#32452;&#25104;&#12290;&#36825;&#20123;&#32593;&#32476;&#34987;&#32852;&#21512;&#35757;&#32451;&#20197;&#20248;&#21270;&#30721;&#29575;-&#22833;&#30495;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#22312;&#21508;&#31181;&#21387;&#32553;&#22330;&#26223;&#20013;&#26174;&#30528;&#25913;&#21892;&#26631;&#20934;&#32534;&#35299;&#30721;&#22120;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#38656;&#35201;&#19968;&#20010;&#21487;&#24494;&#30340;&#26631;&#20934;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#20195;&#29702;&#65292;&#23427;&#21253;&#25324;&#26102;&#38388;&#22788;&#29702;&#12289;&#36816;&#21160;&#34917;&#20607;&#12289;&#20869;/&#38388;&#27169;&#24335;&#20915;&#31574;&#21644;&#24490;&#29615;&#28388;&#27874;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20851;&#38190;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#32452;&#20214;&#30340;&#21487;&#24494;&#36924;&#36817;&#65292;&#24182;&#35777;&#26126;&#20102;&#22841;&#24515;&#30340;&#31070;&#32463;&#32534;&#30721;&#30456;&#23545;&#20110;&#22312;&#20004;&#20010;&#37325;&#35201;&#22330;&#26223;&#20013;&#21387;&#32553;&#36755;&#20837;&#35270;&#39057;&#30340;&#21407;&#22987;&#24103;&#32780;&#35328;&#65292;&#20855;&#26377;&#26174;&#30528;&#26356;&#22909;&#30340;&#30721;&#29575;&#22833;&#30495;&#24615;&#33021;&#12290;&#22312;&#36890;&#36807;&#20302;&#20998;&#36776;&#29575;HEVC&#20256;&#36755;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#30340;&#24773;&#20917;&#19979;&#65292;&#22841;&#24515;&#31995;&#32479;&#33719;&#24471;&#20102;6.5 dB&#30340;PSNR&#25913;&#21892;&#65307;&#22312;&#21478;&#19968;&#31181;&#22330;&#26223;&#20013;&#65292;&#21387;&#32553;&#22823;&#35789;&#27719;&#35821;&#38899;&#35782;&#21035;&#35270;&#39057;&#22312;0.02 bpp&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#22841;&#24515;&#31995;&#32479;&#65292;&#33719;&#24471;&#20102;30%&#30340;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose sandwiched video compression -- a video compression system that wraps neural networks around a standard video codec. The sandwich framework consists of a neural pre- and post-processor with a standard video codec between them. The networks are trained jointly to optimize a rate-distortion loss function with the goal of significantly improving over the standard codec in various compression scenarios. End-to-end training in this setting requires a differentiable proxy for the standard video codec, which incorporates temporal processing with motion compensation, inter/intra mode decisions, and in-loop filtering. We propose differentiable approximations to key video codec components and demonstrate that the neural codes of the sandwich lead to significantly better rate-distortion performance compared to compressing the original frames of the input video in two important scenarios. When transporting high-resolution video via low-resolution HEVC, the sandwich system obtains 6.5 dB
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20844;&#20849;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#26497;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#38544;&#24335;&#23398;&#20064;&#19968;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#20197;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#8220;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#8221;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#65292;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#38750;&#27861;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2303.11470</link><description>&lt;p&gt;
&#20320;&#26377;&#22312;&#20351;&#29992;&#25105;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#21527;&#65311;&#20351;&#29992;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#23454;&#29616;&#20844;&#20849;&#25968;&#25454;&#38598;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking. (arXiv:2303.11470v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11470
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#20844;&#20849;&#25968;&#25454;&#30340;&#23433;&#20840;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#26497;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#38544;&#24335;&#23398;&#20064;&#19968;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#20197;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#27492;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#8220;&#28165;&#27905;&#26631;&#31614;&#32972;&#38376;&#8221;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#65292;&#19981;&#20250;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#26816;&#27979;&#21040;&#38750;&#27861;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#28304;&#28304;&#19981;&#26029;&#30340;&#25903;&#25345;&#35757;&#32451;&#25968;&#25454;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22823;&#37327;&#30340;&#20844;&#20849;&#25968;&#25454;&#20063;&#24341;&#36215;&#20102;&#23545;&#25968;&#25454;&#38598;&#34987;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#20110;&#21830;&#19994;&#30446;&#30340;&#30340;&#25285;&#24551;&#65292;&#36825;&#26159;&#25968;&#25454;&#38598;&#35768;&#21487;&#35777;&#25152;&#31105;&#27490;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32972;&#38376;&#25968;&#23383;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#20445;&#25252;&#20844;&#20849;&#25968;&#25454;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#36890;&#36807;&#21521;&#25968;&#25454;&#38598;&#20013;&#25554;&#20837;&#23569;&#37327;&#30340;&#25968;&#23383;&#27700;&#21360;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#38544;&#24335;&#23398;&#20064;&#30001;&#38450;&#24481;&#32773;&#35774;&#32622;&#30340;&#31192;&#23494;&#20989;&#25968;&#12290;&#36825;&#20010;&#38544;&#34255;&#30340;&#20989;&#25968;&#21487;&#20197;&#20316;&#20026;&#25968;&#23383;&#27700;&#21360;&#65292;&#29992;&#20110;&#36319;&#36394;&#38750;&#27861;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#31532;&#19977;&#26041;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#32972;&#38376;&#25554;&#20837;&#26041;&#27861;&#24448;&#24448;&#28041;&#21450;&#21521;&#35757;&#32451;&#38598;&#20013;&#28155;&#21152;&#20219;&#24847;&#30340;&#12289;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#24182;&#23481;&#26131;&#34987;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#26816;&#27979;&#21040;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28165;&#27905;&#26631;&#35760;&#32972;&#38376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#25968;&#23383;&#27700;&#21360;&#32780;&#19981;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#26816;&#27979;&#38750;&#27861;&#25968;&#25454;&#38598;&#20351;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a backdoor-based watermarking approach that serves as a general framework for safeguarding public-available data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a clean-label backdoo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19971;&#20010;&#24212;&#29992;&#32452;&#21512;&#25968;&#23398;&#20013;&#30340;&#26410;&#35299;&#20043;&#35868;&#65292;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#37327;&#23376;&#35745;&#31639;&#65292;&#31639;&#27861;&#20998;&#21270;&#65292;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65292;&#36229;&#22270;&#20999;&#21106;&#31639;&#27861;&#21644;&#30005;&#21147;&#31995;&#32479;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.11464</link><description>&lt;p&gt;
&#24212;&#29992;&#32452;&#21512;&#25968;&#23398;&#20013;&#30340;&#19971;&#20010;&#26410;&#35299;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
Seven open problems in applied combinatorics. (arXiv:2303.11464v1 [math.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19971;&#20010;&#24212;&#29992;&#32452;&#21512;&#25968;&#23398;&#20013;&#30340;&#26410;&#35299;&#20043;&#35868;&#65292;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#37327;&#23376;&#35745;&#31639;&#65292;&#31639;&#27861;&#20998;&#21270;&#65292;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65292;&#36229;&#22270;&#20999;&#21106;&#31639;&#27861;&#21644;&#30005;&#21147;&#31995;&#32479;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#24212;&#29992;&#32452;&#21512;&#25968;&#23398;&#20013;&#30340;&#19971;&#20010;&#19981;&#21516;&#30340;&#26410;&#35299;&#20043;&#35868;&#12290;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#21253;&#25324;&#37327;&#23376;&#35745;&#31639;&#65292;&#31639;&#27861;&#20998;&#21270;&#65292;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65292;&#36845;&#20195;&#26041;&#27861;&#65292;&#36229;&#22270;&#20999;&#21106;&#31639;&#27861;&#21644;&#30005;&#21147;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present and discuss seven different open problems in applied combinatorics. The application areas relevant to this compilation include quantum computing, algorithmic differentiation, topological data analysis, iterative methods, hypergraph cut algorithms, and power systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20844;&#24179;&#22270;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#32531;&#35299;&#20559;&#35265;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#35774;&#35745;&#22312;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#21151;&#25928;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#22522;&#20934;&#31639;&#27861;&#65292;&#23454;&#29992;&#24230;&#26356;&#39640;&#19988;&#26356;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.11459</link><description>&lt;p&gt;
&#20844;&#27491;&#24863;&#30693;&#30340;&#22270;&#28388;&#27874;&#22120;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Graph Filter Design. (arXiv:2303.11459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20844;&#24179;&#22270;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#32531;&#35299;&#20559;&#35265;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#35774;&#35745;&#22312;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#21151;&#25928;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#22522;&#20934;&#31639;&#27861;&#65292;&#23454;&#29992;&#24230;&#26356;&#39640;&#19988;&#26356;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#29992;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#25968;&#23398;&#24037;&#20855;&#65292;&#20363;&#22914;&#37329;&#34701;&#24066;&#22330;&#21644;&#31038;&#20132;&#32593;&#32476;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22270;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#20559;&#35265;&#22270;&#32467;&#26500;&#19978;&#30340;&#20449;&#24687;&#32858;&#21512;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102; ML &#23545;&#20110;&#21508;&#31181;&#20915;&#31574;&#38382;&#39064;&#20013;&#24050;&#23384;&#22312;&#30340;&#26576;&#20123;&#27424;&#20195;&#34920;&#32676;&#20307;&#30340;&#20559;&#35265;&#30340;&#25918;&#22823;&#12290;&#38754;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20844;&#24179;&#22270;&#28388;&#27874;&#22120;&#65292;&#21487;&#22312;&#21508;&#31181;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#28789;&#27963;&#20351;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;&#28388;&#27874;&#22120;&#35774;&#35745;&#22522;&#20110;&#20559;&#35265;&#20998;&#26512;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#22312;&#32531;&#35299;&#20559;&#35265;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#19982;&#20854;&#26080;&#20851;&#20844;&#24179;&#24615;&#23545;&#24212;&#29289;&#30456;&#27604;&#30340;&#26368;&#20248;&#24615;&#12290;&#33410;&#28857;&#20998;&#31867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#25552;&#35758;&#30340;&#36807;&#28388;&#22120;&#35774;&#35745;&#22312;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#21151;&#25928;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#22522;&#20934;&#31639;&#27861;&#65292;&#23454;&#29992;&#24230;&#26356;&#39640;&#19988;&#26356;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are mathematical tools that can be used to represent complex real-world systems, such as financial markets and social networks. Hence, machine learning (ML) over graphs has attracted significant attention recently. However, it has been demonstrated that ML over graphs amplifies the already existing bias towards certain under-represented groups in various decision-making problems due to the information aggregation over biased graph structures. Faced with this challenge, in this paper, we design a fair graph filter that can be employed in a versatile manner for graph-based learning tasks. The design of the proposed filter is based on a bias analysis and its optimality in mitigating bias compared to its fairness-agnostic counterpart is established. Experiments on real-world networks for node classification demonstrate the efficacy of the proposed filter design in mitigating bias, while attaining similar utility and better stability compared to baseline algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;Codex&#34429;&#28982;&#26377;&#21161;&#20110;&#36991;&#20813;&#19968;&#20123;&#31616;&#21333;Bug&#65292;&#20294;&#32463;&#24120;&#20250;&#29983;&#25104;&#24050;&#30693;&#30340;&#65292;&#36880;&#23383;&#30340;Bug&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#36991;&#20813;&#31574;&#30053;&#26469;&#20943;&#23569;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#22686;&#21152;&#26377;&#21019;&#24847;&#30340;Bug&#30340;&#29983;&#20135;&#12290;</title><link>http://arxiv.org/abs/2303.11455</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31616;&#21333;&#24858;&#34850; Bug
&lt;/p&gt;
&lt;p&gt;
Large Language Models and Simple, Stupid Bugs. (arXiv:2303.11455v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;Codex&#34429;&#28982;&#26377;&#21161;&#20110;&#36991;&#20813;&#19968;&#20123;&#31616;&#21333;Bug&#65292;&#20294;&#32463;&#24120;&#20250;&#29983;&#25104;&#24050;&#30693;&#30340;&#65292;&#36880;&#23383;&#30340;Bug&#65292;&#22240;&#27492;&#38656;&#35201;&#37319;&#21462;&#36991;&#20813;&#31574;&#30053;&#26469;&#20943;&#23569;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#22686;&#21152;&#26377;&#21019;&#24847;&#30340;Bug&#30340;&#29983;&#20135;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#22823;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29992;&#20110;&#36741;&#21161;&#24320;&#21457;&#32773;&#36827;&#34892;&#32534;&#30721;&#20219;&#21153;&#30340;&#22522;&#20110;AI&#30340;&#31995;&#32479;&#21464;&#24471;&#26222;&#36941;&#21487;&#29992;&#65307;Copilot&#20415;&#26159;&#36825;&#26679;&#30340;&#31995;&#32479;&#12290;Copilot&#20351;&#29992;Codex&#36825;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#23436;&#25104;&#19968;&#20010;&#30456;&#24212;&#30340;&#8220;&#25552;&#31034;&#8221;&#21518;&#30340;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;Codex&#26159;&#22312;&#20844;&#20849;GitHub&#23384;&#20648;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#21363;&#21487;&#33021;&#21253;&#21547;&#38169;&#35823;&#21644;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;Codex&#20250;&#22797;&#21046;&#35757;&#32451;&#20013;&#20986;&#29616;&#30340;&#28431;&#27934;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Codex&#29983;&#25104;&#30340;&#19968;&#20010;&#26377;&#36259;&#30340; Bug &#31867;&#22411;&#30340;&#26131;&#21457;&#24615;&#65292;&#21363;&#21333;&#35821;&#21477; Bug&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#31616;&#21333;&#24858;&#34850; Bug &#25110; SStuBs&#12290;&#25105;&#20204;&#21457;&#29616;Codex&#21644;&#31867;&#20284;&#30340;LLMs&#30830;&#23454;&#26377;&#21161;&#20110;&#36991;&#20813;&#19968;&#20123;SStuBs&#65292;&#20294;&#30830;&#23454;&#20250;&#29983;&#25104;&#24050;&#30693;&#30340;&#65292;&#36880;&#23383;&#30340;SStuBs&#65292;&#20854;&#20986;&#29616;&#30340;&#21487;&#33021;&#24615;&#26159;&#24050;&#30693;&#27491;&#30830;&#20195;&#30721;&#30340;2&#20493;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;Codex&#29983;&#25104;&#30340;SStuBs&#30340;&#21518;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#36991;&#20813;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#26377;&#21161;&#20110;&#20943;&#23569;&#24050;&#30693;&#30340;&#65292;&#36880;&#23383;&#30340;SStubs&#30340;&#29983;&#20135;&#65292;&#24182;&#22686;&#21152;&#26377;&#21019;&#24847;&#30340;Bug&#30340;&#29983;&#20135;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of powerful neural language models, AI-based systems to assist developers in coding tasks are becoming widely available; Copilot is one such system. Copilot uses Codex, a large language model (LLM), to complete code conditioned on a preceding "prompt". Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training. In this study, we examine how prone Codex is to generate an interesting bug category, single statement bugs, commonly referred to as simple, stupid bugs or SStuBs in the MSR community. We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as 2x as likely than known, verbatim correct code. We explore the consequences of the Codex generated SStuBs and propose avoidance strategies that suggest the possibility of reducing the production of known, verbatim SStubs, and increase th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;&#38543;&#26426;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#25551;&#36848;&#65292;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#23427;&#20204;&#31867;&#20284;&#20110;&#26080;&#38480;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65288;IGAM&#65289;</title><link>http://arxiv.org/abs/2303.11454</link><description>&lt;p&gt;
&#22914;&#20309;&#25551;&#36848;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29305;&#24615;&#8212;&#8212;&#31532;&#20108;&#37096;&#20998;&#65306;&#20855;&#26377;&#38543;&#26426;&#31532;&#19968;&#23618;&#30340;&#20004;&#23618;&#22810;&#32500;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
How (Implicit) Regularization of ReLU Neural Networks Characterizes the Learned Function -- Part II: the Multi-D Case of Two Layers with Random First Layer. (arXiv:2303.11454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;&#38543;&#26426;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#25551;&#36848;&#65292;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#65292;&#23427;&#20204;&#31867;&#20284;&#20110;&#26080;&#38480;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65288;IGAM&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#20165;&#20248;&#21270;&#20102;&#26368;&#32456;&#23618;&#30340;&#26435;&#37325;&#65292;&#21487;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35745;&#31639;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#24778;&#20154;&#12290;&#26412;&#25991;&#23545;&#20110;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;&#38543;&#26426;&#12289;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#25552;&#20986;&#20102;&#19968;&#20010;&#23439;&#35266;&#31934;&#30830;&#30340;&#29305;&#24449;&#25551;&#36848;&#65292;&#21363;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65288;GAM&#65289;&#31867;&#22411;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#26080;&#38480;&#22810;&#20010;&#26041;&#21521;&#65306;&#26080;&#38480;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#65288;IGAM&#65289;&#12290; IGAM&#34987;&#24418;&#24335;&#21270;&#20026;&#20989;&#25968;&#31354;&#38388;&#20013;&#29305;&#23450;&#27491;&#21017;&#21270;&#27867;&#20989;&#21644;&#30456;&#24403;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#12290;&#26412;&#25991;&#26159;&#22312;&#20808;&#21069;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#23545;&#22810;&#20803;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#25193;&#23637;&#65292;&#25105;&#20204;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#20855;&#26377;ReLU&#28608;&#27963;&#30340;&#23485;&#24335;RSNs&#30340;&#34892;&#20026;&#31867;&#20284;&#20110;&#26679;&#26465;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized neural networks (randomized NNs), where only the terminal layer's weights are optimized constitute a powerful model class to reduce computational time in training the neural network model. At the same time, these models generalize surprisingly well in various regression and classification tasks. In this paper, we give an exact macroscopic characterization (i.e., a characterization in function space) of the generalization behavior of randomized, shallow NNs with ReLU activation (RSNs). We show that RSNs correspond to a generalized additive model (GAM)-typed regression in which infinitely many directions are considered: the infinite generalized additive model (IGAM). The IGAM is formalized as solution to an optimization problem in function space for a specific regularization functional and a fairly general loss. This work is an extension to multivariate NNs of prior work, where we showed how wide RSNs with ReLU activation behave like spline regression under certain conditions 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#30740;&#31350;&#20102;&#22522;&#20110;Group Lasso&#27491;&#21017;&#21270;&#22120;&#30340;&#36138;&#23146;&#21098;&#26525;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20462;&#21098;&#20302;$\ell_2$&#33539;&#25968;&#21015;&#30340;&#35299;&#21487;&#20197;&#27867;&#21270;&#21040;&#26032;&#26679;&#26412;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.11453</link><description>&lt;p&gt;
&#22522;&#20110;Group Lasso&#30340;&#36138;&#23146;&#21098;&#26525;&#22312;&#30697;&#38453;&#24863;&#30693;&#21644;&#20108;&#27425;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#19978;&#21487;&#35777;&#22320;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing and Neural Networks with Quadratic Activations. (arXiv:2303.11453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#20013;&#30740;&#31350;&#20102;&#22522;&#20110;Group Lasso&#27491;&#21017;&#21270;&#22120;&#30340;&#36138;&#23146;&#21098;&#26525;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20462;&#21098;&#20302;$\ell_2$&#33539;&#25968;&#21015;&#30340;&#35299;&#21487;&#20197;&#27867;&#21270;&#21040;&#26032;&#26679;&#26412;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#26041;&#26696;&#24191;&#27867;&#29992;&#20110;&#38477;&#20302;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#23454;&#36341;&#30740;&#31350;&#34920;&#26126;&#65292;&#20462;&#21098;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#24182;&#24494;&#35843;&#21487;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#26679;&#26412;&#19978;&#12290;&#34429;&#28982;&#20197;&#19978;&#34987;&#31216;&#20026;&#21098;&#26525;+&#24494;&#35843;&#30340;&#27969;&#31243;&#22312;&#38477;&#20302;&#35757;&#32451;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#20854;&#32972;&#21518;&#30340;&#29702;&#35770;&#20173;&#28982;&#19981;&#29978;&#20102;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#36229;&#21442;&#25968;&#21270;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#19978;&#30340;&#21098;&#26525;+&#24494;&#35843;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#30495;&#23454;&#32467;&#26524;&#34920;&#31034;&#20026;$U_\star \in \mathbb{R}^{d \times r}$&#65292;&#32780;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#34920;&#31034;&#20026;$U \in \mathbb{R}^{d \times k}$&#65292;&#20854;&#20013;$k \gg r$&#12290;&#25105;&#20204;&#30740;&#31350;&#21152;&#19978;Group Lasso&#27491;&#21017;&#21270;&#22120;&#30340;&#24179;&#28369;&#29256;&#26412;$\sum_{i=1}^k \| U e_i \|_2$&#30340;&#24179;&#22343;&#35823;&#24046;&#30340;&#36817;&#20284;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#35777;&#26126;&#20462;&#21098;&#20302;$\ell_2$&#33539;&#25968;&#21015;&#30340;&#35299;$U_{
&lt;/p&gt;
&lt;p&gt;
Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. Several practical studies have shown that pruning an overparameterized model and fine-tuning generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem, with the ground truth denoted $U_\star \in \mathbb{R}^{d \times r}$ and the overparameterized model $U \in \mathbb{R}^{d \times k}$ with $k \gg r$. We study the approximate local minima of the empirical mean square error, augmented with a smooth version of a group Lasso regularizer, $\sum_{i=1}^k \| U e_i \|_2$ and show that pruning the low $\ell_2$-norm columns results in a solution $U_{\
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#31867;&#25991;&#21270;&#36951;&#20135;&#20013;&#36816;&#29992;&#22270;&#20687;&#20998;&#31867;&#25216;&#26415;&#26102;&#38754;&#20020;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292; &#24182;&#19988;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#19982;&#36801;&#31227;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#26041;&#26696;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#24615;&#21035;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.11449</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#65306;&#20154;&#31867;&#25991;&#21270;&#36951;&#20135;&#20013;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bias mitigation techniques in image classification: fair machine learning in human heritage collections. (arXiv:2303.11449v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11449
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20154;&#31867;&#25991;&#21270;&#36951;&#20135;&#20013;&#36816;&#29992;&#22270;&#20687;&#20998;&#31867;&#25216;&#26415;&#26102;&#38754;&#20020;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292; &#24182;&#19988;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#19982;&#36801;&#31227;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#26041;&#26696;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#24615;&#21035;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#21270;&#20998;&#31867;&#31995;&#32479;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#65292;&#22914;&#26524;&#23427;&#20204;&#27809;&#26377;&#27491;&#30830;&#22320;&#36827;&#34892;&#24037;&#31243;&#21270;&#21644;&#20844;&#24179;&#24615;&#32771;&#34385;&#65292;&#23427;&#20204;&#21487;&#33021;&#23545;&#29305;&#23450;&#20154;&#32676;&#20855;&#26377;&#19981;&#21033;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#24037;&#31243;&#24072;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21069;&#27839;&#30340;&#22270;&#20687;&#20998;&#31867;&#25216;&#26415;&#65292;&#20294;&#22312;&#20154;&#31867;&#25991;&#21270;&#36951;&#20135;&#25910;&#34255;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#20173;&#23384;&#22312;&#24046;&#36317;&#65292;&#22240;&#20026;&#36825;&#37324;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#36136;&#37327;&#20302;&#21155;&#30340;&#20154;&#20204;&#29031;&#29255;&#65292;&#36825;&#20123;&#20154;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#26063;&#35028;&#12289;&#24615;&#21035;&#21644;&#24180;&#40836;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;Xception&#21644;EfficientNet&#35780;&#20272;&#20102;&#19977;&#31181;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#65292;&#29992;&#20110;&#24615;&#21035;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#20844;&#24179;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#20811;&#26381;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#25105;&#20204;&#22312;19&#19990;&#32426;&#21644;20&#19990;&#32426;&#30340;&#25991;&#21270;&#36951;&#20135;&#29031;&#29255;&#25910;&#34255;&#19978;&#35780;&#20272;&#20102;&#20559;&#35265;&#32531;&#35299;&#31649;&#36947;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20351;&#29992;FairFace&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36801;&#31227;&#23398;&#20064;&#23454;&#39564;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#25552;&#20986;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#24615;&#21035;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#24369;&#21183;&#32676;&#20307;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22914;&#20309;&#35299;&#20915;&#20154;&#31867;&#25991;&#21270;&#36951;&#20135;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major problem with using automated classification systems is that if they are not engineered correctly and with fairness considerations, they could be detrimental to certain populations. Furthermore, while engineers have developed cutting-edge technologies for image classification, there is still a gap in the application of these models in human heritage collections, where data sets usually consist of low-quality pictures of people with diverse ethnicity, gender, and age. In this work, we evaluate three bias mitigation techniques using two state-of-the-art neural networks, Xception and EfficientNet, for gender classification. Moreover, we explore the use of transfer learning using a fair data set to overcome the training data scarcity. We evaluated the effectiveness of the bias mitigation pipeline on a cultural heritage collection of photographs from the 19th and 20th centuries, and we used the FairFace data set for the transfer learning experiments. After the evaluation, we found th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26629;&#26684;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#32676;&#31561;&#21464;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20960;&#20309;&#20844;&#24335;&#65292;&#34920;&#26126;L-CNN&#20013;&#30340;&#21367;&#31215;&#26159;SU($N$)&#20027;&#19995;&#19978;&#35268;&#33539;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.11448</link><description>&lt;p&gt;
&#26629;&#26684;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;
Geometrical aspects of lattice gauge equivariant convolutional neural networks. (arXiv:2303.11448v1 [hep-lat])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26629;&#26684;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#23616;&#32676;&#31561;&#21464;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20960;&#20309;&#20844;&#24335;&#65292;&#34920;&#26126;L-CNN&#20013;&#30340;&#21367;&#31215;&#26159;SU($N$)&#20027;&#19995;&#19978;&#35268;&#33539;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26629;&#26684;&#35268;&#33539;&#31561;&#21464;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(L-CNNs)&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#38750;&#38463;&#36125;&#23572;&#26629;&#26684;&#35268;&#33539;&#29702;&#35770;&#65292;&#32780;&#19981;&#36829;&#21453;&#35268;&#33539;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;L-CNNs&#37197;&#22791;&#20840;&#23616;&#32676;&#31561;&#21464;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#20844;&#24335;&#25193;&#23637;&#20026;&#26082;&#31561;&#21464;&#20110;&#24179;&#31227;&#65292;&#21448;&#31561;&#21464;&#20110;&#20840;&#23616;&#32441;&#26684;&#23545;&#31216;&#24615;&#65292;&#20363;&#22914;&#26059;&#36716;&#21644;&#21453;&#23556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;L-CNN&#30340;&#20960;&#20309;&#20844;&#24335;&#65292;&#24182;&#26174;&#31034;L-CNN&#20013;&#30340;&#21367;&#31215;&#26159;SU($N$)&#20027;&#19995;&#19978;&#35268;&#33539;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lattice gauge equivariant convolutional neural networks (L-CNNs) are a framework for convolutional neural networks that can be applied to non-Abelian lattice gauge theories without violating gauge symmetry. We demonstrate how L-CNNs can be equipped with global group equivariance. This allows us to extend the formulation to be equivariant not just under translations but under global lattice symmetries such as rotations and reflections. Additionally, we provide a geometric formulation of L-CNNs and show how convolutions in L-CNNs arise as a special case of gauge equivariant neural networks on SU($N$) principal bundles.
&lt;/p&gt;</description></item><item><title>InDI&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#65292;&#20462;&#22797;&#25928;&#26524;&#26356;&#20855;&#26377;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.11435</link><description>&lt;p&gt;
&#30452;&#25509;&#36845;&#20195;&#21453;&#28436;&#65306;&#22270;&#20687;&#20462;&#22797;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration. (arXiv:2303.11435v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11435
&lt;/p&gt;
&lt;p&gt;
InDI&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#65292;&#20462;&#22797;&#25928;&#26524;&#26356;&#20855;&#26377;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#36845;&#20195;&#21453;&#28436;&#65288;InDI&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#24335;&#22270;&#20687;&#20462;&#22797;&#20844;&#24335;&#65292;&#23427;&#36991;&#20813;&#20102;&#25152;&#35859;&#30340;&#8220;&#22343;&#20540;&#22238;&#24402;&#8221;&#25928;&#24212;&#65292;&#24182;&#29983;&#25104;&#27604;&#29616;&#26377;&#22238;&#24402;&#26041;&#27861;&#26356;&#30495;&#23454;&#21644;&#35814;&#32454;&#30340;&#22270;&#20687;&#12290;&#23427;&#36890;&#36807;&#36880;&#27493;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#26469;&#23454;&#29616;&#65292;&#31867;&#20284;&#20110;&#29983;&#25104;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#22270;&#20687;&#20462;&#22797;&#26159;&#19968;&#20010;&#27424;&#23450;&#38382;&#39064;&#65292;&#22810;&#20010;&#39640;&#36136;&#37327;&#22270;&#20687;&#37117;&#21487;&#33021;&#26159;&#32473;&#23450;&#20302;&#36136;&#37327;&#36755;&#20837;&#30340;&#21487;&#34892;&#37325;&#26500;&#12290;&#22240;&#27492;&#65292;&#21333;&#27493;&#22238;&#24402;&#27169;&#22411;&#30340;&#32467;&#26524;&#36890;&#24120;&#26159;&#25152;&#26377;&#21487;&#33021;&#35299;&#37322;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#22240;&#27492;&#32570;&#20047;&#32454;&#33410;&#21644;&#30495;&#23454;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. % The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#21033;&#29992;&#38774;&#26631;&#21644;&#33647;&#29289;&#30340;&#27979;&#24207;&#20449;&#24687;&#26469;&#39044;&#27979;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;1D&#34920;&#31034;&#65292;&#26080;&#38656;&#39069;&#22806;&#29305;&#24449;&#25110;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2303.11434</link><description>&lt;p&gt;
ResDTA: &#20351;&#29992;&#27531;&#24046;&#36339;&#36291;&#36830;&#25509;&#39044;&#27979;&#33647;&#29289;-&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;
&lt;/p&gt;
&lt;p&gt;
ResDTA: Predicting Drug-Target Binding Affinity Using Residual Skip Connections. (arXiv:2303.11434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#21033;&#29992;&#38774;&#26631;&#21644;&#33647;&#29289;&#30340;&#27979;&#24207;&#20449;&#24687;&#26469;&#39044;&#27979;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;1D&#34920;&#31034;&#65292;&#26080;&#38656;&#39069;&#22806;&#29305;&#24449;&#25110;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26032;&#33647;&#29289;&#38774;&#28857;&#65288;DT&#65289;&#30456;&#20114;&#20316;&#29992;&#26159;&#26032;&#33647;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#22823;&#37096;&#20998;&#29992;&#20110;&#39044;&#27979;DT&#20132;&#20114;&#30340;&#35745;&#31639;&#26426;&#25216;&#26415;&#37117;&#38598;&#20013;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#30446;&#30340;&#22312;&#20110;&#30830;&#23450;DT&#23545;&#26159;&#21542;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#34507;&#30333;&#36136;&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#21017;&#20551;&#35774;&#23384;&#22312;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#32465;&#23450;&#24378;&#24230;&#20540;&#65292;&#20063;&#31216;&#20026;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#39044;&#27979;&#36825;&#20123;&#20540;&#19968;&#30452;&#26159;&#19968;&#20010;&#38590;&#28857;&#12290;&#38543;&#30528;DT&#30693;&#35782;&#24211;&#20013;&#20146;&#21644;&#21147;&#25968;&#25454;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#31561;&#20808;&#36827;&#30340;&#23398;&#20064;&#25216;&#26415;&#26469;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#26469;&#33258;&#38774;&#26631;&#21644;&#33647;&#29289;&#30340;&#27979;&#24207;&#20449;&#24687;&#26469;&#39044;&#27979;DT&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#20351;&#29992;1D&#34920;&#31034;&#26469;&#33258;&#38774;&#26631;&#21644;&#33647;&#29289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#33647;&#29289;&#38774;&#26631;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#29305;&#24449;&#25110;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of novel drug target (DT) interactions is an important step in the drug development process. The majority of computer techniques for predicting DT interactions have focused on binary classification, with the goal of determining whether or not a DT pair interacts. Protein ligand interactions, on the other hand, assume a continuous range of binding strength values, also known as binding affinity, and forecasting this value remains a difficulty. As the amount of affinity data in DT knowledge-bases grows, advanced learning techniques such as deep learning architectures can be used to predict binding affinities. In this paper, we present a deep-learning-based methodology for predicting DT binding affinities using just sequencing information from both targets and drugs. The results show that the proposed deep learning-based model that uses the 1D representations of targets and drugs is an effective approach for drug target binding affinity prediction and it does not require add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#20854;&#20013;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#22312;&#39044;&#27979;&#25151;&#39076;&#26041;&#38754;&#34920;&#29616;&#36739;&#22909;&#65292;&#32780;XGBoost&#22312;&#38271;&#26399;&#25968;&#25454;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#21017;&#29305;&#21035;&#36866;&#29992;&#20110;&#38271;&#26399;ECG&#35760;&#24405;&#30340;&#20998;&#31867;&#21644;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2303.11429</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24515;&#34880;&#31649;&#30142;&#30149;&#26816;&#27979;&#65306;&#24515;&#30005;&#22270;&#20449;&#21495;&#24615;&#33021;&#19982;&#22797;&#26434;&#24615;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based detection of cardiovascular disease using ECG signals: performance vs. complexity. (arXiv:2303.11429v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#20854;&#20013;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#22312;&#39044;&#27979;&#25151;&#39076;&#26041;&#38754;&#34920;&#29616;&#36739;&#22909;&#65292;&#32780;XGBoost&#22312;&#38271;&#26399;&#25968;&#25454;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#21017;&#29305;&#21035;&#36866;&#29992;&#20110;&#38271;&#26399;ECG&#35760;&#24405;&#30340;&#20998;&#31867;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#22312;&#29616;&#20195;&#31038;&#20250;&#20173;&#26159;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#12290;&#22312;&#38750;&#20405;&#20837;&#24615;&#25216;&#26415;&#20013;&#65292;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#26816;&#27979;&#24515;&#33039;&#27963;&#21160;&#24322;&#24120;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;ECG&#35299;&#35835;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#19988;&#32791;&#26102;&#12290;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21487;&#20197;&#21450;&#26089;&#26816;&#27979;&#30142;&#30149;&#21487;&#33021;&#20250;&#38450;&#27490;&#27515;&#20129;&#21644;&#24182;&#21457;&#30151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20174;ECG&#35760;&#24405;&#20013;&#20998;&#31867;&#24515;&#33039;&#30142;&#30149;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#24314;&#35758;&#20351;&#29992;ECG&#20449;&#21495;&#30340;&#27850;&#26494;&#20998;&#24067;&#34920;&#31034;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65288;ResNet50&#21644;DenseNet121&#23398;&#20064;&#20102;&#27850;&#26494;&#20998;&#24067;&#65289;&#65292;&#22312;&#39044;&#27979;&#25151;&#39076;&#26041;&#38754;&#34920;&#29616;&#19981;&#38169;&#20294;&#19981;&#33021;&#39044;&#27979;&#20854;&#20182;&#31867;&#22411;&#30340;&#24515;&#24459;&#22833;&#24120;&#12290;XGBoost&#65292;&#19968;&#31181;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#65292;&#22312;&#38271;&#26399;&#25968;&#25454;&#20013;&#34920;&#29616;&#33391;&#22909;&#20294;&#30001;&#20110;&#39044;&#22788;&#29702;&#38454;&#27573;&#20013;&#39640;&#32791;&#26102;&#35745;&#31639;&#65292;&#25512;&#26029;&#26102;&#38388;&#36739;&#38271;&#12290;&#26368;&#21518;&#65292;&#19968;&#32500;&#21367;&#31215;&#27169;&#22411;&#29305;&#21035;&#36866;&#29992;&#20110;&#38271;&#26399;ECG&#35760;&#24405;&#20998;&#31867;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular disease remains a significant problem in modern society. Among non-invasive techniques, the electrocardiogram (ECG) is one of the most reliable methods for detecting abnormalities in cardiac activities. However, ECG interpretation requires expert knowledge and it is time-consuming. Developing a novel method to detect the disease early could prevent death and complication. The paper presents novel various approaches for classifying cardiac diseases from ECG recordings. The first approach suggests the Poincare representation of ECG signal and deep-learning-based image classifiers (ResNet50 and DenseNet121 were learned over Poincare diagrams), which showed decent performance in predicting AF (atrial fibrillation) but not other types of arrhythmia. XGBoost, a gradient-boosting model, showed an acceptable performance in long-term data but had a long inference time due to highly-consuming calculation within the pre-processing phase. Finally, the 1D convolutional model, specifi
&lt;/p&gt;</description></item><item><title>LHCb&#23454;&#39564;&#20013;&#30340;90%&#35745;&#31639;&#36164;&#28304;&#29992;&#20110;&#29983;&#20135;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;Lamarr&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;LHCb&#23454;&#39564;&#30340;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#21152;&#24555;&#20102;&#27169;&#25311;&#20135;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.11428</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;LHCb&#36229;&#24555;&#36895;&#27169;&#25311;&#31995;&#32479;Lamarr&#22312;Gauss&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Lamarr: LHCb ultra-fast simulation based on machine learning models deployed within Gauss. (arXiv:2303.11428v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11428
&lt;/p&gt;
&lt;p&gt;
LHCb&#23454;&#39564;&#20013;&#30340;90%&#35745;&#31639;&#36164;&#28304;&#29992;&#20110;&#29983;&#20135;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;Lamarr&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;LHCb&#23454;&#39564;&#30340;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#21152;&#24555;&#20102;&#27169;&#25311;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LHCb&#23454;&#39564;&#21487;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#32422;90%&#29992;&#20110;&#29983;&#20135;Large Hadron Collider&#65288;LHC&#65289;&#36816;&#34892;2&#30340;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#12290;&#21319;&#32423;&#21518;&#30340;LHCb&#25506;&#27979;&#22120;&#23558;&#33021;&#22815;&#25910;&#38598;&#26356;&#22810;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#27169;&#25311;&#20107;&#20214;&#26469;&#20998;&#26512;&#23558;&#22312;&#36816;&#34892;3&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#27169;&#25311;&#26159;&#20998;&#26512;&#30340;&#20851;&#38190;&#38656;&#27714;&#65292;&#20197;&#35299;&#37322;&#20449;&#21495;&#19982;&#32972;&#26223;&#24182;&#27979;&#37327;&#25928;&#29575;&#12290;&#36825;&#31181;&#38656;&#35201;&#30340;&#27169;&#25311;&#23558;&#36828;&#36828;&#36229;&#20986;&#24050;&#25215;&#35834;&#30340;&#36164;&#28304;&#65292;&#38656;&#35201;&#25216;&#26415;&#21644;&#25216;&#24039;&#30340;&#28436;&#21464;&#26469;&#29983;&#20135;&#36825;&#20123;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Lamarr&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Gaudi&#26694;&#26550;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#23545;LHCb&#23454;&#39564;&#30340;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#21152;&#24555;&#20102;&#27169;&#25311;&#20135;&#20986;&#12290;&#20351;&#29992;&#22522;&#20110;&#22810;&#31181;&#31639;&#27861;&#21644;&#31574;&#30053;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#21442;&#25968;&#21270;&#20102;LHCb&#25506;&#27979;&#22120;&#21333;&#20010;&#32452;&#20214;&#30340;&#39640;&#32423;&#21709;&#24212;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
About 90% of the computing resources available to the LHCb experiment has been spent to produce simulated data samples for Run 2 of the Large Hadron Collider at CERN. The upgraded LHCb detector will be able to collect larger data samples, requiring many more simulated events to analyze the data to be collected in Run 3. Simulation is a key necessity of analysis to interpret signal vs background and measure efficiencies. The needed simulation will far exceed the pledged resources, requiring an evolution in technologies and techniques to produce these simulated data samples. In this contribution, we discuss Lamarr, a Gaudi-based framework to speed-up the simulation production parametrizing both the detector response and the reconstruction algorithms of the LHCb experiment. Deep Generative Models powered by several algorithms and strategies are employed to effectively parametrize the high-level response of the single components of the LHCb detector, encoding within neural networks the exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#31283;&#20581;&#39044;&#32534;&#30721;&#65292;&#20197;&#23454;&#29616;&#21327;&#20316;&#22810;&#27874;&#26463;&#21355;&#26143;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.11427</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;&#31283;&#20581;&#39044;&#32534;&#30721;&#20197;&#23454;&#29616;&#21327;&#20316;&#22810;&#27874;&#26463;&#21355;&#26143;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Learning Model-Free Robust Precoding for Cooperative Multibeam Satellite Communications. (arXiv:2303.11427v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#19988;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#30340;&#31283;&#20581;&#39044;&#32534;&#30721;&#65292;&#20197;&#23454;&#29616;&#21327;&#20316;&#22810;&#27874;&#26463;&#21355;&#26143;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#30340;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#21040;&#25163;&#25345;&#35774;&#22791;&#30340;&#38142;&#25509;&#26377;&#26395;&#25104;&#20026;&#21355;&#26143;&#36890;&#20449;&#30340;&#26032;&#26102;&#20195;&#12290;&#31354;&#38388;&#20998;&#38598;&#22810;&#22336;&#39044;&#32534;&#30721;&#26159;&#19968;&#31181;&#20943;&#23569;&#21355;&#26143;&#27874;&#26463;&#24178;&#25200;&#12289;&#36890;&#36807;&#21355;&#26143;&#21512;&#20316;&#37325;&#26032;&#20351;&#29992;&#39057;&#29575;&#20174;&#32780;&#25552;&#39640;&#39057;&#35889;&#25928;&#29575;&#30340;&#25216;&#26415;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#38024;&#23545;&#20960;&#31181;&#24773;&#20917;&#24050;&#32463;&#25552;&#20986;&#20102;&#20855;&#26377;&#23436;&#32654;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#26368;&#20248;&#39044;&#32534;&#30721;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#20165;&#20855;&#26377;&#19981;&#23436;&#32654;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#31283;&#20581;&#39044;&#32534;&#30721;&#20027;&#35201;&#38024;&#23545;&#31616;&#21270;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#20302;&#22320;&#29699;&#36712;&#36947;&#21355;&#26143;&#24212;&#29992;&#26469;&#35828;&#65292;&#36825;&#31181;&#31616;&#21270;&#27169;&#22411;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#36719;&#24615;Actor-Critic&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20989;&#25968;&#36924;&#36817;&#33021;&#21147;&#65292;&#23398;&#20064;&#19981;&#38656;&#35201;&#31995;&#32479;&#32570;&#38519;&#30693;&#35782;&#30340;&#31283;&#20581;&#39044;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Direct Low Earth Orbit satellite-to-handheld links are expected to be part of a new era in satellite communications. Space-Division Multiple Access precoding is a technique that reduces interference among satellite beams, therefore increasing spectral efficiency by allowing cooperating satellites to reuse frequency. Over the past decades, optimal precoding solutions with perfect channel state information have been proposed for several scenarios, whereas robust precoding with only imperfect channel state information has been mostly studied for simplified models. In particular, for Low Earth Orbit satellite applications such simplified models might not be accurate. In this paper, we use the function approximation capabilities of the Soft Actor-Critic deep Reinforcement Learning algorithm to learn robust precoding with no knowledge of the system imperfections.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#27874;&#25955;&#23556;&#21464;&#25442;&#21644;1D-CNN&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#32467;&#21512;&#30340;&#24515;&#26434;&#38899;&#33258;&#21160;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;97.98%&#30340;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.11423</link><description>&lt;p&gt;
&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21464;&#25442;&#21644;1D-CNN&#36827;&#34892;&#21548;&#35786;&#22120;&#24515;&#26434;&#38899;&#21644;&#24322;&#24120;PCG&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Heart Murmur and Abnormal PCG Detection via Wavelet Scattering Transform &amp; a 1D-CNN. (arXiv:2303.11423v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11423
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#27874;&#25955;&#23556;&#21464;&#25442;&#21644;1D-CNN&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#32467;&#21512;&#30340;&#24515;&#26434;&#38899;&#33258;&#21160;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;97.98%&#30340;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#21548;&#35786;&#22120;&#24515;&#26434;&#38899;&#30340;&#33258;&#21160;&#21644;&#20934;&#30830;&#26816;&#27979;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#20844;&#20849;PCG&#25968;&#25454;&#38598;&#65288;CirCor Digiscope 2022&#21644;PCG 2016&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#19977;&#20010;&#33258;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#21367;&#31215;RNN&#65288;C-RNN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work leverages deep learning (DL) techniques in order to do automatic and accurate heart murmur detection from phonocardiogram (PCG) recordings. Two public PCG datasets (CirCor Digiscope 2022 dataset and PCG 2016 dataset) from Physionet online database are utilized to train and test three custom neural networks (NN): a 1D convolutional neural network (CNN), a long short-term memory (LSTM) recurrent neural network (RNN), and a convolutional RNN (C-RNN). Under our proposed method, we first do pre-processing on both datasets in order to prepare the data for the NNs. Key pre-processing steps include the following: denoising, segmentation, re-labeling of noise-only segments, data normalization, and time-frequency analysis of the PCG segments using wavelet scattering transform. To evaluate the performance of the three NNs we have implemented, we conduct four experiments, first three using PCG 2022 dataset, and fourth using PCG 2016 dataset. It turns out that our custom 1D-CNN outperform
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#22495;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#32593;&#32476;&#65292;&#20854;&#20013;&#34701;&#21512;&#20102;&#26102;&#39057;&#22495;&#21644;&#31354;&#38388;&#22495;&#30340;&#22810;&#37325;&#34920;&#31034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;EEG&#24773;&#24863;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11421</link><description>&lt;p&gt;
&#34701;&#21512;&#26102;&#39057;&#21644;&#31354;&#38388;&#34920;&#31034;&#26469;&#25913;&#21892;&#22522;&#20110;EEG&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving EEG-based Emotion Recognition by Fusing Time-frequency And Spatial Representations. (arXiv:2303.11421v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11421
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#22495;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#32593;&#32476;&#65292;&#20854;&#20013;&#34701;&#21512;&#20102;&#26102;&#39057;&#22495;&#21644;&#31354;&#38388;&#22495;&#30340;&#22810;&#37325;&#34920;&#31034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;EEG&#24773;&#24863;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;EEG&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#20154;&#20204;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24456;&#23569;&#32771;&#34385;&#23558;&#21478;&#19968;&#20010;&#39046;&#22495;&#30340;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#24212;&#29992;&#20110;&#26102;&#39057;&#22495;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#22495;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#30340;EEG&#20449;&#21495;&#20998;&#31867;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#22495;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#32593;&#32476;&#26356;&#19987;&#27880;&#20110;&#19982;&#33041;&#27963;&#21160;&#21644;&#24605;&#32500;&#21464;&#21270;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#34701;&#21512;&#26041;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;EEG&#24773;&#24863;&#35782;&#21035;&#32593;&#32476;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#34701;&#21512;&#26102;&#39057;&#22495;&#21644;&#31354;&#38388;&#22495;&#30340;&#22810;&#37325;&#34920;&#31034;&#26041;&#27861;&#65292;&#20248;&#20110;&#20808;&#21069;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#26041;&#27861;&#65292;&#22312;&#24403;&#21069;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using deep learning methods to classify EEG signals can accurately identify people's emotions. However, existing studies have rarely considered the application of the information in another domain's representations to feature selection in the time-frequency domain. We propose a classification network of EEG signals based on the cross-domain feature fusion method, which makes the network more focused on the features most related to brain activities and thinking changes by using the multi-domain attention mechanism. In addition, we propose a two-step fusion method and apply these methods to the EEG emotion recognition network. Experimental results show that our proposed network, which combines multiple representations in the time-frequency domain and spatial domain, outperforms previous methods on public datasets and achieves state-of-the-art at present.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#28857;&#20113;&#37319;&#26679;&#30340;&#36890;&#29992;&#38598;&#25104;&#26694;&#26550;&#65292;&#30001;&#22810;&#31181;&#37319;&#26679;&#26041;&#27861;&#32852;&#21512;&#20351;&#29992;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11419</link><description>&lt;p&gt;
EPiC: &#22522;&#20110;&#37096;&#20998;&#28857;&#20113;&#38598;&#25104;&#30340;&#40065;&#26834;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EPiC: Ensemble of Partial Point Clouds for Robust Classification. (arXiv:2303.11419v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#28857;&#20113;&#37319;&#26679;&#30340;&#36890;&#29992;&#38598;&#25104;&#26694;&#26550;&#65292;&#30001;&#22810;&#31181;&#37319;&#26679;&#26041;&#27861;&#32852;&#21512;&#20351;&#29992;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#20013;&#30340;&#28857;&#20113;&#20998;&#31867;&#26469;&#35828;&#65292;&#30001;&#20110;&#28040;&#36153;&#22411;3D&#20256;&#24863;&#22120;&#36890;&#24120;&#37319;&#38598;&#30340;&#26159;&#37096;&#20998;&#21644;&#24102;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#65292;&#19988;&#20250;&#21463;&#21040;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#32780;&#21464;&#24471;&#38477;&#36136;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#28857;&#20113;&#37319;&#26679;&#30340;&#36890;&#29992;&#38598;&#25104;&#26694;&#26550;&#12290;&#27599;&#20010;&#38598;&#25104;&#25104;&#21592;&#21482;&#26292;&#38706;&#20110;&#37096;&#20998;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#21516;&#26102;&#20351;&#29992;&#20004;&#31181;&#22522;&#20110;&#34917;&#19969;&#21644;&#26354;&#32447;&#30340;&#23616;&#37096;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#19968;&#31181;&#20840;&#23616;&#30340;&#38543;&#26426;&#37319;&#26679;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21508;&#31181;&#23616;&#37096;&#21644;&#20840;&#23616;&#27745;&#26579;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#39030;&#32423;&#20998;&#31867;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#20351;&#29992;&#20102;Ren&#31561;&#20154;[24]&#24341;&#20837;&#30340;&#26368;&#26032;ModelNet-C&#25968;&#25454;&#24211;&#65292;&#22312;&#19981;&#32463;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#32463;&#36807;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#37117;&#36798;&#21040;&#20102;&#26368;&#20248;&#65288;SOTA&#65289;&#12290;&#25105;&#20204;&#30340;&#26410;&#32463;&#25968;&#25454;&#22686;&#24378;&#30340;&#22343;&#20540;&#33104;&#34432;&#35823;&#24046;&#65288;mCE&#65289;&#20026;0.64&#65288;&#24403;&#21069;SOTA&#20026;0.86&#65289;&#65292;&#32463;&#36807;&#25968;&#25454;&#22686;&#24378;&#21518;&#20026;0.50&#65288;&#24403;&#21069;SOTA&#20026;0.57&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#26679;&#24615;&#21644;&#21487;&#35270;&#21270;&#20998;&#26512;&#20102;&#36825;&#20123;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust point cloud classification is crucial for real-world applications, as consumer-type 3D sensors often yield partial and noisy data, degraded by various artifacts. In this work we propose a general ensemble framework, based on partial point cloud sampling. Each ensemble member is exposed to only partial input data. Three sampling strategies are used jointly, two local ones, based on patches and curves, and a global one of random sampling. We demonstrate the robustness of our method to various local and global degradations. We show that our framework significantly improves the robustness of top classification netowrks by a large margin. Our experimental setting uses the recently introduced ModelNet-C database by Ren et al.[24], where we reach SOTA both on unaugmented and on augmented data. Our unaugmented mean Corruption Error (mCE) is 0.64 (current SOTA is 0.86) and 0.50 for augmented data (current SOTA is 0.57). We analyze and explain these remarkable results through diversity an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2303.11413</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25391;&#21160;&#20449;&#21495;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#33050;&#27493;&#24341;&#36215;&#30340;&#32467;&#26500;&#25391;&#21160;&#20449;&#21495;&#34987;&#24191;&#27867;&#29992;&#20110;&#20154;&#21592;&#35782;&#21035;&#12289;&#23450;&#20301;&#12289;&#20154;&#31867;&#27963;&#21160;&#25512;&#26029;&#12289;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#22122;&#22768;&#12289;&#30005;&#30913;&#24178;&#25200;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23454;&#38469;&#37319;&#38598;&#30340;&#20449;&#21495;&#36890;&#24120;&#20250;&#24102;&#26377;&#22122;&#22768;&#12290;&#22122;&#22768;&#30340;&#23384;&#22312;&#24433;&#21709;&#20102;&#20449;&#21495;&#22788;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26368;&#32456;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#35823;&#24046;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#21253;&#25324;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;OVAE&#26041;&#27861;&#65292;&#21487;&#20197;&#32422;&#26463;&#28508;&#21464;&#37327;&#31354;&#38388;&#20195;&#30721;&#19982;&#29983;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#38142;&#25509;&#65292;&#20197;&#29983;&#25104;&#19982;&#30005;&#21147;&#31995;&#32479;&#20013;&#29305;&#23450;&#30340;&#39640;&#39118;&#38505;&#29366;&#24577;&#23545;&#40784;&#30340;&#20855;&#26377;&#23450;&#21521;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#23545;&#20851;&#38190;&#20107;&#20214;&#30340;&#39044;&#27979;&#21644;&#32531;&#35299;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11410</link><description>&lt;p&gt;
&#20351;&#29992;&#23450;&#21521;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#39118;&#38505;&#29366;&#24577;&#30340;&#23450;&#21521;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Targeted Analysis of High-Risk States Using an Oriented Variational Autoencoder. (arXiv:2303.11410v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;OVAE&#26041;&#27861;&#65292;&#21487;&#20197;&#32422;&#26463;&#28508;&#21464;&#37327;&#31354;&#38388;&#20195;&#30721;&#19982;&#29983;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#38142;&#25509;&#65292;&#20197;&#29983;&#25104;&#19982;&#30005;&#21147;&#31995;&#32479;&#20013;&#29305;&#23450;&#30340;&#39640;&#39118;&#38505;&#29366;&#24577;&#23545;&#40784;&#30340;&#20855;&#26377;&#23450;&#21521;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#23545;&#20851;&#38190;&#20107;&#20214;&#30340;&#39044;&#27979;&#21644;&#32531;&#35299;&#30340;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34987;&#35757;&#32451;&#20197;&#29983;&#25104;&#25429;&#25417;&#21382;&#21490;&#25968;&#25454;&#36793;&#38469;&#20998;&#24067;&#21644;&#22810;&#20803;&#20381;&#36182;&#24615;&#30340;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#12290;VAE&#30340;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#22352;&#26631;&#24050;&#34987;&#35777;&#26126;&#19982;&#25968;&#25454;&#30340;&#27010;&#24565;&#29305;&#24449;&#30456;&#20851;&#65292;&#36825;&#21487;&#20197;&#21033;&#29992;&#26469;&#21512;&#25104;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#23450;&#21521;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;VAE&#30340;&#28508;&#21464;&#37327;&#31354;&#38388;&#20195;&#30721;&#23545;&#24212;&#29305;&#23450;&#23646;&#24615;&#30340;&#20301;&#32622;&#24182;&#19981;&#21463;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#30340;&#25968;&#25454;&#21487;&#33021;&#38656;&#35201;&#23558;&#20855;&#26377;&#23545;&#24212;&#38590;&#20197;&#33719;&#24471;&#26631;&#31614;&#30340;&#25968;&#25454;&#39304;&#20837;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20351;&#25968;&#25454;&#29983;&#25104;&#26356;&#20855;&#21487;&#25511;&#24615;&#21644;&#25928;&#29575;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21521;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;OVAE&#65289;&#65292;&#20197;Spearman&#30456;&#20851;&#24615;&#30340;&#24418;&#24335;&#32422;&#26463;&#28508;&#21464;&#37327;&#31354;&#38388;&#20195;&#30721;&#19982;&#29983;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#38142;&#25509;&#65292;&#36825;&#20026;&#25968;&#25454;&#21512;&#25104;&#36807;&#31243;&#25552;&#20379;&#20102;&#26356;&#31934;&#32454;&#30340;&#25511;&#21046;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#35201;&#24615;&#37319;&#26679;&#36807;&#31243;&#65292;&#20197;&#38024;&#23545;&#30005;&#21147;&#31995;&#32479;&#20013;&#29305;&#23450;&#30340;&#39640;&#39118;&#38505;&#29366;&#24577;&#12290; OVAE&#34987;&#35777;&#26126;&#33021;&#22815;&#29983;&#25104;&#19982;&#25152;&#38656;&#39640;&#39118;&#38505;&#29366;&#24577;&#23545;&#40784;&#30340;&#20855;&#26377;&#23450;&#21521;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#30005;&#21147;&#31995;&#32479;&#20013;&#20851;&#38190;&#20107;&#20214;&#30340;&#26356;&#26377;&#25928;&#39044;&#27979;&#21644;&#32531;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoder (VAE) neural networks can be trained to generate power system states that capture both marginal distribution and multivariate dependencies of historical data. The coordinates of the latent space codes of VAEs have been shown to correlate with conceptual features of the data, which can be leveraged to synthesize targeted data with desired features. However, the locations of the VAEs' latent space codes that correspond to specific properties are not constrained. Additionally, the generation of data with specific characteristics may require data with corresponding hard-to-get labels fed into the generative model for training. In this paper, to make data generation more controllable and efficient, an oriented variation autoencoder (OVAE) is proposed to constrain the link between latent space code and generated data in the form of a Spearman correlation, which provides increased control over the data synthesis process. On this basis, an importance sampling process is
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11403</link><description>&lt;p&gt;
eP-ALM:&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24863;&#30693;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
eP-ALM: Efficient Perceptual Augmentation of Language Models. (arXiv:2303.11403v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#23545;&#27604;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#26041;&#27861;eP-ALM&#65292;&#21487;&#20197;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#36824;&#33021;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36804;&#20170;&#20026;&#27490;&#32473;&#19990;&#30028;&#30041;&#19979;&#20102;&#28145;&#21051;&#21360;&#35937;&#65292;&#20855;&#26377;&#22823;&#35268;&#27169;&#27169;&#22411;&#25152;&#20855;&#26377;&#30340;&#38750;&#21516;&#23547;&#24120;&#30340;&#33021;&#21147;&#12290;&#22312;&#35270;&#35273;&#26041;&#38754;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#65288;&#21363;ViT&#65289;&#20063;&#22312;&#36861;&#38543;&#21516;&#19968;&#36235;&#21183;&#65292;&#21462;&#24471;&#20102;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#38543;&#30528;&#36825;&#31181;&#21333;&#27169;&#22411;&#30340;&#20016;&#23500;&#22810;&#26679;&#65292;&#33258;&#28982;&#20250;&#24341;&#21457;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#38656;&#35201;&#36319;&#38543;&#36825;&#20010;&#36235;&#21183;&#26469;&#22788;&#29702;&#22810;&#27169;&#24577;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21162;&#21147;&#38598;&#20013;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#24182;&#25552;&#20986;&#29992;&#24863;&#30693;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#20182;&#20204;&#20173;&#28982;&#35757;&#32451;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#20381;&#36182;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#22312;&#24040;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#20851;&#27880;Zero-Shot&#21644;In Context Learning&#65292;&#35266;&#23519;&#21040;&#20004;&#31181;&#33539;&#24335;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;eP-ALM&#65292;&#19968;&#31181;&#23558;&#35270;&#35273;&#24863;&#30693;&#20449;&#24687;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#35270;&#35273;&#24863;&#30693;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#20855;&#26377;&#26497;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26032;&#30340;&#39044;&#35757;&#32451;&#65292;&#20173;&#28982;&#22312;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have so far impressed the world, with unprecedented capabilities that emerge in models at large scales. On the vision side, transformer models (i.e., ViT) are following the same trend, achieving the best performance on challenging benchmarks. With the abundance of such unimodal models, a natural question arises; do we need also to follow this trend to tackle multimodal tasks? In this work, we propose to rather direct effort to efficient adaptations of existing models, and propose to augment Language Models with perception. Existing approaches for adapting pretrained models for vision-language tasks still rely on several key components that hinder their efficiency. In particular, they still train a large number of parameters, rely on large multimodal pretraining, use encoders (e.g., CLIP) trained on huge image-text datasets, and add significant inference overhead. In addition, most of these approaches have focused on Zero-Shot and In Context Learning, with l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20845;&#31181;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#33322;&#31354;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#20256;&#32479;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#26377;&#31867;&#20284;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11389</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#23454;&#29616;&#33322;&#31354;&#22330;&#26223;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Combining Deep Metric Learning Approaches for Aerial Scene Classification. (arXiv:2303.11389v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20845;&#31181;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#33322;&#31354;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#20256;&#32479;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#26377;&#31867;&#20284;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#31354;&#22330;&#26223;&#20998;&#31867;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36965;&#24863;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#65288;&#20363;&#22914;&#20892;&#19994;&#12289;&#28023;&#28393;&#21644;&#28207;&#21475;&#65289;&#23545;&#36965;&#24863;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#26631;&#31614;&#12290;&#30001;&#20110;&#25968;&#25454;&#38598;&#22270;&#20687;&#20013;&#25152;&#21253;&#21547;&#30340;&#23545;&#35937;&#20855;&#26377;&#19981;&#21516;&#30340;&#23610;&#24230;&#21644;&#26041;&#21521;&#65292;&#22240;&#27492;&#35813;&#20219;&#21153;&#20855;&#26377;&#39640;&#24230;&#30340;&#31867;&#20869;&#21464;&#21270;&#12290;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;CNN&#26550;&#26500;&#30340;&#20351;&#29992;&#26159;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#30340;&#26367;&#20195;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#24120;&#65292;&#36825;&#20123;CNN&#34987;&#29992;&#20110;&#25191;&#34892;&#20256;&#32479;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21478;&#19968;&#20010;&#19981;&#22826;&#24120;&#29992;&#30340;&#36828;&#31243;&#24863;&#30693;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#21487;&#33021;&#26159;&#20351;&#29992;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65288;DML&#65289;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20845;&#31181;DML&#26041;&#27861;&#36827;&#34892;&#33322;&#31354;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#19982;&#22235;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;CNN&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#65288;UMDA&#65289;&#23558;&#23427;&#20204;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23454;&#39564;&#21487;&#35266;&#23519;&#21040;&#65292;DML&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#20256;&#32479;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#31934;&#24230;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#36890;&#36807;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#32467;&#21512;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aerial scene classification, which aims to semantically label remote sensing images in a set of predefined classes (e.g., agricultural, beach, and harbor), is a very challenging task in remote sensing due to high intra-class variability and the different scales and orientations of the objects present in the dataset images. In remote sensing area, the use of CNN architectures as an alternative solution is also a reality for scene classification tasks. Generally, these CNNs are used to perform the traditional image classification task. However, another less used way to classify remote sensing image might be the one that uses deep metric learning (DML) approaches. In this sense, this work proposes to employ six DML approaches for aerial scene classification tasks, analysing their behave with four different pre-trained CNNs as well as combining them through the use of evolutionary computation algorithm (UMDA). In performed experiments, it is possible to observe than DML approaches can achi
&lt;/p&gt;</description></item><item><title>MM-REACT&#26159;&#19968;&#31181;&#34701;&#21512;&#35270;&#35273;&#19987;&#23478;&#21644;ChatGPT&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#21644;&#34892;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25991;&#23383;&#25552;&#31034;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;ChatGPT&#21644;&#21508;&#31181;&#35270;&#35273;&#19987;&#23478;&#30340;&#21327;&#21516;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#39640;&#32423;&#35270;&#35273;&#29702;&#35299;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.11381</link><description>&lt;p&gt;
MM-REACT&#65306;&#20419;&#36827;ChatGPT&#36827;&#34892;&#22810;&#27169;&#24577;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#31995;&#32479;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action. (arXiv:2303.11381v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11381
&lt;/p&gt;
&lt;p&gt;
MM-REACT&#26159;&#19968;&#31181;&#34701;&#21512;&#35270;&#35273;&#19987;&#23478;&#21644;ChatGPT&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#21644;&#34892;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#25991;&#23383;&#25552;&#31034;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;ChatGPT&#21644;&#21508;&#31181;&#35270;&#35273;&#19987;&#23478;&#30340;&#21327;&#21516;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#39640;&#32423;&#35270;&#35273;&#29702;&#35299;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MM-REACT&#65292;&#19968;&#31181;&#23558;ChatGPT&#19982;&#35270;&#35273;&#19987;&#23478;&#27744;&#38598;&#25104;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#31995;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23450;&#20041;&#24182;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#20808;&#36827;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#24456;&#26377;&#36259;&#65292;&#20294;&#21487;&#33021;&#36229;&#20986;&#20102;&#29616;&#26377;&#35270;&#35273;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#39640;&#32423;&#35270;&#35273;&#26234;&#33021;&#65292;MM-REACT&#24341;&#20837;&#20102;&#19968;&#31181;&#25991;&#26412;&#25552;&#31034;&#35774;&#35745;&#65292;&#21487;&#20197;&#34920;&#31034;&#25991;&#26412;&#25551;&#36848;&#12289;&#25991;&#26412;&#21270;&#30340;&#31354;&#38388;&#22352;&#26631;&#21644;&#23545;&#40784;&#30340;&#25991;&#20214;&#21517;&#65292;&#29992;&#20110;&#22788;&#29702;&#22270;&#20687;&#21644;&#35270;&#39057;&#31561;&#23494;&#38598;&#30340;&#35270;&#35273;&#20449;&#21495;&#12290;MM-REACT&#30340;&#25552;&#31034;&#35774;&#35745;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#25509;&#21463;&#12289;&#20851;&#32852;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;ChatGPT&#21644;&#21508;&#31181;&#35270;&#35273;&#19987;&#23478;&#30340;&#21327;&#21516;&#32452;&#21512;&#12290;&#38646;&#26679;&#26412;&#23454;&#39564;&#35777;&#26126;&#20102;MM-REACT&#35299;&#20915;&#24863;&#20852;&#36259;&#30340;&#29305;&#23450;&#33021;&#21147;&#20197;&#21450;&#22312;&#38656;&#35201;&#39640;&#32423;&#35270;&#35273;&#29702;&#35299;&#30340;&#19981;&#21516;&#22330;&#26223;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;M
&lt;/p&gt;
&lt;p&gt;
We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and aligned file names for dense visual signals such as images and videos. MM-REACT's prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require advanced visual understanding. Furthermore, we discuss and compare M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#27969;&#22270;&#26657;&#20934;&#30340;&#26694;&#26550;&#65292; &#24182;&#32467;&#21512;&#20808;&#21069;&#36741;&#21161;&#36807;&#31243;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#39640;&#32500;&#36870;&#38382;&#39064;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#39640;&#32500;&#31995;&#32479;&#20013;&#30340;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.11379</link><description>&lt;p&gt;
&#21033;&#29992;&#36741;&#21161;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#30340;&#36816;&#31639;&#22120;&#23398;&#20064;&#26469;&#35299;&#20915;&#39640;&#32500;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving High-Dimensional Inverse Problems with Auxiliary Uncertainty via Operator Learning with Limited Data. (arXiv:2303.11379v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#27969;&#22270;&#26657;&#20934;&#30340;&#26694;&#26550;&#65292; &#24182;&#32467;&#21512;&#20808;&#21069;&#36741;&#21161;&#36807;&#31243;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#39640;&#32500;&#36870;&#38382;&#39064;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#39640;&#32500;&#31995;&#32479;&#20013;&#30340;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#22823;&#35268;&#27169;&#31995;&#32479;&#65288;&#22914;&#27668;&#20505;&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#24433;&#21709;&#26159;&#30001;&#22810;&#31181;&#26410;&#34987;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#24341;&#36215;&#30340;&#12290;&#26681;&#25454;&#31995;&#32479;&#29366;&#24577;&#30340;&#35266;&#23519;&#32467;&#26524;&#26469;&#35782;&#21035;&#20986;&#36825;&#20123;&#24433;&#21709;&#22240;&#32032;&#23545;&#20110;&#24402;&#22240;&#21644;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#32467;&#26524;&#21448;&#20026;&#20915;&#31574;&#21046;&#23450;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#36870;&#38382;&#39064;&#30340;&#38590;&#28857;&#22312;&#20110;&#26080;&#27861;&#38548;&#31163;&#24433;&#21709;&#22240;&#32032;&#21644;&#35745;&#31639;&#27169;&#22411;&#27169;&#25311;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#27969;&#22270;&#26657;&#20934;&#65292;&#24182;&#32467;&#21512;&#20808;&#21069;&#36741;&#21161;&#36807;&#31243;&#30340;&#30693;&#35782;&#26469;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#19979;&#30340;&#39640;&#32500;&#36870;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22312;&#35266;&#23519;&#25968;&#25454;&#31232;&#30095;&#22122;&#22768;&#36739;&#22823;&#30340;&#39640;&#32500;&#31995;&#32479;&#20013;&#30830;&#23450;&#20986;&#28304;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26377;&#38480;&#25968;&#25454;&#28304;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In complex large-scale systems such as climate, important effects are caused by a combination of confounding processes that are not fully observable. The identification of sources from observations of system state is vital for attribution and prediction, which inform critical policy decisions. The difficulty of these types of inverse problems lies in the inability to isolate sources and the cost of simulating computational models. Surrogate models may enable the many-query algorithms required for source identification, but data challenges arise from high dimensionality of the state and source, limited ensembles of costly model simulations to train a surrogate model, and few and potentially noisy state observations for inversion due to measurement limitations. The influence of auxiliary processes adds an additional layer of uncertainty that further confounds source identification. We introduce a framework based on (1) calibrating deep neural network surrogates to the flow maps provided 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GNN-Ensemble&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;&#38543;&#26426;&#20915;&#31574;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#65292;&#20197;&#25552;&#39640;GNNs&#30340;&#24615;&#33021;&#65292;&#27867;&#21270;&#33021;&#21147;&#21644;&#25239;&#25915;&#20987;&#24615;&#65292;&#24182;&#36981;&#24490;&#38543;&#26426;&#24314;&#27169;&#30340;&#21407;&#21017;&#12290;</title><link>http://arxiv.org/abs/2303.11376</link><description>&lt;p&gt;
GNN-Ensemble&#65306;&#38754;&#21521;&#38543;&#26426;&#20915;&#31574;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GNN-Ensemble: Towards Random Decision Graph Neural Networks. (arXiv:2303.11376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GNN-Ensemble&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;&#38543;&#26426;&#20915;&#31574;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#21512;&#65292;&#20197;&#25552;&#39640;GNNs&#30340;&#24615;&#33021;&#65292;&#27867;&#21270;&#33021;&#21147;&#21644;&#25239;&#25915;&#20987;&#24615;&#65292;&#24182;&#36981;&#24490;&#38543;&#26426;&#24314;&#27169;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#30340;&#24212;&#29992;&#36890;&#24120;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#12290;GNNs&#38656;&#35201;&#20174;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#30340;&#27169;&#24335;&#65292;&#20197;&#23545;&#22823;&#37327;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#12290;GNNs&#30340;&#22686;&#21152;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21333;&#28857;&#27169;&#22411;&#21442;&#25968;&#21021;&#22987;&#21270;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#36807;&#24230;&#36866;&#24212;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20247;&#25152;&#21608;&#30693;GNNs&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GNN-Ensemble&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36981;&#24490;&#38543;&#26426;&#24314;&#27169;&#30340;&#21407;&#21017;&#65292;&#22312;&#25299;&#25169;&#19978;&#38543;&#26426;&#36873;&#25321;&#23376;&#32467;&#26500;&#20013;&#26500;&#24314;&#22810;&#20010;GNNs&#26469;&#26500;&#24314;&#38543;&#26426;&#20915;&#31574;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#23481;&#37327;&#21487;&#20197;&#20219;&#24847;&#25193;&#23637;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have enjoyed wide spread applications in graph-structured data. However, existing graph based applications commonly lack annotated data. GNNs are required to learn latent patterns from a limited amount of training data to perform inferences on a vast amount of test data. The increased complexity of GNNs, as well as a single point of model parameter initialization, usually lead to overfitting and sub-optimal performance. In addition, it is known that GNNs are vulnerable to adversarial attacks. In this paper, we push one step forward on the ensemble learning of GNNs with improved accuracy, generalization, and adversarial robustness. Following the principles of stochastic modeling, we propose a new method called GNN-Ensemble to construct an ensemble of random decision graph neural networks whose capacity can be arbitrarily expanded for improvement in performance. The essence of the method is to build multiple GNNs in randomly selected substructures in the topo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32422;&#26463;&#28385;&#36275;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#32423;&#32467;&#26500;&#30340;&#25277;&#35937;&#23454;&#29616;&#20102;&#22312;&#32452;&#21512;&#29366;&#24577;&#19979;&#30340;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11373</link><description>&lt;p&gt;
&#31070;&#32463;&#32422;&#26463;&#28385;&#36275;&#65306;&#23618;&#32423;&#25277;&#35937;&#23454;&#29616;&#32452;&#21512;&#24335;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Neural Constraint Satisfaction: Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement. (arXiv:2303.11373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32422;&#26463;&#28385;&#36275;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#32423;&#32467;&#26500;&#30340;&#25277;&#35937;&#23454;&#29616;&#20102;&#22312;&#32452;&#21512;&#29366;&#24577;&#19979;&#30340;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#23454;&#20307;&#26426;&#22120;&#20154;&#26234;&#33021;&#30340;&#38382;&#39064;&#32780;&#35328;&#65292;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#27867;&#21270;&#21040;&#26497;&#22823;&#32452;&#21512;&#30340;&#23454;&#20307;&#29366;&#24577;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36825;&#20123;&#23454;&#20307;&#30340;&#34920;&#31034;&#26159;&#26410;&#30693;&#30340;&#65292;&#24517;&#39035;&#20174;&#24863;&#23448;&#36755;&#20837;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#36825;&#20123;&#24213;&#23618;&#23454;&#20307;&#65292;&#24182;&#20174;&#38750;&#32467;&#26500;&#21270;&#35270;&#35273;&#36755;&#20837;&#20013;&#23454;&#29616;&#32452;&#21512;&#27867;&#21270;&#12290;&#36890;&#36807;&#22312;&#20687;&#32032;&#32858;&#31867;&#19978;&#26500;&#24314;&#19968;&#20010;&#22240;&#23376;&#21270;&#30340;&#36716;&#25442;&#22270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#20013;&#23454;&#20307;&#29366;&#24577;&#21644;&#29615;&#22659;&#29289;&#20307;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#20851;&#31995;&#24320;&#21457;&#20102;&#19968;&#31181;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#27867;&#21270;&#21040;&#19981;&#21516;&#25968;&#37327;&#21644;&#37197;&#32622;&#30340;&#29289;&#20307;&#65292;&#24403;&#22312;&#27169;&#25311;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#24403;&#21069;&#30340;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object rearrangement is a challenge for embodied agents because solving these tasks requires generalizing across a combinatorially large set of configurations of entities and their locations. Worse, the representations of these entities are unknown and must be inferred from sensory percepts. We present a hierarchical abstraction approach to uncover these underlying entities and achieve combinatorial generalization from unstructured visual inputs. By constructing a factorized transition graph over clusters of entity representations inferred from pixels, we show how to learn a correspondence between intervening on states of entities in the agent's model and acting on objects in the environment. We use this correspondence to develop a method for control that generalizes to different numbers and configurations of objects, which outperforms current offline deep RL methods when evaluated on simulated rearrangement tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;EEG&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#27880;&#24847;&#21147;&#29366;&#24577;&#20998;&#31867;&#65292;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11371</link><description>&lt;p&gt;
&#38024;&#23545;&#27880;&#24847;&#21147;&#29366;&#24577;&#20998;&#31867;&#30340;&#20248;&#21270;&#39044;&#22788;&#29702;&#21644;&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimized preprocessing and Tiny ML for Attention State Classification. (arXiv:2303.11371v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;EEG&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#27880;&#24847;&#21147;&#29366;&#24577;&#20998;&#31867;&#65292;&#22312;&#24615;&#33021;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;EEG&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#27880;&#24847;&#21147;&#29366;&#24577;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#35748;&#30693;&#36127;&#33655;&#20219;&#21153;&#26399;&#38388;&#25910;&#38598;&#30340;EEG&#35760;&#24405;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#20998;&#31867;&#27880;&#24847;&#21147;&#29366;&#24577;&#65292;&#24182;&#22312;&#20998;&#31867;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new approach to mental state classification from EEG signals by combining signal processing techniques and machine learning (ML) algorithms. We evaluate the performance of the proposed method on a dataset of EEG recordings collected during a cognitive load task and compared it to other state-of-the-art methods. The results show that the proposed method achieves high accuracy in classifying mental states and outperforms state-of-the-art methods in terms of classification accuracy and computational efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;iPSRL&#21644;iRLSVI&#65292;&#26088;&#22312;&#35299;&#20915;&#32473;&#23450;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#25022;&#65292;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.11369</link><description>&lt;p&gt;
&#12298;&#26725;&#25509;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#20010;&#20048;&#35266;&#30340;&#25925;&#20107;&#12299;
&lt;/p&gt;
&lt;p&gt;
Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale. (arXiv:2303.11369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;iPSRL&#21644;iRLSVI&#65292;&#26088;&#22312;&#35299;&#20915;&#32473;&#23450;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#25022;&#65292;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20197;&#19979;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#19981;&#23436;&#32654;&#19987;&#23478;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#65292;&#26368;&#22909;&#30340;&#26041;&#24335;&#26159;&#20160;&#20040;&#26469;&#21033;&#29992;&#23427;&#26469;&#24341;&#23548; MDP &#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#34920;&#29616;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#24773;&#21518;&#39564;&#37319;&#26679;&#30340; RL&#65288;iPSRL&#65289;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#19987;&#23478;&#30340;&#34892;&#20026;&#31574;&#30053;&#20449;&#24687;&#26469;&#29983;&#25104;&#31163;&#32447;&#25968;&#25454;&#38598;&#12290;&#22914;&#26524;&#19987;&#23478;&#36275;&#22815;&#33021;&#24178;&#65292;&#21017;&#20854;&#32047;&#31215;&#36125;&#21494;&#26031;&#36951;&#25022;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#22823;&#23567; N &#19979;&#20250;&#25351;&#25968;&#24555;&#36895;&#19979;&#38477;&#21040;&#38646;&#12290;&#30001;&#20110;&#35813;&#31639;&#27861;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#36807;&#39640;&#65292;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#20102; iRLSVI &#31639;&#27861;&#65292;&#21487;&#30475;&#20316;&#26159;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#30340; RLSVI &#31639;&#27861;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20004;&#20010;&#22522;&#20934;&#65288;&#27809;&#26377;&#31163;&#32447;&#25968;&#25454;&#65292;&#25110;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#20294;&#19981;&#21033;&#29992;&#29983;&#25104;&#31574;&#30053;&#20449;&#24687;&#65289;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340; iRLSVI &#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#36951;&#25022;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26725;&#25509;&#20102;&#22312;&#32447; RL &#21644;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the following problem: Given an offline demonstration dataset from an imperfect expert, what is the best way to leverage it to bootstrap online learning performance in MDPs. We first propose an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline dataset, and information about the expert's behavioral policy used to generate the offline dataset. Its cumulative Bayesian regret goes down to zero exponentially fast in N, the offline dataset size if the expert is competent enough. Since this algorithm is computationally impractical, we then propose the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm for online RL, and imitation learning. Our empirical results show that the proposed iRLSVI algorithm is able to achieve significant reduction in regret as compared to two baselines: no offline data, and offline dataset but used without information about the generative policy. Our algorithm bridges online RL and imitation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39640;&#32500;Transformer&#30340;&#26550;&#26500;HDformer&#65292;&#24182;&#21033;&#29992;&#38271;&#36317;&#31163;PPG&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;TSA&#65292;&#25104;&#21151;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.11340</link><description>&lt;p&gt;
HDformer: &#19968;&#31181;&#21033;&#29992;&#38271;&#36317;&#31163;&#34880;&#31649;&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#30340;&#39640;&#32500;Transformer
&lt;/p&gt;
&lt;p&gt;
HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals. (arXiv:2303.11340v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39640;&#32500;Transformer&#30340;&#26550;&#26500;HDformer&#65292;&#24182;&#21033;&#29992;&#38271;&#36317;&#31163;PPG&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;TSA&#65292;&#25104;&#21151;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#24615;&#38382;&#39064;&#65292;&#26089;&#26399;&#26816;&#27979;&#26377;&#21161;&#20110;&#39044;&#38450;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#24050;&#20986;&#29616;&#23558;&#24515;&#34880;&#31649;&#20449;&#21495;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24335;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#38480;&#21046;&#20854;&#20020;&#24202;&#24212;&#29992;&#30340;&#26159;&#26377;&#38480;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#21363;Higher Dimensional Transformer&#65288;HDformer&#65289;&#65292;&#23427;&#21033;&#29992;&#38271;&#36317;&#31163;&#20809;&#30005;&#23481;&#31215;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#26469;&#26816;&#27979;&#31958;&#23615;&#30149;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30740;&#31350;&#24120;&#29992;&#30340;&#19981;&#36275;&#19968;&#20998;&#38047;&#30340;PPG&#20449;&#21495;&#65292;&#38271;&#36317;&#31163;PPG&#21253;&#21547;&#26356;&#24191;&#27867;&#12289;&#26356;&#28145;&#20837;&#30340;&#20449;&#21495;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#22686;&#21152;&#22788;&#29702;&#38271;&#36317;&#31163;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;Time Square Attention&#65288;TSA&#65289;&#65292;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#21516;&#26102;&#20445;&#30041;&#26412;&#22320;/&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#23558;&#19968;&#32500;&#36755;&#20837; &#36716;&#25442;&#20026;&#20108;&#32500;&#34920;&#31034;&#65292;&#24182;&#23558;&#30456;&#37051;&#28857;&#32452;&#25104;&#19968;&#20010;&#21333;&#29420;&#30340;2D&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetes mellitus is a worldwide concern, and early detection can help to prevent serious complications. Low-cost, non-invasive detection methods, which take cardiovascular signals into deep learning models, have emerged. However, limited accuracy constrains their clinical usage. In this paper, we present a new Transformer-based architecture, Higher Dimensional Transformer (HDformer), which takes long-range photoplethysmography (PPG) signals to detect diabetes. The long-range PPG contains broader and deeper signal contextual information compared to the less-than-one-minute PPG signals commonly utilized in existing research. To increase the capability and efficiency of processing the long range data, we propose a new attention module Time Square Attention (TSA), reducing the volume of the tokens by more than 10x, while retaining the local/global dependencies. It converts the 1-dimensional inputs into 2-dimensional representations and groups adjacent points into a single 2D token, using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; FedMAE&#65292;&#21487;&#20197;&#21033;&#29992;&#36731;&#37327;&#32423;&#35774;&#22791;&#19978;&#30340;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#22270;&#20687;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;FedMAE&#21487;&#20197;&#39044;&#35757;&#32451;&#19968;&#20010;&#21333;&#22359;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#23558;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#21333;&#22359;MAE&#32423;&#32852;&#22312;&#26381;&#21153;&#22120;&#19978;&#26500;&#24314;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22810;&#22359;ViT&#39592;&#24178;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedMAE&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;FSSL&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11339</link><description>&lt;p&gt;
FedMAE&#65306;&#24102;&#26377;&#21333;&#22359;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#30340;&#32852;&#37030;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMAE: Federated Self-Supervised Learning with One-Block Masked Auto-Encoder. (arXiv:2303.11339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32852;&#37030;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; FedMAE&#65292;&#21487;&#20197;&#21033;&#29992;&#36731;&#37327;&#32423;&#35774;&#22791;&#19978;&#30340;&#22823;&#35268;&#27169;&#26410;&#26631;&#35760;&#22270;&#20687;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#12290;FedMAE&#21487;&#20197;&#39044;&#35757;&#32451;&#19968;&#20010;&#21333;&#22359;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#23558;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#21333;&#22359;MAE&#32423;&#32852;&#22312;&#26381;&#21153;&#22120;&#19978;&#26500;&#24314;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22810;&#22359;ViT&#39592;&#24178;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedMAE&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;FSSL&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#24320;&#22987;&#20851;&#27880;&#22914;&#20309;&#21033;&#29992;&#29992;&#25143;&#35774;&#22791;&#20013;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21407;&#22240;&#26159;&#29992;&#25143;&#20851;&#27880;&#38544;&#31169;&#65292;&#25104;&#26412;&#39640;&#65292;&#25110;&#32773;&#32570;&#20047;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;Federated Semi-Supervised/Self-Supervised Learning&#65288;FSSL&#65289;&#26041;&#27861;&#30001;&#20110;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#32780;&#26080;&#27861;&#23398;&#20064;&#22823;&#35268;&#27169;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;FedMAE&#65292;&#21363;Federated Masked AutoEncoder&#65292;&#20197;&#35299;&#20915;&#22914;&#20309;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22823;&#23610;&#24230;&#22270;&#20687;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;FedMAE&#21487;&#20197;&#20351;&#29992;&#36731;&#37327;&#32423;&#23458;&#25143;&#31471;&#35774;&#22791;&#20013;&#30340;&#22823;&#22411;&#22270;&#20687;&#39044;&#35757;&#32451;&#21333;&#22359;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#65292;&#28982;&#21518;&#22312;&#26381;&#21153;&#22120;&#20013;&#32423;&#32852;&#22810;&#20010;&#39044;&#35757;&#32451;&#30340;&#21333;&#22359;MAE&#20197;&#26500;&#24314;&#19979;&#28216;&#20219;&#21153;&#30340;&#22810;&#22359;ViT&#39592;&#24178;&#12290; &#22270;&#20687;&#37325;&#24314;&#21644;&#20998;&#31867;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;FSSL&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;FedMAE&#33719;&#24471;&#20102;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latest federated learning (FL) methods started to focus on how to use unlabeled data in clients for training due to users' privacy concerns, high labeling costs, or lack of expertise. However, current Federated Semi-Supervised/Self-Supervised Learning (FSSL) approaches fail to learn large-scale images because of the limited computing resources of local clients. In this paper, we introduce a new framework FedMAE, which stands for Federated Masked AutoEncoder, to address the problem of how to utilize unlabeled large-scale images for FL. Specifically, FedMAE can pre-train one-block Masked AutoEncoder (MAE) using large images in lightweight client devices, and then cascades multiple pre-trained one-block MAEs in the server to build a multi-block ViT backbone for downstream tasks. Theoretical analysis and experimental results on image reconstruction and classification show that our FedMAE achieves superior performance compared to the state-of-the-art FSSL methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#27867;&#21270;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#19987;&#38376;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20013;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;DGNet-Bio&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;DGNet-Bio&#22312;ECG&#21644;EEG&#20998;&#31867;&#39046;&#22495;&#27867;&#21270;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11338</link><description>&lt;p&gt;
&#38754;&#21521;&#24515;&#30005;&#22270;&#21644;&#33041;&#30005;&#22270;&#20998;&#31867;&#30340;&#39046;&#22495;&#27867;&#21270;&#65306;&#31639;&#27861;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks. (arXiv:2303.11338v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#27867;&#21270;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#19987;&#38376;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20013;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;DGNet-Bio&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;DGNet-Bio&#22312;ECG&#21644;EEG&#20998;&#31867;&#39046;&#22495;&#27867;&#21270;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#33021;&#22815;&#22312;&#21307;&#30103;&#20445;&#20581;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#29282;&#22266;&#22320;&#30830;&#31435;&#33258;&#24049;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#65292;&#24403;&#27169;&#22411;&#38754;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#23601;&#26159;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#35780;&#20272;DG&#31639;&#27861;&#30340;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20013;&#30340;DG&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#29983;&#29289;&#20449;&#21495;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#65292;&#24182;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#24320;&#28304;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#27867;&#21270;&#35780;&#20272;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;DG&#31639;&#27861;&#25913;&#36827;&#20026;1D&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;DGNet-Bio&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20013;&#30340;DG&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DGNet-Bio&#22312;&#26032;&#25552;&#20986;&#30340;ECG&#21644;EEG&#20998;&#31867;&#39046;&#22495;&#27867;&#21270;&#22522;&#20934;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their immense success in numerous fields, machine and deep learning systems have not have not yet been able to firmly establish themselves in mission-critical applications in healthcare. One of the main reasons lies in the fact that when models are presented with previously unseen, Out-of-Distribution samples, their performance deteriorates significantly. This is known as the Domain Generalization (DG) problem. Our objective in this work is to propose a benchmark for evaluating DG algorithms, in addition to introducing a novel architecture for tackling DG in biosignal classification. In this paper, we describe the Domain Generalization problem for biosignals, focusing on electrocardiograms (ECG) and electroencephalograms (EEG) and propose and implement an open-source biosignal DG evaluation benchmark. Furthermore, we adapt state-of-the-art DG algorithms from computer vision to the problem of 1D biosignal classification and evaluate their effectiveness. Finally, we also introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#35745;&#31639;&#30340;&#40065;&#26834;&#32858;&#21512;&#26041;&#27861;&#26469;&#38450;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#25915;&#20987;&#65292;&#35813;&#26041;&#27861;&#20998;&#37197;&#26435;&#37325;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#25928;&#24212;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#31934;&#24230;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#24182;&#19988;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2303.11337</link><description>&lt;p&gt;
&#36882;&#24402;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#22522;&#20110;&#40065;&#26834;&#32858;&#21512;&#25216;&#26415;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recursive Euclidean Distance Based Robust Aggregation Technique For Federated Learning. (arXiv:2303.11337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#35745;&#31639;&#30340;&#40065;&#26834;&#32858;&#21512;&#26041;&#27861;&#26469;&#38450;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#25915;&#20987;&#65292;&#35813;&#26041;&#27861;&#20998;&#37197;&#26435;&#37325;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#25928;&#24212;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#31934;&#24230;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#24182;&#19988;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#38544;&#31169;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#30340;&#32858;&#21512;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#65292;&#22914;&#32972;&#21253;&#25915;&#20987;&#12289;&#26631;&#31614;&#32763;&#36716;&#21644;&#25104;&#21592;&#25512;&#26029;&#12290;&#24694;&#24847;&#29992;&#25143;&#26088;&#22312;&#36890;&#36807;&#24694;&#24847;&#25968;&#25454;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#26469;&#30772;&#22351;&#21512;&#20316;&#23398;&#20064;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#35745;&#31639;&#30340;&#26032;&#22411;&#40065;&#26834;&#32858;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27979;&#37327;&#26412;&#22320;&#27169;&#22411;&#19982;&#20043;&#21069;&#20840;&#23616;&#27169;&#22411;&#30340;&#36317;&#31163;&#65292;&#24182;&#30456;&#24212;&#22320;&#20998;&#37197;&#26435;&#37325;&#12290;&#36828;&#31163;&#20840;&#23616;&#27169;&#22411;&#30340;&#26412;&#22320;&#27169;&#22411;&#34987;&#20998;&#37197;&#36739;&#23567;&#30340;&#26435;&#37325;&#65292;&#20197;&#26368;&#23567;&#21270;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#31934;&#24230;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#19988;&#26102;&#38388;&#22797;&#26434;&#24230;&#20943;&#23569;&#19981;&#36229;&#36807;$55\%$&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#65306;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#32858;&#21512;&#25216;&#26415;&#65292;&#20197;&#38450;&#33539;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24694;&#24847;&#25915;&#20987;&#12290;2&#65289;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#31934;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity as a solution to data availability and privacy challenges in machine learning. However, the aggregation process of local model updates to obtain a global model in federated learning is susceptible to malicious attacks, such as backdoor poisoning, label-flipping, and membership inference. Malicious users aim to sabotage the collaborative learning process by training the local model with malicious data. In this paper, we propose a novel robust aggregation approach based on recursive Euclidean distance calculation. Our approach measures the distance of the local models from the previous global model and assigns weights accordingly. Local models far away from the global model are assigned smaller weights to minimize the data poisoning effect during aggregation. Our experiments demonstrate that the proposed algorithm outperforms state-of-the-art algorithms by at least $5\%$ in accuracy while reducing time complexity by less than $55\%$. Our contribut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992; Integrated Gradients &#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#29305;&#24449;&#24402;&#23646;&#26102;&#30340;&#21487;&#35299;&#37322;&#24615;&#26497;&#38480;&#65292;&#35777;&#26126;&#20165;&#36890;&#36807;&#29305;&#24449;&#24433;&#21709;&#20540;&#30340;&#25490;&#24207;&#26080;&#27861;&#21487;&#38752;&#22320;&#35782;&#21035;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#37325;&#35201;&#24615;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#20197;&#35780;&#20272;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11336</link><description>&lt;p&gt;
&#30740;&#31350;&#29992;&#20110;&#22522;&#22240;&#34920;&#36798;&#27169;&#22411;&#30340; Integrated Gradients &#30340;&#21487;&#35299;&#37322;&#24615;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Studying Limits of Explainability by Integrated Gradients for Gene Expression Models. (arXiv:2303.11336v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992; Integrated Gradients &#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#29305;&#24449;&#24402;&#23646;&#26102;&#30340;&#21487;&#35299;&#37322;&#24615;&#26497;&#38480;&#65292;&#35777;&#26126;&#20165;&#36890;&#36807;&#29305;&#24449;&#24433;&#21709;&#20540;&#30340;&#25490;&#24207;&#26080;&#27861;&#21487;&#38752;&#22320;&#35782;&#21035;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#37325;&#35201;&#24615;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#20197;&#35780;&#20272;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#39537;&#21160;&#32454;&#32990;&#29983;&#21629;&#21608;&#26399;&#30340;&#20998;&#23376;&#36807;&#31243;&#26159;&#29983;&#29289;&#23398;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#35299;&#23494;&#22797;&#26434;&#30340;&#32454;&#32990;&#30456;&#20114;&#20316;&#29992;&#65292;&#23558;&#31185;&#23398;&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#26631;&#31614;&#25968;&#25454;&#25110;&#22270;&#24418;&#19978;&#30340;&#32463;&#20856;&#23398;&#20064;&#38382;&#39064;&#65292;&#20363;&#22914;&#26469;&#33258;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#30340;&#34920;&#22411;&#39044;&#27979;&#12290;&#22312;&#36825;&#20123;&#24037;&#20316;&#20013;&#65292;&#20010;&#20307;&#39044;&#27979;&#30340;&#36755;&#20837;&#29305;&#24449;&#32463;&#24120;&#34987;&#35299;&#37322;&#20026;&#34920;&#22411;&#25104;&#22240;&#30340;&#25351;&#31034;&#24615;&#26631;&#24535;&#65292;&#20363;&#22914;&#30284;&#30151;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#25506;&#35752; Integrated Gradients &#22312;&#26426;&#22120;&#23398;&#20064;&#29305;&#24449;&#24402;&#23646;&#20013;&#25152;&#37492;&#21035;&#21040;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#22312;&#30284;&#30151;&#22522;&#22240;&#32452;&#22270;&#35889;&#19978;&#30340;&#19968;&#20010;&#20196;&#20154;&#28608;&#21160;&#30340;&#26696;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#36890;&#36807;&#29305;&#24449;&#24433;&#21709;&#20540;&#30340;&#25490;&#24207;&#24182;&#19981;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#38590;&#20197;&#35780;&#20272;&#29305;&#24449;&#24402;&#23646;&#30340;&#27491;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#30340;&#26497;&#38480;&#65292;&#21363;&#22312;&#32473;&#23450;&#26041;&#27861;&#30340;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#25105;&#20204;&#21487;&#20197;&#21487;&#38752;&#22320;&#24471;&#20986;&#22810;&#23569;&#26377;&#20851;&#29983;&#29289;&#26631;&#24535;&#29289;&#30456;&#20851;&#24615;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the molecular processes that drive cellular life is a fundamental question in biological research. Ambitious programs have gathered a number of molecular datasets on large populations. To decipher the complex cellular interactions, recent work has turned to supervised machine learning methods. The scientific questions are formulated as classical learning problems on tabular data or on graphs, e.g. phenotype prediction from gene expression data. In these works, the input features on which the individual predictions are predominantly based are often interpreted as indicative of the cause of the phenotype, such as cancer identification. Here, we propose to explore the relevance of the biomarkers identified by Integrated Gradients, an explainability method for feature attribution in machine learning. Through a motivating example on The Cancer Genome Atlas, we show that ranking features by importance is not enough to robustly identify biomarkers. As it is difficult to evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;INN&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#29942;&#39048;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#32463;&#20856;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.11239</link><description>&lt;p&gt;
&#23558;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Training Invertible Neural Networks as Autoencoders. (arXiv:2303.11239v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;INN&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#29942;&#39048;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#32463;&#20856;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#22120;&#33021;&#22815;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#26377;&#29992;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#20316;&#20026;&#65288;&#21464;&#20998;&#65289;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#31216;&#20026;INN&#65288;&#21464;&#20998;&#65289;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;MNIST&#65292;CIFAR&#21644;CelebA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#29942;&#39048;&#22823;&#23567;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;INN&#33258;&#21160;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#19982;&#32463;&#20856;&#33258;&#21160;&#32534;&#30721;&#22120;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#29942;&#39048;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;INN&#33258;&#21160;&#32534;&#30721;&#22120;&#20248;&#20110;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#12290;&#22522;&#20110;&#23454;&#35777;&#32467;&#26524;&#65292;&#25105;&#20204;&#20551;&#35774;INN&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#33021;&#27809;&#26377;&#20219;&#20309;&#22266;&#26377;&#20449;&#24687;&#25439;&#22833;&#65292;&#22240;&#27492;&#19981;&#21463;&#26368;&#22823;&#23618;&#25968;&#65288;&#28145;&#24230;&#65289;&#30340;&#38480;&#21046;&#65292;&#36798;&#21040;&#35813;&#23618;&#25968;&#21518;&#21482;&#33021;&#23454;&#29616;&#27425;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders are able to learn useful data representations in an unsupervised matter and have been widely used in various machine learning and computer vision tasks. In this work, we present methods to train Invertible Neural Networks (INNs) as (variational) autoencoders which we call INN (variational) autoencoders. Our experiments on MNIST, CIFAR and CelebA show that for low bottleneck sizes our INN autoencoder achieves results similar to the classical autoencoder. However, for large bottleneck sizes our INN autoencoder outperforms its classical counterpart. Based on the empirical results, we hypothesize that INN autoencoders might not have any intrinsic information loss and thereby are not bounded to a maximal number of layers (depth) after which only suboptimal results can be achieved.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.10880</link><description>&lt;p&gt;
&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;: &#36890;&#36807;&#35302;&#35273;&#23454;&#29616;&#25163;&#37096;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rotating without Seeing: Towards In-hand Dexterity through Touch. (arXiv:2303.10880v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#24863;&#20449;&#24687;&#22312;&#20154;&#31867;&#28789;&#24039;&#24615;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#30340;&#25509;&#35302;&#20449;&#24687;&#65292;&#30452;&#25509;&#20174;&#35270;&#35273;&#20013;&#26080;&#27861;&#25512;&#26029;&#12290;&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#20351;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#20855;&#22791;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#20351;&#29992;&#35206;&#30422;&#25972;&#20010;&#26426;&#22120;&#20154;&#25163;&#30340;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#65288;&#35302;&#25720;&#25110;&#26410;&#35302;&#25720;&#65289;&#20195;&#26367;&#20165;&#20165;&#22312;&#23567;&#21306;&#22495;&#20869;&#36827;&#34892;&#31934;&#20934;&#30340;&#35302;&#35273;&#20256;&#24863;&#65292;&#20351;&#31995;&#32479;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#35206;&#30422;&#33539;&#22260;&#24191;&#31561;&#20248;&#28857;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#26679;&#30340;&#29289;&#20307;&#27169;&#25311;&#20013;&#35757;&#32451;&#20986;&#20102;&#19968;&#31181;&#35302;&#24863;&#26059;&#36716;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25163;&#19978;&#30452;&#25509;&#23454;&#26045;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#26032;&#22411;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we present Touch Dexterity, a new system that can perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#12289;&#20248;&#21270;&#12289;&#25928;&#29575;&#21644;&#31934;&#24230;&#30340;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#26368;&#21069;&#27839;&#30340;&#20248;&#21270;&#12289;&#33410;&#33021;&#21644;&#35780;&#20215;&#26041;&#27861;&#65292;&#26041;&#20415;&#26032;&#20174;&#19994;&#32773;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.10780</link><description>&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;&#65306;&#35299;&#37322;&#12289;&#20248;&#21270;&#12289;&#25928;&#29575;&#21644;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efficiency, and Best Practices. (arXiv:2303.10780v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#12289;&#20248;&#21270;&#12289;&#25928;&#29575;&#21644;&#31934;&#24230;&#30340;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#26368;&#21069;&#27839;&#30340;&#20248;&#21270;&#12289;&#33410;&#33021;&#21644;&#35780;&#20215;&#26041;&#27861;&#65292;&#26041;&#20415;&#26032;&#20174;&#19994;&#32773;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#25345;&#32493;&#28608;&#21457;&#30528;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#34987;&#20302;&#20272;&#21644;&#26410;&#32463;&#35843;&#26597;&#30340;&#31070;&#32463;&#35745;&#31639;&#39046;&#22495;&#26159;&#31526;&#21512;&#29983;&#29289;&#23398;&#12289;&#39640;&#25928;&#33021;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#20204;&#30340;&#28508;&#21147;&#22312;&#23588;&#20854;&#36866;&#21512;&#20302;&#21151;&#32791;&#12289;&#31227;&#21160;&#25110;&#20854;&#20182;&#30828;&#20214;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#12289;&#20248;&#21270;&#12289;&#25928;&#29575;&#21644;&#31934;&#24230;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#35782;&#21035;&#12289;&#35752;&#35770;&#21644;&#27604;&#36739;&#26368;&#21069;&#27839;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#12289;&#33410;&#33021;&#21644;&#35780;&#20215;&#26041;&#27861;&#65292;&#20174;&#22522;&#26412;&#21407;&#29702;&#20986;&#21457;&#65292;&#26041;&#20415;&#26032;&#20174;&#19994;&#32773;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networks. Key contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitioners.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20957;&#32858;&#27010;&#24565;&#65292;&#26500;&#24314;&#22312;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#32467;&#26524;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#30340;&#31038;&#21306;&#21457;&#29616;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.10167</link><description>&lt;p&gt;
&#24191;&#20041;&#21010;&#20998;&#23616;&#37096;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
Generalized partitioned local depth. (arXiv:2303.10167v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20957;&#32858;&#27010;&#24565;&#65292;&#26500;&#24314;&#22312;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#32467;&#26524;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#30340;&#31038;&#21306;&#21457;&#29616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#36817;&#30001;Berenhaut&#12289;Moore&#21644;Melvin [Proccedings of the National Academy of Sciences, 119 (4) (2022)]&#25552;&#20986;&#30340;&#20957;&#32858;&#27010;&#24565;&#30340;&#27010;&#25324;&#12290;&#25152;&#25552;&#20986;&#30340;&#34920;&#36848;&#22522;&#20110;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#24182;&#25552;&#28860;&#20102;&#20004;&#20010;&#20851;&#38190;&#27010;&#29575;&#27010;&#24565;&#65306;&#23616;&#37096;&#30456;&#20851;&#24615;&#21644;&#25903;&#25345;&#20998;&#21106;&#12290;&#26089;&#26399;&#32467;&#26524;&#22312;&#26032;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#25193;&#23637;&#65292;&#24182;&#21253;&#25324;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#20013;&#25581;&#31034;&#31038;&#21306;&#30340;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide a generalization of the concept of cohesion as introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. Earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;GRACE&#27169;&#22411;&#21487;&#20197;&#20174;&#35013;&#37197;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#39044;&#27979;&#21487;&#34892;&#30340;&#35013;&#37197;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2303.10135</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#39640;&#25928;&#21487;&#34892;&#30340;&#26426;&#22120;&#20154;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning. (arXiv:2303.10135v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;GRACE&#27169;&#22411;&#21487;&#20197;&#20174;&#35013;&#37197;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#39044;&#27979;&#21487;&#34892;&#30340;&#35013;&#37197;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#20154;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#65288;RASP&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29616;&#20195;&#21046;&#36896;&#19994;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#24212;&#21147;&#65292;&#38543;&#30528;&#23545;&#26356;&#22823;&#37327;&#21270;&#29983;&#20135;&#38656;&#27714;&#30340;&#19981;&#26029;&#22686;&#38271;&#12290;&#23454;&#29616;&#36825;&#31181;&#33258;&#21160;&#21270;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#22312;&#20110;&#20174;&#19981;&#26029;&#22686;&#21152;&#30340;&#28508;&#22312;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#36827;&#34892;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#35013;&#37197;&#36824;&#38656;&#35201;&#25104;&#26412;&#26114;&#36149;&#30340;&#21487;&#34892;&#24615;&#26816;&#26597;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#25324;&#20135;&#21697;&#35013;&#37197;&#22270;&#30340;&#22270;&#24418;&#26041;&#27861;&#21644;&#19968;&#20010;&#21517;&#20026;GRACE&#30340;&#31574;&#30053;&#26550;&#26500;&#65292;&#29992;&#20110;&#35013;&#37197;&#24207;&#21015;&#29983;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;GRACE&#20174;&#22270;&#24418;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#24182;&#36880;&#27493;&#39044;&#27979;&#35013;&#37197;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#22312;&#27169;&#25311;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#39044;&#27979;&#38109;&#22411;&#26448;&#20135;&#21697;&#21464;&#20307;&#30340;&#21487;&#34892;&#35013;&#37197;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve productivity and resilience in modern manufacturing along with the growing need for greater product customization. One of the main challenges in realizing such automation resides in efficiently finding solutions from a growing number of potential sequences for increasingly complex assemblies. Besides, costly feasibility checks are always required for the robotic system. To address this, we propose a holistic graphical approach including a graph representation called Assembly Graph for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE for assembly sequence generation. Secondly, we use GRACE to extract meaningful information from the graph input and predict assembly sequences in a step-by-step manner. In experiments, we show that our approach can predict feasible assembly sequences across product variants of aluminum profiles based on data collected in simulation of a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09280</link><description>&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#65306;&#24212;&#29992;&#20110;&#38544;&#34255;&#20960;&#20309;&#32467;&#26500;&#30340;&#38750;&#20405;&#20837;&#24335;&#25506;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries. (arXiv:2303.09280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#30005;&#30913;&#12289;&#22768;&#23398;&#25110;&#26426;&#26800;&#36127;&#36733;&#20174;&#34920;&#38754;&#27979;&#37327;&#20013;&#26816;&#27979;&#38544;&#34255;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#38750;&#20405;&#20837;&#25104;&#20687;&#25216;&#26415;&#30340;&#30446;&#26631;&#12290;&#30001;&#20110;&#26410;&#30693;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#24418;&#29366;&#12289;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#20197;&#21450;&#29289;&#29702;&#35268;&#24459;&#30340;&#22797;&#26434;&#24615;&#65292;&#35299;&#20915;&#36870;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34920;&#29616;&#20986;&#35768;&#22810;&#20248;&#28857;&#65292;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#38382;&#39064;&#21453;&#28436;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#20855;&#26377;&#20808;&#39564;&#26410;&#30693;&#25299;&#25169;&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PINNs&#30340;&#25299;&#25169;&#20248;&#21270;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#27809;&#26377;&#24418;&#29366;&#25968;&#37327;&#25110;&#31867;&#22411;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#20801;&#35768;&#20219;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#36890;&#36807;&#20351;&#29992;&#26448;&#26009;&#23494;&#24230;&#22330;&#26469;&#34920;&#31034;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;Eikonal&#27491;&#21017;&#21270;&#25509;&#36817;&#20108;&#36827;&#21046;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#27979;&#38544;&#21547;&#34394;&#31354;&#21644;&#21253;&#21547;&#29289;&#30340;&#25968;&#37327;&#12289;&#20301;&#32622;&#21644;&#24418;&#29366;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws. Physics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology. Here, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. We validate our framework by detecting the number, locations, and shapes of hidden voids and inclusio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09065</link><description>&lt;p&gt;
&#22522;&#20110;t-SPN&#21644;&#28388;&#27874;&#30340;&#32454;&#32990;&#20998;&#31867;&#30340;&#26368;&#22823;&#38388;&#38548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Margin Learning of t-SPNs for Cell Classification with Filtering. (arXiv:2303.09065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#27010;&#29575;&#20307;&#31995;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#26641;&#24418;&#27714;&#21644;&#20135;&#21697;&#32593;&#32476;(t-SPN)&#65292;&#29992;&#20110;&#32454;&#32990;&#20998;&#31867;&#12290;&#26500;&#24314;t-SPN&#30340;&#30446;&#30340;&#26159;&#34920;&#31034;&#26410;&#24402;&#19968;&#21270;&#27010;&#29575;&#20316;&#20026;&#26368;&#30456;&#20284;&#30340;&#32454;&#32990;&#31867;&#21035;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#26469;&#23398;&#20064;&#26500;&#24314;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#36793;&#32536;&#26159;&#30495;&#23454;&#26631;&#31614;&#21644;&#26368;&#26377;&#31454;&#20105;&#21147;&#30340;&#38169;&#35823;&#26631;&#31614;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#24046;&#12290;&#20026;&#20102;&#22686;&#24378;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;L2&#27491;&#21017;&#21270;&#65288;REG&#65289;&#21644;&#26368;&#22823;&#38388;&#38548;&#65288;MM&#65289;&#26631;&#20934;&#12290;&#20026;&#20102;&#31361;&#20986;&#32454;&#32990;&#29305;&#24449;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#30340;&#26377;&#25928;&#24615;&#65306;&#29702;&#24819;&#39640;&#36890;&#28388;&#27874;&#21644;&#25289;&#26222;&#25289;&#26031;&#28388;&#27874;(Log)&#12290;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#26368;&#22823;&#38388;&#38548;&#20934;&#21017;&#19982;&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm based on a deep probabilistic architecture referred to as a tree-structured sum-product network (t-SPN) is considered for cell classification. The t-SPN is constructed such that the unnormalized probability is represented as conditional probabilities of a subset of most similar cell classes. The constructed t-SPN architecture is learned by maximizing the margin, which is the difference in the conditional probability between the true and the most competitive false label. To enhance the generalization ability of the architecture, L2-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#21046;&#29305;&#24449;&#24037;&#31243;&#21644;&#24207;&#21015;&#23398;&#20064;&#22120;&#65292;&#25552;&#20986;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#39044;&#27979;&#30828;&#30424;&#21097;&#20313;&#23551;&#21629;&#65292;&#20174;&#32780;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;</title><link>http://arxiv.org/abs/2303.08955</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;&#30828;&#30424;&#23551;&#21629;&#22823;&#35268;&#27169;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large-scale End-of-Life Prediction of Hard Disks in Distributed Datacenters. (arXiv:2303.08955v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#21046;&#29305;&#24449;&#24037;&#31243;&#21644;&#24207;&#21015;&#23398;&#20064;&#22120;&#65292;&#25552;&#20986;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#39044;&#27979;&#30828;&#30424;&#21097;&#20313;&#23551;&#21629;&#65292;&#20174;&#32780;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#27599;&#22825;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#23384;&#20648;&#22312;&#20215;&#26684;&#20415;&#23452;&#30340;&#30828;&#30424;&#20013;&#65292;&#29992;&#20110;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#33322;&#22825;&#31561;&#37325;&#35201;&#39046;&#22495;&#12290;&#30828;&#30424;&#30340;&#36807;&#26089;&#25439;&#22351;&#21644;&#25968;&#25454;&#30340;&#20002;&#22833;&#21487;&#33021;&#20250;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#21518;&#26524;&#12290;&#20026;&#20102;&#38477;&#20302;&#25925;&#38556;&#30340;&#39118;&#38505;&#65292;&#20113;&#23384;&#20648;&#25552;&#20379;&#21830;&#25191;&#34892;&#22522;&#20110;&#26465;&#20214;&#30340;&#30417;&#25511;&#24182;&#22312;&#25925;&#38556;&#20043;&#21069;&#26356;&#25442;&#30828;&#30424;&#12290;&#36890;&#36807;&#20272;&#35745;&#30828;&#30424;&#21097;&#20313;&#23551;&#21629;&#65292;&#21487;&#20197;&#39044;&#27979;&#29305;&#23450;&#35774;&#22791;&#30340;&#25925;&#38556;&#26102;&#38388;&#24182;&#22312;&#21512;&#36866;&#30340;&#26102;&#38388;&#20869;&#26367;&#25442;&#30828;&#30424;&#65292;&#30830;&#20445;&#26368;&#22823;&#21033;&#29992;&#29575;&#21516;&#26102;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#12290;&#26412;&#25991;&#20351;&#29992;&#23450;&#21046;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#19968;&#22871;&#24207;&#21015;&#23398;&#20064;&#22120;&#23545;&#26497;&#24230;&#20559;&#26012;&#30340;&#20581;&#24247;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#27979;&#20998;&#26512;&#12290;&#36807;&#21435;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20351;&#29992;LSTM&#39044;&#27979;&#21097;&#20313;&#23551;&#21629;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LSTM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On a daily basis, data centers process huge volumes of data backed by the proliferation of inexpensive hard disks. Data stored in these disks serve a range of critical functional needs from financial, and healthcare to aerospace. As such, premature disk failure and consequent loss of data can be catastrophic. To mitigate the risk of failures, cloud storage providers perform condition-based monitoring and replace hard disks before they fail. By estimating the remaining useful life of hard disk drives, one can predict the time-to-failure of a particular device and replace it at the right time, ensuring maximum utilization whilst reducing operational costs. In this work, large-scale predictive analyses are performed using severely skewed health statistics data by incorporating customized feature engineering and a suite of sequence learners. Past work suggests using LSTMs as an excellent approach to predicting remaining useful life. To this end, we present an encoder-decoder LSTM model whe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07543</link><description>&lt;p&gt;
WDiscOOD&#65306;&#36890;&#36807;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#21306;&#20998;&#24230;&#20248;&#21270;&#30340;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#22312;&#36935;&#21040;&#26410;&#30693;&#27010;&#24565;&#30340;&#24773;&#24418;&#19979;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#20010;&#25361;&#25112;&#31361;&#26174;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#31354;&#38388;OOD&#26816;&#27979;&#20998;&#25968;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#31867;&#21035;&#29305;&#23450;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#8212;&#8212;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#65292;&#20854;&#20013;ID&#31867;&#22312;&#21028;&#21035;&#23376;&#31354;&#38388;&#20013;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#65292;&#24182;&#22312;&#27531;&#24046;&#23376;&#31354;&#38388;&#20013;&#34987;&#32039;&#23494;&#22320;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#22312;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#23558;&#26469;&#33258;&#36755;&#20837;&#25968;&#25454;&#19982;ID&#20998;&#24067;&#30340;&#20559;&#24046;&#32452;&#21512;&#36215;&#26469;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WDiscOOD&#65292;&#22312;&#35206;&#30422;&#22810;&#31181;&#20998;&#24067;&#20559;&#31227;&#30340;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#12290;WDiscOOD&#22312;&#28145;&#24230;&#20998;&#31867;&#22120;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;</title><link>http://arxiv.org/abs/2303.07397</link><description>&lt;p&gt;
&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#36825;&#31181;&#22330;&#26223;&#65306;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#25191;&#34892;&#25805;&#20316;&#20174;&#19968;&#20010;&#33410;&#28857;&#21040;&#21478;&#19968;&#20010;&#33410;&#28857;&#26469;&#23548;&#33322;&#28508;&#22312;&#22270;&#12290;&#25152;&#36873;&#25805;&#20316;&#30830;&#23450;&#20102;&#19979;&#19968;&#20010;&#35775;&#38382;&#33410;&#28857;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#27599;&#20010;&#33410;&#28857;&#22788;&#65292;&#26234;&#33021;&#20307;&#25910;&#21040;&#19968;&#20010;&#35266;&#27979;&#65292;&#20294;&#35813;&#35266;&#27979;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#22240;&#27492;&#23427;&#19981;&#33021;&#21807;&#19968;&#22320;&#26631;&#35782;&#33410;&#28857;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#21035;&#21517;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25919;&#31574;&#65292;&#35813;&#25919;&#31574;&#32422;&#31561;&#20110;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#65288;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#32034;&#39044;&#31639;&#19979;&#22914;&#20309;&#24674;&#22797;&#22270;&#34920;&#65289;&#12290;&#22312;&#38750;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;&#23545;&#20110;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#36866;&#29992;&#30340;&#22522;&#32447;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30456;&#23545;&#20110;&#38543;&#26426;&#31574;&#30053;&#26356;&#24555;&#30340;&#24674;&#22797;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24674;&#22797;&#36895;&#24230;&#27604;&#38543;&#26426;&#31574;&#30053;&#24555;&#25351;&#25968;&#20493;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#31216;&#20026; eFeX&#65288;&#26469;&#33258;&#20110; efficient exploration &#30340;&#32553;&#20889;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#20026;&#38271;&#23614;&#20998;&#31867;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#25903;&#25345;&#65292;&#24182;&#37319;&#29992;&#32508;&#21512;&#39118;&#38505;&#21644;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#20197;&#25552;&#39640;&#25152;&#26377;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#8220;&#38271;&#23614;&#8221;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.06075</link><description>&lt;p&gt;
&#20174;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#35282;&#24230;&#30475;&#24453;&#38271;&#23614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Long-tailed Classification from a Bayesian-decision-theory Perspective. (arXiv:2303.06075v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#20026;&#38271;&#23614;&#20998;&#31867;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#25903;&#25345;&#65292;&#24182;&#37319;&#29992;&#32508;&#21512;&#39118;&#38505;&#21644;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#20197;&#25552;&#39640;&#25152;&#26377;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#8220;&#38271;&#23614;&#8221;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#30001;&#20110;&#31867;&#21035;&#27010;&#29575;&#30340;&#20005;&#37325;&#19981;&#24179;&#34913;&#21644;&#23545;&#31216;&#38169;&#35823;&#39044;&#27979;&#25104;&#26412;&#23384;&#22312;&#23614;&#37096;&#25935;&#24863;&#39118;&#38505;&#32780;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#23581;&#35797;&#37319;&#29992;&#37325;&#26032;&#24179;&#34913;&#25439;&#22833;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#32463;&#39564;&#32467;&#26524;&#65292;&#32570;&#20047;&#29702;&#35770;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#20915;&#31574;&#25439;&#22833;&#65292;&#23427;&#21051;&#30011;&#20102;&#19982;&#23614;&#37096;&#31867;&#21035;&#30456;&#20851;&#30340;&#19981;&#21516;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#35282;&#24230;&#30475;&#24453;&#38271;&#23614;&#20998;&#31867;&#30340;&#36890;&#29992;&#19988;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#23427;&#32479;&#19968;&#20102;&#21253;&#25324;&#37325;&#26032;&#24179;&#34913;&#21644;&#38598;&#25104;&#26041;&#27861;&#22312;&#20869;&#30340;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#20026;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#30475;&#65292;&#25105;&#20204;&#22522;&#20110;&#32508;&#21512;&#39118;&#38505;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#21644;&#19968;&#20010;&#36125;&#21494;&#26031;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#25152;&#26377;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#8220;&#38271;&#23614;&#8221;&#31867;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20219;&#21153;&#33258;&#36866;&#24212;&#20915;&#31574;&#25439;&#22833;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-tailed classification poses a challenge due to its heavy imbalance in class probabilities and tail-sensitivity risks with asymmetric misprediction costs. Recent attempts have used re-balancing loss and ensemble methods, but they are largely heuristic and depend heavily on empirical results, lacking theoretical explanation. Furthermore, existing methods overlook the decision loss, which characterizes different costs associated with tailed classes. This paper presents a general and principled framework from a Bayesian-decision-theory perspective, which unifies existing techniques including re-balancing and ensemble methods, and provides theoretical justifications for their effectiveness. From this perspective, we derive a novel objective based on the integrated risk and a Bayesian deep-ensemble approach to improve the accuracy of all classes, especially the "tail". Besides, our framework allows for task-adaptive decision loss which provides provably optimal decisions in varying task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05382</link><description>&lt;p&gt;
ChatGPT&#24050;&#22312;&#22320;&#24179;&#32447;&#19978;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23601;&#26159;&#25105;&#20204;&#38656;&#35201;&#30340;&#26234;&#33021;&#20132;&#36890;&#35299;&#20915;&#26041;&#26696;&#65311;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. (arXiv:2303.05382v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#35299;&#20915;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#23637;&#31034;&#20102;&#36825;&#31181;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;&#20855;&#26377;60&#20159;&#21442;&#25968;&#30340;&#37325;&#35201;&#22823;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#12290;ChatGPT&#23637;&#31034;&#20102;LLM&#30340;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#23545;&#35805;&#21709;&#24212;&#26041;&#38754;&#12290;&#38543;&#30528;LLM&#22312;&#21508;&#31181;&#30740;&#31350;&#25110;&#24037;&#31243;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#29616;&#22312;&#26159;&#26102;&#20505;&#35774;&#24819;LLM&#22914;&#20309;&#38761;&#26032;&#25105;&#20204;&#22788;&#29702;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#26041;&#24335;&#20102;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#20915;&#20851;&#38190;&#20132;&#36890;&#38382;&#39064;&#26041;&#38754;&#30340;&#26410;&#26469;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;LLM&#65292;&#26234;&#33021;&#31995;&#32479;&#36824;&#21487;&#20197;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#20132;&#36890;&#25968;&#25454;&#24182;&#36890;&#36807;LLM&#25191;&#34892;&#20132;&#36890;&#36816;&#33829;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;LLM&#35013;&#22791;&#30340;&#36825;&#20123;&#28508;&#22312;&#30340;&#20132;&#36890;&#24212;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35777;&#26126;&#36825;&#31181;&#28508;&#21147;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#30896;&#25758;&#25253;&#21578;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#26694;&#26550;&#20316;&#20026;&#29992;&#20363;&#12290;&#23613;&#31649;&#23384;&#22312;&#28508;&#22312;&#30340;&#30410;&#22788;&#65292;&#20294;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, developed by OpenAI, is one of the milestone large language models (LLMs) with 6 billion parameters. ChatGPT has demonstrated the impressive language understanding capability of LLM, particularly in generating conversational response. As LLMs start to gain more attention in various research or engineering domains, it is time to envision how LLM may revolutionize the way we approach intelligent transportation systems. This paper explores the future applications of LLM in addressing key transportation problems. By leveraging LLM with cross-modal encoder, an intelligent system can also process traffic data from different modalities and execute transportation operations through an LLM. We present and validate these potential transportation applications equipped by LLM. To further demonstrate this potential, we also provide a concrete smartphone-based crash report auto-generation and analysis framework as a use case. Despite the potential benefits, challenges related to data privac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22914;&#20309;&#22266;&#26377;&#22320;&#25506;&#32034;&#34892;&#21160;&#31354;&#38388;&#65292;&#24182;&#39318;&#27425;&#30830;&#23450;&#20102;&#19981;&#38656;&#35201;&#25506;&#32034;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#34892;&#21160;&#31354;&#38388;&#25506;&#32034;&#24182;&#19981;&#19968;&#23450;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2303.04386</link><description>&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#22266;&#26377;&#22320;&#25506;&#32034;&#34892;&#21160;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent Inherently Explores Action Space. (arXiv:2303.04386v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22914;&#20309;&#22266;&#26377;&#22320;&#25506;&#32034;&#34892;&#21160;&#31354;&#38388;&#65292;&#24182;&#39318;&#27425;&#30830;&#23450;&#20102;&#19981;&#38656;&#35201;&#25506;&#32034;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#34892;&#21160;&#31354;&#38388;&#25506;&#32034;&#24182;&#19981;&#19968;&#23450;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#27809;&#26377;&#21152;&#20837;&#25506;&#32034;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#26377;&#38480;&#29366;&#24577;&#21644;&#34892;&#21160;&#31354;&#38388;&#19979;&#30340;&#36890;&#29992;&#22686;&#24378;&#23398;&#20064;&#38382;&#39064;&#65292;&#21407;&#20197;&#20026;&#22312;&#34892;&#21160;&#31354;&#38388;&#20869;&#36827;&#34892;&#26174;&#24335;&#25506;&#32034;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#20197;&#36991;&#20813;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#21095;&#28872;&#38477;&#20302;&#12290;&#26412;&#25991;&#39318;&#27425;&#30830;&#23450;&#20102;&#22312;&#32447;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;$\tilde{\mathcal{O}}(1/\epsilon^2)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#31574;&#30053;&#35780;&#20272;&#31639;&#23376;&#21644;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#38543;&#26426;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#65288; SPMD&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SPMD&#36890;&#36807;&#20854;&#22312;&#32447;&#35780;&#20272;&#31639;&#23376;&#22266;&#26377;&#22320;&#25506;&#32034;&#34892;&#21160;&#31354;&#38388;&#65292;&#26126;&#30830;&#30340;&#34892;&#21160;&#31354;&#38388;&#25506;&#32034;&#24182;&#19981;&#19968;&#23450;&#26159;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#23454;&#29616;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#20854;&#20013;&#19968;&#20010;&#35780;&#20272;&#31639;&#23376;&#31216;&#20026;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#20272;&#35745;&#65292;&#21478;&#19968;&#20010;&#35780;&#20272;&#31639;&#23376;&#31216;&#20026;&#22522;&#20110;&#31574;&#30053;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explicit exploration in the action space was assumed to be indispensable for online policy gradient methods to avoid a drastic degradation in sample complexity, for solving general reinforcement learning problems over finite state and action spaces. In this paper, we establish for the first time an $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity for online policy gradient methods without incorporating any exploration strategies. The essential development consists of two new on-policy evaluation operators and a novel analysis of the stochastic policy mirror descent method (SPMD). SPMD with the first evaluation operator, called value-based estimation, tailors to the Kullback-Leibler divergence. Provided the Markov chains on the state space of generated policies are uniformly mixing with non-diminishing minimal visitation measure, an $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexity is obtained with a linear dependence on the size of the action space. SPMD with the second evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#26469;&#35299;&#20915;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01593</link><description>&lt;p&gt;
QAID&#65306;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QAID: Question Answering Inspired Few-shot Intent Detection. (arXiv:2303.01593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#30340;Few-shot&#24847;&#22270;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#26469;&#35299;&#20915;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#26816;&#27979;&#28041;&#21450;&#21040;&#19968;&#20123;&#35821;&#20041;&#30456;&#20284;&#30340;&#32454;&#31890;&#24230;&#24847;&#22270;&#65292;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#24847;&#22270;&#26816;&#27979;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20219;&#21153;&#65292;&#23558;&#35805;&#35821;&#21644;&#24847;&#22270;&#21517;&#20316;&#20026;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#38382;&#39064;-&#22238;&#31572;&#26816;&#32034;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#22521;&#35757;&#27169;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#25209;&#37327;&#23545;&#27604;&#25439;&#22833;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#22521;&#35757;&#26469;&#25913;&#21892;&#26597;&#35810;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#22312;&#24494;&#35843;&#38454;&#27573;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#26597;&#35810;&#21644;&#21516;&#19968;&#24847;&#22270;&#31572;&#26696;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#21270;&#20196;&#29260;&#32423;&#30456;&#20284;&#24230;&#20998;&#25968;&#12290;&#25105;&#20497;&#22312;&#19977;&#20010;few-shot&#24847;&#22270;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;&#26368;&#20248;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#12290;&#27492;&#26041;&#27861;&#24050;&#24212;&#29992;&#22312;Concrete-ML&#24320;&#28304;&#24211;&#20013;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.01254</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#26641;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Tree-Based Inference with Fully Homomorphic Encryption. (arXiv:2303.01254v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#65292;&#24182;&#24471;&#21040;&#20102;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#12290;&#27492;&#26041;&#27861;&#24050;&#24212;&#29992;&#22312;Concrete-ML&#24320;&#28304;&#24211;&#20013;&#65292;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;(PETs)&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#21516;&#26102;&#20801;&#35768;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#8212;&#8212;&#20840;&#21516;&#24577;&#21152;&#23494;(FHE)&#65292;&#23427;&#20801;&#35768;&#23545;&#21152;&#23494;&#25968;&#25454;&#36827;&#34892;&#20219;&#24847;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;FHE&#24212;&#29992;&#20110;&#22522;&#20110;&#26641;&#22411;&#27169;&#22411;&#30340;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#24471;&#21040;&#20102;&#38024;&#23545;&#21152;&#23494;&#34920;&#26684;&#25968;&#25454;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#26641;&#22411;&#27169;&#22411;&#65292;&#21253;&#25324;&#20915;&#31574;&#26641;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#22686;&#24378;&#26641;&#65292;&#24182;&#24050;&#23454;&#29616;&#22312;Concrete-ML&#24211;&#20013;&#65292;&#35813;&#24211;&#22312;https://github.com/zama-ai/concrete-ml. &#24320;&#28304;&#12290;&#36890;&#36807;&#36873;&#25321;&#19968;&#32452;&#24212;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;FHE&#29256;&#26412;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#38750;&#24120;&#25509;&#36817;&#26410;&#21463;&#20445;&#25252;&#30340;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy enhancing technologies (PETs) have been proposed as a way to protect the privacy of data while still allowing for data analysis. In this work, we focus on Fully Homomorphic Encryption (FHE), a powerful tool that allows for arbitrary computations to be performed on encrypted data. FHE has received lots of attention in the past few years and has reached realistic execution times and correctness.  More precisely, we explain in this paper how we apply FHE to tree-based models and get state-of-the-art solutions over encrypted tabular data. We show that our method is applicable to a wide range of tree-based models, including decision trees, random forests, and gradient boosted trees, and has been implemented within the Concrete-ML library, which is open-source at https://github.com/zama-ai/concrete-ml. With a selected set of use-cases, we demonstrate that our FHE version is very close to the unprotected version in terms of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#21106;&#27169;&#22411;Opto-UNet&#65292;&#29992;&#20110;&#20998;&#21106;&#38745;&#33033;&#22721;&#32467;&#26500;&#65292;&#24182;&#37319;&#29992;U-Net&#26550;&#26500;&#25645;&#24314;&#65292;&#20351;&#29992;&#23380;&#27934;&#21644;&#21487;&#20998;&#31163;&#21367;&#31215;&#25552;&#21462;&#31354;&#38388;&#24191;&#33539;&#22260;&#21644;&#21487;&#20998;&#31163;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2302.14808</link><description>&lt;p&gt;
Opto-UNet&#65306;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#20013;&#38745;&#33033;&#26354;&#24352;&#20998;&#21106;&#30340;&#20248;&#21270;UNet
&lt;/p&gt;
&lt;p&gt;
Opto-UNet: Optimized UNet for Segmentation of Varicose Veins in Optical Coherence Tomography. (arXiv:2302.14808v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#21106;&#27169;&#22411;Opto-UNet&#65292;&#29992;&#20110;&#20998;&#21106;&#38745;&#33033;&#22721;&#32467;&#26500;&#65292;&#24182;&#37319;&#29992;U-Net&#26550;&#26500;&#25645;&#24314;&#65292;&#20351;&#29992;&#23380;&#27934;&#21644;&#21487;&#20998;&#31163;&#21367;&#31215;&#25552;&#21462;&#31354;&#38388;&#24191;&#33539;&#22260;&#21644;&#21487;&#20998;&#31163;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#38745;&#33033;&#23545;&#20110;&#23558;&#34880;&#28082;&#20174;&#36523;&#20307;&#21508;&#37096;&#20301;&#36755;&#36865;&#33267;&#24515;&#33039;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#38745;&#33033;&#30142;&#30149;&#21487;&#33021;&#23548;&#33268;&#38745;&#33033;&#21151;&#33021;&#19981;&#33391;&#12290;&#20854;&#20013;&#19968;&#31181;&#30142;&#30149;&#26159;&#38745;&#33033;&#26354;&#24352;&#65292;&#34880;&#28082;&#21487;&#33021;&#20250;&#21457;&#29983;&#21453;&#27969;&#65292;&#23548;&#33268;&#38745;&#33033;&#21387;&#21147;&#22686;&#21152;&#25110;&#30001;&#20110;&#38745;&#33033;&#32467;&#26500;&#30340;&#21464;&#21270;&#32780;&#23548;&#33268;&#34880;&#27969;&#21463;&#38480;&#12290;&#20026;&#20102;&#26816;&#26597;&#38745;&#33033;&#26354;&#24352;&#30340;&#21151;&#33021;&#29305;&#24449;&#65292;&#30740;&#31350;&#38745;&#33033;&#22721;&#30340;&#29289;&#29702;&#21644;&#29983;&#29289;&#21147;&#23398;&#29305;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#21106;&#27169;&#22411;Opto-UNet&#65292;&#29992;&#20110;&#20998;&#21106;&#38745;&#33033;&#22721;&#32467;&#26500;&#12290;&#37319;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#31995;&#32479;&#33719;&#21462;&#38745;&#33033;&#26354;&#24352;&#22270;&#20687;&#12290;&#30001;&#20110;&#25552;&#21462;&#30340;&#38745;&#33033;&#24418;&#29366;&#19981;&#22343;&#21248;&#65292;&#22240;&#27492;&#38656;&#35201;&#36866;&#24403;&#30340;&#20998;&#21106;&#26041;&#27861;&#26469;&#20998;&#21106;&#38745;&#33033;&#22721;&#12290;Opto-UNet&#27169;&#22411;&#22522;&#20110;U-Net&#26550;&#26500;&#65292;&#20854;&#20013;&#25972;&#21512;&#20102;&#19968;&#20010;&#26032;&#30340;&#22359;&#65292;&#37319;&#29992;&#23380;&#27934;&#21644;&#21487;&#20998;&#31163;&#21367;&#31215;&#26469;&#25552;&#21462;&#31354;&#38388;&#24191;&#33539;&#22260;&#21644;&#21487;&#20998;&#31163;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human veins are important for carrying the blood from the body-parts to the heart. The improper functioning of the human veins may arise from several venous diseases. Varicose vein is one such disease wherein back flow of blood can occur, often resulting in increased venous pressure or restricted blood flow due to changes in the structure of vein. To examine the functional characteristics of the varicose vein, it is crucial to study the physical and bio mechanical properties of the vein. This work proposes a segmentation model Opto-UNet, for segmenting the venous wall structure. Optical Coherence Tomography system is used to acquire images of varicose vein. As the extracted vein is not uniform in shape, hence adequate method of segmentation is required to segment the venous wall. Opto-UNet model is based on the U-Net architecture wherein a new block is integrated into the architecture, employing atrous and separable convolution to extract spatially wide-range and separable features map
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; Vid2Seq&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23494;&#38598;&#20107;&#20214;&#23383;&#24149;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#36716;&#24405;&#35821;&#38899;&#30340;&#21477;&#23376;&#36793;&#30028;&#36716;&#21270;&#20026;&#20266;&#20107;&#20214;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#36716;&#24405;&#35821;&#38899;&#21477;&#23376;&#20316;&#20026;&#20266;&#20107;&#20214;&#23383;&#24149;&#65292;&#25105;&#20204;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#27880; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26159;&#30446;&#21069;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2302.14115</link><description>&lt;p&gt;
Vid2Seq: &#29992;&#20110;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning. (arXiv:2302.14115v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Vid2Seq&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#22823;&#35268;&#27169; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#23494;&#38598;&#20107;&#20214;&#23383;&#24149;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#36716;&#24405;&#35821;&#38899;&#30340;&#21477;&#23376;&#36793;&#30028;&#36716;&#21270;&#20026;&#20266;&#20107;&#20214;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#36716;&#24405;&#35821;&#38899;&#21477;&#23376;&#20316;&#20026;&#20266;&#20107;&#20214;&#23383;&#24149;&#65292;&#25105;&#20204;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#27880; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26159;&#30446;&#21069;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Vid2Seq&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#21333;&#32423;&#23494;&#38598;&#20107;&#20214;&#23383;&#24149;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;&#22823;&#35268;&#27169; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#12290; Vid2Seq &#26550;&#26500;&#36890;&#36807;&#29305;&#27530;&#30340;&#26102;&#38388;&#26631;&#35760;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#26080;&#32541;&#22320;&#39044;&#27979;&#20107;&#20214;&#36793;&#30028;&#21644;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26410;&#26631;&#27880; narrated &#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#23558;&#36716;&#24405;&#35821;&#38899;&#30340;&#21477;&#23376;&#36793;&#30028;&#36716;&#21270;&#20026;&#20266;&#20107;&#20214;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#36716;&#24405;&#35821;&#38899;&#21477;&#23376;&#20316;&#20026;&#20266;&#20107;&#20214;&#23383;&#24149;&#12290;&#20351;&#29992; YT-Temporal-1B &#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340; Vid2Seq &#27169;&#22411;&#22312;&#21508;&#31181;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324; YouCook2&#12289;ViTT &#21644; ActivityNet Captions&#12290; Vid2Seq &#36824;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#35270;&#39057;&#27573;&#33853;&#23383;&#24149;&#21644;&#35270;&#39057;&#29255;&#27573;&#23383;&#24149;&#30340;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale. The Vid2Seq architecture augments a language model with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence. Such a unified model requires large-scale training data, which is not available in current annotated datasets. We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sentence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions. The resulting Vid2Seq model pretrained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions. Vid2Seq also generalizes well to the tasks of video paragraph captioning and video clip captionin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;GNN&#35299;&#37322;&#26041;&#27861;PaGE-Link&#65292;&#29992;&#20110;&#24322;&#26500;&#38142;&#25509;&#39044;&#27979;&#65292;&#20855;&#26377;&#36830;&#25509;&#21487;&#35299;&#37322;&#24615;&#65292;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#21644;&#22788;&#29702;&#22270;&#24418;&#24322;&#26500;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.12465</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26041;&#27861;&#29992;&#20110;&#24322;&#26500;&#38142;&#25509;&#39044;&#27979;&#65288;PaGE-Link&#65289;
&lt;/p&gt;
&lt;p&gt;
PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction. (arXiv:2302.12465v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;GNN&#35299;&#37322;&#26041;&#27861;PaGE-Link&#65292;&#29992;&#20110;&#24322;&#26500;&#38142;&#25509;&#39044;&#27979;&#65292;&#20855;&#26377;&#36830;&#25509;&#21487;&#35299;&#37322;&#24615;&#65292;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#21644;&#22788;&#29702;&#22270;&#24418;&#24322;&#26500;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#24050;&#25104;&#20026;&#40657;&#31665;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#27169;&#22411;&#34892;&#20026;&#30340;&#36866;&#24403;&#35299;&#37322;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26356;&#36127;&#36131;&#20219;&#30340;&#27169;&#22411;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26368;&#36817;&#22312;&#35768;&#22810;&#22270;&#24418;ML&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#35299;&#37322;&#23427;&#20204;&#24050;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#38142;&#25509;&#39044;&#27979;&#65288;LP&#65289;&#26041;&#38754;&#65292;GNN&#30340;&#35299;&#37322;&#23578;&#32570;&#23569;&#25991;&#29486;&#25903;&#25345;&#12290; LP&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;GNN&#20219;&#21153;&#65292;&#23545;&#24212;&#20110;Web&#19978;&#30340;&#25512;&#33616;&#21644;&#36190;&#21161;&#25628;&#32034;&#31561;&#24212;&#29992;&#31243;&#24207;&#12290;&#37492;&#20110;&#29616;&#26377;&#30340;GNN&#35299;&#37322;&#26041;&#27861;&#20165;&#35299;&#20915;&#33410;&#28857;/&#22270;&#32423;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36335;&#24452;&#30340;GNN&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#24322;&#26500;&#38142;&#25509;&#39044;&#27979;&#65288;PaGE-Link&#65289;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#20855;&#26377;&#36830;&#25509;&#21487;&#35299;&#37322;&#24615;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#27169;&#22411;&#21487;&#25193;&#23637;&#24615;&#21644;&#22788;&#29702;&#22270;&#24418;&#24322;&#26500;&#24615;&#33021;&#21147;&#12290;&#23450;&#24615;&#22320;&#65292;PaGE-Link&#21487;&#20197;&#29983;&#25104;&#23558;&#33410;&#28857;&#23545;&#36830;&#25509;&#36215;&#26469;&#30340;&#36335;&#24452;&#20316;&#20026;&#35299;&#37322;&#65292;&#33258;&#28982;&#22320;&#25429;&#25417;&#21040;&#36830;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#38480;&#21046;&#21322;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#26412;&#36136;&#22343;&#20540;&#27169;&#22411;&#21450;&#20854;Karcher&#22343;&#20540;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#22806;&#37096;&#20449;&#21495;&#21152;&#22122;&#22768;&#27169;&#22411;&#19979;&#30340;&#30830;&#23450;&#24615;&#35823;&#24046;&#30028;&#65292;&#24182;&#34920;&#26126;LRC-dPCA&#31639;&#27861;&#19982;&#20840;&#26679;&#26412;PCA&#31639;&#27861;&#24615;&#33021;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2302.12426</link><description>&lt;p&gt;
&#38480;&#21046;&#21322;&#27491;&#23450;&#30697;&#38453;&#30340;Karcher&#22343;&#20540;&#30340;&#32479;&#35745;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Statistical Analysis of Karcher Means for Random Restricted PSD Matrices. (arXiv:2302.12426v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#38480;&#21046;&#21322;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#26412;&#36136;&#22343;&#20540;&#27169;&#22411;&#21450;&#20854;Karcher&#22343;&#20540;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#22806;&#37096;&#20449;&#21495;&#21152;&#22122;&#22768;&#27169;&#22411;&#19979;&#30340;&#30830;&#23450;&#24615;&#35823;&#24046;&#30028;&#65292;&#24182;&#34920;&#26126;LRC-dPCA&#31639;&#27861;&#19982;&#20840;&#26679;&#26412;PCA&#31639;&#27861;&#24615;&#33021;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20960;&#20309;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#30001;&#20110;&#21487;&#33021;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#27969;&#24418;&#32467;&#26500;&#32780;&#32570;&#20047;&#38750;&#28176;&#36827;&#32479;&#35745;&#20998;&#26512;&#12290;&#26412;&#25991;&#22312;&#38480;&#21046;&#21322;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#30740;&#31350;&#26412;&#36136;&#22343;&#20540;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;Karcher&#22343;&#20540;&#30340;&#38750;&#28176;&#36827;&#32479;&#35745;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#19968;&#33324;&#30340;&#22806;&#37096;&#20449;&#21495;&#21152;&#22122;&#22768;&#27169;&#22411;&#65292;&#22312;&#27492;&#27169;&#22411;&#19979;&#25552;&#20379;&#20102;Karcher&#22343;&#20540;&#30340;&#30830;&#23450;&#24615;&#35823;&#24046;&#30028;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20998;&#24067;&#24335;&#20027;&#25104;&#20998;&#20998;&#26512;&#31639;&#27861;LRC-dPCA&#30340;&#24615;&#33021;&#19982;&#20840;&#26679;&#26412;PCA&#31639;&#27861;&#30456;&#21516;&#12290;&#25968;&#20540;&#23454;&#39564;&#20805;&#20998;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic statistical analysis is often missing for modern geometry-aware machine learning algorithms due to the possibly intricate non-linear manifold structure. This paper studies an intrinsic mean model on the manifold of restricted positive semi-definite matrices and provides a non-asymptotic statistical analysis of the Karcher mean. We also consider a general extrinsic signal-plus-noise model, under which a deterministic error bound of the Karcher mean is provided. As an application, we show that the distributed principal component analysis algorithm, LRC-dPCA, achieves the same performance as the full sample PCA algorithm. Numerical experiments lend strong support to our theories.
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#23545;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20449;&#20219;&#24230;&#30340;&#25928;&#26524;&#26377;&#23616;&#38480;&#24615;&#65292;&#36879;&#26126;&#24230;&#21644;&#20005;&#26684;&#30340;&#39564;&#35777;&#26356;&#36866;&#21512;&#25171;&#36896;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2302.11577</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26080;&#27861;&#25552;&#20379;&#26368;&#32456;&#29992;&#25143;&#25152;&#35201;&#27714;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explainable AI does not provide the explanations end-users are asking for. (arXiv:2302.11577v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11577
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#23545;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20449;&#20219;&#24230;&#30340;&#25928;&#26524;&#26377;&#23616;&#38480;&#24615;&#65292;&#36879;&#26126;&#24230;&#21644;&#20005;&#26684;&#30340;&#39564;&#35777;&#26356;&#36866;&#21512;&#25171;&#36896;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#32463;&#24120;&#34987;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#29992;&#25143;&#35201;&#27714;&#20351;&#29992;&#65292;&#26088;&#22312;&#20102;&#35299;&#22797;&#26434;&#27169;&#22411;&#21450;&#20854;&#30456;&#20851;&#39044;&#27979;&#65292;&#24182;&#24314;&#31435;&#20449;&#20219;&#12290;&#34429;&#28982;&#22312;&#24320;&#21457;&#30340;&#26576;&#20123;&#29305;&#23450;&#20219;&#21153;&#20013;&#26159;&#36866;&#29992;&#30340;&#65292;&#20294;&#32452;&#32455;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#22686;&#24378;&#23545;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20449;&#20219;&#26102;&#65292;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21518;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;XAI&#22312;&#37096;&#32626;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#35748;&#20026;&#36879;&#26126;&#24230;&#21644;&#20005;&#26684;&#30340;&#39564;&#35777;&#26356;&#36866;&#21512;&#33719;&#24471;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) techniques are frequently required by users in many AI systems with the goal of understanding complex models, their associated predictions, and gaining trust. While suitable for some specific tasks during development, their adoption by organisations to enhance trust in machine learning systems has unintended consequences. In this paper we discuss XAI's limitations in deployment and conclude that transparency alongside with rigorous validation are better suited to gaining trust in AI systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;</title><link>http://arxiv.org/abs/2302.09738</link><description>&lt;p&gt;
&#31616;&#21270;&#22522;&#20110;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#65292;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#29992;&#20110;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#30340;&#40654;&#26364;&#23376;&#27969;&#24418;&#20248;&#21270;&#22312;&#35745;&#31639;&#19978;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#30830;&#20445;&#36845;&#20195;&#20445;&#25345;&#22312;&#23376;&#27969;&#24418;&#19978;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#22256;&#38590;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#20223;&#23556;&#19981;&#21464;&#24230;&#37327;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#23376;&#27969;&#24418;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#31616;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#40654;&#26364;&#27491;&#24120;&#22352;&#26631;&#30340;&#24191;&#20041;&#29256;&#26412;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#21160;&#24577;&#22320;&#31616;&#21270;&#20026;&#27431;&#20960;&#37324;&#24471;&#26080;&#32422;&#26463;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#31616;&#21270;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#21327;&#26041;&#24046;&#26041;&#27861;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; GPT4MIA &#26041;&#27861;&#65292;&#21033;&#29992; GPT-3 &#20316;&#20026;&#25554;&#20837;&#24335;&#26816;&#39564;&#24037;&#20855;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65307;&#35813;&#26041;&#27861;&#22312;&#25552;&#31034;&#32467;&#26500;&#35774;&#35745;&#12289;&#26679;&#26412;&#36873;&#25321;&#21450;&#25552;&#31034;&#25490;&#24207;&#31561;&#26041;&#38754;&#20248;&#21270;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08722</link><description>&lt;p&gt;
GPT4MIA: &#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (GPT-3) &#20316;&#20026;&#25554;&#20837;&#24335;&#26816;&#39564;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
GPT4MIA: Utilizing Generative Pre-trained Transformer (GPT-3) as A Plug-and-Play Transductive Model for Medical Image Analysis. (arXiv:2302.08722v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; GPT4MIA &#26041;&#27861;&#65292;&#21033;&#29992; GPT-3 &#20316;&#20026;&#25554;&#20837;&#24335;&#26816;&#39564;&#24037;&#20855;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#65307;&#35813;&#26041;&#27861;&#22312;&#25552;&#31034;&#32467;&#26500;&#35774;&#35745;&#12289;&#26679;&#26412;&#36873;&#25321;&#21450;&#25552;&#31034;&#25490;&#24207;&#31561;&#26041;&#38754;&#20248;&#21270;&#65292;&#33021;&#26377;&#25928;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; GPT4MIA &#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (GPT) &#20316;&#20026;&#25554;&#20837;&#24335;&#26816;&#39564;&#24037;&#20855;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512; (MIA)&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20687; GPT-3 &#36825;&#26679;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25554;&#20837;&#24335;&#26816;&#39564;&#27169;&#22411;&#29992;&#20110; MIA&#12290;&#22312;&#26041;&#27861;&#23398;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#25216;&#26415;&#22788;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#26356;&#22909;&#30340;&#25552;&#31034;&#32467;&#26500;&#35774;&#35745;&#12289;&#26679;&#26412;&#36873;&#25321;&#20197;&#21450;&#20195;&#34920;&#24615;&#26679;&#26412;/&#29305;&#24449;&#30340;&#25552;&#31034;&#25490;&#24207;&#65292;&#20197;&#25552;&#39640; GPT4MIA &#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#20004;&#31181;&#20855;&#20307;&#30340; GPT4MIA &#20351;&#29992;&#26696;&#20363; (&#24102;&#26377;&#24037;&#20316;&#27969;&#31243;)&#65306;(1) &#26816;&#27979;&#39044;&#27979;&#38169;&#35823;&#21644; (2) &#25913;&#36827;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19982;&#24050;&#32463;&#24314;&#31435;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411; (&#20363;&#22914; ResNet) &#21327;&#21516;&#24037;&#20316;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#21033;&#29992;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892; MIA &#20219;&#21153;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach (called GPT4MIA) that utilizes Generative Pre-trained Transformer (GPT) as a plug-and-play transductive inference tool for medical image analysis (MIA). We provide theoretical analysis on why a large pre-trained language model such as GPT-3 can be used as a plug-and-play transductive inference model for MIA. At the methodological level, we develop several technical treatments to improve the efficiency and effectiveness of GPT4MIA, including better prompt structure design, sample selection, and prompt ordering of representative samples/features. We present two concrete use cases (with workflow) of GPT4MIA: (1) detecting prediction errors and (2) improving prediction accuracy, working in conjecture with well-established vision-based models for image classification (e.g., ResNet). Experiments validate that our proposed method is effective for these two tasks. We further discuss the opportunities and challenges in utilizing Transformer-based large
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#19968;&#39033;&#30495;&#23454;&#29992;&#20363;&#30340;&#8220;SimEvals&#8221;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#22312;&#30005;&#23376;&#21830;&#21153;&#27450;&#35784;&#26816;&#27979;&#20013;&#35299;&#37322;&#26159;&#21542;&#21487;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#20915;&#31574;&#65292;&#24182;&#35777;&#23454;&#20102;&#20854;&#19982;&#29992;&#25143;&#30740;&#31350;&#30340;&#32467;&#35770;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2302.07444</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#29992;&#25143;&#30740;&#31350;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24615;&#35780;&#20272;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Case Study on Designing Evaluations of ML Explanations with Simulated User Studies. (arXiv:2302.07444v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#19968;&#39033;&#30495;&#23454;&#29992;&#20363;&#30340;&#8220;SimEvals&#8221;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#22312;&#30005;&#23376;&#21830;&#21153;&#27450;&#35784;&#26816;&#27979;&#20013;&#35299;&#37322;&#26159;&#21542;&#21487;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#20915;&#31574;&#65292;&#24182;&#35777;&#23454;&#20102;&#20854;&#19982;&#29992;&#25143;&#30740;&#31350;&#30340;&#32467;&#35770;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#20197;&#30830;&#23450;&#27169;&#22411;&#35299;&#37322;&#22312;&#24110;&#21161;&#20154;&#31867;&#20915;&#31574;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#26102;&#65292;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#29992;&#20363;&#12289;&#25968;&#25454;&#21644;&#29992;&#25143;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#36164;&#28304;&#65292;&#20351;&#24471;&#21482;&#33021;&#35780;&#20272;&#23569;&#37327;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20154;&#31867;&#29992;&#25143;&#20195;&#29702;&#30340;&#27169;&#25311;&#29992;&#25143;&#35780;&#20272;&#65288;SimEvals&#65289;&#20316;&#20026;&#36873;&#25321;&#26377;&#21069;&#26223;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#29992;&#20363;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425; SimEvals&#65292;&#20197;&#35780;&#20272;&#35299;&#37322;&#26159;&#21542;&#21487;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#22312;&#30005;&#23376;&#21830;&#21153;&#27450;&#35784;&#26816;&#27979;&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#20915;&#31574;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102; SimEvals &#26159;&#21542;&#33021;&#22815;&#35777;&#23454;&#22312;&#36825;&#20010;&#27450;&#35784;&#26816;&#27979;&#32972;&#26223;&#19979;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616; SimEvals &#34920;&#26126;&#65292;&#25152;&#26377;&#32771;&#34385;&#30340;&#35299;&#37322;&#22120;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#36229;&#36807;&#27809;&#26377;&#35299;&#37322;&#30340;&#22522;&#20934; -- &#36825;&#19982;&#21407;&#22987;&#29992;&#25143;&#30740;&#31350;&#30340;&#32467;&#35770;&#30456;&#31526;&#12290;&#36825;&#20123;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
When conducting user studies to ascertain the usefulness of model explanations in aiding human decision-making, it is important to use real-world use cases, data, and users. However, this process can be resource-intensive, allowing only a limited number of explanation methods to be evaluated. Simulated user evaluations (SimEvals), which use machine learning models as a proxy for human users, have been proposed as an intermediate step to select promising explanation methods. In this work, we conduct the first SimEvals on a real-world use case to evaluate whether explanations can better support ML-assisted decision-making in e-commerce fraud detection. We study whether SimEvals can corroborate findings from a user study conducted in this fraud detection context. In particular, we find that SimEvals suggest that all considered explainers are equally performant, and none beat a baseline without explanations -- this matches the conclusions of the original user study. Such correspondences be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;</title><link>http://arxiv.org/abs/2302.02601</link><description>&lt;p&gt;
&#23398;&#20064;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#36229;&#36234;&#38142;&#25509;&#39044;&#27979;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction. (arXiv:2302.02601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20351;&#29992;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#24050;&#30693;&#20107;&#23454;&#12290;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#20165;&#32771;&#34385;&#23454;&#20307;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#32771;&#34385;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26356;&#39640;&#32423;&#30340;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20363;&#22914;&#65292;$\langle T_1$, PrerequisiteFor, $T_2\rangle$&#65292;&#20854;&#20013;PrerequisiteFor&#26159;&#26356;&#39640;&#32423;&#21035;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23450;&#20041;&#19968;&#20010;&#30001;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#30340;&#19977;&#20803;&#32452;&#32452;&#25104;&#30340;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;BiVE&#36890;&#36807;&#32771;&#34385;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#19977;&#20803;&#32452;&#30340;&#32467;&#26500;&#26469;&#23398;&#20064;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is (Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a prerequisite for $T_2$. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., $\langle T_1$, PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level tripl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02119</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#23545;&#25112;&#23454;&#29616;&#22810;&#26679;&#21270;&#35825;&#23548;&#30340;&#29615;&#22659;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#26469;&#35782;&#21035;&#29615;&#22659;&#20013;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#23558;&#22810;&#26679;&#24615;&#24341;&#20837;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29615;&#22659;&#35774;&#35745;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#29615;&#22659;&#20998;&#24067;&#35774;&#35745;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20986;&#35757;&#32451;&#26377;&#25928;&#30340;&#36890;&#29992;&#33021;&#21147;&#20195;&#29702;&#30340;&#21069;&#26223;&#12290;&#23427;&#30340;&#25104;&#21151;&#37096;&#20998;&#22312;&#20110;&#19968;&#31181;&#33258;&#36866;&#24212;&#35838;&#31243;&#23398;&#20064;&#30340;&#24418;&#24335;&#65292;&#35813;&#24418;&#24335;&#36890;&#36807;&#29983;&#25104;&#20195;&#29702;&#33021;&#21147;&#30340;&#21069;&#27839;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29615;&#22659;&#35774;&#35745;&#26694;&#26550;&#32463;&#24120;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#21457;&#29616;&#26377;&#25928;&#32423;&#21035;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#39640;&#25104;&#26412;&#20132;&#20114;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#22312;&#38750;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26694;&#26550;&#20013;&#24341;&#20837;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#32473;&#23450;&#32423;&#21035;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35266;&#23519;/&#38544;&#34255;&#29366;&#24577;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#26469;&#34920;&#24449;&#20004;&#20010;&#32423;&#21035;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#65292;&#36825;&#23545;&#20110;&#26377;&#25928;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#33258;&#25105;&#23545;&#25112;&#25216;&#26415;&#65292;&#20351;&#24471;&#29615;&#22659;&#29983;&#25104;&#22120;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniTNT&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#20860;&#39038;&#22330;&#26223;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#19982;&#20808;&#21069;&#30340;&#27169;&#24577;&#34701;&#21512;&#25552;&#39640;&#20102;&#22270;&#20687;&#38382;&#39064;&#22238;&#31572;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.07389</link><description>&lt;p&gt;
&#36208;&#21521;&#26082;&#33021;&#30475;&#21448;&#33021;&#35835;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Models that Can See and Read. (arXiv:2301.07389v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniTNT&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#20860;&#39038;&#22330;&#26223;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#29702;&#35299;&#65292;&#36890;&#36807;&#19982;&#20808;&#21069;&#30340;&#27169;&#24577;&#34701;&#21512;&#25552;&#39640;&#20102;&#22270;&#20687;&#38382;&#39064;&#22238;&#31572;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65288;CAP&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20043;&#19968;&#65292;&#23427;&#20204;&#26377;&#30528;&#31867;&#20284;&#30340;&#22330;&#26223;&#25991;&#26412;&#29256;&#26412;&#65292;&#38656;&#35201;&#20174;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#36827;&#34892;&#25512;&#29702;&#12290;&#23613;&#31649;&#23427;&#20204;&#26126;&#26174;&#30456;&#20284;&#65292;&#20294;&#20004;&#32773;&#29420;&#31435;&#22788;&#29702;&#65292;&#20135;&#29983;&#21487;&#20197;&#30475;&#25110;&#35835;&#20294;&#19981;&#33021;&#20004;&#32773;&#20860;&#22791;&#30340;&#19987;&#38376;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#29616;&#35937;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#24314;&#35758;UniTNT&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#25991;&#26412;-&#38750;&#25991;&#26412;&#26041;&#27861;&#65292;&#20026;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#25552;&#20379;&#22330;&#26223;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#22330;&#26223;&#25991;&#26412;&#20449;&#24687;&#35270;&#20026;&#19968;&#31181;&#39069;&#22806;&#30340;&#27169;&#24577;&#65292;&#24182;&#36890;&#36807;&#25351;&#23450;&#30340;&#27169;&#22359;&#23558;&#20854;&#19982;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#24443;&#24213;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;UniTNT&#26159;&#31532;&#19968;&#20010;&#25104;&#21151;&#22788;&#29702;&#20004;&#31181;&#20219;&#21153;&#31867;&#22411;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22330;&#26223;&#25991;&#26412;&#30340;&#29702;&#35299;&#33021;&#21147;&#21487;&#20197;&#23558;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;VQA&#21644;CAP&#19978;&#30340;&#24615;&#33021;&#25552;&#39640;&#39640;&#36798;2.69&#65285;&#21644;0.6 CIDEr&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) and Image Captioning (CAP), which are among the most popular vision-language tasks, have analogous scene-text versions that require reasoning from the text in the image. Despite their obvious resemblance, the two are treated independently and, as we show, yield task-specific methods that can either see or read, but not both. In this work, we conduct an in-depth analysis of this phenomenon and propose UniTNT, a Unified Text-Non-Text approach, which grants existing multimodal architectures scene-text understanding capabilities. Specifically, we treat scene-text information as an additional modality, fusing it with any pretrained encoder-decoder-based architecture via designated modules. Thorough experiments reveal that UniTNT leads to the first single model that successfully handles both task types. Moreover, we show that scene-text understanding capabilities can boost vision-language models' performance on general VQA and CAP by up to 2.69% and 0.6 CIDEr,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.05599</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#30701;SSVEP&#25968;&#25454;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Short-length SSVEP data extension by a novel generative adversarial networks based framework. (arXiv:2301.05599v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;SSVEP&#30340;&#33041;&#26426;&#25509;&#21475;&#22240;&#20854;&#39640;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#21644;&#30446;&#26631;&#25968;&#37327;&#21487;&#29992;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29992;&#25143;&#26657;&#20934;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#25968;&#25454;&#38271;&#24230;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#21019;&#24314;&#21512;&#25104;&#30340;&#33041;&#30005;&#25968;&#25454;&#65292;&#26377;&#26395;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GANs&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#29992;&#20110;&#25968;&#25454;&#38271;&#24230;&#25193;&#23637;&#12290;TEGAN&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#26032;&#39062;&#30340;U&#22411;&#29983;&#25104;&#22120;&#26550;&#26500;&#21644;&#19968;&#20010;&#36741;&#21161;&#20998;&#31867;&#22120;&#21152;&#20837;&#21040;&#32593;&#32476;&#32467;&#26500;&#20013;&#65292;TEGAN&#21487;&#20197;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#20135;&#29983;&#26377;&#26465;&#20214;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;TEGAN&#29983;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#25152;&#38656;&#30340;&#26657;&#20934;&#26102;&#38388;&#24182;&#25913;&#21892;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high information transfer rate (ITR) and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP signals into long-length artificial SSVEP signals. By incorporating a novel U-Net generator architecture and an auxiliary classifier into the network architecture, the TEGAN could produce conditioned features in the synthetic data. Additionally, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.09010</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#25351;&#25968;&#26631;&#20934;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39118;&#38505;&#20013;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#21644;&#31995;&#32479;&#21442;&#25968;&#25200;&#21160;&#30340;&#24433;&#21709;&#32780;&#19981;&#22815;&#31283;&#20581;&#12290;&#22240;&#27492;,&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#65292;&#26679;&#26412;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#27169;&#22411;&#39118;&#38505;&#25935;&#24863;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#21464;&#20307;&#65292;&#20854;&#23454;&#29616;&#36807;&#31243;&#31867;&#20284;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#26631;&#20934;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#31574;&#30053;&#39118;&#38505;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#33945;&#29305;&#21345;&#32599;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#22312;&#32447;(&#26102;&#38388;&#24046;&#20998;)&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#26631;&#20934;&#30340;&#20351;&#29992;&#33021;&#22815;&#25512;&#24191;&#24120;&#29992;&#30340;&#29305;&#23450;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#25670;&#21160;&#26438;&#21644;&#25670;&#25670;&#26438;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#23454;&#29616;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#22686;&#21152;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#30340;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#65292;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2212.08189</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#22312;&#32447;&#30830;&#23450;&#24615;&#36864;&#28779;&#65306;&#19968;&#31181;&#20998;&#23618;&#21644;&#28176;&#36827;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Multi-Resolution Online Deterministic Annealing: A Hierarchical and Progressive Learning Architecture. (arXiv:2212.08189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#28176;&#22686;&#21152;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#30340;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#65292;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#36880;&#27493;&#36924;&#36817;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20998;&#23618;&#23398;&#20064;&#31639;&#27861;&#23545;&#20110;&#20915;&#31574;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20998;&#23618;&#23398;&#20064;&#32467;&#26500;&#65292;&#22522;&#20110;&#21487;&#33021;&#30340;&#22810;&#20998;&#36776;&#29575;&#25968;&#25454;&#31354;&#38388;&#30340;&#28176;&#36827;&#20998;&#21306;&#12290;&#26368;&#20248;&#20998;&#21306;&#36890;&#36807;&#35299;&#20915;&#19968;&#31995;&#21015;&#20248;&#21270;&#23376;&#38382;&#39064;&#36880;&#27493;&#36924;&#36817;&#65292;&#29983;&#25104;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#30340;&#23376;&#38598;&#25968;&#37327;&#30340;&#20998;&#21306;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#23545;&#27599;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#21487;&#20197;&#20351;&#29992;&#26080;&#26799;&#24230;&#38543;&#26426;&#36924;&#36817;&#26356;&#26032;&#36827;&#34892;&#22312;&#32447;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#22312;&#20998;&#21306;&#30340;&#27599;&#20010;&#23376;&#38598;&#20013;&#23450;&#20041;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21452;&#26102;&#38388;&#23610;&#24230;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#29702;&#35770;&#35299;&#20915;&#12290;&#36825;&#27169;&#25311;&#20102;&#19968;&#31181;&#36864;&#28779;&#36807;&#31243;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#24378;&#22823;&#19988;&#21487;&#35299;&#37322;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36880;&#27493;&#22686;&#21152;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical learning algorithms that gradually approximate a solution to a data-driven optimization problem are essential to decision-making systems, especially under limitations on time and computational resources. In this study, we introduce a general-purpose hierarchical learning architecture that is based on the progressive partitioning of a possibly multi-resolution data space. The optimal partition is gradually approximated by solving a sequence of optimization sub-problems that yield a sequence of partitions with increasing number of subsets. We show that the solution of each optimization problem can be estimated online using gradient-free stochastic approximation updates. As a consequence, a function approximation problem can be defined within each subset of the partition and solved using the theory of two-timescale stochastic approximation algorithms. This simulates an annealing process and defines a robust and interpretable heuristic method to gradually increase the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#24418;&#21464;&#25442;&#22120;&#65288;TERT&#65289;&#30340;&#39640;&#23481;&#37327; Transformer &#27169;&#22411;&#65292;&#29992;&#20110;&#21508;&#31181;&#22320;&#24418;&#20013;&#22235;&#36275;&#36816;&#21160;&#25511;&#21046;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351; Transformer &#26356;&#22909;&#22320;&#24212;&#29992;&#20110; sim-to-real &#22330;&#26223;&#20013;&#12290;</title><link>http://arxiv.org/abs/2212.07740</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#24418;&#21464;&#25442;&#22120;&#30340;&#22235;&#36275;&#36816;&#21160;&#27169;&#25311;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer. (arXiv:2212.07740v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#24418;&#21464;&#25442;&#22120;&#65288;TERT&#65289;&#30340;&#39640;&#23481;&#37327; Transformer &#27169;&#22411;&#65292;&#29992;&#20110;&#21508;&#31181;&#22320;&#24418;&#20013;&#22235;&#36275;&#36816;&#21160;&#25511;&#21046;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351; Transformer &#26356;&#22909;&#22320;&#24212;&#29992;&#20110; sim-to-real &#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#31181;&#22320;&#24418;&#30340;&#36275;&#24335; locomotion &#20013;&#36890;&#36807;&#22312;&#29289;&#29702;&#27169;&#25311;&#20013;&#35757;&#32451;&#31574;&#30053;&#24182;&#23558;&#20854;&#36801;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#65288;&#21363; sim-to-real &#36801;&#31227;&#65289;&#19978;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#23481;&#37327;&#21644;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#21487;&#33021;&#38459;&#30861;&#23427;&#20204;&#22312;&#26356;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;Transformer &#26550;&#26500;&#22312;&#24191;&#27867;&#30340;&#22823;&#35268;&#27169;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#24050;&#32463;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#36234;&#24615;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20915;&#31574;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Terrain Transformer&#65288;TERT&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#21508;&#31181;&#22320;&#24418;&#20013;&#22235;&#36275;&#36816;&#21160;&#25511;&#21046;&#30340;&#39640;&#23481;&#37327; Transformer &#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992; Transformer &#22312; sim-to-real &#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#65292;&#21253;&#25324;&#31163;&#32447;&#39044;&#35757;&#32451;&#38454;&#27573;&#21644;&#22312;&#32447;&#26657;&#27491;&#38454;&#27573;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#23558; Transformer &#19982; privilege
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning has recently emerged as an appealing alternative for legged locomotion over multiple terrains by training a policy in physical simulation and then transferring it to the real world (i.e., sim-to-real transfer). Despite considerable progress, the capacity and scalability of traditional neural networks are still limited, which may hinder their applications in more complex environments. In contrast, the Transformer architecture has shown its superiority in a wide range of large-scale sequence modeling tasks, including natural language processing and decision-making problems. In this paper, we propose Terrain Transformer (TERT), a high-capacity Transformer model for quadrupedal locomotion control on various terrains. Furthermore, to better leverage Transformer in sim-to-real scenarios, we present a novel two-stage training framework consisting of an offline pretraining stage and an online correction stage, which can naturally integrate Transformer with privilege
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#65292;&#33258;&#21160;&#25552;&#20379;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#36827;&#34892;&#31574;&#30053;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.07398</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Policy Adaptation from Foundation Model Feedback. (arXiv:2212.07398v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#65292;&#33258;&#21160;&#25552;&#20379;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#36827;&#34892;&#31574;&#30053;&#24494;&#35843;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#20026;&#26500;&#24314;&#36890;&#29992;&#26426;&#22120;&#20154;&#24102;&#26469;&#20102;&#26174;&#33879;&#36827;&#27493;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23558;&#22330;&#26223;&#21644;&#25351;&#20196;&#32534;&#30721;&#20026;&#20915;&#31574;&#36755;&#20837;&#65292;&#25351;&#20196;&#26465;&#20214;&#21270;&#31574;&#30053;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#23545;&#35937;&#21644;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#23613;&#31649;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#65292;&#20294;&#31574;&#30053;&#22312;&#36935;&#21040;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#25110;&#29615;&#22659;&#26102;&#20173;&#28982;&#22833;&#36133;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#31574;&#30053;&#36866;&#24212;&#65288;PAFF&#65289;&#12290;&#24403;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#37096;&#32626;&#21040;&#26032;&#20219;&#21153;&#25110;&#26032;&#29615;&#22659;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#35753;&#31574;&#30053;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#25351;&#20196;&#36827;&#34892;&#28436;&#31034;&#12290;&#34429;&#28982;&#25191;&#34892;&#21487;&#33021;&#20986;&#29616;&#38169;&#35823;&#65292;&#20294;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#21453;&#39304;&#26469;&#37325;&#26032;&#26631;&#35760;&#28436;&#31034;&#12290;&#36825;&#33258;&#21160;&#20026;&#31574;&#30053;&#24494;&#35843;&#25552;&#20379;&#20102;&#26032;&#30340;&#28436;&#31034;-&#25351;&#20196;&#25968;&#25454;&#23545;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#26159;&#22312;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#12289;&#20219;&#21153;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PAFF&#22312;&#26368;&#32456;&#20219;&#21153;&#25104;&#21151;&#29575;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress on vision-language foundation models have brought significant advancement to building general-purpose robots. By using the pre-trained models to encode the scene and instructions as inputs for decision making, the instruction-conditioned policy can generalize across different objects and tasks. While this is encouraging, the policy still fails in most cases given an unseen task or environment. In this work, we propose Policy Adaptation from Foundation model Feedback (PAFF). When deploying the trained policy to a new task or a new environment, we first let the policy play with randomly generated instructions to record the demonstrations. While the execution could be wrong, we can use the pre-trained foundation models to provide feedback to relabel the demonstrations. This automatically provides new pairs of demonstration-instruction data for policy fine-tuning. We evaluate our method on a broad range of experiments with the focus on generalization on unseen objects, unse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#30340;&#35843;&#24230;&#31574;&#30053;&#21644;&#32858;&#21512;&#21152;&#26435;&#35774;&#35745;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#20449;&#36947;&#24863;&#30693;&#25968;&#25454;&#37325;&#35201;&#24615;&#30340;&#35843;&#24230;&#31574;&#30053;&#21644;&#8220;&#24180;&#40836;&#24863;&#30693;&#8221;&#30340;&#32858;&#21512;&#21152;&#26435;&#35774;&#35745;&#26469;&#35299;&#20915;FL&#31995;&#32479;&#20013;&#30340;&#8220;&#25302;&#27795;&#8221;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.07356</link><description>&lt;p&gt;
&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#35843;&#24230;&#21644;&#32858;&#21512;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scheduling and Aggregation Design for Asynchronous Federated Learning over Wireless Networks. (arXiv:2212.07356v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#30340;&#35843;&#24230;&#31574;&#30053;&#21644;&#32858;&#21512;&#21152;&#26435;&#35774;&#35745;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#20449;&#36947;&#24863;&#30693;&#25968;&#25454;&#37325;&#35201;&#24615;&#30340;&#35843;&#24230;&#31574;&#30053;&#21644;&#8220;&#24180;&#40836;&#24863;&#30693;&#8221;&#30340;&#32858;&#21512;&#21152;&#26435;&#35774;&#35745;&#26469;&#35299;&#20915;FL&#31995;&#32479;&#20013;&#30340;&#8220;&#25302;&#27795;&#8221;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#21644;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#32858;&#21512;&#26469;&#22312;&#20998;&#24067;&#24335;&#20195;&#29702;&#38388;&#35757;&#32451;&#36890;&#29992;&#30340;ML&#27169;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;FL&#35774;&#35745;&#65292;&#37319;&#29992;&#21608;&#26399;&#24615;&#30340;&#32858;&#21512;&#26469;&#35299;&#20915;FL&#31995;&#32479;&#20013;&#30340;&#8220;&#25302;&#27795;&#8221;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#26377;&#38480;&#30340;&#26080;&#32447;&#36890;&#20449;&#36164;&#28304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#35843;&#24230;&#31574;&#30053;&#21644;&#32858;&#21512;&#35774;&#35745;&#23545;&#25910;&#25947;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#38477;&#20302;&#32858;&#21512;&#27169;&#22411;&#26356;&#26032;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35843;&#24230;&#31574;&#30053;&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#20102;&#29992;&#25143;&#35774;&#22791;&#30340;&#20449;&#36947;&#36136;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#34920;&#31034;&#12290;&#36890;&#36807;&#20223;&#30495;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#20449;&#36947;&#24863;&#30693;&#25968;&#25454;&#37325;&#35201;&#24615;&#30340;&#35843;&#24230;&#31574;&#30053;&#30456;&#23545;&#20110;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;&#25552;&#20986;&#30340;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#8220;&#24180;&#40836;&#24863;&#30693;&#8221;&#30340;&#32858;&#21512;&#21152;&#26435;&#35774;&#35745;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a collaborative machine learning (ML) framework that combines on-device training and server-based aggregation to train a common ML model among distributed agents. In this work, we propose an asynchronous FL design with periodic aggregation to tackle the straggler issue in FL systems. Considering limited wireless communication resources, we investigate the effect of different scheduling policies and aggregation designs on the convergence performance. Driven by the importance of reducing the bias and variance of the aggregated model updates, we propose a scheduling policy that jointly considers the channel quality and training data representation of user devices. The effectiveness of our channel-aware data-importance-based scheduling policy, compared with state-of-the-art methods proposed for synchronous FL, is validated through simulations. Moreover, we show that an ``age-aware'' aggregation weighting design can significantly improve the learning performance i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MELD-FAIR&#26469;&#35299;&#20915;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20351;&#29992;&#20027;&#21160;&#35828;&#35805;&#32773;&#26816;&#27979;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#37325;&#26032;&#23545;&#40784;&#20102;MELD&#35270;&#39057;&#65292;&#24182;&#25104;&#21151;&#25429;&#33719;&#20102;&#35762;&#35805;&#32773;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;</title><link>http://arxiv.org/abs/2211.15377</link><description>&lt;p&gt;
&#35841;&#30340;&#24773;&#32490;&#26356;&#37325;&#35201;&#65311;&#27809;&#26377;&#20808;&#21069;&#30693;&#35782;&#30340;&#35828;&#35805;&#27963;&#21160;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whose Emotion Matters? Speaking Activity Localisation without Prior Knowledge. (arXiv:2211.15377v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MELD-FAIR&#26469;&#35299;&#20915;&#24773;&#24863;&#35782;&#21035;&#20013;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#20351;&#29992;&#20027;&#21160;&#35828;&#35805;&#32773;&#26816;&#27979;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#37325;&#26032;&#23545;&#40784;&#20102;MELD&#35270;&#39057;&#65292;&#24182;&#25104;&#21151;&#25429;&#33719;&#20102;&#35762;&#35805;&#32773;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#30340;&#20219;&#21153;&#21463;&#30410;&#20110;&#22810;&#31181;&#27169;&#24577;&#30340;&#21487;&#29992;&#24615;&#65292;&#20363;&#22914;&#22312;&#22522;&#20110;&#35270;&#39057;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#32447;&#25968;&#25454;&#38598;&#65288;MELD&#65289;&#20013;&#25552;&#20379;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#26041;&#27861;&#20351;&#29992;&#20102;MELD&#35270;&#39057;&#20013;&#30340;&#22768;&#23398;&#21644;&#35270;&#35273;&#20449;&#24687;&#12290;&#36825;&#26377;&#20004;&#20010;&#21407;&#22240;&#65306;&#39318;&#20808;&#65292;MELD&#20013;&#30340;&#26631;&#31614;&#21040;&#35270;&#39057;&#30340;&#23545;&#40784;&#26159;&#26377;&#22122;&#22768;&#30340;&#65292;&#36825;&#20351;&#24471;&#37027;&#20123;&#35270;&#39057;&#25104;&#20026;&#20102;&#24773;&#24863;&#35821;&#38899;&#25968;&#25454;&#30340;&#19981;&#21487;&#38752;&#26469;&#28304;&#12290;&#20854;&#27425;&#65292;&#20250;&#35805;&#21487;&#20197;&#28041;&#21450;&#21040;&#21516;&#19968;&#22330;&#26223;&#20013;&#30340;&#20960;&#20010;&#20154;&#65292;&#36825;&#38656;&#35201;&#23450;&#20301;&#35805;&#35821;&#26469;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26368;&#36817;&#30340;&#20027;&#21160;&#35828;&#35805;&#32773;&#26816;&#27979;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#24102;&#26377;&#22266;&#23450;&#38899;&#39057;&#35270;&#35273;&#20449;&#24687;&#30340;MELD-FAIR&#65292;&#33021;&#22815;&#37325;&#26032;&#23545;&#40784;MELD&#35270;&#39057;&#24182;&#25429;&#33719;96.92&#65285;&#30340;MELD&#20013;&#25552;&#20379;&#30340;&#35805;&#35821;&#30340;&#35762;&#35805;&#32773;&#38754;&#37096;&#34920;&#24773;&#12290;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#22768;&#38899;&#35782;&#21035;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37325;&#26032;&#23545;&#40784;&#30340;MELD-FAIR&#35270;&#39057;&#26356;&#28165;&#26224;&#22320;&#26174;&#31034;&#20102;&#35762;&#35805;&#32773;&#30340;&#38754;&#37096;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of emotion recognition in conversations (ERC) benefits from the availability of multiple modalities, as provided, for example, in the video-based Multimodal EmotionLines Dataset (MELD). However, only a few research approaches use both acoustic and visual information from the MELD videos. There are two reasons for this: First, label-to-video alignments in MELD are noisy, making those videos an unreliable source of emotional speech data. Second, conversations can involve several people in the same scene, which requires the localisation of the utterance source. In this paper, we introduce MELD with Fixed Audiovisual Information via Realignment (MELD-FAIR) by using recent active speaker detection and automatic speech recognition models, we are able to realign the videos of MELD and capture the facial expressions from speakers in 96.92% of the utterances provided in MELD. Experiments with a self-supervised voice recognition model indicate that the realigned MELD-FAIR videos more cl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22478;&#24066;&#22320;&#22270;&#19978;&#26381;&#21153;&#20110;&#38543;&#26426;&#20986;&#29616;&#30340;&#35831;&#27714;&#65292;&#21487;&#20197;&#20135;&#29983;&#21327;&#35843;&#20316;&#29992;&#24182;&#32771;&#34385;&#20808;&#21069;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#26469;&#35831;&#27714;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#38656;&#27714;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.14983</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#36335;&#30001;&#21644;&#25509;&#36865;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21487;&#36866;&#24212;&#38656;&#27714;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multiagent Reinforcement Learning for Autonomous Routing and Pickup Problem with Adaptation to Variable Demand. (arXiv:2211.14983v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22478;&#24066;&#22320;&#22270;&#19978;&#26381;&#21153;&#20110;&#38543;&#26426;&#20986;&#29616;&#30340;&#35831;&#27714;&#65292;&#21487;&#20197;&#20135;&#29983;&#21327;&#35843;&#20316;&#29992;&#24182;&#32771;&#34385;&#20808;&#21069;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#26469;&#35831;&#27714;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#38656;&#27714;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#19968;&#32452;&#33258;&#20027;&#36710;&#36742;&#22312;&#22478;&#24066;&#22320;&#22270;&#19978;&#26381;&#21153;&#20110;&#38543;&#26426;&#20986;&#29616;&#30340;&#35831;&#27714;&#26102;&#29983;&#25104;&#36335;&#30001;/&#25509;&#36865;&#31574;&#30053;&#12290;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#30340;&#31574;&#30053;&#26159;&#65306;1&#65289;&#20135;&#29983;&#21327;&#35843;&#20316;&#29992;&#65292;&#20174;&#32780;&#20943;&#23569;&#20026;&#26381;&#21153;&#35831;&#27714;&#31561;&#24453;&#30340;&#26102;&#38388;&#65307;2&#65289;&#26159;&#38750;&#36817;&#35270;&#31574;&#30053;&#65292;&#24182;&#32771;&#34385;&#20808;&#21069;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#26469;&#35831;&#27714;&#65307;3&#65289;&#21487;&#20197;&#36866;&#24212;&#22522;&#30784;&#38656;&#27714;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#31574;&#30053;&#26159;&#36866;&#24212;&#22478;&#24066;&#29615;&#22659;&#20013;&#23454;&#38469;&#38656;&#27714;&#26465;&#20214;&#30340;&#27874;&#21160;&#65292;&#20363;&#22914;&#39640;&#23792;&#26102;&#38388;&#21644;&#38750;&#39640;&#23792;&#26102;&#38388;&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23454;&#29616;&#65306;(i)&#33021;&#22815;&#25913;&#36827;&#31163;&#32447;&#35757;&#32451;&#31574;&#30053;&#24615;&#33021;&#30340;&#22312;&#32447;&#29609;&#31639;&#27861;&#65292;&#21644;(ii)&#19968;&#31181;&#31163;&#32447;&#36924;&#36817;&#26041;&#26696;&#65292;&#20801;&#35768;&#36866;&#24212;&#22522;&#20110;&#38656;&#27714;&#27169;&#22411;&#30340;&#21464;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;Wasserstein&#36317;&#31163;&#30340;q-valid&#21322;&#24452;&#26469;&#37327;&#21270;&#26377;&#25928;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25105;&#20204;&#24050;&#23398;&#20064;&#31574;&#30053;&#23545;&#19981;&#21516;&#38656;&#27714;&#20998;&#24067;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#27604;&#22522;&#32447;&#21551;&#21457;&#24335;&#31574;&#30053;&#25913;&#21892;&#24179;&#22343;&#31561;&#24453;&#26102;&#38388;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#38656;&#27714;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive a learning framework to generate routing/pickup policies for a fleet of autonomous vehicles tasked with servicing stochastically appearing requests on a city map. We focus on policies that 1) give rise to coordination amongst the vehicles, thereby reducing wait times for servicing requests, 2) are non-myopic, and consider a-priori potential future requests, 3) can adapt to changes in the underlying demand distribution. Specifically, we are interested in policies that are adaptive to fluctuations of actual demand conditions in urban environments, such as on-peak vs. off-peak hours. We achieve this through a combination of (i) an online play algorithm that improves the performance of an offline-trained policy, and (ii) an offline approximation scheme that allows for adapting to changes in the underlying demand model. In particular, we achieve adaptivity of our learned policy to different demand distributions by quantifying a region of validity using the q-valid radius of a Wass
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#24182;&#23558;&#20854;&#20248;&#21270;&#20026;&#25439;&#22833;&#20540;&#21644;&#25439;&#22833;&#38160;&#24230;&#65292;SAMSON&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#22122;&#22768;&#30828;&#20214;&#30340;&#25512;&#26029;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20851;&#20110;&#30446;&#26631;&#30828;&#20214;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2211.11561</link><description>&lt;p&gt;
SAMSON&#65306;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#38382;&#39064;&#30340;&#24322;&#24120;&#24402;&#19968;&#21270;&#23610;&#24230;&#19979;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAMSON: Sharpness-Aware Minimization Scaled by Outlier Normalization for Improving DNN Generalization and Robustness. (arXiv:2211.11561v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11561
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#24182;&#23558;&#20854;&#20248;&#21270;&#20026;&#25439;&#22833;&#20540;&#21644;&#25439;&#22833;&#38160;&#24230;&#65292;SAMSON&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#22122;&#22768;&#30828;&#20214;&#30340;&#25512;&#26029;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20851;&#20110;&#30446;&#26631;&#30828;&#20214;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#25928;&#36739;&#39640;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21152;&#36895;&#22120;&#23481;&#26131;&#20986;&#29616;&#38750;&#29702;&#24819;&#24773;&#20917;&#65292;&#20174;&#32780;&#38477;&#20302;DNN&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#35757;&#32451;&#26399;&#38388;&#21521;DNN&#26435;&#37325;&#28155;&#21152;&#25200;&#21160;&#65292;&#20197;&#27169;&#25311;&#22122;&#22768;&#30828;&#20214;&#19978;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#20851;&#20110;&#30446;&#26631;&#30828;&#20214;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#20250;&#23548;&#33268;&#22312;DNN&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#38477;&#20302;&#21069;&#32773;&#20197;&#25552;&#39640;&#21518;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24212;&#29992;&#38160;&#24230;&#24863;&#30693;&#35757;&#32451;&#65292;&#22312;&#20248;&#21270;&#25439;&#22833;&#20540;&#21644;&#25439;&#22833;&#38160;&#24230;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#22122;&#22768;&#30828;&#20214;&#30340;&#25512;&#26029;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#26377;&#20851;&#30446;&#26631;&#30828;&#20214;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#38160;&#24230;&#24863;&#30693;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#23558;&#32473;&#23450;&#26435;&#37325;&#30340;&#26368;&#22351;&#24773;&#20917;&#25200;&#21160;&#21462;&#20915;&#20110;&#20854;&#22823;&#23567;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26435;&#37325;&#20998;&#24067;&#30340;&#33539;&#22260;&#12290;&#36825;&#26159;&#36890;&#36807;&#25191;&#34892;&#22312;&#24322;&#24120;&#20540;&#24402;&#19968;&#21270;&#23610;&#24230;&#19979;&#30340;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26469;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient deep neural network (DNN) accelerators are prone to non-idealities that degrade DNN performance at inference time. To mitigate such degradation, existing methods typically add perturbations to the DNN weights during training to simulate inference on noisy hardware. However, this often requires knowledge about the target hardware and leads to a trade-off between DNN performance and robustness, decreasing the former to increase the latter. In this work, we show that applying sharpness-aware training, by optimizing for both the loss value and loss sharpness, significantly improves robustness to noisy hardware at inference time without relying on any assumptions about the target hardware. In particular, we propose a new adaptive sharpness-aware method that conditions the worst-case perturbation of a given weight not only on its magnitude but also on the range of the weight distribution. This is achieved by performing sharpness-aware minimization scaled by outlier minimizat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#8212;&#8212;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#65288;ConvGNP&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25918;&#32622;&#25928;&#29575;&#12290;ConvGNP&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#32852;&#21512;&#39640;&#26031;&#20998;&#24067;&#65292;&#36890;&#36807;&#23398;&#20064;&#31354;&#38388;&#21644;&#23395;&#33410;&#24615;&#38750;&#24179;&#31283;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38750;&#24179;&#31283;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.10381</link><description>&lt;p&gt;
&#24102;&#26377;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#30340;&#29615;&#22659;&#20256;&#24863;&#22120;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Environmental Sensor Placement with Convolutional Gaussian Neural Processes. (arXiv:2211.10381v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#8212;&#8212;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#65288;ConvGNP&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25918;&#32622;&#25928;&#29575;&#12290;ConvGNP&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#32852;&#21512;&#39640;&#26031;&#20998;&#24067;&#65292;&#36890;&#36807;&#23398;&#20064;&#31354;&#38388;&#21644;&#23395;&#33410;&#24615;&#38750;&#24179;&#31283;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38750;&#24179;&#31283;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#20256;&#24863;&#22120;&#23545;&#20110;&#30417;&#27979;&#22825;&#27668;&#21644;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#21335;&#26497;&#36825;&#26679;&#30340;&#20559;&#36828;&#22320;&#21306;&#65292;&#26368;&#22823;&#21270;&#27979;&#37327;&#20449;&#24687;&#21644;&#26377;&#25928;&#25918;&#32622;&#20256;&#24863;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#26032;&#20256;&#24863;&#22120;&#25552;&#20379;&#30340;&#19981;&#30830;&#23450;&#24615;&#20943;&#23569;&#26469;&#35780;&#20272;&#25918;&#32622;&#20449;&#24687;&#12290;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#20294;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#38750;&#24179;&#31283;&#34892;&#20026;&#24182;&#32553;&#25918;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#65288;ConvGNP&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;ConvGNP&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#20219;&#24847;&#30446;&#26631;&#20301;&#32622;&#30340;&#32852;&#21512;&#39640;&#26031;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20351;&#29992;&#27169;&#25311;&#30340;&#21335;&#26497;&#22320;&#21306;&#22320;&#38754;&#28201;&#24230;&#24322;&#24120;&#20316;&#20026;&#30495;&#23454;&#25968;&#25454;&#65292;ConvGNP&#23398;&#20064;&#20102;&#31354;&#38388;&#21644;&#23395;&#33410;&#24615;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#20248;&#20110;&#38750;&#24179;&#31283;GP&#22522;&#32447;&#12290;&#22312;&#27169;&#25311;&#30340;s&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Environmental sensors are crucial for monitoring weather conditions and the impacts of climate change. However, it is challenging to maximise measurement informativeness and place sensors efficiently, particularly in remote regions like Antarctica. Probabilistic machine learning models can evaluate placement informativeness by predicting the uncertainty reduction provided by a new sensor. Gaussian process (GP) models are widely used for this purpose, but they struggle with capturing complex non-stationary behaviour and scaling to large datasets. This paper proposes using a convolutional Gaussian neural process (ConvGNP) to address these issues. A ConvGNP uses neural networks to parameterise a joint Gaussian distribution at arbitrary target locations, enabling flexibility and scalability. Using simulated surface air temperature anomaly over Antarctica as ground truth, the ConvGNP learns spatial and seasonal non-stationarities, outperforming a non-stationary GP baseline. In a simulated s
&lt;/p&gt;</description></item><item><title>DeepSense 6G&#26159;&#19968;&#20010;&#22823;&#22411;&#30495;&#23454;&#19990;&#30028;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#36890;&#20449;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#21160;&#22312;&#22810;&#27169;&#24577;&#24863;&#30693;&#12289;&#36890;&#20449;&#21644;&#23450;&#20301;&#20132;&#21449;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.09769</link><description>&lt;p&gt;
DeepSense 6G&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#36890;&#20449;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
DeepSense 6G: A Large-Scale Real-World Multi-Modal Sensing and Communication Dataset. (arXiv:2211.09769v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09769
&lt;/p&gt;
&lt;p&gt;
DeepSense 6G&#26159;&#19968;&#20010;&#22823;&#22411;&#30495;&#23454;&#19990;&#30028;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#36890;&#20449;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#21160;&#22312;&#22810;&#27169;&#24577;&#24863;&#30693;&#12289;&#36890;&#20449;&#21644;&#23450;&#20301;&#20132;&#21449;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DeepSense 6G&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20849;&#23384;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#36890;&#20449;&#25968;&#25454;&#30340;&#30495;&#23454;&#19990;&#30028;&#27979;&#37327;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;DeepSense 6G&#25968;&#25454;&#38598;&#26088;&#22312;&#25512;&#21160;&#22312;&#22810;&#27169;&#24577;&#24863;&#30693;&#12289;&#36890;&#20449;&#21644;&#23450;&#20301;&#20132;&#21449;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;DeepSense&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#12289;&#37319;&#29992;&#30340;&#27979;&#35797;&#24179;&#21488;&#12289;&#25968;&#25454;&#25910;&#38598;&#21644;&#22788;&#29702;&#26041;&#27861;&#12289;&#37096;&#32626;&#22330;&#26223;&#20197;&#21450;&#31034;&#20363;&#24212;&#29992;&#65292;&#26088;&#22312;&#20419;&#36827;&#22810;&#27169;&#24577;&#24863;&#30693;&#21644;&#36890;&#20449;&#25968;&#25454;&#38598;&#30340;&#37319;&#29992;&#21644;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents the DeepSense 6G dataset, which is a large-scale dataset based on real-world measurements of co-existing multi-modal sensing and communication data. The DeepSense 6G dataset is built to advance deep learning research in a wide range of applications in the intersection of multi-modal sensing, communication, and positioning. This article provides a detailed overview of the DeepSense dataset structure, adopted testbeds, data collection and processing methodology, deployment scenarios, and example applications, with the objective of facilitating the adoption and reproducibility of multi-modal sensing and communication datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#38024;&#23545;&#22810;&#20013;&#24515;&#33041;&#40836;&#39044;&#27979;&#22238;&#24402;&#38382;&#39064;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#39640;&#25968;&#25454;&#21644;&#26631;&#31614;&#30340;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;MRI&#25195;&#25551;&#19979;&#33041;&#40836;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#21644;&#26368;&#20339;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.08326</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22810;&#20013;&#24515;&#33041;&#40836;&#39044;&#27979;&#22238;&#24402;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning for regression in multi-site brain age prediction. (arXiv:2211.08326v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#38024;&#23545;&#22810;&#20013;&#24515;&#33041;&#40836;&#39044;&#27979;&#22238;&#24402;&#38382;&#39064;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#39640;&#25968;&#25454;&#21644;&#26631;&#31614;&#30340;&#31283;&#20581;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;MRI&#25195;&#25551;&#19979;&#33041;&#40836;&#39044;&#27979;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#21644;&#26368;&#20339;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#24433;&#20687;&#23398;&#20013;&#65292;&#26500;&#24314;&#20934;&#30830;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#33041;&#40836;&#39044;&#27979;&#26159;&#19968;&#20010;&#38750;&#24120;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#24182;&#23547;&#25214;&#26032;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20026;&#20102;&#20272;&#35745;&#20934;&#30830;&#19988;&#21487;&#27867;&#21270;&#30340;&#27169;&#22411;&#65292;&#24050;&#32463;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#36890;&#24120;&#26159;&#22810;&#20013;&#24515;&#21644;&#22810;&#25195;&#25551;&#22120;&#30340;&#12290;&#36825;&#31181;&#22823;&#35268;&#27169;&#24322;&#26500;&#24615;&#20250;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#19982;&#29305;&#23450;&#22330;&#22320;&#30456;&#20851;&#30340;&#22122;&#38899;&#12290;&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#22312;&#25968;&#25454;&#25110;&#26631;&#31614;&#22122;&#22768;&#26041;&#38754;&#26356;&#20026;&#31283;&#20581;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#22238;&#24402;&#25439;&#22833;&#65292;&#29992;&#20110;MRI&#25195;&#25551;&#30340;&#33041;&#40836;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;OpenBHB&#25361;&#25112;&#36187;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#26368;&#20339;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25239;&#22330;&#22320;&#30456;&#20851;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building accurate Deep Learning (DL) models for brain age prediction is a very relevant topic in neuroimaging, as it could help better understand neurodegenerative disorders and find new biomarkers. To estimate accurate and generalizable models, large datasets have been collected, which are often multi-site and multi-scanner. This large heterogeneity negatively affects the generalization performance of DL models since they are prone to overfit site-related noise. Recently, contrastive learning approaches have been shown to be more robust against noise in data or labels. For this reason, we propose a novel contrastive learning regression loss for robust brain age prediction using MRI scans. Our method achieves state-of-the-art performance on the OpenBHB challenge, yielding the best generalization capability and robustness to site-related noise.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#26377;&#36164;&#28304;&#32447;&#24615;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#36739;&#20302;&#30340;&#21518;&#24724;&#12290;&#27492;&#22806;&#65292;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#31639;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.07484</link><description>&lt;p&gt;
&#24102;&#35013;&#36733;&#21644;&#35206;&#30422;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#65306;&#22522;&#20110;&#22238;&#24402;&#30340;&#27169;&#22359;&#21270;Lagrangian&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression. (arXiv:2211.07484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#26377;&#36164;&#28304;&#32447;&#24615;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#36739;&#20302;&#30340;&#21518;&#24724;&#12290;&#27492;&#22806;&#65292;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#31639;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#20854;&#20013;&#31639;&#27861;&#22312;&#24635;&#28040;&#36153;&#30340;&#32447;&#24615;&#32422;&#26463;&#19979;&#20351;&#29992;&#22810;&#20010;&#36164;&#28304;&#12290;&#36825;&#20010;&#38382;&#39064;&#25512;&#24191;&#20102;&#24102;&#32972;&#21253;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;(CBwK)&#65292;&#20801;&#35768;&#35013;&#36733;&#21644;&#35206;&#30422;&#32422;&#26463;&#65292;&#20197;&#21450;&#27491;&#36127;&#36164;&#28304;&#28040;&#32791;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#33021;&#22815;&#23454;&#29616;&#36864;&#21270;&#30340;&#21518;&#24724;&#12290;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#23545;&#20110;CBwK&#65292;&#23427;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;LagrangianBwK(Immorlica&#31561;&#20154;&#65292;FOCS 2019)&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;CBwK&#30340;Lagrangian&#25216;&#26415;&#65292;&#20197;&#21450;SquareCB(Foster&#21644;Rakhlin&#65292;ICML 2020)&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#30340;&#22238;&#24402;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21033;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#26412;&#36136;&#19978;&#30340;&#27169;&#22359;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a variant of contextual bandits in which the algorithm consumes multiple resources subject to linear constraints on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK), allowing for packing and covering constraints, as well as positive and negative resource consumption. We present a new algorithm that is simple, computationally efficient, and admits vanishing regret. It is statistically optimal for CBwK when an algorithm must stop once some constraint is violated. Our algorithm builds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based technique for contextual bandits. Our analysis leverages the inherent modularity of both techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;MixMask&#65292;&#22312;Siamese ConvNets&#20013;&#23454;&#29616;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.11456</link><description>&lt;p&gt;
MixMask: &#37325;&#26032;&#23457;&#35270;Siamese ConvNets&#30340;&#36974;&#30422;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
MixMask: Revisiting Masking Strategy for Siamese ConvNets. (arXiv:2210.11456v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;MixMask&#65292;&#22312;Siamese ConvNets&#20013;&#23454;&#29616;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36827;&#23637;&#23558;Masked Image Modeling&#65288;MIM&#65289;&#21644;Siamese&#32593;&#32476;&#25972;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;Siamese ConvNets&#20013;&#24212;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;&#25830;&#38500;&#30340;&#36974;&#30422;&#31574;&#30053;&#26102;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#65288;I&#65289;&#22312;&#36830;&#32493;&#22788;&#29702;&#25968;&#25454;&#26102;&#19981;&#33021;&#25918;&#24323;&#19981;&#30456;&#20851;&#30340;&#36974;&#30422;&#21306;&#22495;&#65292;&#23548;&#33268;&#35757;&#32451;&#25928;&#29575;&#20302;&#20110;ViT&#27169;&#22411;;&#65288;II&#65289;&#22522;&#20110;&#25830;&#38500;&#30340;&#36974;&#30422;&#19982;Siamese ConvNets&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19981;&#21305;&#37197;&#65292;&#19982;MIM&#26041;&#27861;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixMask&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#39321;&#33609;&#36974;&#30422;&#26041;&#27861;&#20013;&#22270;&#20687;&#20013;&#30340;&#38543;&#26426;&#36974;&#30422;&#21306;&#22495;&#23548;&#33268;&#20449;&#24687;&#19981;&#23436;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#32771;&#34385;&#20004;&#20010;&#19981;&#21516;&#28151;&#21512;&#35270;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#21464;&#21270;&#65292;&#20197;&#36866;&#24212;&#38598;&#25104;&#26550;&#26500;&#24182;&#38450;&#27490;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MixMask&#26174;&#30528;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning have integrated Masked Image Modeling (MIM) and Siamese Networks into a unified framework that leverages the benefits of both techniques. However, several issues remain unaddressed when applying conventional erase-based masking with Siamese ConvNets. These include (I) the inability to drop uninformative masked regions in ConvNets as they process data continuously, resulting in low training efficiency compared to ViT models; and (II) the mismatch between erase-based masking and the contrastive-based objective in Siamese ConvNets, which differs from the MIM approach. In this paper, we propose a filling-based masking strategy called MixMask to prevent information incompleteness caused by the randomly erased regions in an image in the vanilla masking method. Furthermore, we introduce a flexible loss function design that considers the semantic distance change between two different mixed views to adapt the integrated architecture and prevent mismat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26234;&#33021;&#25216;&#26415;&#20013;&#26085;&#30410;&#26222;&#36941;&#30340;&#25991;&#26412;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#31867;&#21035;&#65306;&#38544;&#34109;&#19981;&#23433;&#20840;&#25991;&#26412;&#12290;&#35813;&#25991;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#31995;&#32479;&#20869;&#37096;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09306</link><description>&lt;p&gt;
&#32531;&#35299;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#20013;&#38544;&#34109;&#19981;&#23433;&#20840;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Mitigating Covertly Unsafe Text within Natural Language Systems. (arXiv:2210.09306v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26234;&#33021;&#25216;&#26415;&#20013;&#26085;&#30410;&#26222;&#36941;&#30340;&#25991;&#26412;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#31867;&#21035;&#65306;&#38544;&#34109;&#19981;&#23433;&#20840;&#25991;&#26412;&#12290;&#35813;&#25991;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#26234;&#33021;&#31995;&#32479;&#20869;&#37096;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25216;&#26415;&#20013;&#19968;&#20010;&#26085;&#30410;&#26222;&#36941;&#30340;&#38382;&#39064;&#26159;&#25991;&#26412;&#23433;&#20840;&#24615;&#65292;&#22240;&#20026;&#19981;&#21463;&#25511;&#21046;&#30340;&#31995;&#32479;&#21487;&#33021;&#20250;&#21521;&#29992;&#25143;&#29983;&#25104;&#23548;&#33268;&#20260;&#23475;&#25110;&#23041;&#32961;&#29983;&#21629;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#23548;&#33268;&#36523;&#20307;&#20260;&#23475;&#30340;&#29983;&#25104;&#35821;&#21477;&#30340;&#26126;&#30830;&#31243;&#24230;&#19981;&#21516;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#21487;&#33021;&#23548;&#33268;&#36523;&#20307;&#20260;&#23475;&#30340;&#25991;&#26412;&#31867;&#22411;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#23588;&#20854;&#26410;&#34987;&#25506;&#32034;&#30340;&#31867;&#21035;&#65306;&#38544;&#34109;&#19981;&#23433;&#20840;&#25991;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#35299;&#20102;&#36825;&#20010;&#31867;&#21035;&#24182;&#20998;&#26512;&#20102;&#27599;&#20010;&#23567;&#31867;&#21035;&#20013;&#25991;&#26412;&#30340;&#29983;&#25104;&#26041;&#24335;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23450;&#20041;&#20102;&#23548;&#33268;&#29289;&#29702;&#20260;&#23475;&#30340;&#38544;&#34109;&#19981;&#23433;&#20840;&#35821;&#35328;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#36825;&#20010;&#24494;&#22937;&#20294;&#21361;&#38505;&#30340;&#38382;&#39064;&#38656;&#35201;&#25104;&#20026;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#30417;&#31649;&#26426;&#26500;&#30340;&#20248;&#20808;&#32771;&#34385;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#21551;&#21457;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#24110;&#21161;&#25552;&#39640;&#26234;&#33021;&#31995;&#32479;&#20869;&#37096;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
An increasingly prevalent problem for intelligent technologies is text safety, as uncontrolled systems may generate recommendations to their users that lead to injury or life-threatening consequences. However, the degree of explicitness of a generated statement that can cause physical harm varies. In this paper, we distinguish types of text that can lead to physical harm and establish one particularly underexplored category: covertly unsafe text. Then, we further break down this category with respect to the system's information and discuss solutions to mitigate the generation of text in each of these subcategories. Ultimately, our work defines the problem of covertly unsafe language that causes physical harm and argues that this subtle yet dangerous issue needs to be prioritized by stakeholders and regulators. We highlight mitigation strategies to inspire future researchers to tackle this challenging problem and help improve safety within smart systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#23457;&#35745;&#36825;&#20010;&#36807;&#31243;&#65292;&#20197;&#30830;&#20445;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#24471;&#21040;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2210.09126</link><description>&lt;p&gt;
&#21487;&#39564;&#35777;&#19988;&#20855;&#26377;&#35777;&#26126;&#23433;&#20840;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Verifiable and Provably Secure Machine Unlearning. (arXiv:2210.09126v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#23457;&#35745;&#36825;&#20010;&#36807;&#31243;&#65292;&#20197;&#30830;&#20445;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#24471;&#21040;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;&#26088;&#22312;&#22312;&#35757;&#32451;&#21518;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31227;&#38500;&#26576;&#20123;&#28857;&#65307;&#20363;&#22914;&#24403;&#29992;&#25143;&#35831;&#27714;&#21024;&#38500;&#25968;&#25454;&#26102;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;&#65292;&#20294;&#26159;&#27809;&#26377;&#19968;&#31181;&#31639;&#27861;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#23457;&#35745;&#36825;&#20010;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#25143;&#26080;&#27861;&#36890;&#36807;&#26816;&#26597;&#27169;&#22411;&#26412;&#36523;&#26469;&#39564;&#35777;&#20854;&#25968;&#25454;&#26159;&#21542;&#24050;&#34987;&#21024;&#38500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#19981;&#26159;&#32771;&#34385;&#27169;&#22411;&#21442;&#25968;&#65292;&#32780;&#26159;&#23558;&#21487;&#39564;&#35777;&#30340;&#31639;&#27861;&#35270;&#20026;&#19968;&#31181;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#39564;&#35777;&#21435;&#38500;&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#21152;&#23494;&#23450;&#20041;&#65292;&#20197;&#27491;&#24335;&#25429;&#25417;&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;&#31995;&#32479;&#30340;&#20445;&#35777;&#12290;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26381;&#21153;&#22120;&#39318;&#20808;&#35745;&#31639;&#19968;&#20010;&#35777;&#26126;&#65292;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598; $D$ &#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#32473;&#23450;&#19968;&#20010;&#35201;&#21024;&#38500;&#30340;&#29992;&#25143;&#25968;&#25454;&#28857; $d$&#65292;&#26381;&#21153;&#22120;&#20351;&#29992;&#21435;&#38500;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#12290;&#28982;&#21518;&#23427;&#25552;&#20379;&#27491;&#30830;&#25191;&#34892;&#21435;&#38500;&#31639;&#27861;&#24182;&#19988; $d \notin D'$ &#30340;&#35777;&#26126;&#65292;&#20854;&#20013; $D'$ &#26159;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning aims to remove points from the training dataset of a machine learning model after training; for example when a user requests their data to be deleted. While many machine unlearning methods have been proposed, none of them enable users to audit the procedure. Furthermore, recent work shows a user is unable to verify if their data was unlearnt from an inspection of the model alone. Rather than reasoning about model parameters, we propose to view verifiable unlearning as a security problem. To this end, we present the first cryptographic definition of verifiable unlearning to formally capture the guarantees of a machine unlearning system. In this framework, the server first computes a proof that the model was trained on a dataset $D$. Given a user data point $d$ requested to be deleted, the server updates the model using an unlearning algorithm. It then provides a proof of the correct execution of unlearning and that $d \notin D'$, where $D'$ is the new training dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#31639;&#27861;&#25928;&#29575;&#39640;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#38382;&#39064;&#24182;&#21046;&#23450;&#20102;&#20998;&#31867;&#27861;&#12290;&#20998;&#31867;&#27861;&#31361;&#26174;&#20102;&#30475;&#20284;&#19981;&#21516;&#26041;&#27861;&#30340;&#20849;&#21516;&#28857;&#65292;&#24182;&#25581;&#31034;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26368;&#20339;&#23454;&#36341;&#20197;&#23454;&#29616;&#20840;&#38754;&#12289;&#20844;&#27491;&#21644;&#21487;&#38752;&#30340;&#36895;&#24230;&#25552;&#21319;&#25216;&#26415;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#35757;&#32451;&#27969;&#31243;&#20013;&#24120;&#35265;&#30340;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2210.06640</link><description>&lt;p&gt;
&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#31639;&#27861;&#36235;&#21183;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities. (arXiv:2210.06640v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#31639;&#27861;&#25928;&#29575;&#39640;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#38382;&#39064;&#24182;&#21046;&#23450;&#20102;&#20998;&#31867;&#27861;&#12290;&#20998;&#31867;&#27861;&#31361;&#26174;&#20102;&#30475;&#20284;&#19981;&#21516;&#26041;&#27861;&#30340;&#20849;&#21516;&#28857;&#65292;&#24182;&#25581;&#31034;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26368;&#20339;&#23454;&#36341;&#20197;&#23454;&#29616;&#20840;&#38754;&#12289;&#20844;&#27491;&#21644;&#21487;&#38752;&#30340;&#36895;&#24230;&#25552;&#21319;&#25216;&#26415;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#35757;&#32451;&#27969;&#31243;&#20013;&#24120;&#35265;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25152;&#20135;&#29983;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#25104;&#26412;&#24050;&#32463;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#23545;&#35757;&#32451;&#31243;&#24207;&#35821;&#20041;&#30340;&#21464;&#21270;&#26469;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#21363;&#31639;&#27861;&#25928;&#29575;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#26412;&#25991;&#23545;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#31995;&#32479;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;"&#31639;&#27861;&#21152;&#36895;"&#38382;&#39064;&#36827;&#34892;&#20102;&#27491;&#24335;&#21270;&#65292;&#28982;&#21518;&#21033;&#29992;&#31639;&#27861;&#25928;&#29575;&#39640;&#35757;&#32451;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#26469;&#21046;&#23450;&#20102;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#31361;&#26174;&#20102;&#30475;&#20284;&#19981;&#21516;&#26041;&#27861;&#30340;&#20849;&#21516;&#28857;&#65292;&#24182;&#25581;&#31034;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#35780;&#20272;&#26368;&#20339;&#23454;&#36341;&#20197;&#23454;&#29616;&#20840;&#38754;&#12289;&#20844;&#27491;&#21644;&#21487;&#38752;&#30340;&#36895;&#24230;&#25552;&#21319;&#25216;&#26415;&#27604;&#36739;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#24110;&#21161;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35757;&#32451;&#27969;&#31243;&#20013;&#24120;&#35265;&#30340;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep learning has made great progress in recent years, the exploding economic and environmental costs of training neural networks are becoming unsustainable. To address this problem, there has been a great deal of research on *algorithmically-efficient deep learning*, which seeks to reduce training costs not at the hardware or implementation level, but through changes in the semantics of the training program. In this paper, we present a structured and comprehensive overview of the research in this field. First, we formalize the *algorithmic speedup* problem, then we use fundamental building blocks of algorithmically efficient training to develop a taxonomy. Our taxonomy highlights commonalities of seemingly disparate methods and reveals current research gaps. Next, we present evaluation best practices to enable comprehensive, fair, and reliable comparisons of speedup techniques. To further aid research and applications, we discuss common bottlenecks in the training pipeline (i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#21644;&#29983;&#25104;&#26032;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#20102;&#21069;&#20154;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#20986;&#19968;&#22871;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2210.04366</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#20307;&#21160;&#20316;&#21512;&#25104;&#36827;&#34892;&#35745;&#31639;&#32534;&#33310;
&lt;/p&gt;
&lt;p&gt;
Computational Choreography using Human Motion Synthesis. (arXiv:2210.04366v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#21644;&#29983;&#25104;&#26032;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#20102;&#21069;&#20154;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#20986;&#19968;&#22871;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#24212;&#35813;&#34987;&#35757;&#32451;&#26469;&#20998;&#26512;&#20154;&#20307;&#34920;&#28436;&#33402;&#26415;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21512;&#25104;&#33402;&#26415;&#20154;&#20307;&#21160;&#20316;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#20013;&#30340;&#38382;&#39064;&#20219;&#21153;&#21253;&#25324;&#39044;&#27979;&#37326;&#22806;&#29615;&#22659;&#20013;&#20154;&#20307;&#36816;&#21160;&#65292;&#20197;&#21450;&#29983;&#25104;&#22522;&#20110;&#36825;&#20123;&#39044;&#27979;&#30340;&#26032;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#23558;&#35752;&#35770;&#19968;&#20010;&#38750;&#20256;&#32479;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#21363;&#23558;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#33310;&#36424;&#21160;&#20316;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#26174;&#33879;&#30340;&#21162;&#21147;&#65292;&#20197;&#35745;&#31639;&#30340;&#26041;&#24335;&#20998;&#26512;&#33310;&#36424;&#21160;&#20316;&#65292;&#20363;&#22914;Everybody Dance Now&#65288;EDN&#65289;&#23398;&#20064;&#27169;&#22411;&#21644;Cal Poly&#30805;&#22763;&#35770;&#25991;Take The Lead&#65288;TTL&#65289;&#12290;&#25105;&#20204;&#26377;&#25928;&#22320;&#23558;&#36825;&#20004;&#20010;&#20316;&#21697;&#19982;&#25105;&#20204;&#33258;&#24049;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#20102;&#19968;&#31181;&#26032;&#30340;&#33310;&#36424;&#21160;&#20316;&#39044;&#27979;&#31995;&#32479;&#12289;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#21644;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Should deep learning models be trained to analyze human performance art? To help answer this question, we explore an application of deep neural networks to synthesize artistic human motion. Problem tasks in human motion synthesis can include predicting the motions of humans in-the-wild, as well as generating new sequences of motions based on said predictions. We will discuss the potential of a less traditional application, where learning models are applied to predicting dance movements. There have been notable, recent efforts to analyze dance movements in a computational light, such as the Everybody Dance Now (EDN) learning model and a Cal Poly master's thesis, Take The Lead (TTL). We have effectively combined these two works along with our own deep neural network to produce a new system for dance motion prediction, image-to-image translation, and video generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;NGC&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#37051;&#36817;&#20195;&#29702;&#30340;&#33258;&#26799;&#24230;&#65292;&#27169;&#22411;&#21464;&#24322;&#20132;&#21449;&#26799;&#24230;&#21644;&#25968;&#25454;&#21464;&#24322;&#20132;&#21449;&#26799;&#24230;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#26469;&#20462;&#25913;&#27599;&#20010;&#20195;&#29702;&#30340;&#23616;&#37096;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.14390</link><description>&lt;p&gt;
&#37051;&#22495;&#26799;&#24230;&#32858;&#31867;&#65306;&#19968;&#31181;&#29992;&#20110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20998;&#24067;&#30340;&#39640;&#25928;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neighborhood Gradient Clustering: An Efficient Decentralized Learning Method for Non-IID Data Distributions. (arXiv:2209.14390v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;NGC&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#37051;&#36817;&#20195;&#29702;&#30340;&#33258;&#26799;&#24230;&#65292;&#27169;&#22411;&#21464;&#24322;&#20132;&#21449;&#26799;&#24230;&#21644;&#25968;&#25454;&#21464;&#24322;&#20132;&#21449;&#26799;&#24230;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#26469;&#20462;&#25913;&#27599;&#20010;&#20195;&#29702;&#30340;&#23616;&#37096;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#21487;&#33021;&#22312;&#20195;&#29702;&#20043;&#38388;&#20855;&#26377;&#26174;&#30528;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21435;&#20013;&#24515;&#21270;&#31639;&#27861;&#22823;&#22810;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;Neighborhood Gradient Clustering (NGC)&#65292;&#35813;&#31639;&#27861;&#20462;&#25913;&#27599;&#20010;&#20195;&#29702;&#30340;&#23616;&#37096;&#26799;&#24230;&#65292;&#20351;&#29992;&#33258;&#36523;&#21644;&#20132;&#21449;&#26799;&#24230;&#20449;&#24687;&#12290;&#20132;&#21449;&#26799;&#24230;&#26159;&#25351;&#30456;&#37051;&#20195;&#29702;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#23548;&#25968;&#19982;&#21478;&#19968;&#20010;&#20195;&#29702;&#25968;&#25454;&#38598;&#20851;&#20110;&#21442;&#25968;&#30340;&#23548;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#35813;&#26041;&#27861;&#23558;&#27169;&#22411;&#30340;&#23616;&#37096;&#26799;&#24230;&#26367;&#25442;&#20026;&#33258;&#26799;&#24230;&#65292;&#27169;&#22411;&#21464;&#24322;&#20132;&#21449;&#26799;&#24230;&#65288;&#30456;&#37051;&#20195;&#29702;&#20851;&#20110;&#26412;&#22320;&#25968;&#25454;&#38598;&#21442;&#25968;&#30340;&#23548;&#25968;&#65289;&#21644;&#25968;&#25454;&#21464;&#24322;&#20132;&#21449;&#26799;&#24230;&#65288;&#26412;&#22320;&#27169;&#22411;&#20851;&#20110;&#37051;&#23621;&#25968;&#25454;&#38598;&#30340;&#23548;&#25968;&#65289;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized learning over distributed datasets can have significantly different data distributions across the agents. The current state-of-the-art decentralized algorithms mostly assume the data distributions to be Independent and Identically Distributed. This paper focuses on improving decentralized learning over non-IID data. We propose \textit{Neighborhood Gradient Clustering (NGC)}, a novel decentralized learning algorithm that modifies the local gradients of each agent using self- and cross-gradient information. Cross-gradients for a pair of neighboring agents are the derivatives of the model parameters of an agent with respect to the dataset of the other agent. In particular, the proposed method replaces the local gradients of the model with the weighted mean of the self-gradients, model-variant cross-gradients (derivatives of the neighbors' parameters with respect to the local dataset), and data-variant cross-gradients (derivatives of the local model with respect to its neighb
&lt;/p&gt;</description></item><item><title>AirTrack&#26159;&#19968;&#20010;&#29992;&#20110;&#23567;&#22411;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#23454;&#26102;&#35270;&#35273;&#26816;&#27979;&#21644;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#20840;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20197;&#25552;&#39640;&#26816;&#27979;&#21644;&#36319;&#36394;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;Amazon AOT&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#21516;&#26102;&#65292;&#22810;&#27425;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#30340;&#39134;&#34892;&#27979;&#35797;&#22343;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12849</link><description>&lt;p&gt;
AirTrack&#65306;&#29992;&#20110;&#38271;&#31243;&#39134;&#26426;&#26816;&#27979;&#21644;&#36319;&#36394;&#30340;&#26426;&#36733;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AirTrack: Onboard Deep Learning Framework for Long-Range Aircraft Detection and Tracking. (arXiv:2209.12849v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12849
&lt;/p&gt;
&lt;p&gt;
AirTrack&#26159;&#19968;&#20010;&#29992;&#20110;&#23567;&#22411;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#23454;&#26102;&#35270;&#35273;&#26816;&#27979;&#21644;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#20840;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20197;&#25552;&#39640;&#26816;&#27979;&#21644;&#36319;&#36394;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;Amazon AOT&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#21516;&#26102;&#65292;&#22810;&#27425;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#30340;&#39134;&#34892;&#27979;&#35797;&#22343;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#31995;&#32479;&#23433;&#20840;&#36816;&#34892;&#38656;&#35201;&#20855;&#22791;&#26816;&#27979;&#21644;&#36991;&#20813;&#30896;&#25758;&#65288;DAA&#65289;&#33021;&#21147;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;AirTrack&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#26102;&#30340;&#12289;&#20165;&#20351;&#29992;&#35270;&#35273;&#20449;&#24687;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#23567;&#22411;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#22823;&#23567;&#12289;&#37325;&#37327;&#21644;&#21151;&#32791;&#65288;SWaP&#65289;&#38480;&#21046;&#12290;&#37492;&#20110;&#36828;&#31243;&#39134;&#26426;&#30340;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20840;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#23545;&#40784;&#36830;&#32493;&#22270;&#20687;&#20197;&#28040;&#38500;&#33258;&#25105;&#36816;&#21160;&#12290;&#23545;&#40784;&#21518;&#30340;&#22270;&#20687;&#22312;&#32423;&#32852;&#30340;&#20027;&#35201;&#21644;&#27425;&#35201;&#20998;&#31867;&#22120;&#20013;&#20351;&#29992;&#65292;&#20197;&#25552;&#39640;&#22810;&#20010;&#25351;&#26631;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AirTrack&#22312;Amazon&#31354;&#20013;&#29289;&#20307;&#36319;&#36394;&#65288;AOT&#65289;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#32447;&#12290;&#22810;&#27425;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#30340;&#39134;&#34892;&#27979;&#35797;&#65292;&#21253;&#25324;&#19982;&#36890;&#29992;&#33322;&#31354;&#20132;&#36890;&#20114;&#21160;&#30340;Cessna 182&#21644;Bell&#30452;&#21319;&#26426;&#21521;&#21463;&#25511;&#39134;&#34892;&#26080;&#20154;&#26426;&#38752;&#36817;&#30340;&#38468;&#36817;&#30896;&#25758;&#39134;&#34892;&#27979;&#35797;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#28385;&#36275;&#26032;&#24341;&#20837;&#30340;A&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detect-and-Avoid (DAA) capabilities are critical for safe operations of unmanned aircraft systems (UAS). This paper introduces, AirTrack, a real-time vision-only detect and tracking framework that respects the size, weight, and power (SWaP) constraints of sUAS systems. Given the low Signal-to-Noise ratios (SNR) of far away aircraft, we propose using full resolution images in a deep learning framework that aligns successive images to remove ego-motion. The aligned images are then used downstream in cascaded primary and secondary classifiers to improve detection and tracking performance on multiple metrics. We show that AirTrack outperforms state-of-the art baselines on the Amazon Airborne Object Tracking (AOT) Dataset. Multiple real world flight tests with a Cessna 182 interacting with general aviation traffic and additional near-collision flight tests with a Bell helicopter flying towards a UAS in a controlled setting showcase that the proposed approach satisfies the newly introduced A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31867;&#22411;&#25512;&#26029;&#31995;&#32479;Type4Py&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Type4Py&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#31867;&#22411;&#25512;&#26029;&#26102;&#33021;&#22815;&#25552;&#20379;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09189</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31867;&#22411;&#25512;&#26029;&#31995;&#32479;&#30340;&#36328;&#22495;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Evaluation of a Deep Learning-Based Type Inference System. (arXiv:2208.09189v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31867;&#22411;&#25512;&#26029;&#31995;&#32479;Type4Py&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Type4Py&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#31867;&#22411;&#25512;&#26029;&#26102;&#33021;&#22815;&#25552;&#20379;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36873;&#31867;&#22411;&#27880;&#37322;&#20801;&#35768;&#22312;&#21160;&#24577;&#32534;&#31243;&#35821;&#35328;&#20013;&#22686;&#21152;&#38745;&#24577;&#31867;&#22411;&#29305;&#24615;&#65292;&#20363;&#22914;&#26356;&#22909;&#30340;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDE&#65289;&#25903;&#25345;&#12289;&#26356;&#31934;&#30830;&#30340;&#31243;&#24207;&#20998;&#26512;&#20197;&#21450;&#31867;&#22411;&#30456;&#20851;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#30340;&#26089;&#26399;&#26816;&#27979;&#21644;&#39044;&#38450;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31867;&#22411;&#25512;&#26029;&#20026;&#33258;&#21160;&#21270;&#27492;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#20351;&#29992;&#21462;&#20915;&#20110;&#23427;&#20204;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#34987;&#24212;&#29992;&#20110;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#36328;&#39046;&#22495;&#23454;&#39564;&#65292;&#23558;Type4Py&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31867;&#22411;&#25512;&#26029;&#31995;&#32479;&#30340;&#20195;&#34920;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#35299;&#20915;&#20197;&#19979;&#38382;&#39064;&#65306;&#31867;&#19981;&#24179;&#34913;&#12289;&#35789;&#27719;&#34920;&#22806;&#21333;&#35789;&#12289;&#25968;&#25454;&#38598;&#36716;&#31227;&#21644;&#26410;&#30693;&#31867;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;ManyTypes4Py&#21644;CrossDomainTypes4Py&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;&#21518;&#32773;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#31867;&#22411;&#25512;&#26029;&#31995;&#32479;&#65292;&#22312;&#35813;&#24773;&#20917;&#19979;&#31243;&#24207;&#39046;&#22495;&#19982;&#35757;&#32451;&#38598;&#20013;&#30340;&#39046;&#22495;&#19981;&#21516;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;Type4Py&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#31867;&#22411;&#25512;&#26029;&#25552;&#20379;&#20102;&#21487;&#20280;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optional type annotations allow for enriching dynamic programming languages with static typing features like better Integrated Development Environment (IDE) support, more precise program analysis, and early detection and prevention of type-related runtime errors. Machine learning-based type inference promises interesting results for automating this task. However, the practical usage of such systems depends on their ability to generalize across different domains, as they are often applied outside their training domain. In this work, we investigate Type4Py as a representative of state-of-the-art deep learning-based type inference systems, by conducting extensive cross-domain experiments. Thereby, we address the following problems: class imbalances, out-of-vocabulary words, dataset shifts, and unknown classes. To perform such experiments, we use the datasets ManyTypes4Py and CrossDomainTypes4Py. The latter we introduce in this paper. Our dataset enables the evaluation of type inference sy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#24037;&#20855;&#20197;&#23454;&#29616;&#22240;&#26524;&#21457;&#29616;&#21518;&#30340;&#26377;&#25928;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#36816;&#34892;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21518;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#23548;&#33268;&#32463;&#20856;&#32622;&#20449;&#21306;&#38388;&#30340;&#35206;&#30422;&#20445;&#35777;&#26080;&#25928;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.05949</link><description>&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#21518;&#30340;&#26377;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Valid Inference after Causal Discovery. (arXiv:2208.05949v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#24037;&#20855;&#20197;&#23454;&#29616;&#22240;&#26524;&#21457;&#29616;&#21518;&#30340;&#26377;&#25928;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#36816;&#34892;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21518;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#23548;&#33268;&#32463;&#20856;&#32622;&#20449;&#21306;&#38388;&#30340;&#35206;&#30422;&#20445;&#35777;&#26080;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#21644;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#20219;&#21153;&#12290;&#34429;&#28982;&#24050;&#32463;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#21333;&#29420;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#26159;&#21516;&#26102;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#26102;&#20250;&#20986;&#29616;&#32479;&#35745;&#19978;&#30340;&#25361;&#25112;&#65306;&#22312;&#23545;&#30456;&#21516;&#25968;&#25454;&#36816;&#34892;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21518;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#20250;&#23548;&#33268;"&#21452;&#37325;&#25361;&#36873;"&#65292;&#20174;&#32780;&#20351;&#32463;&#20856;&#32622;&#20449;&#21306;&#38388;&#30340;&#35206;&#30422;&#20445;&#35777;&#26080;&#25928;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#22240;&#26524;&#21457;&#29616;&#21518;&#26377;&#25928;&#30340;&#25512;&#26029;&#24037;&#20855;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22825;&#30495;&#32452;&#21512;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21644;&#38543;&#21518;&#25512;&#26029;&#31639;&#27861;&#20250;&#23548;&#33268;&#39640;&#24230;&#33192;&#32960;&#30340;&#35823;&#35206;&#30422;&#29575;&#65292;&#32780;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#21017;&#25552;&#20379;&#21487;&#38752;&#30340;&#35206;&#30422;&#24182;&#23454;&#29616;&#27604;&#25968;&#25454;&#20998;&#21106;&#26356;&#20934;&#30830;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery and causal effect estimation are two fundamental tasks in causal inference. While many methods have been developed for each task individually, statistical challenges arise when applying these methods jointly: estimating causal effects after running causal discovery algorithms on the same data leads to "double dipping," invalidating the coverage guarantees of classical confidence intervals. To this end, we develop tools for valid post-causal-discovery inference. Across empirical studies, we show that a naive combination of causal discovery and subsequent inference algorithms leads to highly inflated miscoverage rates; on the other hand, applying our method provides reliable coverage while achieving more accurate causal discovery than data splitting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PhyGNNet&#65292;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#21306;&#22495;&#21010;&#20998;&#20026;&#35268;&#21017;&#30340;&#32593;&#26684;&#24182;&#26500;&#36896;&#20559;&#24494;&#20998;&#25439;&#22833;&#26469;&#20248;&#21270;&#32593;&#32476;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#33539;&#22260;&#20869;&#20855;&#26377;&#26356;&#22909;&#30340;&#25311;&#21512;&#33021;&#21147;&#21644;&#22806;&#25512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.04319</link><description>&lt;p&gt;
PhyGNNet&#65306;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26102;&#31354;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
PhyGNNet: Solving spatiotemporal PDEs with Physics-informed Graph Neural Network. (arXiv:2208.04319v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PhyGNNet&#65292;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#21306;&#22495;&#21010;&#20998;&#20026;&#35268;&#21017;&#30340;&#32593;&#26684;&#24182;&#26500;&#36896;&#20559;&#24494;&#20998;&#25439;&#22833;&#26469;&#20248;&#21270;&#32593;&#32476;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#33539;&#22260;&#20869;&#20855;&#26377;&#26356;&#22909;&#30340;&#25311;&#21512;&#33021;&#21147;&#21644;&#22806;&#25512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#29289;&#29702;&#12289;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#37325;&#35201;&#30340;&#30740;&#31350;&#25163;&#27573;&#65292;&#36817;&#24180;&#26469;&#65292;PINN&#20316;&#20026;&#25968;&#20540;&#26041;&#27861;&#30340;&#19968;&#31181;&#36817;&#20284;&#26367;&#20195;&#21697;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;PINN&#29992;&#20840;&#36830;&#25509;&#32593;&#32476;&#20316;&#20026;&#20854;&#27169;&#22411;&#65292;&#20854;&#25311;&#21512;&#33021;&#21147;&#21644;&#26102;&#38388;&#31354;&#38388;&#22806;&#25512;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20986;&#20102;PhyGNNet&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#35813;&#32593;&#32476;&#30001;&#32534;&#30721;&#22120;&#12289;&#22788;&#29702;&#22120;&#21644;&#35299;&#30721;&#22120;&#22359;&#32452;&#25104;&#12290;&#29305;&#21035;&#30340;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#21306;&#22495;&#21010;&#20998;&#20026;&#35268;&#21017;&#30340;&#32593;&#26684;&#65292;&#23450;&#20041;&#22312;&#32593;&#26684;&#19978;&#30340;&#20559;&#24494;&#20998;&#31639;&#23376;&#65292;&#28982;&#21518;&#26500;&#36896;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#25439;&#22833;&#20197;&#20248;&#21270;&#26500;&#24314;PhyGNNet&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;Burgers&#26041;&#31243;&#21644;&#28909;&#26041;&#31243;&#19978;&#36827;&#34892;&#27604;&#36739;&#23454;&#39564;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#33539;&#22260;&#20869;&#25317;&#26377;&#26356;&#22909;&#30340;&#25311;&#21512;&#33021;&#21147;&#21644;&#22806;&#25512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving partial differential equations (PDEs) is an important research means in the fields of physics, biology, and chemistry. As an approximate alternative to numerical methods, PINN has received extensive attention and played an important role in many fields. However, PINN uses a fully connected network as its model, which has limited fitting ability and limited extrapolation ability in both time and space. In this paper, we propose PhyGNNet for solving partial differential equations on the basics of a graph neural network which consists of encoder, processer, and decoder blocks. In particular, we divide the computing area into regular grids, define partial differential operators on the grids, then construct pde loss for the network to optimize to build PhyGNNet model. What's more, we conduct comparative experiments on Burgers equation and heat equation to validate our approach, the results show that our method has better fit ability and extrapolation ability both in time and spatial
&lt;/p&gt;</description></item><item><title>TaDaa&#26159;&#19968;&#20010;&#21033;&#29992;&#26368;&#26032;&#30340;Transformer&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#23454;&#26102;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;&#65292;&#23427;&#21487;&#20197;&#20998;&#37197;&#38382;&#39064;&#32473;&#27491;&#30830;&#30340;&#32452;&#12289;&#20998;&#37197;&#38382;&#39064;&#32473;&#26368;&#20339;&#30340;&#35299;&#20915;&#32773;&#65292;&#24182;&#21521;&#35299;&#20915;&#32773;&#25552;&#20379;&#26368;&#30456;&#20851;&#30340;&#20808;&#21069;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#26497;&#22823;&#22320;&#25552;&#39640;&#24179;&#22343;&#35299;&#20915;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2207.11187</link><description>&lt;p&gt;
TaDaa: &#29992;&#20110;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#23454;&#26102;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;
&lt;/p&gt;
&lt;p&gt;
TaDaa: real time Ticket Assignment Deep learning Auto Advisor for customer support, help desk, and issue ticketing systems. (arXiv:2207.11187v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11187
&lt;/p&gt;
&lt;p&gt;
TaDaa&#26159;&#19968;&#20010;&#21033;&#29992;&#26368;&#26032;&#30340;Transformer&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#23454;&#26102;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;&#65292;&#23427;&#21487;&#20197;&#20998;&#37197;&#38382;&#39064;&#32473;&#27491;&#30830;&#30340;&#32452;&#12289;&#20998;&#37197;&#38382;&#39064;&#32473;&#26368;&#20339;&#30340;&#35299;&#20915;&#32773;&#65292;&#24182;&#21521;&#35299;&#20915;&#32773;&#25552;&#20379;&#26368;&#30456;&#20851;&#30340;&#20808;&#21069;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20854;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#26497;&#22823;&#22320;&#25552;&#39640;&#24179;&#22343;&#35299;&#20915;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TaDaa&#65306;&#31080;&#21153;&#20998;&#37197;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#39038;&#38382;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;Transformer&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24555;&#36895;&#20998;&#37197;&#32452;&#32455;&#20869;&#30340;&#38382;&#39064;&#65292;&#22914;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#20854;&#20182;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#12290;&#35813;&#39033;&#30446;&#25552;&#20379;&#20197;&#19979;&#21151;&#33021;&#65306;1&#65289;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#27491;&#30830;&#30340;&#32452;&#65307;2&#65289;&#23558;&#38382;&#39064;&#20998;&#37197;&#32473;&#26368;&#20339;&#30340;&#35299;&#20915;&#32773;&#65307;3&#65289;&#21521;&#35299;&#20915;&#32773;&#25552;&#20379;&#26368;&#30456;&#20851;&#30340;&#20808;&#21069;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#26679;&#26412;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;3k+&#20010;&#32452;&#21644;10k+&#20010;&#35299;&#20915;&#32773;&#65292;&#23454;&#29616;&#20102;95.2%&#30340;&#21069;&#19977;&#24314;&#35758;&#20934;&#30830;&#29575;&#21644;79.0%&#30340;&#21069;&#20116;&#35299;&#20915;&#32773;&#24314;&#35758;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#30740;&#31350;&#23558;&#22823;&#22823;&#25552;&#39640;&#23458;&#25143;&#25903;&#25345;&#12289;&#24110;&#21161;&#21488;&#21644;&#38382;&#39064;&#30331;&#35760;&#31995;&#32479;&#30340;&#24179;&#22343;&#35299;&#20915;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes TaDaa: Ticket Assignment Deep learning Auto Advisor, which leverages the latest Transformers models and machine learning techniques quickly assign issues within an organization, like customer support, help desk and alike issue ticketing systems. The project provides functionality to 1) assign an issue to the correct group, 2) assign an issue to the best resolver, and 3) provide the most relevant previously solved tickets to resolvers. We leverage one ticketing system sample dataset, with over 3k+ groups and over 10k+ resolvers to obtain a 95.2% top 3 accuracy on group suggestions and a 79.0% top 5 accuracy on resolver suggestions. We hope this research will greatly improve average issue resolution time on customer support, help desk, and issue ticketing systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;D3G&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.08892</link><description>&lt;p&gt;
D3G: &#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
D3G: Learning Multi-robot Coordination from Demonstrations. (arXiv:2207.08892v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;D3G&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#21487;&#24494;&#21160;&#24577;&#28216;&#25103;&#65288;D3G&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#12290;&#25105;&#20204;&#23558;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#34920;&#31034;&#20026;&#19968;&#20010;&#21160;&#24577;&#28216;&#25103;&#65292;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#21463;&#20854;&#33258;&#36523;&#21160;&#24577;&#21644;&#30446;&#26631;&#30340;&#25511;&#21046;&#65292;&#21516;&#26102;&#20063;&#21462;&#20915;&#20110;&#20854;&#20182;&#26426;&#22120;&#20154;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#35843;&#25972;&#27599;&#20010;&#26426;&#22120;&#20154;&#30340;&#30446;&#26631;&#21644;&#21160;&#24577;&#65292;&#21487;&#20197;&#36866;&#24212;&#21327;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;D3G&#20351;&#27599;&#20010;&#26426;&#22120;&#20154;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36712;&#36857;&#19982;&#28436;&#31034;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#22312;&#20998;&#24067;&#24335;&#26041;&#24335;&#19979;&#33258;&#21160;&#35843;&#25972;&#20854;&#20010;&#20307;&#21160;&#24577;&#21644;&#30446;&#26631;&#12290;&#35813;&#23398;&#20064;&#26694;&#26550;&#20855;&#26377;&#26032;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#21521;&#20256;&#36882;&#65292;&#25152;&#26377;&#26426;&#22120;&#20154;&#21512;&#20316;&#23547;&#25214;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#65292;&#20197;&#21450;&#19968;&#20010;&#21453;&#21521;&#20256;&#36882;&#65292;&#22312;&#36890;&#20449;&#22270;&#20013;&#20256;&#25773;&#26799;&#24230;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#27979;&#35797;&#20102;D3G&#65292;&#24182;&#32473;&#20986;&#20102;&#19981;&#21516;&#20219;&#21153;&#37197;&#32622;&#30340;&#20004;&#31181;&#26426;&#22120;&#20154;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;D3G&#23398;&#20064;&#22810;&#26426;&#22120;&#20154;&#21327;&#35843;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a Distributed Differentiable Dynamic Game (D3G) framework, which enables learning multi-robot coordination from demonstrations. We represent multi-robot coordination as a dynamic game, where the behavior of a robot is dictated by its own dynamics and objective that also depends on others' behavior. The coordination thus can be adapted by tuning the objective and dynamics of each robot. The proposed D3G enables each robot to automatically tune its individual dynamics and objectives in a distributed manner by minimizing the mismatch between its trajectory and demonstrations. This learning framework features a new design, including a forward-pass, where all robots collaboratively seek Nash equilibrium of a game, and a backward-pass, where gradients are propagated via the communication graph. We test the D3G in simulation with two types of robots given different task configurations. The results validate the capability of D3G for learning multi-robot coordination from de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.01955</link><description>&lt;p&gt;
Ask-AC: &#19968;&#31181;&#24490;&#29615;&#20013;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ask-AC: An Initiative Advisor-in-the-Loop Actor-Critic Framework. (arXiv:2207.01955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;Ask-AC&#65292;&#23427;&#26367;&#25442;&#20102;&#20256;&#32479;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23450;&#21046;&#21270;&#21644;&#39640;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#39038;&#38382;&#24178;&#39044;&#21644;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20132;&#20114;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#21462;&#24471;&#20102;&#24456;&#22810;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#26696;&#20173;&#28982;&#20381;&#36182;&#20110;&#26469;&#33258;&#39038;&#38382;&#19987;&#23478;&#30340;&#34987;&#21160;&#30417;&#30563;&#20449;&#21495;&#65292;&#24418;&#24335;&#21253;&#25324;&#25345;&#32493;&#30417;&#25511;&#25110;&#39044;&#23450;&#20041;&#35268;&#21017;&#65292;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#19968;&#31181;&#40635;&#28902;&#32780;&#26114;&#36149;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#39038;&#38382;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26694;&#26550;&#65292;&#31216;&#20026;Ask-AC&#65292;&#23427;&#29992;&#19968;&#20010;&#21452;&#21521;&#30340;&#23398;&#20064;&#32773;&#20027;&#21160;&#26426;&#21046;&#26367;&#25442;&#20102;&#21333;&#21521;&#30340;&#39038;&#38382;&#25351;&#23548;&#26426;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23398;&#20064;&#32773;&#21644;&#39038;&#38382;&#20043;&#38388;&#30340;&#23450;&#21046;&#21270;&#21644;&#26377;&#25928;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;Ask-AC &#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#65292;&#20998;&#21035;&#26159;&#21160;&#20316;&#35831;&#27714;&#32773;&#21644;&#33258;&#36866;&#24212;&#29366;&#24577;&#36873;&#25321;&#22120;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#32435;&#20837;&#21508;&#31181;&#31163;&#25955;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#20013;&#12290;&#21069;&#32773;&#20801;&#35768;&#20195;&#29702;&#20027;&#21160;&#23547;&#27714;&#19981;&#30830;&#23450;&#29366;&#24577;&#19979;&#30340;&#39038;&#38382;&#24178;&#39044;&#65292;&#21518;&#32773;&#21017;&#21487;&#20197;&#35782;&#21035;&#28431;&#25481;&#30340;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising results achieved, state-of-the-art interactive reinforcement learning schemes rely on passively receiving supervision signals from advisor experts, in the form of either continuous monitoring or pre-defined rules, which inevitably result in a cumbersome and expensive learning process. In this paper, we introduce a novel initiative advisor-in-the-loop actor-critic framework, termed as Ask-AC, that replaces the unilateral advisor-guidance mechanism with a bidirectional learner-initiative one, and thereby enables a customized and efficacious message exchange between learner and advisor. At the heart of Ask-AC are two complementary components, namely action requester and adaptive state selector, that can be readily incorporated into various discrete actor-critic architectures. The former component allows the agent to initiatively seek advisor intervention in the presence of uncertain states, while the latter identifies the unstable states potentially missed by the for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;MDP&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#29616;&#23454;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#24178;&#25200;&#22240;&#32032;&#21435;&#38500;&#65292;&#23398;&#20064;&#19968;&#20010;&#26356;&#22909;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#21152;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2206.15477</link><description>&lt;p&gt;
&#28040;&#38500;&#22122;&#22768;&#30340;MDPs&#65306;&#23398;&#20064;&#27604;&#29616;&#23454;&#19990;&#30028;&#26412;&#36523;&#26356;&#22909;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoised MDPs: Learning World Models Better Than the World Itself. (arXiv:2206.15477v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;MDP&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#29616;&#23454;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#24178;&#25200;&#22240;&#32032;&#21435;&#38500;&#65292;&#23398;&#20064;&#19968;&#20010;&#26356;&#22909;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31163;&#20449;&#21495;&#19982;&#22122;&#22768;&#65292;&#24182;&#33021;&#29702;&#24615;&#22320;&#25226;&#25569;&#26377;&#25928;&#20449;&#24687;&#23545;&#20110;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20351;&#24471;&#20154;&#31867;&#21487;&#20197;&#39640;&#25928;&#22320;&#23436;&#25104;&#29616;&#23454;&#20219;&#21153;&#32780;&#19981;&#24517;&#32771;&#34385;&#25152;&#26377;&#21487;&#33021;&#30340;&#28902;&#29712;&#22240;&#32032;&#12290;&#37027;&#20040;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22914;&#20309;&#25165;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#65311;&#20195;&#29702;&#21487;&#20197;&#25918;&#24323;&#21738;&#20123;&#20449;&#24687;&#20197;&#36991;&#20813;&#22122;&#22768;&#30340;&#24178;&#25200;&#65311;&#26412;&#25991;&#22522;&#20110;&#21487;&#25511;&#24615;&#21644;&#19982;&#22870;&#21169;&#30340;&#20851;&#31995;&#23558;&#20449;&#24687;&#20998;&#20026;&#22235;&#31867;&#65292;&#24182;&#23558;&#26377;&#29992;&#20449;&#24687;&#23450;&#20041;&#20026;&#26082;&#21487;&#25511;&#21046;&#21448;&#19982;&#22870;&#21169;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#28548;&#28165;&#20102;&#21508;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#30340;&#20808;&#21069;&#24037;&#20316;&#21024;&#38500;&#30340;&#20449;&#24687;&#31867;&#22411;&#65292;&#24182;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#28040;&#38500;&#26576;&#20123;&#22122;&#22768;&#24178;&#25200;&#30340;&#21435;&#22122;MDP&#30340;&#26041;&#27861;&#12290;&#22312;DeepMind&#25511;&#21046;&#22871;&#20214;&#21644;RoboDesk&#30340;&#21508;&#31181;&#21464;&#20307;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20165;&#20351;&#29992;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20197;&#21450;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#21435;&#22122;&#19990;&#30028;&#27169;&#22411;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to separate signal from noise, and reason with clean abstractions, is critical to intelligence. With this ability, humans can efficiently perform real world tasks without considering all possible nuisance factors.How can artificial agents do the same? What kind of information can agents safely discard as noises?  In this work, we categorize information out in the wild into four types based on controllability and relation with reward, and formulate useful information as that which is both controllable and reward-relevant. This framework clarifies the kinds information removed by various prior work on representation learning in reinforcement learning (RL), and leads to our proposed approach of learning a Denoised MDP that explicitly factors out certain noise distractors. Extensive experiments on variants of DeepMind Control Suite and RoboDesk demonstrate superior performance of our denoised world model over using raw observations alone, and over prior works, across policy opt
&lt;/p&gt;</description></item><item><title>Merak&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#19977;&#32500;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#20854;&#20182;&#26694;&#26550;&#20013;&#38656;&#35201;&#25163;&#21160;&#20462;&#25913;&#27169;&#22411;&#25165;&#21487;&#24182;&#34892;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#12289;GPU&#20869;&#23384;&#21644;&#32593;&#32476;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.04959</link><description>&lt;p&gt;
Merak&#65306;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;DNN&#35757;&#32451;&#26694;&#26550;&#65292;&#20855;&#22791;&#33258;&#21160;&#21270;&#30340;&#19977;&#32500;&#24182;&#34892;&#25216;&#26415;&#65292;&#36866;&#29992;&#20110;&#24222;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merak: An Efficient Distributed DNN Training Framework with Automated 3D Parallelism for Giant Foundation Models. (arXiv:2206.04959v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04959
&lt;/p&gt;
&lt;p&gt;
Merak&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#19977;&#32500;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#35299;&#20915;&#20102;&#24403;&#21069;&#20854;&#20182;&#26694;&#26550;&#20013;&#38656;&#35201;&#25163;&#21160;&#20462;&#25913;&#27169;&#22411;&#25165;&#21487;&#24182;&#34892;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#12289;GPU&#20869;&#23384;&#21644;&#32593;&#32476;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#27491;&#22312;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20027;&#27969;&#12290;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#24222;&#22823;&#65292;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#22987;&#32456;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#12290;&#38500;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#65292;&#35757;&#32451;&#36807;&#31243;&#36824;&#26497;&#20854;&#20381;&#36182;&#20869;&#23384;&#21644;&#36890;&#20449;&#65292;&#36825;&#23601;&#38656;&#35201;&#24212;&#29992;&#19977;&#32500;&#24182;&#34892;&#25216;&#26415;&#65292;&#21363;&#38598;&#25104;&#25968;&#25454;&#24182;&#34892;&#12289;&#31649;&#36947;&#27169;&#22411;&#24182;&#34892;&#21644;&#24352;&#37327;&#27169;&#22411;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#30740;&#21457;&#20102;&#19968;&#20123;&#33258;&#23450;&#20041;&#36719;&#20214;&#26694;&#26550;&#65292;&#22914;Megatron-LM&#21644;DeepSpeed&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#19977;&#20301;&#24182;&#34892;&#25216;&#26415;&#26694;&#26550;&#20173;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;i&#65289;&#23545;&#20110;&#38656;&#35201;&#25163;&#21160;&#20462;&#25913;&#27169;&#22411;&#20197;&#24182;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#26469;&#35828;&#65292;&#26694;&#26550;&#24182;&#19981;&#36879;&#26126;&#12290;ii&#65289;&#23427;&#20204;&#30340;&#35745;&#31639;&#12289;GPU&#20869;&#23384;&#21644;&#32593;&#32476;&#24102;&#23485;&#21033;&#29992;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Merak&#65292;&#19968;&#20010;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#30340;&#33258;&#21160;&#21270;&#19977;&#32500;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are becoming the dominant deep learning technologies. Pretraining a foundation model is always time-consumed due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the training process is extremely memory-intensive and communication-intensive. These features make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism and tensor model parallelism, to achieve high training efficiency.  To achieve this goal, some custom software frameworks such as Megatron-LM and DeepSpeed are developed. However, current 3D parallelism frameworks still meet two issues: i) they are not transparent to model developers, which need to manually modify the model to parallelize training. ii) their utilization of computation, GPU memory and network bandwidth are not sufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22359;&#8212;&#8212;GAMR&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;(&#35270;&#35273;)&#25512;&#29702;&#30340;&#24341;&#23548;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20197;&#21160;&#24577;&#36873;&#25321;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#24182;&#23558;&#20854;&#36335;&#30001;&#21040;&#35760;&#24518;&#20013;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.04928</link><description>&lt;p&gt;
GAMR: &#19968;&#31181;&#29992;&#20110; (&#35270;&#35273;) &#25512;&#29702;&#30340;&#24341;&#23548;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GAMR: A Guided Attention Model for (visual) Reasoning. (arXiv:2206.04928v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22359;&#8212;&#8212;GAMR&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;(&#35270;&#35273;)&#25512;&#29702;&#30340;&#24341;&#23548;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20197;&#21160;&#24577;&#36873;&#25321;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#24182;&#23558;&#20854;&#36335;&#30001;&#21040;&#35760;&#24518;&#20013;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#28789;&#27963;&#20998;&#26512;&#21644;&#29702;&#35299;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#26041;&#38754;&#20173;&#28982;&#20248;&#20110;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#25512;&#29702;&#27169;&#22359;&#8212;&#8212;&#24341;&#23548;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;(GAMR)&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#21644;&#36335;&#30001;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#21040;&#35760;&#24518;&#20013;&#30340;&#27880;&#24847;&#21147;&#36716;&#31227;&#24207;&#21015;&#26469;&#20307;&#29616;&#20027;&#21160;&#35270;&#35273;&#29702;&#35770;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;GAMR&#33021;&#22815;&#20197;&#31283;&#20581;&#19988;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#35270;&#35273;&#20363;&#31243;&#12290;&#27492;&#22806;&#65292;GAMR&#22312;&#20840;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35745;&#31639;&#25903;&#25345;&#65292;&#25903;&#25345;&#35748;&#30693;&#29702;&#35770;&#20551;&#35774;&#38656;&#35201;&#27880;&#24847;&#21147;&#21644;&#35760;&#24518;&#20043;&#38388;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21160;&#24577;&#22320;&#32500;&#25252;&#21644;&#25805;&#20316;&#20219;&#21153;&#30456;&#20851;&#30340;&#35270;&#35273;&#20449;&#24687;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans continue to outperform modern AI systems in their ability to flexibly parse and understand complex visual scenes. Here, we present a novel module for visual reasoning, the Guided Attention Model for (visual) Reasoning (GAMR), which instantiates an active vision theory -- positing that the brain solves complex visual reasoning problems dynamically -- via sequences of attention shifts to select and route task-relevant visual information into memory. Experiments on an array of visual reasoning tasks and datasets demonstrate GAMR's ability to learn visual routines in a robust and sample-efficient manner. In addition, GAMR is shown to be capable of zero-shot generalization on completely novel reasoning tasks. Overall, our work provides computational support for cognitive theories that postulate the need for a critical interplay between attention and memory to dynamically maintain and manipulate task-relevant visual information to solve complex visual reasoning tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#25552;&#21462;&#20986;&#39046;&#22495;&#30693;&#35782;&#24182;&#24314;&#27169;&#20998;&#31163;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#20026;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.14230</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#35821;&#20041;&#24341;&#23548;&#30340;&#23545;&#25239;&#35757;&#32451;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Semantics-guided Adversarial Training for Trajectory Prediction. (arXiv:2205.14230v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14230
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#25552;&#21462;&#20986;&#39046;&#22495;&#30693;&#35782;&#24182;&#24314;&#27169;&#20998;&#31163;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#20026;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31561;&#33258;&#20027;&#31995;&#32479;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#8212;&#8212;&#39044;&#27979;&#21608;&#22260;&#29289;&#20307;&#30340;&#36712;&#36857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;&#30456;&#36739;&#20110;&#20856;&#22411;&#30340;&#22270;&#20687;&#20219;&#21153;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#38754;&#20020;&#26356;&#22810;&#30340;&#38543;&#26426;&#36755;&#20837;&#21644;&#32570;&#23569;&#31867;&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#24314;&#27169;&#20998;&#31163;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#20026;&#23545;&#25239;&#35757;&#32451;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#28508;&#22312;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the trajectories of surrounding objects is a critical task for self-driving vehicles and many other autonomous systems. Recent works demonstrate that adversarial attacks on trajectory prediction, where small crafted perturbations are introduced to history trajectories, may significantly mislead the prediction of future trajectories and induce unsafe planning. However, few works have addressed enhancing the robustness of this important safety-critical task.In this paper, we present a novel adversarial training method for trajectory prediction. Compared with typical adversarial training on image tasks, our work is challenged by more random input with rich context and a lack of class labels. To address these challenges, we propose a method based on a semi-supervised adversarial autoencoder, which models disentangled semantic features with domain knowledge and provides additional latent labels for the adversarial training. Extensive experiments with different types of attacks de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#12289;&#31283;&#20581;&#19988;&#23454;&#29992;&#30340;&#26041;&#24046;&#35843;&#25972;&#21435;&#20559;&#65288;VAD&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26497;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#26631;&#23450;&#32463;&#24120;&#21463;&#21040;&#30340;&#26497;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.09809</link><description>&lt;p&gt;
&#26631;&#23450;&#33267;&#20851;&#37325;&#35201;&#65306;&#35299;&#20915;&#22823;&#35268;&#27169;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26497;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Calibration Matters: Tackling Maximization Bias in Large-scale Advertising Recommendation Systems. (arXiv:2205.09809v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#12289;&#31283;&#20581;&#19988;&#23454;&#29992;&#30340;&#26041;&#24046;&#35843;&#25972;&#21435;&#20559;&#65288;VAD&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26497;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#32531;&#35299;&#26631;&#23450;&#32463;&#24120;&#21463;&#21040;&#30340;&#26497;&#22823;&#21270;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#30340;&#23454;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#23450;&#26159;&#25351;&#24179;&#22343;&#39044;&#27979;&#28857;&#20987;&#29575;&#19982;&#30495;&#23454;&#28857;&#20987;&#29575;&#20043;&#27604;&#12290;&#26631;&#23450;&#30340;&#20248;&#21270;&#23545;&#35768;&#22810;&#22312;&#32447;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#24191;&#21578;&#31454;&#20215;&#21644;&#21521;&#24191;&#21578;&#23458;&#25143;&#25910;&#21462;&#30340;&#36153;&#29992;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#26631;&#23450;&#20248;&#21270;&#32463;&#24120;&#21463;&#21040;&#31216;&#20026;&#8220;&#26497;&#22823;&#21270;&#20559;&#24046;&#8221;&#30340;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#26497;&#22823;&#21270;&#20559;&#24046;&#25351;&#30340;&#26159;&#39044;&#27979;&#20540;&#30340;&#26368;&#22823;&#20540;&#39640;&#20272;&#20102;&#30495;&#27491;&#30340;&#26368;&#22823;&#20540;&#30340;&#29616;&#35937;&#12290;&#35813;&#38382;&#39064;&#26159;&#30001;&#20110;&#26631;&#23450;&#26159;&#22312;&#39044;&#27979;&#27169;&#22411;&#26412;&#36523;&#36873;&#25321;&#30340;&#38598;&#21512;&#19978;&#35745;&#31639;&#30340;&#12290;&#21363;&#20351;&#21487;&#20197;&#22312;&#27599;&#20010;&#25968;&#25454;&#28857;&#19978;&#23454;&#29616;&#26080;&#20559;&#30340;&#39044;&#27979;&#65292;&#35813;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#23384;&#22312;&#21327;&#21464;&#37327;&#31227;&#20301;&#26102;&#20250;&#21464;&#24471;&#26356;&#31967;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#23545;&#26497;&#22823;&#21270;&#20559;&#24046;&#36827;&#34892;&#37327;&#21270;&#24182;&#25552;&#20986;&#26041;&#24046;&#35843;&#25972;&#21435;&#20559;&#65288;VAD&#65289;&#20803;&#31639;&#27861;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#39640;&#25928;&#12289;&#31283;&#20581;&#19988;&#23454;&#29992;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibration is defined as the ratio of the average predicted click rate to the true click rate. The optimization of calibration is essential to many online advertising recommendation systems because it directly affects the downstream bids in ads auctions and the amount of money charged to advertisers. Despite its importance, calibration optimization often suffers from a problem called "maximization bias". Maximization bias refers to the phenomenon that the maximum of predicted values overestimates the true maximum. The problem is introduced because the calibration is computed on the set selected by the prediction model itself. It persists even if unbiased predictions can be achieved on every datapoint and worsens when covariate shifts exist between the training and test sets. To mitigate this problem, we theorize the quantification of maximization bias and propose a variance-adjusting debiasing (VAD) meta-algorithm in this paper. The algorithm is efficient, robust, and practical as it 
&lt;/p&gt;</description></item><item><title>DeepGraviLens&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.00701</link><description>&lt;p&gt;
DeepGraviLens&#65306;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DeepGraviLens: a Multi-Modal Architecture for Classifying Gravitational Lensing Data. (arXiv:2205.00701v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00701
&lt;/p&gt;
&lt;p&gt;
DeepGraviLens&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#24341;&#21147;&#36879;&#38236;&#25968;&#25454;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#36879;&#38236;&#26159;&#30001;&#22823;&#36136;&#37327;&#29289;&#20307;&#20135;&#29983;&#30340;&#30456;&#23545;&#35770;&#25928;&#24212;&#65292;&#20250;&#24367;&#26354;&#20854;&#21608;&#22260;&#30340;&#26102;&#31354;&#12290;&#36825;&#26159;&#22825;&#20307;&#29289;&#29702;&#23398;&#20013;&#19968;&#20010;&#28145;&#20837;&#30740;&#31350;&#30340;&#35838;&#39064;&#65292;&#20801;&#35768;&#39564;&#35777;&#29702;&#35770;&#30456;&#23545;&#35770;&#32467;&#26524;&#24182;&#30740;&#31350;&#19968;&#20123;&#21542;&#21017;&#19981;&#21487;&#35265;&#30340;&#24494;&#24369;&#22825;&#20307;&#29289;&#20307;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24341;&#21147;&#36879;&#38236;&#29616;&#35937;&#30340;&#20998;&#26512;&#65292;&#36890;&#36807;&#26816;&#27979;&#19982;&#20142;&#24230;&#21464;&#21270;&#26102;&#38388;&#24207;&#21015;&#30456;&#20851;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#36879;&#38236;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20165;&#32771;&#34385;&#22270;&#20687;&#32780;&#24573;&#30053;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#35201;&#20040;&#22312;&#26368;&#22256;&#38590;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30456;&#23545;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; DeepGraviLens&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#65292;&#29992;&#20110;&#20998;&#31867;&#23646;&#20110;&#19968;&#20010;&#38750;&#36879;&#38236;&#31995;&#32479;&#31867;&#22411;&#21644;&#19977;&#20010;&#36879;&#38236;&#31995;&#32479;&#31867;&#22411;&#30340;&#26102;&#31354;&#25968;&#25454;&#12290;&#23427;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36807;&#24403;&#21069;&#30340; state-of-art &#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#32422; 19% &#21040; 43%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#25152;&#32771;&#34385;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gravitational lensing is the relativistic effect generated by massive bodies, which bend the space-time surrounding them. It is a deeply investigated topic in astrophysics and allows validating theoretical relativistic results and studying faint astrophysical objects that would not be visible otherwise. In recent years Machine Learning methods have been applied to support the analysis of the gravitational lensing phenomena by detecting lensing effects in data sets consisting of images associated with brightness variation time series. However, the state-of-art approaches either consider only images and neglect time-series data or achieve relatively low accuracy on the most difficult data sets. This paper introduces DeepGraviLens, a novel multi-modal network that classifies spatio-temporal data belonging to one non-lensed system type and three lensed system types. It surpasses the current state of the art accuracy results by $\approx$ 19% to $\approx$ 43%, depending on the considered dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21644;&#25511;&#21046;&#35821;&#38899;&#30340;&#28304;-&#28388;&#27874;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#28304;-&#28388;&#27874;&#27169;&#22411;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#32534;&#30721;$f_0$&#21644;&#21069;&#19977;&#20010;&#20849;&#25391;&#23792;&#39057;&#29575;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#23376;&#31354;&#38388;&#21487;&#20197;&#34987;&#29992;&#20110;&#23545;&#35821;&#38899;&#36827;&#34892;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2204.07075</link><description>&lt;p&gt;
&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21644;&#25511;&#21046;&#35821;&#38899;&#30340;&#28304;-&#28388;&#27874;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning and controlling the source-filter representation of speech with a variational autoencoder. (arXiv:2204.07075v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21644;&#25511;&#21046;&#35821;&#38899;&#30340;&#28304;-&#28388;&#27874;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#28304;-&#28388;&#27874;&#27169;&#22411;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#32534;&#30721;$f_0$&#21644;&#21069;&#19977;&#20010;&#20849;&#25391;&#23792;&#39057;&#29575;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#23376;&#31354;&#38388;&#21487;&#20197;&#34987;&#29992;&#20110;&#23545;&#35821;&#38899;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#25511;&#21046;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#34920;&#31034;&#23545;&#20110;&#20998;&#26512;&#12289;&#36716;&#25442;&#21644;&#29983;&#25104;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#35821;&#38899;&#22788;&#29702;&#26041;&#38754;&#65292;&#21463;&#21040;&#22768;&#38899;&#29983;&#29702;&#23398;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#28304;-&#28388;&#27874;&#27169;&#22411;&#35748;&#20026;&#35821;&#38899;&#20449;&#21495;&#26159;&#20174;&#20960;&#20010;&#29420;&#31435;&#19988;&#29289;&#29702;&#24847;&#20041;&#36830;&#32493;&#30340;&#28508;&#22312;&#22240;&#32032;&#20135;&#29983;&#30340;&#65292;&#20854;&#20013;&#22522;&#39057;$f_0$&#21644;&#20849;&#25391;&#23792;&#26159;&#26368;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#20174;&#19968;&#20010;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#33258;&#28982;&#35821;&#38899;&#20449;&#21495;&#19978;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#24320;&#22987;&#65292;&#23637;&#31034;&#20102;&#35821;&#38899;&#20135;&#29983;&#30340;&#28304;-&#28388;&#27874;&#27169;&#22411;&#33258;&#28982;&#22320;&#26174;&#29616;&#20026;VAE&#28508;&#22312;&#31354;&#38388;&#30340;&#27491;&#20132;&#23376;&#31354;&#38388;&#12290;&#20165;&#20351;&#29992;&#20154;&#24037;&#35821;&#38899;&#21512;&#25104;&#22120;&#29983;&#25104;&#30340;&#23569;&#37327;&#26631;&#35760;&#35821;&#38899;&#20449;&#21495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#32534;&#30721;$f_0$&#21644;&#21069;&#19977;&#20010;&#20849;&#25391;&#23792;&#39057;&#29575;&#30340;&#28508;&#22312;&#23376;&#31354;&#38388;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#23376;&#31354;&#38388;&#21487;&#20197;&#34987;&#29992;&#20110;&#23545;&#35821;&#38899;&#36827;&#34892;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and controlling latent representations in deep generative models is a challenging yet important problem for analyzing, transforming and generating various types of data. In speech processing, inspiring from the anatomical mechanisms of phonation, the source-filter model considers that speech signals are produced from a few independent and physically meaningful continuous latent factors, among which the fundamental frequency $f_0$ and the formants are of primary importance. In this work, we start from a variational autoencoder (VAE) trained in an unsupervised manner on a large dataset of unlabeled natural speech signals, and we show that the source-filter model of speech production naturally arises as orthogonal subspaces of the VAE latent space. Using only a few seconds of labeled speech signals generated with an artificial speech synthesizer, we propose a method to identify the latent subspaces encoding $f_0$ and the first three formant frequencies, we show that these su
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#23458;&#35266;&#30446;&#26631;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#30340;&#26032;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2203.07120</link><description>&lt;p&gt;
&#31070;&#32463;&#28040;&#24687;&#20256;&#36882;&#29992;&#20110;&#23458;&#35266;&#30446;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Message Passing for Objective-Based Uncertainty Quantification and Optimal Experimental Design. (arXiv:2203.07120v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#23454;&#29616;&#23458;&#35266;&#30446;&#26631;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#30340;&#26032;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#31185;&#23398;&#24212;&#29992;&#28041;&#21450;&#23545;&#20855;&#26377;&#35768;&#22810;&#26410;&#30693;&#21442;&#25968;&#30340;&#22797;&#26434;&#19981;&#30830;&#23450;&#31995;&#32479;&#36827;&#34892;&#25968;&#23398;&#24314;&#27169;&#12290;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#20934;&#30830;&#30340;&#21442;&#25968;&#20272;&#35745;&#36890;&#24120;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#26159;&#19981;&#20805;&#20998;&#30340;&#65292;&#33719;&#21462;&#26356;&#22810;&#25968;&#25454;&#30340;&#25104;&#26412;&#21487;&#33021;&#24456;&#39640;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#35774;&#35745;&#24378;&#22823;&#30340;&#25805;&#20316;&#31526;&#65292;&#24182;&#35774;&#35745;&#26368;&#20248;&#23454;&#39564;&#26469;&#26377;&#25928;&#22320;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#36825;&#20123;&#25805;&#20316;&#31526;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various real-world scientific applications involve the mathematical modeling of complex uncertain systems with numerous unknown parameters. Accurate parameter estimation is often practically infeasible in such systems, as the available training data may be insufficient and the cost of acquiring additional data may be high. In such cases, based on a Bayesian paradigm, we can design robust operators retaining the best overall performance across all possible models and design optimal experiments that can effectively reduce uncertainty to enhance the performance of such operators maximally. While objective-based uncertainty quantification (objective-UQ) based on MOCU (mean objective cost of uncertainty) provides an effective means for quantifying uncertainty in complex systems, the high computational cost of estimating MOCU has been a challenge in applying it to real-world scientific/engineering problems. In this work, we propose a novel scheme to reduce the computational cost for objectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#34987;&#33258;&#28982;&#25197;&#26354;&#30340;&#36172;&#24466;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;HubUCB&#31639;&#27861;&#65292;&#21033;&#29992;Huber&#20272;&#35745;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.03186</link><description>&lt;p&gt;
&#34987;&#33258;&#28982;&#25197;&#26354;&#30340;&#36172;&#24466;&#38382;&#39064;: &#23545;&#36951;&#25022;&#21644;&#40065;&#26834;&#20048;&#35266;&#31639;&#27861;&#30340;&#19979;&#38480;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bandits Corrupted by Nature: Lower Bounds on Regret and Robust Optimistic Algorithm. (arXiv:2203.03186v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#34987;&#33258;&#28982;&#25197;&#26354;&#30340;&#36172;&#24466;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;HubUCB&#31639;&#27861;&#65292;&#21033;&#29992;Huber&#20272;&#35745;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36172;&#24466;&#38382;&#39064;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21363;&#22312;$k$&#20010;&#26410;&#30693;&#22870;&#21169;&#20998;&#24067;&#20026;&#37325;&#23614;&#20998;&#24067;&#30340;&#33218;&#20013;&#36873;&#25321;&#65292;&#22312;&#27599;&#36718;&#25805;&#20316;&#20013;&#65292;&#20197;$1-\varepsilon \in(0.5,1]$&#30340;&#27010;&#29575;&#26469;&#33258;&#22870;&#21169;&#20998;&#24067;&#65292;&#20197;$\varepsilon \in[0,0.5)$&#30340;&#27010;&#29575;&#26469;&#33258;&#26410;&#30693;&#30340;&#25197;&#26354;&#20998;&#24067;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20219;&#20309;&#25197;&#26354;&#36172;&#24466;&#31639;&#27861;&#8220;&#36951;&#25022;&#8221;&#30340;&#19968;&#20010;&#38382;&#39064;&#30456;&#20851;&#19979;&#38480;&#12290;&#36739;&#20043;&#20122;&#39640;&#26031;&#25110;&#37325;&#23614;&#22870;&#21169;&#30340;&#32463;&#20856;&#38543;&#26426;&#36172;&#24466;&#38382;&#39064;&#65292;&#19978;&#36848;&#32467;&#26524;&#34920;&#26126;&#25197;&#26354;&#36172;&#24466;&#38382;&#39064;&#26356;&#20026;&#22256;&#38590;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#25197;&#26354;&#36172;&#24466;&#30340;&#19968;&#31181;&#26032;&#22411;&#19978;&#32622;&#20449;&#30028;&#31639;&#27861;&#65292;&#21517;&#20026; HubUCB&#65292;&#35813;&#31639;&#27861;&#22522;&#20110; Huber &#30340;&#40065;&#26834;&#22343;&#20540;&#20272;&#35745;&#65292;&#21033;&#29992; Huber &#20272;&#35745;&#37327;&#30340;&#19968;&#31181;&#26032;&#22411;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#35777;&#26126;&#20102; HubUCB &#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the corrupted bandit problem, i.e. a stochastic multi-armed bandit problem with $k$ unknown reward distributions, which are heavy-tailed and corrupted by a history-independent adversary or Nature. To be specific, the reward obtained by playing an arm comes from corresponding heavy-tailed reward distribution with probability $1-\varepsilon \in (0.5,1]$ and an arbitrary corruption distribution of unbounded support with probability $\varepsilon \in [0,0.5)$.  First, we provide $\textit{a problem-dependent lower bound on the regret}$ of any corrupted bandit algorithm. The lower bounds indicate that the corrupted bandit problem is harder than the classical stochastic bandit problem with sub-Gaussian or heavy-tail rewards.  Following that, we propose a novel UCB-type algorithm for corrupted bandits, namely HubUCB, that builds on Huber's estimator for robust mean estimation. Leveraging a novel concentration inequality of Huber's estimator, we prove that HubUCB achieves a near-optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#38382;&#39064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27491;&#24335;&#39564;&#35777;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22312;&#21069;&#26399;&#23398;&#20064;&#21644;&#21518;&#26399;&#39564;&#35777;&#20043;&#38388;&#38590;&#20197;&#33719;&#24471;&#35777;&#20070;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26799;&#24230;&#26469;&#24494;&#20998;&#35777;&#20070;&#21644;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#22823;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#25214;&#21040;&#20855;&#26377;&#20445;&#25252;&#20989;&#25968;&#21644;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#30340;&#21487;&#34892;&#25511;&#21046;&#22120;&#65292;&#30830;&#20445;&#20102;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.12243</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#35777;&#23454;&#24615;&#20248;&#21270;&#19982;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Joint Differentiable Optimization and Verification for Certified Reinforcement Learning. (arXiv:2201.12243v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#38382;&#39064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27491;&#24335;&#39564;&#35777;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22312;&#21069;&#26399;&#23398;&#20064;&#21644;&#21518;&#26399;&#39564;&#35777;&#20043;&#38388;&#38590;&#20197;&#33719;&#24471;&#35777;&#20070;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26799;&#24230;&#26469;&#24494;&#20998;&#35777;&#20070;&#21644;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#22823;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#25214;&#21040;&#20855;&#26377;&#20445;&#25252;&#20989;&#25968;&#21644;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#30340;&#21487;&#34892;&#25511;&#21046;&#22120;&#65292;&#30830;&#20445;&#20102;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#31995;&#32479;&#30340;&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26681;&#25454;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#27491;&#24335;&#35748;&#35777;&#31995;&#32479;&#23646;&#24615;&#65288;&#20363;&#22914;&#23433;&#20840;&#12289;&#31283;&#23450;&#65289;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#23398;&#20064;&#25511;&#21046;&#22120;&#20043;&#21518;&#25165;&#24212;&#29992;&#24418;&#24335;&#39564;&#35777;&#65292;&#22312;&#32463;&#21382;&#20102;&#22810;&#27425;&#36845;&#20195;&#30340;&#23398;&#20064;&#21644;&#39564;&#35777;&#20043;&#21518;&#35821;&#35328;&#32771;&#21462;&#24471;&#21040;&#20219;&#20309;&#35777;&#20070;&#26377;&#26102;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#35299;&#20915;&#19968;&#20010;&#26032;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26469;&#21516;&#26102;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#21644;&#27491;&#24335;&#39564;&#35777;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#30001;&#20215;&#20540;&#20989;&#25968;&#21644;&#35777;&#20070;&#30340;&#26799;&#24230;&#36827;&#34892;&#24494;&#20998;&#12290;&#22312;&#21508;&#31181;&#31034;&#20363;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#38543;&#26426;&#20540;&#26799;&#24230;&#65288;SVG&#65289;&#26041;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26080;&#27169;&#22411;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#23547;&#25214;&#20855;&#26377;&#20445;&#25252;&#20989;&#25968;&#21644;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#30340;&#21487;&#34892;&#25511;&#21046;&#22120;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#20445;&#35777;&#20102;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In model-based reinforcement learning for safety-critical control systems, it is important to formally certify system properties (e.g., safety, stability) under the learned controller. However, as existing methods typically apply formal verification \emph{after} the controller has been learned, it is sometimes difficult to obtain any certificate, even after many iterations between learning and verification. To address this challenge, we propose a framework that jointly conducts reinforcement learning and formal verification by formulating and solving a novel bilevel optimization problem, which is differentiable by the gradients from the value function and certificates. Experiments on a variety of examples demonstrate the significant advantages of our framework over the model-based stochastic value gradient (SVG) method and the model-free proximal policy optimization (PPO) method in finding feasible controllers with barrier functions and Lyapunov functions that ensure system safety and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38134;&#34892;&#21453;&#27927;&#38065;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#23458;&#25143;&#39118;&#38505;&#35780;&#20272;&#21644;&#21487;&#30097;&#34892;&#20026;&#26631;&#35782;&#20004;&#20010;&#26680;&#24515;&#35201;&#32032;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12289;&#21322;&#30417;&#30563;&#21644;&#28145;&#24230;&#23398;&#20064;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#32467;&#26524;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.04207</link><description>&lt;p&gt;
&#29992;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#25171;&#20987;&#27927;&#38065;&#65306;&#32508;&#36848;&#19982;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
Fighting Money Laundering with Statistics and Machine Learning: An Introduction and Review. (arXiv:2201.04207v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38134;&#34892;&#21453;&#27927;&#38065;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#23458;&#25143;&#39118;&#38505;&#35780;&#20272;&#21644;&#21487;&#30097;&#34892;&#20026;&#26631;&#35782;&#20004;&#20010;&#26680;&#24515;&#35201;&#32032;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12289;&#21322;&#30417;&#30563;&#21644;&#28145;&#24230;&#23398;&#20064;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#32467;&#26524;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27927;&#38065;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#20840;&#29699;&#24615;&#38382;&#39064;&#65292;&#20294;&#26159;&#38024;&#23545;&#21453;&#27927;&#38065;&#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#31185;&#23398;&#25991;&#29486;&#21364;&#24456;&#23569;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#38134;&#34892;&#21453;&#27927;&#38065;&#65292;&#24182;&#25552;&#20379;&#20102;&#25991;&#29486;&#32508;&#36848;&#21644;&#20171;&#32461;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26415;&#35821;&#65292;&#20854;&#20013;&#21253;&#25324;&#23458;&#25143;&#39118;&#38505;&#35780;&#20272;&#21644;&#21487;&#30097;&#34892;&#20026;&#26631;&#35782;&#20004;&#20010;&#26680;&#24515;&#35201;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23458;&#25143;&#39118;&#38505;&#35780;&#20272;&#26159;&#36890;&#36807;&#35786;&#26029;&#26469;&#23547;&#25214;&#21644;&#35299;&#37322;&#39118;&#38505;&#22240;&#32032;&#65292;&#32780;&#21487;&#30097;&#34892;&#20026;&#26631;&#35782;&#21017;&#26159;&#36890;&#36807;&#26410;&#20844;&#24320;&#30340;&#29305;&#24449;&#21644;&#25163;&#24037;&#39118;&#38505;&#25351;&#25968;&#26469;&#23454;&#29616;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#38656;&#35201;&#26356;&#22810;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#36825;&#21487;&#33021;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#65292;&#20854;&#20182;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#21322;&#30417;&#30563;&#21644;&#28145;&#24230;&#23398;&#20064;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#32467;&#26524;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Money laundering is a profound global problem. Nonetheless, there is little scientific literature on statistical and machine learning methods for anti-money laundering. In this paper, we focus on anti-money laundering in banks and provide an introduction and review of the literature. We propose a unifying terminology with two central elements: (i) client risk profiling and (ii) suspicious behavior flagging. We find that client risk profiling is characterized by diagnostics, i.e., efforts to find and explain risk factors. On the other hand, suspicious behavior flagging is characterized by non-disclosed features and hand-crafted risk indices. Finally, we discuss directions for future research. One major challenge is the need for more public data sets. This may potentially be addressed by synthetic data generation. Other possible research directions include semi-supervised and deep learning, interpretability, and fairness of the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SMM&#30340;&#25193;&#23637;&#65292;&#35813;&#25193;&#23637;&#25903;&#25345;&#20195;&#29702;&#20989;&#25968;&#30340;&#24369;&#20984;&#24615;&#21644;&#22359;&#22810;&#20984;&#24615;&#65292;&#24182;&#22312;&#35299;&#20915;&#38750;&#20984;&#32422;&#26463;&#38382;&#39064;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.01652</link><description>&lt;p&gt;
&#24369;&#20984;&#21644;&#22810;&#20984;&#20195;&#29702;&#20989;&#25968;&#30340;&#38543;&#26426;&#27491;&#21017;&#21270;&#20027;&#23548;&#26497;&#23567;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic regularized majorization-minimization with weakly convex and multi-convex surrogates. (arXiv:2201.01652v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SMM&#30340;&#25193;&#23637;&#65292;&#35813;&#25193;&#23637;&#25903;&#25345;&#20195;&#29702;&#20989;&#25968;&#30340;&#24369;&#20984;&#24615;&#21644;&#22359;&#22810;&#20984;&#24615;&#65292;&#24182;&#22312;&#35299;&#20915;&#38750;&#20984;&#32422;&#26463;&#38382;&#39064;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20027;&#23548;&#26497;&#23567;&#21270;&#65288;SMM&#65289;&#26159;&#19968;&#31867;&#37319;&#26679;&#26032;&#25968;&#25454;&#28857;&#24182;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#20195;&#29702;&#20989;&#25968;&#30340;&#36882;&#24402;&#24179;&#22343;&#25968;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#35201;&#27714;&#20195;&#29702;&#20989;&#25968;&#26159;&#24378;&#20984;&#30340;&#65292;&#38750;&#20984;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#29575;&#20998;&#26512;&#24182;&#19981;&#21487;&#34892;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SMM&#30340;&#25193;&#23637;&#65292;&#35813;&#25193;&#23637;&#20801;&#35768;&#20195;&#29702;&#20989;&#25968;&#20165;&#20026;&#24369;&#20984;&#25110;&#22359;&#22810;&#20984;&#65292;&#24182;&#19988;&#24179;&#22343;&#20195;&#29702;&#20989;&#25968;&#22312;&#36817;&#31471;&#27491;&#21017;&#21270;&#25110;&#22359;&#26368;&#23567;&#21270;&#30340;&#20943;&#23567;&#21322;&#24452;&#20013;&#36817;&#20284;&#26368;&#23567;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#20984;&#32422;&#26463;&#19979;&#22788;&#29702;&#38750;i.i.d.&#25968;&#25454;&#26679;&#26412;&#30340;&#19968;&#38454;&#26368;&#20248;&#24615;&#24046;&#36317;&#20197;&#36895;&#29575;$O((\log n)^{1+\epsilon}/n^{1/2})$&#19979;&#38477;&#65292;&#26399;&#26395;&#25439;&#22833;&#20026;$O((\log n)^{1+\epsilon}/n^{1/4})$&#65292;&#20854;&#20013;$n$&#34920;&#31034;&#22788;&#29702;&#30340;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#12290;&#22312;&#19968;&#20123;&#39069;&#22806;&#30340;&#20551;&#35774;&#19979;&#65292;&#36824;&#25552;&#20379;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#21518;&#26399;&#25910;&#25947;&#36895;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#23637;&#31034;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic majorization-minimization (SMM) is a class of stochastic optimization algorithms that proceed by sampling new data points and minimizing a recursive average of surrogate functions of an objective function. The surrogates are required to be strongly convex and convergence rate analysis for the general non-convex setting was not available. In this paper, we propose an extension of SMM where surrogates are allowed to be only weakly convex or block multi-convex, and the averaged surrogates are approximately minimized with proximal regularization or block-minimized within diminishing radii, respectively. For the general nonconvex constrained setting with non-i.i.d. data samples, we show that the first-order optimality gap of the proposed algorithm decays at the rate $O((\log n)^{1+\epsilon}/n^{1/2})$ for the empirical loss and $O((\log n)^{1+\epsilon}/n^{1/4})$ for the expected loss, where $n$ denotes the number of data samples processed. Under some additional assumption, the lat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23574;&#38160;&#24863;&#30693;&#37327;&#21270;&#65288;SAQ&#65289;&#65292;&#23427;&#21033;&#29992;&#23574;&#38160;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#26469;&#24179;&#28369;&#23574;&#38160;&#30340;&#37327;&#21270;&#22122;&#22768;&#21644;&#25200;&#21160;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#21387;&#32553;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAQ &#22312; ImageNet &#19978;&#30340;&#31934;&#24230;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#39640;&#20986;3.6&#65285;&#12290;</title><link>http://arxiv.org/abs/2111.12273</link><description>&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23574;&#38160;&#24863;&#30693;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware Quantization for Deep Neural Networks. (arXiv:2111.12273v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.12273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23574;&#38160;&#24863;&#30693;&#37327;&#21270;&#65288;SAQ&#65289;&#65292;&#23427;&#21033;&#29992;&#23574;&#38160;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#26469;&#24179;&#28369;&#23574;&#38160;&#30340;&#37327;&#21270;&#22122;&#22768;&#21644;&#25200;&#21160;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#21387;&#32553;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAQ &#22312; ImageNet &#19978;&#30340;&#31934;&#24230;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#39640;&#20986;3.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#37327;&#21270;&#26159;&#27169;&#22411;&#21387;&#32553;&#30340;&#20027;&#23548;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#35757;&#32451;&#20013;&#37327;&#21270;&#26435;&#37325;&#30340;&#31361;&#28982;&#21464;&#21270;&#36890;&#24120;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#25439;&#22833;&#27874;&#21160;&#65292;&#23548;&#33268;&#23574;&#38160;&#30340;&#25439;&#22833;&#22320;&#24418;&#65292;&#20351;&#26799;&#24230;&#19981;&#31283;&#23450;&#65292;&#20174;&#32780;&#38477;&#20302;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#23574;&#38160;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#26469;&#24179;&#28369;&#25439;&#22833;&#22320;&#24418;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;SAM&#24212;&#29992;&#20110;&#37327;&#21270;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#25200;&#21160;&#19981;&#21305;&#37197;&#25110;&#32553;&#20943;&#38382;&#39064;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#23574;&#38160;&#24863;&#30693;&#37327;&#21270;&#65288;SAQ&#65289;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;SAM&#22312;&#27169;&#22411;&#21387;&#32553;&#20013;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#37327;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#37327;&#21270;&#21644;SAM&#35270;&#20026;&#20998;&#21035;&#24341;&#20837;&#27169;&#22411;&#26435;&#37325;&#30340;&#37327;&#21270;&#22122;&#22768;&#21644;&#23545;&#25239;&#25200;&#21160;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#12290;&#26681;&#25454;&#22122;&#22768;&#21644;&#25200;&#21160;&#26415;&#35821;&#26159;&#24179;&#28369;&#36824;&#26159;&#23574;&#38160;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#31867;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#31181;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAQ&#65292;&#26126;&#30830;&#22320;&#21033;&#29992;SAM&#24179;&#28369;&#23574;&#38160;&#30340;&#37327;&#21270;&#22122;&#22768;&#21644;&#25200;&#21160;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#21387;&#32553;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SAQ&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#27604;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#23545;&#25163;&#22312;ImageNet&#19978;&#30340;&#31934;&#24230;&#39640;&#36798;3.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network quantization is a dominant paradigm of model compression. However, the abrupt changes in quantized weights during training often lead to severe loss fluctuations and result in a sharp loss landscape, making the gradients unstable and thus degrading the performance. Recently, Sharpness-Aware Minimization (SAM) has been proposed to smooth the loss landscape and improve the generalization performance of the models. Nevertheless, directly applying SAM to the quantized models can lead to perturbation mismatch or diminishment issues, resulting in suboptimal performance. In this paper, we propose a novel method, dubbed Sharpness-Aware Quantization (SAQ), to explore the effect of SAM in model compression, particularly quantization for the first time. Specifically, we first provide a unified view of quantization and SAM by treating them as introducing quantization noises and adversarial perturbations to the model weights, respectively. According to whether the noise and perturbation ter
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#20351;&#29992;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#21462;&#20195;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;DeBERTaV3&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.09543</link><description>&lt;p&gt;
DeBERTaV3&#65306;&#20351;&#29992;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#30340;ELECTRA&#39118;&#26684;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;DeBERTa&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing. (arXiv:2111.09543v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#20351;&#29992;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#21462;&#20195;&#20102;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;DeBERTaV3&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;DeBERTaV3&#65292;&#23427;&#36890;&#36807;&#23558;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#26367;&#25442;&#20026;&#26356;&#21152;&#26679;&#26412;&#26377;&#25928;&#30340;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#65288;RTD&#65289;&#26469;&#25913;&#36827;&#21407;&#22987;&#30340;DeBERTa&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ELECTRA&#20013;&#30340;&#39321;&#33609;&#23884;&#20837;&#20849;&#20139;&#20250;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#35757;&#32451;&#25439;&#22833;&#23558;&#20196;&#29260;&#23884;&#20837;&#25289;&#21521;&#19981;&#21516;&#30340;&#26041;&#21521;&#65292;&#20250;&#36896;&#25104;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#21435;&#32806;&#21512;&#23884;&#20837;&#20849;&#20139;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#8220;&#25300;&#27827;&#8221;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;DeBERTa&#30456;&#21516;&#30340;&#35774;&#32622;&#39044;&#35757;&#32451;&#20102;DeBERTaV3&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#30340;&#20248;&#31168;&#24615;&#33021;&#12290;&#20197;&#20843;&#39033;&#20219;&#21153;&#20026;&#20363;&#30340;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;DeBERTaV3 Large&#27169;&#22411;&#24179;&#22343;&#24471;&#20998;&#20026;91.37&#65285;&#65292;&#27604;D&#39640;1.37&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37% average score, which is 1.37% over D
&lt;/p&gt;</description></item><item><title>CoReS&#26159;&#19968;&#31181;&#22522;&#20110;&#24179;&#31283;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20869;&#37096;&#29305;&#24449;&#34920;&#31034;&#27169;&#22411;&#65292;&#24182;&#20351;&#20854;&#19982;&#20043;&#21069;&#23398;&#20064;&#30340;&#27169;&#22411;&#8220;&#20860;&#23481;&#8221;&#12290;&#36825;&#20351;&#24471;&#22312;&#36880;&#27493;&#21319;&#32423;&#34920;&#31034;&#27169;&#22411;&#26102;&#65292;&#26080;&#38656;&#20026;&#25152;&#26377;&#20043;&#21069;&#30475;&#21040;&#30340;&#22270;&#20687;&#25552;&#21462;&#26032;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2111.07632</link><description>&lt;p&gt;
CoReS: &#22522;&#20110;&#24179;&#31283;&#24615;&#30340;&#20860;&#23481;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoReS: Compatible Representations via Stationarity. (arXiv:2111.07632v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07632
&lt;/p&gt;
&lt;p&gt;
CoReS&#26159;&#19968;&#31181;&#22522;&#20110;&#24179;&#31283;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20869;&#37096;&#29305;&#24449;&#34920;&#31034;&#27169;&#22411;&#65292;&#24182;&#20351;&#20854;&#19982;&#20043;&#21069;&#23398;&#20064;&#30340;&#27169;&#22411;&#8220;&#20860;&#23481;&#8221;&#12290;&#36825;&#20351;&#24471;&#22312;&#36880;&#27493;&#21319;&#32423;&#34920;&#31034;&#27169;&#22411;&#26102;&#65292;&#26080;&#38656;&#20026;&#25152;&#26377;&#20043;&#21069;&#30475;&#21040;&#30340;&#22270;&#20687;&#25552;&#21462;&#26032;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#23398;&#20064;&#20869;&#37096;&#29305;&#24449;&#34920;&#31034;&#27169;&#22411;&#65292;&#20351;&#20854;&#19982;&#20808;&#21069;&#23398;&#20064;&#30340;&#27169;&#22411;&#8220;&#20860;&#23481;&#8221;&#12290;&#20860;&#23481;&#29305;&#24449;&#20351;&#26087;&#21644;&#26032;&#30340;&#29305;&#24449;&#21487;&#20197;&#30452;&#25509;&#27604;&#36739;&#65292;&#20801;&#35768;&#23427;&#20204;&#22312;&#26102;&#38388;&#19978;&#20114;&#25442;&#20351;&#29992;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#22312;&#36880;&#27493;&#21319;&#32423;&#34920;&#31034;&#27169;&#22411;&#26102;&#65292;&#22312;&#22270;&#24211;&#20013;&#25552;&#21462;&#25152;&#26377;&#20808;&#21069;&#30475;&#21040;&#30340;&#22270;&#20687;&#30340;&#26032;&#29305;&#24449;&#30340;&#38656;&#35201;&#65292;&#36825;&#36890;&#24120;&#22312;&#24222;&#22823;&#30340;&#22270;&#24211;&#38598;&#21644;/&#25110;&#23454;&#26102;&#31995;&#32479;&#20013;&#38750;&#24120;&#26114;&#36149;&#25110;&#19981;&#21487;&#34892;&#65288;&#21363;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#12289;&#31038;&#20132;&#32593;&#32476;&#12289;&#32456;&#36523;&#23398;&#20064;&#31995;&#32479;&#12289;&#26426;&#22120;&#20154;&#21644;&#30417;&#25511;&#31995;&#32479;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#22522;&#20110;&#24179;&#31283;&#24615;&#30340;&#20860;&#23481;&#34920;&#31034;&#65288;CoReS&#65289;&#65292;&#36890;&#36807;&#40723;&#21169;&#34920;&#31034;&#27169;&#22411;&#30340;&#24179;&#31283;&#24615;&#26469;&#23454;&#29616;&#20860;&#23481;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20808;&#21069;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#24179;&#31283;&#24615;&#20351;&#29305;&#24449;&#30340;&#32479;&#35745;&#29305;&#24615;&#22312;&#26102;&#38388;&#20559;&#31227;&#19979;&#19981;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method to learn internal feature representation models that are \textit{compatible} with previously learned ones. Compatible features enable for direct comparison of old and new learned features, allowing them to be used interchangeably over time. This eliminates the need for visual search systems to extract new features for all previously seen images in the gallery-set when sequentially upgrading the representation model. Extracting new features is typically quite expensive or infeasible in the case of very large gallery-sets and/or real time systems (i.e., face-recognition systems, social networks, life-long learning systems, robotics and surveillance systems). Our approach, called Compatible Representations via Stationarity (CoReS), achieves compatibility by encouraging stationarity to the learned representation model without relying on previously learned models. Stationarity allows features' statistical properties not to change under time shift so 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#65292;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2110.15829</link><description>&lt;p&gt;
&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Holistic Deep Learning. (arXiv:2110.15829v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#65292;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a holistic deep learning framework that addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. A prescriptive approach is provided to support practitioners in selecting an appropriate training loss function based on their specific objectives.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#39564;&#35777;&#25286;&#20998;&#30340;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36825;&#22312;&#23545;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#32467;&#26524;&#36827;&#19968;&#27493;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#21644;SHAP&#20540;&#20998;&#26512;&#36827;&#34892;&#39564;&#35777;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#21644;&#26435;&#34913;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#36341;&#32773;&#24212;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#25351;&#23548;&#24615;&#26041;&#27861;&#65292;&#26681;&#25454;&#20182;&#20204;&#30340;&#20855;&#20307;&#30446;&#26631;&#65292;&#25552;&#20379;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;&#25152;&#26377;&#29992;&#20110;&#37325;&#29616;&#32467;&#26524;&#30340;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/kimvc7/HDL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel holistic deep learning framework that simultaneously addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework holistically improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. The results are further validated by ablation experiments and SHAP value analysis, which reveal the interactions and trade-offs between the different evaluation metrics. To support practitioners applying our framework, we provide a prescriptive approach that offers recommendations for selecting an appropriate training loss function based on their specific objectives. All the code to reproduce the results can be found at https://github.com/kimvc7/HDL.
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#24212;&#29992;&#38754;&#20020;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#36755;&#20837;&#26469;&#32469;&#36807;&#31995;&#32479;&#24341;&#23548;&#38169;&#35823;&#39044;&#27979;&#65292;&#25991;&#31456;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#21644;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/1911.02621</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;--&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Threat of Adversarial Attacks on Machine Learning in Network Security -- A Survey. (arXiv:1911.02621v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.02621
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#24212;&#29992;&#38754;&#20020;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#36755;&#20837;&#26469;&#32469;&#36807;&#31995;&#32479;&#24341;&#23548;&#38169;&#35823;&#39044;&#27979;&#65292;&#25991;&#31456;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#21644;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20351;&#35768;&#22810;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26356;&#24555;&#12289;&#26356;&#20934;&#30830;&#12289;&#26356;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#38754;&#20020;&#27604;&#20854;&#20182;&#39046;&#22495;&#26356;&#19981;&#25104;&#27604;&#20363;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#12290;&#36825;&#26159;&#22240;&#20026;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#12289;&#20837;&#20405;&#26816;&#27979;&#21644;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#26412;&#36523;&#23601;&#26159;&#23545;&#25239;&#24615;&#30340;&#12290;&#22312;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#21487;&#33021;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#22330;&#20891;&#22791;&#31454;&#36187;&#30340;&#36807;&#31243;&#20013;&#65292;&#25915;&#20987;&#32773;&#19981;&#26029;&#22320;&#29992;&#19987;&#38376;&#35774;&#35745;&#26469;&#32469;&#36807;&#31995;&#32479;&#30340;&#36755;&#20837;&#26469;&#25506;&#27979;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#24182;&#35825;&#23548;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20219;&#21153;&#21644;&#28145;&#24230;&#30340;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32593;&#32476;&#23433;&#20840;&#24212;&#29992;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#38450;&#24481;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models have made many decision support systems to be faster, more accurate, and more efficient. However, applications of machine learning in network security face a more disproportionate threat of active adversarial attacks compared to other domains. This is because machine learning applications in network security such as malware detection, intrusion detection, and spam filtering are by themselves adversarial in nature. In what could be considered an arm's race between attackers and defenders, adversaries constantly probe machine learning systems with inputs that are explicitly designed to bypass the system and induce a wrong prediction. In this survey, we first provide a taxonomy of machine learning techniques, tasks, and depth. We then introduce a classification of machine learning in network security applications. Next, we examine various adversarial attacks against machine learning in network security and introduce two classification approaches for adversarial att
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#35861;&#24182;&#24471;&#21040;&#20102;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/1703.01347</link><description>&lt;p&gt;
&#24102;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#65306;&#26397;&#21521;&#36125;&#21494;&#26031;&#31070;&#35861;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles. (arXiv:1703.01347v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1703.01347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#35861;&#24182;&#24471;&#21040;&#20102;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#22122;&#22768;&#21644;&#32570;&#22833;&#39033;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#22122;&#22768;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#35266;&#27979;&#22122;&#22768;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#32473;&#20986;&#30340;&#36125;&#21494;&#26031;&#31070;&#35861;&#12290;&#25105;&#20204;&#30340;&#36125;&#21494;&#26031;&#20998;&#26512;&#21457;&#29616;&#65292;&#26368;&#20248;&#20551;&#35774;&#21487;&#33021;&#20250;&#36828;&#31163;&#28508;&#22312;&#30340;&#21487;&#23454;&#29616;&#20989;&#25968;&#65292;&#36825;&#21462;&#20915;&#20110;&#22122;&#22768;&#29305;&#24449;&#65292;&#36825;&#26159;&#39640;&#24230;&#38750;&#30452;&#35266;&#30340;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#30340;&#26080;&#22122;&#22768;&#35774;&#32622;&#19979;&#19981;&#20250;&#21457;&#29983;&#12290;&#36825;&#24847;&#21619;&#30528;&#32463;&#20856;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#38750;&#24179;&#20961;&#30340;&#36951;&#25022;&#30028;&#65288;regret bound&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#26088;&#22312;&#20174;&#36825;&#20010;&#27169;&#22411;&#19979;&#30340;&#35266;&#23519;&#20449;&#24687;&#20013;&#23454;&#29616;&#36125;&#21494;&#26031;&#31070;&#35861;&#65292;&#24403;&#26377;&#22823;&#37327;&#25163;&#33218;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;$\tilde{O}(d\sqrt{T})$&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study contextual linear bandit problems under feature uncertainty; they are noisy with missing entries. To address the challenges of the noise, we analyze Bayesian oracles given observed noisy features. Our Bayesian analysis finds that the optimal hypothesis can be far from the underlying realizability function, depending on the noise characteristics, which are highly non-intuitive and do not occur for classical noiseless setups. This implies that classical approaches cannot guarantee a non-trivial regret bound. Therefore, we propose an algorithm that aims at the Bayesian oracle from observed information under this model, achieving $\tilde{O}(d\sqrt{T})$ regret bound when there is a large number of arms. We demonstrate the proposed algorithm using synthetic and real-world datasets.
&lt;/p&gt;</description></item></channel></rss>