<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;BiLipNet&#65292;&#23427;&#20855;&#26377;&#35843;&#25511;&#36755;&#20986;&#25935;&#24863;&#24615;&#21644;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#26500;&#24314;&#20102;Bi-Lipschitz&#32593;&#32476;&#12290;&#21478;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#30340;PLNet&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#24212;&#29992;&#20110;&#23398;&#20064;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#30340;&#20248;&#21183;&#29305;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01344</link><description>&lt;p&gt;
&#21333;&#35843;&#12289;Bi-Lipschitz&#21644;Polyak-\L{}ojasiewicz&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Monotone, Bi-Lipschitz, and Polyak-\L{}ojasiewicz Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01344
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;BiLipNet&#65292;&#23427;&#20855;&#26377;&#35843;&#25511;&#36755;&#20986;&#25935;&#24863;&#24615;&#21644;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#36890;&#36807;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#26500;&#24314;&#20102;Bi-Lipschitz&#32593;&#32476;&#12290;&#21478;&#22806;&#65292;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#30340;PLNet&#65292;&#24182;&#20171;&#32461;&#20102;&#20854;&#24212;&#29992;&#20110;&#23398;&#20064;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#30340;&#20248;&#21183;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;BiLipNet&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#36870;&#30340;\emph{Bi-Lipschitz}&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#25511;&#21046;&#20854;\emph{Lipschitzness}&#65288;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#36755;&#20986;&#25935;&#24863;&#24615;&#65289;&#21644;\emph{inverse Lipschitzness}&#65288;&#19981;&#21516;&#36755;&#20986;&#30340;&#36755;&#20837;&#21487;&#21306;&#20998;&#24615;&#65289;&#30340;&#33021;&#21147;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#36870;&#27531;&#24046;&#23618;&#65292;&#20855;&#26377;&#35748;&#35777;&#30340;&#24378;&#21333;&#35843;&#24615;&#21644;Lipschitz&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#27491;&#20132;&#23618;&#32452;&#21512;&#20197;&#26500;&#24314;Bi-Lipschitz&#32593;&#32476;&#12290;&#35748;&#35777;&#26159;&#22522;&#20110;&#22686;&#37327;&#20108;&#27425;&#32422;&#26463;&#30340;&#65292;&#19982;&#35889;&#24402;&#19968;&#21270;&#30456;&#27604;&#65292;&#23427;&#33021;&#23454;&#29616;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#21453;&#21521;&#35745;&#31639;&#24418;&#24335;&#21270;&#20026;&#19977;&#31639;&#23376;&#20998;&#35010;&#38382;&#39064;&#65292;&#24050;&#30693;&#23384;&#22312;&#24555;&#36895;&#31639;&#27861;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;Bi-Lipschitz&#32593;&#32476;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#37327;&#36755;&#20986;&#32593;&#32476;&#65292;&#21363;PLNet&#65292;&#23427;&#28385;&#36275;Polyak-\L{}ojasiewicz&#26465;&#20214;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#26377;&#21033;&#29305;&#24615;&#30340;&#38750;&#20984;&#20195;&#29702;&#25439;&#22833;&#65292;&#20363;&#22914;&#29420;&#29305;&#24615;&#21644;&#39640;&#25928;&#35745;&#31639;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new \emph{bi-Lipschitz} invertible neural network, the BiLipNet, which has the ability to control both its \emph{Lipschitzness} (output sensitivity to input perturbations) and \emph{inverse Lipschitzness} (input distinguishability from different outputs). The main contribution is a novel invertible residual layer with certified strong monotonicity and Lipschitzness, which we compose with orthogonal layers to build bi-Lipschitz networks. The certification is based on incremental quadratic constraints, which achieves much tighter bounds compared to spectral normalization. Moreover, we formulate the model inverse calculation as a three-operator splitting problem, for which fast algorithms are known. Based on the proposed bi-Lipschitz network, we introduce a new scalar-output network, the PLNet, which satisfies the Polyak-\L{}ojasiewicz condition. It can be applied to learn non-convex surrogate losses with favourable properties, e.g., a unique and efficiently-computab
&lt;/p&gt;</description></item><item><title>Spiking CenterNet&#26159;&#19968;&#31181;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#33021;&#37327;&#12289;&#23567;&#22411;&#21270;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#22312;&#25361;&#25112;&#24615;&#30340;GEN1&#27773;&#36710;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#30340;&#33021;&#37327;&#19981;&#21040;&#19968;&#21322;&#65292;&#34920;&#29616;&#36229;&#36807;&#20102;&#21487;&#27604;&#36739;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01287</link><description>&lt;p&gt;
Spiking CenterNet:&#19968;&#31181;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#33976;&#39311;&#22686;&#24378;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Spiking CenterNet: A Distillation-boosted Spiking Neural Network for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01287
&lt;/p&gt;
&lt;p&gt;
Spiking CenterNet&#26159;&#19968;&#31181;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#33021;&#37327;&#12289;&#23567;&#22411;&#21270;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#22312;&#25361;&#25112;&#24615;&#30340;GEN1&#27773;&#36710;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#30340;&#33021;&#37327;&#19981;&#21040;&#19968;&#21322;&#65292;&#34920;&#29616;&#36229;&#36807;&#20102;&#21487;&#27604;&#36739;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#27668;&#20505;&#21464;&#21270;&#26102;&#20195;&#65292;&#23545;&#20110;&#39640;&#25928;&#33021;&#37327;&#12289;&#23567;&#22411;&#21270;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#19968;&#31181;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20854;&#20855;&#26377;&#20107;&#20214;&#39537;&#21160;&#30340;&#20449;&#24687;&#27969;&#21644;&#31232;&#30095;&#28608;&#27963;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20107;&#20214;&#25968;&#25454;&#30340;&#30446;&#26631;&#26816;&#27979;&#30340;Spiking CenterNet&#12290;&#23427;&#23558;SNN CenterNet&#36866;&#24212;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;M2U-Net&#30340;&#35299;&#30721;&#22120;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Prophesee&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;GEN1&#27773;&#36710;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#21487;&#27604;&#36739;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#19988;&#20351;&#29992;&#30340;&#33021;&#37327;&#19981;&#21040;&#19968;&#21322;&#12290;&#23558;&#38750;&#33033;&#20914;&#32769;&#24072;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#25105;&#20204;&#30340;SNN&#20013;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#33033;&#20914;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#39318;&#20010;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of AI at the edge, self-driving cars, and climate change, the need for energy-efficient, small, embedded AI is growing. Spiking Neural Networks (SNNs) are a promising approach to address this challenge, with their event-driven information flow and sparse activations. We propose Spiking CenterNet for object detection on event data. It combines an SNN CenterNet adaptation with an efficient M2U-Net-based decoder. Our model significantly outperforms comparable previous work on Prophesee's challenging GEN1 Automotive Detection Dataset while using less than half the energy. Distilling the knowledge of a non-spiking teacher into our SNN further increases performance. To the best of our knowledge, our work is the first approach that takes advantage of knowledge distillation in the field of spiking object detection.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#27700;&#24211;&#35745;&#31639;&#21644;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#39044;&#27979;&#22797;&#26434;&#21644;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24403;&#21333;&#29420;&#20351;&#29992;RC&#21644;NGRC&#32452;&#20214;&#19981;&#36275;&#20197;&#28385;&#36275;&#26102;&#65292;&#28151;&#21512;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.18953</link><description>&lt;p&gt;
&#23558;&#20256;&#32479;&#21644;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#28151;&#21512;&#20197;&#31934;&#30830;&#39640;&#25928;&#22320;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Hybridizing Traditional and Next-Generation Reservoir Computing to Accurately and Efficiently Forecast Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18953
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20256;&#32479;&#27700;&#24211;&#35745;&#31639;&#21644;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#39044;&#27979;&#22797;&#26434;&#21644;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#24403;&#21333;&#29420;&#20351;&#29992;RC&#21644;NGRC&#32452;&#20214;&#19981;&#36275;&#20197;&#28385;&#36275;&#26102;&#65292;&#28151;&#21512;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#24211;&#35745;&#31639;&#65288;RCs&#65289;&#26159;&#24378;&#22823;&#30340;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#65288;NGRCs&#65289;&#65292;&#30456;&#27604;RCs&#65292;&#23427;&#20204;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#22914;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#36739;&#20302;&#30340;&#25968;&#25454;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;NGRCs&#19982;RCs&#23384;&#22312;&#30528;&#33258;&#36523;&#29420;&#29305;&#30340;&#23454;&#38469;&#22256;&#38590;&#65292;&#21253;&#25324;&#23545;&#25968;&#25454;&#20013;&#30340;&#37319;&#26679;&#26102;&#38388;&#21644;&#38750;&#32447;&#24615;&#31867;&#22411;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22797;&#26434;&#21644;&#28151;&#27788;&#21160;&#21147;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28151;&#21512;RC-NGRC&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#28151;&#21512;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#30340;&#30701;&#26399;&#39044;&#27979;&#65292;&#24182;&#22312;RC&#21644;NGRC&#21508;&#33258;&#29420;&#31435;&#26102;&#33021;&#22815;&#25429;&#25417;&#21160;&#21147;&#31995;&#32479;&#30340;&#38271;&#26399;&#32479;&#35745;&#24773;&#20917;&#12290;&#24403;&#20004;&#20010;&#32452;&#20214;&#30340;&#39044;&#27979;&#33021;&#21147;&#37117;&#21463;&#38480;&#26102;&#65292;&#20363;&#22914;&#23545;&#20110;&#23567;&#30340;RC&#21644;&#22823;&#30340;&#37319;&#26679;&#26102;&#38388;&#26102;&#65292;&#28151;&#21512;RC-NGRC&#26041;&#27861;&#30340;&#20248;&#21183;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18953v1 Announce Type: new  Abstract: Reservoir computers (RCs) are powerful machine learning architectures for time series prediction. Recently, next generation reservoir computers (NGRCs) have been introduced, offering distinct advantages over RCs, such as reduced computational expense and lower data requirements. However, NGRCs have their own practical difficulties distinct from those of RCs, including sensitivity to sampling time and type of nonlinearities in the data. Here, we introduce a hybrid RC-NGRC approach for time series forecasting of complex and chaotic dynamical systems. We show that our hybrid approach can produce accurate short term predictions and capture the long term statistics of dynamical systems in situations where the RC and NGRC components alone are insufficient. The advantage of the hybrid RC-NGRC approach is most pronounced when both components are limited in their prediction capabilities, e.g. for a small RC and a large sampling time in the traini
&lt;/p&gt;</description></item><item><title>NL-ITI&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;ITI&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.18680</link><description>&lt;p&gt;
NL-ITI&#65306;&#20248;&#21270;&#25506;&#27979;&#21644;&#24178;&#39044;&#20197;&#25913;&#36827;ITI&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18680
&lt;/p&gt;
&lt;p&gt;
NL-ITI&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;ITI&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#23481;&#26131;&#36820;&#22238;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25512;&#29702;&#26102;&#24178;&#39044;(Inference-Time-Intervention, ITI)&#26041;&#27861;&#24341;&#20837;&#30340;&#33539;&#24335;&#12290;&#39318;&#20808;&#65292;ITI&#26041;&#27861;&#35782;&#21035;&#21253;&#21547;&#26368;&#22810;&#25152;&#38656;&#30693;&#35782;&#31867;&#22411;(&#20363;&#22914;&#30495;&#23454;&#20449;&#24687;)&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#38543;&#21518;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;LLM&#28608;&#27963;&#34987;&#31227;&#21160;&#21040;&#25152;&#36873;&#27880;&#24847;&#21147;&#22836;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;-&#38750;&#32447;&#24615;ITI(NL-ITI)&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;ITI&#26694;&#26550;&#12290;NL-ITI&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;TruthfulQA&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#22522;&#20934;&#19978;&#30456;&#23545;&#20110;&#22522;&#32447;ITI&#32467;&#26524;&#25253;&#21578;&#20102;&#32422;14%&#30340;MC1&#25351;&#26631;&#25913;&#36827;&#12290;NL-ITI&#36824;&#22312;&#20854;&#20182;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25104;&#32489;-&#22312;MMLU&#30340;&#21830;&#19994;&#20262;&#29702;&#23376;&#39046;&#22495;&#19978;&#65292;&#27604;&#22522;&#32447;LLaMA2-7B&#26377;&#32422;18%&#30340;MC1&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;NL-ITI&#22312;&#25928;&#26524;&#26356;&#22909;&#30340;&#21516;&#26102;&#20063;&#26356;&#23569;&#20405;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18680v1 Announce Type: new  Abstract: Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in t
&lt;/p&gt;</description></item><item><title>ABLQ&#26426;&#21046;&#22312;&#19981;&#21516;&#25209;&#22788;&#29702;&#37319;&#26679;&#19979;&#30340;&#38544;&#31169;&#20445;&#35777;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#65292;DP-SGD&#30340;&#23454;&#38469;&#23454;&#29616;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#27927;&#29260;&#30340;&#26041;&#27861;&#65292;&#20294;&#26356;&#21487;&#38752;&#30340;&#38544;&#31169;&#20998;&#26512;&#21364;&#26469;&#33258;&#20110;&#22522;&#20110;&#27850;&#26494;&#23376;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17673</link><description>&lt;p&gt;
DP-SGD&#30340;&#38544;&#31169;&#24615;&#26377;&#22810;&#31169;&#23494;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Private is DP-SGD?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17673
&lt;/p&gt;
&lt;p&gt;
ABLQ&#26426;&#21046;&#22312;&#19981;&#21516;&#25209;&#22788;&#29702;&#37319;&#26679;&#19979;&#30340;&#38544;&#31169;&#20445;&#35777;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#65292;DP-SGD&#30340;&#23454;&#38469;&#23454;&#29616;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#27927;&#29260;&#30340;&#26041;&#27861;&#65292;&#20294;&#26356;&#21487;&#38752;&#30340;&#38544;&#31169;&#20998;&#26512;&#21364;&#26469;&#33258;&#20110;&#22522;&#20110;&#27850;&#26494;&#23376;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25209;&#22788;&#29702;&#37319;&#26679;&#19979;&#65292;&#33258;&#36866;&#24212;&#25209;&#37327;&#32447;&#24615;&#26597;&#35810;&#65288;ABLQ&#65289;&#26426;&#21046;&#30340;&#38544;&#31169;&#20445;&#35777;&#20043;&#38388;&#23384;&#22312;&#30528;&#23454;&#36136;&#24615;&#24046;&#36317;&#65306;&#65288;i&#65289;&#27927;&#29260;&#65292;&#21644;&#65288;ii&#65289;&#27850;&#26494;&#23376;&#37319;&#26679;&#65307;&#20856;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20998;&#26512;&#36890;&#36807;&#23558;&#20854;&#35299;&#37322;&#20026;ABLQ&#30340;&#21518;&#22788;&#29702;&#26469;&#36827;&#34892;&#12290;&#34429;&#28982;&#22522;&#20110;&#27927;&#29260;&#30340;DP-SGD&#22312;&#23454;&#38469;&#23454;&#29616;&#20013;&#26356;&#24120;&#29992;&#65292;&#20294;&#23427;&#22312;&#38544;&#31169;&#20998;&#26512;&#19978;&#26082;&#19981;&#26131;&#20110;&#35299;&#26512;&#20063;&#19981;&#26131;&#20110;&#25968;&#20540;&#35745;&#31639;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#27850;&#26494;&#23376;&#37319;&#26679;&#30340;DP-SGD&#38590;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#65292;&#20294;&#20855;&#26377;&#33391;&#22909;&#29702;&#35299;&#30340;&#38544;&#31169;&#20998;&#26512;&#65292;&#26377;&#22810;&#20010;&#24320;&#28304;&#30340;&#25968;&#20540;&#32039;&#23494;&#30340;&#38544;&#31169;&#36134;&#25143;&#21487;&#29992;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#20570;&#27861;&#65292;&#21363;&#20351;&#29992;&#22522;&#20110;&#27927;&#29260;&#30340;DP-SGD&#65292;&#20294;&#20351;&#29992;&#30456;&#24212;&#27850;&#26494;&#23376;&#37319;&#26679;&#29256;&#26412;&#30340;&#38544;&#31169;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38544;&#31169;&#20998;&#26512;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17673v1 Announce Type: new  Abstract: We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis. On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;STGED&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25112;&#26415;&#36890;&#20449;&#32593;&#32476;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#29366;&#24577;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#26469;&#29366;&#24577;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.13872</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22270;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#25112;&#26415;&#32593;&#32476;&#26410;&#26469;&#29366;&#24577;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;STGED&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25112;&#26415;&#36890;&#20449;&#32593;&#32476;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#29366;&#24577;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#26469;&#29366;&#24577;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
tbd:&#25112;&#26415;&#33258;&#32452;&#32455;&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#21160;&#24577;&#21644;&#22810;&#36339;&#29305;&#24615;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#32593;&#32476;&#36830;&#25509;&#23545;&#20110;&#26377;&#25928;&#30340;&#36164;&#28304;&#20998;&#37197;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;STGED&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25112;&#26415;&#36890;&#20449;&#32593;&#32476;&#65292;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#29366;&#24577;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#26469;&#23398;&#20064;&#28508;&#22312;&#30340;&#25112;&#26415;&#34892;&#20026;&#12290;STGED&#23618;&#27425;&#22320;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#26426;&#21046;&#23545;&#19968;&#31995;&#21015;&#36890;&#20449;&#32593;&#32476;&#29366;&#24577;&#36827;&#34892;&#31354;&#38388;&#32534;&#30721;&#65292;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#29366;&#24577;&#30340;&#28436;&#21464;&#36827;&#34892;&#26102;&#38388;&#32534;&#30721;&#65292;&#24182;&#21033;&#29992;&#20840;&#36830;&#25509;&#21069;&#39304;&#32593;&#32476;&#26469;&#35299;&#30721;&#26410;&#26469;&#29366;&#24577;&#19979;&#30340;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;STGED&#22312;&#19981;&#21516;&#26102;&#38388;&#27493;&#36755;&#20837;&#19979;&#19968;&#30452;&#27604;&#22522;&#32447;&#27169;&#22411;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13872v1 Announce Type: new  Abstract: Resource allocation in tactical ad-hoc networks presents unique challenges due to their dynamic and multi-hop nature. Accurate prediction of future network connectivity is essential for effective resource allocation in such environments. In this paper, we introduce the Spatial-Temporal Graph Encoder-Decoder (STGED) framework for Tactical Communication Networks that leverages both spatial and temporal features of network states to learn latent tactical behaviors effectively. STGED hierarchically utilizes graph-based attention mechanism to spatially encode a series of communication network states, leverages a recurrent neural network to temporally encode the evolution of states, and a fully-connected feed-forward network to decode the connectivity in the future state. Through extensive experiments, we demonstrate that STGED consistently outperforms baseline models by large margins across different time-steps input, achieving an accuracy of
&lt;/p&gt;</description></item><item><title>ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09871</link><description>&lt;p&gt;
ThermoHands&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20027;&#35266;&#35270;&#35282;&#28909;&#22270;&#20013;&#20272;&#35745;3D&#25163;&#37096;&#23039;&#21183;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09871
&lt;/p&gt;
&lt;p&gt;
ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ThermoHands&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#28909;&#22270;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#20811;&#26381;&#35832;&#22914;&#20809;&#29031;&#21464;&#21270;&#21644;&#36974;&#25377;&#65288;&#20363;&#22914;&#25163;&#37096;&#31359;&#25140;&#29289;&#65289;&#31561;&#25361;&#25112;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;28&#21517;&#20027;&#20307;&#36827;&#34892;&#25163;-&#29289;&#20307;&#21644;&#25163;-&#34394;&#25311;&#20132;&#20114;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#20934;&#30830;&#26631;&#27880;&#20102;3D&#25163;&#37096;&#23039;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#21033;&#29992;&#21452;transformer&#27169;&#22359;&#22312;&#28909;&#22270;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;TheFormer&#30340;&#39046;&#20808;&#24615;&#33021;&#65292;&#24182;&#30830;&#35748;&#20102;&#28909;&#25104;&#20687;&#22312;&#23454;&#29616;&#24694;&#21155;&#26465;&#20214;&#19979;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09871v1 Announce Type: cross  Abstract: In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The benchmark includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.
&lt;/p&gt;</description></item><item><title>BurstAttention&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#20869;&#23384;&#35775;&#38382;&#21644;&#36890;&#20449;&#25805;&#20316;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#26497;&#38271;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.09347</link><description>&lt;p&gt;
BurstAttention&#65306;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#26497;&#38271;&#24207;&#21015;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09347
&lt;/p&gt;
&lt;p&gt;
BurstAttention&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#20869;&#23384;&#35775;&#38382;&#21644;&#36890;&#20449;&#25805;&#20316;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#26497;&#38271;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#22312;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#36825;&#20123;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#20063;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;BurstAttention&#8221;&#30340;&#20998;&#24067;&#24335;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#20840;&#23616;&#38598;&#32676;&#21644;&#26412;&#22320;&#35774;&#22791;&#32423;&#21035;&#30340;&#20869;&#23384;&#35775;&#38382;&#21644;&#36890;&#20449;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09347v1 Announce Type: cross  Abstract: Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence proce
&lt;/p&gt;</description></item><item><title>LiveCodeBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#32858;&#28966;&#20110;&#20174;LeetCode&#12289;AtCoder&#21644;CodeForces&#31561;&#24179;&#21488;&#36830;&#32493;&#25910;&#38598;&#30340;&#26032;&#38382;&#39064;&#65292;&#35206;&#30422;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#12289;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#31561;&#26356;&#24191;&#27867;&#30340;&#20195;&#30721;&#30456;&#20851;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07974</link><description>&lt;p&gt;
LiveCodeBench&#65306;&#29992;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#21644;&#26080;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07974
&lt;/p&gt;
&lt;p&gt;
LiveCodeBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#32858;&#28966;&#20110;&#20174;LeetCode&#12289;AtCoder&#21644;CodeForces&#31561;&#24179;&#21488;&#36830;&#32493;&#25910;&#38598;&#30340;&#26032;&#38382;&#39064;&#65292;&#35206;&#30422;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#12289;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#31561;&#26356;&#24191;&#27867;&#30340;&#20195;&#30721;&#30456;&#20851;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#39046;&#22495;&#65292;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;LLMs&#30340;&#24320;&#21457;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#65288;&#20363;&#22914;HumanEval&#65292;MBPP&#65289;&#19981;&#20877;&#36275;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;LiveCodeBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#23427;&#20250;&#20174;&#19977;&#20010;&#31454;&#36187;&#24179;&#21488;&#65288;LeetCode&#12289;AtCoder&#21644;CodeForces&#65289;&#19978;&#36830;&#32493;&#22320;&#25910;&#38598;&#26032;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#36824;&#30528;&#37325;&#20851;&#27880;&#26356;&#24191;&#27867;&#30340;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#33021;&#21147;&#65292;&#22914;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#21644;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20195;&#30721;&#29983;&#25104;&#12290;&#30446;&#21069;&#65292;LiveCodeBench&#25176;&#31649;&#20102;&#22312;2023&#24180;5&#26376;&#33267;2024&#24180;2&#26376;&#20043;&#38388;&#21457;&#24067;&#30340;400&#20010;&#39640;&#36136;&#37327;&#32534;&#30721;&#38382;&#39064;&#12290;&#25105;&#20204;&#24050;&#32463;&#35780;&#20272;&#20102;9&#20010;&#22522;&#26412;LLMs&#21644;20&#20010;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07974v1 Announce Type: cross  Abstract: Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24357;&#21512;&#20102;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#26368;&#21518;&#36845;&#20195;&#30340;&#33391;&#22909;&#24615;&#33021;&#19982;&#29616;&#26377;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07723</link><description>&lt;p&gt;
&#20851;&#20110;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Last-Iterate Convergence of Shuffling Gradient Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07723
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#38024;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24357;&#21512;&#20102;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#26368;&#21518;&#36845;&#20195;&#30340;&#33391;&#22909;&#24615;&#33021;&#19982;&#29616;&#26377;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#65292;&#20063;&#34987;&#31216;&#20026;&#26080;&#26367;&#25442;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#21253;&#25324;&#19977;&#31181;&#27969;&#34892;&#31639;&#27861;&#65306;Random Reshuffle&#65288;RR&#65289;&#12289;Shuffle Once&#65288;SO&#65289;&#21644;Incremental Gradient&#65288;IG&#65289;&#12290;&#19982;&#32463;&#39564;&#25104;&#21151;&#30456;&#27604;&#65292;&#38271;&#26399;&#20197;&#26469;&#23545;&#20110;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#24182;&#19981;&#20805;&#20998;&#20102;&#35299;&#12290;&#26368;&#36817;&#65292;&#21482;&#20026;&#20984;&#20989;&#25968;&#30340;&#24179;&#22343;&#36845;&#20195;&#21644;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#21518;&#36845;&#20195;&#65288;&#20197;&#24179;&#26041;&#36317;&#31163;&#20026;&#24230;&#37327;&#65289;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#20989;&#25968;&#20540;&#24046;&#20316;&#20026;&#25910;&#25947;&#20934;&#21017;&#26102;&#65292;&#29616;&#26377;&#29702;&#35770;&#26080;&#27861;&#35299;&#37322;&#22312;&#19981;&#21516;&#35774;&#32622;&#20013;&#65288;&#20363;&#22914;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#65289;&#26368;&#21518;&#36845;&#20195;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#23454;&#36341;&#19982;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#38024;&#23545;&#30446;&#26631;&#20989;&#25968;&#35777;&#26126;&#20102;&#27927;&#29260;&#26799;&#24230;&#26041;&#27861;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07723v1 Announce Type: new  Abstract: Shuffling gradient methods, which are also known as stochastic gradient descent (SGD) without replacement, are widely implemented in practice, particularly including three popular algorithms: Random Reshuffle (RR), Shuffle Once (SO), and Incremental Gradient (IG). Compared to the empirical success, the theoretical guarantee of shuffling gradient methods was not well-understanding for a long time. Until recently, the convergence rates had just been established for the average iterate for convex functions and the last iterate for strongly convex problems (using squared distance as the metric). However, when using the function value gap as the convergence criterion, existing theories cannot interpret the good performance of the last iterate in different settings (e.g., constrained optimization). To bridge this gap between practice and theory, we prove last-iterate convergence rates for shuffling gradient methods with respect to the objectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dataset Condensation&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#22495;&#21305;&#37197;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.07245</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#22495;&#21305;&#37197;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Dataset Condensation for Time Series Classification via Dual Domain Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Dataset Condensation&#8221;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#22495;&#21305;&#37197;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#21508;&#31181;&#30740;&#31350;&#39046;&#22495;&#20013;&#34987;&#35777;&#26126;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#31649;&#29702;&#22823;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;&#8220;&#25968;&#25454;&#38598;&#21387;&#32553;&#8221;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#29983;&#25104;&#19968;&#20010;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#22312;&#35832;&#22914;&#20998;&#31867;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#19982;&#23436;&#25972;&#30495;&#23454;&#25968;&#25454;&#38598;&#30456;&#36817;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22270;&#20687;&#21644;&#22270;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#23558;&#23427;&#20204;&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;&#39057;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07245v1 Announce Type: new  Abstract: Time series data has been demonstrated to be crucial in various research fields. The management of large quantities of time series data presents challenges in terms of deep learning tasks, particularly for training a deep neural network. Recently, a technique named \textit{Dataset Condensation} has emerged as a solution to this problem. This technique generates a smaller synthetic dataset that has comparable performance to the full real dataset in downstream tasks such as classification. However, previous methods are primarily designed for image and graph datasets, and directly adapting them to the time series dataset leads to suboptimal performance due to their inability to effectively leverage the rich information inherent in time series data, particularly in the frequency domain. In this paper, we propose a novel framework named Dataset \textit{\textbf{Cond}}ensation for \textit{\textbf{T}}ime \textit{\textbf{S}}eries \textit{\textbf{
&lt;/p&gt;</description></item><item><title>Caduceus &#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#21453;&#21521;&#20114;&#34917;&#24615;&#24182;&#20855;&#26377;&#21452;&#21521;&#24615;&#30340;&#38271;&#33539;&#22260; DNA &#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#22312;&#19979;&#28216;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.03234</link><description>&lt;p&gt;
Caduceus: &#21452;&#21521;&#31561;&#21464;&#38271;&#33539;&#22260; DNA &#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03234
&lt;/p&gt;
&lt;p&gt;
Caduceus &#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#21453;&#21521;&#20114;&#34917;&#24615;&#24182;&#20855;&#26377;&#21452;&#21521;&#24615;&#30340;&#38271;&#33539;&#22260; DNA &#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#22312;&#19979;&#28216;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#24207;&#21015;&#24314;&#27169;&#24341;&#21457;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#29616;&#22312;&#25193;&#23637;&#33267;&#29983;&#29289;&#23398;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#28982;&#32780;&#65292;&#24314;&#27169;&#22522;&#22240;&#32452;&#24207;&#21015;&#24341;&#20837;&#20102;&#25361;&#25112;&#65292;&#22914;&#38656;&#35201;&#24314;&#27169;&#38271;&#33539;&#22260;&#29255;&#27573;&#30456;&#20114;&#20316;&#29992;&#65292;&#22522;&#22240;&#32452;&#19978;&#28216;&#21644;&#19979;&#28216;&#21306;&#22495;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450; DNA &#30340;&#21453;&#21521;&#20114;&#34917;&#24615;&#65288;RC&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#36825;&#20123;&#25361;&#25112;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#23427;&#22522;&#20110;&#38271;&#33539;&#22260;&#30340; Mamba &#22359;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#25903;&#25345;&#21452;&#21521;&#24615;&#30340; BiMamba &#32452;&#20214;&#65292;&#20197;&#21450;&#25903;&#25345; RC &#31561;&#21464;&#24615;&#30340; MambaDNA &#22359;&#12290;&#25105;&#20204;&#20197; MambaDNA &#20026;&#22522;&#30784;&#65292;&#21019;&#36896;&#20102; Caduceus&#65292;&#31532;&#19968;&#20010;&#25903;&#25345; RC &#31561;&#21464;&#24615;&#30340;&#21452;&#21521;&#38271;&#33539;&#22260; DNA &#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#31574;&#30053;&#65292;&#29983;&#25104;&#20102; Caduceus DNA &#22522;&#30784;&#27169;&#22411;&#12290;Caduceus &#22312;&#19979;&#28216;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20197;&#21069;&#30340;&#38271;&#33539;&#22260;&#27169;&#22411;&#65307;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38271;&#33539;&#22260;&#21464;&#24322;&#25928;&#24212;&#39044;&#27979;&#20219;&#21153;&#19978;&#65292;Caduceus &#36229;&#36234;&#20102;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03234v1 Announce Type: cross  Abstract: Large-scale sequence modeling has sparked rapid advances that now extend into biology and genomics. However, modeling genomic sequences introduces challenges such as the need to model long-range token interactions, the effects of upstream and downstream regions of the genome, and the reverse complementarity (RC) of DNA. Here, we propose an architecture motivated by these challenges that builds off the long-range Mamba block, and extends it to a BiMamba component that supports bi-directionality, and to a MambaDNA block that additionally supports RC equivariance. We use MambaDNA as the basis of Caduceus, the first family of RC equivariant bi-directional long-range DNA language models, and we introduce pre-training and fine-tuning strategies that yield Caduceus DNA foundation models. Caduceus outperforms previous long-range models on downstream benchmarks; on a challenging long-range variant effect prediction task, Caduceus exceeds the pe
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#20419;&#20351;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.02437</link><description>&lt;p&gt;
SoK: &#32852;&#37030;&#21453;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
SoK: Challenges and Opportunities in Federated Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02437
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#20419;&#20351;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20110;2017&#24180;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20419;&#36827;&#20102;&#19981;&#20449;&#20219;&#26041;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#26080;&#38656;&#21508;&#26041;&#26126;&#30830;&#20849;&#20139;&#20854;&#25968;&#25454;&#12290;&#36825;&#20801;&#35768;&#22312;&#23562;&#37325;GDPR&#21644;CPRA&#31561;&#38544;&#31169;&#35268;&#23450;&#30340;&#21516;&#26102;&#65292;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26032;&#20852;&#30340;&#38544;&#31169;&#35201;&#27714;&#21487;&#33021;&#35201;&#27714;&#27169;&#22411;&#25152;&#26377;&#32773;&#33021;&#22815;&#8220;&#36951;&#24536;&#8221;&#19968;&#20123;&#24050;&#23398;&#20064;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#24403;&#25968;&#25454;&#25152;&#26377;&#32773;&#25110;&#25191;&#27861;&#26426;&#26500;&#35201;&#27714;&#26102;&#12290;&#36825;&#20652;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26426;&#22120;&#21453;&#23398;&#20064;&#8221;&#30340;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;FL&#30340;&#32972;&#26223;&#19979;&#65292;&#35768;&#22810;&#20026;&#38598;&#20013;&#24335;&#29615;&#22659;&#24320;&#21457;&#30340;&#21453;&#23398;&#20064;&#25216;&#26415;&#24182;&#19981;&#23481;&#26131;&#24212;&#29992;&#65281;&#36825;&#26159;&#30001;&#20110;FL&#20013;&#38598;&#20013;&#24335;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#20043;&#38388;&#30340;&#29420;&#29305;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#20114;&#21160;&#24615;&#12289;&#38543;&#26426;&#24615;&#12289;&#24322;&#26500;&#24615;&#21644;&#26377;&#38480;&#21487;&#35775;&#38382;&#24615;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#24037;&#20316;&#32858;&#28966;&#20110;&#24320;&#21457;&#36866;&#29992;&#20110;FL&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02437v1 Announce Type: cross  Abstract: Federated learning (FL), introduced in 2017, facilitates collaborative learning between non-trusting parties with no need for the parties to explicitly share their data among themselves. This allows training models on user data while respecting privacy regulations such as GDPR and CPRA. However, emerging privacy requirements may mandate model owners to be able to \emph{forget} some learned data, e.g., when requested by data owners or law enforcement. This has given birth to an active field of research called \emph{machine unlearning}. In the context of FL, many techniques developed for unlearning in centralized settings are not trivially applicable! This is due to the unique differences between centralized and distributed learning, in particular, interactivity, stochasticity, heterogeneity, and limited accessibility in FL. In response, a recent line of work has focused on developing unlearning mechanisms tailored to FL.   This SoK pape
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#37329;&#23383;&#22612;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02354</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Field Neural Networks for Air Quality Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;&#21644;&#37329;&#23383;&#22612;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#38382;&#39064;&#26088;&#22312;&#21033;&#29992;&#26469;&#33258;&#26377;&#38480;&#35266;&#27979;&#31449;&#30340;&#21382;&#21490;&#25968;&#25454;&#25512;&#26029;&#26410;&#30693;&#20301;&#32622;&#30340;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#12290;&#32771;&#34385;&#21040;&#35266;&#27979;&#31449;&#39640;&#26114;&#30340;&#32500;&#25252;&#25104;&#26412;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#33391;&#22909;&#30340;&#25512;&#26029;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#33410;&#32422;&#25104;&#26412;&#24182;&#32454;&#21270;&#25968;&#25454;&#31890;&#24230;&#12290;&#23613;&#31649;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#23545;&#29616;&#23454;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#21644;&#31163;&#25955;&#25968;&#25454;&#32467;&#26500;&#24314;&#27169;&#38480;&#21046;&#20102;&#28508;&#21147;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#21363;&#26102;&#31354;&#22330;&#31070;&#32463;&#32593;&#32476;&#65292;&#21450;&#20854;&#23545;&#24212;&#30340;&#26032;&#26694;&#26550;&#65292;&#37329;&#23383;&#22612;&#25512;&#26029;&#65292;&#23558;&#20004;&#31181;&#19981;&#21516;&#30340;&#26102;&#31354;&#35266;&#28857;&#65292;&#22330;&#21644;&#22270;&#65292;&#30456;&#32467;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20013;&#22269;&#22823;&#38470;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#31354;&#27668;&#36136;&#37327;&#25512;&#26029;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02354v1 Announce Type: cross  Abstract: The air quality inference problem aims to utilize historical data from a limited number of observation sites to infer the air quality index at an unknown location. Considering the sparsity of data due to the high maintenance cost of the stations, good inference algorithms can effectively save the cost and refine the data granularity. While spatio-temporal graph neural networks have made excellent progress on this problem, their non-Euclidean and discrete data structure modeling of reality limits its potential. In this work, we make the first attempt to combine two different spatio-temporal perspectives, fields and graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and its corresponding new framework, Pyramidal Inference. Extensive experiments validate that our model achieves state-of-the-art performance in nationwide air quality inference in the Chinese Mainland, demonstrating the superiority of our proposed model 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36229;&#20986;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02271</link><description>&lt;p&gt;
RIFF: &#23398;&#20064;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#25913;&#20889;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02271
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#36229;&#20986;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#31934;&#30830;&#22320;&#20026;&#19979;&#28216;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20248;&#21270;&#36755;&#20837;&#25552;&#31034;&#25110;&#35843;&#25972;&#23569;&#37327;&#27169;&#22411;&#21442;&#25968;&#65288;&#20363;&#22914; LoRA&#65289;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25913;&#21464;&#21407;&#22987;&#20219;&#21153;&#30340;&#36755;&#20837;&#25991;&#26412;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#26368;&#26377;&#25928;&#22320;&#37325;&#20889;&#36755;&#20837;&#25991;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#37322;&#20041;&#27169;&#22411;&#12290;&#20351;&#29992;&#20845;&#20010;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#29992;&#37322;&#20041;&#20016;&#23500;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#36229;&#20986;&#20102;&#20165;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02271v1 Announce Type: new  Abstract: Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter-efficient fine-tuning methods that optimize input prompts or adjust a small number of model parameters (e.g LoRA). In this study, we explore the impact of altering the input text of the original task in conjunction with parameter-efficient fine-tuning methods. To most effectively rewrite the input text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood objective. Using six few-shot text classification datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#22522;&#20110;&#23376;&#40784;&#27425;&#31639;&#23376;&#21644;&#38750;&#32447;&#24615;Perron-Frobenius&#29702;&#35770;&#65292;&#20026;&#38544;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#28857;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.00720</link><description>&lt;p&gt;
&#23376;&#40784;&#27425;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Subhomogeneous Deep Equilibrium Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#22522;&#20110;&#23376;&#40784;&#27425;&#31639;&#23376;&#21644;&#38750;&#32447;&#24615;Perron-Frobenius&#29702;&#35770;&#65292;&#20026;&#38544;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#28857;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#25552;&#20379;&#20102;&#26356;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#24180;&#26469;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20316;&#20026;&#20256;&#32479;&#32593;&#32476;&#30340;&#24378;&#22823;&#26367;&#20195;&#26041;&#26696;&#32780;&#21457;&#23637;&#22766;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#20445;&#35777;&#65292;&#24341;&#21457;&#31283;&#23450;&#24615;&#12289;&#24615;&#33021;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#22522;&#20110;&#23376;&#40784;&#27425;&#31639;&#23376;&#21644;&#38750;&#32447;&#24615;Perron-Frobenius&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38544;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22266;&#23450;&#28857;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#30340;&#26032;&#20998;&#26512;&#12290;&#19982;&#20808;&#21069;&#31867;&#20284;&#20998;&#26512;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20801;&#35768;&#23545;&#21442;&#25968;&#30697;&#38453;&#25552;&#20986;&#26356;&#24369;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#20026;&#23450;&#20041;&#33391;&#22909;&#30340;&#38544;&#24335;&#32593;&#32476;&#25552;&#20379;&#26356;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#21069;&#39304;&#12289;&#21367;&#31215;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#23376;&#40784;&#27425;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00720v1 Announce Type: new  Abstract: Implicit-depth neural networks have grown as powerful alternatives to traditional networks in various applications in recent years. However, these models often lack guarantees of existence and uniqueness, raising stability, performance, and reproducibility issues. In this paper, we present a new analysis of the existence and uniqueness of fixed points for implicit-depth neural networks based on the concept of subhomogeneous operators and the nonlinear Perron-Frobenius theory. Compared to previous similar analyses, our theory allows for weaker assumptions on the parameter matrices, thus yielding a more flexible framework for well-defined implicit networks. We illustrate the performance of the resulting subhomogeneous networks on feed-forward, convolutional, and graph neural network examples.
&lt;/p&gt;</description></item><item><title>Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18334</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#29992;&#20110;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18334
&lt;/p&gt;
&lt;p&gt;
Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bonito&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#65292;&#29992;&#20110;&#26465;&#20214;&#20219;&#21153;&#29983;&#25104;&#65306;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;&#30340;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#29992;&#25143;&#19987;&#38376;&#30340;&#31169;&#20154;&#25968;&#25454;&#19978;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;1.65M&#20010;&#31034;&#20363;&#30340;&#26032;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;Bonito&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#37325;&#26032;&#28151;&#21512;&#25104;&#20803;&#27169;&#26495;&#32780;&#21019;&#24314;&#30340;&#12290;&#25968;&#25454;&#38598;&#30340;&#20803;&#27169;&#26495;&#20135;&#29983;&#35757;&#32451;&#31034;&#20363;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#21644;&#20219;&#21153;&#23646;&#24615;&#65292;&#36755;&#20986;&#21253;&#25324;&#25351;&#20196;&#21644;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;Bonito&#20026;&#19971;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#21512;&#25104;&#20219;&#21153;&#65292;&#36328;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411; -- &#26159;&#38750;&#38382;&#31572;&#12289;&#25277;&#21462;&#24335;&#38382;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702; -- &#24182;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Bonito&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18334v1 Announce Type: new  Abstract: We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.18059</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#65292;&#22312;&#20445;&#35777;&#26816;&#27979;&#24615;&#30340;&#21516;&#26102;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#28508;&#22312;&#22320;&#23384;&#22312;&#35823;&#23548;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#21306;&#20998;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#21644;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#26469;&#21152;&#20197;&#35268;&#33539;&#30340;&#24517;&#35201;&#24615;&#12290;&#27700;&#21360;&#25216;&#26415;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#28041;&#21450;&#22312;LLM&#25512;&#29702;&#38454;&#27573;&#21521;&#25991;&#26412;&#20013;&#23884;&#20837;&#38544;&#34255;&#26631;&#35760;&#65292;&#32780;&#36825;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#19981;&#21487;&#24863;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#31639;&#27861;&#38754;&#20020;&#30528;&#23454;&#29616;&#25554;&#20837;&#27700;&#21360;&#30340;&#21487;&#26816;&#27979;&#24615;&#21644;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#20004;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#22686;&#24378;&#20854;&#20013;&#19968;&#20010;&#26041;&#38754;&#24120;&#24120;&#20250;&#25439;&#23475;&#21478;&#19968;&#20010;&#26041;&#38754;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#27700;&#21360;&#25216;&#26415;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#32593;&#32476;&#29983;&#25104;&#29305;&#23450;&#20196;&#29260;&#27700;&#21360;logits&#21644;&#20998;&#21106;&#27604;&#29575;&#12290;&#36890;&#36807;&#21033;&#29992;MOO&#26469;&#20248;&#21270;&#26816;&#27979;&#21644;&#35821;&#20041;&#30446;&#26631;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#23454;&#29616;&#20102;&#21487;&#26816;&#27979;&#24615;&#21644;&#35821;&#20041;&#23436;&#25972;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18059v1 Announce Type: cross  Abstract: Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Current watermarking algorithms, however, face the challenge of achieving both the detectability of inserted watermarks and the semantic integrity of generated texts, where enhancing one aspect often undermines the other. To overcome this, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that ou
&lt;/p&gt;</description></item><item><title>&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.17641</link><description>&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
Variational Learning is Effective for Large Deep Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17641
&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#35777;&#35777;&#25454;&#65292;&#21453;&#39539;&#20102;&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#26080;&#25928;&#30340;&#26222;&#36941;&#30475;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;Improved Variational Online Newton (IVON)&#30340;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#65288;&#22914;GPT-2&#21644;ResNets&#65289;&#26102;&#22987;&#32456;&#33021;&#22815;&#19982;Adam&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#23427;&#12290;IVON&#30340;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#19982;Adam&#30456;&#21516;&#65292;&#20294;&#20854;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#22909;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IVON&#30340;&#20960;&#31181;&#26032;&#29992;&#20363;&#65292;&#20854;&#20013;&#25105;&#20204;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#27169;&#22411;&#21512;&#24182;&#65292;&#22312;&#20934;&#30830;&#39044;&#27979;&#27867;&#21270;&#35823;&#24046;&#21644;&#24544;&#23454;&#20272;&#35745;&#23545;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#37327;&#25903;&#25345;&#21464;&#20998;&#23398;&#20064;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17641v1 Announce Type: cross  Abstract: We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.
&lt;/p&gt;</description></item><item><title>LCEN&#31639;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#12289;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#31232;&#30095;&#22320;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17120</link><description>&lt;p&gt;
LCEN&#65306;&#19968;&#31181;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17120
&lt;/p&gt;
&lt;p&gt;
LCEN&#31639;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#12289;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#31232;&#30095;&#22320;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#40657;&#30418;&#26550;&#26500;&#20855;&#26377;&#20248;&#21183;&#65292;&#22312;&#20851;&#38190;&#39046;&#22495;&#22914;&#33322;&#31354;&#25110;&#21307;&#23398;&#20013;&#65292;&#21487;&#35299;&#37322;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26368;&#31616;&#21333;&#12289;&#26368;&#24120;&#29992;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#65288;&#22914;LASSO&#25110;EN&#65289;&#20165;&#38480;&#20110;&#32447;&#24615;&#39044;&#27979;&#65292;&#24182;&#19988;&#29305;&#24449;&#36873;&#25321;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LASSO-Clip-EN&#65288;LCEN&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#12289;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;LCEN&#22312;&#22810;&#31181;&#20154;&#24037;&#21644;&#23454;&#35777;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#29983;&#25104;&#27604;&#20854;&#20182;&#24120;&#29992;&#26550;&#26500;&#26356;&#20934;&#30830;&#12289;&#26356;&#31232;&#30095;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;&#65292;LCEN&#23545;&#25968;&#25454;&#38598;&#21644;&#24314;&#27169;&#20013;&#36890;&#24120;&#23384;&#22312;&#30340;&#35768;&#22810;&#38382;&#39064;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#22122;&#22768;&#12289;&#22810;&#37325;&#20849;&#32447;&#24615;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#36229;&#21442;&#25968;&#26041;&#24046;&#12290;LCEN&#36824;&#33021;&#22815;&#20174;&#23454;&#35777;&#25968;&#25454;&#20013;&#37325;&#26032;&#21457;&#29616;&#22810;&#20010;&#29289;&#29702;&#23450;&#24459;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17120v1 Announce Type: new  Abstract: Interpretable architectures can have advantages over black-box architectures, and interpretability is essential for the application of machine learning in critical settings, such as aviation or medicine. However, the simplest, most commonly used interpretable architectures (such as LASSO or EN) are limited to linear predictions and have poor feature selection capabilities. In this work, we introduce the LASSO-Clip-EN (LCEN) algorithm for the creation of nonlinear, interpretable machine learning models. LCEN is tested on a wide variety of artificial and empirical datasets, creating more accurate, sparser models than other commonly used architectures. These experiments reveal that LCEN is robust against many issues typically present in datasets and modeling, including noise, multicollinearity, data scarcity, and hyperparameter variance. LCEN is also able to rediscover multiple physical laws from empirical data and, for processes with no kn
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#34892;&#22870;&#21169;&#38598;&#27010;&#24565;&#65292;&#20197;&#24212;&#23545;&#31163;&#32447;&#35774;&#23450;&#30340;&#26426;&#20250;&#21644;&#38480;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#20272;&#35745;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15392</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#24378;&#21270;&#23398;&#20064;&#65306;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#27010;&#24565;&#21644;&#21487;&#35777;&#26126;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Offline Inverse RL: New Solution Concepts and Provably Efficient Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15392
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#34892;&#22870;&#21169;&#38598;&#27010;&#24565;&#65292;&#20197;&#24212;&#23545;&#31163;&#32447;&#35774;&#23450;&#30340;&#26426;&#20250;&#21644;&#38480;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#20272;&#35745;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26088;&#22312;&#20174;&#34892;&#20026;&#28436;&#31034;&#20013;&#24674;&#22797;&#19987;&#23478;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#30446;&#21069;&#24050;&#32463;&#36880;&#28176;&#20197;&#20272;&#35745;&#21487;&#34892;&#22870;&#21169;&#38598;&#21512;&#20316;&#20026;IRL&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#36873;&#25321;&#21333;&#19968;&#22870;&#21169;&#25512;&#36831;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#29616;&#26377;&#30340;&#21046;&#23450;&#21644;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38024;&#23545;&#22312;&#32447;&#35774;&#32622;&#25552;&#20986;&#65292;&#24182;&#36798;&#21040;&#20998;&#26512;&#65292;&#36825;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#26126;&#26174;&#19981;&#29616;&#23454;&#65292;&#22312;&#37027;&#37324;&#31163;&#32447;&#25968;&#25454;&#38598;&#26356;&#20026;&#26222;&#36941;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#25429;&#25417;&#31163;&#32447;&#29615;&#22659;&#26426;&#36935;&#21644;&#38480;&#21046;&#30340;&#21487;&#34892;&#22870;&#21169;&#38598;&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#20272;&#35745;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15392v1 Announce Type: new  Abstract: Inverse reinforcement learning (IRL) aims to recover the reward function of an expert agent from demonstrations of behavior. It is well known that the IRL problem is fundamentally ill-posed, i.e., many reward functions can explain the demonstrations. For this reason, IRL has been recently reframed in terms of estimating the feasible reward set, thus, postponing the selection of a single reward. However, so far, the available formulations and algorithmic solutions have been proposed and analyzed mainly for the online setting, where the learner can interact with the environment and query the expert at will. This is clearly unrealistic in most practical applications, where the availability of an offline dataset is a much more common scenario. In this paper, we introduce a novel notion of feasible reward set capturing the opportunities and limitations of the offline setting and we analyze the complexity of its estimation. This requires the i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;&#65292;&#24212;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#19981;&#21516;&#39118;&#26684;&#65292;&#21516;&#26102;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.15332</link><description>&lt;p&gt;
&#20998;&#31867;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#31181;&#20851;&#20110;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Categorical Deep Learning: An Algebraic Theory of Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#20195;&#25968;&#29702;&#35770;&#65292;&#24212;&#29992;&#33539;&#30068;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#26725;&#26753;&#65292;&#26377;&#25928;&#22320;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#19981;&#21516;&#39118;&#26684;&#65292;&#21516;&#26102;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#25351;&#23450;&#21644;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#36890;&#29992;&#26694;&#26550;&#30340;&#31435;&#22330;&#12290;&#25105;&#20204;&#35748;&#20026;&#21040;&#30446;&#21069;&#20026;&#27490;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#20851;&#38190;&#23581;&#35797;&#32570;&#20047;&#19968;&#31181;&#19968;&#33268;&#30340;&#26725;&#26753;&#65292;&#33021;&#22815;&#25351;&#23450;&#27169;&#22411;&#24517;&#39035;&#28385;&#36275;&#30340;&#32422;&#26463;&#24182;&#35268;&#23450;&#23427;&#20204;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;&#19987;&#27880;&#20110;&#26500;&#24314;&#36825;&#26679;&#19968;&#20010;&#26725;&#26753;&#65292;&#25105;&#20204;&#24314;&#35758;&#24212;&#29992;&#33539;&#30068;&#35770;&#8212;&#8212;&#20934;&#30830;&#22320;&#35828;&#65292;&#21333;&#23376;&#20540;&#20110;&#21442;&#25968;&#26144;&#23556;&#30340;&#20108;&#33539;&#30068;&#30340;&#36890;&#29992;&#20195;&#25968;&#8212;&#8212;&#20316;&#20026;&#19968;&#31181;&#21333;&#19968;&#29702;&#35770;&#65292;&#20248;&#38597;&#22320;&#21253;&#21547;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#36825;&#20004;&#31181;&#39118;&#26684;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#29702;&#35770;&#22914;&#20309;&#24674;&#22797;&#30001;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#23548;&#33268;&#30340;&#32422;&#26463;&#65292;&#20197;&#21450;&#20174;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#31181;&#26550;&#26500;&#65288;&#22914;RNNs&#65289;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#19968;&#29702;&#35770;&#22914;&#20309;&#33258;&#28982;&#22320;&#32534;&#30721;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#33258;&#21160;&#26426;&#29702;&#35770;&#20013;&#30340;&#35768;&#22810;&#26631;&#20934;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15332v1 Announce Type: cross  Abstract: We present our position on the elusive quest for a general-purpose framework for specifying and studying deep learning architectures. Our opinion is that the key attempts made so far lack a coherent bridge between specifying constraints which models must satisfy and specifying their implementations. Focusing on building a such a bridge, we propose to apply category theory -- precisely, the universal algebra of monads valued in a 2-category of parametric maps -- as a single theory elegantly subsuming both of these flavours of neural network design. To defend our position, we show how this theory recovers constraints induced by geometric deep learning, as well as implementations of many architectures drawn from the diverse landscape of neural networks, such as RNNs. We also illustrate how the theory naturally encodes many standard constructs in computer science and automata theory.
&lt;/p&gt;</description></item><item><title>PEMT &#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#26435;&#37325;&#32452;&#21512;&#25429;&#33719;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15082</link><description>&lt;p&gt;
PEMT: &#22810;&#20219;&#21153;&#30456;&#20851;&#24615;&#24341;&#23548;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15082
&lt;/p&gt;
&lt;p&gt;
PEMT &#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#65292;&#20197;&#26435;&#37325;&#32452;&#21512;&#25429;&#33719;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#65292;&#20174;&#32780;&#26377;&#25928;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#20316;&#20026;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#24050;&#32463;&#23835;&#36215;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#20219;&#21153;&#36716;&#31227;&#30693;&#35782;&#21040;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#20197;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#20135;&#29983;&#20102;&#36234;&#26469;&#36234;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#36866;&#37197;&#22120;&#65292;&#35201;&#20040;&#20174;&#28304;&#20219;&#21153;&#20013;&#25552;&#21462;&#20849;&#20139;&#30693;&#35782;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21644;&#28304;&#20219;&#21153;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEMT&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#36801;&#31227;&#23398;&#20064;&#30340;&#21019;&#26032;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#12290;PEMT&#23558;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26694;&#26550;&#25193;&#23637;&#20026;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#30340;&#21152;&#26435;&#32452;&#21512;&#20197;&#25429;&#33719;&#21487;&#36716;&#31227;&#30693;&#35782;&#12290;&#36825;&#20123;&#26435;&#37325;&#30001;&#19968;&#20010;&#38376;&#25511;&#21333;&#20803;&#30830;&#23450;&#65292;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#27979;&#37327;&#30446;&#26631;&#20219;&#21153;&#21644;&#27599;&#20010;&#28304;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15082v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as an effective method for adapting pre-trained language models to various tasks efficiently. Recently, there has been a growing interest in transferring knowledge from one or multiple tasks to the downstream target task to achieve performance improvements. However, current approaches typically either train adapters on individual tasks or distill shared knowledge from source tasks, failing to fully exploit task-specific knowledge and the correlation between source and target tasks. To overcome these limitations, we propose PEMT, a novel parameter-efficient fine-tuning framework based on multi-task transfer learning. PEMT extends the mixture-of-experts (MoE) framework to capture the transferable knowledge as a weighted combination of adapters trained on source tasks. These weights are determined by a gated unit, measuring the correlation between the target and each source task using task 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#24182;&#36890;&#36807;&#21452;&#37325;&#31283;&#20581;CPO(DR-CPO)&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.14979</link><description>&lt;p&gt;
&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#26159;&#19968;&#20010;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Optimizing Language Models for Human Preferences is a Causal Inference Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#24182;&#36890;&#36807;&#21452;&#37325;&#31283;&#20581;CPO(DR-CPO)&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23398;&#26415;&#21644;&#21830;&#19994;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#25991;&#26412;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20102;&#20174;&#30452;&#25509;&#32467;&#26524;&#25968;&#25454;&#38598;&#20013;&#38024;&#23545;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#30001;&#19968;&#27573;&#25991;&#26412;&#21644;&#19968;&#20010;&#34913;&#37327;&#35835;&#32773;&#21709;&#24212;&#30340;&#30456;&#20851;&#25968;&#20540;&#32467;&#26524;&#32452;&#25104;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#24212;&#23558;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#35270;&#20026;&#19968;&#20010;&#22240;&#26524;&#38382;&#39064;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#27491;&#30830;&#23398;&#20064;&#25991;&#26412;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#27491;&#24335;&#21270;&#20102;&#36825;&#20010;&#22240;&#26524;&#35821;&#35328;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;--&#22240;&#26524;&#20559;&#22909;&#20248;&#21270;(CPO)--&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26080;&#20559;&#26367;&#20195;&#30446;&#26631;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#21452;&#37325;&#31283;&#20581;&#30340;CPO(DR-CPO)&#25193;&#23637;CPO&#65292;&#38477;&#20302;&#20102;&#26367;&#20195;&#30446;&#26631;&#30340;&#26041;&#24046;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#26126;&#26174;&#24378;&#26377;&#21147;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14979v1 Announce Type: cross  Abstract: As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarante
&lt;/p&gt;</description></item><item><title>Equilibrium K-Means&#65288;EKM&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;K&#22343;&#20540;&#31867;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#32858;&#31867;&#20013;&#24515;&#22312;&#22823;&#31867;&#31751;&#20013;&#24515;&#32858;&#38598;&#30340;&#20542;&#21521;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14490</link><description>&lt;p&gt;
&#20351;&#29992;Equilibrium K-Means&#36827;&#34892;&#19981;&#24179;&#34913;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imbalanced Data Clustering using Equilibrium K-Means
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14490
&lt;/p&gt;
&lt;p&gt;
Equilibrium K-Means&#65288;EKM&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;K&#22343;&#20540;&#31867;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#32858;&#31867;&#20013;&#24515;&#22312;&#22823;&#31867;&#31751;&#20013;&#24515;&#32858;&#38598;&#30340;&#20542;&#21521;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#25968;&#25454;&#25351;&#30340;&#26159;&#25968;&#25454;&#28857;&#22312;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#20998;&#24067;&#19981;&#22343;&#34913;&#65292;&#36825;&#32473;&#20256;&#32479;&#30340;&#30828;&#32858;&#31867;&#31639;&#27861;&#21644;&#27169;&#31946;&#32858;&#31867;&#31639;&#27861;&#65288;&#22914;&#30828;K&#22343;&#20540;&#65288;HKM&#65292;&#25110;&#32773;Lloyd&#31639;&#27861;&#65289;&#21644;&#27169;&#31946;K&#22343;&#20540;&#65288;FKM&#65292;&#25110;&#32773;Bezdek&#31639;&#27861;&#65289;&#65289;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#31616;&#21333;&#30340;K&#22343;&#20540;&#31867;&#22411;&#31639;&#27861;&#8212;&#8212;Equilibrium K-Means&#65288;EKM&#65289;&#65292;&#23427;&#22312;&#20004;&#20010;&#27493;&#39588;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26524;&#65292;&#20943;&#23569;&#20102;&#32858;&#31867;&#20013;&#24515;&#21521;&#22823;&#31867;&#31751;&#20013;&#24515;&#32858;&#38598;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;HKM&#12289;FKM&#21644;EKM&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#34920;&#26126;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#20855;&#26377;&#26126;&#30830;&#20851;&#31995;&#30340;&#29275;&#39039;&#26041;&#27861;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;EKM&#20855;&#26377;&#19982;FKM&#30456;&#21516;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#20294;&#23545;&#20854;&#25104;&#21592;&#23450;&#20041;&#25552;&#20379;&#20102;&#26356;&#28165;&#26224;&#30340;&#29289;&#29702;&#24847;&#20041;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#21313;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;EKM&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14490v1 Announce Type: new  Abstract: Imbalanced data, characterized by an unequal distribution of data points across different clusters, poses a challenge for traditional hard and fuzzy clustering algorithms, such as hard K-means (HKM, or Lloyd's algorithm) and fuzzy K-means (FKM, or Bezdek's algorithm). This paper introduces equilibrium K-means (EKM), a novel and simple K-means-type algorithm that alternates between just two steps, yielding significantly improved clustering results for imbalanced data by reducing the tendency of centroids to crowd together in the center of large clusters. We also present a unifying perspective for HKM, FKM, and EKM, showing they are essentially gradient descent algorithms with an explicit relationship to Newton's method. EKM has the same time and space complexity as FKM but offers a clearer physical meaning for its membership definition. We illustrate the performance of EKM on two synthetic and ten real datasets, comparing it to various cl
&lt;/p&gt;</description></item><item><title>Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;</title><link>https://arxiv.org/abs/2402.13212</link><description>&lt;p&gt;
&#36719;&#33258;&#19968;&#33268;&#24615;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency Improves Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13212
&lt;/p&gt;
&lt;p&gt;
Soft Self-Consistency (Soft-SC)&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26367;&#20195;&#33258;&#19968;&#33268;&#24615;&#30340;&#22810;&#25968;&#25237;&#31080;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22312;&#28041;&#21450;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#30340;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21487;&#20197;&#36890;&#36807;&#23545;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#25277;&#26679;&#21644;&#35780;&#20998;&#26469;&#25913;&#36827;&#65292;&#20197;&#36873;&#25321;&#26368;&#32456;&#31572;&#26696;&#12290;&#24403;&#21069;&#30340;&#8220;&#25277;&#26679;&#21644;&#36873;&#25321;&#8221;&#26041;&#27861;&#22914;&#33258;&#19968;&#33268;&#24615;&#65288;SC&#65289;&#20381;&#36182;&#20110;&#22810;&#25968;&#25237;&#31080;&#26469;&#35780;&#20998;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#26377;&#35768;&#22810;&#19981;&#21516;&#19988;&#26377;&#25928;&#30340;&#31572;&#26696;&#26102;&#65292;&#36890;&#36807;&#25237;&#31080;&#36827;&#34892;&#36873;&#25321;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#12290;&#36825;&#20351;&#24471;SC&#22312;&#28041;&#21450;&#39034;&#24207;&#29983;&#25104;&#22810;&#20010;&#21160;&#20316;&#65288;&#31572;&#26696;&#65289;&#30340;&#20114;&#21160;&#20219;&#21153;&#26102;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#30830;&#23450;&#22823;&#22810;&#25968;&#25237;&#31080;&#26410;&#33021;&#20026;&#27492;&#31867;&#20219;&#21153;&#25552;&#20379;&#19968;&#33268;&#30340;&#25910;&#30410;&#20043;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36719;&#21270;&#35780;&#20998;&#26631;&#20934;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36719;&#33258;&#19968;&#33268;&#24615;&#65288;Soft-SC&#65289;&#65292;&#23427;&#29992;&#27169;&#22411;&#21487;&#33021;&#24615;&#35745;&#31639;&#36830;&#32493;&#20998;&#25968;&#26469;&#21462;&#20195;SC&#30340;&#19981;&#36830;&#32493;&#35780;&#20998;&#65292;&#21363;&#20351;&#21160;&#20316;&#20998;&#24067;&#31232;&#30095;&#65292;&#20063;&#20801;&#35768;&#36873;&#25321;&#12290;&#36719;&#33258;&#19968;&#33268;&#24615;&#22312;&#38271;&#26399;&#20114;&#21160;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#26679;&#26412;&#21644;&#25237;&#31080;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13212v1 Announce Type: cross  Abstract: Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requi
&lt;/p&gt;</description></item><item><title>TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.12991</link><description>&lt;p&gt;
TRAP: &#38754;&#21521;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#30340;&#26377;&#38024;&#23545;&#24615;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#35825;&#39285;
&lt;/p&gt;
&lt;p&gt;
TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12991
&lt;/p&gt;
&lt;p&gt;
TRAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Targeted Random Adversarial Prompt (TRAP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;LLM&#30340;&#20351;&#29992;&#65292;&#24182;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26381;&#21153;&#21644;&#27169;&#22411;&#36890;&#24120;&#20276;&#38543;&#30528;&#20851;&#20110;&#35841;&#21487;&#20197;&#20351;&#29992;&#23427;&#20204;&#20197;&#21450;&#20182;&#20204;&#24517;&#39035;&#22914;&#20309;&#20351;&#29992;&#23427;&#20204;&#30340;&#27861;&#24459;&#35268;&#23450;&#12290;&#35780;&#20272;&#21457;&#24067;&#30340;LLMs&#30340;&#21512;&#35268;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#35268;&#23450;&#20445;&#25252;&#20102;LLM&#36129;&#29486;&#32773;&#30340;&#21033;&#30410;&#24182;&#38450;&#27490;&#20102;&#28389;&#29992;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#40657;&#30418;&#36523;&#20221;&#39564;&#35777;&#65288;BBIV&#65289;&#30340;&#26032;&#38382;&#39064;&#12290;&#20854;&#30446;&#26631;&#26159;&#30830;&#23450;&#31532;&#19977;&#26041;&#24212;&#29992;&#26159;&#21542;&#36890;&#36807;&#20854;&#32842;&#22825;&#21151;&#33021;&#20351;&#29992;&#26576;&#20010;&#29305;&#23450;&#30340;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30446;&#26631;&#38543;&#26426;&#23545;&#25239;&#25552;&#31034;&#65288;TRAP&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#27491;&#22312;&#20351;&#29992;&#30340;&#20855;&#20307;LLM&#12290;&#25105;&#20204;&#37325;&#26032;&#21033;&#29992;&#20102;&#26368;&#21021;&#29992;&#20110;&#36234;&#29425;&#30340;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#20197;&#20174;&#30446;&#26631;LLM&#33719;&#24471;&#39044;&#23450;&#20041;&#30340;&#31572;&#26696;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#21017;&#32473;&#20986;&#38543;&#26426;&#31572;&#26696;&#12290;TRAP&#21487;&#20197;&#22312;&#21333;&#27425;&#20132;&#20114;&#21518;&#20197;&#36229;&#36807;95%&#30340;&#30495;&#38451;&#24615;&#29575;&#21644;&#20302;&#20110;0.2%&#30340;&#35823;&#25253;&#29575;&#26816;&#27979;&#30446;&#26631;LLMs&#12290;&#21363;&#20351;LLM&#26377;&#19981;&#20250;&#26174;&#33879;&#25913;&#21464;&#30340;&#32454;&#24494;&#21464;&#21270;&#65292;TRAP&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12991v1 Announce Type: cross  Abstract: Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;Reflect-RL&#65292;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#36825;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21453;&#23556;&#27169;&#22411;&#21327;&#21161;&#65292;&#24182;&#37319;&#29992;&#20102;&#36127;&#20363;&#29983;&#25104;&#12289;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12621</link><description>&lt;p&gt;
Reflect-RL&#65306;&#20004;&#20010;&#29609;&#23478;&#22312;&#32447;RL&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reflect-RL: Two-Player Online RL Fine-Tuning for LMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12621
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;Reflect-RL&#65292;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#36825;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21453;&#23556;&#27169;&#22411;&#21327;&#21161;&#65292;&#24182;&#37319;&#29992;&#20102;&#36127;&#20363;&#29983;&#25104;&#12289;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20854;&#33021;&#21147;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#38656;&#35201;&#22810;&#36718;&#20132;&#20114;&#30340;&#20219;&#21153;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#24615;&#65292;&#22240;&#27492;&#20165;&#22312;&#26377;&#38480;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#23581;&#35797;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#21046;&#23450;&#29615;&#22659;&#20869;&#30452;&#25509;&#23545;LM&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;LM&#36827;&#34892;&#24494;&#35843;&#30340;&#26377;&#25928;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Reflect-RL&#65292;&#19968;&#20010;&#20004;&#20010;&#29609;&#23478;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22312;&#32447;RL&#23545;LM&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#19968;&#20010;&#20923;&#32467;&#30340;&#21453;&#23556;&#27169;&#22411;&#36741;&#21161;&#31574;&#30053;&#27169;&#22411;&#12290;&#20026;&#20102;&#20026;&#28909;&#36523;SFT&#38454;&#27573;&#29983;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#36127;&#20363;&#29983;&#25104;&#26469;&#22686;&#24378;&#21453;&#23556;&#27169;&#22411;&#30340;&#32416;&#38169;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21333;&#25552;&#31034;&#21160;&#20316;&#26522;&#20030;&#65292;&#24182;&#24212;&#29992;&#20102;&#35838;&#31243;&#23398;&#20064;&#35753;&#31574;&#30053;&#27169;&#22411;&#23398;&#20064;&#26356;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12621v1 Announce Type: cross  Abstract: As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12424</link><description>&lt;p&gt;
&#34920;&#26684;&#20316;&#20026;&#22270;&#29255;&#65311;&#25506;&#35752;LLM&#22312;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#19978;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#27604;&#36739;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#34920;&#26684;&#34920;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#21644;&#25968;&#25454;&#26684;&#24335;&#30740;&#31350;&#20102;&#21508;&#31181;LLM&#22312;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#20845;&#20010;&#38024;&#23545;&#19982;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#22914;&#38382;&#31572;&#21644;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;LLM&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#19978;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#31181;&#22522;&#20110;&#25991;&#26412;&#21644;&#19977;&#31181;&#22522;&#20110;&#22270;&#20687;&#30340;&#34920;&#26684;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#21644;&#25552;&#31034;&#23545;LLM&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#34920;&#26684;&#30456;&#20851;&#20219;&#21153;&#19978;&#26377;&#25928;&#20351;&#29992;LLM&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12424v1 Announce Type: cross  Abstract: In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;Koopman&#31639;&#23376;&#25552;&#21462;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#65292;&#35777;&#26126;&#20102;&#21463;&#38480;&#38750;&#32447;&#24615;&#24050;&#36275;&#22815;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#22788;&#29702;&#22823;&#22411;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.11740</link><description>&lt;p&gt;
&#21033;&#29992;Koopman&#31639;&#23376;&#25552;&#21462;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#24182;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Extraction of nonlinearity in neural networks and model compression with Koopman operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;Koopman&#31639;&#23376;&#25552;&#21462;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#65292;&#35777;&#26126;&#20102;&#21463;&#38480;&#38750;&#32447;&#24615;&#24050;&#36275;&#22815;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#22788;&#29702;&#22823;&#22411;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23545;&#20998;&#31867;&#20219;&#21153;&#30340;&#20851;&#38190;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;Koopman&#31639;&#23376;&#12289;&#25193;&#23637;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#21644;&#24352;&#37327;&#21015;&#36710;&#26684;&#24335;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21463;&#38480;&#38750;&#32447;&#24615;&#24050;&#32463;&#36275;&#20197;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#30340;&#20998;&#31867;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#22788;&#29702;&#22823;&#22411;&#32593;&#32476;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#21033;&#29992;Koopman&#31639;&#23376;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#22788;&#29702;&#20013;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39640;&#24230;&#21387;&#32553;&#27169;&#22411;&#35774;&#32622;&#19979;&#22312;&#25163;&#20889;&#25968;&#23383;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#35201;&#20040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#65292;&#35201;&#20040;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11740v1 Announce Type: new  Abstract: Nonlinearity plays a crucial role in deep neural networks. In this paper, we first investigate the degree to which the nonlinearity of the neural network is essential. For this purpose, we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format. The results imply that restricted nonlinearity is enough for the classification of handwritten numbers. Then, we propose a model compression method for deep neural networks, which could be beneficial to handling large networks in resource-constrained environments. Leveraging the Koopman operator, the proposed method enables us to use linear algebra in the internal processing of neural networks. We numerically show that the proposed method performs comparably or better than conventional methods in highly compressed model settings for the handwritten number recognition task.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#27169;&#25311;&#38750;&#32447;&#24615;&#30005;&#38459;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#24555;&#36895;&#12289;&#31934;&#30830;&#30340;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.11674</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#27169;&#25311;&#38750;&#32447;&#24615;&#30005;&#38459;&#32593;&#32476;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fast Algorithm to Simulate Nonlinear Resistive Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11674
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#27169;&#25311;&#38750;&#32447;&#24615;&#30005;&#38459;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20855;&#26377;&#32447;&#24615;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#24555;&#36895;&#12289;&#31934;&#30830;&#30340;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#33021;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#30005;&#38459;&#32593;&#32476;&#20316;&#20026;&#26367;&#20195;&#20256;&#32479;&#22522;&#20110;GPU&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26041;&#24335;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#32593;&#32476;&#21033;&#29992;&#30005;&#36335;&#30340;&#29289;&#29702;&#29305;&#24615;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#21487;&#36890;&#36807;&#23616;&#37096;&#35757;&#32451;&#25216;&#26415;&#65288;&#22914;&#24179;&#34913;&#20256;&#25773;&#65289;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#22312;&#21151;&#32791;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#20294;&#39640;&#25928;&#27169;&#25311;&#36825;&#20123;&#30005;&#38459;&#32593;&#32476;&#30340;&#25361;&#25112;&#19968;&#30452;&#26159;&#35780;&#20272;&#20854;&#21487;&#25193;&#23637;&#24615;&#30340;&#37325;&#35201;&#29942;&#39048;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#23616;&#38480;&#20110;&#32447;&#24615;&#32593;&#32476;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;SPICE&#31561;&#29616;&#23454;&#20294;&#36895;&#24230;&#36739;&#24930;&#30340;&#30005;&#36335;&#27169;&#25311;&#22120;&#12290;&#22312;&#20551;&#23450;&#29702;&#24819;&#30005;&#36335;&#20803;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#32447;&#24615;&#30005;&#38459;&#32593;&#32476;&#27169;&#25311;&#26041;&#27861;&#65292;&#23558;&#20854;&#26500;&#24314;&#20026;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24555;&#36895;&#12289;&#31934;&#30830;&#30340;&#22352;&#26631;&#19979;&#38477;&#31639;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#26041;&#27861;&#26174;&#33879;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11674v1 Announce Type: cross  Abstract: In the quest for energy-efficient artificial intelligence systems, resistor networks are attracting interest as an alternative to conventional GPU-based neural networks. These networks leverage the physics of electrical circuits for inference and can be optimized with local training techniques such as equilibrium propagation. Despite their potential advantage in terms of power consumption, the challenge of efficiently simulating these resistor networks has been a significant bottleneck to assess their scalability, with current methods either being limited to linear networks or relying on realistic, yet slow circuit simulators like SPICE. Assuming ideal circuit elements, we introduce a novel approach for the simulation of nonlinear resistive networks, which we frame as a quadratic programming problem with linear inequality constraints, and which we solve using a fast, exact coordinate descent algorithm. Our simulation methodology signif
&lt;/p&gt;</description></item><item><title>LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11485</link><description>&lt;p&gt;
LEIA: &#21033;&#29992;&#22522;&#20110;&#23454;&#20307;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#20419;&#36827;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11485
&lt;/p&gt;
&lt;p&gt;
LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;&#33521;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20854;&#20182;&#35821;&#35328;&#30340;&#25805;&#20316;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#25928;&#29575;&#21644;&#28508;&#21147;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#36328;&#35821;&#35328;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;LEIA&#65292;&#19968;&#31181;&#21033;&#29992;&#36328;&#35821;&#35328;&#23545;&#40784;&#30340;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#30340;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#33521;&#35821;&#23454;&#20307;&#21517;&#31216;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20174;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;LEIA&#65292;&#20351;&#29992;7B&#21442;&#25968;&#30340;LLMs&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#22686;&#30410;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/studio-ousia/leia&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11485v1 Announce Type: cross  Abstract: Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.
&lt;/p&gt;</description></item><item><title>Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11463</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21560;&#24341;&#23376;&#35760;&#24518;&#65306;&#28151;&#27788;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11463
&lt;/p&gt;
&lt;p&gt;
Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24573;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#28304;&#33258;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#23548;&#33268;&#32570;&#20047;&#22806;&#25512;&#21644;&#28436;&#21270;&#33021;&#21147;&#12290; &#37492;&#21035;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\textbf{\textit{Attraos}}&#23558;&#28151;&#27788;&#29702;&#35770;&#34701;&#20837;&#21040;LTSF&#20013;&#65292;&#23558;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26410;&#30693;&#39640;&#32500;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#12290; &#22312;&#21560;&#24341;&#23376;&#19981;&#21464;&#24615;&#30340;&#27010;&#24565;&#19979;&#65292;Attraos&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#26469;&#35760;&#24518;&#21382;&#21490;&#21160;&#24577;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#22686;&#24378;&#30340;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290; &#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#20016;&#23500;&#30340;&#32463;&#39564;&#35777;&#25454;&#19968;&#33268;&#34920;&#26126;&#65292;Attraos&#22312;&#20027;&#27969;LTSF&#25968;&#25454;&#38598;&#21644;&#28151;&#27788;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21508;&#31181;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.11138</link><description>&lt;p&gt;
&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Contrastive Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#19968;&#30452;&#34987;&#29992;&#20316;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#38754;&#20020;&#26410;&#30693;&#25351;&#20196;&#26102;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#30456;&#21516;&#30340;&#25351;&#20196;&#20197;&#31245;&#24494;&#21464;&#21270;&#30340;&#24418;&#24335;&#25110;&#35821;&#35328;&#39118;&#26684;&#25552;&#20986;&#26102;&#20250;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#26126;LLMs&#23545;&#25991;&#26412;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#21644;&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#21487;&#20449;&#24230;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#22823;&#21270;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#25351;&#20196;-&#23454;&#20363;&#23545;&#30340;&#38544;&#34255;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#23567;&#21270;&#35821;&#20041;&#19978;&#19981;&#21516;&#30340;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#37322;&#20041;&#20219;&#21153;&#25351;&#20196;&#65292;&#25193;&#20805;&#29616;&#26377;&#30340;FLAN&#38598;&#21512;&#12290;&#22312;PromptBench&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65288;CoIN&#65289;&#19968;&#30452;&#25552;&#39640;&#20102;LLMs&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11138v1 Announce Type: cross  Abstract: Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructio
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10890</link><description>&lt;p&gt;
LLM&#35268;&#21010;&#20013;&#26641;&#25628;&#32034;&#20309;&#26102;&#26377;&#29992;&#65311;&#21462;&#20915;&#20110;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
When is Tree Search Useful for LLM Planning? It Depends on the Discriminator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10890
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22312;&#22810;&#27493;&#38382;&#39064;&#19979;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#35268;&#21010;&#26041;&#27861;&#19977;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20808;&#36827;&#35268;&#21010;&#26041;&#27861;&#65292;&#36845;&#20195;&#26657;&#27491;&#21644;&#26641;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#37492;&#21035;&#20934;&#30830;&#24615;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#22312;&#20351;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#25110;&#26356;&#31616;&#21333;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26102;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#20004;&#39033;&#20219;&#21153;&#65292;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21644;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#30340;&#37492;&#21035;&#22120;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65307;&#65288;2&#65289;&#24403;&#21069;LLMs&#30340;&#37492;&#21035;&#33021;&#21147;&#23578;&#26410;&#28385;&#36275;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#23454;&#29616;&#36825;&#31181;&#25913;&#36827;&#30340;&#38656;&#27714;&#65307;&#65288;3&#65289;&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#37492;&#21035;&#22120;&#26102;&#65292;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10890v1 Announce Type: cross  Abstract: In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compare
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#20998;&#35299;&#21644;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#21516;&#26469;&#28304;&#65292;&#24182;&#28548;&#28165;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10727</link><description>&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#23454;&#29616;&#20005;&#26684;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10727
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#20998;&#35299;&#21644;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#21516;&#26469;&#28304;&#65292;&#24182;&#28548;&#28165;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#39044;&#27979;&#27169;&#22411;&#24212;&#29992;&#20013;&#65292;&#21306;&#20998;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#20294;&#24182;&#27809;&#26377;&#20005;&#26684;&#30340;&#23450;&#20041;&#26469;&#35299;&#24320;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25514;&#26045;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#26377;&#20123;&#19981;&#28165;&#26224;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26681;&#26893;&#20110;&#32479;&#35745;&#25512;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#19981;&#20165;&#20801;&#35768;&#21019;&#24314;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#36824;&#28548;&#28165;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32479;&#35745;&#39118;&#38505;&#26469;&#21306;&#20998;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#25104;&#20998;&#65292;&#24182;&#21033;&#29992;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#23545;&#20854;&#36827;&#34892;&#37327;&#21270;&#12290;&#20026;&#20102;&#20351;&#20854;&#22312;&#23454;&#36341;&#20013;&#26131;&#20110;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36825;&#19968;&#26694;&#26550;&#20013;&#25972;&#21512;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#24819;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#25152;&#25552;&#36817;&#20284;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10727v1 Announce Type: cross  Abstract: Distinguishing sources of predictive uncertainty is of crucial importance in the application of forecasting models across various domains. Despite the presence of a great variety of proposed uncertainty measures, there are no strict definitions to disentangle them. Furthermore, the relationship between different measures of uncertainty quantification remains somewhat unclear. In this work, we introduce a general framework, rooted in statistical reasoning, which not only allows the creation of new uncertainty measures but also clarifies their interrelations. Our approach leverages statistical risk to distinguish aleatoric and epistemic uncertainty components and utilizes proper scoring rules to quantify them. To make it practically tractable, we propose an idea to incorporate Bayesian reasoning into this framework and discuss the properties of the proposed approximation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;</title><link>https://arxiv.org/abs/2402.10571</link><description>&lt;p&gt;
&#20855;&#26377;&#20559;&#32622;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference Optimization with an Offset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DPO&#30340;&#27867;&#21270;&#24418;&#24335;&#65292;&#31216;&#20026;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#23558;&#27599;&#20010;&#20559;&#22909;&#23545;&#35270;&#20026;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10571v1 Announce Type: cross  Abstract: Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset valu
&lt;/p&gt;</description></item><item><title>&#23558;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;&#65292;&#20351;&#29992;Primitive Sequence Encoding (PRISE)&#26041;&#27861;&#32467;&#21512;&#36830;&#32493;&#21160;&#20316;&#37327;&#21270;&#19982;BPE&#26469;&#23398;&#20064;&#24378;&#22823;&#30340;&#21160;&#20316;&#25277;&#35937;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2402.10450</link><description>&lt;p&gt;
PRISE&#65306;&#23558;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10450
&lt;/p&gt;
&lt;p&gt;
&#23558;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;&#65292;&#20351;&#29992;Primitive Sequence Encoding (PRISE)&#26041;&#27861;&#32467;&#21512;&#36830;&#32493;&#21160;&#20316;&#37327;&#21270;&#19982;BPE&#26469;&#23398;&#20064;&#24378;&#22823;&#30340;&#21160;&#20316;&#25277;&#35937;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#20197;&#21450;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26159;&#24207;&#36143;&#20915;&#31574;&#20013;&#30340;&#24378;&#22823;&#30693;&#35782;&#20849;&#20139;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#23558;&#35825;&#23548;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;LLM&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#19968;&#20010;&#24494;&#22937;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998; -- &#36755;&#20837;&#26631;&#35760;&#21270;&#36890;&#36807;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289; -- &#24102;&#21040;&#20102;&#36830;&#32493;&#25511;&#21046;&#39046;&#22495;&#20013;&#23398;&#20064;&#21487;&#21464;&#26102;&#38388;&#36328;&#24230;&#25216;&#33021;&#30340; seemingly distant &#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;Primitive Sequence Encoding&#65288;PRISE&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#36830;&#32493;&#21160;&#20316;&#37327;&#21270;&#19982;BPE&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#24378;&#22823;&#30340;&#21160;&#20316;&#25277;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;PRISE&#20174;&#19968;&#32452;&#26426;&#22120;&#20154;&#25805;&#20316;&#28436;&#31034;&#20013;&#21457;&#29616;&#30340;&#39640;&#32423;&#25216;&#33021;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#20197;&#21450;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312; https: &#25918;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10450v1 Announce Type: new  Abstract: Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as few-shot imitation learning on unseen tasks. Our code will be released at https:/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28378;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#21435;&#22122;&#24182;&#26681;&#25454;&#24103;&#22312;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20808;&#21518;&#20998;&#37197;&#19981;&#21516;&#30340;&#22122;&#22768;&#37327;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35270;&#39057;&#39044;&#27979;&#21644;&#28151;&#27788;&#27969;&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25193;&#25955;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09470</link><description>&lt;p&gt;
&#28378;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rolling Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28378;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#21435;&#22122;&#24182;&#26681;&#25454;&#24103;&#22312;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20808;&#21518;&#20998;&#37197;&#19981;&#21516;&#30340;&#22122;&#22768;&#37327;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35270;&#39057;&#39044;&#27979;&#21644;&#28151;&#27788;&#27969;&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25193;&#25955;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#26102;&#38388;&#25968;&#25454;&#65292;&#22914;&#35270;&#39057;&#12289;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#25110;&#27668;&#20505;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#21518;&#32493;&#24103;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#37327;&#35270;&#20026;&#30456;&#31561;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#28378;&#21160;&#25193;&#25955;&#65306;&#19968;&#31181;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#21435;&#22122;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#30830;&#20445;&#25193;&#25955;&#36807;&#31243;&#36880;&#28176;&#36890;&#36807;&#26102;&#38388;&#36827;&#34892;&#30772;&#22351;&#65292;&#36890;&#36807;&#23558;&#26356;&#22810;&#30340;&#22122;&#22768;&#20998;&#37197;&#32473;&#24207;&#21015;&#20013;&#20986;&#29616;&#36739;&#26202;&#30340;&#24103;&#65292;&#21453;&#26144;&#20986;&#38543;&#30528;&#29983;&#25104;&#36807;&#31243;&#30340;&#23637;&#24320;&#65292;&#23545;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#36234;&#26469;&#36234;&#22823;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#26102;&#38388;&#21160;&#24577;&#22797;&#26434;&#26102;&#65292;&#28378;&#21160;&#25193;&#25955;&#20248;&#20110;&#26631;&#20934;&#25193;&#25955;&#12290;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;Kinetics-600&#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#35270;&#39057;&#39044;&#27979;&#20219;&#21153;&#21644;&#28151;&#27788;&#27969;&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09470v1 Announce Type: new  Abstract: Diffusion models have recently been increasingly applied to temporal data such as video, fluid mechanics simulations, or climate data. These methods generally treat subsequent frames equally regarding the amount of noise in the diffusion process. This paper explores Rolling Diffusion: a new approach that uses a sliding window denoising process. It ensures that the diffusion process progressively corrupts through time by assigning more noise to frames that appear later in a sequence, reflecting greater uncertainty about the future as the generation process unfolds. Empirically, we show that when the temporal dynamics are complex, Rolling Diffusion is superior to standard diffusion. In particular, this result is demonstrated in a video prediction task using the Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting experiment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#23545;&#20110;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#30446;&#26631;&#27169;&#24335;&#30340;&#21516;&#24577;&#35745;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.08595</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#65306;&#20851;&#20110;&#22522;&#30784;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Homomorphism Counts for Graph Neural Networks: All About That Basis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#24577;&#35745;&#25968;&#23545;&#20110;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#26469;&#34701;&#21512;&#30446;&#26631;&#27169;&#24335;&#30340;&#21516;&#24577;&#35745;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#19988;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#29992;&#20110;&#23398;&#20064;&#22270;&#19978;&#19981;&#21464;&#20989;&#25968;&#30340;&#26550;&#26500;&#12290;&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#36136;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#19982;&#20854;&#34920;&#36798;&#33021;&#21147;&#30456;&#20851;&#30340;&#38480;&#21046;&#12290;&#23427;&#20204;&#26080;&#27861;&#35745;&#25968;&#22270;&#20013;&#30340;&#26576;&#20123;&#27169;&#24335;&#65288;&#20363;&#22914;&#24490;&#29615;&#65289;&#26159;&#36825;&#20123;&#38480;&#21046;&#30340;&#26680;&#24515;&#65292;&#22240;&#20026;&#35768;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#20381;&#36182;&#20110;&#35745;&#25968;&#36825;&#20123;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20004;&#31181;&#31361;&#20986;&#30340;&#33539;&#20363;&#26088;&#22312;&#36890;&#36807;&#20016;&#23500;&#22270;&#29305;&#24449;&#30340;&#23376;&#22270;&#25110;&#21516;&#24577;&#27169;&#24335;&#35745;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#37117;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#20027;&#24352;&#37319;&#29992;&#19968;&#31181;&#26356;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#23558;&#30446;&#26631;&#27169;&#24335;&#30340;&#8220;&#22522;&#30784;&#8221;&#20013;&#30340;&#21516;&#24577;&#35745;&#25968;&#32435;&#20837;&#32771;&#34385;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#20135;&#29983;&#20102;&#26356;&#21152;&#34920;&#36798;&#21147;&#30340;&#26550;&#26500;&#65292;&#32780;&#19981;&#20250;&#24102;&#26469;&#20219;&#20309;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31995;&#21015;&#29702;&#35770;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are architectures for learning invariant functions over graphs. A large body of work has investigated the properties of graph neural networks and identified several limitations, particularly pertaining to their expressive power. Their inability to count certain patterns (e.g., cycles) in a graph lies at the heart of such limitations, since many functions to be learned rely on the ability of counting such patterns. Two prominent paradigms aim to address this limitation by enriching the graph features with subgraph or homomorphism pattern counts. In this work, we show that both of these approaches are sub-optimal in a certain sense and argue for a more fine-grained approach, which incorporates the homomorphism counts of all structures in the "basis" of the target pattern. This yields strictly more expressive architectures without incurring any additional overhead in terms of computational complexity compared to existing approaches. We prove a series of theoretical r
&lt;/p&gt;</description></item><item><title>DiffUse&#26159;&#19968;&#31181;&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32858;&#31867;&#25991;&#26412;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#24182;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07891</link><description>&lt;p&gt;
&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Label-Efficient Model Selection for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07891
&lt;/p&gt;
&lt;p&gt;
DiffUse&#26159;&#19968;&#31181;&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32858;&#31867;&#25991;&#26412;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#24182;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32473;&#23450;&#30446;&#26631;&#20219;&#21153;&#30340;&#27169;&#22411;&#36873;&#25321;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#38656;&#35201;&#23545;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#30340;&#36136;&#37327;&#36827;&#34892;&#24191;&#27867;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DiffUse&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#20505;&#36873;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;DiffUse&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#22312;&#35780;&#20272;&#20013;&#23453;&#36149;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;DiffUse&#36890;&#36807;&#32858;&#31867;&#34920;&#31034;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#26234;&#33021;&#36873;&#25321;&#23454;&#20363;&#12290;&#22240;&#27492;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#20986;&#19968;&#20123;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#20363;&#23376;&#26469;&#36827;&#34892;&#20559;&#22909;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#21160;&#24577;&#30830;&#23450;&#35201;&#27880;&#37322;&#30340;&#23454;&#20363;&#25968;&#37327;&#12290;&#36890;&#36807;&#23545;&#25968;&#30334;&#20010;&#27169;&#22411;&#23545;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DiffUse&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#65292;&#26368;&#22810;&#21487;&#20943;&#23569;75%&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#35780;&#20272;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models. DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06733</link><description>&lt;p&gt;
NICE: &#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#36824;&#26159;&#19981;&#20248;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
NICE: To Optimize In-Context Examples or Not?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;ICE&#65289;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#22312;&#25552;&#31034;&#20449;&#24687;&#20013;&#35201;&#20040;&#26159;&#22266;&#23450;&#30340;&#65292;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#25351;&#20196;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#34920;&#38754;&#19978;&#30340;&#20849;&#35782;&#65306;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#32463;&#36807;&#25351;&#23548;&#30340;LLMs&#25361;&#25112;&#36825;&#19968;&#20849;&#35782;&#65292;&#30740;&#31350;&#22312;&#25552;&#20379;&#20102;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26159;&#21542;&#24517;&#35201;&#65292;&#24182;&#21457;&#29616;&#26377;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;"&#24230;&#37327;&#26631;&#20934;"&#65288;Metric&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#25351;&#20196;&#38598;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06700</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Entropy-Regularized Token-Level Policy Optimization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#12289;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25110;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#25110;RLHF&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#20811;&#26381;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#38754;&#20020;&#30528;&#37325;&#37325;&#22256;&#38590;&#65306;1&#65289;&#30001;&#20110;&#24040;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#38656;&#35201;&#25506;&#32034;&#32780;&#20135;&#29983;&#30340;&#19981;&#31283;&#23450;&#24615;&#65307;2&#65289;&#22522;&#20110;&#21160;&#20316;&#32423;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#20934;&#30830;&#24314;&#27169;&#35821;&#26009;&#24211;&#25968;&#25454;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#65288;ETPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22312;&#20196;&#29260;&#32423;&#20248;&#21270;LLMs&#32780;&#35774;&#35745;&#30340;&#29109;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;ETPO&#30340;&#26680;&#24515;&#26159;&#25105;&#20204;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#20196;&#29260;&#36719;Bellman&#26356;&#26032;&#31639;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#31639;&#23376;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#26144;&#23556;&#65292;&#25552;&#20986;&#20102;&#36866;&#24212;&#26377;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#26144;&#23556;&#26694;&#26550;&#65292;&#24182;&#21457;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#23398;&#20064;PtO&#26144;&#23556;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#21644;&#20808;&#23398;&#20064;&#35299;&#31639;&#23376;&#20877;&#35745;&#31639;&#21487;&#35266;&#27979;&#20540;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06031</link><description>&lt;p&gt;
&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#26144;&#23556;&#30340;&#31639;&#23376;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An operator learning perspective on parameter-to-observable maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#31639;&#23376;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#26144;&#23556;&#65292;&#25552;&#20986;&#20102;&#36866;&#24212;&#26377;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#26144;&#23556;&#26694;&#26550;&#65292;&#24182;&#21457;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#23398;&#20064;PtO&#26144;&#23556;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#21644;&#20808;&#23398;&#20064;&#35299;&#31639;&#23376;&#20877;&#35745;&#31639;&#21487;&#35266;&#27979;&#20540;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#39640;&#25928;&#30340;&#21442;&#25968;&#21270;&#29289;&#29702;&#27169;&#22411;&#26367;&#20195;&#21697;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#31639;&#23376;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#21482;&#26377;&#26377;&#38480;&#32500;&#30340;&#27169;&#22411;&#36755;&#20837;&#21442;&#25968;&#21270;&#25110;&#26377;&#38480;&#32500;&#30340;&#27169;&#22411;&#36755;&#20986;&#21487;&#35266;&#27979;&#25968;&#25454;&#21487;&#29992;&#65292;&#32780;&#19981;&#26159;&#20840;&#22330;&#27979;&#37327;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65292;&#24341;&#20837;&#20102;&#20613;&#37324;&#21494;&#31070;&#32463;&#26144;&#23556;&#65288;Fourier Neural Mappings&#65292;FNMs&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#36866;&#24212;&#36825;&#26679;&#30340;&#26377;&#38480;&#32500;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#26412;&#25991;&#20026;&#35813;&#26041;&#27861;&#21457;&#23637;&#20102;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#12290;&#27492;&#22806;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#24213;&#23618;&#30340;&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#65288;PtO&#65289;&#26144;&#23556;&#26159;&#36890;&#36807;&#26080;&#31351;&#32500;&#31639;&#23376;&#26469;&#38544;&#24335;&#23450;&#20041;&#30340;&#65292;&#20363;&#22914;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#26159;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;PtO&#26144;&#23556;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#36824;&#26159;&#39318;&#20808;&#23398;&#20064;&#35299;&#31639;&#23376;&#65292;&#28982;&#21518;&#35745;&#31639;&#21487;&#35266;&#27979;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computationally efficient surrogates for parametrized physical models play a crucial role in science and engineering. Operator learning provides data-driven surrogates that map between function spaces. However, instead of full-field measurements, often the available data are only finite-dimensional parametrizations of model inputs or finite observables of model outputs. Building off of Fourier Neural Operators, this paper introduces the Fourier Neural Mappings (FNMs) framework that is able to accommodate such finite-dimensional inputs and outputs. The paper develops universal approximation theorems for the method. Moreover, in many applications the underlying parameter-to-observable (PtO) map is defined implicitly through an infinite-dimensional operator, such as the solution operator of a partial differential equation. A natural question is whether it is more data-efficient to learn the PtO map end-to-end or first learn the solution operator and subsequently compute the observable fro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31163;&#25955;&#27969;&#27169;&#22411;&#65288;DFMs&#65289;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#23454;&#29616;&#31163;&#25955;&#31354;&#38388;&#27969;&#21305;&#37197;&#30340;&#31163;&#25955;&#31561;&#25928;&#65292;&#20026;&#23558;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#26041;&#27861;&#22312;&#34507;&#30333;&#36136;&#20849;&#35774;&#35745;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04997</link><description>&lt;p&gt;
&#22312;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#29983;&#25104;&#22411;&#27969;&#65306;&#23454;&#29616;&#22810;&#27169;&#24577;&#27969;&#24182;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#20849;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31163;&#25955;&#27969;&#27169;&#22411;&#65288;DFMs&#65289;&#65292;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#23454;&#29616;&#31163;&#25955;&#31354;&#38388;&#27969;&#21305;&#37197;&#30340;&#31163;&#25955;&#31561;&#25928;&#65292;&#20026;&#23558;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#26041;&#27861;&#22312;&#34507;&#30333;&#36136;&#20849;&#35774;&#35745;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#30456;&#32467;&#21512;&#23545;&#20110;&#29983;&#25104;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#25955;&#27969;&#27169;&#22411;&#65288;DFMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27969;&#30340;&#31163;&#25955;&#25968;&#25454;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#23558;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#27169;&#24577;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#31163;&#25955;&#31354;&#38388;&#27969;&#21305;&#37197;&#30340;&#31163;&#25955;&#31561;&#25928;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;&#26469;&#23454;&#29616;&#12290;DFMs&#36890;&#36807;&#31616;&#21333;&#30340;&#25512;&#23548;&#21253;&#25324;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29305;&#23450;&#23454;&#20363;&#65292;&#21516;&#26102;&#20801;&#35768;&#22312;&#29616;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#19978;&#25913;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#21033;&#29992;DFMs&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#20110;&#27969;&#30340;&#24314;&#27169;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#27492;&#33021;&#21147;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#20849;&#35774;&#35745;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#34507;&#30333;&#36136;&#32467;&#26500;&#21644;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20849;&#35774;&#35745;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#21516;&#26102;&#20801;&#35768;&#20351;&#29992;&#21516;&#19968;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#24207;&#21015;&#25110;&#32467;&#26500;&#30340;&#28789;&#27963;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining discrete and continuous data is an important capability for generative models. We present Discrete Flow Models (DFMs), a new flow-based model of discrete data that provides the missing link in enabling flow-based generative models to be applied to multimodal continuous and discrete data problems. Our key insight is that the discrete equivalent of continuous space flow matching can be realized using Continuous Time Markov Chains. DFMs benefit from a simple derivation that includes discrete diffusion models as a specific instance while allowing improved performance over existing diffusion-based approaches. We utilize our DFMs method to build a multimodal flow-based modeling framework. We apply this capability to the task of protein co-design, wherein we learn a model for jointly generating protein structure and sequence. Our approach achieves state-of-the-art co-design performance while allowing the same multimodal model to be used for flexible generation of the sequence or str
&lt;/p&gt;</description></item><item><title>A-X&#20381;&#36182;&#20851;&#31995;&#26159;&#24433;&#21709;&#22270;&#21367;&#31215;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#29305;&#24449;&#37325;&#25490;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04621</link><description>&lt;p&gt;
&#22270;&#25299;&#25169;&#32467;&#26500;&#19978;&#30340;&#29305;&#24449;&#20998;&#24067;&#35843;&#33410;&#20102;&#22270;&#21367;&#31215;&#30340;&#25928;&#26524;&#65306;&#21516;&#36136;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04621
&lt;/p&gt;
&lt;p&gt;
A-X&#20381;&#36182;&#20851;&#31995;&#26159;&#24433;&#21709;&#22270;&#21367;&#31215;&#25928;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#29305;&#24449;&#37325;&#25490;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#37325;&#25490;&#21516;&#19968;&#31867;&#21035;&#33410;&#28857;&#20043;&#38388;&#30340;&#29305;&#24449;&#21521;&#37327;&#22914;&#20309;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65311;&#30452;&#35266;&#22320;&#35828;&#65292;&#29305;&#24449;&#37325;&#25490;&#25200;&#20081;&#20102;GNNs&#20174;&#22270;&#25299;&#25169;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65288;A-X&#20381;&#36182;&#20851;&#31995;&#65289;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;GNNs&#30340;&#23398;&#20064;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#29305;&#24449;&#37325;&#25490;&#20043;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#12290;&#30001;&#20110;&#24573;&#35270;&#20102;A-X&#20381;&#36182;&#20851;&#31995;&#23545;GNNs&#30340;&#24433;&#21709;&#65292;&#20808;&#21069;&#30340;&#25991;&#29486;&#27809;&#26377;&#32473;&#20986;&#23545;&#35813;&#29616;&#35937;&#30340;&#28385;&#24847;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22914;&#20309;&#22312;&#25511;&#21046;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#24230;&#37327;A-X&#20381;&#36182;&#20851;&#31995;&#65311;&#20854;&#27425;&#65292;A-X&#20381;&#36182;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;GNNs&#65311;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#65288;i&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#24230;&#37327;A-X&#20381;&#36182;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#65288;ii&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#25511;&#21046;A-X&#20381;&#36182;&#20851;&#31995;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#65292;&#65288;iii&#65289;&#24314;&#31435;&#20102;A-X&#20381;&#36182;&#20851;&#31995;&#19982;&#22270;&#21367;&#31215;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35770;&#65292;&#20197;&#21450;&#65288;iv&#65289;&#23545;&#23454;&#38469;&#22270;&#36827;&#34892;&#20102;&#19982;&#29702;&#35770;&#19968;&#33268;&#30340;&#23454;&#35777;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;A-X&#20381;&#36182;&#20851;&#31995;&#23545;GNNs&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
How would randomly shuffling feature vectors among nodes from the same class affect graph neural networks (GNNs)? The feature shuffle, intuitively, perturbs the dependence between graph topology and features (A-X dependence) for GNNs to learn from. Surprisingly, we observe a consistent and significant improvement in GNN performance following the feature shuffle. Having overlooked the impact of A-X dependence on GNNs, the prior literature does not provide a satisfactory understanding of the phenomenon. Thus, we raise two research questions. First, how should A-X dependence be measured, while controlling for potential confounds? Second, how does A-X dependence affect GNNs? In response, we (i) propose a principled measure for A-X dependence, (ii) design a random graph model that controls A-X dependence, (iii) establish a theory on how A-X dependence relates to graph convolution, and (iv) present empirical analysis on real-world graphs that aligns with the theory. We conclude that A-X depe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#27788;&#31995;&#32479;&#30340;&#19981;&#21464;&#27979;&#24230;&#21644;&#21160;&#24577;&#26469;&#35299;&#20915;&#23398;&#20064;&#21160;&#24577;&#30340;&#22256;&#38590;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26694;&#26550;&#22312;&#36712;&#36857;&#38271;&#24230;&#22686;&#21152;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04467</link><description>&lt;p&gt;
DySLIM: &#21033;&#29992;&#19981;&#21464;&#27979;&#24230;&#23454;&#29616;&#28151;&#27788;&#31995;&#32479;&#30340;&#21160;&#24577;&#31283;&#23450;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DySLIM: Dynamics Stable Learning by Invariant Measure for Chaotic Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#27788;&#31995;&#32479;&#30340;&#19981;&#21464;&#27979;&#24230;&#21644;&#21160;&#24577;&#26469;&#35299;&#20915;&#23398;&#20064;&#21160;&#24577;&#30340;&#22256;&#38590;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26694;&#26550;&#22312;&#36712;&#36857;&#38271;&#24230;&#22686;&#21152;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32791;&#25955;&#24615;&#28151;&#27788;&#31995;&#32479;&#20013;&#23398;&#20064;&#21160;&#24577;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#22266;&#26377;&#30340;&#19981;&#31283;&#23450;&#24615;&#23548;&#33268;&#23398;&#20064;&#21160;&#24577;&#30340;&#35823;&#24046;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#31995;&#32479;&#34920;&#29616;&#20986;&#36941;&#21382;&#24615;&#21644;&#21560;&#24341;&#23376;&#65306;&#19968;&#20010;&#32039;&#20945;&#32780;&#39640;&#24230;&#22797;&#26434;&#30340;&#27969;&#24418;&#65292;&#36712;&#36857;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#35813;&#27969;&#24418;&#65292;&#24182;&#25903;&#25345;&#19968;&#20010;&#19981;&#21464;&#27979;&#24230;&#65292;&#21363;&#19968;&#20010;&#22312;&#21160;&#24577;&#20316;&#29992;&#19979;&#19981;&#21464;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#20915;&#23450;&#20102;&#31995;&#32479;&#30340;&#38271;&#26399;&#32479;&#35745;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#32467;&#26500;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#19981;&#21464;&#27979;&#24230;&#20197;&#21450;&#21160;&#24577;&#65292;&#19982;&#36890;&#24120;&#21482;&#38024;&#23545;&#36712;&#36857;&#20043;&#38388;&#30340;&#35823;&#24046;&#30340;&#20856;&#22411;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#22312;&#36712;&#36857;&#38271;&#24230;&#22686;&#21152;&#26102;&#24448;&#24448;&#20250;&#21457;&#25955;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#22788;&#29702;&#19988;&#26679;&#26412;&#39640;&#25928;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#21487;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#23398;&#20064;&#30446;&#26631;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning dynamics from dissipative chaotic systems is notoriously difficult due to their inherent instability, as formalized by their positive Lyapunov exponents, which exponentially amplify errors in the learned dynamics. However, many of these systems exhibit ergodicity and an attractor: a compact and highly complex manifold, to which trajectories converge in finite-time, that supports an invariant measure, i.e., a probability distribution that is invariant under the action of the dynamics, which dictates the long-term statistical behavior of the system. In this work, we leverage this structure to propose a new framework that targets learning the invariant measure as well as the dynamics, in contrast with typical methods that only target the misfit between trajectories, which often leads to divergence as the trajectories' length increases. We use our framework to propose a tractable and sample efficient objective that can be used with any existing learning objectives. Our Dynamics St
&lt;/p&gt;</description></item><item><title>&#22797;&#21512;&#22238;&#25253;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#26041;&#24046;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#36129;&#29486;&#21644;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.03903</link><description>&lt;p&gt;
&#22797;&#21512;&#22238;&#25253;&#38477;&#20302;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Compound Returns Reduce Variance in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03903
&lt;/p&gt;
&lt;p&gt;
&#22797;&#21512;&#22238;&#25253;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#26041;&#24046;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#36129;&#29486;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#22238;&#25253;&#65292;&#20363;&#22914;$n$&#27493;&#22238;&#25253;&#21644;$\lambda$&#22238;&#25253;&#65292;&#36890;&#24120;&#29992;&#20110;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#22810;&#27493;&#22238;&#25253;&#30340;&#26041;&#24046;&#25104;&#20026;&#20854;&#38271;&#24230;&#30340;&#38480;&#21046;&#22240;&#32032;&#65292;&#36807;&#24230;&#36828;&#26395;&#26410;&#26469;&#20250;&#22686;&#21152;&#26041;&#24046;&#24182;&#36870;&#36716;&#22810;&#27493;&#23398;&#20064;&#30340;&#22909;&#22788;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#21512;&#22238;&#25253;&#65288;$n$&#27493;&#22238;&#25253;&#30340;&#21152;&#26435;&#24179;&#22343;&#65289;&#38477;&#20302;&#26041;&#24046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#20219;&#20309;&#19982;&#32473;&#23450;$n$&#27493;&#22238;&#25253;&#20855;&#26377;&#30456;&#21516;&#25910;&#32553;&#27169;&#25968;&#30340;&#22797;&#21512;&#22238;&#25253;&#30340;&#26041;&#24046;&#20005;&#26684;&#36739;&#20302;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#31181;&#38477;&#20302;&#26041;&#24046;&#30340;&#29305;&#24615;&#25913;&#21892;&#20102;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#19968;&#33324;&#22797;&#21512;&#22238;&#25253;&#30340;&#23454;&#26045;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#21161;&#22238;&#25253;&#65292;&#23427;&#20204;&#22312;&#20445;&#25345;&#39640;&#25928;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#26041;&#24046;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#23567;&#25209;&#37327;&#32463;&#39564;&#22238;&#25918;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#26174;&#31034;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Multistep returns, such as $n$-step returns and $\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#21152;&#26435;&#34928;&#20943;&#27491;&#21017;&#21270;&#19979;&#21450;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#35777;&#26126;&#20102;&#24403;&#35757;&#32451;&#25968;&#25454;&#26159;&#38543;&#26426;&#30340;&#26102;&#20505;&#65292;&#30456;&#23545;&#26368;&#20248;&#24615;&#24046;&#36317;&#21487;&#20197;&#34987;&#19968;&#20010;$O(\sqrt{\log n})$&#30340;&#22240;&#23376;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25910;&#25947;&#21040;&#35757;&#32451;&#25439;&#22833;&#36739;&#20302;&#30340;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.03625</link><description>&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20984;&#26494;&#24347;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36924;&#36817;&#20840;&#23616;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#21152;&#26435;&#34928;&#20943;&#27491;&#21017;&#21270;&#19979;&#21450;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#35777;&#26126;&#20102;&#24403;&#35757;&#32451;&#25968;&#25454;&#26159;&#38543;&#26426;&#30340;&#26102;&#20505;&#65292;&#30456;&#23545;&#26368;&#20248;&#24615;&#24046;&#36317;&#21487;&#20197;&#34987;&#19968;&#20010;$O(\sqrt{\log n})$&#30340;&#22240;&#23376;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25910;&#25947;&#21040;&#35757;&#32451;&#25439;&#22833;&#36739;&#20302;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#21152;&#26435;&#34928;&#20943;&#27491;&#21017;&#21270;&#19979;&#21450;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#35757;&#32451;&#25968;&#25454;&#26159;&#38543;&#26426;&#30340;&#26102;&#20505;&#65292;&#21407;&#22987;&#38382;&#39064;&#19982;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#30456;&#23545;&#26368;&#20248;&#24615;&#24046;&#36317;&#21487;&#20197;&#34987;&#19968;&#20010;$O(\sqrt{\log n})$&#30340;&#22240;&#23376;&#30028;&#38480;&#65292;&#20854;&#20013;$n$&#26159;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#24212;&#29992;&#21487;&#20197;&#23548;&#20986;&#19968;&#20010;&#21487;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#22312;&#23545;&#25968;&#22240;&#23376;&#33539;&#22260;&#20869;&#35299;&#20915;&#21407;&#22987;&#30340;&#38750;&#20984;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21442;&#25968;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;&#65292;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25910;&#25947;&#21040;&#35757;&#32451;&#25439;&#22833;&#36739;&#20302;&#30340;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30456;&#23545;&#20110;&#29616;&#26377;&#32467;&#26524;&#32780;&#35328;&#26159;&#25351;&#25968;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations. We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of $O(\sqrt{\log n})$, where $n$ is the number of training samples. A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor. Moreover, under mild assumptions, we show that with random initialization on the parameters local gradient methods almost surely converge to a point that has low training loss. Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#37319;&#29992;&#38543;&#26426;&#30697;&#38453;&#26041;&#27861;&#65292;&#22312;&#20302;&#22810;&#32447;&#24615;&#31209;&#24352;&#37327;&#36924;&#36817;&#20013;&#23637;&#31034;&#20102;&#23545;&#31181;&#26893;&#30340;&#20302;&#31209;&#20449;&#21495;&#30340;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#22823;&#32500;&#35889;&#34892;&#20026;&#21644;&#20449;&#22122;&#27604;&#20934;&#30830;&#39044;&#27979;&#20102;&#37325;&#24314;&#24615;&#33021;&#65292;&#24182;&#32473;&#20986;&#20102;HOOI&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.03169</link><description>&lt;p&gt;
&#20302;&#22810;&#32447;&#24615;&#31209;&#24352;&#37327;&#36924;&#36817;&#30340;&#38543;&#26426;&#30697;&#38453;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#37319;&#29992;&#38543;&#26426;&#30697;&#38453;&#26041;&#27861;&#65292;&#22312;&#20302;&#22810;&#32447;&#24615;&#31209;&#24352;&#37327;&#36924;&#36817;&#20013;&#23637;&#31034;&#20102;&#23545;&#31181;&#26893;&#30340;&#20302;&#31209;&#20449;&#21495;&#30340;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#22823;&#32500;&#35889;&#34892;&#20026;&#21644;&#20449;&#22122;&#27604;&#20934;&#30830;&#39044;&#27979;&#20102;&#37325;&#24314;&#24615;&#33021;&#65292;&#24182;&#32473;&#20986;&#20102;HOOI&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#35745;&#31639;&#38408;&#20540;&#38468;&#36817;&#30340;&#19968;&#33324;&#23574;&#23792;&#24352;&#37327;&#27169;&#22411;&#65292;&#23545;&#31181;&#26893;&#30340;&#20302;&#31209;&#20449;&#21495;&#20272;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35748;&#35782;&#12290;&#20381;&#38752;&#22823;&#22411;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#25968;&#25454;&#24352;&#37327;&#30340;&#23637;&#24320;&#30340;&#22823;&#32500;&#35889;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#20915;&#23450;&#20027;&#35201;&#20449;&#21495;&#26041;&#21521;&#21487;&#26816;&#27979;&#24615;&#30340;&#30456;&#20851;&#20449;&#22122;&#27604;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#22312;&#38750;&#24179;&#20961;&#21306;&#22495;&#30340;&#25130;&#26029;&#22810;&#32447;&#24615;&#22855;&#24322;&#20540;&#20998;&#35299;(MLSVD)&#30340;&#37325;&#24314;&#24615;&#33021;&#12290;&#36825;&#19968;&#28857;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20316;&#20026;&#26356;&#39640;&#38454;&#27491;&#20132;&#36845;&#20195;(HOOI)&#26041;&#26696;&#30340;&#21021;&#22987;&#21270;&#65292;&#20854;&#25910;&#25947;&#21040;&#26368;&#20339;&#20302;&#22810;&#32447;&#24615;&#31209;&#36924;&#36817;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;HOOI&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#22312;&#22823;&#32500;&#26497;&#38480;&#19979;&#25910;&#25947;&#21069;&#30340;&#36845;&#20195;&#27425;&#25968;&#36235;&#20110;1&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive understanding of the estimation of a planted low-rank signal from a general spiked tensor model near the computational threshold. Relying on standard tools from the theory of large random matrices, we characterize the large-dimensional spectral behavior of the unfoldings of the data tensor and exhibit relevant signal-to-noise ratios governing the detectability of the principal directions of the signal. These results allow to accurately predict the reconstruction performance of truncated multilinear SVD (MLSVD) in the non-trivial regime. This is particularly important since it serves as an initialization of the higher-order orthogonal iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank approximation depends entirely on its initialization. We give a sufficient condition for the convergence of HOOI and show that the number of iterations before convergence tends to $1$ in the large-dimensional limit.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auxiliary-Delayed Reinforcement Learning (AD-RL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03141</link><description>&lt;p&gt;
&#20351;&#29992;&#36741;&#21161;&#30701;&#26102;&#24310;&#20219;&#21153;&#25552;&#21319;&#38271;&#26102;&#24310;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Long-Delayed Reinforcement Learning with Auxiliary Short-Delayed Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03141
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auxiliary-Delayed Reinforcement Learning (AD-RL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#24773;&#26223;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24310;&#36831;&#24773;&#26223;&#26159;&#25351;&#35266;&#23519;&#21644;&#20132;&#20114;&#23384;&#22312;&#24310;&#36831;&#30340;&#24120;&#35265;&#23454;&#38469;&#24773;&#20917;&#12290;&#29616;&#26377;&#25216;&#26415;&#20013;&#65292;&#29366;&#24577;&#22686;&#24378;&#25216;&#26415;&#22312;&#24310;&#36831;&#27493;&#39588;&#20013;&#21487;&#33021;&#20250;&#20986;&#29616;&#29366;&#24577;&#31354;&#38388;&#25193;&#22823;&#25110;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Auxiliary-Delayed Reinforcement Learning&#65288;AD-RL&#65289;&#65292;&#21033;&#29992;&#19968;&#20010;&#36741;&#21161;&#30340;&#30701;&#26102;&#24310;&#20219;&#21153;&#26469;&#21152;&#36895;&#38271;&#26102;&#24310;&#20219;&#21153;&#30340;&#23398;&#20064;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AD-RL&#22312;&#30701;&#26102;&#24310;&#20219;&#21153;&#20013;&#23398;&#20064;&#20540;&#20989;&#25968;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#38271;&#26102;&#24310;&#20219;&#21153;&#20013;&#30340;&#33258;&#20030;&#21644;&#31574;&#30053;&#25913;&#36827;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#19982;&#30452;&#25509;&#22312;&#21407;&#22987;&#38271;&#26102;&#24310;&#20219;&#21153;&#19978;&#23398;&#20064;&#30456;&#27604;&#65292;&#36825;&#26679;&#20570;&#21487;&#20197;&#22823;&#22823;&#20943;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is challenging in delayed scenarios, a common real-world situation where observations and interactions occur with delays. State-of-the-art (SOTA) state-augmentation techniques either suffer from the state-space explosion along with the delayed steps, or performance degeneration in stochastic environments. To address these challenges, our novel Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary short-delayed task to accelerate the learning on a long-delayed task without compromising the performance in stochastic environments. Specifically, AD-RL learns the value function in the short-delayed task and then employs it with the bootstrapping and policy improvement techniques in the long-delayed task. We theoretically show that this can greatly reduce the sample complexity compared to directly learning on the original long-delayed task. On deterministic and stochastic benchmarks, our method remarkably outperforms the SOTAs in both sample efficienc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.02500</link><description>&lt;p&gt;
&#28857;&#20113;&#38382;&#39064;:&#37325;&#26032;&#24605;&#32771;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#21644;&#27867;&#21270;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#27169;&#24577;&#23545;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#35266;&#27979;&#31354;&#38388;&#23545;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#19977;&#31181;&#20027;&#35201;&#27169;&#24577;&#65306;RGB&#65292;RGB-D&#21644;&#28857;&#20113;&#12290;&#36890;&#36807;&#22312;&#36229;&#36807;17&#20010;&#19981;&#21516;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#28041;&#21450;&#20004;&#20010;&#22522;&#20934;&#21644;&#20223;&#30495;&#22120;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36235;&#21183;&#65306;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35774;&#35745;&#65292;&#36890;&#24120;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;RGB&#21644;RGB-D&#30340;&#23545;&#24212;&#29289;&#12290;&#36825;&#22312;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28857;&#20113;&#35266;&#27979;&#22312;&#30456;&#26426;&#35270;&#35282;&#12289;&#29031;&#26126;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#32972;&#26223;&#22806;&#35266;&#31561;&#21508;&#31181;&#20960;&#20309;&#21644;&#35270;&#35273;&#32447;&#32034;&#26041;&#38754;&#65292;&#37117;&#33021;&#25552;&#39640;&#31574;&#30053;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19977;&#32500;&#28857;&#20113;&#26159;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#26377;&#20215;&#20540;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25152;&#26377;&#30340;&#20195;&#30721;&#21644;&#26816;&#26597;&#28857;&#65292;&#24076;&#26395;&#25105;&#20204;&#30340;&#35266;&#28857;&#33021;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the influence of different observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. Through extensive experimentation on over 17 varied contact-rich manipulation tasks, conducted across two benchmarks and simulators, we have observed a notable trend: point cloud-based methods, even those with the simplest designs, frequently surpass their RGB and RGB-D counterparts in performance. This remains consistent in both scenarios: training from scratch and utilizing pretraining. Furthermore, our findings indicate that point cloud observations lead to improved policy zero-shot generalization in relation to various geometry and visual clues, including camera viewpoints, lighting conditions, noise levels and background appearance. The outcomes suggest that 3D point cloud is a valuable observation modality for intricate robotic tasks. We will open-source all our codes and checkpoints, hoping that our insights can help de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00759</link><description>&lt;p&gt;
&#26500;&#24314;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Building Expressive and Tractable Probabilistic Generative Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#22266;&#26377;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#31361;&#20986;&#20102;&#20351;PCs&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30340;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#36890;&#36807;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#27010;&#24565;&#26469;&#26500;&#24314;&#28145;&#24230;&#21644;&#28151;&#21512;PCs&#30340;&#21162;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#22810;&#32452;&#23398;&#20064;&#25193;&#23637;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#36755;&#20986;&#21487;&#35299;&#37322;&#19988;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#26641;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#24191;&#20041;&#21270;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00258</link><description>&lt;p&gt;
&#22810;&#32452;&#23398;&#20064;&#30340;&#23618;&#27425;&#32452;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-group Learning for Hierarchical Groups
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22810;&#32452;&#23398;&#20064;&#25193;&#23637;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#24773;&#20917;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#36755;&#20986;&#21487;&#35299;&#37322;&#19988;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#26641;&#39044;&#27979;&#22120;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#24191;&#20041;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32452;&#23398;&#20064;&#27169;&#22411;&#23558;&#23398;&#20064;&#22330;&#26223;&#35268;&#33539;&#21270;&#20026;&#21333;&#19968;&#39044;&#27979;&#22120;&#22312;&#22810;&#20010;&#21487;&#33021;&#37325;&#21472;&#30340;&#20852;&#36259;&#23376;&#32452;&#19978;&#24517;&#39035;&#24191;&#20041;&#21270;&#12290;&#25105;&#20204;&#23558;&#22810;&#32452;&#23398;&#20064;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#20102;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#33258;&#28982;&#24773;&#20917;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#29992;&#20110;&#36755;&#20986;&#21487;&#35299;&#37322;&#19988;&#30830;&#23450;&#24615;&#30340;&#20915;&#31574;&#26641;&#39044;&#27979;&#22120;&#65292;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#32463;&#39564;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#20855;&#26377;&#23618;&#27425;&#32452;&#32467;&#26500;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26377;&#21560;&#24341;&#21147;&#30340;&#24191;&#20041;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-group learning model formalizes the learning scenario in which a single predictor must generalize well on multiple, possibly overlapping subgroups of interest. We extend the study of multi-group learning to the natural case where the groups are hierarchically structured. We design an algorithm for this setting that outputs an interpretable and deterministic decision tree predictor with near-optimal sample complexity. We then conduct an empirical evaluation of our algorithm and find that it achieves attractive generalization properties on real datasets with hierarchical group structure.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#21457;&#29616;&#32479;&#35745;&#27169;&#24335;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#20174;&#39640;&#38454;&#32047;&#31215;&#37327;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.14922</link><description>&lt;p&gt;
&#20174;&#39640;&#38454;&#32479;&#35745;&#37327;&#20013;&#39640;&#25928;&#23398;&#20064;&#65306;&#20551;&#35774;&#26816;&#39564;&#12289;&#38543;&#26426;&#29305;&#24449;&#21644;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning from higher-order statistics, efficiently: hypothesis tests, random features, and neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14922
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#21457;&#29616;&#32479;&#35745;&#27169;&#24335;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#20174;&#39640;&#38454;&#32047;&#31215;&#37327;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25797;&#38271;&#21457;&#29616;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#30340;&#32479;&#35745;&#27169;&#24335;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24230;&#37327;&#19977;&#20010;&#25110;&#26356;&#22810;&#21464;&#37327;&#38388;&#30340;&#38750;&#39640;&#26031;&#30456;&#20851;&#24615;&#30340;&#39640;&#38454;&#32047;&#31215;&#37327;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#29305;&#21035;&#37325;&#35201;&#12290;&#20294;&#31070;&#32463;&#32593;&#32476;&#26377;&#22810;&#26377;&#25928;&#22320;&#20174;&#39640;&#38454;&#32047;&#31215;&#37327;&#20013;&#25552;&#21462;&#29305;&#24449;&#65311;&#25105;&#20204;&#22312;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#37324;&#32479;&#35745;&#23398;&#23478;&#38656;&#35201;&#20174;$d$&#32500;&#36755;&#20837;&#30340;&#38454;-$p\ge 4$&#32047;&#31215;&#37327;&#20013;&#24674;&#22797;&#20986;&#19968;&#20010;&#29305;&#26435;&#26041;&#21521;&#25110;&#8220;&#23574;&#23792;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#25152;&#38656;&#26679;&#26412;&#25968;$n$&#26469;&#34920;&#24449;&#24674;&#22797;&#23574;&#23792;&#30340;&#22522;&#26412;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#65292;&#20197;&#24378;&#28872;&#21306;&#20998;&#26469;&#33258;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#21644;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#36755;&#20837;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32479;&#35745;&#19978;&#30340;&#21487;&#21306;&#20998;&#24615;&#38656;&#35201;$n\gtrsim d$&#20010;&#26679;&#26412;&#65292;&#32780;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21306;&#20998;&#36825;&#20004;&#20010;&#20998;&#24067;&#21017;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14922v2 Announce Type: replace-cross  Abstract: Neural networks excel at discovering statistical patterns in high-dimensional data sets. In practice, higher-order cumulants, which quantify the non-Gaussian correlations between three or more variables, are particularly important for the performance of neural networks. But how efficient are neural networks at extracting features from higher-order cumulants? We study this question in the spiked cumulant model, where the statistician needs to recover a privileged direction or "spike" from the order-$p\ge 4$ cumulants of $d$-dimensional inputs. We first characterise the fundamental statistical and computational limits of recovering the spike by analysing the number of samples $n$ required to strongly distinguish between inputs from the spiked cumulant model and isotropic Gaussian inputs. We find that statistical distinguishability requires $n\gtrsim d$ samples, while distinguishing the two distributions in polynomial time require
&lt;/p&gt;</description></item><item><title>ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2401.16467</link><description>&lt;p&gt;
ReGAL: &#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReGAL: Refactoring Programs to Discover Generalizable Abstractions. (arXiv:2401.16467v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16467
&lt;/p&gt;
&lt;p&gt;
ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#24320;&#21457;&#26377;&#29992;&#25277;&#35937;&#25152;&#38656;&#30340;&#20840;&#23616;&#35270;&#35282;&#65307;&#23427;&#20204;&#36890;&#24120;&#19968;&#27425;&#39044;&#27979;&#19968;&#20010;&#31243;&#24207;&#65292;&#32463;&#24120;&#37325;&#22797;&#30456;&#21516;&#30340;&#21151;&#33021;&#12290;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#20887;&#20313;&#20195;&#30721;&#26082;&#20302;&#25928;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36890;&#29992;&#25277;&#35937;&#23398;&#20064;&#30340;&#37325;&#26500;&#26041;&#27861;&#65288;ReGAL&#65289;&#65292;&#36890;&#36807;&#20195;&#30721;&#37325;&#26500;&#26469;&#23398;&#20064;&#21487;&#37325;&#29992;&#20989;&#25968;&#24211;&#65292;&#21363;&#22312;&#19981;&#25913;&#21464;&#20195;&#30721;&#25191;&#34892;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#37325;&#32452;&#20195;&#30721;&#12290;ReGAL&#20174;&#19968;&#23567;&#32452;&#29616;&#26377;&#31243;&#24207;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#25191;&#34892;&#39564;&#35777;&#21644;&#32454;&#21270;&#25277;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ReGAL&#21457;&#29616;&#30340;&#20849;&#20139;&#20989;&#25968;&#24211;&#20351;&#24471;&#22312;&#19981;&#21516;&#39046;&#22495;&#39044;&#27979;&#31243;&#24207;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;LOGO&#22270;&#24418;&#29983;&#25104;&#12289;&#26085;&#26399;&#25512;&#29702;&#21644;&#22522;&#20110;Minecraft&#30340;&#25991;&#23383;&#28216;&#25103;TextCraft&#65289;&#19978;&#65292;&#24320;&#28304;&#21644;&#19987;&#26377;&#30340;LLMs&#22312;&#20351;&#29992;ReGAL&#20989;&#25968;&#24211;&#39044;&#27979;&#31243;&#24207;&#26102;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e. restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On three datasets (LOGO graphics generation, Date reasoning, and TextCraft, a Minecraft-based text game), both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL fun
&lt;/p&gt;</description></item><item><title>VisualWebArena&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#23545;&#20195;&#29702;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#35201;&#27714;&#20195;&#29702;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#26469;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.13649</link><description>&lt;p&gt;
VisualWebArena: &#22312;&#30495;&#23454;&#35270;&#35273;Web&#20219;&#21153;&#19978;&#35780;&#20272;&#22810;&#27169;&#24577;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks. (arXiv:2401.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13649
&lt;/p&gt;
&lt;p&gt;
VisualWebArena&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#24615;&#33021;&#30340;&#22522;&#20934;&#65292;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#23545;&#20195;&#29702;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#35201;&#27714;&#20195;&#29702;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#26469;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#32593;&#32476;&#19978;&#36827;&#34892;&#35745;&#21010;&#12289;&#25512;&#29702;&#21644;&#25191;&#34892;&#21160;&#20316;&#30340;&#33258;&#20027;&#20195;&#29702;&#20026;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#25991;&#26412;&#30340;&#20195;&#29702;&#65292;&#22312;&#25928;&#26524;&#19978;&#24573;&#35270;&#20102;&#35768;&#22810;&#38656;&#35201;&#35270;&#35273;&#20449;&#24687;&#25165;&#33021;&#26377;&#25928;&#35299;&#20915;&#30340;&#33258;&#28982;&#20219;&#21153;&#12290;&#37492;&#20110;&#22823;&#22810;&#25968;&#35745;&#31639;&#26426;&#30028;&#38754;&#26159;&#20026;&#20154;&#31867;&#24863;&#30693;&#32780;&#35774;&#35745;&#30340;&#65292;&#35270;&#35273;&#20449;&#24687;&#24448;&#24448;&#20197;&#25991;&#26412;&#25968;&#25454;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#30340;&#26041;&#24335;&#22686;&#24378;&#25991;&#26412;&#25968;&#25454;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VisualWebArena&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;Web&#20195;&#29702;&#22312;&#30495;&#23454;&#30340;&#8220;&#35270;&#35273;&#22522;&#30784;&#20219;&#21153;&#8221;&#19978;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;VisualWebArena&#21253;&#25324;&#19968;&#32452;&#22810;&#26679;&#19988;&#22797;&#26434;&#30340;&#22522;&#20110;Web&#30340;&#20219;&#21153;&#65292;&#35780;&#20272;&#33258;&#20027;&#22810;&#27169;&#24577;&#20195;&#29702;&#30340;&#21508;&#31181;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#25191;&#34892;&#65292;&#20195;&#29702;&#38656;&#35201;&#20934;&#30830;&#22788;&#29702;&#22270;&#20687;-&#25991;&#26412;&#36755;&#20837;&#65292;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#21160;&#20316;&#20197;&#23436;&#25104;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an ext
&lt;/p&gt;</description></item><item><title>Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2401.10774</link><description>&lt;p&gt;
Medusa: &#22810;&#35299;&#30721;&#22836;&#30340;&#31616;&#27905;LLM&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. (arXiv:2401.10774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10774
&lt;/p&gt;
&lt;p&gt;
Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#21463;&#38480;&#20110;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#24182;&#34892;&#24615;&#32570;&#22833;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#25805;&#20316;&#21463;&#38480;&#20110;&#21152;&#36895;&#22120;&#30340;&#20869;&#23384;&#24102;&#23485;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#31867;&#20284;&#20110;&#25512;&#27979;&#35299;&#30721;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#33719;&#24471;&#21644;&#32500;&#25252;&#29420;&#31435;&#30340;&#33609;&#31295;&#27169;&#22411;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#39069;&#22806;&#30340;&#35299;&#30721;&#22836;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#65292;&#20197;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#12290;Medusa&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#21516;&#26102;&#26500;&#36896;&#22810;&#20010;&#20505;&#36873;&#24310;&#32493;&#24182;&#36827;&#34892;&#39564;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#22788;&#29702;&#65292;Medusa&#22312;&#21333;&#27493;&#24310;&#36831;&#26041;&#38754;&#20165;&#24341;&#20837;&#20102;&#26368;&#23567;&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25152;&#38656;&#30340;&#35299;&#30721;&#27493;&#39588;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.  We present two levels of fine-tuning procedures for Medusa to meet the needs o
&lt;/p&gt;</description></item><item><title>MELODY&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#23454;&#20307;&#32423;&#21035;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#37096;&#32626;&#30340;&#24322;&#26500;&#24615;&#12289;&#20302;&#24310;&#36831;&#23481;&#24525;&#24230;&#12289;&#27169;&#31946;&#30340;&#24322;&#24120;&#23450;&#20041;&#21644;&#26377;&#38480;&#30340;&#30417;&#30563;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10338</link><description>&lt;p&gt;
MELODY: &#24378;&#22823;&#30340;&#21322;&#30417;&#30563;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#22522;&#20110;&#23454;&#20307;&#32423;&#21035;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MELODY: Robust Semi-Supervised Hybrid Model for Entity-Level Online Anomaly Detection with Multivariate Time Series. (arXiv:2401.10338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10338
&lt;/p&gt;
&lt;p&gt;
MELODY&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#23454;&#20307;&#32423;&#21035;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#37096;&#32626;&#30340;&#24322;&#26500;&#24615;&#12289;&#20302;&#24310;&#36831;&#23481;&#24525;&#24230;&#12289;&#27169;&#31946;&#30340;&#24322;&#24120;&#23450;&#20041;&#21644;&#26377;&#38480;&#30340;&#30417;&#30563;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;IT&#31995;&#32479;&#20013;&#65292;&#36719;&#20214;&#37096;&#32626;&#26159;&#22312;&#32447;&#26381;&#21153;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#20195;&#30721;&#32463;&#24120;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26377;&#25925;&#38556;&#30340;&#20195;&#30721;&#26356;&#25913;&#21487;&#33021;&#20250;&#38477;&#20302;&#30446;&#26631;&#26381;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19979;&#28216;&#26381;&#21153;&#20013;&#24341;&#36215;&#36830;&#38145;&#25925;&#38556;&#12290;&#22240;&#27492;&#65292;&#24212;&#20840;&#38754;&#30417;&#27979;&#36719;&#20214;&#37096;&#32626;&#65292;&#24182;&#21450;&#26102;&#26816;&#27979;&#20986;&#24322;&#24120;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#32626;&#30340;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19982;&#35813;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#30456;&#20851;&#30340;&#23454;&#20307;&#32423;&#21035;&#65288;&#20363;&#22914;&#65292;&#37096;&#32626;&#65289;&#30456;&#23545;&#20110;&#26356;&#20856;&#22411;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#36825;&#20123;&#29420;&#29305;&#30340;&#25361;&#25112;&#21253;&#25324;&#37096;&#32626;&#30340;&#24322;&#26500;&#24615;&#12289;&#20302;&#24310;&#36831;&#23481;&#24525;&#24230;&#12289;&#27169;&#31946;&#30340;&#24322;&#24120;&#23450;&#20041;&#21644;&#26377;&#38480;&#30340;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#23454;&#20307;&#32423;&#21035;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#30340;&#21322;&#30417;&#30563;&#28151;&#21512;&#27169;&#22411;&#65288;MELODY&#65289;&#12290;MELODY&#39318;&#20808;&#23558;&#19981;&#21516;&#23454;&#20307;&#30340;MTS&#36716;&#25442;&#20026;&#30456;&#21516;&#30340;&#24418;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
In large IT systems, software deployment is a crucial process in online services as their code is regularly updated. However, a faulty code change may degrade the target service's performance and cause cascading outages in downstream services. Thus, software deployments should be comprehensively monitored, and their anomalies should be detected timely. In this paper, we study the problem of anomaly detection for deployments. We begin by identifying the challenges unique to this anomaly detection problem, which is at entity-level (e.g., deployments), relative to the more typical problem of anomaly detection in multivariate time series (MTS). The unique challenges include the heterogeneity of deployments, the low latency tolerance, the ambiguous anomaly definition, and the limited supervision. To address them, we propose a novel framework, semi-supervised hybrid Model for Entity-Level Online Detection of anomalY (MELODY). MELODY first transforms the MTS of different entities to the same 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#20505;&#36873;&#27744;&#21644;QE&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06688</link><description>&lt;p&gt;
&#19981;&#35201;&#25490;&#21517;&#65292;&#35201;&#21512;&#24182;&#65281;&#20351;&#29992;&#36136;&#37327;&#20272;&#35745;&#26469;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation. (arXiv:2401.06688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#20505;&#36873;&#27744;&#21644;QE&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36890;&#36807;&#32473;&#23450;&#28304;&#21477;&#23376;&#20272;&#35745;&#30446;&#26631;&#21477;&#23376;&#30340;&#27010;&#29575;&#65292;&#20294;&#36825;&#20123;&#20272;&#35745;&#21487;&#33021;&#19982;&#20154;&#31867;&#21916;&#22909;&#19981;&#19968;&#33268;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QE-fusion&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26356;&#33021;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#30340;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#65288;QE&#65289;&#26469;&#32508;&#21512;&#25913;&#36827;&#32763;&#35793;&#32467;&#26524;&#12290;QE-fusion&#21033;&#29992;&#20174;&#27169;&#22411;&#20013;&#25277;&#21462;&#30340;&#20505;&#36873;&#27744;&#65292;&#20351;&#29992;&#20687;CometKiwi&#36825;&#26679;&#30340;QE&#25351;&#26631;&#32452;&#21512;&#19981;&#21516;&#20505;&#36873;&#30340;&#29255;&#27573;&#12290;&#25105;&#20204;&#23558;QE-fusion&#19982;&#27874;&#26463;&#25628;&#32034;&#21644;&#26368;&#36817;&#30340;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#65288;&#22914;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#25110;QE-&#37325;&#26032;&#25490;&#24207;&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#24403;&#24212;&#29992;&#20110;&#29992;&#20110;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;PolyLM&#12289;XGLM&#12289;Llama2&#21644;Mistral&#65289;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#65288;NLLB&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;COMET&#21644;BLEURT&#35780;&#20998;&#26041;&#38754;&#22987;&#32456;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#28085;&#30422;&#20116;&#31181;&#35821;&#35328;&#23545;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#26356;&#22823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method utilizing a quality estimation metric (QE) that better correlates with human judgments to synthesize improved translations. QE-fusion leverages a candidate pool sampled from a model, combining spans from different candidates using QE metrics such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#21449;&#29109;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#31070;&#32463;&#25240;&#21472;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#25240;&#21472;&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#31867;&#22343;&#20540;&#30340;&#20960;&#20309;&#29305;&#24615;&#20250;&#21457;&#29983;&#20559;&#31227;&#12290;</title><link>http://arxiv.org/abs/2401.02058</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#32422;&#26463;ReLU&#29305;&#24449;&#27169;&#22411;&#36827;&#34892;&#20132;&#21449;&#29109;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#31070;&#32463;&#25240;&#21472;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Feature Model. (arXiv:2401.02058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#21449;&#29109;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#31070;&#32463;&#25240;&#21472;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;&#22312;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#25240;&#21472;&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#31867;&#22343;&#20540;&#30340;&#20960;&#20309;&#29305;&#24615;&#20250;&#21457;&#29983;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#33539;&#24335;&#21253;&#25324;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#23558;&#35757;&#32451;&#25439;&#22833;&#20540;&#25512;&#21521;&#38646;&#65292;&#21363;&#20351;&#35757;&#32451;&#35823;&#24046;&#24050;&#32463;&#28040;&#22833;&#12290;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#35266;&#23519;&#21040;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20250;&#25240;&#21472;&#21040;&#23427;&#20204;&#30340;&#31867;&#22343;&#20540;&#65292;&#24182;&#19988;&#36825;&#20123;&#31867;&#22343;&#20540;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#31616;&#21333;&#20856;&#22411;&#31561;&#35282;&#32039;&#26694;&#65288;ETF&#65289;&#30340;&#39030;&#28857;&#12290;&#36825;&#19968;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#25240;&#21472;&#65288;NC&#65289;&#12290;&#20026;&#20102;&#20174;&#29702;&#35770;&#19978;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#26469;&#35777;&#26126;NC&#20986;&#29616;&#22312;&#35757;&#32451;&#38382;&#39064;&#30340;&#20840;&#23616;&#35299;&#20013;&#12290;&#28982;&#32780;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#26102;&#65292;&#19968;&#20123;NC&#29305;&#24615;&#23558;&#19981;&#20877;&#25104;&#31435;&#12290;&#20363;&#22914;&#65292;&#24403;&#25439;&#22833;&#25910;&#25947;&#26102;&#65292;&#31867;&#22343;&#20540;&#20960;&#20309;&#20250;&#20559;&#31163;&#31616;&#21333;&#20856;&#22411;&#31561;&#35282;&#32039;&#26694;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;NC&#25512;&#24191;&#21040;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#26080;&#32422;&#26463;ReLU&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#35757;&#32451;&#38382;&#39064;&#30340;&#20840;&#23616;&#35299;&#20013;&#65292;&#24403;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#26102;&#65292;NC&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#23545;&#20110;&#31867;&#22343;&#20540;&#30340;&#20960;&#20309;&#29305;&#24615;&#20250;&#21457;&#29983;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current paradigm of training deep neural networks for classification tasks includes minimizing the empirical risk that pushes the training loss value towards zero, even after the training error has been vanished. In this terminal phase of training, it has been observed that the last-layer features collapse to their class-means and these class-means converge to the vertices of a simplex Equiangular Tight Frame (ETF). This phenomenon is termed as Neural Collapse (NC). To theoretically understand this phenomenon, recent works employ a simplified unconstrained feature model to prove that NC emerges at the global solutions of the training problem. However, when the training dataset is class-imbalanced, some NC properties will no longer be true. For example, the class-means geometry will skew away from the simplex ETF when the loss converges. In this paper, we generalize NC to imbalanced regime for cross-entropy loss under the unconstrained ReLU feature model. We prove that, while the wi
&lt;/p&gt;</description></item><item><title>SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00793</link><description>&lt;p&gt;
SecFormer&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00793
&lt;/p&gt;
&lt;p&gt;
SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#20113;&#24179;&#21488;&#19978;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#20379;&#25512;&#29702;&#26381;&#21153;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#38544;&#31169;&#38382;&#39064;&#26085;&#30410;&#21152;&#21095;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#25237;&#36164;&#35745;&#21010;&#21644;&#38134;&#34892;&#36134;&#25143;&#31561;&#25935;&#24863;&#25968;&#25454;&#12290;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMPC&#65289;&#34987;&#35270;&#20026;&#20445;&#25252;&#25512;&#29702;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#38544;&#31169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;SMPC&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20943;&#36895;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;Transformer&#26550;&#26500;&#20013;&#30340;&#20247;&#22810;&#38750;&#32447;&#24615;&#25805;&#20316;&#19981;&#36866;&#21512;SMPC&#65292;&#24182;&#19988;&#38590;&#20197;&#26377;&#25928;&#35268;&#36991;&#25110;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;SecFormer&#65292;&#20197;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#26045;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#25105;&#20204;&#25104;&#21151;&#28040;&#38500;&#20102;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;Nesterov&#21160;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#24555;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;softmax&#31574;&#30053;&#21442;&#25968;&#21270;&#20013;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#23427;&#20197; $\tilde{O}(1/t^2)$ &#30340;&#36895;&#29575;&#25910;&#25947;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;Nesterov&#21152;&#36895;&#26799;&#24230;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.11897</link><description>&lt;p&gt;
&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;&#65306;&#20851;&#20110;&#24212;&#29992;Nesterov&#21160;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning. (arXiv:2310.11897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;Nesterov&#21160;&#37327;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26356;&#24555;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;softmax&#31574;&#30053;&#21442;&#25968;&#21270;&#20013;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#23427;&#20197; $\tilde{O}(1/t^2)$ &#30340;&#36895;&#29575;&#25910;&#25947;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;Nesterov&#21152;&#36895;&#26799;&#24230;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#22312;&#38750;&#27491;&#21017;&#21270;&#34920;&#26684;softmax&#35774;&#32622;&#20013;&#20197;&#920;(1/t)&#30340;&#36895;&#29575;&#20840;&#23616;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20165;&#20351;&#29992;&#19968;&#38454;&#26356;&#26032;&#36827;&#19968;&#27493;&#25913;&#36827;&#36825;&#31181;&#25910;&#25947;&#36895;&#24230;&#12290;&#26412;&#25991;&#20174;&#21160;&#37327;&#30340;&#35282;&#24230;&#22238;&#31572;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#33879;&#21517;&#30340;Nesterov&#21152;&#36895;&#26799;&#24230;&#65288;NAG&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#31216;&#20043;&#20026; \textit{&#21152;&#36895;&#31574;&#30053;&#26799;&#24230;}&#65288;APG&#65289;&#12290;&#20026;&#20102;&#23637;&#31034;APG&#22312;&#23454;&#29616;&#26356;&#24555;&#20840;&#23616;&#25910;&#25947;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#20351;&#29992;&#30495;&#23454;&#26799;&#24230;&#26102;&#65292;&#20855;&#26377; softmax &#31574;&#30053;&#21442;&#25968;&#21270;&#30340;APG&#20197; $\tilde{O}(1/t^2)$ &#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;NAG&#22312;RL&#39046;&#22495;&#20013;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#30340;&#31532;&#19968;&#20010;&#34920;&#24449;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#20381;&#36182;&#20110;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;&#19981;&#35770;&#21021;&#22987;&#21270;&#22914;&#20309;&#65292;APG&#26368;&#32456;&#21487;&#20197;&#36798;&#21040;&#36817;&#20046;&#23616;&#37096;&#25910;&#25947;&#30340;&#22320;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy gradient methods have recently been shown to enjoy global convergence at a $\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-con
&lt;/p&gt;</description></item><item><title>AdaLomo&#26159;&#19968;&#31181;&#20302;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.10195</link><description>&lt;p&gt;
AdaLomo: &#20302;&#20869;&#23384;&#20248;&#21270;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;
&lt;/p&gt;
&lt;p&gt;
AdaLomo: Low-memory Optimization with Adaptive Learning Rate. (arXiv:2310.10195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10195
&lt;/p&gt;
&lt;p&gt;
AdaLomo&#26159;&#19968;&#31181;&#20302;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#35268;&#27169;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#20869;&#23384;&#65292;&#20174;&#32780;&#35774;&#32622;&#20102;&#24456;&#39640;&#30340;&#38376;&#27099;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#30340;&#20302;&#20869;&#23384;&#20248;&#21270;&#65288;LOMO&#65289;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#65292;&#20294;&#20854;&#20248;&#21270;&#25216;&#26415;&#31867;&#20284;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#23545;&#36229;&#21442;&#25968;&#25935;&#24863;&#24182;&#23637;&#29616;&#20986;&#27425;&#20248;&#30340;&#25910;&#25947;&#24615;&#65292;&#26080;&#27861;&#19982;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;AdamW&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#36890;&#36807;&#23545;Adam&#20248;&#21270;&#22120;&#36827;&#34892;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#21160;&#37327;&#26469;&#35828;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#23545;&#20110;&#24357;&#21512;&#24046;&#36317;&#26356;&#20026;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;&#20302;&#20869;&#23384;&#20248;&#21270;&#65288;AdaLomo&#65289;&#65292;&#20026;&#27599;&#20010;&#21442;&#25968;&#25552;&#20379;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#20026;&#20102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#65292;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#29366;&#24577;&#20013;&#37319;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26469;&#20272;&#35745;&#20108;&#38454;&#30697;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20998;&#32452;&#26356;&#26032;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update norma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;DPZero&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#19982;&#32500;&#24230;&#26080;&#20851;&#19988;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#20869;&#23384;&#21644;&#38544;&#31169;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.09639</link><description>&lt;p&gt;
DPZero&#65306;&#19982;&#32500;&#24230;&#26080;&#20851;&#19988;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DPZero: Dimension-Independent and Differentially Private Zeroth-Order Optimization. (arXiv:2310.09639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09639
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;DPZero&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#19982;&#32500;&#24230;&#26080;&#20851;&#19988;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#20869;&#23384;&#21644;&#38544;&#31169;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32454;&#35843;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#36341;&#20013;&#65292;&#38754;&#20020;&#30528;&#20869;&#23384;&#21644;&#38544;&#31169;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#38543;&#30528;LLM&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#38271;&#65292;&#36798;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#26041;&#27861;&#25152;&#38656;&#30340;&#20869;&#23384;&#28040;&#32791;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;LLM&#20542;&#21521;&#20110;&#35760;&#24518;&#21644;&#27844;&#38706;&#25935;&#24863;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24517;&#39035;&#20445;&#25252;&#32454;&#35843;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#38646;&#38454;&#26041;&#27861;&#19982;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#30456;&#32467;&#21512;&#29992;&#20110;LLM&#30340;&#32454;&#35843;&#30340;&#28508;&#21147;&#12290;&#38646;&#38454;&#26041;&#27861;&#20165;&#20381;&#36182;&#21069;&#21521;&#20256;&#36882;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;&#23427;&#20204;&#19982;&#26631;&#20934;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#32467;&#21512;&#22312;&#19968;&#36215;&#20250;&#23548;&#33268;&#32500;&#24230;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DPZero&#65292;&#19968;&#31181;&#20855;&#26377;&#36817;&#20046;&#32500;&#24230;&#26080;&#20851;&#29575;&#30340;&#26032;&#22411;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
The widespread practice of fine-tuning pretrained large language models (LLMs) on domain-specific data faces two major challenges in memory and privacy. First, as the size of LLMs continue to grow, encompassing billions of parameters, the memory demands of gradient-based training methods via backpropagation become prohibitively high. Second, given the tendency of LLMs to memorize and disclose sensitive training data, the privacy of fine-tuning data must be respected. To this end, we explore the potential of zeroth-order methods in differentially private optimization for fine-tuning LLMs. Zeroth-order methods, which rely solely on forward passes, substantially reduce memory consumption during training. However, directly combining them with standard differential privacy mechanism poses dimension-dependent complexity. To bridge the gap, we introduce DPZero, a novel differentially private zeroth-order algorithm with nearly dimension-independent rates. Our theoretical analysis reveals that 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07579</link><description>&lt;p&gt;
In-Context Unlearning: &#22522;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#28040;&#38500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07579
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#28040;&#38500;&#23545;&#20110;&#24456;&#22823;&#27169;&#22411;&#26469;&#35828;&#22312;&#35745;&#31639;&#19978;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#34892;&#24615;&#21644;&#20415;&#25463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#26159;&#30740;&#31350;&#22914;&#20309;&#39640;&#25928;&#22320;&#21435;&#38500;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36817;&#26469;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38656;&#35201;&#36981;&#23432;&#35832;&#22914;&#34987;&#36951;&#24536;&#26435;&#31561;&#38544;&#31169;&#27861;&#35268;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#22312;&#29256;&#26435;&#38382;&#39064;&#19978;LLM&#65288;&#35821;&#35328;&#27169;&#22411;&#65289;&#23588;&#20854;&#30456;&#20851;&#65292;&#20294;&#22312;&#38750;&#24120;&#22823;&#30340;&#27169;&#22411;&#19978;&#23454;&#29616;&#31934;&#30830;&#28040;&#38500;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36817;&#20284;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#31639;&#27861;&#20851;&#38190;&#20381;&#36182;&#20110;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#35775;&#38382;&#26469;&#26356;&#26032;&#23427;&#20204;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#30001;&#20110;&#35745;&#31639;&#32422;&#26463;&#25110;&#36890;&#36807;API&#35775;&#38382;LLM&#32780;&#26080;&#27861;&#28385;&#36275;&#36825;&#31181;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#28040;&#38500;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#28040;&#38500;&#8221;&#65292;&#23427;&#25552;&#20379;&#20102;&#19978;&#19979;&#25991;&#30340;&#36755;&#20837;&#19988;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#28040;&#38500;&#29305;&#23450;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;i
&lt;/p&gt;
&lt;p&gt;
Machine unlearning, the study of efficiently removing the impact of specific training points on the trained model, has garnered increased attention of late, driven by the need to comply with privacy regulations like the Right to be Forgotten. Although unlearning is particularly relevant for LLMs in light of the copyright issues they raise, achieving precise unlearning is computationally infeasible for very large models. To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or when the LLM is accessed via API. In this work, we propose a new class of unlearning methods for LLMs we call ''In-Context Unlearning'', providing inputs in context and without having to update model parameters. To unlearn a particular training instance, we provide the i
&lt;/p&gt;</description></item><item><title>&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#29983;&#25104;&#21547;&#26377;&#30495;&#23454;&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAPS&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#33293;&#24323;&#38500;&#26368;&#22823;softmax&#27010;&#29575;&#20043;&#22806;&#30340;&#25152;&#26377;&#27010;&#29575;&#20540;&#65292;&#20943;&#23569;&#20102;&#38750;&#19968;&#33268;&#24615;&#35780;&#20998;&#23545;&#27010;&#29575;&#20540;&#30340;&#20381;&#36182;&#24615;&#65292;&#36798;&#21040;&#20102;&#29983;&#25104;&#23567;&#23610;&#23544;&#38598;&#21512;&#21644;&#20256;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.06430</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#31614;&#25490;&#21517;&#23454;&#29616;&#28145;&#24230;&#20998;&#31867;&#22120;&#30340;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction for Deep Classifier via Label Ranking. (arXiv:2310.06430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06430
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#29983;&#25104;&#21547;&#26377;&#30495;&#23454;&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SAPS&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#33293;&#24323;&#38500;&#26368;&#22823;softmax&#27010;&#29575;&#20043;&#22806;&#30340;&#25152;&#26377;&#27010;&#29575;&#20540;&#65292;&#20943;&#23569;&#20102;&#38750;&#19968;&#33268;&#24615;&#35780;&#20998;&#23545;&#27010;&#29575;&#20540;&#30340;&#20381;&#36182;&#24615;&#65292;&#36798;&#21040;&#20102;&#29983;&#25104;&#23567;&#23610;&#23544;&#38598;&#21512;&#21644;&#20256;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#29983;&#25104;&#21547;&#26377;&#25152;&#38656;&#35206;&#30422;&#20445;&#35777;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20135;&#29983;&#30340;&#39044;&#27979;&#27010;&#29575;&#36890;&#24120;&#26159;&#38169;&#35823;&#26657;&#20934;&#30340;&#65292;&#23548;&#33268;&#31526;&#21512;&#39044;&#27979;&#20013;&#30340;&#22823;&#22411;&#39044;&#27979;&#38598;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#24573;&#30053;&#27010;&#29575;&#20540;&#21487;&#20197;&#32531;&#35299;&#38169;&#35823;&#26657;&#20934;&#27010;&#29575;&#20540;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25490;&#24207;&#33258;&#36866;&#24212;&#39044;&#27979;&#38598;&#8221;&#65288;SAPS&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33293;&#24323;&#38500;&#26368;&#22823;softmax&#27010;&#29575;&#20043;&#22806;&#30340;&#25152;&#26377;&#27010;&#29575;&#20540;&#12290;SAPS&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#20445;&#30041;&#19981;&#30830;&#23450;&#24615;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#23613;&#37327;&#20943;&#23569;&#38750;&#19968;&#33268;&#24615;&#35780;&#20998;&#23545;&#27010;&#29575;&#20540;&#30340;&#20381;&#36182;&#24615;&#12290;&#36825;&#26679;&#65292;SAPS&#21487;&#20197;&#29983;&#25104;&#23567;&#23610;&#23544;&#30340;&#38598;&#21512;&#65292;&#24182;&#20256;&#36798;&#27599;&#20010;&#23454;&#20363;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;SAPS&#30340;&#26377;&#38480;&#26679;&#26412;&#35206;&#30422;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#26399;&#26395;&#20540;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a statistical framework that generates prediction sets containing ground-truth labels with a desired coverage guarantee. The predicted probabilities produced by machine learning models are generally miscalibrated, leading to large prediction sets in conformal prediction. In this paper, we empirically and theoretically show that disregarding the probabilities' value will mitigate the undesirable effect of miscalibrated probability values. Then, we propose a novel algorithm named $\textit{Sorted Adaptive prediction sets}$ (SAPS), which discards all the probability values except for the maximum softmax probability. The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information. In this manner, SAPS can produce sets of small size and communicate instance-wise uncertainty. Theoretically, we provide a finite-sample coverage guarantee of SAPS and show that the expected value of se
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04406</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#32479;&#19968;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#12289;&#34892;&#21160;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04406
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#31995;&#21015;&#20915;&#31574;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#34892;&#21160;&#36807;&#31243;&#65292;&#24182;&#26410;&#33021;&#24191;&#27867;&#37096;&#32626;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATS&#65288;&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;LLMs&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#20114;&#21327;&#21516;&#12290;LATS&#20511;&#37492;&#20102;&#27169;&#22411;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24605;&#24819;&#65292;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#12289;&#20215;&#20540;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#65292;&#37325;&#26032;&#21033;&#29992;&#20854;&#28508;&#22312;&#30340;&#20248;&#21183;&#20197;&#25552;&#21319;&#20915;&#31574;&#33021;&#21147;&#12290;&#20851;&#38190;&#30340;&#19968;&#28857;&#26159;LATS&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#32534;&#31243;&#12289;HotPotQA&#21644;WebShop&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LATS&#22312;&#25512;&#29702;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#32534;&#31243;&#26041;&#38754;&#65292;LATS&#23454;&#29616;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21487;&#32553;&#25918;&#25512;&#33616;&#27169;&#22411;&#20013;&#23884;&#20837;&#23618;&#30340;&#23849;&#28291;&#29616;&#35937;&#65292;&#21457;&#29616;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;&#23884;&#20837;&#23398;&#20064;&#65292;&#20294;&#20063;&#26159;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.04400</link><description>&lt;p&gt;
&#35770;&#21487;&#25193;&#23637;&#25512;&#33616;&#27169;&#22411;&#20013;&#23884;&#20837;&#22349;&#32553;&#29616;&#35937;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Embedding Collapse when Scaling up Recommendation Models. (arXiv:2310.04400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04400
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21487;&#32553;&#25918;&#25512;&#33616;&#27169;&#22411;&#20013;&#23884;&#20837;&#23618;&#30340;&#23849;&#28291;&#29616;&#35937;&#65292;&#21457;&#29616;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#38480;&#21046;&#20102;&#23884;&#20837;&#23398;&#20064;&#65292;&#20294;&#20063;&#26159;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#21457;&#20102;&#24320;&#21457;&#22823;&#22411;&#25512;&#33616;&#27169;&#22411;&#20197;&#21033;&#29992;&#22823;&#37327;&#21487;&#29992;&#25968;&#25454;&#30340;&#26377;&#21069;&#26223;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35797;&#39564;&#25918;&#22823;&#29616;&#26377;&#30340;&#25512;&#33616;&#27169;&#22411;&#26102;&#21457;&#29616;&#65292;&#25193;&#22823;&#30340;&#27169;&#22411;&#24182;&#27809;&#26377;&#20196;&#20154;&#28385;&#24847;&#30340;&#25913;&#36827;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#22823;&#27169;&#22411;&#30340;&#23884;&#20837;&#23618;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#23884;&#20837;&#22349;&#32553;&#29616;&#35937;&#65292;&#36825;&#26368;&#32456;&#38459;&#30861;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#36825;&#31181;&#29616;&#35937;&#20013;&#65292;&#23884;&#20837;&#30697;&#38453;&#20542;&#21521;&#20110;&#23384;&#22312;&#20110;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25512;&#33616;&#27169;&#22411;&#29305;&#23450;&#30340;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#20855;&#26377;&#21452;&#37325;&#20316;&#29992;&#12290;&#19968;&#26041;&#38754;&#65292;&#24403;&#19982;&#22349;&#32553;&#30340;&#23884;&#20837;&#20132;&#20114;&#26102;&#65292;&#35813;&#20132;&#20114;&#38480;&#21046;&#20102;&#23884;&#20837;&#23398;&#20064;&#65292;&#21152;&#21095;&#20102;&#23849;&#28291;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29305;&#24449;&#20132;&#20114;&#23545;&#20110;&#32531;&#35299;&#20551;&#29305;&#24449;&#30340;&#25311;&#21512;&#33267;&#20851;&#37325;&#35201;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep foundation models have led to a promising trend of developing large recommendation models to leverage vast amounts of available data. However, we experiment to scale up existing recommendation models and observe that the enlarged models do not improve satisfactorily. In this context, we investigate the embedding layers of enlarged models and identify a phenomenon of embedding collapse, which ultimately hinders scalability, wherein the embedding matrix tends to reside in a low-dimensional subspace. Through empirical and theoretical analysis, we demonstrate that the feature interaction module specific to recommendation models has a two-sided effect. On the one hand, the interaction restricts embedding learning when interacting with collapsed embeddings, exacerbating the collapse issue. On the other hand, feature interaction is crucial in mitigating the fitting of spurious features, thereby improving scalability. Based on this analysis, we propose a simple yet effe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35299;&#32806;&#26102;&#38388;&#22270;&#32593;&#32476;&#30340;&#26680;&#24515;&#27169;&#22359;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#35299;&#32806;&#26102;&#38388;&#22270;&#32593;&#32476; (LDTGN)&#12290;&#22312;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#36807;&#31243;&#20013;&#65292;LDTGN&#34920;&#29616;&#20986;&#19982;&#20043;&#21069;&#26041;&#27861;&#21487;&#27604;&#29978;&#33267;&#39046;&#20808;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02721</link><description>&lt;p&gt;
&#21033;&#29992;&#27169;&#22359;&#35299;&#32806;&#25552;&#21319;&#26102;&#38388;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Leveraging Temporal Graph Networks Using Module Decoupling. (arXiv:2310.02721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35299;&#32806;&#26102;&#38388;&#22270;&#32593;&#32476;&#30340;&#26680;&#24515;&#27169;&#22359;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#35299;&#32806;&#26102;&#38388;&#22270;&#32593;&#32476; (LDTGN)&#12290;&#22312;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#36807;&#31243;&#20013;&#65292;LDTGN&#34920;&#29616;&#20986;&#19982;&#20043;&#21069;&#26041;&#27861;&#21487;&#27604;&#29978;&#33267;&#39046;&#20808;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#26174;&#33879;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#25209;&#22788;&#29702;&#26469;&#26367;&#20195;&#36880;&#20010;&#26356;&#26032;&#12290;&#37319;&#29992;&#25209;&#22788;&#29702;&#20351;&#24471;&#36825;&#20123;&#25216;&#26415;&#22312;&#27969;&#24335;&#22330;&#26223;&#20013;&#33021;&#22815;&#22788;&#29702;&#26497;&#24555;&#36895;&#24230;&#30340;&#22270;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25209;&#22788;&#29702;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#26356;&#26032;&#39057;&#29575;&#38477;&#20302;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#31574;&#30053;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#20351;&#29992;&#25209;&#22788;&#29702;&#30340;&#21516;&#26102;&#39057;&#32321;&#22320;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#22270;&#32593;&#32476;&#30340;&#26680;&#24515;&#27169;&#22359;&#36827;&#34892;&#35299;&#32806;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#36827;&#34892;&#23454;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36731;&#37327;&#32423;&#35299;&#32806;&#26102;&#38388;&#22270;&#32593;&#32476; (LDTGN)&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#39640;&#25928;&#30340;&#23398;&#20064;&#21160;&#24577;&#22270;&#30340;&#27169;&#22411;&#12290;LDTGN&#22312;&#21508;&#31181;&#21160;&#24577;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#22312;&#21534;&#21520;&#37327;&#26174;&#33879;&#39640;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#21487;&#27604;&#25110;&#20855;&#26377;&#26368;&#26032;&#25104;&#26524;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;be&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36229;&#36807;&#20043;&#21069;&#30340;&#26041;&#27861;20%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern approaches for learning on dynamic graphs have adopted the use of batches instead of applying updates one by one. The use of batches allows these techniques to become helpful in streaming scenarios where updates to graphs are received at extreme speeds. Using batches, however, forces the models to update infrequently, which results in the degradation of their performance. In this work, we suggest a decoupling strategy that enables the models to update frequently while using batches. By decoupling the core modules of temporal graph networks and implementing them using a minimal number of learnable parameters, we have developed the Lightweight Decoupled Temporal Graph Network (LDTGN), an exceptionally efficient model for learning on dynamic graphs. LDTG was validated on various dynamic graph benchmarks, providing comparable or state-of-the-art results with significantly higher throughput than previous art. Notably, our method outperforms previous approaches by more than 20\% on be
&lt;/p&gt;</description></item><item><title>GenCO&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#25972;&#21512;&#20102;&#23884;&#20837;&#30340;&#32452;&#21512;&#27714;&#35299;&#22120;&#21644;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#21457;&#29616;&#19982;&#38750;&#32447;&#24615;&#30446;&#26631;&#19968;&#33268;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.02442</link><description>&lt;p&gt;
GenCO: &#29992;&#20110;&#20855;&#26377;&#32452;&#21512;&#29305;&#24449;&#30340;&#35774;&#35745;&#38382;&#39064;&#30340;&#29983;&#25104;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GenCO: Generating Diverse Solutions to Design Problems with Combinatorial Nature. (arXiv:2310.02442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02442
&lt;/p&gt;
&lt;p&gt;
GenCO&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#25972;&#21512;&#20102;&#23884;&#20837;&#30340;&#32452;&#21512;&#27714;&#35299;&#22120;&#21644;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#21457;&#29616;&#19982;&#38750;&#32447;&#24615;&#30446;&#26631;&#19968;&#33268;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;GAN&#25110;VAE&#65289;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#35937;&#65288;&#22914;&#22270;&#20687;&#65289;&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#65292;&#20197;&#24110;&#21161;&#35299;&#20915;&#35768;&#22810;&#20256;&#32479;&#19978;&#30001;&#20154;&#31867;&#23436;&#25104;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36229;&#36234;&#22270;&#20687;&#29983;&#25104;&#65292;&#22312;&#26356;&#19968;&#33324;&#30340;&#35774;&#35745;&#38382;&#39064;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#35774;&#35745;&#30340;&#22810;&#26679;&#24615;&#21644;&#32422;&#26463;&#30340;&#19968;&#33268;&#24615;&#37117;&#24456;&#37325;&#35201;&#12290;&#36825;&#26679;&#30340;&#35774;&#32622;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#12289;&#21160;&#30011;&#12289;&#24037;&#19994;&#35774;&#35745;&#12289;&#26448;&#26009;&#31185;&#23398;&#31561;&#39046;&#22495;&#20013;&#37117;&#26377;&#24212;&#29992;&#65292;&#20854;&#20013;&#25105;&#20204;&#24076;&#26395;&#29983;&#25104;&#22120;&#30340;&#36755;&#20986;&#36981;&#24490;&#31163;&#25955;/&#32452;&#21512;&#32422;&#26463;&#24182;&#24809;&#32602;&#20219;&#20309;&#20559;&#31163;&#65292;&#36825;&#23545;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#20248;&#21270;&#27714;&#35299;&#22120;&#26469;&#35828;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GenCO&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23427;&#38598;&#25104;&#20102;&#23884;&#20837;&#30340;&#32452;&#21512;&#27714;&#35299;&#22120;&#21644;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#26088;&#22312;&#21457;&#29616;&#19982;&#38750;&#32447;&#24615;&#30446;&#26631;&#19968;&#33268;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating diverse objects (e.g., images) using generative models (such as GAN or VAE) has achieved impressive results in the recent years, to help solve many design problems that are traditionally done by humans. Going beyond image generation, we aim to find solutions to more general design problems, in which both the diversity of the design and conformity of constraints are important. Such a setting has applications in computer graphics, animation, industrial design, material science, etc, in which we may want the output of the generator to follow discrete/combinatorial constraints and penalize any deviation, which is non-trivial with existing generative models and optimization solvers. To address this, we propose GenCO, a novel framework that conducts end-to-end training of deep generative models integrated with embedded combinatorial solvers, aiming to uncover high-quality solutions aligned with nonlinear objectives. While structurally akin to conventional generative models, GenCO 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SCoRe&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#22330;&#26223;&#19979;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23376;&#27169;&#32452;&#21512;&#20989;&#25968;&#65292;&#25105;&#20204;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#29305;&#24449;&#31751;&#30340;&#22810;&#26679;&#24615;&#21644;&#21512;&#20316;&#24615;&#12290;&#36825;&#23545;&#20110;&#20811;&#26381;&#31867;&#21035;&#19981;&#24179;&#34913;&#22312;&#33258;&#20027;&#23548;&#33322;&#21644;&#21307;&#23398;&#35786;&#26029;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.00165</link><description>&lt;p&gt;
SCoRe&#65306;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#31867;&#21035;&#19981;&#24179;&#34913;&#22330;&#26223;&#30340;&#23376;&#27169;&#32858;&#21512;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SCoRe: Submodular Combinatorial Representation Learning for Real-World Class-Imbalanced Settings. (arXiv:2310.00165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SCoRe&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#22330;&#26223;&#19979;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23376;&#27169;&#32452;&#21512;&#20989;&#25968;&#65292;&#25105;&#20204;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#29305;&#24449;&#31751;&#30340;&#22810;&#26679;&#24615;&#21644;&#21512;&#20316;&#24615;&#12290;&#36825;&#23545;&#20110;&#20811;&#26381;&#31867;&#21035;&#19981;&#24179;&#34913;&#22312;&#33258;&#20027;&#23548;&#33322;&#21644;&#21307;&#23398;&#35786;&#26029;&#31561;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#28436;&#36827;&#36807;&#31243;&#20013;&#65292;&#30495;&#23454;&#19990;&#30028;&#31867;&#21035;&#19981;&#24179;&#34913;&#22330;&#26223;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#32597;&#35265;&#31867;&#21035;&#30340;&#35270;&#35273;&#21644;&#32467;&#26500;&#29305;&#24449;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#38480;&#21046;&#20102;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26377;&#21306;&#20998;&#24230;&#30340;&#29305;&#24449;&#31751;&#12290;&#36825;&#20307;&#29616;&#20026;&#25968;&#25454;&#38598;&#20013;&#32597;&#35265;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#22823;&#22411;&#31867;&#38388;&#20559;&#24046;&#20197;&#21450;&#20016;&#23500;&#31867;&#21035;&#20043;&#38388;&#30340;&#39640;&#20869;&#31867;&#21464;&#21270;&#12290;&#34429;&#28982;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#22312;&#33258;&#20027;&#23548;&#33322;&#21644;&#21307;&#23398;&#35786;&#26029;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#65292;&#20173;&#38656;&#35201;&#20316;&#20986;&#37325;&#22823;&#25913;&#36827;&#26469;&#20811;&#26381;&#31867;&#21035;&#19981;&#24179;&#34913;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#38598;&#21512;&#30340;&#32452;&#21512;&#20989;&#25968;&#65292;&#22914;&#23376;&#27169;&#20449;&#24687;&#24230;&#37327;&#65292;&#20855;&#26377;&#27169;&#25311;&#29305;&#24449;&#31751;&#22810;&#26679;&#24615;&#21644;&#21512;&#20316;&#24615;&#30340;&#29305;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SCoRe&#65288;&#23376;&#27169;&#32858;&#21512;&#34920;&#31034;&#23398;&#20064;&#65289;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23376;&#27169;C&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation Learning in real-world class-imbalanced settings has emerged as a challenging task in the evolution of deep learning. Lack of diversity in visual and structural features for rare classes restricts modern neural networks to learn discriminative feature clusters. This manifests in the form of large inter-class bias between rare object classes and elevated intra-class variance among abundant classes in the dataset. Although deep metric learning approaches have shown promise in this domain, significant improvements need to be made to overcome the challenges associated with class-imbalance in mission critical tasks like autonomous navigation and medical diagnostics. Set-based combinatorial functions like Submodular Information Measures exhibit properties that allow them to simultaneously model diversity and cooperation among feature clusters. In this paper, we introduce the SCoRe (Submodular Combinatorial Representation Learning) framework and propose a family of Submodular C
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#30340;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29702;&#35770;&#65292;&#25506;&#32034;&#20102;&#23427;&#19982;&#32463;&#20856;&#22270;&#20687;&#28388;&#27874;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#19982;&#25193;&#25955;&#21644;&#28183;&#36879;&#28388;&#27874;&#30340;&#27010;&#24565;&#21644;&#32463;&#39564;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.08511</link><description>&lt;p&gt;
&#24191;&#20041;&#27010;&#29575;&#25193;&#25955;&#23610;&#24230;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Generalised Probabilistic Diffusion Scale-Spaces. (arXiv:2309.08511v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#30340;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29702;&#35770;&#65292;&#25506;&#32034;&#20102;&#23427;&#19982;&#32463;&#20856;&#22270;&#20687;&#28388;&#27874;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#19982;&#25193;&#25955;&#21644;&#28183;&#36879;&#28388;&#27874;&#30340;&#27010;&#24565;&#21644;&#32463;&#39564;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#22312;&#20174;&#23398;&#20064;&#30340;&#20998;&#24067;&#20013;&#37319;&#26679;&#26032;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#26368;&#21021;&#30001;&#29289;&#29702;&#23398;&#20013;&#30340;&#28418;&#31227;&#25193;&#25955;&#27010;&#24565;&#25512;&#21160;&#65292;&#23427;&#20204;&#36890;&#36807;&#27491;&#21521;&#36807;&#31243;&#24212;&#29992;&#22270;&#20687;&#25200;&#21160;&#65292;&#22914;&#22122;&#22768;&#21644;&#27169;&#31946;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30456;&#24212;&#30340;&#23398;&#20064;&#36870;&#36807;&#31243;&#29983;&#25104;&#22270;&#20687;&#65292;&#24182;&#21487;&#22312;&#38468;&#21152;&#20449;&#24687;&#26465;&#20214;&#19979;&#36827;&#34892;&#35843;&#25972;&#65292;&#20174;&#32780;&#23548;&#33268;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#23454;&#36341;&#23548;&#21521;&#30340;&#25193;&#23637;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29702;&#35770;&#32972;&#26223;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#65292;&#23588;&#20854;&#26159;&#19982;&#28418;&#31227;&#25193;&#25955;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#38416;&#26126;&#19982;&#32463;&#20856;&#22270;&#20687;&#28388;&#27874;&#30340;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#30340;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29702;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#25193;&#25955;&#21644;&#28183;&#36879;&#28388;&#27874;&#30340;&#27010;&#24565;&#21644;&#32463;&#39564;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic diffusion models excel at sampling new images from learned distributions. Originally motivated by drift-diffusion concepts from physics, they apply image perturbations such as noise and blur in a forward process that results in a tractable probability distribution. A corresponding learned reverse process generates images and can be conditioned on side information, which leads to a wide variety of practical applications. Most of the research focus currently lies on practice-oriented extensions. In contrast, the theoretical background remains largely unexplored, in particular the relations to drift-diffusion. In order to shed light on these connections to classical image filtering, we propose a generalised scale-space theory for probabilistic diffusion models. Moreover, we show conceptual and empirical connections to diffusion and osmosis filters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06054</link><description>&lt;p&gt;
&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#23545;&#21512;&#25104;&#20219;&#21153;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#65292;&#30740;&#31350;&#20102;&#34920;&#31034;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#23545;&#23398;&#20064;&#24615;&#33021;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21363;&#20174;&#19978;&#19979;&#25991;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#26159;Transformer&#30340;&#19968;&#39033;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39537;&#21160;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#23578;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#34920;&#31034;&#23398;&#20064;&#35282;&#24230;&#36827;&#34892;&#35843;&#26597;&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#34920;&#31034;&#26356;&#21152;&#22797;&#26434;&#65292;&#34920;&#31034;&#21487;&#20197;&#21463;&#21040;&#27169;&#22411;&#26435;&#37325;&#21644;&#19978;&#19979;&#25991;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#19978;&#36848;&#20004;&#20010;&#27010;&#24565;&#26041;&#38754;&#30340;&#34920;&#31034;&#20998;&#21035;&#31216;&#20026;&#26435;&#37325;&#20869;&#37096;&#25104;&#20998;&#21644;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20004;&#20010;&#25104;&#20998;&#22914;&#20309;&#24433;&#21709;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#35774;&#35745;&#20004;&#20010;&#25506;&#38024;&#65292;&#21363;&#26435;&#37325;&#20869;&#37096;&#25506;&#38024;&#21644;&#19978;&#19979;&#25991;&#25506;&#38024;&#65292;&#20998;&#21035;&#35780;&#20272;&#36825;&#20004;&#20010;&#25104;&#20998;&#12290;&#25105;&#20204;&#35777;&#26126;&#19978;&#19979;&#25991;&#20869;&#37096;&#25104;&#20998;&#30340;&#22909;&#22351;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#65292;&#36825;&#34920;&#26126;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32416;&#32544;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, i.e., learning from in-context samples, is an impressive ability of Transformer. However, the mechanism driving the in-context learning is not yet fully understood. In this study, we aim to investigate from an underexplored perspective of representation learning. The representation is more complex for in-context learning senario, where the representation can be impacted by both model weights and in-context samples. We refer the above two conceptually aspects of representation as in-weight component and in-context component, respectively. To study how the two components affect in-context learning capabilities, we construct a novel synthetic task, making it possible to device two probes, in-weights probe and in-context probe, to evaluate the two components, respectively. We demonstrate that the goodness of in-context component is highly related to the in-context learning performance, which indicates the entanglement between in-context learning and representation lear
&lt;/p&gt;</description></item><item><title>RepCodec&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#24182;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00169</link><description>&lt;p&gt;
RepCodec:&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#26631;&#35760;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
RepCodec: A Speech Representation Codec for Speech Tokenization. (arXiv:2309.00169v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00169
&lt;/p&gt;
&lt;p&gt;
RepCodec&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#24182;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#31163;&#25955;&#35821;&#38899;&#26631;&#35760;&#22312;&#23558;&#35821;&#38899;&#27880;&#20837;LLMs&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31163;&#25955;&#21270;&#23548;&#33268;&#20102;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#20123;&#31163;&#25955;&#35821;&#38899;&#26631;&#35760;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepCodec&#65292;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#35821;&#38899;&#26631;&#35760;&#30340;&#26032;&#22411;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#12290;&#19982;&#37325;&#26032;&#26500;&#24314;&#21407;&#22987;&#38899;&#39057;&#30340;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#19981;&#21516;&#65292;RepCodec&#36890;&#36807;&#20174;&#35821;&#38899;&#32534;&#30721;&#22120;&#65288;&#22914;HuBERT&#25110;data2vec&#65289;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#26469;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#12290;&#35821;&#38899;&#32534;&#30721;&#22120;&#12289;&#32534;&#35299;&#30721;&#22120;&#21644;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#20849;&#21516;&#26500;&#25104;&#19968;&#20010;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#30340;&#27969;&#27700;&#32447;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30001;&#20110;&#20854;&#22686;&#24378;&#30340;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#23454;&#29616;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#36798;&#21040;&#21516;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08858</link><description>&lt;p&gt;
&#27809;&#26377;&#27169;&#22411;&#30340;&#31639;&#27861;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Model-Free Algorithm with Improved Sample Efficiency for Zero-Sum Markov Games. (arXiv:2308.08858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#23454;&#29616;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#36798;&#21040;&#21516;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#38382;&#39064;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#26377;&#38480;&#26102;&#38388;&#27573;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$O(H^3SAB/\epsilon^2)$&#25214;&#21040;$\epsilon$-&#26368;&#20248;&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#65292;&#20854;&#20013;$H$&#26159;&#26102;&#38388;&#27573;&#65292;$S$&#26159;&#29366;&#24577;&#25968;&#37327;&#65288;$A$&#21644;$B$&#20998;&#21035;&#34920;&#31034;&#20004;&#20010;&#29609;&#23478;&#30340;&#21160;&#20316;&#25968;&#37327;&#65289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#36825;&#26679;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#33258;&#30001;&#30340;&#38454;&#27573;&#24615;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#23454;&#29616;&#20102;&#19982;&#26368;&#20339;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#22240;&#27492;&#39318;&#27425;&#35777;&#26126;&#20102;&#27169;&#22411;&#33258;&#30001;&#31639;&#27861;&#21487;&#20197;&#22312;&#26102;&#38388;&#27573;&#20381;&#36182;&#24615;&#26041;&#38754;&#20139;&#21463;&#19982;&#27169;&#22411;&#20026;&#22522;&#30784;&#31639;&#27861;&#30456;&#21516;&#30340;&#20248;&#21270;&#25928;&#26524;&#12290;&#23545;&#20110;$H$&#30340;&#20381;&#36182;&#24615;&#30340;&#20027;&#35201;&#25913;&#36827;&#26469;&#28304;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The problem of two-player zero-sum Markov games has recently attracted increasing interests in theoretical studies of multi-agent reinforcement learning (RL). In particular, for finite-horizon episodic Markov decision processes (MDPs), it has been shown that model-based algorithms can find an $\epsilon$-optimal Nash Equilibrium (NE) with the sample complexity of $O(H^3SAB/\epsilon^2)$, which is optimal in the dependence of the horizon $H$ and the number of states $S$ (where $A$ and $B$ denote the number of actions of the two players, respectively). However, none of the existing model-free algorithms can achieve such an optimality. In this work, we propose a model-free stage-based Q-learning algorithm and show that it achieves the same sample complexity as the best model-based algorithm, and hence for the first time demonstrate that model-free algorithms can enjoy the same optimality in the $H$ dependence as model-based algorithms. The main improvement of the dependency on $H$ arises by
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#21442;&#25968;&#21270;&#32593;&#26684;&#21270;&#21644;&#27169;&#25311;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34746;&#26059;&#31649;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#26032;&#30340;&#21453;&#24212;&#22120;&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#27809;&#26377;&#26799;&#24230;&#30340;&#38750;&#23616;&#37096;&#20248;&#21270;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2308.08841</link><description>&lt;p&gt;
&#36890;&#36807;CFD&#32806;&#21512;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#19979;&#30340;&#26032;&#21453;&#24212;&#22120;&#35774;&#35745;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Assisted Discovery of Novel Reactor Designs via CFD-Coupled Multi-fidelity Bayesian Optimisation. (arXiv:2308.08841v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#21442;&#25968;&#21270;&#32593;&#26684;&#21270;&#21644;&#27169;&#25311;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34746;&#26059;&#31649;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#26032;&#30340;&#21453;&#24212;&#22120;&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#27809;&#26377;&#26799;&#24230;&#30340;&#38750;&#23616;&#37096;&#20248;&#21270;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#21046;&#36896;&#25216;&#26415;&#20351;&#24471;&#26356;&#20808;&#36827;&#30340;&#21453;&#24212;&#22120;&#20960;&#20309;&#32467;&#26500;&#25104;&#20026;&#21487;&#33021;&#65292;&#20026;&#26356;&#22823;&#21644;&#26356;&#22797;&#26434;&#30340;&#35774;&#35745;&#31354;&#38388;&#25552;&#20379;&#20102;&#28508;&#22312;&#26426;&#20250;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#30830;&#23450;&#21644;&#20248;&#21270;&#26377;&#24076;&#26395;&#30340;&#37197;&#32622;&#23545;&#20110;&#29616;&#26377;&#30340;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#26041;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#37492;&#20110;&#31639;&#27861;&#25913;&#36827;&#21644;&#28155;&#21152;&#21046;&#36896;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34746;&#26059;&#31649;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21464;&#25130;&#38754;&#21644;&#34746;&#26059;&#36335;&#24452;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#39640;&#32500;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#30830;&#20445;&#21487;&#34892;&#30340;&#38750;&#23616;&#37096;&#20248;&#21270;&#65292;&#22312;&#27809;&#26377;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24212;&#29992;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25551;&#36848;&#20102;&#22810;&#20010;&#36830;&#32493;&#30340;&#31934;&#24230;&#65292;&#24182;&#19982;&#21442;&#25968;&#21270;&#32593;&#26684;&#21270;&#21644;&#27169;&#25311;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20302;&#36136;&#37327;&#12289;
&lt;/p&gt;
&lt;p&gt;
Additive manufacturing has enabled the production of more advanced reactor geometries, resulting in the potential for significantly larger and more complex design spaces. Identifying and optimising promising configurations within broader design spaces presents a significant challenge for existing human-centric design approaches. As such, existing parameterisations of coiled-tube reactor geometries are low-dimensional with expensive optimisation limiting more complex solutions. Given algorithmic improvements and the onset of additive manufacturing, we propose two novel coiled-tube parameterisations enabling the variation of cross-section and coil path, resulting in a series of high dimensional, complex optimisation problems. To ensure tractable, non-local optimisation where gradients are not available, we apply multi-fidelity Bayesian optimisation. Our approach characterises multiple continuous fidelities and is coupled with parameterised meshing and simulation, enabling lower quality, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;</title><link>http://arxiv.org/abs/2308.07876</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26412;&#30693;&#35782;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;ChatGPT&#26469;&#21512;&#25104;&#25919;&#27835;&#38646;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20107;&#20214;&#32534;&#30721;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#36828;&#36229;&#36807;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#26032;&#30340;&#27880;&#37322;&#65292;&#24573;&#35270;&#20102;&#19987;&#23478;&#25968;&#25454;&#24211;&#20013;&#30340;&#22823;&#37327;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#12289;&#35821;&#24577;&#21644;&#31867;&#21035;&#28040;&#27495;&#30340;&#19981;&#21516;&#23618;&#27425;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;ChatGPT&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#31361;&#20986;&#20102;ZSP&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;ZSP&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25552;&#39640;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#26469;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#30830;&#20445;&#20854;&#22312;&#26679;&#26412;&#22823;&#23567;&#36235;&#36817;&#26080;&#31351;&#26102;&#19982;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#35823;&#24046;&#25910;&#25947;&#20026;&#38646;&#65292;&#24182;&#19988;&#36828;&#31163;&#22797;&#21046;&#35757;&#32451;&#25968;&#25454;&#20013;&#31034;&#20363;&#30340;&#20219;&#20309;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2307.16422</link><description>&lt;p&gt;
&#20445;&#35777;&#20174;&#32463;&#39564;&#20998;&#24067;&#20013;&#26368;&#22823;&#20559;&#24046;&#30340;&#26368;&#20339;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution. (arXiv:2307.16422v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#26469;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#30830;&#20445;&#20854;&#22312;&#26679;&#26412;&#22823;&#23567;&#36235;&#36817;&#26080;&#31351;&#26102;&#19982;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#35823;&#24046;&#25910;&#25947;&#20026;&#38646;&#65292;&#24182;&#19988;&#36828;&#31163;&#22797;&#21046;&#35757;&#32451;&#25968;&#25454;&#20013;&#31034;&#20363;&#30340;&#20219;&#20309;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#21644;&#24037;&#19994;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#25311;&#20174;&#26410;&#30693;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#26032;&#31034;&#20363;&#65292;&#21516;&#26102;&#30830;&#20445;&#22810;&#26679;&#24615;&#24182;&#36991;&#20813;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#22797;&#21046;&#31034;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20004;&#20010;&#23646;&#24615;&#65306;&#65288;i&#65289;&#23558;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#26367;&#25442;&#30340;&#35823;&#24046;&#22312;&#26679;&#26412;&#22823;&#23567;&#36235;&#36817;&#26080;&#31351;&#26102;&#24212;&#26368;&#20339;&#25910;&#25947;&#20110;&#38646;&#65307;&#65288;ii&#65289;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#24212;&#36828;&#31163;&#22797;&#21046;&#35757;&#32451;&#25968;&#25454;&#20013;&#31034;&#20363;&#30340;&#20219;&#20309;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20197;&#26377;&#38480;&#26679;&#26412;&#39118;&#38505;&#30028;&#20026;&#24418;&#24335;&#30340;&#38750;&#28176;&#36817;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#36825;&#20123;&#23646;&#24615;&#65292;&#24182;&#21462;&#20915;&#20110;&#30456;&#20851;&#21442;&#25968;&#65292;&#22914;&#26679;&#26412;&#22823;&#23567;&#12289;&#29615;&#22659;&#31354;&#38388;&#30340;&#32500;&#25968;&#21644;&#28508;&#31354;&#38388;&#30340;&#32500;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#21508;&#31181;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling is a widely-used machine learning method with various applications in scientific and industrial fields. Its primary objective is to simulate new examples drawn from an unknown distribution given training data while ensuring diversity and avoiding replication of examples from the training data.  This paper presents theoretical insights into training a generative model with two properties: (i) the error of replacing the true data-generating distribution with the trained data-generating distribution should optimally converge to zero as the sample size approaches infinity, and (ii) the trained data-generating distribution should be far enough from any distribution replicating examples in the training data.  We provide non-asymptotic results in the form of finite sample risk bounds that quantify these properties and depend on relevant parameters such as sample size, the dimension of the ambient space, and the dimension of the latent space. Our results are applicable to g
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.15593</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Robust Distortion-free Watermarks for Language Models. (arXiv:2307.15593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#40065;&#26834;&#26080;&#30072;&#21464;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#38543;&#26426;&#25968;&#24207;&#21015;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#20998;&#24067;&#30340;&#21069;&#25552;&#19979;&#23545;&#27700;&#21360;&#25991;&#26412;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#22312;&#22810;&#31181;&#25913;&#20889;&#25915;&#20987;&#19979;&#20381;&#28982;&#20445;&#25345;&#36739;&#39640;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;40-50%&#30340;&#38543;&#26426;&#25200;&#21160;&#19979;&#20173;&#21487;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#27700;&#21360;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#27700;&#21360;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36825;&#20123;&#27700;&#21360;&#23545;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25991;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#35777;&#29983;&#25104;&#39044;&#31639;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#29992;&#38543;&#26426;&#27700;&#21360;&#23494;&#38053;&#35745;&#31639;&#30340;&#38543;&#26426;&#25968;&#24207;&#21015;&#26144;&#23556;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#26469;&#29983;&#25104;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#12290;&#35201;&#26816;&#27979;&#27700;&#21360;&#25991;&#26412;&#65292;&#21482;&#35201;&#30693;&#36947;&#23494;&#38053;&#30340;&#20219;&#20309;&#19968;&#26041;&#37117;&#21487;&#20197;&#23558;&#25991;&#26412;&#19982;&#38543;&#26426;&#25968;&#24207;&#21015;&#23545;&#40784;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#37319;&#26679;&#26041;&#26696;&#26469;&#23454;&#20363;&#21270;&#27700;&#21360;&#26041;&#27861;&#65306;&#21453;&#21464;&#25442;&#37319;&#26679;&#21644;&#25351;&#25968;&#26368;&#23567;&#37319;&#26679;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27700;&#21360;&#24212;&#29992;&#20110;&#19977;&#20010;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;OPT-1.3B&#12289;LLaMA-7B&#21644;Alpaca-7B&#65292;&#20197;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#30340;&#32479;&#35745;&#21151;&#25928;&#21644;&#23545;&#21508;&#31181;&#25913;&#20889;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;OPT-1.3B&#21644;LLaMA-7B&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25200;&#21160;&#20102;40-50%&#30340;&#35789;&#20803;&#21518;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#65288;$p \leq 0.01$&#65289;&#65292;&#21482;&#38656;&#35201;35&#20010;&#35789;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50$\% of the tokens via random
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#20855;&#22791;&#22810;&#31181;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05141</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#36816;&#21160;&#21407;&#29702;&#19982;&#36125;&#21494;&#26031;&#32858;&#21512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Deep Probabilistic Movement Primitives with a Bayesian Aggregator. (arXiv:2307.05141v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05141
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#20855;&#22791;&#22810;&#31181;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#21407;&#29702;&#26159;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#28436;&#31034;&#38598;&#21512;&#20013;&#22797;&#21046;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#20801;&#35768;&#26102;&#38388;&#35843;&#21046;&#36816;&#21160;&#65288;&#21152;&#36895;&#25110;&#20943;&#36895;&#22797;&#21046;&#36816;&#21160;&#65289;&#12289;&#28151;&#21512;&#65288;&#23558;&#20004;&#20010;&#36816;&#21160;&#21512;&#24182;&#20026;&#19968;&#20010;&#65289;&#12289;&#36890;&#36807;&#28857;&#35843;&#33410;&#65288;&#23558;&#36816;&#21160;&#32422;&#26463;&#21040;&#29305;&#23450;&#30340;&#36890;&#36807;&#28857;&#65289;&#21644;&#19978;&#19979;&#25991;&#35843;&#33410;&#65288;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#29983;&#25104;&#36816;&#21160;&#65292;&#20363;&#22914;&#29289;&#20307;&#30340;&#20301;&#32622;&#65289;&#23637;&#31034;&#20986;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19968;&#20123;&#24418;&#24335;&#30340;&#36755;&#20837;&#35843;&#33410;&#25110;&#26102;&#38388;&#35843;&#21046;&#34920;&#36798;&#20013;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#25552;&#20986;&#19968;&#20010;&#21333;&#19968;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20855;&#22791;&#25152;&#26377;&#20808;&#21069;&#30340;&#25805;&#20316;&#65292;&#36825;&#38480;&#21046;&#20102;&#31070;&#32463;&#36816;&#21160;&#21407;&#29702;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Movement primitives are trainable parametric models that reproduce robotic movements starting from a limited set of demonstrations. Previous works proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of movements (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object). Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations. However, there has not been a single unified deep motor primitive's model proposed that is capable of all previous operations, limiting neural motor primitive's potential applications. This paper proposes a deep movement primitive arch
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#32467;&#26524;&#26159;&#21542;&#26222;&#36866;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#27880;&#20837;&#35821;&#20041;&#20445;&#25345;&#30340;&#26356;&#25913;&#26469;&#25193;&#22823;&#27979;&#35797;&#38598;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#20934;&#30830;&#29575;&#26174;&#33879;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#20998;&#31867;&#26102;&#20351;&#29992;&#20102;&#19968;&#20123;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#24674;&#22797;&#21040;&#20043;&#21069;&#30340;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.17193</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#28431;&#27934;&#26816;&#27979;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Limits of Machine Learning for Automatic Vulnerability Detection. (arXiv:2306.17193v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17193
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#32467;&#26524;&#26159;&#21542;&#26222;&#36866;&#20173;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#27880;&#20837;&#35821;&#20041;&#20445;&#25345;&#30340;&#26356;&#25913;&#26469;&#25193;&#22823;&#27979;&#35797;&#38598;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#20934;&#30830;&#29575;&#26174;&#33879;&#19979;&#38477;&#65292;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#20998;&#31867;&#26102;&#20351;&#29992;&#20102;&#19968;&#20123;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#24674;&#22797;&#21040;&#20043;&#21069;&#30340;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#26368;&#26032;&#32467;&#26524;&#38750;&#24120;&#26377;&#24076;&#26395;&#65306;&#20165;&#32473;&#23450;&#20989;&#25968;$f$&#30340;&#28304;&#20195;&#30721;&#65292;&#32463;&#36807;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#39640;&#36798;70%&#30340;&#20934;&#30830;&#29575;&#21028;&#26029;$f$&#26159;&#21542;&#23384;&#22312;&#23433;&#20840;&#28431;&#27934;&#12290;&#20294;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#26222;&#36866;&#65292;&#32780;&#19981;&#20165;&#38480;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#27880;&#20837;&#35821;&#20041;&#20445;&#25345;&#30340;&#26356;&#25913;&#26469;&#25193;&#22823;&#27979;&#35797;&#38598;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26174;&#33879;&#19979;&#38477;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#35813;&#27169;&#22411;&#22312;&#20998;&#31867;&#26102;&#20351;&#29992;&#20102;&#19968;&#20123;&#26080;&#20851;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#22686;&#21152;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#22312;&#25193;&#23637;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#26524;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#24674;&#22797;&#21040;&#20043;&#21069;&#30340;&#27700;&#24179;&#12290;&#26412;&#25991;&#22797;&#21046;&#24182;&#32487;&#32493;&#20102;&#36825;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#32773;&#26356;&#22909;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#22312;&#28431;&#27934;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent results of machine learning for automatic vulnerability detection have been very promising indeed: Given only the source code of a function $f$, models trained by machine learning techniques can decide if $f$ contains a security flaw with up to 70% accuracy.  But how do we know that these results are general and not specific to the datasets? To study this question, researchers proposed to amplify the testing set by injecting semantic preserving changes and found that the model's accuracy significantly drops. In other words, the model uses some unrelated features during classification. In order to increase the robustness of the model, researchers proposed to train on amplified training data, and indeed model accuracy increased to previous levels.  In this paper, we replicate and continue this investigation, and provide an actionable model benchmarking methodology to help researchers better evaluate advances in machine learning for vulnerability detection. Specifically, we propose
&lt;/p&gt;</description></item><item><title>STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09381</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation. (arXiv:2306.09381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09381
&lt;/p&gt;
&lt;p&gt;
STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#22312;&#25919;&#31574;&#20915;&#31574;&#21644;&#32463;&#27982;&#34892;&#20026;&#30740;&#31350;&#20013;&#26377;&#30528;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#26088;&#22312;&#32473;&#23450;&#19968;&#23567;&#32452;&#36712;&#36857;&#25968;&#25454;&#29983;&#25104;&#20154;&#31867;&#31227;&#21160;&#36712;&#36857;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#22320;&#28857;&#20043;&#38388;&#30340;&#38745;&#24577;&#20851;&#31995;&#65292;&#32780;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;SpatioTemporal-Augmented gRaph&#31070;&#32463;&#32593;&#32476;&#65288;STAR&#65289;&#65292;&#26469;&#27169;&#25311;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal co
&lt;/p&gt;</description></item><item><title>&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.08141</link><description>&lt;p&gt;
ArtWhisperer&#65306;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#33402;&#26415;&#21019;&#20316;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08141
&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#29992;&#25143;&#22914;&#20309;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#29983;&#25104;&#25152;&#38656;&#30340;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ArtWhisperer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#29992;&#25143;&#20250;&#24471;&#21040;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#20197;&#20415;&#29983;&#25104;&#31867;&#20284;&#30446;&#26631;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20132;&#20114;&#30340;&#35760;&#24405;&#65307;&#27599;&#20010;&#20132;&#20114;&#37117;&#23545;&#24212;&#30528;&#29992;&#25143;&#21019;&#24314;&#30340;&#19968;&#20010;&#25552;&#31034;&#35789;&#21644;&#30456;&#24212;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#35760;&#24405;&#37117;&#26159;&#37325;&#22797;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#25552;&#31034;&#35789;&#20197;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#29420;&#29305;&#36830;&#32493;&#25968;&#25454;&#38598;&#12290;&#22312;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#25552;&#31034;&#35789;&#20132;&#20114;&#21644;&#29992;&#25143;&#31574;&#30053;&#30340;&#29305;&#24449;&#12290;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#30495;&#23454;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#25216;&#26415;&#21521;BO&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#25968;&#25454;&#28857;&#65292;&#37319;&#29992;&#26032;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#36229;&#21442;&#25968;&#20272;&#35745;&#65292;&#20197;&#36798;&#21040;&#27425;&#32447;&#24615;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.06844</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#20559;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#20272;&#35745;&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Bayesian Optimization with Unbiased Gaussian Process Hyperparameter Estimation. (arXiv:2306.06844v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06844
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#30495;&#23454;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#25216;&#26415;&#21521;BO&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#25968;&#25454;&#28857;&#65292;&#37319;&#29992;&#26032;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#36229;&#21442;&#25968;&#20272;&#35745;&#65292;&#20197;&#36798;&#21040;&#27425;&#32447;&#24615;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#21644;&#29702;&#35770;&#20445;&#35777;&#65292;&#21462;&#20915;&#20110;&#27491;&#30830;&#20272;&#35745;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#20540;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#24120;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#21487;&#33021;&#20250;&#24341;&#36215;&#25968;&#25454;&#20559;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268;&#36229;&#21442;&#25968;&#20272;&#35745;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#20107;&#20808;&#19981;&#30693;&#36947;&#30495;&#23454;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#24182;&#38656;&#35201;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#26102;&#65292;&#35813;&#26041;&#27861;&#20063;&#33021;&#22815;&#27425;&#32447;&#24615;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#25216;&#26415;(EXP3)&#21521;BO&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#25968;&#25454;&#28857;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#20272;&#35745;&#36807;&#31243;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process (GP) based Bayesian optimization (BO) is a powerful method for optimizing black-box functions efficiently. The practical performance and theoretical guarantees associated with this approach depend on having the correct GP hyperparameter values, which are usually unknown in advance and need to be estimated from the observed data. However, in practice, these estimations could be incorrect due to biased data sampling strategies commonly used in BO. This can lead to degraded performance and break the sub-linear global convergence guarantee of BO. To address this issue, we propose a new BO method that can sub-linearly converge to the global optimum of the objective function even when the true GP hyperparameters are unknown in advance and need to be estimated from the observed data. Our method uses a multi-armed bandit technique (EXP3) to add random data points to the BO process, and employs a novel training loss function for the GP hyperparameter estimation process that ens
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIBA&#30340;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#23384;&#22312;&#30340;&#21487;&#35265;&#25110;&#31232;&#30095;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06209</link><description>&lt;p&gt;
&#31232;&#30095;&#38544;&#24418;&#35302;&#21457;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack with Sparse and Invisible Trigger. (arXiv:2306.06209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIBA&#30340;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#23384;&#22312;&#30340;&#21487;&#35265;&#25110;&#31232;&#30095;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#22312;&#23567;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#24471;&#21463;&#23475;&#30340;&#27169;&#22411;&#23545;&#27491;&#24120;&#26679;&#26412;&#26377;&#27491;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#23558;&#24102;&#26377;&#35302;&#21457;&#22120;&#30340;&#26679;&#26412;&#24402;&#31867;&#20026;&#30446;&#26631;&#20998;&#31867;&#12290;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#20852;&#32780;&#21448;&#21361;&#38505;&#30340;&#35757;&#32451;&#38454;&#27573;&#23041;&#32961;&#65292;&#23545;DNN&#24212;&#29992;&#24102;&#26469;&#20005;&#37325;&#39118;&#38505;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#35265;&#25110;&#31232;&#30095;&#24615;&#19981;&#36275;&#65292;&#22240;&#27492;&#19981;&#22815;&#38544;&#31192;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#35774;&#35745;&#26377;&#25928;&#30340;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#26102;&#65292;&#19981;&#33021;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#26041;&#27861;&#32452;&#21512;&#36215;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#35302;&#21457;&#22120;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20855;&#26377;&#31232;&#30095;&#24615;&#21644;&#38544;&#31192;&#24615;&#32422;&#26463;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#65292;&#31216;&#20026;&#31232;&#30095;&#38544;&#24418;&#21518;&#38376;&#25915;&#20987;&#65288;SIBA&#65289;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to backdoor attacks, where the adversary manipulates a small portion of training data such that the victim model predicts normally on the benign samples but classifies the triggered samples as the target class. The backdoor attack is an emerging yet threatening training-phase threat, leading to serious risks in DNN-based applications. In this paper, we revisit the trigger patterns of existing backdoor attacks. We reveal that they are either visible or not sparse and therefore are not stealthy enough. More importantly, it is not feasible to simply combine existing methods to design an effective sparse and invisible backdoor attack. To address this problem, we formulate the trigger generation as a bi-level optimization problem with sparsity and invisibility constraints and propose an effective method to solve it. The proposed method is dubbed sparse and invisible backdoor attack (SIBA). We conduct extensive experiments on benchmark datasets unde
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.05001</link><description>&lt;p&gt;
COURIER: &#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#29305;&#24449;&#39044;&#35757;&#32451;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#36824;&#21407;
&lt;/p&gt;
&lt;p&gt;
COURIER: Contrastive User Intention Reconstruction for Large-Scale Pre-Train of Image Features. (arXiv:2306.05001v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#23186;&#20307;&#20114;&#32852;&#32593;&#30340;&#21457;&#23637;&#65292;&#35270;&#35273;&#29305;&#24449;&#24050;&#25104;&#20026;&#24433;&#21709;&#29992;&#25143;&#20852;&#36259;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#23558;&#35270;&#35273;&#29305;&#24449;&#34701;&#20837;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#26041;&#21521;&#26159;&#39047;&#20855;&#21069;&#26223;&#30340;&#12290;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#23558;&#24050;&#26377;&#39044;&#35757;&#32451;&#26041;&#27861;&#35757;&#32451;&#24471;&#21040;&#30340;&#22270;&#20687;&#23884;&#20837;&#27880;&#20837;&#21040;&#27169;&#22411;&#20013;&#20165;&#33021;&#20135;&#29983;&#36739;&#23567;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#24402;&#22240;&#20110;&#20197;&#19979;&#20004;&#20010;&#21407;&#22240;&#65306;&#39318;&#20808;&#65292;&#39044;&#35757;&#32451;&#26041;&#27861;&#26159;&#20026;&#20102;&#26126;&#30830;&#30340;&#12289;&#37325;&#28857;&#25918;&#22312;&#35821;&#20041;&#29305;&#24449;&#19978;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#65292;&#26080;&#27861;&#23398;&#20064;&#21040;&#20010;&#24615;&#21270;&#25512;&#33616;&#20013;&#30340;&#20010;&#20154;&#20852;&#36259;; &#20854;&#27425;&#65292;&#39044;&#35757;&#32451;&#30340;&#21482;&#21253;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#22270;&#20687;&#23884;&#20837;&#19982;&#25105;&#20204;&#22312;CTR&#39044;&#27979;&#20219;&#21153;&#20013;&#25152;&#24050;&#26377;&#30340;&#31867;&#21035;&#21644;&#29289;&#21697;&#26631;&#39064;&#31561;&#35821;&#20041;&#29305;&#24449;&#30456;&#27604;&#26377;&#36739;&#23567;&#30340;&#20449;&#24687;&#22686;&#30410;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#31181;&#38024;&#23545;&#25512;&#33616;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#20197;&#33719;&#24471;&#26356;&#22810;&#25913;&#36827;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of the multi-media internet, visual characteristics have become an important factor affecting user interests. Thus, incorporating visual features is a promising direction for further performance improvements in click-through rate (CTR) prediction. However, we found that simply injecting the image embeddings trained with established pre-training methods only has marginal improvements. We attribute the failure to two reasons: First, The pre-training methods are designed for well-defined computer vision tasks concentrating on semantic features, and they cannot learn personalized interest in recommendations. Secondly, pre-trained image embeddings only containing semantic information have little information gain, considering we already have semantic features such as categories and item titles as inputs in the CTR prediction task. We argue that a pre-training method tailored for recommendation is necessary for further improvements. To this end, we propose a recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;SGD&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#8220;&#25237;&#30707;&#26426;&#8221;&#20248;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#22686;&#21152;&#19982;&#30495;&#23454;&#39044;&#27979;&#22120;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#23545;&#40784;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#35777;&#26126;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#21487;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04815</link><description>&lt;p&gt;
SGD&#20013;&#30340;&#25237;&#30707;&#26426;&#65306;&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#21450;&#20854;&#36890;&#36807;&#29305;&#24449;&#23398;&#20064;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning. (arXiv:2306.04815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;SGD&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#8220;&#25237;&#30707;&#26426;&#8221;&#20248;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#22686;&#21152;&#19982;&#30495;&#23454;&#39044;&#27979;&#22120;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#23545;&#40784;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#24182;&#35777;&#26126;&#36739;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#21487;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36827;&#34892;&#35757;&#32451;&#26102;&#20026;&#20160;&#20040;&#32463;&#24120;&#20986;&#29616;&#35757;&#32451;&#25439;&#22833;&#23574;&#23792;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;SGD&#35757;&#32451;&#25439;&#22833;&#20013;&#30340;&#23574;&#23792;&#26159;&#8220;&#25237;&#30707;&#26426;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#20248;&#21270;&#29616;&#35937;&#65292;&#26368;&#21021;&#22312;[Lewkowycz&#31561;&#20154;&#65292;2020&#24180;]&#30340;&#22823;&#23398;&#20064;&#29575;GD&#20013;&#35266;&#23519;&#21040;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25237;&#30707;&#26426;&#20986;&#29616;&#22312;&#30001;&#27491;&#20999;&#20869;&#26680;&#30340;&#21069;&#20960;&#20010;&#29305;&#24449;&#21521;&#37327;&#25152;&#24352;&#25104;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#36866;&#29992;&#20110;GD&#21644;SGD&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#65292;&#21363;&#25237;&#30707;&#26426;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#19982;&#30495;&#23454;&#39044;&#27979;&#22120;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#65288;AGOP&#65289;&#23545;&#40784;&#26469;&#20419;&#36827;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;SGD&#20013;&#65292;&#26356;&#23567;&#30340;&#25209;&#37327;&#22823;&#23567;&#20250;&#23548;&#33268;&#26356;&#22810;&#30340;&#25237;&#30707;&#26426;&#20986;&#29616;&#65292;&#20174;&#32780;&#25552;&#39640;AGOP&#23545;&#40784;&#21644;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we first present an explanation regarding the common occurrence of spikes in the training loss when neural networks are trained with stochastic gradient descent (SGD). We provide evidence that the spikes in the training loss of SGD are "catapults", an optimization phenomenon originally observed in GD with large learning rates in [Lewkowycz et al. 2020]. We empirically show that these catapults occur in a low-dimensional subspace spanned by the top eigenvectors of the tangent kernel, for both GD and SGD. Second, we posit an explanation for how catapults lead to better generalization by demonstrating that catapults promote feature learning by increasing alignment with the Average Gradient Outer Product (AGOP) of the true predictor. Furthermore, we demonstrate that a smaller batch size in SGD induces a larger number of catapults, thereby improving AGOP alignment and test performance.
&lt;/p&gt;</description></item><item><title>C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.16209</link><description>&lt;p&gt;
C-MCTS: &#23433;&#20840;&#35268;&#21010;&#19982;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
C-MCTS: Safe Planning with Monte Carlo Tree Search. (arXiv:2305.16209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16209
&lt;/p&gt;
&lt;p&gt;
C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#21487;&#20197;&#35299;&#20915;&#21463;&#32422;&#26463;&#30340;&#23433;&#20840;&#20915;&#31574;&#38382;&#39064;&#12290;&#23613;&#31649;CMDP&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;MCTS&#31561;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;CMDP&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#22312;&#25104;&#26412;&#26041;&#38754;&#20445;&#23432;&#34892;&#20107;&#65292;&#36890;&#36807;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#25104;&#26412;&#20272;&#35745;&#26469;&#36991;&#20813;&#36829;&#21453;&#32422;&#26463;&#65292;&#20294;&#36825;&#31181;&#20272;&#35745;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;MCTS&#65288;C-MCTS&#65289;&#65292;&#23427;&#20351;&#29992;&#20808;&#21069;&#22312;&#20195;&#29702;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#35757;&#32451;&#30340;&#23433;&#20840;&#35780;&#21028;&#22120;&#26469;&#20272;&#35745;&#25104;&#26412;&#12290;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#35780;&#21028;&#22120;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#12290;C-MCTS&#28385;&#36275;&#25104;&#26412;&#32422;&#26463;&#65292;&#20294;&#25805;&#20316;&#25509;&#36817;&#32422;&#26463;&#36793;&#30028;&#65292;&#27604;&#20197;&#24448;&#30340;&#24037;&#20316;&#33719;&#24471;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;&#20316;&#20026;&#19968;&#20010;&#24456;&#22909;&#30340;&#21103;&#20135;&#21697;&#65292;&#36825;&#20010;&#35268;&#21010;&#22120;&#22312;&#35268;&#21010;&#27493;&#39588;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#27169;&#22411;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
The Constrained Markov Decision Process (CMDP) formulation allows to solve safety-critical decision making tasks that are subject to constraints. While CMDPs have been extensively studied in the Reinforcement Learning literature, little attention has been given to sampling-based planning algorithms such as MCTS for solving them. Previous approaches perform conservatively with respect to costs as they avoid constraint violations by using Monte Carlo cost estimates that suffer from high variance. We propose Constrained MCTS (C-MCTS), which estimates cost using a safety critic that is trained with Temporal Difference learning in an offline phase prior to agent deployment. The critic limits exploration by pruning unsafe trajectories within MCTS during deployment. C-MCTS satisfies cost constraints but operates closer to the constraint boundary, achieving higher rewards than previous work. As a nice byproduct, the planner is more efficient w.r.t. planning steps. Most importantly, under model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;KL&#25955;&#24230;&#38477;&#20302;&#30340;&#26032;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#26679;&#26412;&#36827;&#34892;&#35745;&#31639;&#32780;&#19981;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#65292;&#20855;&#26377;&#27604;SVGD&#26356;&#31616;&#21333;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#23545;&#20110;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#20063;&#26377;&#20248;&#21270;&#65292;&#25552;&#21319;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.15577</link><description>&lt;p&gt;
&#37319;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#30340;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Variational Gradient Descent using Local Linear Models. (arXiv:2305.15577v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#23454;&#29616;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;KL&#25955;&#24230;&#38477;&#20302;&#30340;&#26032;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#20351;&#29992;&#26679;&#26412;&#36827;&#34892;&#35745;&#31639;&#32780;&#19981;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#65292;&#20855;&#26377;&#27604;SVGD&#26356;&#31616;&#21333;&#26377;&#25928;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#23545;&#20110;&#39640;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#20063;&#26377;&#20248;&#21270;&#65292;&#25552;&#21319;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) &#33021;&#22815;&#27839;&#30528;&#36712;&#36857;&#20256;&#36755;&#31890;&#23376;&#65292;&#20174;&#32780;&#20943;&#23569;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#20294;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#26469;&#35745;&#31639;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SVGD&#35270;&#35282;&#65292;&#23558;&#20854;&#35270;&#20026;&#21453;&#21521;KL&#26799;&#24230;&#27969;&#30340;&#23616;&#37096;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#35270;&#35282;&#21551;&#21457;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#26469;&#23454;&#29616;&#30456;&#21516;&#30446;&#30340;&#30340;&#26032;&#20272;&#35745;&#22120;&#12290;&#36825;&#20123;&#25552;&#35758;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#20165;&#20351;&#29992;&#30446;&#26631;&#21644;&#31890;&#23376;&#20998;&#24067;&#30340;&#26679;&#26412;&#36827;&#34892;&#35745;&#31639;&#65292;&#32780;&#19981;&#38656;&#35201;&#30446;&#26631;&#24471;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#35758;&#30340;&#21464;&#20998;&#26799;&#24230;&#20272;&#35745;&#22120;&#21033;&#29992;&#20102;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20272;&#35745;&#20559;&#24046;&#19982;SVGD&#30456;&#24403;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#20855;&#26377;&#35745;&#31639;&#31616;&#20415;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#39640;&#32500;&#26799;&#24230;&#27969;&#30340;&#20272;&#35745;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#20010;&#20302;&#32500;&#20272;&#35745;&#38382;&#39064;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#22909;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;&#25105;&#20204;&#23545;&#25552;&#35758;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) can transport particles along trajectories that reduce the KL divergence between the target and particle distribution but requires the target score function to compute the update. We introduce a new perspective on SVGD that views it as a local estimator of the reversed KL gradient flow. This perspective inspires us to propose new estimators that use local linear models to achieve the same purpose. The proposed estimators can be computed using only samples from the target and particle distribution without needing the target score function. Our proposed variational gradient estimators utilize local linear models, resulting in computational simplicity while maintaining effectiveness comparable to SVGD in terms of estimation biases. Additionally, we demonstrate that under a mild assumption, the estimation of high-dimensional gradient flow can be translated into a lower-dimensional estimation problem, leading to improved estimation accuracy. We vali
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#35789;&#20856;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.14592</link><description>&lt;p&gt;
&#24102;&#35789;&#20856;&#30340;&#25351;&#20196;&#20248;&#21270;&#29992;&#20110;&#38646;&#26679;&#24335;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Lexicons for Zero-Shot Style Classification. (arXiv:2305.14592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14592
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35789;&#20856;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#26684;&#29992;&#20110;&#20256;&#36798;&#20316;&#32773;&#30340;&#24847;&#22270;&#21644;&#24577;&#24230;&#12290;&#23613;&#31649;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#39118;&#26684;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20808;&#21069;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;&#24102;&#26631;&#31614;&#30340;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#12290;&#21551;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23545;&#39118;&#26684;&#36827;&#34892;&#20998;&#31867;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35821;&#35328;&#39118;&#26684;&#21487;&#33021;&#24456;&#38590;&#23450;&#20041;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#39118;&#26684;&#35789;&#20856;&#20316;&#20026;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#35782;&#21035;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#39118;&#26684;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#35789;&#20856;&#30340;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36716;&#31227;&#12290;&#25105;&#20204;&#23558;&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Style is used to convey authors' intentions and attitudes. Despite the success of large pre-trained language models on style classification, prior work relies on fine-tuning with labeled examples. Prompting large language models to classify style without fine-tuning is challenging because language styles can be difficult to define. In this study, we investigate the effectiveness of style lexicons as a means for instructing language models how to identify new styles that are unseen during training. Our experiments show that lexicon-based instructions improve transfer zero-shot performance significantly. We will release our code and data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{R}$-&#20809;&#28369;Banach&#31354;&#38388;&#20013;&#25903;&#25345;PINNs&#35823;&#24046;&#20272;&#35745;&#30340;&#38750;&#32447;&#24615;&#26041;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#38480;&#21046;&#27531;&#24046;&#30340;Bramble-Hilbert&#24341;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.11915</link><description>&lt;p&gt;
&#22312;$\mathbb{R}$-&#20809;&#28369;Banach&#31354;&#38388;&#20013;&#65292;PINNs&#35823;&#24046;&#20272;&#35745;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
PINNs error estimates for nonlinear equations in $\mathbb{R}$-smooth Banach spaces. (arXiv:2305.11915v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\mathbb{R}$-&#20809;&#28369;Banach&#31354;&#38388;&#20013;&#25903;&#25345;PINNs&#35823;&#24046;&#20272;&#35745;&#30340;&#38750;&#32447;&#24615;&#26041;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#29992;&#20110;&#38480;&#21046;&#27531;&#24046;&#30340;Bramble-Hilbert&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#31639;&#23376;&#24418;&#24335;&#25551;&#36848;&#20102;&#19968;&#31867;&#25903;&#25345;PINN&#35823;&#24046;&#20272;&#35745;&#30340;PDE&#65292;&#24182;&#19988;&#23545;&#20110;$L^p$&#31354;&#38388;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;Bramble-Hilbert&#24341;&#29702;&#65292;&#20316;&#20026;&#19982;PINN&#27531;&#24046;&#36793;&#30028;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paper, we describe in operator form classes of PDEs that admit PINN's error estimation. Also, for $L^p$ spaces, we obtain a Bramble-Hilbert type lemma that is a tool for PINN's residuals bounding.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;&#30340;&#26032;&#39062;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#31561;&#21516;&#20110;&#22522;&#20110;&#20869;&#26680;&#23725;&#22238;&#24402;&#30340;&#21333;&#20010;&#27424;&#24179;&#28369;&#23725;&#22238;&#24402;&#65292;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#29305;&#23450;&#30340;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#36873;&#25321;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.14545</link><description>&lt;p&gt;
&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;&#20316;&#20026;&#32447;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Augmented balancing weights as linear regression. (arXiv:2304.14545v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;&#30340;&#26032;&#39062;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#31561;&#21516;&#20110;&#22522;&#20110;&#20869;&#26680;&#23725;&#22238;&#24402;&#30340;&#21333;&#20010;&#27424;&#24179;&#28369;&#23725;&#22238;&#24402;&#65292;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#29305;&#23450;&#30340;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#36873;&#25321;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20110;&#33258;&#21160;&#21435;&#20559;&#37325;&#37325;&#37197;(AutoDML)&#30340;&#26032;&#39062;&#29305;&#24449;&#25551;&#36848;&#12290;&#36825;&#20123;&#20272;&#31639;&#22120;&#23558;&#32467;&#26524;&#24314;&#27169;&#19982;&#37325;&#37197;&#30456;&#32467;&#21512;&#65292;&#30452;&#25509;&#20272;&#35745;&#21453;&#21521;&#20542;&#21521;&#31215;&#20998;&#26435;&#37325;&#12290;&#24403;&#32467;&#26524;&#19982;&#26435;&#37325;&#27169;&#22411;&#37117;&#26159;&#26576;&#20123;&#65288;&#21487;&#33021;&#26159;&#26080;&#38480;&#30340;&#65289;&#22522;&#30784;&#20013;&#30340;&#32447;&#24615;&#26102;&#65292;&#25105;&#20204;&#34920;&#26126;&#22686;&#24378;&#30340;&#20272;&#31639;&#22120;&#31561;&#21516;&#20110;&#20855;&#26377;&#23558;&#21407;&#22987;&#32467;&#26524;&#27169;&#22411;&#31995;&#25968;&#21644;OLS&#30456;&#32467;&#21512;&#30340;&#31995;&#25968;&#30340;&#21333;&#20010;&#32447;&#24615;&#27169;&#22411;&#65307;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#65292;&#22686;&#24378;&#20272;&#31639;&#22120;&#21512;&#24182;&#20026;&#20165;&#20351;&#29992;OLS. &#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#29305;&#23450;&#30340;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#36873;&#25321;&#19978;&#12290;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#65292;&#20351;&#29992;(&#20869;&#26680;)&#23725;&#22238;&#24402;&#20316;&#20026;&#32467;&#26524;&#21644;&#37325;&#37197;&#27169;&#22411;&#30340;&#32852;&#21512;&#20272;&#31639;&#22120;&#31561;&#21516;&#20110;&#21333;&#20010;&#12289;&#27424;&#24179;&#28369;(&#20869;&#26680;)&#23725;&#22238;&#24402;&#65307;&#24403;&#32771;&#34385;&#21040;&#28176;&#36817;&#36895;&#29575;&#26102;&#65292;&#36825;&#19968;&#32467;&#26524;&#20063;&#25104;&#31435;&#12290;&#24403;&#20195;&#26367;&#26435;&#37325;&#27169;&#22411;&#20026;&#22871;&#32034;&#22238;&#24402;&#26102;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#29305;&#27530;&#24773;&#20917;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#24182;&#19988;&#28436;&#31034;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
We provide a novel characterization of augmented balancing weights, also known as Automatic Debiased Machine Learning (AutoDML). These estimators combine outcome modeling with balancing weights, which estimate inverse propensity score weights directly. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the original outcome model coefficients and OLS; in many settings, the augmented estimator collapses to OLS alone. We then extend these results to specific choices of outcome and weighting models. We first show that the combined estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression; this also holds when considering asymptotic rates. When the weighting model is instead lasso regression, we give closed-form expressions for special cases and demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#20165;&#20855;&#26377;&#23545;&#27491;&#24120;&#26680;&#24515;&#30340;&#29983;&#25104;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#26102;&#65292;&#33719;&#24471;&#949;-&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;sa&#65288;s-&#65289;&#30697;&#24418;&#19981;&#30830;&#23450;&#38598;&#21512;&#65292;&#24050;&#30693;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|&#65289;&#65288;&#21709;&#24212;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|^2&#65289;&#65289;&#65292;&#23545;&#20110;&#29305;&#23450;&#31639;&#27861;&#21644;&#22522;&#20110;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#12289;KL&#25110;&#21345;&#26041;&#25955;&#24230;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2302.05372</link><description>&lt;p&gt;
&#36808;&#21521;&#27169;&#22411;&#22522;&#30784;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Minimax Optimality of Model-based Robust Reinforcement Learning. (arXiv:2302.05372v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#20165;&#20855;&#26377;&#23545;&#27491;&#24120;&#26680;&#24515;&#30340;&#29983;&#25104;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#26102;&#65292;&#33719;&#24471;&#949;-&#26368;&#20248;&#31574;&#30053;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;sa&#65288;s-&#65289;&#30697;&#24418;&#19981;&#30830;&#23450;&#38598;&#21512;&#65292;&#24050;&#30693;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|&#65289;&#65288;&#21709;&#24212;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|^2&#65289;&#65289;&#65292;&#23545;&#20110;&#29305;&#23450;&#31639;&#27861;&#21644;&#22522;&#20110;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#12289;KL&#25110;&#21345;&#26041;&#25955;&#24230;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21482;&#26377;&#23545;&#27491;&#24120;&#26680;&#24515;&#30340;&#29983;&#25104;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#26102;&#65292;&#33719;&#24471;&#949;-&#26368;&#20248;&#31574;&#30053;&#30340;&#37319;&#26679;&#22797;&#26434;&#24230;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38750;&#40065;&#26834;&#24773;&#20917;&#19979;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#19988;&#24050;&#30693;&#20219;&#20309;&#24212;&#29992;&#20110;&#32463;&#39564;MDP&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#29992;&#949;^2/&#65288;H^3 * |S| * |A|&#65289;&#20010;&#26679;&#26412;&#26469;&#20272;&#35745;&#65292;&#22343;&#21487;&#25552;&#20379;&#949;-&#26368;&#20248;&#31574;&#30053;&#65292;&#20174;&#32780;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#12290;&#40065;&#26834;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#26356;&#21152;&#23569;&#35265;&#12290;&#23545;&#20110;sa&#65288;s-&#65289;&#30697;&#24418;&#19981;&#30830;&#23450;&#38598;&#21512;&#65292;&#24050;&#30693;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|&#65289;&#65288;&#21709;&#24212;&#20026;&#949;^2/&#65288;H^4 * |S|^2 * |A|^2&#65289;&#65289;&#65292;&#23545;&#20110;&#29305;&#23450;&#31639;&#27861;&#21644;&#22522;&#20110;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#12289;KL&#25110;&#21345;&#26041;&#25955;&#24230;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#29992;Lp&#29699;&#23450;&#20041;&#30340;&#19981;&#30830;&#23450;&#38598;&#21512;&#65288;&#22238;&#22797;&#21040;TV&#24773;&#20917;&#65289;&#65292;&#24182;&#19988;...
&lt;/p&gt;
&lt;p&gt;
We study the sample complexity of obtaining an $\epsilon$-optimal policy in \emph{Robust} discounted Markov Decision Processes (RMDPs), given only access to a generative model of the nominal kernel. This problem is widely studied in the non-robust case, and it is known that any planning approach applied to an empirical MDP estimated with $\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A \mid}{\epsilon^2})$ samples provides an $\epsilon$-optimal policy, which is minimax optimal. Results in the robust case are much more scarce. For $sa$(resp $s$-)rectangular uncertainty sets, the best known sample complexity is $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid}{\epsilon^2})$ (resp. $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid^2}{\epsilon^2})$), for specific algorithms and when the uncertainty set is based on the total variation (TV), the KL or the Chi-square divergences. In this paper, we consider uncertainty sets defined with an $L_p$-ball (recovering the TV case), and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#26694;&#26550;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.08146</link><description>&lt;p&gt;
&#20320;&#25152;&#22312;&#31038;&#21306;&#21457;&#29983;&#20102;&#20160;&#20040;&#65311;&#19968;&#31181;&#24369;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#21457;&#29616;&#26412;&#22320;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News. (arXiv:2301.08146v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#26041;&#27861;&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#26694;&#26550;&#21644;&#33258;&#21160;&#21270;&#25968;&#25454;&#22788;&#29702;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35206;&#30422;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#22320;&#26032;&#38395;&#26159;&#24433;&#21709;&#29305;&#23450;&#22320;&#29702;&#21306;&#22495;&#65288;&#22914;&#22478;&#24066;&#12289;&#21439;&#21644;&#24030;&#65289;&#29992;&#25143;&#30340;&#26032;&#38395;&#23376;&#38598;&#12290;&#26816;&#27979;&#26412;&#22320;&#26032;&#38395;&#26159;&#20934;&#30830;&#22320;&#25512;&#33616;&#26412;&#22320;&#26032;&#38395;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22522;&#20110;&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38598;&#25104;&#21270;&#30340;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#26412;&#22320;&#26032;&#38395;&#26816;&#27979;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#26412;&#22320;&#26032;&#38395;&#25512;&#33616;&#12290;&#26412;&#25991;&#30528;&#37325;&#20171;&#32461;&#20102;&#31649;&#36947;&#30340;&#31532;&#19968;&#27493;&#39588;&#65306;&#65288;1&#65289;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;&#33258;&#21160;&#25968;&#25454;&#22788;&#29702;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#65288;2&#65289;&#21487;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#35774;&#32622;&#12290;&#19982;&#26031;&#22374;&#31119;CoreNLP NER&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#22312;&#32463;&#36807;&#30495;&#23454;&#19990;&#30028;&#21644;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#35780;&#20272;&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local news articles are a subset of news that impact users in a geographical area, such as a city, county, or state. Detecting local news (Step 1) and subsequently deciding its geographical location as well as radius of impact (Step 2) are two important steps towards accurate local news recommendation. Naive rule-based methods, such as detecting city names from the news title, tend to give erroneous results due to lack of understanding of the news content. Empowered by the latest development in natural language processing, we develop an integrated pipeline that enables automatic local news detection and content-based local news recommendations. In this paper, we focus on Step 1 of the pipeline, which highlights: (1) a weakly supervised framework incorporated with domain knowledge and auto data processing, and (2) scalability to multi-lingual settings. Compared with Stanford CoreNLP NER model, our pipeline has higher precision and recall evaluated on a real-world and human-labeled datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#35302;&#21457;&#31639;&#27861;ET-GP-UCB&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#27010;&#29575;&#22343;&#21248;&#35823;&#24046;&#30028;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#26410;&#30693;&#21464;&#21270;&#36895;&#29575;&#30340;&#24773;&#20917;&#19979;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#23454;&#38469;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ET-GP-UCB&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.10790</link><description>&lt;p&gt;
&#20107;&#20214;&#35302;&#21457;&#30340;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Event-Triggered Time-Varying Bayesian Optimization. (arXiv:2208.10790v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#35302;&#21457;&#31639;&#27861;ET-GP-UCB&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#27010;&#29575;&#22343;&#21248;&#35823;&#24046;&#30028;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#26410;&#30693;&#21464;&#21270;&#36895;&#29575;&#30340;&#24773;&#20917;&#19979;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#23454;&#38469;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ET-GP-UCB&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;TVBO&#65289;&#39034;&#24207;&#20248;&#21270;&#26102;&#21464;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#26102;&#38388;&#21464;&#21270;&#19979;&#30340;&#21208;&#25506;&#19982;&#24320;&#21457;&#30340;&#26435;&#34913;&#12290;&#24403;&#21069;&#30340;TVBO&#26041;&#27861;&#38656;&#35201;&#23545;&#21464;&#21270;&#36895;&#29575;&#26377;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21464;&#21270;&#36895;&#29575;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#35302;&#21457;&#31639;&#27861;ET-GP-UCB&#65292;&#23427;&#23558;&#20248;&#21270;&#38382;&#39064;&#35270;&#20026;&#38745;&#24577;&#38382;&#39064;&#65292;&#30452;&#21040;&#22312;&#32447;&#26816;&#27979;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#21464;&#21270;&#24182;&#37325;&#32622;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#36866;&#24212;&#23454;&#38469;&#30340;&#26102;&#38388;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#20107;&#20214;&#35302;&#21457;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#20351;&#29992;&#30340;&#27010;&#29575;&#22343;&#21248;&#35823;&#24046;&#30028;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;ET-GP-UCB&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;ET-GP-UCB&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#35774;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sequentially optimizing a time-varying objective function using time-varying Bayesian optimization (TVBO). Here, the key challenge is the exploration-exploitation trade-off under time variations. Current approaches to TVBO require prior knowledge of a constant rate of change. However, in practice, the rate of change is usually unknown. We propose an event-triggered algorithm, ET-GP-UCB, that treats the optimization problem as static until it detects changes in the objective function online and then resets the dataset. This allows the algorithm to adapt to realized temporal changes without the need for prior knowledge. The event-trigger is based on probabilistic uniform error bounds used in Gaussian process regression. We provide regret bounds for ET-GP-UCB and show in numerical experiments that it outperforms state-of-the-art algorithms on synthetic and real-world data. Furthermore, these results demonstrate that ET-GP-UCB is readily applicable to various set
&lt;/p&gt;</description></item></channel></rss>