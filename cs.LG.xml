<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2404.01299</link><description>&lt;p&gt;
CausalChaos!&#25968;&#25454;&#38598;&#65306;&#22522;&#20110;&#21160;&#24577;&#35270;&#35273;&#22330;&#26223;&#20013;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#20840;&#38754;&#22240;&#26524;&#34892;&#21160;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01299
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21345;&#36890;&#22270;&#20687;&#26500;&#24314;&#30340;CausalChaos!&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26356;&#38271;&#22240;&#26524;&#38142;&#30340;&#22240;&#26524;&#38382;&#31572;&#65292;&#36890;&#36807;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#23637;&#31034;&#25361;&#25112;&#24615;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#22810;&#20855;&#25361;&#25112;&#24615;&#19988;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#28982;&#32780;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#22240;&#26524;&#25512;&#29702;&#20998;&#26512;&#26041;&#38754;&#24448;&#24448;&#32570;&#20047;&#28145;&#24230;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21033;&#29992;&#21345;&#36890;&#30340;&#29420;&#29305;&#23646;&#24615;&#26500;&#24314;&#20102;CausalChaos!&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22240;&#26524;&#38382;&#31572;&#65288;Why-QA&#65289;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#26631;&#24535;&#24615;&#30340;&#8220;&#29483;&#21644;&#32769;&#40736;&#8221;&#21345;&#36890;&#31995;&#21015;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21608;&#21040;&#30340;&#38382;&#39064;&#21644;&#22810;&#23618;&#27425;&#31572;&#26696;&#65292;&#21253;&#21547;&#30528;&#23884;&#20837;&#21160;&#24577;&#20114;&#21160;&#21644;&#35270;&#35273;&#20013;&#30340;&#26356;&#38271;&#22240;&#26524;&#38142;&#65292;&#21516;&#26102;&#21160;&#30011;&#21407;&#29702;&#20801;&#35768;&#21160;&#30011;&#24072;&#21019;&#36896;&#23450;&#20041;&#26126;&#30830;&#12289;&#26126;&#20102;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#20123;&#22240;&#32032;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#30828;&#36127;&#37319;&#26679;&#65292;&#21253;&#25324;CausalConfusion&#29256;&#26412;&#12290;&#34429;&#28982;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#24335;&#31572;&#26696;&#26041;&#38754;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20026;&#20808;&#36827;/&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#24314;&#27169;&#21644;&#32852;&#21512;&#24314;&#27169;&#31561;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01299v1 Announce Type: cross  Abstract: Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning analysis. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic "Tom and Jerry" cartoon series. With thoughtful questions and multi-level answers, our dataset contains much longer causal chains embedded in dynamic interactions and visuals, at the same time principles of animation allows animators to create well-defined, unambiguous causal relationships. These factors allow models to solve more challenging, yet well-defined causal relationships. We also introduce hard negative mining, including CausalConfusion version. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling and joint modeling of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23433;&#20840;&#32858;&#21512;&#22312;&#38544;&#31169;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#38754;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26102;&#24182;&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#31169;&#23494;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17775</link><description>&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#22312;&#38754;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26102;&#24182;&#38750;&#31169;&#23494;&#30340;
&lt;/p&gt;
&lt;p&gt;
Secure Aggregation is Not Private Against Membership Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23433;&#20840;&#32858;&#21512;&#22312;&#38544;&#31169;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#38754;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26102;&#24182;&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#31169;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#65288;SecAgg&#65289;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#38544;&#31169;&#22686;&#24378;&#26426;&#21046;&#65292;&#20165;&#20801;&#35768;&#26381;&#21153;&#22120;&#35775;&#38382;&#27169;&#22411;&#26356;&#26032;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#26356;&#26032;&#30340;&#26426;&#23494;&#24615;&#12290;&#23613;&#31649;&#26377;&#20851;SecAgg&#20445;&#25252;&#38544;&#31169;&#33021;&#21147;&#30340;&#24191;&#27867;&#22768;&#26126;&#65292;&#20294;&#32570;&#20047;&#23545;&#20854;&#38544;&#31169;&#24615;&#30340;&#27491;&#24335;&#20998;&#26512;&#65292;&#22240;&#27492;&#36825;&#20123;&#20551;&#35774;&#26159;&#19981;&#21512;&#29702;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;SecAgg&#35270;&#20026;&#27599;&#20010;&#23616;&#37096;&#26356;&#26032;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#26426;&#21046;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;SecAgg&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#25915;&#20987;&#26041;&#24335;&#65292;&#20854;&#20013;&#23545;&#25163;&#26381;&#21153;&#22120;&#35797;&#22270;&#22312;SecAgg&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#19968;&#35757;&#32451;&#36718;&#20013;&#25512;&#26029;&#20986;&#23458;&#25143;&#31471;&#25552;&#20132;&#30340;&#26356;&#26032;&#21521;&#37327;&#26159;&#20004;&#20010;&#21487;&#33021;&#21521;&#37327;&#20013;&#30340;&#21738;&#19968;&#20010;&#12290;&#36890;&#36807;&#36827;&#34892;&#38544;&#31169;&#23457;&#26680;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#25915;&#20987;&#30340;&#25104;&#21151;&#27010;&#29575;&#65292;&#24182;&#37327;&#21270;&#20102;SecAgg&#25552;&#20379;&#30340;LDP&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#25968;&#23383;&#32467;&#26524;&#25581;&#31034;&#20102;&#65292;&#19982;&#26222;&#36941;&#22768;&#26126;&#30456;&#21453;&#65292;SecAgg&#24182;&#27809;&#26377;&#25552;&#20379;&#31169;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17775v1 Announce Type: new  Abstract: Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates. Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified. In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update. We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg. By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg. Our numerical results unveil that, contrary to prevailing claims, SecAgg offer
&lt;/p&gt;</description></item><item><title>VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16973</link><description>&lt;p&gt;
VoiceCraft&#65306;&#37326;&#22806;&#38646;-shot&#35821;&#38899;&#32534;&#36753;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16973
&lt;/p&gt;
&lt;p&gt;
VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;VoiceCraft&#65292;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#22768;&#20070;&#12289;&#20114;&#32852;&#32593;&#35270;&#39057;&#21644;&#25773;&#23458;&#19978;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;VoiceCraft&#37319;&#29992;Transformer&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#35760;&#37325;&#25490;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#21644;&#24310;&#36831;&#22534;&#21472;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#26377;&#24207;&#21015;&#20869;&#30340;&#29983;&#25104;&#12290;&#22312;&#35821;&#38899;&#32534;&#36753;&#20219;&#21153;&#19978;&#65292;VoiceCraft&#29983;&#25104;&#30340;&#32534;&#36753;&#35821;&#38899;&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#20960;&#20046;&#19982;&#26410;&#32534;&#36753;&#30340;&#24405;&#38899;&#38590;&#20197;&#21306;&#20998;&#65292;&#32463;&#20154;&#31867;&#35780;&#20272;&#65307;&#23545;&#20110;&#38646;-shot TTS&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21253;&#25324;VALLE&#21644;&#27969;&#34892;&#30340;&#21830;&#19994;&#27169;&#22411;XTTS-v2&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#26679;&#21475;&#38899;&#12289;&#35821;&#38899;&#39118;&#26684;&#12289;&#24405;&#21046;&#26465;&#20214;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#38899;&#20048;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30495;&#23454;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#22987;&#32456;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16973v1 Announce Type: cross  Abstract: We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07362</link><description>&lt;p&gt;
&#25361;&#25112;&#36951;&#24536;&#65306;&#25581;&#31034;&#26426;&#22120;&#36951;&#24536;&#20013;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;
&lt;/p&gt;
&lt;p&gt;
Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#31038;&#21306;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21518;&#26377;&#36873;&#25321;&#24615;&#22320;&#8220;&#36951;&#24536;&#8221;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#24341;&#20986;&#20102;&#26426;&#22120;&#36951;&#24536;(Machine Unlearning, MU)&#38382;&#39064;&#65292;&#26088;&#22312;&#28040;&#38500;&#36873;&#23450;&#25968;&#25454;&#28857;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#27169;&#22411;&#22312;&#36951;&#24536;&#21518;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;MU&#26041;&#27861;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#65292;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#38543;&#26426;&#25968;&#25454;&#36951;&#24536;&#19978;&#65292;&#24573;&#35270;&#20102;&#23545;&#20110;&#30495;&#23454;&#34913;&#37327;&#36951;&#24536;&#24615;&#33021;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#37325;&#35201;&#25506;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MU&#35780;&#20272;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#30830;&#23450;&#37027;&#20123;&#23545;&#24433;&#21709;&#25830;&#38500;&#26500;&#25104;&#26368;&#22823;&#25361;&#25112;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#25214;&#20986;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#12290;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#21407;&#21017;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22312;&#19978;&#23618;&#20248;&#21270;&#20013;&#30340;&#36951;&#24536;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06064</link><description>&lt;p&gt;
L$^2$GC: &#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#26694;&#26550;&#65292;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#30340;&#26641;&#29366;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#33410;&#28857;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#29992;&#20110;&#23545;&#22270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32447;&#24615;GCN&#27169;&#22411;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#25191;&#34892;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#65292;&#36825;&#24182;&#27809;&#26377;&#26126;&#30830;&#25429;&#25417;&#21040;&#20316;&#20026;&#22270;&#27169;&#22411;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#31867;&#20284;&#26641;&#29366;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#26412;&#25991;&#23581;&#35797;&#23558;&#21452;&#26354;&#31354;&#38388;&#24341;&#20837;&#32447;&#24615;GCN&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27931;&#20262;&#20857;&#32447;&#24615;GCN&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22270;&#33410;&#28857;&#30340;&#23398;&#20064;&#29305;&#24449;&#26144;&#23556;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#36827;&#34892;&#27931;&#20262;&#20857;&#32447;&#24615;&#29305;&#24449;&#21464;&#25442;&#65292;&#20197;&#25429;&#33719;&#25968;&#25454;&#30340;&#28508;&#22312;&#26641;&#29366;&#32467;&#26500;&#12290;&#22312;&#26631;&#20934;&#24341;&#25991;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Citeseer&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;74.7%&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#22312;PubMed&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;81.3%&#30340;&#20934;&#30830;&#24230;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35757;&#32451;&#33267;&#23569;&#36798;&#21040;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06064v1 Announce Type: cross  Abstract: Linear Graph Convolutional Networks (GCNs) are used to classify the node in the graph data. However, we note that most existing linear GCN models perform neural network operations in Euclidean space, which do not explicitly capture the tree-like hierarchical structure exhibited in real-world datasets that modeled as graphs. In this paper, we attempt to introduce hyperbolic space into linear GCN and propose a novel framework for Lorentzian linear GCN. Specifically, we map the learned features of graph nodes into hyperbolic space, and then perform a Lorentzian linear feature transformation to capture the underlying tree-like structure of data. Experimental results on standard citation networks datasets with semi-supervised learning show that our approach yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and 81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be trained up to two orders of magnitu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26550;&#26500;&#29992;&#20110;&#20248;&#21270;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#35745;&#31639;&#32422;&#26463;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#31649;&#29702;&#25928;&#29575;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#65292;&#20419;&#36827;&#20102;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04546</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#30340;&#26550;&#26500;&#34013;&#22270;
&lt;/p&gt;
&lt;p&gt;
Architectural Blueprint For Heterogeneity-Resilient Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04546
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26550;&#26500;&#29992;&#20110;&#20248;&#21270;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#35745;&#31639;&#32422;&#26463;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#31649;&#29702;&#25928;&#29575;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#24320;&#38144;&#65292;&#20419;&#36827;&#20102;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#23618;&#26550;&#26500;&#29992;&#20110;&#20248;&#21270;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#35813;&#26550;&#26500;&#35299;&#20915;&#20102;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#35745;&#31639;&#32422;&#26463;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#12289;&#38544;&#31169;&#20445;&#25252;&#30340;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#35813;&#26550;&#26500;&#30456;&#23545;&#20110;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#31649;&#29702;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#24378;&#35843;&#20102;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#20419;&#36827;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04546v1 Announce Type: new  Abstract: This paper proposes a novel three tier architecture for federated learning to optimize edge computing environments. The proposed architecture addresses the challenges associated with client data heterogeneity and computational constraints. It introduces a scalable, privacy preserving framework that enhances the efficiency of distributed machine learning. Through experimentation, the paper demonstrates the architecture capability to manage non IID data sets more effectively than traditional federated learning models. Additionally, the paper highlights the potential of this innovative approach to significantly improve model accuracy, reduce communication overhead, and facilitate broader adoption of federated learning technologies.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.02411</link><description>&lt;p&gt;
NiNformer: &#19968;&#31181;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#30340;&#32593;&#32476;&#20013;&#32593;&#32476;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#20027;&#35201;&#32452;&#20214;&#65292;&#33258;&#24341;&#20837;&#20197;&#26469;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36328;&#36234;&#20102;&#35768;&#22810;&#39046;&#22495;&#21644;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26426;&#21046;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#34987;&#24212;&#29992;&#20026;Vision Transformer ViT&#65292;&#24182;&#19988;&#20854;&#29992;&#36884;&#24050;&#25193;&#23637;&#21040;&#35270;&#35273;&#39046;&#22495;&#30340;&#35768;&#22810;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#23613;&#31649;&#35813;&#26426;&#21046;&#38750;&#24120;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#33021;&#21147;&#65292;&#20294;&#20854;&#32570;&#28857;&#26159;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#26377;&#25928;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#35774;&#35745;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#21644;&#32531;&#35299;&#25968;&#25454;&#22823;&#23567;&#35201;&#27714;&#12290;&#22312;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20123;&#23581;&#35797;&#30340;&#20363;&#23376;&#21253;&#25324;MLP-Mixer&#12289;Conv-Mixer&#12289;Perciver-IO&#31561;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#20316;&#20026;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02411v1 Announce Type: cross  Abstract: The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an 
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#22312;&#20855;&#26377;&#28857;&#22855;&#24322;&#24615;&#30340;Gevrey&#31867;&#38382;&#39064;&#19978;&#20855;&#26377;&#25351;&#25968;&#34920;&#36798;&#33021;&#21147;&#65292;&#20013;&#38388;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;ReLU&#21644;ReLU$^2$&#28608;&#27963;&#32452;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#31934;&#30830;&#27169;&#25311;&#39640;&#38454;&#26377;&#38480;&#20803;&#12290;</title><link>https://arxiv.org/abs/2403.02035</link><description>&lt;p&gt;
ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#22312;&#20855;&#26377;&#28857;&#22855;&#24322;&#24615;&#30340;Gevrey&#31867;&#38382;&#39064;&#19978;&#30340;&#25351;&#25968;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exponential Expressivity of ReLU$^k$ Neural Networks on Gevrey Classes with Point Singularities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02035
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#22312;&#20855;&#26377;&#28857;&#22855;&#24322;&#24615;&#30340;Gevrey&#31867;&#38382;&#39064;&#19978;&#20855;&#26377;&#25351;&#25968;&#34920;&#36798;&#33021;&#21147;&#65292;&#20013;&#38388;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;ReLU&#21644;ReLU$^2$&#28608;&#27963;&#32452;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#31934;&#30830;&#27169;&#25311;&#39640;&#38454;&#26377;&#38480;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#26377;&#30028;&#30340;&#22810;&#38754;&#20307;&#22495;$\mathrm{D} \subset \mathbb{R}^d$, $d=2,3$&#20013;&#20855;&#26377;&#28857;&#22855;&#24322;&#24615;&#30340;&#24179;&#28369;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#36895;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Sobolev&#31354;&#38388;&#20013;&#30340;&#25351;&#25968;&#27169;&#25311;&#36895;&#29575;&#65292;&#36825;&#19982;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#20197;&#21450;Gevrey&#27491;&#21017;&#35299;&#31867;&#20869;&#30340;&#38750;&#38646;&#31995;&#25968;&#30340;&#25968;&#37327;&#26377;&#20851;&#65292;&#36825;&#20123;&#35299;&#31867;&#26159;&#26681;&#25454;$\mathrm{D}$&#20013;&#21152;&#26435;Sobolev&#26631;&#24230;&#23450;&#20041;&#30340;&#65292;&#21253;&#25324;I.M. Babu\v{s}ka&#21644;B.Q. Guo&#30340;&#21487;&#25968;&#33539;&#25968;&#31354;&#38388;&#12290;&#20316;&#20026;&#20013;&#38388;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20219;&#24847;&#27491;&#21017;&#30340;&#31616;&#21333;&#24418;&#22495;$\mathrm{D} \subset \mathbb{R}^d$, $d\geq 2$&#19978;&#30340;&#36830;&#32493;&#12289;&#20998;&#27573;&#22810;&#39033;&#24335;&#39640;&#38454;(``$p$-version'')&#26377;&#38480;&#20803;&#65292;&#20854;&#20803;&#32032;&#22810;&#39033;&#24335;&#24230;&#20026;$p\in\mathbb{N}$&#65292;&#21487;&#20197;&#34987;ReLU&#21644;ReLU$^2$&#28608;&#27963;&#32452;&#21512;&#30340;&#31070;&#32463;&#32593;&#32476;&#31934;&#30830;&#27169;&#25311;&#12290;&#22312;&#24418;&#29366;&#27491;&#21017;&#30340;&#22810;&#38754;&#20307;&#22495;$\mathrm{D}$&#30340;&#31616;&#21333;&#24418;&#22495;&#21010;&#20998;&#19978;&#65292;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#20197;&#21450;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02035v1 Announce Type: cross  Abstract: We analyze deep Neural Network emulation rates of smooth functions with point singularities in bounded, polytopal domains $\mathrm{D} \subset \mathbb{R}^d$, $d=2,3$. We prove exponential emulation rates in Sobolev spaces in terms of the number of neurons and in terms of the number of nonzero coefficients for Gevrey-regular solution classes defined in terms of weighted Sobolev scales in $\mathrm{D}$, comprising the countably-normed spaces of I.M. Babu\v{s}ka and B.Q. Guo.   As intermediate result, we prove that continuous, piecewise polynomial high order (``$p$-version'') finite elements with elementwise polynomial degree $p\in\mathbb{N}$ on arbitrary, regular, simplicial partitions of polyhedral domains $\mathrm{D} \subset \mathbb{R}^d$, $d\geq 2$ can be exactly emulated by neural networks combining ReLU and ReLU$^2$ activations. On shape-regular, simplicial partitions of polytopal domains $\mathrm{D}$, both the number of neurons and t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.00252</link><description>&lt;p&gt;
EUROPA&#65306;&#19968;&#20010;&#27861;&#24459;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EUROPA: A Legal Multilingual Keyphrase Generation Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;EUROPA&#65292;&#21253;&#21547;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#65292;&#34920;&#26126;&#22312;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#20027;&#35201;&#22312;&#23398;&#26415;&#30740;&#31350;&#25991;&#31456;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#25506;&#32034;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#31185;&#23398;&#39046;&#22495;&#21644;&#33521;&#35821;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EUROPA&#65292;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#22810;&#35821;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290; &#23427;&#28304;&#33258;&#27431;&#27954;&#27861;&#38498;&#30340;&#27861;&#24459;&#21028;&#20915;&#65292;&#24182;&#21253;&#21547;&#20102;&#25152;&#26377;24&#31181;&#27431;&#30431;&#23448;&#26041;&#35821;&#35328;&#20013;&#30340;&#23454;&#20363;&#12290; &#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#19978;&#36816;&#34892;&#22810;&#35821;&#35328;&#27169;&#22411;&#24182;&#20998;&#26512;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22312;&#20687;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#23450;&#39046;&#22495;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00252v1 Announce Type: cross  Abstract: Keyphrase generation has primarily been explored within the context of academic research articles, with a particular focus on scientific domains and the English language. In this work, we present EUROPA, a dataset for multilingual keyphrase generation in the legal domain. It is derived from legal judgments from the Court of Justice of the European Union (EU), and contains instances in all 24 EU official languages. We run multilingual models on our corpus and analyze the results, showing room for improvement on a domain-specific multilingual corpus such as the one we present.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35843;&#26597;&#20102;&#20010;&#24615;&#21270;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#24739;&#32773;&#20449;&#24687;&#32570;&#22833;&#26102;&#65292;&#22899;&#24615;&#30340;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#36136;&#37327;&#26126;&#26174;&#20302;&#20110;&#30007;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.19226</link><description>&lt;p&gt;
&#25506;&#31350;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#24930;&#24615;&#30140;&#30171;&#20010;&#24615;&#21270;&#25252;&#29702;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating Gender Fairness in Machine Learning-driven Personalized Care for Chronic Pain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19226
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35843;&#26597;&#20102;&#20010;&#24615;&#21270;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#24739;&#32773;&#20449;&#24687;&#32570;&#22833;&#26102;&#65292;&#22899;&#24615;&#30340;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#36136;&#37327;&#26126;&#26174;&#20302;&#20110;&#30007;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#35843;&#26597;&#20102;&#20010;&#24615;&#21270;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#12290;&#21033;&#29992;&#19978;&#19979;&#25991;&#33218;&#26694;&#26550;&#65292;&#20351;&#29992;LinUCB&#31639;&#27861;&#23545;&#21253;&#21547;164&#20301;&#24739;&#32773;&#22312;&#27599;&#20010;10&#27425;&#20250;&#35805;&#20013;&#30340;&#20114;&#21160;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#21046;&#23450;&#21644;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#31639;&#27861;&#21442;&#25968;&#36827;&#34892;&#35843;&#25972;&#20250;&#24433;&#21709;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#20294;&#36825;&#31181;&#24433;&#21709;&#22312;&#19981;&#21516;&#24615;&#21035;&#38388;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#24739;&#32773;&#20449;&#24687;&#65292;&#22914;&#33258;&#25105;&#25253;&#21578;&#30340;&#30140;&#30171;&#27979;&#37327;&#20540;&#65292;&#32570;&#22833;&#26102;&#65292;&#22899;&#24615;&#30340;&#30140;&#30171;&#25252;&#29702;&#25512;&#33616;&#36136;&#37327;&#26126;&#26174;&#20302;&#20110;&#30007;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19226v1 Announce Type: new  Abstract: This study investigates gender fairness in personalized pain care recommendations using machine learning algorithms. Leveraging a contextual bandits framework, personalized recommendations are formulated and evaluated using LinUCB algorithm on a dataset comprising interactions with $164$ patients across $10$ sessions each. Results indicate that while adjustments to algorithm parameters influence the quality of pain care recommendations, this impact remains consistent across genders. However, when certain patient information, such as self-reported pain measurements, is absent, the quality of pain care recommendations for women is notably inferior to that for men.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#26032;&#39062;&#24615;&#30340;&#22522;&#20110;&#26680;&#30340;&#29109;&#26032;&#39062;&#24615; (KEN) &#20998;&#25968;</title><link>https://arxiv.org/abs/2402.17287</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#27169;&#22411;&#29109;&#20540;&#26032;&#39062;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Evaluation of Entropy-based Novelty of Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#26032;&#39062;&#24615;&#30340;&#22522;&#20110;&#26680;&#30340;&#29109;&#26032;&#39062;&#24615; (KEN) &#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#21644;&#26550;&#26500;&#30340;&#24040;&#22823;&#21457;&#23637;&#38656;&#35201;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30456;&#23545;&#20110;&#21442;&#32771;&#25968;&#25454;&#38598;&#25110;&#22522;&#32447;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#39062;&#24615;&#12290; &#34429;&#28982;&#26368;&#36817;&#30340;&#25991;&#29486;&#24050;&#24191;&#27867;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#20294;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#30340;&#27169;&#22411;&#26032;&#39062;&#24615;&#35780;&#20272;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#19979;&#30340;&#26032;&#39062;&#24615;&#35780;&#20272;&#65292;&#24182;&#23581;&#35797;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#32473;&#23450;&#29983;&#25104;&#27169;&#22411; $\mathcal{G}$ &#30340;&#26679;&#26412;&#21644;&#21442;&#32771;&#25968;&#25454;&#38598; $\mathcal{S}$&#65292;&#25105;&#20204;&#22914;&#20309;&#21457;&#29616;&#24182;&#35745;&#31639; $\mathcal{G}$ &#27604; $\mathcal{S}$ &#20013;&#26356;&#39057;&#32321;&#22320;&#34920;&#36798;&#30340;&#27169;&#24335;&#12290; &#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#35889;&#26041;&#27861;&#26469;&#25551;&#36848;&#36825;&#19968;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#30340;&#29109;&#26032;&#39062;&#24615; (KEN) &#20998;&#25968;&#26469;&#37327;&#21270;&#22522;&#20110;&#27169;&#24335;&#30340;&#26032;&#39062;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17287v1 Announce Type: new  Abstract: The massive developments of generative model frameworks and architectures require principled methods for the evaluation of a model's novelty compared to a reference dataset or baseline generative models. While the recent literature has extensively studied the evaluation of the quality, diversity, and generalizability of generative models, the assessment of a model's novelty compared to a baseline model has not been adequately studied in the machine learning community. In this work, we focus on the novelty assessment under multi-modal generative models and attempt to answer the following question: Given the samples of a generative model $\mathcal{G}$ and a reference dataset $\mathcal{S}$, how can we discover and count the modes expressed by $\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral approach to the described task and propose the Kernel-based Entropic Novelty (KEN) score to quantify the mode-based novelty 
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;GRU&#26550;&#26500;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#22797;&#26434;&#25705;&#25830;&#23450;&#24459;&#21160;&#21147;&#23398;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14148</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#19982;&#25705;&#25830;&#65306;&#28369;&#21160;&#12289;&#20445;&#25345;&#12289;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neural Networks and Friction: Slide, Hold, Learn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14148
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;GRU&#26550;&#26500;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#22797;&#26434;&#25705;&#25830;&#23450;&#24459;&#21160;&#21147;&#23398;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#26550;&#26500;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#20855;&#26377;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#20013;&#36895;&#29575;&#19982;&#29366;&#24577;&#25705;&#25830;&#23450;&#24459;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#12290;&#29992;&#20110;&#35757;&#32451;&#32593;&#32476;&#30340;&#25968;&#25454;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#36895;&#29575;&#19982;&#29366;&#24577;&#25705;&#25830;&#26041;&#31243;&#32467;&#21512;&#29366;&#24577;&#28436;&#21270;&#32769;&#21270;&#23450;&#24459;&#29983;&#25104;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#21046;&#23450;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#26126;&#30830;&#32771;&#34385;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21021;&#22987;&#26465;&#20214;&#12289;&#30452;&#25509;&#25928;&#24212;&#20197;&#21450;&#29366;&#24577;&#21464;&#37327;&#30340;&#28436;&#21464;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;GRU&#26550;&#26500;&#30340;RNN&#33021;&#22815;&#26377;&#25928;&#23398;&#20064;&#39044;&#27979;&#25705;&#25830;&#31995;&#25968;&#30001;&#20110;&#36895;&#24230;&#36339;&#36291;&#32780;&#20135;&#29983;&#30340;&#21464;&#21270;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#27169;&#25311;&#25705;&#25830;&#36807;&#31243;&#29289;&#29702;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14148v1 Announce Type: cross  Abstract: In this study, it is demonstrated that Recurrent Neural Networks (RNNs), specifically those utilizing Gated Recurrent Unit (GRU) architecture, possess the capability to learn the complex dynamics of rate-and-state friction laws from synthetic data. The data employed for training the network is generated through the application of traditional rate-and-state friction equations coupled with the aging law for state evolution. A novel aspect of our approach is the formulation of a loss function that explicitly accounts for initial conditions, the direct effect, and the evolution of state variables during training. It is found that the RNN, with its GRU architecture, effectively learns to predict changes in the friction coefficient resulting from velocity jumps, thereby showcasing the potential of machine learning models in understanding and simulating the physics of frictional processes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30456;&#27604;&#31454;&#20105;&#32773;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12269</link><description>&lt;p&gt;
&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12269
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#21305;&#37197;&#36827;&#34892;&#20219;&#24847;&#22823;&#23567;&#22270;&#30340;&#31471;&#23545;&#31471;&#30417;&#30563;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30456;&#27604;&#31454;&#20105;&#32773;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#30340;&#30417;&#30563;&#22270;&#39044;&#27979;&#65288;SGP&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21407;&#22987;&#30340;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#25439;&#22833;&#65292;&#37096;&#20998;&#25513;&#30721;&#34701;&#21512;&#30340;Gromov-Wasserstein&#25439;&#22833;&#65288;PM-FGW&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#21033;&#29992;&#22270;&#34920;&#31034;&#65292;&#27604;&#22914;&#37051;&#25509;&#21644;&#29305;&#24449;&#30697;&#38453;&#12290;PM-FGW&#20855;&#26377;SGP&#30340;&#25152;&#26377;&#29702;&#24819;&#23646;&#24615;&#65306;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#24615;&#65292;&#21487;&#24494;&#20998;&#24615;&#65292;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#30340;&#22635;&#20805;&#34920;&#31034;&#20197;&#21450;&#23427;&#20204;&#30340;&#25513;&#30721;&#21521;&#37327;&#22788;&#29702;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#37096;&#20998;&#65292;&#19977;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#19968;&#20010;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65288;image2graph&#65289;&#21644;&#20004;&#20010;&#30495;&#23454;&#20219;&#21153;&#65292;&#22270;&#20687;&#21040;&#22320;&#22270;&#21644;&#25351;&#32441;&#21040;&#20998;&#23376; - &#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30456;&#27604;&#31454;&#20105;&#32773;&#30340;&#25928;&#29575;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12269v1 Announce Type: new  Abstract: We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#19982;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30456;&#32467;&#21512;&#30340;VerSAILLE&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10998</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#30830;&#20445;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provably Safe Neural Network Controllers via Differential Dynamic Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#19982;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30456;&#32467;&#21512;&#30340;VerSAILLE&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20316;&#20026;&#38754;&#21521;&#30446;&#26631;&#30340;&#25511;&#21046;&#22120;&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#39564;&#35777;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#31995;&#32479;&#65288;NNCS&#65289;&#30340;&#23433;&#20840;&#24615;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;NN&#26469;&#35828;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#23545;&#26080;&#30028;&#26102;&#38388;&#33539;&#22260;&#36827;&#34892;&#23433;&#20840;&#24615;&#39564;&#35777;&#26102;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;VerSAILLE&#65288;&#36890;&#36807;&#36923;&#36753;&#38142;&#25509;&#21253;&#39564;&#35777;&#30340;&#21487;&#39564;&#35777;&#23433;&#20840;&#20154;&#24037;&#26234;&#33021;&#65289;&#65306;&#36825;&#26159;&#24046;&#20998;&#21160;&#24577;&#36923;&#36753;&#65288;dL&#65289;&#21644;NN&#39564;&#35777;&#32452;&#21512;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#12290;&#36890;&#36807;&#21512;&#20316;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;NN&#39564;&#35777;&#24037;&#20855;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#30041;dL&#30340;&#20005;&#35880;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25511;&#21046;&#22120;&#20449;&#23553;&#30340;&#23433;&#20840;&#24615;&#35777;&#26126;&#65292;&#20197;&#35777;&#26126;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#19978;&#20855;&#20307;NNCS&#30340;&#23433;&#20840;&#24615;&#12290;VerSAILLE&#23548;&#33268;&#30340;NN&#39564;&#35777;&#23646;&#24615;&#36890;&#24120;&#38656;&#35201;&#38750;&#32447;&#24615;&#31639;&#26415;&#65292;&#32780;&#39640;&#25928;&#30340;NN&#39564;&#35777;&#24037;&#20855;&#20165;&#25903;&#25345;&#32447;&#24615;&#31639;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10998v1 Announce Type: cross  Abstract: While neural networks (NNs) have a large potential as goal-oriented controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs -- especially when safety is needed for unbounded time horizons. One reason for this is the intractability of NN and hybrid system analysis. We introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first approach for the combination of differential dynamic logic (dL) and NN verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of dL. We reflect a safety proof for a controller envelope in an NN to prove the safety of concrete NNCS on an infinite-time horizon. The NN verification properties resulting from VerSAILLE typically require nonlinear arithmetic while efficient NN verification tools merely support linear arithmetic. T
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#20540;&#26469;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#22312;&#35299;&#37322;&#27169;&#22411;&#36755;&#20986;&#26102;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#25552;&#20379;&#30340;&#35814;&#32454;&#21644;&#26377;&#27934;&#23519;&#21147;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.09947</link><description>&lt;p&gt;
&#29992;&#20998;&#24067;&#20540;&#35299;&#37322;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Explaining Probabilistic Models with Distributional Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#27010;&#29575;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#20540;&#26469;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#22312;&#35299;&#37322;&#27169;&#22411;&#36755;&#20986;&#26102;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#25552;&#20379;&#30340;&#35814;&#32454;&#21644;&#26377;&#27934;&#23519;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#37325;&#35201;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20998;&#25903;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#21338;&#24328;&#29702;&#35770;&#35299;&#37322;&#21487;&#33021;&#20250;&#35823;&#23548;&#25110;&#38590;&#20197;&#35299;&#37322;&#12290;&#25105;&#20204;&#35748;&#20026;&#36890;&#24120;&#23384;&#22312;&#30528;&#19968;&#20010;&#37325;&#35201;&#30340;&#19981;&#21305;&#37197;&#65292;&#21363;&#20154;&#20204;&#24076;&#26395;&#35299;&#37322;&#30340;&#20869;&#23481;&#65288;&#20363;&#22914;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#65289;&#19982;&#24403;&#21069;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;SHAP&#65289;&#25152;&#35299;&#37322;&#30340;&#20869;&#23481;&#65288;&#20363;&#22914;&#31867;&#21035;&#30340;&#27010;&#29575;&#65289;&#20043;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#25512;&#24191;&#21512;&#20316;&#21338;&#24328;&#21644;&#20215;&#20540;&#31639;&#23376;&#65292;&#26469;&#35299;&#20915;&#27010;&#29575;&#27169;&#22411;&#30340;&#36825;&#31181;&#24046;&#36317;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#24067;&#20540;&#65292;&#36825;&#26159;&#19968;&#31181;&#38543;&#26426;&#21464;&#37327;&#65292;&#29992;&#20110;&#36861;&#36394;&#27169;&#22411;&#36755;&#20986;&#30340;&#21464;&#21270;&#65288;&#20363;&#22914;&#39044;&#27979;&#31867;&#21035;&#30340;&#21453;&#36716;&#65289;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#20855;&#26377;&#39640;&#26031;&#12289;&#20271;&#21162;&#21033;&#21644;&#20998;&#31867;&#25903;&#20184;&#30340;&#21338;&#24328;&#20013;&#30340;&#20998;&#24067;&#20540;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#20960;&#20010;&#29305;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#21644;&#26377;&#27934;&#23519;&#21147;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09947v1 Announce Type: new  Abstract: A large branch of explainable machine learning is grounded in cooperative game theory. However, research indicates that game-theoretic explanations may mislead or be hard to interpret. We argue that often there is a critical mismatch between what one wishes to explain (e.g. the output of a classifier) and what current methods such as SHAP explain (e.g. the scalar probability of a class). This paper addresses such gap for probabilistic models by generalising cooperative games and value operators. We introduce the distributional values, random variables that track changes in the model output (e.g. flipping of the predicted class) and derive their analytic expressions for games with Gaussian, Bernoulli and Categorical payoffs. We further establish several characterising properties, and show that our framework provides fine-grained and insightful explanations with case studies on vision and language models.
&lt;/p&gt;</description></item><item><title>QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.09739</link><description>&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;QuRating&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QuRating: Selecting High-Quality Data for Training Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09739
&lt;/p&gt;
&lt;p&gt;
QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#21019;&#24314;&#33021;&#21147;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#24456;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QuRating&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22235;&#20010;&#29305;&#24449; - &#20889;&#20316;&#39118;&#26684;&#12289;&#25152;&#38656;&#19987;&#19994;&#30693;&#35782;&#12289;&#20107;&#23454;&#21644;&#29712;&#20107;&#20197;&#21450;&#25945;&#32946;&#20215;&#20540;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36776;&#21035;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#36827;&#34892;&#25991;&#26412;&#30340;&#37197;&#23545;&#21028;&#26029;&#26041;&#38754;&#27604;&#30452;&#25509;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#26356;&#22909;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;QuRater&#27169;&#22411;&#65292;&#20174;&#37197;&#23545;&#21028;&#26029;&#20013;&#23398;&#20064;&#26631;&#37327;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#23427;&#20026;260B&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#27599;&#20010;&#26631;&#20934;&#36827;&#34892;&#36136;&#37327;&#35780;&#32423;&#27880;&#37322;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#36136;&#37327;&#35780;&#32423;&#36873;&#25321;&#20102;30B&#20010;&#20196;&#29260;&#65292;&#24182;&#22312;&#25152;&#36873;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;13&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09739v1 Announce Type: new  Abstract: Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts &amp; trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and di
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#21382;&#21490;&#24207;&#21015;&#30340;&#25361;&#25112;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#26368;&#39640;&#21487;&#20943;&#23569;89.43%&#30340;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.09573</link><description>&lt;p&gt;
&#34676;&#34678;&#24341;&#36215;&#30340;&#21464;&#21270;&#65306;&#21033;&#29992;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#36827;&#34892;&#36828;&#35265;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Changes by Butterflies: Farsighted Forecasting with Group Reservoir Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09573
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#35299;&#20915;&#21382;&#21490;&#24207;&#21015;&#30340;&#25361;&#25112;&#21644;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#26368;&#39640;&#21487;&#20943;&#23569;89.43%&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#27788;&#20013;&#65292;&#20004;&#20010;&#21021;&#22987;&#26465;&#20214;&#20043;&#38388;&#30340;&#24494;&#23567;&#24046;&#24322;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21576;&#25351;&#25968;&#32423;&#25918;&#22823;&#65292;&#23548;&#33268;&#36965;&#36828;&#30340;&#32467;&#26524;&#65292;&#20063;&#34987;&#31216;&#20026;&#34676;&#34678;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#36828;&#26399;&#20805;&#28385;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#38590;&#20197;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#20648;&#22791;&#36716;&#25442;&#22120;&#26469;&#36890;&#36807;&#20811;&#26381;&#28151;&#27788;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65288;1&#65289;&#22823;&#37327;&#30340;&#21382;&#21490;&#24207;&#21015;&#21644;&#65288;2&#65289;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#25935;&#24863;&#24615;&#26469;&#26356;&#20934;&#30830;&#12289;&#26356;&#31283;&#20581;&#22320;&#39044;&#27979;&#38271;&#26399;&#20107;&#20214;&#12290;&#23558;&#19968;&#20010;&#20648;&#22791;&#35013;&#32622;&#36830;&#25509;&#21040;&#36716;&#25442;&#22120;&#19978;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#19968;&#32452;&#20648;&#22791;&#35013;&#32622;&#26469;&#20943;&#23569;&#30001;&#20110;&#21021;&#22987;&#21270;&#21464;&#21270;&#32780;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#65292;&#21253;&#25324;NLinear&#12289;Pyformer&#12289;Informer&#12289;Autoformer&#21644;&#22522;&#20934;Transformer&#65292;&#20854;&#35823;&#24046;&#20943;&#23569;&#39640;&#36798;-89.43&#65285;&#65292;&#36866;&#29992;&#20110;ETTh&#12289;ETTm&#21644;&#31354;&#27668;&#36136;&#37327;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09573v1 Announce Type: cross  Abstract: In Chaos, a minor divergence between two initial conditions exhibits exponential amplification over time, leading to far-away outcomes, known as the butterfly effect. Thus, the distant future is full of uncertainty and hard to forecast. We introduce Group Reservoir Transformer to predict long-term events more accurately and robustly by overcoming two challenges in Chaos: (1) the extensive historical sequences and (2) the sensitivity to initial conditions. A reservoir is attached to a Transformer to efficiently handle arbitrarily long historical lengths, with an extension of a group of reservoirs to reduce the uncertainty due to the initialization variations. Our architecture consistently outperforms state-of-the-art DNN models in multivariate time series, including NLinear, Pyformer, Informer, Autoformer, and the baseline Transformer, with an error reduction of up to -89.43\% in various fields such as ETTh, ETTm, and air quality, demon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#30340;&#26657;&#20934;&#24615;&#36136;&#65292;&#24182;&#21457;&#29616;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#19988;VLMs&#21482;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#36827;&#34892;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07417</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26657;&#20934;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study Into What Matters for Calibrating Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#30340;&#26657;&#20934;&#24615;&#36136;&#65292;&#24182;&#21457;&#29616;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#19988;VLMs&#21482;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#36827;&#34892;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24050;&#25104;&#20026;&#38646;&#26679;&#26412;&#35782;&#21035;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#22330;&#26223;&#21644;&#37325;&#35201;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#39046;&#22495;&#38656;&#35201;&#23545;&#20854;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#26377;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#65292;&#36825;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#30693;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#31574;&#30053;&#19979;VLMs&#30340;&#26657;&#20934;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;&#22312;&#19968;&#20010;&#39046;&#22495;&#12289;&#26631;&#31614;&#38598;&#25110;&#23618;&#27425;&#32423;&#21035;&#20013;&#36827;&#34892;&#26657;&#20934;&#26102;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;VLMs&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;VLMs&#26412;&#36523;&#24182;&#19981;&#20855;&#22791;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#26174;&#33879;&#19988;&#19968;&#33268;&#22320;&#25552;&#21319;&#26657;&#20934;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#20998;&#24067;&#36716;&#21464;&#21644;&#26631;&#31614;&#38598;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;VLMs&#21487;&#20197;&#36890;&#36807;&#24456;&#23569;&#30340;&#31034;&#20363;&#36827;&#34892;&#26657;&#20934;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision--Language Models (VLMs) have emerged as the dominant approach for zero-shot recognition, adept at handling diverse scenarios and significant distribution changes. However, their deployment in risk-sensitive areas requires a deeper understanding of their uncertainty estimation capabilities, a relatively uncharted area. In this study, we explore the calibration properties of VLMs across different architectures, datasets, and training strategies. In particular, we analyze the uncertainty estimation performance of VLMs when calibrated in one domain, label set or hierarchy level, and tested in a different one. Our findings reveal that while VLMs are not inherently calibrated for uncertainty, temperature scaling significantly and consistently improves calibration, even across shifts in distribution and changes in label set. Moreover, VLMs can be calibrated with a very small set of examples. Through detailed experimentation, we highlight the potential applications and importance of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#25552;&#31034;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Distance-Aware Ca &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04655</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Calibration for Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#25552;&#31034;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Distance-Aware Ca &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; (VLM) &#24050;&#32463;&#25104;&#20026;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#22788;&#29702;&#22270;&#20687;&#35782;&#21035;&#12289;&#25991;&#26412;&#39537;&#21160;&#30340;&#35270;&#35273;&#20869;&#23481;&#29983;&#25104;&#12289;&#35270;&#35273;&#32842;&#22825;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#24320;&#25918;&#35789;&#27719;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#22312;&#25552;&#39640; VLM &#19979;&#28216;&#24615;&#33021;&#30340;&#36866;&#24212;&#26041;&#27861;&#19978;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#36164;&#28304;&#65292;&#23588;&#20854;&#26159;&#22312;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;&#22914;&#25552;&#31034;&#23398;&#20064;&#65289;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#34987;&#22823;&#22823;&#24573;&#35270;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#22312;&#24494;&#35843;&#30340; VLM &#20013;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#38382;&#39064;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#36825;&#26679;&#30340;&#27169;&#22411;&#26102;&#20250;&#22823;&#22823;&#38477;&#20302;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#30740;&#31350;&#25552;&#31034;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#32622;&#20449;&#24230;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#24320;&#25918;&#35789;&#27719;&#30340;&#35774;&#32622;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; "Distance-Aware Ca"
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20843;&#21350;&#30340;&#26799;&#24230;&#32534;&#30721;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65288;GOCO&#65289;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04193</link><description>&lt;p&gt;
&#36991;&#20813;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Gradient Coding in Decentralized Learning for Evading Stragglers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20843;&#21350;&#30340;&#26799;&#24230;&#32534;&#30721;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65288;GOCO&#65289;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23384;&#22312;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#23613;&#31649;&#26799;&#24230;&#32534;&#30721;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20197;&#36991;&#20813;&#24310;&#36831;&#33410;&#28857;&#65292;&#21363;&#35774;&#22791;&#20351;&#29992;&#20887;&#20313;&#35757;&#32451;&#25968;&#25454;&#21457;&#36865;&#32534;&#30721;&#26799;&#24230;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#25216;&#26415;&#30452;&#25509;&#24212;&#29992;&#20110;&#20998;&#25955;&#24335;&#23398;&#20064;&#22330;&#26223;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20843;&#21350;&#30340;&#26799;&#24230;&#32534;&#30721;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65288;GOCO&#65289;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#20026;&#20102;&#36991;&#20813;&#24310;&#36831;&#33410;&#28857;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21442;&#25968;&#21521;&#37327;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#32534;&#30721;&#26694;&#26550;&#30340;&#32534;&#30721;&#26799;&#24230;&#36827;&#34892;&#26412;&#22320;&#26356;&#26032;&#65292;&#28982;&#21518;&#20197;&#20843;&#21350;&#26041;&#24335;&#36827;&#34892;&#24179;&#22343;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;GOCO&#22312;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#30456;&#23545;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#21270;&#23545;&#38544;&#31169;&#25512;&#26029;&#20013;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20943;&#23569;ReLU&#28608;&#27963;&#20989;&#25968;&#25968;&#37327;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#38477;&#20302;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23545;&#20110;&#22810;&#25968;&#32676;&#20307;&#21017;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#37319;&#29992;&#31616;&#21333;&#30340;&#24494;&#35843;&#27493;&#39588;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03629</link><description>&lt;p&gt;
&#31169;&#26377;&#25512;&#26029;&#30340;&#32447;&#24615;&#21270;&#23545;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#19981;&#23545;&#31216;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Disparate Impact on Group Accuracy of Linearization for Private Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#21270;&#23545;&#38544;&#31169;&#25512;&#26029;&#20013;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20943;&#23569;ReLU&#28608;&#27963;&#20989;&#25968;&#25968;&#37327;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#38477;&#20302;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23545;&#20110;&#22810;&#25968;&#32676;&#20307;&#21017;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#37319;&#29992;&#31616;&#21333;&#30340;&#24494;&#35843;&#27493;&#39588;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#23545;&#20855;&#26377;&#23494;&#30721;&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#25512;&#26029;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20013;&#26114;&#36149;&#30340;&#21152;&#23494;&#35745;&#31639;&#30340;&#29942;&#39048;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#24314;&#35758;&#22312;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#20013;&#32447;&#24615;&#21270;&#30446;&#26631;&#37096;&#20998;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#65292;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#24448;&#24448;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#35745;&#31639;&#20248;&#21183;&#21487;&#33021;&#23548;&#33268;&#20844;&#24179;&#24615;&#25104;&#26412;&#22686;&#21152;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#20943;&#23569;ReLU&#28608;&#27963;&#20989;&#25968;&#25968;&#37327;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#38477;&#20302;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23545;&#20110;&#22810;&#25968;&#32676;&#20307;&#21017;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#22312;&#23545;&#20915;&#31574;&#36793;&#30028;&#24615;&#36136;&#36827;&#34892;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#20102;&#25968;&#23398;&#35299;&#37322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#26222;&#36941;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#31243;&#24207;&#25913;&#21464;&#32447;&#24615;&#27169;&#22411;&#30340;&#24494;&#35843;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the fine-tuning step for linearized models ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03479</link><description>&lt;p&gt;
ICED: &#36890;&#36807;&#19978;&#19979;&#25991;&#29615;&#22659;&#35774;&#35745;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#36890;&#24120;&#32570;&#20047;&#25104;&#21151;&#22320;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#30340;&#37319;&#26679;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#25512;&#24191;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20849;&#20139;&#22522;&#26412;&#23618;&#30340;&#28145;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#26681;&#25454;&#20854;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35757;&#32451;&#32423;&#21035;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#36825;&#20026;&#26576;&#20123;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#23454;&#29616;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20855;&#26377;&#26356;&#22810;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;UED&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#23454;&#20363;&#65292;&#20174;&#32780;&#24433;&#21709;&#20195;&#29702;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the trainin
&lt;/p&gt;</description></item><item><title>&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02287</link><description>&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#30784;&#30340;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Future Directions in Foundations of Graph Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02287
&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26410;&#26469;&#26041;&#21521;&#24212;&#35813;&#26159;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#29702;&#35770;&#65292;&#20174;&#26356;&#23436;&#25972;&#30340;&#35282;&#24230;&#25506;&#31350;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#25968;&#25454;&#22312;&#19981;&#21516;&#23398;&#31185;&#65288;&#20174;&#29983;&#21629;&#31185;&#23398;&#21040;&#31038;&#20250;&#31185;&#23398;&#21644;&#24037;&#31243;&#31185;&#23398;&#65289;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#24341;&#36215;&#20102;&#20154;&#20204;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23545;GNNs&#24615;&#36136;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38750;&#24120;&#19981;&#23436;&#25972;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#21457;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#38416;&#26126;GNNs&#31895;&#31890;&#24230;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#65292;&#20027;&#35201;&#37319;&#29992;&#32452;&#21512;&#25216;&#24039;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#19982;&#23454;&#36341;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#20248;&#21270;&#25216;&#26415;&#35757;&#32451;GNNs&#26102;&#65292;&#23545;GNNs&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#31687;&#23450;&#20301;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38656;&#35201;&#23558;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#21457;&#23637;&#19968;&#20010;&#26356;&#21152;&#22343;&#34913;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#19978;&#26469;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#36798;&#33021;&#21147;&#12289;&#27867;&#21270;&#21644;&#20248;&#21270;&#30340;&#30456;&#20114;&#20851;&#31995;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning on graphs, especially using graph neural networks (GNNs), has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Despite their practical success, our theoretical understanding of the properties of GNNs remains highly incomplete. Recent theoretical advancements primarily focus on elucidating the coarse-grained expressive power of GNNs, predominantly employing combinatorial techniques. However, these studies do not perfectly align with practice, particularly in understanding the generalization behavior of GNNs when trained with stochastic first-order optimization techniques. In this position paper, we argue that the graph machine learning community needs to shift its attention to developing a more balanced theory of graph machine learning, focusing on a more thorough understanding of the interplay of expressive power, generalization, and optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2401.17626</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Generative AI to Generate Test Data Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20551;&#25968;&#25454;&#26159;&#29616;&#20195;&#36719;&#20214;&#27979;&#35797;&#30340;&#37325;&#35201;&#32500;&#24230;&#20043;&#19968;&#65292;&#20247;&#22810;&#25968;&#25454;&#20266;&#36896;&#24211;&#30340;&#25968;&#37327;&#21644;&#37325;&#35201;&#24615;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#20266;&#36896;&#24211;&#30340;&#24320;&#21457;&#32773;&#26080;&#27861;&#28385;&#36275;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#21644;&#39046;&#22495;&#25152;&#38656;&#29983;&#25104;&#30340;&#24191;&#27867;&#25968;&#25454;&#33539;&#22260;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#31034;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#38598;&#25104;&#32423;&#21035;&#19978;&#25191;&#34892;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#65306;1&#65289;&#21407;&#22987;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#65292;2&#65289;&#21512;&#25104;&#29305;&#23450;&#35821;&#35328;&#30340;&#31243;&#24207;&#20197;&#29983;&#25104;&#26377;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;3&#65289;&#29983;&#25104;&#20351;&#29992;&#23574;&#31471;&#20266;&#36896;&#24211;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;LLMs&#20026;11&#20010;&#39046;&#22495;&#29983;&#25104;&#27979;&#35797;&#25968;&#25454;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;&#38598;&#25104;&#32423;&#21035;&#19978;&#65292;LLMs&#33021;&#22815;&#25104;&#21151;&#22320;&#29983;&#25104;&#36924;&#30495;&#30340;&#27979;&#35797;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating fake data is an essential dimension of modern software testing, as demonstrated by the number and significance of data faking libraries. Yet, developers of faking libraries cannot keep up with the wide range of data to be generated for different natural languages and domains. In this paper, we assess the ability of generative AI for generating test data in different domains. We design three types of prompts for Large Language Models (LLMs), which perform test data generation tasks at different levels of integrability: 1) raw test data generation, 2) synthesizing programs in a specific language that generate useful test data, and 3) producing programs that use state-of-the-art faker libraries. We evaluate our approach by prompting LLMs to generate test data for 11 domains. The results show that LLMs can successfully generate realistic test data generators in a wide range of domains at all three levels of integrability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.13327</link><description>&lt;p&gt;
&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#30340;&#24773;&#22659;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Reinforcement Learning for Variable Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#20808;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#22810;&#24773;&#33410;&#35757;&#32451;&#30340;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#21487;&#20197;&#22312;&#24773;&#22659;&#20013;&#27867;&#21270;&#21040;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#24341;&#20837;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#36890;&#24120;&#38656;&#35201;&#25968;&#25454;&#37325;&#26032;&#25910;&#38598;&#21644;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21482;&#35757;&#32451;&#19968;&#27425;&#30340;Headless-AD&#27169;&#22411;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20855;&#26377;&#21487;&#21464;&#22823;&#23567;&#12289;&#35821;&#20041;&#20869;&#23481;&#21644;&#39034;&#24207;&#30340;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#22312;&#20271;&#21162;&#21033;&#21644;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#20197;&#21450;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Headless-AD&#22312;&#20174;&#26410;&#36935;&#21040;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#20960;&#20010;&#29615;&#22659;&#37197;&#32622;&#19978;&#32988;&#36807;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;&#20986;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#12290;</title><link>https://arxiv.org/abs/2311.13261</link><description>&lt;p&gt;
&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#30340;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Immunohistochemistry guided segmentation of benign epithelial cells, in situ lesions, and invasive epithelial cells in breast cancer slides
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13261
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;&#20986;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#20999;&#29255;&#12290;&#33258;&#21160;&#35780;&#20272;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#25928;&#29575;&#65292;&#24182;&#24110;&#21161;&#25214;&#21040;&#24418;&#24577;&#29305;&#24449;&#19982;&#20020;&#24202;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36776;&#35748;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#65292;&#24182;&#23558;&#20854;&#19982;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#21644;&#21407;&#20301;&#30149;&#21464;&#20998;&#24320;&#23558;&#26159;&#31532;&#19968;&#27493;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#26579;&#33394;&#34880;&#32418;&#34507;&#30333;&#21644;&#21980;&#37240;&#24615;&#26579;&#33394;&#32454;&#32990;&#35282;&#34507;&#30333;(CK) AE1/AE3 HE&#20999;&#29255;&#65292;&#20197;&#21450;&#30149;&#29702;&#23398;&#23478;&#30340;&#27880;&#37322;&#29983;&#25104;&#20102;&#19978;&#30382;&#22522;&#26412;&#30495;&#20540;&#25513;&#27169;&#12290;HE/CK&#22270;&#20687;&#23545;&#34987;&#29992;&#20110;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25968;&#25454;&#22686;&#24378;&#34987;&#29992;&#26469;&#20351;&#27169;&#22411;&#26356;&#31283;&#20581;&#12290;839&#21517;&#24739;&#32773;&#30340;&#32452;&#32455;&#24494;&#38453;&#21015;&#65288;TMAs&#65289;&#21644;&#20004;&#21517;&#24739;&#32773;&#30340;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#29992;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13261v2 Announce Type: replace-cross  Abstract: Digital pathology enables automatic analysis of histopathological sections using artificial intelligence (AI). Automatic evaluation could improve diagnostic efficiency and help find associations between morphological features and clinical outcome. For development of such prediction models, identifying invasive epithelial cells, and separating these from benign epithelial cells and in situ lesions would be the first step. In this study, we aimed to develop an AI model for segmentation of epithelial cells in sections from breast cancer. We generated epithelial ground truth masks by restaining hematoxylin and eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists' annotations. HE/CK image pairs were used to train a convolutional neural network, and data augmentation was used to make the model more robust. Tissue microarrays (TMAs) from 839 patients, and whole slide images from two patients were used for training an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#22312;&#28436;&#35762;&#25991;&#26412;&#20013;&#21306;&#20998;&#21457;&#35328;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2311.07564</link><description>&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#33021;&#21542;&#21306;&#20998;&#28436;&#35762;&#25991;&#26412;&#20013;&#30340;&#21457;&#35328;&#20154;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#24402;&#23646;&#27169;&#22411;&#22312;&#28436;&#35762;&#25991;&#26412;&#20013;&#21306;&#20998;&#21457;&#35328;&#20154;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#39564;&#35777;&#26159;&#30830;&#23450;&#20004;&#20010;&#19981;&#21516;&#20070;&#38754;&#26679;&#26412;&#26159;&#21542;&#21516;&#23646;&#19968;&#20316;&#32773;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#28041;&#21450;&#23545;&#20070;&#38754;&#25991;&#26412;&#30340;&#24402;&#22240;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36716;&#24405;&#28436;&#35762;&#30340;&#24402;&#23646;&#38382;&#39064;&#65292;&#36825;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#35768;&#22810;&#25991;&#20307;&#29305;&#24449;&#65292;&#22914;&#26631;&#28857;&#21644;&#22823;&#20889;&#65292;&#22312;&#36825;&#31181;&#24773;&#22659;&#19979;&#24182;&#19981;&#20855;&#22791;&#20449;&#24687;&#37327;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36716;&#24405;&#30340;&#28436;&#35762;&#21576;&#29616;&#20854;&#20182;&#27169;&#24335;&#65292;&#22914;&#22635;&#20805;&#35789;&#21644;&#22238;&#24212;&#24615;&#22768;&#38899;&#65288;&#20363;&#22914;&#8220;&#21999;&#8221;&#65292;&#8220;&#21999;&#65292;&#21999;&#8221;&#65289;&#65292;&#36825;&#20123;&#21487;&#33021;&#26159;&#19981;&#21516;&#21457;&#35328;&#20154;&#30340;&#29305;&#24449;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20197;&#20250;&#35805;&#28436;&#35762;&#25991;&#26412;&#20026;&#37325;&#28857;&#30340;&#21457;&#35328;&#20154;&#24402;&#23646;&#22522;&#20934;&#12290;&#20026;&#20102;&#38480;&#21046;&#21457;&#35328;&#20154;&#19982;&#35805;&#39064;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#65292;&#25105;&#20204;&#20351;&#29992;&#20250;&#35805;&#25552;&#31034;&#21644;&#21442;&#19982;&#21516;&#19968;&#23545;&#35805;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#19981;&#21516;&#38590;&#24230;&#30340;&#39564;&#35777;&#35797;&#39564;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#22312;&#36825;&#19968;&#26032;&#22522;&#20934;&#19978;&#24314;&#31435;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07564v2 Announce Type: replace  Abstract: Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., 'um', 'uh-huh'), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;AdamQLR&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#36807;&#23558;K-FAC&#20013;&#30340;&#25216;&#26415;&#19982;Adam&#30340;&#26356;&#26032;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#20108;&#38454;&#25968;&#25454;&#19978;&#30340;Adam&#34892;&#20026;&#32780;&#24471;&#21040;&#21551;&#21457;&#12290;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;AdamQLR&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#25512;&#24191;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.14963</link><description>&lt;p&gt;
&#36890;&#36807;&#20108;&#38454;&#36879;&#38236;&#30475;Adam
&lt;/p&gt;
&lt;p&gt;
Adam through a Second-Order Lens. (arXiv:2310.14963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14963
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;AdamQLR&#65292;&#23427;&#26159;&#19968;&#20010;&#36890;&#36807;&#23558;K-FAC&#20013;&#30340;&#25216;&#26415;&#19982;Adam&#30340;&#26356;&#26032;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;&#20108;&#38454;&#25968;&#25454;&#19978;&#30340;Adam&#34892;&#20026;&#32780;&#24471;&#21040;&#21551;&#21457;&#12290;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;AdamQLR&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#25512;&#24191;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#30740;&#31350;&#23384;&#22312;&#19968;&#31181;&#32039;&#24352;&#29366;&#24577;&#65292;&#21363;&#31532;&#19968;&#38454;&#26799;&#24230;&#27861;&#65288;&#22914;SGD&#21644;Adam&#65289;&#30340;&#35745;&#31639;&#25928;&#29575;&#19982;&#31532;&#20108;&#38454;&#26354;&#29575;&#27861;&#65288;&#22914;&#25311;&#29275;&#39039;&#26041;&#27861;&#21644;K-FAC&#65289;&#30340;&#29702;&#35770;&#25928;&#29575;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#25105;&#20204;&#35797;&#22270;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#21040;&#19968;&#20010;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#31639;&#27861;&#20013;&#12290;&#27880;&#24847;&#21040;&#20108;&#38454;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31283;&#23450;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65288;&#22914;Levenberg-Marquardt&#38459;&#23612;&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;AdamQLR&#65306;&#19968;&#20010;&#23558;K-FAC&#20013;&#30340;&#38459;&#23612;&#21644;&#23398;&#20064;&#29575;&#36873;&#25321;&#25216;&#26415;&#19982;Adam&#25552;&#20986;&#30340;&#26356;&#26032;&#26041;&#21521;&#30456;&#32467;&#21512;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#32771;&#34385;Adam&#22312;&#20108;&#38454;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#32780;&#24471;&#21040;&#21551;&#21457;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#35268;&#27169;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;AdamQLR&#65292;&#22312;&#36816;&#34892;&#26102;&#38388;&#19982;&#31454;&#20105;&#24615;&#25512;&#24191;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research into optimisation for deep learning is characterised by a tension between the computational efficiency of first-order, gradient-based methods (such as SGD and Adam) and the theoretical efficiency of second-order, curvature-based methods (such as quasi-Newton methods and K-FAC). We seek to combine the benefits of both approaches into a single computationally-efficient algorithm. Noting that second-order methods often depend on stabilising heuristics (such as Levenberg-Marquardt damping), we propose AdamQLR: an optimiser combining damping and learning rate selection techniques from K-FAC (Martens and Grosse, 2015) with the update directions proposed by Adam, inspired by considering Adam through a second-order lens. We evaluate AdamQLR on a range of regression and classification tasks at various scales, achieving competitive generalisation performance vs runtime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;6G&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;&#36890;&#36807;&#22312;&#25509;&#25910;&#22120;&#19978;&#20351;&#29992;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;SemantIC&#33021;&#22815;&#36845;&#20195;&#28040;&#38500;&#20449;&#21495;&#20013;&#30340;&#22122;&#22768;&#21644;&#35821;&#20041;&#24178;&#25200;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SemantIC&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12768</link><description>&lt;p&gt;
SemantIC: &#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#22312;6G&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SemantIC: Semantic Interference Cancellation Towards 6G Wireless Communications. (arXiv:2310.12768v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;6G&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;&#36890;&#36807;&#22312;&#25509;&#25910;&#22120;&#19978;&#20351;&#29992;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;SemantIC&#33021;&#22815;&#36845;&#20195;&#28040;&#38500;&#20449;&#21495;&#20013;&#30340;&#22122;&#22768;&#21644;&#35821;&#20041;&#24178;&#25200;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SemantIC&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25239;&#24178;&#25200;&#25216;&#26415;&#65292;&#21363;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;SemantIC&#21482;&#38656;&#35201;&#25509;&#25910;&#22120;&#23558;&#20449;&#36947;&#35299;&#30721;&#22120;&#19982;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#36830;&#25509;&#36215;&#26469;&#12290;&#36825;&#26500;&#24314;&#20102;&#19968;&#20010;&#36845;&#20195;&#24490;&#29615;&#65292;&#20132;&#26367;&#28040;&#38500;&#20449;&#21495;&#22495;&#21644;&#35821;&#20041;&#22495;&#20013;&#30340;&#22122;&#22768;&#12290;&#20174;&#32593;&#32476;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#35757;&#32451;&#23384;&#20648;&#20102;&#36741;&#21161;&#20449;&#24687;&#65292;&#24182;&#22312;&#36845;&#20195;&#35299;&#30721;&#20013;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#65292;&#20316;&#20026;Wyner-Ziv&#23450;&#29702;&#30340;&#19968;&#31181;&#23454;&#29616;&#12290;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;SemantIC&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter proposes a novel anti-interference technique, semantic interference cancellation (SemantIC), for enhancing information quality towards the sixth-generation (6G) wireless networks. SemantIC only requires the receiver to concatenate the channel decoder with a semantic auto-encoder. This constructs a turbo loop which iteratively and alternately eliminates noise in the signal domain and the semantic domain. From the viewpoint of network information theory, the neural network of the semantic auto-encoder stores side information by training, and provides side information in iterative decoding, as an implementation of the Wyner-Ziv theorem. Simulation results verify the performance improvement by SemantIC without extra channel resource cost.
&lt;/p&gt;</description></item><item><title>SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03684</link><description>&lt;p&gt;
SmoothLLM&#65306;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03684
&lt;/p&gt;
&lt;p&gt;
SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21162;&#21147;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#65288;&#22914;GPT&#12289;Llama&#12289;Claude&#21644;PaLM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#65292;&#21363;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#27450;&#39575;&#65292;&#20197;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLM&#19978;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#30340;&#25552;&#31034;&#23545;&#23383;&#31526;&#32423;&#21035;&#30340;&#25913;&#21464;&#24456;&#33030;&#24369;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#39318;&#20808;&#38543;&#26426;&#25200;&#21160;&#32473;&#23450;&#36755;&#20837;&#25552;&#31034;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#28982;&#21518;&#27719;&#24635;&#30456;&#24212;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;SmoothLLM&#23558;&#20247;&#22810;&#28909;&#38376;LLM&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#23545;&#25915;&#20987;&#32531;&#35299;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#20351;&#29992;&#30340;&#26597;&#35810;&#25968;&#37327;&#27604;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;LLM&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36817;&#20284;&#36870;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.00177</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36842;&#37324;&#20999;&#29305;&#21644;&#35834;&#26364;&#36793;&#30028;&#26465;&#20214;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#27850;&#26494;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions. (arXiv:2310.00177v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00177
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#30340;&#31070;&#32463;&#39044;&#22788;&#29702;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36817;&#20284;&#36870;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#39044;&#22788;&#29702;&#30340;&#36845;&#20195;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20855;&#26377;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#30340;&#27850;&#26494;&#26041;&#31243;&#12290;&#27850;&#26494;&#26041;&#31243;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65306;&#23427;&#25511;&#21046;&#30528;&#24191;&#27867;&#30340;&#29289;&#29702;&#29616;&#35937;&#65292;&#22312;&#35768;&#22810;&#25968;&#20540;&#31639;&#27861;&#20013;&#20316;&#20026;&#23376;&#38382;&#39064;&#20986;&#29616;&#65292;&#24182;&#19988;&#20316;&#20026;&#26356;&#24191;&#27867;&#30340;&#26925;&#22278;PDE&#31867;&#30340;&#27169;&#22411;&#38382;&#39064;&#12290;&#26368;&#27969;&#34892;&#30340;&#27850;&#26494;&#31163;&#25955;&#21270;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#22823;&#22411;&#31232;&#30095;&#32447;&#24615;&#31995;&#32479;&#12290;&#22312;&#39640;&#20998;&#36776;&#29575;&#21644;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#36845;&#20195;&#27714;&#35299;&#22120;&#32467;&#21512;&#24378;&#22823;&#30340;&#39044;&#22788;&#29702;&#22120;&#21487;&#20197;&#25552;&#20379;&#20248;&#21183;&#12290;&#25105;&#20204;&#27714;&#35299;&#22120;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#21487;&#20197;&#36817;&#20284;&#31163;&#25955;&#32467;&#26500;&#32593;&#26684;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#36870;&#31639;&#23376;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#24418;&#29366;&#30340;&#22495;&#21644;&#28151;&#21512;&#36793;&#30028;&#26465;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#38382;&#39064;&#30340;&#32467;&#26500;&#28608;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#36793;&#30028;&#26465;&#20214;&#19979;&#65292;&#35813;&#26550;&#26500;&#20063;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#39044;&#22788;&#29702;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26696;&#20363;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. The Poisson equation is ubiquitous in scientific computing: it governs a wide array of physical phenomena, arises as a subproblem in many numerical algorithms, and serves as a model problem for the broader class of elliptic PDEs. The most popular Poisson discretizations yield large sparse linear systems. At high resolution, and for performance-critical applications, iterative solvers can be advantageous for these -- but only when paired with powerful preconditioners. The core of our solver is a neural network trained to approximate the inverse of a discrete structured-grid Laplace operator for a domain of arbitrary shape and with mixed boundary conditions. The structure of this problem motivates a novel network architecture that we demonstrate is highly effective as a preconditioner even for boundary conditions outside the training set. We show that on challenging test cases aris
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16888</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#22120;&#33719;&#21462;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#25237;&#36164;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer. (arXiv:2309.16888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Transformer-based Multivariate Time Series Classifier (TMTSC)&#26469;&#39044;&#27979;&#39118;&#38505;&#25237;&#36164;&#21644;&#25104;&#38271;&#36164;&#26412;&#30340;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#65292;&#20197;&#20248;&#21270;&#25237;&#36164;&#30446;&#26631;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#31169;&#21215;&#32929;&#26435;&#65288;PE&#65289;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20026;&#39118;&#38505;&#25237;&#36164;&#65288;VC&#65289;&#21644;&#25104;&#38271;&#36164;&#26412;&#65288;GC&#65289;&#23547;&#25214;&#25237;&#36164;&#30446;&#26631;&#65288;&#21363;&#20844;&#21496;&#65289;&#26041;&#38754;&#12290;&#25105;&#20204;&#23545;&#30456;&#20851;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;&#65288;TMTSC&#65289;&#26469;&#39044;&#27979;&#20219;&#20309;&#20505;&#36873;&#20844;&#21496;&#30340;&#25104;&#21151;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#36890;&#36807;&#23558;&#23547;&#25214;&#25237;&#36164;&#38382;&#39064;&#27491;&#24335;&#23450;&#20041;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#20248;&#21270;VC&#21644;GC&#25237;&#36164;&#30340;&#23547;&#25214;&#25928;&#26524;&#12290;&#25105;&#20204;&#20381;&#27425;&#20171;&#32461;&#20102;&#25105;&#20204;&#23454;&#29616;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#37096;&#20998;&#20849;&#21516; contribut &#21040;&#20102;&#22312;VC/GC&#23547;&#25214;&#20013;&#25104;&#21151;&#24212;&#29992;TMTSC&#65306;&#36755;&#20837;&#29305;&#24449;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#20197;&#21450;&#22522;&#20110;&#25237;&#36164;&#32773;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#21010;&#20998;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#19977;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the growing application of data-driven approaches within the Private Equity (PE) industry, particularly in sourcing investment targets (i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present a comprehensive review of the relevant approaches and propose a novel approach leveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for predicting the success likelihood of any candidate company. The objective of our research is to optimize sourcing performance for VC and GC investments by formally defining the sourcing problem as a multivariate time series classification task. We consecutively introduce the key components of our implementation which collectively contribute to the successful application of TMTSC in VC/GC sourcing: input features, model architecture, optimization target, and investor-centric data augmentation and split. Our extensive experiments on four datasets, benchmarked towards three popular baselines, demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.06908</link><description>&lt;p&gt;
&#36208;&#21521;TopMost&#65306;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Towards the TopMost: A Topic Modeling System Toolkit. (arXiv:2309.06908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TopMost&#30340;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65292;&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#21644;&#20855;&#26377;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#24050;&#32463;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#34987;&#25552;&#20986;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#22312;&#31070;&#32463;&#21464;&#20998;&#25512;&#26029;&#30340;&#25512;&#21160;&#19979;&#36817;&#26399;&#24471;&#21040;&#20102;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20027;&#39064;&#27169;&#22411;&#37319;&#29992;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24555;&#36895;&#21033;&#29992;&#21644;&#20844;&#24179;&#27604;&#36739;&#12290;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20027;&#39064;&#24314;&#27169;&#31995;&#32479;&#24037;&#20855;&#21253;&#65288;TopMost&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;TopMost&#36890;&#36807;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#20027;&#39064;&#24314;&#27169;&#22330;&#26223;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#12289;&#27169;&#22411;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#30340;&#23436;&#25972;&#29983;&#21629;&#21608;&#26399;&#65292;&#33073;&#39062;&#32780;&#20986;&#12290;TopMost&#30340;&#39640;&#24230;&#20957;&#32858;&#21147;&#21644;&#35299;&#32806;&#27169;&#22359;&#21270;&#35774;&#35745;&#21487;&#20197;&#24555;&#36895;&#21033;&#29992;&#65292;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#28789;&#27963;&#25193;&#23637;&#19981;&#21516;&#30340;&#20027;&#39064;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#20419;&#36827;&#20027;&#39064;&#27169;&#22411;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25945;&#31243;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/bobxwu/topmost &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models have been proposed for decades with various applications and recently refreshed by the neural variational inference. However, these topic models adopt totally distinct dataset, implementation, and evaluation settings, which hinders their quick utilization and fair comparisons. This greatly hinders the research progress of topic models. To address these issues, in this paper we propose a Topic Modeling System Toolkit (TopMost). Compared to existing toolkits, TopMost stands out by covering a wider range of topic modeling scenarios including complete lifecycles with dataset pre-processing, model training, testing, and evaluations. The highly cohesive and decoupled modular design of TopMost enables quick utilization, fair comparisons, and flexible extensions of different topic models. This can facilitate the research and applications of topic models. Our code, tutorials, and documentation are available at https://github.com/bobxwu/topmost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06255</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#22686;&#24378;&#22810;&#27169;&#24577;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24322;&#36136;&#20449;&#24687;&#20849;&#21516;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21327;&#20316;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#23613;&#20154;&#24847;&#30340;&#38382;&#39064;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#20849;&#21516;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#12290;&#19968;&#20123;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#35782;&#21035;&#21644;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#36739;&#24046;&#30340;&#27169;&#24577;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#23545;&#26679;&#26412;&#32423;&#21035;&#22810;&#27169;&#24577;&#21327;&#20316;&#30340;&#32454;&#31890;&#24230;&#35266;&#23519;&#21644;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#35266;&#23519;&#21644;&#25913;&#36827;&#27169;&#24577;&#20043;&#38388;&#32454;&#31890;&#24230;&#30340;&#21327;&#20316;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#27169;&#24577;&#24046;&#24322;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#21487;&#33021;&#21464;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#26102;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#27169;&#24577;&#35780;&#20272;&#65292;&#25105;&#20204;&#36951;&#25022;&#22320;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this iss
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35757;&#32451;&#20998;&#35299;&#25581;&#31034;&#20302;&#31209;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#22823;&#27169;&#22411;&#26102;&#30340;&#32791;&#26102;&#21644;&#36164;&#28304;&#28040;&#32791;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.14929</link><description>&lt;p&gt;
Maestro: &#36890;&#36807;&#21487;&#35757;&#32451;&#20998;&#35299;&#25581;&#31034;&#20302;&#31209;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Maestro: Uncovering Low-Rank Structures via Trainable Decomposition. (arXiv:2308.14929v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35757;&#32451;&#20998;&#35299;&#25581;&#31034;&#20302;&#31209;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#22823;&#27169;&#22411;&#26102;&#30340;&#32791;&#26102;&#21644;&#36164;&#28304;&#28040;&#32791;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#36817;&#24180;&#26469;&#25512;&#21160;&#21644;&#20419;&#25104;&#20102;&#20154;&#24037;&#26234;&#33021;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#24182;&#24212;&#23545;&#26032;&#20852;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#21253;&#25324;AR/VR&#21644;&#26234;&#33021;&#21161;&#25163;&#65292;&#36825;&#20123;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#32791;&#26102;&#36153;&#21147;&#65292;&#36890;&#24120;&#21482;&#33021;&#29983;&#25104;&#19968;&#20010;&#36866;&#24212;&#25152;&#26377;&#30446;&#26631;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#27169;&#22411;&#26435;&#37325;&#21644;&#26356;&#26032;&#30340;&#21098;&#26525;&#12289;&#31232;&#30095;&#21270;&#25110;&#37327;&#21270;&#12290;&#34429;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;&#65292;&#20294;&#24448;&#24448;&#20250;&#36896;&#25104;&#35745;&#31639;&#24320;&#38144;&#25110;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#21478;&#22806;&#65292;&#36824;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#26041;&#27861;&#23558;&#20302;&#31209;&#21387;&#32553;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;(&#20363;&#22914;SVD)&#24120;&#24120;&#20381;&#36182;&#20110;&#35745;&#31639;&#26114;&#36149;&#30340;&#23618;&#27425;&#20998;&#35299;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#22914;DNNs&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#20998;&#35299;&#25581;&#31034;&#20302;&#31209;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have been a large driver and enabler for AI breakthroughs in recent years. These models have been getting larger in their attempt to become more accurate and tackle new upcoming use-cases, including AR/VR and intelligent assistants. However, the training process of such large models is a costly and time-consuming process, which typically yields a single model to fit all targets. To mitigate this, various techniques have been proposed in the literature, including pruning, sparsification or quantization of the model weights and updates. While able to achieve high compression rates, they often incur computational overheads or accuracy penalties. Alternatively, factorization methods have been leveraged to incorporate low-rank compression in the training process. Similarly, such techniques (e.g.,~SVD) frequently rely on the computationally expensive decomposition of layers and are potentially sub-optimal for non-linear models, such as DNNs. In this work, we take 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20154;&#33080;&#22270;&#20687;&#21464;&#24418;&#21644;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#33080;&#21464;&#24418;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13888</link><description>&lt;p&gt;
&#20154;&#33080;&#22270;&#20687;&#30340;&#31070;&#32463;&#38544;&#24335;&#24418;&#21464;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Morphing of Face Images. (arXiv:2308.13888v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20154;&#33080;&#22270;&#20687;&#21464;&#24418;&#21644;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#33080;&#21464;&#24418;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#21464;&#24418;&#26159;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20855;&#26377;&#20247;&#22810;&#33402;&#26415;&#21644;&#21462;&#35777;&#24212;&#29992;&#12290;&#30001;&#20110;&#23039;&#24577;&#12289;&#20809;&#29031;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30340;&#21464;&#21270;&#65292;&#23427;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#36825;&#20010;&#20219;&#21153;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#30340;&#21464;&#24418;&#21644;&#26080;&#32541;&#36807;&#28193;&#30340;&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#22352;&#26631;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#20154;&#33080;&#22270;&#20687;&#30340;&#36825;&#31181;&#21464;&#24418;&#21644;&#28151;&#21512;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31163;&#25955;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26102;&#38388;&#20381;&#36182;&#30340;&#65292;&#20801;&#35768;&#23545;&#30446;&#26631;&#22270;&#20687;&#36827;&#34892;&#36830;&#32493;&#30340;&#21464;&#24418;&#21644;&#28151;&#21512;&#12290;&#22312;&#21464;&#24418;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#26102;&#38388;&#20381;&#36182;&#21464;&#24418;&#30340;&#30452;&#25509;&#21644;&#36870;&#21464;&#25442;&#12290;&#21069;&#32773;&#36127;&#36131;&#23558;&#30446;&#26631;&#22270;&#20687;&#21464;&#24418;&#20026;&#28304;&#22270;&#20687;&#65292;&#32780;&#21518;&#32773;&#21017;&#29992;&#20110;&#22312;&#30456;&#21453;&#26041;&#21521;&#36827;&#34892;&#21464;&#24418;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#21464;&#24418;&#32593;&#32476;&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face morphing is one of the seminal problems in computer graphics, with numerous artistic and forensic applications. It is notoriously challenging due to pose, lighting, gender, and ethnicity variations. Generally, this task consists of a warping for feature alignment and a blending for a seamless transition between the warped images.  We propose to leverage coordinate-based neural networks to represent such warpings and blendings of face images. During training, we exploit the smoothness and flexibility of such networks, by combining energy functionals employed in classical approaches without discretizations. Additionally, our method is time-dependent, allowing a continuous warping, and blending of the target images.  During warping inference, we need both direct and inverse transformations of the time-dependent warping. The first is responsible for morphing the target image into the source image, while the inverse is used for morphing in the opposite direction. Our neural warping sto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861; NeuralClothSim&#65292;&#20351;&#29992;&#34180;&#22771;&#29702;&#35770;&#21644;&#31070;&#32463;&#21464;&#24418;&#22330;&#36827;&#34892;&#34920;&#38754;&#28436;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#25361;&#25112;&#65292;&#20026;&#29289;&#29702;&#21512;&#29702;&#30340;&#24067;&#26009;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2308.12970</link><description>&lt;p&gt;
NeuralClothSim: &#31070;&#32463;&#21464;&#24418;&#22330;&#19982;Kirchhoff-Love&#34180;&#22771;&#29702;&#35770;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory. (arXiv:2308.12970v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861; NeuralClothSim&#65292;&#20351;&#29992;&#34180;&#22771;&#29702;&#35770;&#21644;&#31070;&#32463;&#21464;&#24418;&#22330;&#36827;&#34892;&#34920;&#38754;&#28436;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#25361;&#25112;&#65292;&#20026;&#29289;&#29702;&#21512;&#29702;&#30340;&#24067;&#26009;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#26009;&#27169;&#25311;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#25991;&#29486;&#20013;&#26377;&#22823;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24067;&#26009;&#27169;&#25311;&#22120;&#20135;&#29983;&#31526;&#21512;&#19981;&#21516;&#31867;&#22411;&#36793;&#30028;&#26465;&#20214;&#30340;&#36924;&#30495;&#24067;&#26009;&#21464;&#24418;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25805;&#20316;&#21407;&#29702;&#22312;&#20960;&#20010;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65306;&#23427;&#20204;&#22312;&#20855;&#26377;&#22266;&#23450;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#26174;&#24335;&#34920;&#38754;&#34920;&#31034;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#25191;&#34892;&#19968;&#31995;&#21015;&#31163;&#25955;&#21270;&#30340;&#26356;&#26032;&#65288;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#65289;&#65292;&#24182;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#29616;&#26377;&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#36890;&#24120;&#24182;&#19981;&#30452;&#35266;&#65292;&#36825;&#22312;&#23558;&#20854;&#38598;&#25104;&#21040;&#29616;&#20195;&#31070;&#32463;&#26550;&#26500;&#20013;&#26102;&#36896;&#25104;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#19978;&#36848;&#38480;&#21046;&#65292;&#26412;&#25991;&#20174;&#26681;&#26412;&#19978;&#20197;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#35270;&#35282;&#26469;&#32771;&#34385;&#29289;&#29702;&#21512;&#29702;&#30340;&#24067;&#26009;&#27169;&#25311;&#65292;&#24182;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralClothSim&#65292;&#21363;&#19968;&#31181;&#20351;&#29992;&#34180;&#22771;&#30340;&#26032;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861;&#65292;&#20854;&#20013;&#34920;&#38754;&#28436;&#21270;&#36890;&#36807;&#31070;&#32463;&#21464;&#24418;&#22330;&#31561;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloth simulation is an extensively studied problem, with a plethora of solutions available in computer graphics literature. Existing cloth simulators produce realistic cloth deformations that obey different types of boundary conditions. Nevertheless, their operational principle remains limited in several ways: They operate on explicit surface representations with a fixed spatial resolution, perform a series of discretised updates (which bounds their temporal resolution), and require comparably large amounts of storage. Moreover, back-propagating gradients through the existing solvers is often not straightforward, which poses additional challenges when integrating them into modern neural architectures. In response to the limitations mentioned above, this paper takes a fundamentally different perspective on physically-plausible cloth simulation and re-thinks this long-standing problem: We propose NeuralClothSim, i.e., a new cloth simulation approach using thin shells, in which surface ev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#34917;&#20607;&#36229;bolic PIDE&#25511;&#21046;&#30340;&#31070;&#32463;&#31639;&#23376;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#22522;&#26412;PDE&#25511;&#21046;&#32467;&#26524;&#21040;&#19968;&#20010;&#28041;&#21450;&#29366;&#24577;&#21644;&#31995;&#32479;&#24310;&#36831;&#30340;&#39640;&#32423;&#36229;bolic&#31867;&#12290;&#20351;&#29992;DeepONet&#36924;&#36817;&#31639;&#23376;&#65292;&#21487;&#20197;&#22312;&#26080;&#38480;&#32500;&#24230;&#19978;&#24314;&#31435;&#31283;&#23450;&#30340;&#38381;&#29615;&#21453;&#39304;&#65292;&#24182;&#24320;&#21457;&#20102;&#36924;&#36817;&#30340;&#35266;&#27979;&#32773;&#21644;&#36755;&#20986;&#21453;&#39304;&#23450;&#24459;&#12290;</title><link>http://arxiv.org/abs/2307.11436</link><description>&lt;p&gt;
&#24310;&#36831;&#34917;&#20607;&#36229;bolic PIDE&#25511;&#21046;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Neural Operators for Delay-Compensating Control of Hyperbolic PIDEs. (arXiv:2307.11436v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11436
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#34917;&#20607;&#36229;bolic PIDE&#25511;&#21046;&#30340;&#31070;&#32463;&#31639;&#23376;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#22522;&#26412;PDE&#25511;&#21046;&#32467;&#26524;&#21040;&#19968;&#20010;&#28041;&#21450;&#29366;&#24577;&#21644;&#31995;&#32479;&#24310;&#36831;&#30340;&#39640;&#32423;&#36229;bolic&#31867;&#12290;&#20351;&#29992;DeepONet&#36924;&#36817;&#31639;&#23376;&#65292;&#21487;&#20197;&#22312;&#26080;&#38480;&#32500;&#24230;&#19978;&#24314;&#31435;&#31283;&#23450;&#30340;&#38381;&#29615;&#21453;&#39304;&#65292;&#24182;&#24320;&#21457;&#20102;&#36924;&#36817;&#30340;&#35266;&#27979;&#32773;&#21644;&#36755;&#20986;&#21453;&#39304;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24341;&#20837;&#30340;DeepONet&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#20174;&#22522;&#26412;&#30340;&#36229;bolic&#21644;&#25311;bolic PDE&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#39640;&#32423;&#36229;bolic&#31867;&#65292;&#20854;&#20013;&#21253;&#25324;&#29366;&#24577;&#21644;&#31995;&#32479;&#36755;&#20986;&#25110;&#36755;&#20837;&#30340;&#24310;&#36831;&#12290;PDE&#21453;&#21521;&#35774;&#35745;&#20135;&#29983;&#30340;&#22686;&#30410;&#20989;&#25968;&#26159;&#38750;&#32447;&#24615;&#31639;&#23376;&#30340;&#36755;&#20986;&#65292;&#23558;&#31354;&#38388;&#22495;&#19978;&#30340;&#20989;&#25968;&#26144;&#23556;&#21040;&#31354;&#38388;&#22495;&#19978;&#30340;&#20989;&#25968;&#65292;&#20854;&#20013;&#35813;&#22686;&#30410;&#29983;&#25104;&#31639;&#23376;&#30340;&#36755;&#20837;&#26159;PDE&#30340;&#31995;&#25968;&#12290;&#20351;&#29992;DeepONet&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#35813;&#31639;&#23376;&#65292;&#21487;&#20197;&#35777;&#26126;&#20854;&#20219;&#24847;&#31934;&#24230;&#32039;&#23494;&#12290;&#19968;&#26086;&#22312;&#26080;&#38480;&#32500;&#24230;&#19978;&#20135;&#29983;&#20102;&#36825;&#20010;&#36924;&#36817;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#22312;&#20351;&#29992;&#36817;&#20284;&#22686;&#30410;&#30340;&#21453;&#39304;&#19979;&#24314;&#31435;&#23553;&#38381;&#29615;&#30340;&#31283;&#23450;&#24615;&#12290;&#38500;&#20102;&#25552;&#20379;&#20840;&#29366;&#24577;&#21453;&#39304;&#19979;&#30340;&#36825;&#20123;&#32467;&#26524;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#23637;&#20102;DeepONet&#36924;&#36817;&#30340;&#35266;&#27979;&#32773;&#21644;&#36755;&#20986;&#21453;&#39304;&#23450;&#24459;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#31070;&#32463;&#31639;&#23376;&#36924;&#36817;&#19979;&#30340;&#31283;&#23450;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently introduced DeepONet operator-learning framework for PDE control is extended from the results for basic hyperbolic and parabolic PDEs to an advanced hyperbolic class that involves delays on both the state and the system output or input. The PDE backstepping design produces gain functions that are outputs of a nonlinear operator, mapping functions on a spatial domain into functions on a spatial domain, and where this gain-generating operator's inputs are the PDE's coefficients. The operator is approximated with a DeepONet neural network to a degree of accuracy that is provably arbitrarily tight. Once we produce this approximation-theoretic result in infinite dimension, with it we establish stability in closed loop under feedback that employs approximate gains. In addition to supplying such results under full-state feedback, we also develop DeepONet-approximated observers and output-feedback laws and prove their own stabilizing properties under neural operator approximations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#22270;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#22270;&#24418;&#35270;&#35282;&#26469;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03210</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Sparse Graphical Linear Dynamical Systems. (arXiv:2307.03210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#22270;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#22270;&#24418;&#35270;&#35282;&#26469;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#20247;&#22810;&#39046;&#22495;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#12289;&#22320;&#29699;&#35266;&#27979;&#21644;&#32593;&#32476;&#20998;&#26512;&#12290;&#26377;&#20851;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#30340;&#24191;&#27867;&#30740;&#31350;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#25968;&#23398;&#24037;&#20855;&#65292;&#23427;&#20801;&#35768;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#27010;&#29575;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#23398;&#20064;&#12290;&#22312;SSMs&#20013;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#34987;&#35748;&#20026;&#26159;&#26368;&#22797;&#26434;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#20808;&#39564;&#30693;&#35782;&#30340;&#24341;&#20837;&#26082;&#21487;&#20197;&#31616;&#21270;&#35299;&#37322;&#20063;&#21487;&#20197;&#22797;&#26434;&#21270;&#25512;&#26029;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#35797;&#22270;&#22312;&#19968;&#20123;&#27169;&#22411;&#21442;&#25968;&#20013;&#21152;&#20837;&#22270;&#24418;&#35270;&#35282;&#65292;&#20294;&#23384;&#22312;&#26126;&#26174;&#30340;&#38480;&#21046;&#65292;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#24037;&#20855;&#26088;&#22312;&#21253;&#21547;&#38745;&#24577;&#20449;&#24687;&#25110;&#21160;&#24577;&#20449;&#24687;&#65292;&#20854;&#20013;&#38745;&#24577;&#20449;&#24687;&#20391;&#37325;&#20110;&#29420;&#31435;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#65288;&#20363;&#22914;&#65292;&#22270;&#24418;Lasso&#26041;&#27861;&#65289;&#65292;&#21160;&#24577;&#20449;&#24687;&#20391;&#37325;&#20110;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series datasets are central in numerous fields of science and engineering, such as biomedicine, Earth observation, and network analysis. Extensive research exists on state-space models (SSMs), which are powerful mathematical tools that allow for probabilistic and interpretable learning on time series. Estimating the model parameters in SSMs is arguably one of the most complicated tasks, and the inclusion of prior knowledge is known to both ease the interpretation but also to complicate the inferential tasks. Very recent works have attempted to incorporate a graphical perspective on some of those model parameters, but they present notable limitations that this work addresses. More generally, existing graphical modeling tools are designed to incorporate either static information, focusing on statistical dependencies among independent random variables (e.g., graphical Lasso approach), or dynamic information, emphasizing causal relationships among time series samples (e.g., graphical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;ARDR&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#20197;&#23558;PCA&#23884;&#20837;&#23436;&#20840;&#24674;&#22797;&#65292;&#36890;&#36807;&#31245;&#21152;&#20462;&#25913;&#21487;&#20197;&#29992;LLE&#22797;&#29616;ARDR&#23884;&#20837;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#25104;&#31435;&#65292;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2306.11898</link><description>&lt;p&gt;
&#26080;&#27861;&#35299;&#37322;&#30340;&#35299;&#37322;&#65306;&#35299;&#37322;tSNE&#21644;UMAP&#23884;&#20837;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unexplainable Explanations: Towards Interpreting tSNE and UMAP Embeddings. (arXiv:2306.11898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;ARDR&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#21487;&#20197;&#23558;PCA&#23884;&#20837;&#23436;&#20840;&#24674;&#22797;&#65292;&#36890;&#36807;&#31245;&#21152;&#20462;&#25913;&#21487;&#20197;&#29992;LLE&#22797;&#29616;ARDR&#23884;&#20837;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#25104;&#31435;&#65292;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#22312;&#31354;&#38388;&#35299;&#37322;&#20026;&#21560;&#24341;/&#25490;&#26021;&#38477;&#32500;&#65288;ARDR&#65289;&#26041;&#27861;&#65288;&#22914;tSNE&#21644;UMAP&#65289;&#24050;&#25104;&#20026;&#26631;&#20934;&#12290;&#36825;&#21462;&#20915;&#20110;2D&#34920;&#31034;&#20013;&#30340;&#32467;&#26500;&#19982;&#27169;&#22411;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#19968;&#33268;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#26410;&#32463;&#35777;&#26126;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;ARDR&#31639;&#27861;&#26159;&#21542;&#26377;&#20219;&#20309;&#25910;&#25947;&#20445;&#35777;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;ARDR&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#38477;&#32500;&#25216;&#26415;&#32852;&#31995;&#36215;&#26469;&#65292;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#21560;&#24341;&#21644;&#25490;&#26021;&#26469;&#23436;&#20840;&#24674;&#22797;PCA&#23884;&#20837;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#26524;&#31245;&#21152;&#20462;&#25913;&#65292;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65288;LLE&#65289;&#21487;&#20197;&#22797;&#29616;ARDR&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#20102;&#19968;&#31995;&#21015;&#29468;&#24819;&#65292;&#22914;&#26524;&#36825;&#20123;&#29468;&#24819;&#25104;&#31435;&#65292;&#23601;&#21487;&#20197;&#23558;2D&#23884;&#20837;&#20013;&#30340;&#32467;&#26500;&#24402;&#22240;&#20110;&#36755;&#20837;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has become standard to explain neural network latent spaces with attraction/repulsion dimensionality reduction (ARDR) methods like tSNE and UMAP. This relies on the premise that structure in the 2D representation is consistent with the structure in the model's latent space. However, this is an unproven assumption -- we are unaware of any convergence guarantees for ARDR algorithms. We work on closing this question by relating ARDR methods to classical dimensionality reduction techniques. Specifically, we show that one can fully recover a PCA embedding by applying attractions and repulsions onto a randomly initialized dataset. We also show that, with a small change, Locally Linear Embeddings (LLE) can reproduce ARDR embeddings. Finally, we formalize a series of conjectures that, if true, would allow one to attribute structure in the 2D embedding back to the input distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#35757;&#32451;&#21160;&#24577;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#23376;&#38598;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;ImageNet-1K&#21644;ImageNet-21K&#19978;&#23454;&#29616;&#20102;75&#65285;&#30340;&#26080;&#25439;&#21387;&#32553;&#27604;&#12290;</title><link>http://arxiv.org/abs/2306.05175</link><description>&lt;p&gt;
&#21160;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large-scale Dataset Pruning with Dynamic Uncertainty. (arXiv:2306.05175v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#35757;&#32451;&#21160;&#24577;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#23376;&#38598;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;ImageNet-1K&#21644;ImageNet-21K&#19978;&#23454;&#29616;&#20102;75&#65285;&#30340;&#26080;&#25439;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#25910;&#38598;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#24182;&#22312;&#20854;&#19978;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#26159;&#25512;&#21160;&#25216;&#26415;&#21069;&#36827;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#35745;&#31639;&#25104;&#26412;&#36880;&#28176;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20462;&#21098;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#23376;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#22797;&#26434;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#19988;&#24615;&#33021;&#19979;&#38477;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#35757;&#32451;&#21160;&#24577;&#26469;&#23454;&#29616;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;ImageNet-1K&#21644;ImageNet-21K&#65289;&#21644;&#20808;&#36827;&#27169;&#22411;&#65288;Swin Transformer&#21644;ConvNeXt&#65289;&#19978;&#30740;&#31350;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#24037;&#20316;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;ImageNet-1K&#21644;ImageNet-21K&#19978;&#23454;&#29616;&#20102;75&#65285;&#30340;&#26080;&#25439;&#21387;&#32553;&#27604;&#12290;&#20195;&#30721;&#21644;&#20462;&#21098;&#21518;&#30340;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/BAAI-DCAI/Dataset-Pruning&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state of the art of many learning tasks, e.g., image classification, is advanced by collecting larger datasets and then training larger models on them. As the outcome, the increasing computational cost is becoming unaffordable. In this paper, we investigate how to prune the large-scale datasets, and thus produce an informative subset for training sophisticated deep models with negligible performance drop. We propose a simple yet effective dataset pruning method by exploring both the prediction uncertainty and training dynamics. To our knowledge, this is the first work to study dataset pruning on large-scale datasets, i.e., ImageNet-1K and ImageNet-21K, and advanced models, i.e., Swin Transformer and ConvNeXt. Extensive experimental results indicate that our method outperforms the state of the art and achieves 75% lossless compression ratio on both ImageNet-1K and ImageNet-21K. The code and pruned datasets are available at https://github.com/BAAI-DCAI/Dataset-Pruning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#23558;&#25152;&#26377;&#39044;&#27979;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#35813;&#29616;&#35937;&#23545;&#26550;&#26500;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#26377;&#23454;&#38469;&#25351;&#23548;&#24847;&#20041;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#21644;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.00809</link><description>&lt;p&gt;
&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#65306;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#26576;&#20123;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Initial Guessing Bias: How Untrained Networks Favor Some Classes. (arXiv:2306.00809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#23558;&#25152;&#26377;&#39044;&#27979;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#35813;&#29616;&#35937;&#23545;&#26550;&#26500;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#26377;&#23454;&#38469;&#25351;&#23548;&#24847;&#20041;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#21644;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#29366;&#24577;&#22312;&#35843;&#33410;&#21518;&#32493;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21487;&#20197;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#29978;&#33267;&#22312;&#19981;&#23384;&#22312;&#26174;&#24335;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#27169;&#22411;&#23558;&#25152;&#26377;&#39044;&#27979;&#37117;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#23384;&#22312;&#65292;&#31216;&#20026;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#65288;Initial Guessing Bias&#65292;IGB&#65289;&#65292;&#36825;&#21462;&#20915;&#20110;&#26550;&#26500;&#36873;&#25321;&#65292;&#20363;&#22914;&#28608;&#27963;&#20989;&#25968;&#12289;&#26368;&#22823;&#27744;&#21270;&#23618;&#21644;&#32593;&#32476;&#28145;&#24230;&#12290;&#25105;&#20204;&#23545;IGB&#36827;&#34892;&#30340;&#20998;&#26512;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#21487;&#20197;&#25351;&#23548;&#26550;&#26500;&#30340;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#12289;&#33258;&#24179;&#22343;&#30340;&#30772;&#22351;&#12289;&#26576;&#20123;&#22343;&#22330;&#36817;&#20284;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The initial state of neural networks plays a central role in conditioning the subsequent training dynamics. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a neural network can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We show that the presence of this phenomenon, which we call "Initial Guessing Bias" (IGB), depends on architectural choices such as activation functions, max-pooling layers, and network depth. Our analysis of IGB has practical consequences, in that it guides architecture selection and initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging, the validity of some mean-field approximations, and the non-trivial differences arising with depth.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11616</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#28145;&#24230;&#38598;&#25104;&#65306;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#21644; OOD &#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#25104;&#20013;&#23398;&#20064;&#30340;&#27169;&#24335;&#30340;&#21516;&#36136;&#24615;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20419;&#36827;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#12290;&#36890;&#36807;&#25972;&#21512;&#26174;&#33879;&#24615;&#22270;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#26657;&#20934;&#24615;&#12290;&#22312;&#24050;&#24314;&#31435;&#30340;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
&lt;/p&gt;</description></item><item><title>SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;</title><link>http://arxiv.org/abs/2301.11329</link><description>&lt;p&gt;
SynthMorph&#23454;&#29616;&#30340;&#32771;&#34385;&#35299;&#21078;&#32467;&#26500;&#21644;&#26080;&#20851;&#37319;&#38598;&#26041;&#27861;&#30340;&#32852;&#21512;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
Anatomy-aware and acquisition-agnostic joint registration with SynthMorph. (arXiv:2301.11329v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11329
&lt;/p&gt;
&lt;p&gt;
SynthMorph&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#19978;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#37319;&#29992;&#20102;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#22270;&#20687;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#23556;&#22270;&#20687;&#37197;&#20934;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#22522;&#30707;&#12290;&#34429;&#28982;&#20256;&#32479;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#20248;&#31168;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#20026;&#27599;&#19968;&#23545;&#22270;&#20687;&#36827;&#34892;&#32791;&#26102;&#30340;&#20248;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23558;&#22270;&#20687;&#23545;&#26144;&#23556;&#21040;&#36755;&#20986;&#21464;&#25442;&#30340;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20010;&#20989;&#25968;&#26159;&#24555;&#36895;&#30340;&#65292;&#20294;&#25429;&#25417;&#22823;&#30340;&#21464;&#25442;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#19988;&#22914;&#26524;&#27979;&#35797;&#22270;&#20687;&#30340;&#29305;&#24449;&#20174;&#35757;&#32451;&#39046;&#22495;&#21464;&#21270;&#65292;&#22914;&#20998;&#36776;&#29575;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#20986;&#29616;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;&#20223;&#23556;&#26041;&#27861;&#26159;&#23545;&#35299;&#21078;&#32467;&#26500;&#26080;&#30693;&#30340;&#65292;&#24847;&#21619;&#30528;&#22914;&#26524;&#31639;&#27861;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#25152;&#26377;&#32467;&#26500;&#65292;&#37197;&#20934;&#20250;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#36890;&#36807;SynthMorph&#35299;&#20915;&#20102;&#36825;&#20123;&#32570;&#28857;&#65292;&#23427;&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;DL&#24037;&#20855;&#65292;&#29992;&#20110;&#23545;&#20219;&#20309;&#33041;&#22270;&#20687;&#36827;&#34892;&#32852;&#21512;&#20223;&#23556;-&#21487;&#21464;&#24418;&#37197;&#20934;&#65292;&#26080;&#38656;&#39044;&#22788;&#29702;&#21363;&#21487;&#30452;&#25509;&#20174;MRI&#25195;&#25551;&#20202;&#36827;&#34892;&#25805;&#20316;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;&#26631;&#31614;&#22270;&#29983;&#25104;&#30340;&#20855;&#26377;&#26497;&#22823;&#24046;&#24322;&#30340;&#22270;&#20687;&#26469;&#35757;&#32451;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#30340;&#22810;&#26679;&#21270;&#37319;&#38598;&#35268;&#33539;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#32771;&#34385;&#19981;&#21516;&#30340;&#35299;&#21078;&#29305;&#24449;&#21644;&#23398;&#20064;&#25269;&#21046;&#37319;&#38598;&#29305;&#23450;&#38480;&#21046;&#30340;&#21464;&#25442;&#12290;&#36890;&#36807;&#36825;&#20123;&#21019;&#26032;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#40065;&#26834;&#30340;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affine image registration is a cornerstone of medical-image analysis. While classical algorithms can achieve excellent accuracy, they solve a time-consuming optimization for every image pair. Deep-learning (DL) methods learn a function that maps an image pair to an output transform. Evaluating the function is fast, but capturing large transforms can be challenging, and networks tend to struggle if a test-image characteristic shifts from the training domain, such as resolution. Most affine methods are agnostic to anatomy, meaning the registration will be inaccurate if algorithms consider all structures in the image.  We address these shortcomings with SynthMorph, an easy-to-use DL tool for joint affine-deformable registration of any brain image without preprocessing, right off the MRI scanner. First, we leverage a strategy to train networks with wildly varying images synthesized from label maps, yielding robust performance across acquisition specifics unseen at training. Second, we opti
&lt;/p&gt;</description></item><item><title>L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.14402</link><description>&lt;p&gt;
L2XGNN&#65306;&#23398;&#20064;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
L2XGNN: Learning to Explain Graph Neural Networks. (arXiv:2209.14402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14402
&lt;/p&gt;
&lt;p&gt;
L2XGNN&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#23454;&#29616;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#65292;&#24182;&#23454;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#23398;&#20064;&#35299;&#37322;&#65288;L2X&#65289;&#33539;&#24335;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2XGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#20379;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;L2XGNN&#23398;&#20064;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#36873;&#25321;&#35299;&#37322;&#23376;&#22270;&#65288;&#27169;&#20307;&#65289;&#65292;&#36825;&#20123;&#23376;&#22270;&#20165;&#29992;&#20110;GNN&#30340;&#20449;&#24687;&#20256;&#36882;&#25805;&#20316;&#20013;&#12290;&#23545;&#27169;&#20307;&#26045;&#21152;&#36825;&#26679;&#30340;&#38480;&#21046;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#26131;&#35299;&#37322;&#21644;&#26356;&#26377;&#25928;&#30340;&#35299;&#37322;&#12290;&#22312;&#20960;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;L2XGNN&#23454;&#29616;&#20102;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#21516;&#30340;&#20998;&#31867;&#31934;&#24230;&#65292;&#21516;&#26102;&#30830;&#20445;&#20165;&#20351;&#29992;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;L2XGNN&#33021;&#22815;&#35782;&#21035;&#36127;&#36131;&#39044;&#27979;&#22270;&#23646;&#24615;&#30340;&#27169;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a framework for explainable GNNs which provides faithful explanations by design. L2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2XGNN is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2XGNN achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2XGNN is able to identify motifs responsible for the graph's properties it is intended to predict.
&lt;/p&gt;</description></item></channel></rss>