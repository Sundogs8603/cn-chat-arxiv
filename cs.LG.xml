<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#30340;&#26412;&#20307;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#23398;&#29305;&#24615;&#21644;&#24341;&#20837;&#23884;&#20837;&#24335;&#35270;&#35273;&#65292;&#23454;&#29616;&#20102;&#22312;&#29289;&#29702;&#20132;&#20114;&#20013;&#30340;&#33258;&#36866;&#24212;&#21644;&#31896;&#24377;&#24615;&#24863;&#35273;&#65292;&#21487;&#20197;&#23454;&#26102;&#25512;&#26029;6&#32500;&#21147;&#21644;&#25197;&#30697;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.08538</link><description>&lt;p&gt;
&#20351;&#29992;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#30340;&#26412;&#20307;&#24863;&#30693;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Proprioceptive Learning with Soft Polyhedral Networks. (arXiv:2308.08538v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#30340;&#26412;&#20307;&#24863;&#30693;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#23398;&#29305;&#24615;&#21644;&#24341;&#20837;&#23884;&#20837;&#24335;&#35270;&#35273;&#65292;&#23454;&#29616;&#20102;&#22312;&#29289;&#29702;&#20132;&#20114;&#20013;&#30340;&#33258;&#36866;&#24212;&#21644;&#31896;&#24377;&#24615;&#24863;&#35273;&#65292;&#21487;&#20197;&#23454;&#26102;&#25512;&#26029;6&#32500;&#21147;&#21644;&#25197;&#30697;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#22791;&#23884;&#20837;&#24335;&#35270;&#35273;&#30340;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#20132;&#20114;&#20013;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#26412;&#20307;&#24863;&#30693;&#21644;&#31896;&#24377;&#24615;&#24863;&#35273;&#12290;&#35813;&#35774;&#35745;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#20840;&#21521;&#20132;&#20114;&#30340;&#34987;&#21160;&#36866;&#24212;&#65292;&#24182;&#36890;&#36807;&#20869;&#23884;&#30340;&#24494;&#22411;&#39640;&#36895;&#36816;&#21160;&#36319;&#36394;&#31995;&#32479;&#20197;&#35270;&#35273;&#26041;&#24335;&#25429;&#33719;&#26412;&#20307;&#24863;&#30693;&#30340;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36719;&#22810;&#38754;&#20307;&#32593;&#32476;&#33021;&#22815;&#20197;0.25/0.24/0.35 N&#21644;0.025/0.034/0.006 Nm&#30340;&#31934;&#24230;&#23454;&#26102;&#25512;&#26029;6&#32500;&#21147;&#21644;&#25197;&#30697;&#22312;&#21160;&#24577;&#20132;&#20114;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#28155;&#21152;&#31896;&#24377;&#24615;&#24863;&#21463;&#24615;&#26469;&#22312;&#38745;&#24577;&#36866;&#24212;&#20013;&#22686;&#21152;&#26412;&#20307;&#24863;&#30693;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#39044;&#27979;&#32467;&#26524;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proprioception is the "sixth sense" that detects limb postures with motor neurons. It requires a natural integration between the musculoskeletal systems and sensory receptors, which is challenging among modern robots that aim for lightweight, adaptive, and sensitive designs at a low cost. Here, we present the Soft Polyhedral Network with an embedded vision for physical interactions, capable of adaptive kinesthesia and viscoelastic proprioception by learning kinetic features. This design enables passive adaptations to omni-directional interactions, visually captured by a miniature high-speed motion tracking system embedded inside for proprioceptive learning. The results show that the soft network can infer real-time 6D forces and torques with accuracies of 0.25/0.24/0.35 N and 0.025/0.034/0.006 Nm in dynamic interactions. We also incorporate viscoelasticity in proprioception during static adaptation by adding a creep and relaxation modifier to refine the predicted results. The proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2308.08536</link><description>&lt;p&gt;
Transformers&#33021;&#21542;&#23398;&#20064;&#29992;&#20110;&#26410;&#30693;&#31995;&#32479;&#30340;&#26368;&#20248;&#28388;&#27874;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;transformer&#26469;&#22312;&#26410;&#30693;&#31995;&#32479;&#19978;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#21629;&#21517;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#65292;&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;transformers&#36827;&#34892;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#38382;&#39064;&#65292;&#23427;&#20351;&#29992;&#36807;&#21435;&#30340;&#25152;&#26377;&#36755;&#20986;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20808;&#39564;&#20998;&#24067;&#30340;&#21508;&#31181;&#31995;&#32479;&#26469;&#35757;&#32451;transformer&#65292;&#28982;&#21518;&#22312;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#30456;&#21516;&#20998;&#24067;&#30340;&#31995;&#32479;&#19978;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33719;&#24471;&#30340;transformer&#23601;&#20687;&#19968;&#20010;&#39044;&#27979;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#24182;&#24555;&#36895;&#36866;&#24212;&#21644;&#39044;&#27979;&#19981;&#21516;&#30340;&#31995;&#32479;&#65292;&#22240;&#27492;&#25105;&#20204;&#31216;&#20043;&#20026;&#20803;&#36755;&#20986;&#39044;&#27979;&#22120;&#65288;MOP&#65289;&#12290;&#23613;&#31649;MOP&#27809;&#26377;&#35775;&#38382;&#27169;&#22411;&#30340;&#26435;&#38480;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#65292;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#26368;&#20248;&#36755;&#20986;&#20272;&#35745;&#22120;&#30456;&#24403;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;MOP&#22312;&#20855;&#26377;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22122;&#22768;&#21644;&#26102;&#21464;&#21160;&#24577;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#20063;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have demonstrated remarkable success in natural language processing; however, their potential remains mostly unexplored for problems arising in dynamical systems. In this work, we investigate the optimal output estimation problem using transformers, which generate output predictions using all the past ones. We train the transformer using various systems drawn from a prior distribution and then evaluate its performance on previously unseen systems from the same distribution. As a result, the obtained transformer acts like a prediction algorithm that learns in-context and quickly adapts to and predicts well for different systems - thus we call it meta-output-predictor (MOP). MOP matches the performance of the optimal output estimator, based on Kalman filter, for most linear dynamical systems even though it does not have access to a model. We observe via extensive numerical experiments that MOP also performs well in challenging scenarios with non-i.i.d. noise, time-varying dy
&lt;/p&gt;</description></item><item><title>Painter&#26159;&#19968;&#31181;&#25945;&#23548;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#32472;&#21046;&#32032;&#25551;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#34394;&#25311;&#31508;&#35302;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#21270;&#25104;&#22270;&#20687;&#65292;&#24182;&#20855;&#26377;&#38500;&#21435;&#12289;&#26816;&#27979;&#21644;&#20998;&#31867;&#23545;&#35937;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08520</link><description>&lt;p&gt;
Painter: &#25945;&#23548;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#32472;&#21046;&#32032;&#25551;
&lt;/p&gt;
&lt;p&gt;
Painter: Teaching Auto-regressive Language Models to Draw Sketches. (arXiv:2308.08520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08520
&lt;/p&gt;
&lt;p&gt;
Painter&#26159;&#19968;&#31181;&#25945;&#23548;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#32472;&#21046;&#32032;&#25551;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#34394;&#25311;&#31508;&#35302;&#23558;&#25991;&#26412;&#25551;&#36848;&#36716;&#21270;&#25104;&#22270;&#20687;&#65292;&#24182;&#20855;&#26377;&#38500;&#21435;&#12289;&#26816;&#27979;&#21644;&#20998;&#31867;&#23545;&#35937;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#23427;&#20204;&#20063;&#25104;&#21151;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#26426;&#22120;&#20154;&#12289;&#24378;&#21270;&#23398;&#20064;&#31561;&#20854;&#20182;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#34394;&#25311;&#30340;&#31508;&#35302;&#26469;&#32472;&#21046;&#22270;&#20687;&#65292;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Painter&#65292;&#19968;&#31181;LLM&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#30456;&#24212;&#30340;&#31508;&#35302;&#65292;&#23558;&#29992;&#25143;&#30340;&#25991;&#26412;&#25551;&#36848;&#36716;&#21270;&#20026;&#32032;&#25551;&#12290;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#29616;&#25104;LLM&#26500;&#24314;&#20102;Painter&#65292;&#22312;&#20445;&#30041;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#22312;&#26032;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#24212;&#29992;&#23427;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#23545;&#35937;&#31867;&#22411;&#21644;&#20219;&#21153;&#30340;&#22810;&#23545;&#35937;&#32032;&#25551;&#19982;&#25991;&#26412;&#25552;&#31034;&#37197;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;Painter&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#32032;&#25551;&#65292;&#28165;&#38500;&#30011;&#24067;&#19978;&#30340;&#23545;&#35937;&#65292;&#24182;&#26816;&#27979;&#21644;&#20998;&#31867;&#32032;&#25551;&#20013;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;&#36825;&#26159;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#20351;&#29992;LLMs&#30340;&#21490;&#26080;&#21069;&#20363;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made tremendous progress in natural language understanding and they have also been successfully adopted in other domains such as computer vision, robotics, reinforcement learning, etc. In this work, we apply LLMs to image generation tasks by directly generating the virtual brush strokes to paint an image. We present Painter, an LLM that can convert user prompts in text description format to sketches by generating the corresponding brush strokes in an auto-regressive way. We construct Painter based on off-the-shelf LLM that is pre-trained on a large text corpus, by fine-tuning it on the new task while preserving language understanding capabilities. We create a dataset of diverse multi-object sketches paired with textual prompts that covers several object types and tasks. Painter can generate sketches from text descriptions, remove objects from canvas, and detect and classify objects in sketches. Although this is an unprecedented pioneering work in using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26500;&#24314;RadiologyNET&#30340;&#26041;&#27861;&#65306;&#21033;&#29992;&#22810;&#27169;&#24577;&#26469;&#28304;&#65288;&#22270;&#20687;&#12289;DICOM&#20803;&#25968;&#25454;&#21644;&#35786;&#26029;&#21465;&#36848;&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#27880;&#37322;&#20102;&#26469;&#33258;&#20811;&#32599;&#22320;&#20122;&#37324;&#32822;&#21345;&#20020;&#24202;&#21307;&#38498;&#20013;&#24515;&#30340;&#22823;&#35268;&#27169;&#21307;&#23398;&#25918;&#23556;&#23398;&#22270;&#20687;&#25968;&#25454;&#24211;&#12290;</title><link>http://arxiv.org/abs/2308.08517</link><description>&lt;p&gt;
&#26500;&#24314;RadiologyNET&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#24211;&#30340;&#26080;&#30417;&#30563;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Building RadiologyNET: Unsupervised annotation of a large-scale multimodal medical database. (arXiv:2308.08517v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26500;&#24314;RadiologyNET&#30340;&#26041;&#27861;&#65306;&#21033;&#29992;&#22810;&#27169;&#24577;&#26469;&#28304;&#65288;&#22270;&#20687;&#12289;DICOM&#20803;&#25968;&#25454;&#21644;&#35786;&#26029;&#21465;&#36848;&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#27880;&#37322;&#20102;&#26469;&#33258;&#20811;&#32599;&#22320;&#20122;&#37324;&#32822;&#21345;&#20020;&#24202;&#21307;&#38498;&#20013;&#24515;&#30340;&#22823;&#35268;&#27169;&#21307;&#23398;&#25918;&#23556;&#23398;&#22270;&#20687;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#19982;&#30446;&#26631;&#65306;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#22686;&#38271;&#65292;&#36890;&#36807;&#24320;&#21457;&#20381;&#36182;&#20110;&#27880;&#37322;&#21307;&#23398;&#25918;&#23556;&#23398;&#22270;&#20687;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#27880;&#37322;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#65292;&#22240;&#20026;&#27880;&#37322;&#36807;&#31243;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#26681;&#25454;&#35821;&#20041;&#30456;&#20284;&#24615;&#33258;&#21160;&#27880;&#37322;&#21307;&#23398;&#25918;&#23556;&#23398;&#22270;&#20687;&#25968;&#25454;&#24211;&#12290;&#26448;&#26009;&#19982;&#26041;&#27861;&#65306;&#37319;&#29992;&#19968;&#31181;&#33258;&#21160;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#26469;&#28304;&#65288;&#21253;&#25324;&#22270;&#20687;&#12289;DICOM&#20803;&#25968;&#25454;&#21644;&#35786;&#26029;&#21465;&#36848;&#65289;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26469;&#33258;&#20811;&#32599;&#22320;&#20122;&#37324;&#32822;&#21345;&#20020;&#24202;&#21307;&#38498;&#20013;&#24515;&#30340;&#22823;&#35268;&#27169;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#26469;&#28304;&#65292;&#27979;&#35797;&#20102;&#22810;&#20010;&#21512;&#36866;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#20351;&#29992;k-means&#21644;k-medoids&#32858;&#31867;&#35780;&#20272;&#20854;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background and objective: The usage of machine learning in medical diagnosis and treatment has witnessed significant growth in recent years through the development of computer-aided diagnosis systems that are often relying on annotated medical radiology images. However, the availability of large annotated image datasets remains a major obstacle since the process of annotation is time-consuming and costly. This paper explores how to automatically annotate a database of medical radiology images with regard to their semantic similarity.  Material and methods: An automated, unsupervised approach is used to construct a large annotated dataset of medical radiology images originating from Clinical Hospital Centre Rijeka, Croatia, utilising multimodal sources, including images, DICOM metadata, and narrative diagnoses. Several appropriate feature extractors are tested for each of the data sources, and their utility is evaluated using k-means and k-medoids clustering on a representative data sub
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65288;TOSM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;CT&#21644;MRI&#20013;&#30340;&#19977;&#32500;&#20307;&#37325;&#24314;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#26469;&#20943;&#23569;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#65292;&#28982;&#21518;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26356;&#26032;&#25968;&#25454;&#20998;&#24067;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#19977;&#32500;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2308.08511</link><description>&lt;p&gt;
&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#29992;&#20110;&#35299;&#20915;&#19977;&#32500;&#19981;&#36866;&#23450;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems. (arXiv:2308.08511v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08511
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65288;TOSM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;CT&#21644;MRI&#20013;&#30340;&#19977;&#32500;&#20307;&#37325;&#24314;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#26469;&#20943;&#23569;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#65292;&#28982;&#21518;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26356;&#26032;&#25968;&#25454;&#20998;&#24067;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#19977;&#32500;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#21644;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#26159;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#12290;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;CT&#21644;MRI&#20013;&#36935;&#21040;&#30340;&#19981;&#21516;&#21453;&#38382;&#39064;&#65288;&#22914;&#31232;&#30095;&#35270;&#37326;CT&#21644;&#24555;&#36895;MRI&#37325;&#24314;&#65289;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#19977;&#32500;&#20307;&#37325;&#24314;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#37325;&#24314;&#20108;&#32500;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#37325;&#24314;&#30340;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20013;&#23548;&#33268;&#30456;&#37051;&#20999;&#29255;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#20010;&#21322;&#39034;&#24207;&#22522;&#20110;&#24471;&#20998;&#30340;&#27169;&#22411;&#65288;TOSM&#65289;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;TOSM&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#30456;&#36739;&#20110;&#30452;&#25509;&#22312;&#19977;&#32500;&#20307;&#31215;&#19978;&#24037;&#20316;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#30340;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#37325;&#24314;&#38454;&#27573;&#65292;TOSM&#20250;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26356;&#26032;&#25968;&#25454;&#20998;&#24067;&#65292;&#21033;&#29992;&#19977;&#20010;&#26041;&#21521;&#30340;&#20114;&#34917;&#24471;&#20998;&#65288;sag&#65289;
&lt;/p&gt;
&lt;p&gt;
Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucial technologies in the field of medical imaging. Score-based models have proven to be effective in addressing different inverse problems encountered in CT and MRI, such as sparse-view CT and fast MRI reconstruction. However, these models face challenges in achieving accurate three dimensional (3D) volumetric reconstruction. The existing score-based models primarily focus on reconstructing two dimensional (2D) data distribution, leading to inconsistencies between adjacent slices in the reconstructed 3D volumetric images. To overcome this limitation, we propose a novel two-and-a-half order score-based model (TOSM). During the training phase, our TOSM learns data distributions in 2D space, which reduces the complexity of training compared to directly working on 3D volumes. However, in the reconstruction phase, the TOSM updates the data distribution in 3D space, utilizing complementary scores along three directions (sag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#25429;&#25417;&#21644;&#30417;&#30563;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SVAE&#65289;&#23398;&#20064;&#30340;&#26580;&#24615;&#26426;&#22120;&#25163;&#25351;&#65292;&#25506;&#32034;&#20102;&#20174;&#38470;&#22320;&#21040;&#27700;&#19979;&#30340;&#25235;&#21462;&#30693;&#35782;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30456;&#23545;&#20110;&#21830;&#19994;FT&#20256;&#24863;&#22120;&#20855;&#26377;&#26356;&#22909;&#36866;&#24212;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.08510</link><description>&lt;p&gt;
&#23545;&#23736;&#21040;&#27700;&#19979;&#65306;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#26580;&#36719;&#35302;&#25720;&#23398;&#20064;&#25235;&#21462;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Autoencoding a Soft Touch to Learn Grasping from On-land to Underwater. (arXiv:2308.08510v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#25429;&#25417;&#21644;&#30417;&#30563;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SVAE&#65289;&#23398;&#20064;&#30340;&#26580;&#24615;&#26426;&#22120;&#25163;&#25351;&#65292;&#25506;&#32034;&#20102;&#20174;&#38470;&#22320;&#21040;&#27700;&#19979;&#30340;&#25235;&#21462;&#30693;&#35782;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30456;&#23545;&#20110;&#21830;&#19994;FT&#20256;&#24863;&#22120;&#20855;&#26377;&#26356;&#22909;&#36866;&#24212;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22312;&#25506;&#32034;&#28023;&#27915;&#20013;&#25198;&#28436;&#30528;&#20154;&#31867;&#25805;&#20316;&#32773;&#30340;&#29289;&#29702;&#20195;&#29702;&#30340;&#20851;&#38190;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#21387;&#27700;&#29615;&#22659;&#19979;&#23436;&#20840;&#28024;&#27809;&#24182;&#19988;&#21463;&#21040;&#23569;&#37327;&#21487;&#35265;&#20809;&#24433;&#21709;&#26102;&#65292;&#21487;&#38752;&#22320;&#25235;&#21462;&#29289;&#20307;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#25351;&#23574;&#19982;&#29289;&#20307;&#34920;&#38754;&#20043;&#38388;&#30340;&#27969;&#20307;&#24178;&#25200;&#23545;&#35302;&#35273;&#21147;&#23398;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#35270;&#35273;&#30340;&#26580;&#24615;&#26426;&#22120;&#25163;&#25351;&#65292;&#20351;&#29992;&#30417;&#30563;&#24335;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SVAE&#65289;&#23398;&#20064;6D&#21147;&#21644;&#21147;&#30697;&#65288;FT&#65289;&#26469;&#25506;&#32034;&#20174;&#38470;&#22320;&#21040;&#27700;&#19979;&#30340;&#25235;&#21462;&#30693;&#35782;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#39640;&#24103;&#29575;&#25668;&#20687;&#26426;&#25429;&#25417;&#21040;&#26426;&#22120;&#25163;&#25351;&#22312;&#38470;&#22320;&#21644;&#27700;&#19979;&#19982;&#29289;&#29702;&#29289;&#20307;&#20114;&#21160;&#26102;&#30340;&#25972;&#20307;&#21464;&#24418;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#24471;&#21040;&#30340;SVAE&#27169;&#22411;&#23398;&#21040;&#20102;&#19968;&#31995;&#21015;&#21487;&#20174;&#38470;&#22320;&#36716;&#31227;&#21040;&#27700;&#19979;&#30340;&#26580;&#24615;&#21147;&#23398;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#30456;&#23545;&#21830;&#19994;FT&#20256;&#24863;&#22120;&#32780;&#35328;&#65292;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20855;&#26377;&#26356;&#22909;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#26580;&#36719;&#12289;&#31934;&#32454;&#21644;&#21453;&#24212;&#28789;&#25935;&#30340;&#25235;&#21462;&#65292;&#33021;&#22815;&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Robots play a critical role as the physical agent of human operators in exploring the ocean. However, it remains challenging to grasp objects reliably while fully submerging under a highly pressurized aquatic environment with little visible light, mainly due to the fluidic interference on the tactile mechanics between the finger and object surfaces. This study investigates the transferability of grasping knowledge from on-land to underwater via a vision-based soft robotic finger that learns 6D forces and torques (FT) using a Supervised Variational Autoencoder (SVAE). A high-framerate camera captures the whole-body deformations while a soft robotic finger interacts with physical objects on-land and underwater. Results show that the trained SVAE model learned a series of latent representations of the soft mechanics transferrable from land to water, presenting a superior adaptation to the changing environments against commercial FT sensors. Soft, delicate, and reactive grasping enabled by
&lt;/p&gt;</description></item><item><title>ResBuilder&#26159;&#19968;&#31181;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;ResNet&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21024;&#38500;&#21644;&#25554;&#20837;ResNet&#22359;&#65292;&#25628;&#32034;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.08504</link><description>&lt;p&gt;
ResBuilder: &#20351;&#29992;&#27531;&#24046;&#32467;&#26500;&#33258;&#21160;&#29983;&#25104;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ResBuilder: Automated Learning of Depth with Residual Structures. (arXiv:2308.08504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08504
&lt;/p&gt;
&lt;p&gt;
ResBuilder&#26159;&#19968;&#31181;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;ResNet&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21024;&#38500;&#21644;&#25554;&#20837;ResNet&#22359;&#65292;&#25628;&#32034;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;ResBuilder&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#22836;&#24320;&#22987;&#24320;&#21457;ResNet&#26550;&#26500;&#65292;&#20197;&#22312;&#20013;&#31561;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#12290;&#23427;&#36824;&#21487;&#20197;&#29992;&#20110;&#20462;&#25913;&#29616;&#26377;&#30340;&#26550;&#26500;&#65292;&#24182;&#20855;&#26377;&#21024;&#38500;&#21644;&#25554;&#20837;ResNet&#22359;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;ResNet&#26550;&#26500;&#31354;&#38388;&#20013;&#25628;&#32034;&#36866;&#21512;&#30340;&#26550;&#26500;&#12290;&#22312;&#25105;&#20204;&#23545;&#19981;&#21516;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;ResBuilder&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#33410;&#32422;&#20102;&#19982;&#29616;&#25104;&#30340;ResNet&#30456;&#27604;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;CIFAR10&#19978;&#35843;&#33410;&#20102;&#21442;&#25968;&#65292;&#24471;&#21040;&#20102;&#36866;&#29992;&#20110;&#25152;&#26377;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#21512;&#36866;&#40664;&#35748;&#36873;&#25321;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#23646;&#24615;&#29978;&#33267;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20063;&#26159;&#36866;&#29992;&#30340;&#65292;&#36890;&#36807;&#22312;&#19987;&#26377;&#30340;&#27450;&#35784;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#20351;&#29992;&#40664;&#35748;&#21442;&#25968;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we develop a neural architecture search algorithm, termed Resbuilder, that develops ResNet architectures from scratch that achieve high accuracy at moderate computational cost. It can also be used to modify existing architectures and has the capability to remove and insert ResNet blocks, in this way searching for suitable architectures in the space of ResNet architectures. In our experiments on different image classification datasets, Resbuilder achieves close to state-of-the-art performance while saving computational cost compared to off-the-shelf ResNets. Noteworthy, we once tune the parameters on CIFAR10 which yields a suitable default choice for all other datasets. We demonstrate that this property generalizes even to industrial applications by applying our method with default parameters on a proprietary fraud detection dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#36755;&#20837;&#29305;&#24449;&#26102;&#30340;&#38480;&#21046;&#65292;&#24182;&#36991;&#20813;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08502</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#29992;&#20110;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning based Stacked Regression Approach for Customer Lifetime Value Prediction. (arXiv:2308.08502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08502
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#36755;&#20837;&#29305;&#24449;&#26102;&#30340;&#38480;&#21046;&#65292;&#24182;&#36991;&#20813;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21508;&#22320;&#30340;&#20844;&#21496;&#37117;&#28212;&#26395;&#23450;&#20301;&#28508;&#22312;&#30340;&#39640;&#20215;&#20540;&#23458;&#25143;&#65292;&#20197;&#25193;&#22823;&#25910;&#20837;&#65292;&#32780;&#36825;&#21482;&#33021;&#36890;&#36807;&#26356;&#22909;&#22320;&#20102;&#35299;&#23458;&#25143;&#26469;&#23454;&#29616;&#12290;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#65288;CLV&#65289;&#26159;&#23458;&#25143;&#22312;&#19968;&#27573;&#35268;&#23450;&#30340;&#26102;&#38388;&#20869;&#19982;&#20225;&#19994;&#36827;&#34892;&#30340;&#20132;&#26131;/&#36141;&#20080;&#30340;&#24635;&#36135;&#24065;&#20215;&#20540;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#23458;&#25143;&#20114;&#21160;&#12290;CLV&#22312;&#22810;&#20010;&#19981;&#21516;&#30340;&#21830;&#19994;&#39046;&#22495;&#20013;&#37117;&#26377;&#24212;&#29992;&#65292;&#22914;&#38134;&#34892;&#65292;&#20445;&#38505;&#65292;&#22312;&#32447;&#23089;&#20048;&#65292;&#28216;&#25103;&#21644;&#30005;&#23376;&#21830;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#24067;&#21644;&#22522;&#26412;&#65288;&#26368;&#36817;&#24615;&#65292;&#39057;&#29575;&#21644;&#37329;&#39069;&#65289;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#36755;&#20837;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#26356;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#39046;&#22495;&#20013;&#22686;&#21152;&#20102;&#19981;&#24517;&#35201;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26082;&#33021;&#22815;&#26377;&#25928;&#21448;&#20840;&#38754;&#31616;&#21333;&#26131;&#25026;&#30340;&#31995;&#32479;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#23458;&#25143;&#32456;&#36523;&#20215;&#20540;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Companies across the globe are keen on targeting potential high-value customers in an attempt to expand revenue and this could be achieved only by understanding the customers more. Customer Lifetime Value (CLV) is the total monetary value of transactions/purchases made by a customer with the business over an intended period of time and is used as means to estimate future customer interactions. CLV finds application in a number of distinct business domains such as Banking, Insurance, Online-entertainment, Gaming, and E-Commerce. The existing distribution-based and basic (recency, frequency &amp; monetary) based models face a limitation in terms of handling a wide variety of input features. Moreover, the more advanced Deep learning approaches could be superfluous and add an undesirable element of complexity in certain application areas. We, therefore, propose a system which is able to qualify both as effective, and comprehensive yet simple and interpretable. With that in mind, we develop a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InTune&#65292;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#27969;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#22312;Netflix&#35745;&#31639;&#38598;&#32676;&#20013;&#30340;DLRM&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#27969;&#31243;&#20248;&#21270;&#22120;&#23384;&#22312;&#24615;&#33021;&#19981;&#20339;&#12289;&#39057;&#32321;&#23849;&#28291;&#25110;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#30340;&#38598;&#32676;&#37325;&#32452;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08500</link><description>&lt;p&gt;
InTune:&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#27969;&#20248;&#21270;&#29992;&#20110;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InTune: Reinforcement Learning-based Data Pipeline Optimization for Deep Recommendation Models. (arXiv:2308.08500v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InTune&#65292;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#27969;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#22312;Netflix&#35745;&#31639;&#38598;&#32676;&#20013;&#30340;DLRM&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#27969;&#31243;&#20248;&#21270;&#22120;&#23384;&#22312;&#24615;&#33021;&#19981;&#20339;&#12289;&#39057;&#32321;&#23849;&#28291;&#25110;&#38656;&#35201;&#19981;&#20999;&#23454;&#38469;&#30340;&#38598;&#32676;&#37325;&#32452;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#27169;&#22411;(DLRM)&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#19968;&#20123;&#20844;&#21496;&#27491;&#22312;&#24314;&#35774;&#22823;&#22411;&#35745;&#31639;&#38598;&#32676;&#19987;&#38376;&#29992;&#20110;DLRM&#35757;&#32451;&#65292;&#36827;&#32780;&#25512;&#21160;&#20102;&#23545;&#25104;&#26412;&#21644;&#26102;&#38388;&#30340;&#33410;&#32422;&#20248;&#21270;&#30340;&#26032;&#20852;&#20852;&#36259;&#12290;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#25152;&#38754;&#20020;&#30340;&#31995;&#32479;&#25361;&#25112;&#26159;&#29420;&#29305;&#30340;&#65307;&#23613;&#31649;&#20856;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20219;&#21153;&#30001;&#27169;&#22411;&#25191;&#34892;&#20027;&#23548;&#65292;&#20294;DLRM&#35757;&#32451;&#24615;&#33021;&#20013;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#24448;&#24448;&#26159;&#32447;&#19978;&#25968;&#25454;&#25668;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35813;&#25968;&#25454;&#25668;&#20837;&#38382;&#39064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;DLRM&#35757;&#32451;&#27969;&#31243;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;Netflix&#35745;&#31639;&#38598;&#32676;&#20013;&#30495;&#23454;&#30340;DLRM&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35266;&#23519;&#20102;&#32447;&#19978;&#25668;&#20837;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#24182;&#35782;&#21035;&#20986;&#29616;&#26377;&#27969;&#31243;&#20248;&#21270;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#24037;&#20855;&#35201;&#20040;&#20135;&#29983;&#27425;&#20248;&#24615;&#33021;&#65292;&#35201;&#20040;&#32463;&#24120;&#23849;&#28291;&#65292;&#35201;&#20040;&#38656;&#35201;&#19981;&#29616;&#23454;&#30340;&#38598;&#32676;&#37325;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based recommender models (DLRMs) have become an essential component of many modern recommender systems. Several companies are now building large compute clusters reserved only for DLRM training, driving new interest in cost- and time- saving optimizations. The systems challenges faced in this setting are unique; while typical deep learning training jobs are dominated by model execution, the most important factor in DLRM training performance is often online data ingestion.  In this paper, we explore the unique characteristics of this data ingestion problem and provide insights into DLRM training pipeline bottlenecks and challenges. We study real-world DLRM data processing pipelines taken from our compute cluster at Netflix to observe the performance impacts of online ingestion and to identify shortfalls in existing pipeline optimizers. We find that current tooling either yields sub-optimal performance, frequent crashes, or else requires impractical cluster re-organization 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#23457;&#35745;&#26085;&#24535;&#20316;&#20026;&#30417;&#30563;&#65292;&#23454;&#29616;&#22312;&#29305;&#23450;&#20020;&#24202;&#32972;&#26223;&#19979;&#12289;&#29305;&#23450;&#26102;&#38388;&#28857;&#30340;&#31508;&#35760;&#30456;&#20851;&#24615;&#26816;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20010;&#21035;&#31508;&#35760;&#25776;&#20889;&#20250;&#35805;&#20013;&#21738;&#20123;&#31508;&#35760;&#20250;&#34987;&#38405;&#35835;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20020;&#24202;&#21307;&#29983;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#39640;&#25928;&#22320;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.08494</link><description>&lt;p&gt;
&#29992;&#20110;&#21160;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20449;&#24687;&#26816;&#32034;&#30340;&#26426;&#22120;&#23398;&#20064;&#27010;&#24565;&#21270;
&lt;/p&gt;
&lt;p&gt;
Conceptualizing Machine Learning for Dynamic Information Retrieval of Electronic Health Record Notes. (arXiv:2308.08494v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08494
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#23457;&#35745;&#26085;&#24535;&#20316;&#20026;&#30417;&#30563;&#65292;&#23454;&#29616;&#22312;&#29305;&#23450;&#20020;&#24202;&#32972;&#26223;&#19979;&#12289;&#29305;&#23450;&#26102;&#38388;&#28857;&#30340;&#31508;&#35760;&#30456;&#20851;&#24615;&#26816;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20010;&#21035;&#31508;&#35760;&#25776;&#20889;&#20250;&#35805;&#20013;&#21738;&#20123;&#31508;&#35760;&#20250;&#34987;&#38405;&#35835;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20020;&#24202;&#21307;&#29983;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#39640;&#25928;&#22320;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#21307;&#29983;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#31579;&#36873;&#30149;&#20154;&#31508;&#35760;&#24182;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#35760;&#24405;&#26159;&#20020;&#24202;&#21307;&#29983;&#20518;&#24608;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#36890;&#36807;&#22312;&#35760;&#24405;&#36807;&#31243;&#20013;&#20027;&#21160;&#21644;&#21160;&#24577;&#22320;&#26816;&#32034;&#30456;&#20851;&#31508;&#35760;&#65292;&#25105;&#20204;&#21487;&#20197;&#20943;&#23569;&#26597;&#25214;&#30456;&#20851;&#30149;&#20363;&#21382;&#21490;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27010;&#24565;&#21270;&#20102;&#20351;&#29992;EHR&#23457;&#35745;&#26085;&#24535;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#26469;&#28304;&#65292;&#20197;&#30417;&#30563;&#29305;&#23450;&#20020;&#24202;&#32972;&#26223;&#19979;&#12289;&#29305;&#23450;&#26102;&#38388;&#28857;&#30340;&#31508;&#35760;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#25918;&#22312;&#32039;&#24613;&#31185;&#23460;&#30340;&#21160;&#24577;&#26816;&#32034;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#29420;&#29305;&#20449;&#24687;&#26816;&#32034;&#21644;&#31508;&#35760;&#32534;&#20889;&#27169;&#24335;&#30340;&#39640;&#37325;&#30151;&#35774;&#32622;&#12290;&#25105;&#20204;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#21738;&#20123;&#31508;&#35760;&#20250;&#22312;&#20010;&#21035;&#31508;&#35760;&#25776;&#20889;&#20250;&#35805;&#20013;&#34987;&#38405;&#35835;&#26041;&#38754;&#21487;&#20197;&#23454;&#29616;0.963&#30340;AUC&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#22810;&#21517;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#26356;&#39640;&#25928;&#22320;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The large amount of time clinicians spend sifting through patient notes and documenting in electronic health records (EHRs) is a leading cause of clinician burnout. By proactively and dynamically retrieving relevant notes during the documentation process, we can reduce the effort required to find relevant patient history. In this work, we conceptualize the use of EHR audit logs for machine learning as a source of supervision of note relevance in a specific clinical context, at a particular point in time. Our evaluation focuses on the dynamic retrieval in the emergency department, a high acuity setting with unique patterns of information retrieval and note writing. We show that our methods can achieve an AUC of 0.963 for predicting which notes will be read in an individual note writing session. We additionally conduct a user study with several clinicians and find that our framework can help clinicians retrieve relevant information more efficiently. Demonstrating that our framework and m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2308.08493</link><description>&lt;p&gt;
LLM&#20013;&#30340;&#26102;&#38388;&#26053;&#34892;&#65306;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08493
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#38543;&#26426;&#26679;&#26412;&#20013;&#30340;&#21333;&#20010;&#23454;&#20363;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#21450;&#20351;&#29992;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#26469;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#27745;&#26579;&#30340;&#23454;&#20363;&#21644;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#26159;&#25351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#29702;&#35299;LLMs&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26377;&#25928;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#36890;&#36807;&#35782;&#21035;&#20174;&#23567;&#30340;&#38543;&#26426;&#26679;&#26412;&#20013;&#25277;&#21462;&#30340;&#21333;&#20010;&#23454;&#20363;&#20013;&#30340;&#28508;&#22312;&#27745;&#26579;&#65292;&#28982;&#21518;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20272;&#35745;&#21333;&#20010;&#23454;&#20363;&#30340;&#27745;&#26579;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#8220;&#24341;&#23548;&#25351;&#20196;&#8221;&#65306;&#21363;&#19968;&#20010;&#30001;&#25968;&#25454;&#38598;&#21517;&#31216;&#12289;&#20998;&#21306;&#31867;&#22411;&#21644;&#21442;&#32771;&#23454;&#20363;&#30340;&#21021;&#22987;&#37096;&#20998;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#35201;&#27714;LLM&#23436;&#25104;&#23427;&#12290;&#22914;&#26524;LLM&#30340;&#36755;&#20986;&#19982;&#21442;&#32771;&#23454;&#20363;&#30340;&#21518;&#19968;&#37096;&#20998;&#23436;&#20840;&#25110;&#25509;&#36817;&#21305;&#37197;&#65292;&#37027;&#20040;&#35813;&#23454;&#20363;&#34987;&#26631;&#35760;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;&#20026;&#20102;&#20102;&#35299;&#25972;&#20010;&#20998;&#21306;&#26159;&#21542;&#21463;&#21040;&#27745;&#26579;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24819;&#27861;&#12290;&#31532;&#19968;&#20010;&#24819;&#27861;&#26159;&#26631;&#35760;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#21306;&#65292;&#35813;&#20998;&#21306;&#20013;&#30340;&#23454;&#20363;&#22823;&#22810;&#25968;&#37117;&#34987;&#21028;&#26029;&#20026;&#21463;&#21040;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#25928;&#26524;&#21644;&#24050;&#26377;&#26041;&#27861;&#23545;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;</title><link>http://arxiv.org/abs/2308.08487</link><description>&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Temporal Interest Network for Click-Through Rate Prediction. (arXiv:2308.08487v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#25928;&#26524;&#21644;&#24050;&#26377;&#26041;&#27861;&#23545;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#30340;&#21382;&#21490;&#26159;&#39044;&#27979;&#28857;&#20987;&#29575;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#30446;&#26631;&#39033;&#30446;&#20855;&#26377;&#24378;&#28872;&#30340;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#34429;&#28982;&#24050;&#26377;&#25991;&#29486;&#20998;&#21035;&#30740;&#31350;&#20102;&#36825;&#20123;&#30456;&#20851;&#24615;&#65292;&#20294;&#23578;&#26410;&#20998;&#26512;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#21363;&#34892;&#20026;&#35821;&#20041;&#12289;&#30446;&#26631;&#35821;&#20041;&#12289;&#34892;&#20026;&#26102;&#38388;&#21644;&#30446;&#26631;&#26102;&#38388;&#30340;&#22235;&#37325;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#20197;&#21450;&#29616;&#26377;&#26041;&#27861;&#23398;&#20064;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#27979;&#37327;&#20102;&#22235;&#37325;&#30456;&#20851;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#30452;&#35266;&#32780;&#24378;&#22823;&#30340;&#22235;&#37325;&#27169;&#24335;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#29992;&#25143;&#34892;&#20026;&#26041;&#27861;&#30340;&#23398;&#20064;&#30456;&#20851;&#24615;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#37117;&#27809;&#26377;&#23398;&#20064;&#21040;&#36825;&#26679;&#30340;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#26102;&#38388;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#26469;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.  In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and th
&lt;/p&gt;</description></item><item><title>TBIN&#27169;&#22411;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;&#21644;&#22522;&#20110;&#22359;&#20301;&#31227;&#30340;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#35299;&#20915;&#20102;&#21033;&#29992;&#38271;&#25991;&#26412;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#36827;&#34892;CTR&#39044;&#27979;&#26102;&#30340;&#25130;&#26029;&#38382;&#39064;&#21644;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08483</link><description>&lt;p&gt;
TBIN: &#27169;&#22411;&#21270;&#38271;&#25991;&#26412;&#34892;&#20026;&#25968;&#25454;&#29992;&#20110;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TBIN: Modeling Long Textual Behavior Data for CTR Prediction. (arXiv:2308.08483v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08483
&lt;/p&gt;
&lt;p&gt;
TBIN&#27169;&#22411;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;&#21644;&#22522;&#20110;&#22359;&#20301;&#31227;&#30340;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#35299;&#20915;&#20102;&#21033;&#29992;&#38271;&#25991;&#26412;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#36827;&#34892;CTR&#39044;&#27979;&#26102;&#30340;&#25130;&#26029;&#38382;&#39064;&#21644;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#21463;&#21040;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#32321;&#33635;&#24433;&#21709;&#65292;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#20197;&#25991;&#26412;&#26684;&#24335;&#32452;&#32455;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;LMs&#26469;&#22312;&#35821;&#20041;&#23618;&#38754;&#19978;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#26469;&#25913;&#36827;&#39044;&#27979;&#12290;&#34429;&#28982;&#26377;&#21069;&#26223;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#19981;&#24471;&#19981;&#25130;&#26029;&#25991;&#26412;&#25968;&#25454;&#20197;&#20943;&#23569;LMs&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#30740;&#31350;&#34920;&#26126;&#38271;&#26102;&#38388;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;CTR&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24037;&#20316;&#36890;&#24120;&#23558;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20852;&#36259;&#21387;&#32553;&#25104;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#34892;&#20026;&#30340;&#20852;&#36259;&#20999;&#22359;&#32593;&#32476;&#65288;TBIN&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#39640;&#25928;&#30340;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;&#21644;&#22522;&#20110;&#22359;&#20301;&#31227;&#30340;&#33258;&#27880;&#24847;&#21147;&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#12290;&#24471;&#21040;&#30340;&#29992;&#25143;&#22810;&#26679;&#21270;&#20852;&#36259;&#26159;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction plays a pivotal role in the success of recommendations. Inspired by the recent thriving of language models (LMs), a surge of works improve prediction by organizing user behavior data in a \textbf{textual} format and using LMs to understand user interest at a semantic level. While promising, these works have to truncate the textual data to reduce the quadratic computational overhead of self-attention in LMs. However, it has been studied that long user behavior data can significantly benefit CTR prediction. In addition, these works typically condense user diverse interests into a single feature vector, which hinders the expressive capability of the model. In this paper, we propose a \textbf{T}extual \textbf{B}ehavior-based \textbf{I}nterest Chunking \textbf{N}etwork (TBIN), which tackles the above limitations by combining an efficient locality-sensitive hashing algorithm and a shifted chunk-based self-attention. The resulting user diverse interests are
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24555;&#25463;&#21435;&#20559;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#20219;&#21153;&#23545;&#20559;&#35265;&#23646;&#24615;&#30340;&#23398;&#20064;&#20174;&#20559;&#35265;&#29305;&#24449;&#36716;&#31227;&#21040;&#24555;&#25463;&#29305;&#24449;&#19978;&#65292;&#24182;&#37319;&#29992;&#22240;&#26524;&#24178;&#39044;&#26469;&#28040;&#38500;&#20559;&#35265;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20381;&#36182;&#25935;&#24863;&#31038;&#20250;&#23646;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08482</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24555;&#25463;&#29305;&#24449;&#23454;&#29616;&#20844;&#24179;&#35270;&#35273;&#35782;&#21035;&#30340;&#33391;&#24615;&#25463;&#24452;&#28040;&#38500;&#20559;&#35265;&#65306;&#19968;&#31687;&#23454;&#20363;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Benign Shortcut for Debiasing: Fair Visual Recognition via Intervention with Shortcut Features. (arXiv:2308.08482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#8220;&#24555;&#25463;&#21435;&#20559;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#20219;&#21153;&#23545;&#20559;&#35265;&#23646;&#24615;&#30340;&#23398;&#20064;&#20174;&#20559;&#35265;&#29305;&#24449;&#36716;&#31227;&#21040;&#24555;&#25463;&#29305;&#24449;&#19978;&#65292;&#24182;&#37319;&#29992;&#22240;&#26524;&#24178;&#39044;&#26469;&#28040;&#38500;&#20559;&#35265;&#29305;&#24449;&#30340;&#24433;&#21709;&#65292;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20381;&#36182;&#25935;&#24863;&#31038;&#20250;&#23646;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#23398;&#20250;&#20381;&#36182;&#20110;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#25935;&#24863;&#31038;&#20250;&#23646;&#24615;&#36827;&#34892;&#39044;&#27979;&#65292;&#36825;&#22312;&#31038;&#20250;&#24212;&#29992;&#20013;&#65292;&#22914;&#25307;&#32856;&#12289;&#38134;&#34892;&#21644;&#21009;&#20107;&#21496;&#27861;&#20013;&#65292;&#24102;&#26469;&#37325;&#22823;&#20844;&#24179;&#39118;&#38505;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#20013;&#19982;&#31038;&#20250;&#23646;&#24615;&#30456;&#20851;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#20219;&#21153;&#21644;&#36825;&#20123;&#31038;&#20250;&#23646;&#24615;&#20043;&#38388;&#30340;&#39640;&#30456;&#20851;&#24615;&#20351;&#24471;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23398;&#20064;&#19982;&#21435;&#20559;&#19981;&#20860;&#23481;&#12290;&#37492;&#20110;&#27169;&#22411;&#20559;&#35265;&#26159;&#30001;&#20110;&#23398;&#20064;&#20559;&#35265;&#29305;&#24449;&#65288;&#20363;&#22914;&#24615;&#21035;&#65289;&#26469;&#20248;&#21270;&#30446;&#26631;&#20219;&#21153;&#32780;&#24341;&#36215;&#30340;&#65292;&#25105;&#20204;&#25506;&#35752;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#24555;&#25463;&#29305;&#24449;&#26469;&#26367;&#20195;&#20559;&#35265;&#29305;&#24449;&#22312;&#21435;&#20559;&#30340;&#30446;&#26631;&#20219;&#21153;&#20248;&#21270;&#20013;&#30340;&#20316;&#29992;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24555;&#25463;&#21435;&#20559;&#8221;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#30446;&#26631;&#20219;&#21153;&#23545;&#20559;&#35265;&#23646;&#24615;&#30340;&#23398;&#20064;&#20174;&#20559;&#35265;&#29305;&#24449;&#36716;&#31227;&#21040;&#24555;&#25463;&#29305;&#24449;&#19978;&#65292;&#28982;&#21518;&#37319;&#29992;&#22240;&#26524;&#24178;&#39044;&#26469;&#28040;&#38500;&#20559;&#35265;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often learn to make predictions that rely on sensitive social attributes like gender and race, which poses significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Existing work tackles this issue by minimizing the employed information about social attributes in models for debiasing. However, the high correlation between target task and these social attributes makes learning on the target task incompatible with debiasing. Given that model bias arises due to the learning of bias features (\emph{i.e}., gender) that help target task optimization, we explore the following research question: \emph{Can we leverage shortcut features to replace the role of bias feature in target task optimization for debiasing?} To this end, we propose \emph{Shortcut Debiasing}, to first transfer the target task's learning of bias attributes from bias features to shortcut features, and then employ causal intervention to eliminate sh
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#20013;&#21033;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#36827;&#34892;&#19981;&#24179;&#34913;&#31867;&#21035;&#20013;&#20266;&#36857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#26631;&#35760;&#21307;&#30103;&#25968;&#25454;&#38598;&#21644;&#20266;&#36857;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08480</link><description>&lt;p&gt;
&#21033;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#20013;&#30340;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#36827;&#34892;&#19981;&#24179;&#34913;&#31867;&#21035;&#20013;&#30340;&#20266;&#36857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals. (arXiv:2308.08480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08480
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#20013;&#21033;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#36827;&#34892;&#19981;&#24179;&#34913;&#31867;&#21035;&#20013;&#20266;&#36857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#26631;&#35760;&#21307;&#30103;&#25968;&#25454;&#38598;&#21644;&#20266;&#36857;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#30417;&#27979;&#29983;&#21629;&#20307;&#24449;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#36816;&#21160;&#20266;&#36857;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#24179;&#34913;&#31867;&#21035;&#22330;&#26223;&#20013;&#20351;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#22312;PPG&#26679;&#26412;&#20043;&#38388;&#20256;&#25773;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24178;&#20928;&#30340;PPG&#26679;&#26412;&#26126;&#26174;&#23569;&#20110;&#21463;&#20266;&#36857;&#27745;&#26579;&#30340;&#26679;&#26412;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27809;&#26377;&#20266;&#36857;&#30340;&#31867;&#21035;&#20013;&#65292;&#31934;&#30830;&#24230;&#20026;91%&#65292;&#21484;&#22238;&#29575;&#20026;90%&#65292;F1&#24471;&#20998;&#20026;90%&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#26631;&#35760;&#21307;&#30103;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#24178;&#20928;&#26679;&#26412;&#24456;&#23569;&#12290;&#23545;&#20110;&#20266;&#36857;&#30340;&#20998;&#31867;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#27604;&#36739;&#20102;&#20256;&#32479;&#20998;&#31867;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;MLP&#12289;Transformers&#12289;FCN&#65289;&#31561;&#26377;&#30417;&#30563;&#20998;&#31867;&#22120;&#19982;&#21322;&#30417;&#30563;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#12290;KNN&#26377;&#30417;&#30563;&#27169;&#22411;&#20855;&#26377;89%&#30340;&#31934;&#30830;&#24230;&#12289;95%&#30340;&#21484;&#22238;&#29575;&#21644;92%&#30340;F1&#24471;&#20998;&#65292;&#32467;&#26524;&#33391;&#22909;&#65292;&#20294;&#21322;&#30417;&#30563;&#31639;&#27861;&#22312;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmogram (PPG) signals are widely used in healthcare for monitoring vital signs, but they are susceptible to motion artifacts that can lead to inaccurate interpretations. In this study, the use of label propagation techniques to propagate labels among PPG samples is explored, particularly in imbalanced class scenarios where clean PPG samples are significantly outnumbered by artifact-contaminated samples. With a precision of 91%, a recall of 90% and an F1 score of 90% for the class without artifacts, the results demonstrate its effectiveness in labeling a medical dataset, even when clean samples are rare. For the classification of artifacts our study compares supervised classifiers such as conventional classifiers and neural networks (MLP, Transformers, FCN) with the semi-supervised label propagation algorithm. With a precision of 89%, a recall of 95% and an F1 score of 92%, the KNN supervised model gives good results, but the semi-supervised algorithm performs better in detec
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;LLM4TS&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08469</link><description>&lt;p&gt;
LLM4TS:&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#20004;&#38454;&#27573;&#24494;&#35843;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. (arXiv:2308.08469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08469
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;LLM4TS&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20511;&#37492;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#32479;&#19968;&#27169;&#22411;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#65292;&#25105;&#20204;&#35774;&#24819;&#21019;&#24314;&#19968;&#20010;&#31867;&#20284;&#30340;&#27169;&#22411;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26469;&#26500;&#24314;&#31283;&#20581;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;LLM4TS&#19987;&#27880;&#20110;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#20462;&#34917;&#19982;&#26102;&#38388;&#32534;&#30721;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;LLMs&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20248;&#20808;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#24494;&#35843;&#36807;&#31243;&#65306;&#39318;&#20808;&#36827;&#34892;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#20351;LLMs&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#28982;&#21518;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#19979;&#28216;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#19981;&#36827;&#34892;&#22823;&#37327;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#21457;&#25381;&#39044;&#35757;&#32451;LLMs&#30340;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;(PINN)&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#25972;&#20307;&#31934;&#24230;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23637;&#31034;&#19981;&#21516;&#26550;&#26500;&#36873;&#25321;&#21644;&#35757;&#32451;&#31574;&#30053;&#23545;&#32467;&#26524;&#27169;&#22411;&#30340;&#27979;&#35797;&#31934;&#24230;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.08468</link><description>&lt;p&gt;
&#35757;&#32451;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#19987;&#23478;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
An Expert's Guide to Training Physics-informed Neural Networks. (arXiv:2308.08468v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;(PINN)&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#25972;&#20307;&#31934;&#24230;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23637;&#31034;&#19981;&#21516;&#26550;&#26500;&#36873;&#25321;&#21644;&#35757;&#32451;&#31574;&#30053;&#23545;&#32467;&#26524;&#27169;&#22411;&#30340;&#27979;&#35797;&#31934;&#24230;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;(PINN)&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#26080;&#32541;&#22320;&#21512;&#25104;&#35266;&#27979;&#25968;&#25454;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#21487;&#33021;&#21463;&#21040;&#35757;&#32451;&#30149;&#24577;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#24120;&#24120;&#22240;&#20026;&#32570;&#20047;&#28145;&#24230;&#23398;&#20064;&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#20570;&#20986;&#30340;&#19981;&#33391;&#36873;&#25321;&#32780;&#21463;&#21040;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26368;&#20339;&#23454;&#36341;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;PINN&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#25972;&#20307;&#31934;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#35757;&#32451;PINN&#30340;&#19968;&#20123;&#20027;&#35201;&#22256;&#38590;&#65292;&#24182;&#21576;&#29616;&#20102;&#20840;&#38754;&#21487;&#22797;&#29616;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#26550;&#26500;&#36873;&#25321;&#21644;&#35757;&#32451;&#31574;&#30053;&#23545;&#32467;&#26524;&#27169;&#22411;&#30340;&#27979;&#35797;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#26041;&#27861;&#21644;&#25351;&#23548;&#21407;&#21017;&#21487;&#20197;&#23548;&#33268;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have been popularized as a deep learning framework that can seamlessly synthesize observational data and partial differential equation (PDE) constraints. Their practical effectiveness however can be hampered by training pathologies, but also oftentimes by poor choices made by users who lack deep learning expertise. In this paper we present a series of best practices that can significantly improve the training efficiency and overall accuracy of PINNs. We also put forth a series of challenging benchmark problems that highlight some of the most prominent difficulties in training PINNs, and present comprehensive and fully reproducible ablation studies that demonstrate how different architecture choices and training strategies affect the test accuracy of the resulting models. We show that the methods and guiding principles put forth in this study lead to state-of-the-art results and provide strong baselines that future studies should use for comparis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#21033;&#29992;&#37327;&#23376;&#26680;&#65292;&#25193;&#23637;&#20102;&#31070;&#32463;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08467</link><description>&lt;p&gt;
&#35770;&#31070;&#32463;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
On Neural Quantum Support Vector Machines. (arXiv:2308.08467v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#21033;&#29992;&#37327;&#23376;&#26680;&#65292;&#25193;&#23637;&#20102;&#31070;&#32463;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312; \cite{simon2023algorithms} &#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22235;&#31181;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;NSVMs&#65289;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#21363;&#20855;&#26377;&#37327;&#23376;&#26680;&#30340;NSVMs&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#36825;&#20010;&#24773;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In \cite{simon2023algorithms} we introduced four algorithms for the training of neural support vector machines (NSVMs) and demonstrated their feasibility. In this note we introduce neural quantum support vector machines, that is, NSVMs with a quantum kernel, and extend our results to this setting.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21270;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;&#12290;&#36890;&#36807;&#21033;&#29992;&#23618;&#27425;&#21270;&#22270;&#20687;&#34920;&#31034;&#21644;&#36339;&#36291;&#36830;&#25509;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#32423;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08465</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#21270;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Uncertainty Estimation for Medical Image Segmentation Networks. (arXiv:2308.08465v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08465
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#21270;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;&#12290;&#36890;&#36807;&#21033;&#29992;&#23618;&#27425;&#21270;&#22270;&#20687;&#34920;&#31034;&#21644;&#36339;&#36291;&#36830;&#25509;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#32423;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#37319;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#26159;&#19968;&#20010;&#26412;&#36136;&#19978;&#23384;&#22312;&#27495;&#20041;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#27169;&#22411;&#35757;&#32451;&#20013;&#23384;&#22312;&#22270;&#20687;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#22122;&#22768;&#65289;&#21644;&#25163;&#21160;&#26631;&#27880;&#30340;&#19981;&#30830;&#23450;&#24615;&#65288;&#20154;&#20026;&#38169;&#35823;&#21644;&#20559;&#24046;&#65289;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#65292;&#19981;&#20165;&#38656;&#35201;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#36824;&#38656;&#35201;&#20272;&#35745;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22823;&#22810;&#25968;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#21106;&#32593;&#32476;&#37319;&#29992;&#23618;&#27425;&#21270;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#20174;&#31934;&#32454;&#21040;&#31895;&#31961;&#25552;&#21462;&#22810;&#20010;&#20998;&#36776;&#29575;&#32423;&#21035;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#23618;&#27425;&#21270;&#22270;&#20687;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#12290;&#22810;&#32423;&#19981;&#30830;&#23450;&#24615;&#36890;&#36807;&#36339;&#36291;&#36830;&#25509;&#27169;&#22359;&#24314;&#27169;&#65292;&#28982;&#21518;&#36827;&#34892;&#37319;&#26679;&#20197;&#29983;&#25104;&#39044;&#27979;&#22270;&#20687;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#22320;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#20351;&#29992;&#36825;&#31181;&#23618;&#27425;&#21270;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#27169;&#22359;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#32593;&#32476;&#65288;&#22914;U-net&#65289;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a medical image segmentation model is an inherently ambiguous task, as uncertainties exist in both images (noise) and manual annotations (human errors and bias) used for model training. To build a trustworthy image segmentation model, it is important to not just evaluate its performance but also estimate the uncertainty of the model prediction. Most state-of-the-art image segmentation networks adopt a hierarchical encoder architecture, extracting image features at multiple resolution levels from fine to coarse. In this work, we leverage this hierarchical image representation and propose a simple yet effective method for estimating uncertainties at multiple levels. The multi-level uncertainties are modelled via the skip-connection module and then sampled to generate an uncertainty map for the predicted image segmentation. We demonstrate that a deep learning segmentation network such as U-net, when implemented with such hierarchical uncertainty estimation module, can achieve a h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#23432;&#21452;&#37325;&#31283;&#20581;&#31574;&#30053;&#65288;CDR&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#26377;&#27602;&#25554;&#34917;&#38382;&#39064;&#12290;CDR&#36890;&#36807;&#23457;&#26597;&#25554;&#34917;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#26469;&#36807;&#28388;&#25554;&#34917;&#65292;&#32467;&#26524;&#26174;&#31034;CDR&#20855;&#26377;&#38477;&#20302;&#26041;&#24046;&#21644;&#25913;&#36827;&#23614;&#37096;&#30028;&#38480;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#24182;&#20943;&#23569;&#26377;&#27602;&#25554;&#34917;&#30340;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08461</link><description>&lt;p&gt;
CDR&#65306;&#29992;&#20110;&#21435;&#20559;&#25512;&#33616;&#30340;&#20445;&#23432;&#21452;&#37325;&#31283;&#20581;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CDR: Conservative Doubly Robust Learning for Debiased Recommendation. (arXiv:2308.08461v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#23432;&#21452;&#37325;&#31283;&#20581;&#31574;&#30053;&#65288;CDR&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#26377;&#27602;&#25554;&#34917;&#38382;&#39064;&#12290;CDR&#36890;&#36807;&#23457;&#26597;&#25554;&#34917;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#26469;&#36807;&#28388;&#25554;&#34917;&#65292;&#32467;&#26524;&#26174;&#31034;CDR&#20855;&#26377;&#38477;&#20302;&#26041;&#24046;&#21644;&#25913;&#36827;&#23614;&#37096;&#30028;&#38480;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#24182;&#20943;&#23569;&#26377;&#27602;&#25554;&#34917;&#30340;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#24448;&#24448;&#26159;&#35266;&#23519;&#24615;&#30340;&#32780;&#19981;&#26159;&#23454;&#39564;&#24615;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#20013;&#26222;&#36941;&#23384;&#22312;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#20559;&#24046;&#38382;&#39064;&#24050;&#25104;&#20026;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#21452;&#37325;&#31283;&#20581;&#23398;&#20064;&#65288;DR&#65289;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#30340;&#29305;&#24615;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;DR&#26041;&#27861;&#22312;&#23384;&#22312;&#25152;&#35859;&#30340;&#26377;&#27602;&#25554;&#34917;&#65288;Poisonous Imputation&#65289;&#26102;&#21463;&#21040;&#20005;&#37325;&#24433;&#21709;&#65292;&#25554;&#34917;&#26126;&#26174;&#20559;&#31163;&#30495;&#23454;&#25968;&#25454;&#24182;&#36866;&#24471;&#20854;&#21453;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#23432;&#21452;&#37325;&#31283;&#20581;&#31574;&#30053;&#65288;CDR&#65289;&#65292;&#36890;&#36807;&#23457;&#26597;&#25554;&#34917;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#26469;&#36807;&#28388;&#25554;&#34917;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;CDR&#21487;&#20197;&#38477;&#20302;&#26041;&#24046;&#24182;&#25913;&#36827;&#23614;&#37096;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;CDR&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#30830;&#23454;&#20943;&#23569;&#20102;&#26377;&#27602;&#25554;&#34917;&#30340;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recommendation systems (RS), user behavior data is observational rather than experimental, resulting in widespread bias in the data. Consequently, tackling bias has emerged as a major challenge in the field of recommendation systems. Recently, Doubly Robust Learning (DR) has gained significant attention due to its remarkable performance and robust properties. However, our experimental findings indicate that existing DR methods are severely impacted by the presence of so-called Poisonous Imputation, where the imputation significantly deviates from the truth and becomes counterproductive.  To address this issue, this work proposes Conservative Doubly Robust strategy (CDR) which filters imputations by scrutinizing their mean and variance. Theoretical analyses show that CDR offers reduced variance and improved tail bounds.In addition, our experimental investigations illustrate that CDR significantly enhances performance and can indeed reduce the frequency of poisonous imputation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#65292;&#36890;&#36807;&#27604;&#36739;qGAN&#21644;QCBM&#31561;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08448</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#29616;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;qGAN&#65289;&#21644;QCBM
&lt;/p&gt;
&lt;p&gt;
Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance. (arXiv:2308.08448v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#65292;&#36890;&#36807;&#27604;&#36739;qGAN&#21644;QCBM&#31561;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#26159;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#39046;&#22495;&#65292;&#30001;&#20004;&#20010;&#26368;&#20855;&#21019;&#26032;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#32452;&#25104;&#65306;&#37327;&#23376;&#35745;&#31639;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;ML&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#34987;&#35748;&#20026;&#26159;&#23558;&#21463;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#20852;&#36215;&#24433;&#21709;&#30340;&#31532;&#19968;&#20010;&#39046;&#22495;&#12290;&#36825;&#39033;&#24037;&#20316;&#35752;&#35770;&#20102;&#22312;&#37329;&#34701;&#20013;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#30340;&#19968;&#20123;&#26032;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#24050;&#22312;&#37329;&#34701;&#30028;&#24341;&#36215;&#20851;&#27880;&#30340;QML&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#30495;&#23454;&#37329;&#34701;&#25968;&#25454;&#38598;&#23545;qGAN&#65288;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#21644;QCBM&#65288;&#37327;&#23376;&#30005;&#36335;Born&#26426;&#65289;&#31561;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;qGAN&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#26410;&#26469;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#36890;&#36807;QML&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning (QML) is a cross-disciplinary subject made up of two of the most exciting research areas: quantum computing and classical machine learning (ML), with ML and artificial intelligence (AI) being projected as the first fields that will be impacted by the rise of quantum machines. Quantum computers are being used today in drug discovery, material &amp; molecular modelling and finance. In this work, we discuss some upcoming active new research areas in application of quantum machine learning (QML) in finance. We discuss certain QML models that has become areas of active interest in the financial world for various applications. We use real world financial dataset and compare models such as qGAN (quantum generative adversarial networks) and QCBM (quantum circuit Born machine) among others, using simulated environments. For the qGAN, we define quantum circuits for discriminators and generators and show promises of future quantum advantage via QML in finance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CSPM&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#26102;&#31354;&#20559;&#22909;&#25552;&#21462;&#26469;&#35299;&#20915;&#25353;&#38656;&#39135;&#21697;&#37197;&#36865;CTR&#39044;&#27979;&#20013;&#30340;&#26102;&#31354;&#20449;&#24687;&#24314;&#27169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08446</link><description>&lt;p&gt;
CSPM: &#29992;&#20110;&#25353;&#38656;&#39135;&#21697;&#37197;&#36865;CTR&#39044;&#27979;&#30340;&#23545;&#27604;&#26102;&#31354;&#20559;&#22909;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CSPM: A Contrastive Spatiotemporal Preference Model for CTR Prediction in On-Demand Food Delivery Services. (arXiv:2308.08446v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;CSPM&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#26102;&#31354;&#20559;&#22909;&#25552;&#21462;&#26469;&#35299;&#20915;&#25353;&#38656;&#39135;&#21697;&#37197;&#36865;CTR&#39044;&#27979;&#20013;&#30340;&#26102;&#31354;&#20449;&#24687;&#24314;&#27169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#26159;&#22312;&#32447;&#25353;&#38656;&#39135;&#21697;&#37197;&#36865;&#65288;OFD&#65289;&#24179;&#21488;&#20013;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#29992;&#20110;&#20934;&#30830;&#20272;&#35745;&#29992;&#25143;&#28857;&#20987;&#39135;&#21697;&#39033;&#30446;&#30340;&#27010;&#29575;&#12290;&#19982;&#28120;&#23453;&#21644;&#20122;&#39532;&#36874;&#31561;&#36890;&#29992;&#30005;&#21830;&#24179;&#21488;&#19981;&#21516;&#65292;OFD&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#20852;&#36259;&#26356;&#21152;&#19982;&#22320;&#28857;&#21644;&#26102;&#38388;&#30456;&#20851;&#65292;&#36825;&#26159;&#22240;&#20026;&#23384;&#22312;&#26377;&#38480;&#30340;&#37197;&#36865;&#33539;&#22260;&#21644;&#21306;&#22495;&#21830;&#21697;&#20379;&#24212;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;OFD&#22330;&#26223;&#19979;&#30340;CTR&#39044;&#27979;&#31639;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#25429;&#25417;&#26469;&#33258;&#21382;&#21490;&#34892;&#20026;&#24207;&#21015;&#30340;&#20852;&#36259;&#65292;&#26410;&#33021;&#26377;&#25928;&#22320;&#23545;&#29305;&#24449;&#20013;&#30340;&#22797;&#26434;&#26102;&#31354;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#26102;&#31354;&#20559;&#22909;&#27169;&#22411;&#65288;CSPM&#65289;&#65292;&#36890;&#36807;&#23545;&#27604;&#26102;&#31354;&#34920;&#31034;&#23398;&#20064;&#65288;CSRL&#65289;&#12289;&#26102;&#31354;&#20559;&#22909;&#25552;&#21462;&#22120;&#65288;StPE&#65289;&#21644;&#26102;&#31354;&#20449;&#24687;&#36807;&#28388;&#22120;&#65288;StIF&#65289;&#19977;&#20010;&#27169;&#22359;&#65292;&#23545;&#19981;&#21516;&#25628;&#32034;&#29366;&#24577;&#19979;&#30340;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Click-through rate (CTR) prediction is a crucial task in the context of an online on-demand food delivery (OFD) platform for precisely estimating the probability of a user clicking on food items. Unlike universal e-commerce platforms such as Taobao and Amazon, user behaviors and interests on the OFD platform are more location and time-sensitive due to limited delivery ranges and regional commodity supplies. However, existing CTR prediction algorithms in OFD scenarios concentrate on capturing interest from historical behavior sequences, which fails to effectively model the complex spatiotemporal information within features, leading to poor performance. To address this challenge, this paper introduces the Contrastive Sres under different search states using three modules: contrastive spatiotemporal representation learning (CSRL), spatiotemporal preference extractor (StPE), and spatiotemporal information filter (StIF). CSRL utilizes a contrastive learning framework to generate a spatiotem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21475;&#21507;&#24739;&#32773;&#30340;ASR&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#30340;&#21475;&#21507;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#20005;&#37325;&#31243;&#24230;&#31995;&#25968;&#21644;&#26242;&#20572;&#25554;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#21475;&#21507;&#35821;&#38899;&#30340;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.08438</link><description>&lt;p&gt;
ASR&#25968;&#25454;&#22686;&#24378;&#20013;&#23545;&#21475;&#21507;&#24615;&#35821;&#38899;&#30340;&#20934;&#30830;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accurate synthesis of Dysarthric Speech for ASR data augmentation. (arXiv:2308.08438v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21475;&#21507;&#24739;&#32773;&#30340;ASR&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#30340;&#21475;&#21507;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#20005;&#37325;&#31243;&#24230;&#31995;&#25968;&#21644;&#26242;&#20572;&#25554;&#20837;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#21475;&#21507;&#35821;&#38899;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#21507;&#26159;&#19968;&#31181;&#36816;&#21160;&#35328;&#35821;&#38556;&#30861;&#65292;&#24120;&#36890;&#36807;&#23545;&#35328;&#35821;&#20135;&#29983;&#32908;&#32905;&#30340;&#32531;&#24930;&#12289;&#19981;&#21327;&#35843;&#30340;&#25511;&#21046;&#26469;&#34920;&#24449;&#35821;&#38899;&#21487;&#29702;&#35299;&#24615;&#38477;&#20302;&#12290;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#21475;&#21507;&#24739;&#32773;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#31283;&#20581;&#30340;&#38024;&#23545;&#21475;&#21507;&#30340;ASR&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#35821;&#38899;&#65292;&#32780;&#36825;&#23545;&#20110;&#21475;&#21507;&#24739;&#32773;&#26469;&#35828;&#24182;&#19981;&#23481;&#26131;&#33719;&#24471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21475;&#21507;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;ASR&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12290;&#21475;&#21507;&#33258;&#21457;&#35821;&#38899;&#22312;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#27700;&#24179;&#19978;&#30340;&#38901;&#24459;&#21644;&#22768;&#23398;&#29305;&#24449;&#24046;&#24322;&#26159;&#21475;&#21507;&#35821;&#38899;&#24314;&#27169;&#12289;&#21512;&#25104;&#21644;&#22686;&#24378;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#36827;&#34892;&#21475;&#21507;&#35821;&#38899;&#21512;&#25104;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#22810;&#35805;&#32773;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#21475;&#21507;&#20005;&#37325;&#31243;&#24230;&#31995;&#25968;&#21644;&#26242;&#20572;&#25554;&#20837;&#27169;&#22411;&#65292;&#21512;&#25104;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#30340;&#21475;&#21507;&#35821;&#38899;&#12290;&#20026;&#20102;&#35780;&#20272;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#65292;&#36827;&#34892;&#20102;ASR&#24615;&#33021;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dysarthria is a motor speech disorder often characterized by reduced speech intelligibility through slow, uncoordinated control of speech production muscles. Automatic Speech recognition (ASR) systems can help dysarthric talkers communicate more effectively. However, robust dysarthria-specific ASR requires a significant amount of training speech, which is not readily available for dysarthric talkers. This paper presents a new dysarthric speech synthesis method for the purpose of ASR training data augmentation. Differences in prosodic and acoustic characteristics of dysarthric spontaneous speech at varying severity levels are important components for dysarthric speech modeling, synthesis, and augmentation. For dysarthric speech synthesis, a modified neural multi-talker TTS is implemented by adding a dysarthria severity level coefficient and a pause insertion model to synthesize dysarthric speech for varying severity levels. To evaluate the effectiveness for synthesis of training data fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#31572;&#26469;&#35782;&#21035;&#20195;&#29702;&#20154;&#30340;&#39118;&#38505;&#35268;&#36991;&#12290;&#22312;&#19968;&#26399;&#24773;&#26223;&#21644;&#26080;&#38480;&#26399;&#24773;&#26223;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#35201;&#27714;&#20195;&#29702;&#20154;&#23637;&#31034;&#22905;&#30340;&#26368;&#20248;&#31574;&#30053;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#20351;&#29992;&#38543;&#26426;&#35774;&#35745;&#30340;&#38382;&#39064;&#26469;&#35782;&#21035;&#20195;&#29702;&#20154;&#30340;&#39118;&#38505;&#35268;&#36991;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#26377;&#38480;&#30340;&#20505;&#36873;&#38598;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#20195;&#29702;&#20154;&#30340;&#39118;&#38505;&#35268;&#36991;&#12290;</title><link>http://arxiv.org/abs/2308.08427</link><description>&lt;p&gt;
&#20511;&#21161;&#20132;&#20114;&#24335;&#38382;&#31572;&#36890;&#36807;&#36870;&#24378;&#21270;&#23398;&#20064;&#26469;&#24341;&#23548;&#39118;&#38505;&#35268;&#36991;
&lt;/p&gt;
&lt;p&gt;
Eliciting Risk Aversion with Inverse Reinforcement Learning via Interactive Questioning. (arXiv:2308.08427v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#38382;&#31572;&#26469;&#35782;&#21035;&#20195;&#29702;&#20154;&#30340;&#39118;&#38505;&#35268;&#36991;&#12290;&#22312;&#19968;&#26399;&#24773;&#26223;&#21644;&#26080;&#38480;&#26399;&#24773;&#26223;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#35201;&#27714;&#20195;&#29702;&#20154;&#23637;&#31034;&#22905;&#30340;&#26368;&#20248;&#31574;&#30053;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#20351;&#29992;&#38543;&#26426;&#35774;&#35745;&#30340;&#38382;&#39064;&#26469;&#35782;&#21035;&#20195;&#29702;&#20154;&#30340;&#39118;&#38505;&#35268;&#36991;&#12290;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#26377;&#38480;&#30340;&#20505;&#36873;&#38598;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#20195;&#29702;&#20154;&#30340;&#39118;&#38505;&#35268;&#36991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20132;&#20114;&#24335;&#38382;&#31572;&#26469;&#35782;&#21035;&#20195;&#29702;&#20154;&#30340;&#39118;&#38505;&#35268;&#36991;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#20004;&#31181;&#24773;&#26223;&#20013;&#36827;&#34892;&#65306;&#19968;&#26399;&#24773;&#26223;&#21644;&#26080;&#38480;&#26399;&#24773;&#26223;&#12290;&#22312;&#19968;&#26399;&#24773;&#26223;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#20195;&#29702;&#20154;&#30340;&#39118;&#38505;&#35268;&#36991;&#30001;&#29366;&#24577;&#30340;&#25104;&#26412;&#20989;&#25968;&#21644;&#22833;&#30495;&#39118;&#38505;&#24230;&#37327;&#25152;&#34920;&#24449;&#12290;&#22312;&#26080;&#38480;&#26399;&#24773;&#26223;&#20013;&#65292;&#25105;&#20204;&#29992;&#19968;&#20010;&#39069;&#22806;&#30340;&#25104;&#20998;&#65292;&#25240;&#25187;&#22240;&#23376;&#65292;&#26469;&#24314;&#27169;&#39118;&#38505;&#35268;&#36991;&#12290;&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#21253;&#21547;&#20195;&#29702;&#20154;&#30495;&#23454;&#39118;&#38505;&#35268;&#36991;&#30340;&#26377;&#38480;&#20505;&#36873;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#35201;&#27714;&#20195;&#29702;&#20154;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#23637;&#31034;&#22905;&#30340;&#26368;&#20248;&#25919;&#31574;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#36825;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20195;&#29702;&#20154;&#30340;&#39118;&#38505;&#35268;&#36991;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#38382;&#39064;&#30340;&#25968;&#37327;&#36235;&#36817;&#26080;&#31351;&#22823;&#24182;&#19988;&#38382;&#39064;&#26159;&#38543;&#26426;&#35774;&#35745;&#30340;&#26102;&#20505;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#20195;&#29702;&#20154;&#30340;&#39118;&#38505;&#35268;&#36991;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#29992;&#20110;&#35774;&#35745;&#26368;&#20248;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel framework for identifying an agent's risk aversion using interactive questioning. Our study is conducted in two scenarios: a one-period case and an infinite horizon case. In the one-period case, we assume that the agent's risk aversion is characterized by a cost function of the state and a distortion risk measure. In the infinite horizon case, we model risk aversion with an additional component, a discount factor. Assuming the access to a finite set of candidates containing the agent's true risk aversion, we show that asking the agent to demonstrate her optimal policies in various environment, which may depend on their previous answers, is an effective means of identifying the agent's risk aversion. Specifically, we prove that the agent's risk aversion can be identified as the number of questions tends to infinity, and the questions are randomly designed. We also develop an algorithm for designing optimal questions and provide empirical evidence that our met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24515;&#33039;&#30005;&#29983;&#29702;&#27169;&#22411;&#20013;&#30340;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;GPU&#21152;&#36895;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#26354;&#38754;&#27874;&#26041;&#31243;&#30340;&#21442;&#25968;&#26469;&#37325;&#24314;&#32473;&#23450;&#30340;&#24515;&#30005;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#21487;&#20197;&#20197;&#39640;&#20934;&#30830;&#24615;&#37325;&#24314;&#27169;&#25311;&#24515;&#33039;&#28608;&#27963;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08410</link><description>&lt;p&gt;
&#20174;&#34920;&#38754;&#24515;&#30005;&#22270;&#20013;&#25968;&#23383;&#21452;&#29983;&#24515;&#33039;&#30005;&#29983;&#29702;&#27169;&#22411;&#65306;&#19968;&#31181;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Digital twinning of cardiac electrophysiology models from the surface ECG: a geodesic backpropagation approach. (arXiv:2308.08410v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24515;&#33039;&#30005;&#29983;&#29702;&#27169;&#22411;&#20013;&#30340;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;GPU&#21152;&#36895;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#20248;&#21270;&#26354;&#38754;&#27874;&#26041;&#31243;&#30340;&#21442;&#25968;&#26469;&#37325;&#24314;&#32473;&#23450;&#30340;&#24515;&#30005;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#21487;&#20197;&#20197;&#39640;&#20934;&#30830;&#24615;&#37325;&#24314;&#27169;&#25311;&#24515;&#33039;&#28608;&#27963;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26354;&#38754;&#27874;&#26041;&#31243;&#24050;&#25104;&#20026;&#31934;&#30830;&#39640;&#25928;&#24314;&#27169;&#24515;&#33039;&#30005;&#28608;&#27963;&#30340;&#19981;&#21487;&#25110;&#32570;&#24037;&#20855;&#12290;&#36890;&#36807;&#21305;&#37197;&#20020;&#24202;&#35760;&#24405;&#30340;&#26354;&#38754;&#27874;&#30005;&#22270;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#65292;&#21407;&#21017;&#19978;&#21487;&#20197;&#20197;&#32431;&#38750;&#20405;&#20837;&#30340;&#26041;&#24335;&#26500;&#24314;&#24739;&#32773;&#29305;&#24322;&#24615;&#30340;&#24515;&#33039;&#30005;&#29983;&#29702;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25311;&#21512;&#36807;&#31243;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#27979;&#22320;&#32447;&#21453;&#21521;&#20256;&#25773;&#65288;Geodesic-BP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#36870;&#26354;&#38754;&#27874;&#38382;&#39064;&#12290;Geodesic-BP&#38750;&#24120;&#36866;&#29992;&#20110;&#25903;&#25345;GPU&#21152;&#36895;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20248;&#21270;&#26354;&#38754;&#27874;&#26041;&#31243;&#30340;&#21442;&#25968;&#20197;&#37325;&#29616;&#32473;&#23450;&#30340;ECG&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Geodesic-BP&#22312;&#21512;&#25104;&#27979;&#35797;&#26696;&#20363;&#20013;&#65292;&#22312;&#24314;&#27169;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20197;&#39640;&#20934;&#30830;&#24615;&#37325;&#26500;&#27169;&#25311;&#24515;&#33039;&#28608;&#27963;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20820;&#27169;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#31215;&#26497;&#30340;&#32467;&#26524;&#12290;&#32771;&#34385;&#21040;&#26410;&#26469;&#20010;&#20307;&#21270;&#21307;&#30103;&#30340;&#21457;&#23637;&#65292;Geodesic-BP&#20855;&#26377;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The eikonal equation has become an indispensable tool for modeling cardiac electrical activation accurately and efficiently. In principle, by matching clinically recorded and eikonal-based electrocardiograms (ECGs), it is possible to build patient-specific models of cardiac electrophysiology in a purely non-invasive manner. Nonetheless, the fitting procedure remains a challenging task. The present study introduces a novel method, Geodesic-BP, to solve the inverse eikonal problem. Geodesic-BP is well-suited for GPU-accelerated machine learning frameworks, allowing us to optimize the parameters of the eikonal equation to reproduce a given ECG. We show that Geodesic-BP can reconstruct a simulated cardiac activation with high accuracy in a synthetic test case, even in the presence of modeling inaccuracies. Furthermore, we apply our algorithm to a publicly available dataset of a rabbit model, with very positive results. Given the future shift towards personalized medicine, Geodesic-BP has t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#26041;&#24335;&#12290;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#30830;&#20445;&#20154;&#20204;&#23545;AI&#31995;&#32479;&#30340;&#20449;&#20219;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38500;&#20102;&#35299;&#37322;&#24615;&#20043;&#22806;&#65292;&#36824;&#28041;&#21450;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#31561;&#26041;&#38754;&#12290;&#35813;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#36817;&#26399;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.08407</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;:&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#26041;&#24335;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for clinical risk prediction: a survey of concepts, methods, and modalities. (arXiv:2308.08407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08407
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#26041;&#24335;&#12290;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#30830;&#20445;&#20154;&#20204;&#23545;AI&#31995;&#32479;&#30340;&#20449;&#20219;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38500;&#20102;&#35299;&#37322;&#24615;&#20043;&#22806;&#65292;&#36824;&#28041;&#21450;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#31561;&#26041;&#38754;&#12290;&#35813;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#36817;&#26399;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#25104;&#26524;&#65292;&#22312;&#35786;&#26029;&#21644;&#30142;&#30149;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#23545;&#20854;&#19981;&#36879;&#26126;&#24615;&#12289;&#28508;&#22312;&#20559;&#35265;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#30340;&#25285;&#24551;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#20026;&#20102;&#30830;&#20445;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20449;&#20219;&#21644;&#21487;&#38752;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#20013;&#65292;&#21487;&#35299;&#37322;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#21487;&#35299;&#37322;&#24615;&#36890;&#24120;&#25351;&#30340;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21521;&#20154;&#31867;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20854;&#20915;&#31574;&#36923;&#36753;&#25110;&#20915;&#31574;&#26412;&#36523;&#30340;&#31283;&#20581;&#35299;&#37322;&#33021;&#21147;&#12290;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20013;&#65292;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#31561;&#20854;&#20182;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24615;&#20063;&#20195;&#34920;&#20102;&#36229;&#36234;&#21487;&#35299;&#37322;&#24615;&#26412;&#36523;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#19968;&#36215;&#25110;&#21487;&#20114;&#25442;&#22320;&#20351;&#29992;&#12290;&#26412;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#21457;&#23637;&#36827;&#23637;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in AI applications to healthcare have shown incredible promise in surpassing human performance in diagnosis and disease prognosis. With the increasing complexity of AI models, however, concerns regarding their opacity, potential biases, and the need for interpretability. To ensure trust and reliability in AI systems, especially in clinical risk prediction models, explainability becomes crucial. Explainability is usually referred to as an AI system's ability to provide a robust interpretation of its decision-making logic or the decisions themselves to human stakeholders. In clinical risk prediction, other aspects of explainability like fairness, bias, trust, and transparency also represent important concepts beyond just interpretability. In this review, we address the relationship between these concepts as they are often used together or interchangeably. This review also discusses recent progress in developing explainable models for clinical risk prediction, highligh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#24341;&#25806;&#65292;&#36890;&#36807;&#35745;&#31639;&#25991;&#26723;&#20013;&#21333;&#35789;&#30340;&#30456;&#20851;&#24615;&#21644;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#26041;&#27861;&#26469;&#25512;&#33616;&#35270;&#39057;&#32473;&#29992;&#25143;&#12290;&#21516;&#26102;&#65292;&#36824;&#36890;&#36807;&#35745;&#31639;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#26469;&#35780;&#20272;&#24341;&#25806;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08406</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#27969;&#23186;&#20307;&#24179;&#21488;&#25512;&#33616;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Content-based Recommendation Engine for Video Streaming Platform. (arXiv:2308.08406v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#24341;&#25806;&#65292;&#36890;&#36807;&#35745;&#31639;&#25991;&#26723;&#20013;&#21333;&#35789;&#30340;&#30456;&#20851;&#24615;&#21644;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#26041;&#27861;&#26469;&#25512;&#33616;&#35270;&#39057;&#32473;&#29992;&#25143;&#12290;&#21516;&#26102;&#65292;&#36824;&#36890;&#36807;&#35745;&#31639;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#26469;&#35780;&#20272;&#24341;&#25806;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#24341;&#25806;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21521;&#29992;&#25143;&#24314;&#35758;&#20869;&#23481;&#12289;&#20135;&#21697;&#25110;&#26381;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25512;&#33616;&#24341;&#25806;&#65292;&#26681;&#25454;&#29992;&#25143;&#20043;&#21069;&#30340;&#20852;&#36259;&#21644;&#36873;&#25321;&#65292;&#21521;&#29992;&#25143;&#25552;&#20379;&#35270;&#39057;&#25512;&#33616;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;TF-IDF&#25991;&#26412;&#21521;&#37327;&#21270;&#26041;&#27861;&#26469;&#30830;&#23450;&#25991;&#26723;&#20013;&#21333;&#35789;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#23427;&#20204;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#25214;&#20986;&#27599;&#20010;&#20869;&#23481;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26368;&#21518;&#65292;&#26681;&#25454;&#24471;&#21040;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#20540;&#65292;&#24341;&#25806;&#23558;&#21521;&#29992;&#25143;&#25512;&#33616;&#35270;&#39057;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#23558;&#36890;&#36807;&#35745;&#31639;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#26469;&#34913;&#37327;&#24341;&#25806;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation engine suggest content, product or services to the user by using machine learning algorithm. This paper proposed a content-based recommendation engine for providing video suggestion to the user based on their previous interests and choices. We will use TF-IDF text vectorization method to determine the relevance of words in a document. Then we will find out the similarity between each content by calculating cosine similarity between them. Finally, engine will recommend videos to the users based on the obtained similarity score value. In addition, we will measure the engine's performance by computing precision, recall, and F1 core of the proposed system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#20195;&#29702;&#27169;&#22411;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#26469;&#24555;&#36895;&#39044;&#27979;&#24050;&#29992;&#26680;&#29123;&#26009;&#65288;SNF&#65289;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#34928;&#21464;&#28909;&#21644;&#26680;&#32032;&#27987;&#24230;&#12290;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#36890;&#36807;&#39564;&#35777;&#21644;&#27979;&#35797;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;</title><link>http://arxiv.org/abs/2308.08391</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23545;&#24050;&#29992;&#26680;&#29123;&#26009;&#30340;&#24555;&#36895;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fast Uncertainty Quantification of Spent Nuclear Fuel with Neural Networks. (arXiv:2308.08391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#20195;&#29702;&#27169;&#22411;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#26469;&#24555;&#36895;&#39044;&#27979;&#24050;&#29992;&#26680;&#29123;&#26009;&#65288;SNF&#65289;&#30340;&#29305;&#24615;&#65292;&#21253;&#25324;&#34928;&#21464;&#28909;&#21644;&#26680;&#32032;&#27987;&#24230;&#12290;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#36890;&#36807;&#39564;&#35777;&#21644;&#27979;&#35797;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26680;&#33021;&#29983;&#20135;&#12289;&#24223;&#29289;&#31649;&#29702;&#21644;&#26680;&#30417;&#30563;&#30340;&#23433;&#20840;&#12289;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#65292;&#23545;&#24050;&#29992;&#26680;&#29123;&#26009;&#65288;SNF&#65289;&#29305;&#24615;&#30340;&#20934;&#30830;&#35745;&#31639;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#26159;&#21487;&#38752;&#30340;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#20195;&#29702;&#27169;&#22411;&#26041;&#27861;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#26469;&#39044;&#27979;&#19968;&#31995;&#21015;SNF&#29305;&#24615;&#12290;&#36890;&#36807;&#20174;CASMO5&#26230;&#26684;&#35745;&#31639;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;NN&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;SNF&#30340;&#34928;&#21464;&#28909;&#21644;&#26680;&#32032;&#27987;&#24230;&#65292;&#24182;&#20855;&#26377;&#20851;&#38190;&#36755;&#20837;&#21442;&#25968;&#65288;&#22914;&#23500;&#38598;&#24230;&#12289;&#29123;&#32791;&#12289;&#24490;&#29615;&#20043;&#38388;&#30340;&#20919;&#21364;&#26102;&#38388;&#12289;&#24179;&#22343;&#30844;&#27987;&#24230;&#21644;&#29123;&#26009;&#28201;&#24230;&#65289;&#30340;&#20989;&#25968;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#32463;&#36807;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#34928;&#21464;&#28909;&#27169;&#25311;&#39564;&#35777;&#65292;&#24182;&#19982;&#26469;&#33258;&#20004;&#31181;&#19981;&#21516;&#21387;&#27700;&#31867;&#22411;&#21453;&#24212;&#22534;&#30340;&#19981;&#21516;&#38080;&#27687;&#21270;&#29289;&#29123;&#26009;&#32452;&#20214;&#30340;&#27979;&#35797;&#32467;&#26524;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate calculation and uncertainty quantification of the characteristics of spent nuclear fuel (SNF) play a crucial role in ensuring the safety, efficiency, and sustainability of nuclear energy production, waste management, and nuclear safeguards. State of the art physics-based models, while reliable, are computationally intensive and time-consuming. This paper presents a surrogate modeling approach using neural networks (NN) to predict a number of SNF characteristics with reduced computational costs compared to physics-based models. An NN is trained using data generated from CASMO5 lattice calculations. The trained NN accurately predicts decay heat and nuclide concentrations of SNF, as a function of key input parameters, such as enrichment, burnup, cooling time between cycles, mean boron concentration and fuel temperature. The model is validated against physics-based decay heat simulations and measurements of different uranium oxide fuel assemblies from two different pressurized
&lt;/p&gt;</description></item><item><title>Continuous Sweep&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#20108;&#20803;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#21270;&#31867;&#21035;&#20998;&#24067;&#12289;&#20248;&#21270;&#20915;&#31574;&#36793;&#30028;&#20197;&#21450;&#35745;&#31639;&#22343;&#20540;&#31561;&#26041;&#27861;&#65292;&#23427;&#22312;&#37327;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08387</link><description>&lt;p&gt;
Continuous Sweep: &#19968;&#31181;&#25913;&#36827;&#30340;&#20108;&#20803;&#37327;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Continuous Sweep: an improved, binary quantifier. (arXiv:2308.08387v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08387
&lt;/p&gt;
&lt;p&gt;
Continuous Sweep&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#20108;&#20803;&#37327;&#21270;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21442;&#25968;&#21270;&#31867;&#21035;&#20998;&#24067;&#12289;&#20248;&#21270;&#20915;&#31574;&#36793;&#30028;&#20197;&#21450;&#35745;&#31639;&#22343;&#20540;&#31561;&#26041;&#27861;&#65292;&#23427;&#22312;&#37327;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#19968;&#31181;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#20854;&#20851;&#27880;&#30340;&#26159;&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#31867;&#21035;&#30340;&#26222;&#36941;&#24615;&#65292;&#32780;&#19981;&#26159;&#26631;&#35760;&#20854;&#20010;&#20307;&#35266;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Continuous Sweep&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#20108;&#20803;&#37327;&#21270;&#22120;&#65292;&#21463;&#21040;&#34920;&#29616;&#33391;&#22909;&#30340;Median Sweep&#30340;&#21551;&#21457;&#12290;Median Sweep&#30446;&#21069;&#26159;&#26368;&#22909;&#30340;&#20108;&#20803;&#37327;&#21270;&#22120;&#20043;&#19968;&#65292;&#20294;&#25105;&#20204;&#22312;&#19977;&#20010;&#26041;&#38754;&#25913;&#21464;&#20102;&#36825;&#20010;&#37327;&#21270;&#22120;&#65292;&#21363;1&#65289;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#31867;&#21035;&#20998;&#24067;&#32780;&#19981;&#26159;&#32463;&#39564;&#20998;&#24067;&#65292;2&#65289;&#20248;&#21270;&#20915;&#31574;&#36793;&#30028;&#32780;&#19981;&#26159;&#24212;&#29992;&#31163;&#25955;&#30340;&#20915;&#31574;&#35268;&#21017;&#65292;3&#65289;&#35745;&#31639;&#22343;&#20540;&#32780;&#19981;&#26159;&#20013;&#20301;&#25968;&#12290;&#22312;&#19968;&#33324;&#27169;&#22411;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;Continuous Sweep&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#36825;&#26159;&#37327;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#39318;&#27425;&#29702;&#35770;&#36129;&#29486;&#20043;&#19968;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25512;&#23548;&#20351;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#26368;&#20248;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24191;&#27867;&#30340;&#24773;&#20917;&#19979;&#65292;Continuous Sweep&#20248;&#20110;Median Sweep&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification is a supervised machine learning task, focused on estimating the class prevalence of a dataset rather than labeling its individual observations. We introduce Continuous Sweep, a new parametric binary quantifier inspired by the well-performing Median Sweep. Median Sweep is currently one of the best binary quantifiers, but we have changed this quantifier on three points, namely 1) using parametric class distributions instead of empirical distributions, 2) optimizing decision boundaries instead of applying discrete decision rules, and 3) calculating the mean instead of the median. We derive analytic expressions for the bias and variance of Continuous Sweep under general model assumptions. This is one of the first theoretical contributions in the field of quantification learning. Moreover, these derivations enable us to find the optimal decision boundaries. Finally, our simulation study shows that Continuous Sweep outperforms Median Sweep in a wide range of situations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#35780;&#20272;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#25298;&#32477;&#26354;&#32447;&#26041;&#27861;&#12290;&#20351;&#29992;&#24863;&#30693;&#37327;&#21270;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#26469;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08381</link><description>&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#25298;&#32477;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Precision and Recall Reject Curves for Classification. (arXiv:2308.08381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#35780;&#20272;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#25298;&#32477;&#26354;&#32447;&#26041;&#27861;&#12290;&#20351;&#29992;&#24863;&#30693;&#37327;&#21270;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#26469;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#20998;&#31867;&#22330;&#26223;&#20013;&#65292;&#21482;&#20351;&#29992;&#27169;&#22411;&#39640;&#24230;&#30830;&#23450;&#30340;&#20998;&#31867;&#23454;&#20363;&#26159;&#21487;&#21462;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#20123;&#39640;&#24230;&#30830;&#23450;&#30340;&#23454;&#20363;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20934;&#30830;&#24230;&#25298;&#32477;&#26354;&#32447;&#12290;&#25298;&#32477;&#26354;&#32447;&#20801;&#35768;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#30830;&#23450;&#24230;&#24230;&#37327;&#22312;&#25509;&#21463;&#25110;&#25298;&#32477;&#20998;&#31867;&#30340;&#19968;&#31995;&#21015;&#38408;&#20540;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24230;&#21487;&#33021;&#24182;&#19981;&#36866;&#21512;&#25152;&#26377;&#24212;&#29992;&#31243;&#24207;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#30456;&#21453;&#65292;&#31934;&#30830;&#24230;&#25110;&#21484;&#22238;&#29575;&#21487;&#33021;&#26356;&#21487;&#21462;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#23384;&#22312;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#20013;&#65292;&#20363;&#22914;&#65292;&#22312;&#19981;&#24179;&#34913;&#30340;&#22522;&#20934;&#25968;&#25454;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#30340;&#25298;&#32477;&#26354;&#32447;&#65306;&#21484;&#22238;&#29575;-&#25298;&#32477;&#26354;&#32447;&#21644;&#31934;&#30830;&#24230;-&#25298;&#32477;&#26354;&#32447;&#12290;&#36890;&#36807;&#23398;&#20064;&#21521;&#37327;&#37327;&#21270;&#20013;&#30340;&#21407;&#22411;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#20154;&#24037;&#22522;&#20934;&#25968;&#25454;&#19978;&#23558;&#25552;&#20986;&#30340;&#26354;&#32447;&#19982;&#20934;&#30830;&#24230;&#25298;&#32477;&#26354;&#32447;&#20316;&#20026;&#22522;&#20934;&#36827;&#34892;&#39564;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23384;&#22312;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#22522;&#20934;&#25968;&#25454;&#21644;&#21307;&#23398;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
For some classification scenarios, it is desirable to use only those classification instances that a trained model associates with a high certainty. To obtain such high-certainty instances, previous work has proposed accuracy-reject curves. Reject curves allow to evaluate and compare the performance of different certainty measures over a range of thresholds for accepting or rejecting classifications. However, the accuracy may not be the most suited evaluation metric for all applications, and instead precision or recall may be preferable. This is the case, for example, for data with imbalanced class distributions. We therefore propose reject curves that evaluate precision and recall, the recall-reject curve and the precision-reject curve. Using prototype-based classifiers from learning vector quantization, we first validate the proposed curves on artificial benchmark data against the accuracy reject curve as a baseline. We then show on imbalanced benchmarks and medical, real-world data 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#20256;&#24863;&#22120;&#36873;&#25321;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#20351;&#29992;Gumbel-Softmax&#25216;&#24039;&#36890;&#36807;&#26631;&#20934;&#21453;&#21521;&#20256;&#25773;&#26469;&#23398;&#20064;&#31163;&#25955;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#23551;&#21629;&#65292;&#24182;&#25552;&#39640;&#20219;&#21153;-DNN&#22788;&#29702;&#22810;&#31181;&#21487;&#33021;&#33410;&#28857;&#23376;&#38598;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08379</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#20256;&#24863;&#22120;&#36873;&#25321;&#30340;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36866;&#29992;&#20110;&#24102;&#23485;&#21463;&#38480;&#30340;&#36523;&#20307;&#20256;&#24863;&#22120;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
A distributed neural network architecture for dynamic sensor selection with application to bandwidth-constrained body-sensor networks. (arXiv:2308.08379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08379
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#20256;&#24863;&#22120;&#36873;&#25321;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#20351;&#29992;Gumbel-Softmax&#25216;&#24039;&#36890;&#36807;&#26631;&#20934;&#21453;&#21521;&#20256;&#25773;&#26469;&#23398;&#20064;&#31163;&#25955;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#23551;&#21629;&#65292;&#24182;&#25552;&#39640;&#20219;&#21153;-DNN&#22788;&#29702;&#22810;&#31181;&#21487;&#33021;&#33410;&#28857;&#23376;&#38598;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21160;&#24577;&#20256;&#24863;&#22120;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#38024;&#23545;&#27599;&#20010;&#29305;&#23450;&#36755;&#20837;&#26679;&#26412;&#25512;&#23548;&#20986;&#26368;&#20339;&#30340;&#20256;&#24863;&#22120;&#23376;&#38598;&#36873;&#25321;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#22266;&#23450;&#36873;&#25321;&#12290;&#36825;&#31181;&#21160;&#24577;&#36873;&#25321;&#19982;&#20219;&#21153;&#27169;&#22411;&#19968;&#36215;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20013;&#32852;&#21512;&#23398;&#20064;&#65292;&#20351;&#29992;Gumbel-Softmax&#25216;&#24039;&#36890;&#36807;&#26631;&#20934;&#21453;&#21521;&#20256;&#25773;&#26469;&#23398;&#20064;&#31163;&#25955;&#20915;&#31574;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#21160;&#24577;&#36873;&#25321;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#33410;&#28857;&#30340;&#20256;&#36755;&#39057;&#29575;&#26469;&#22686;&#21152;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;WSN&#65289;&#30340;&#23551;&#21629;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21253;&#25324;&#21160;&#24577;&#31354;&#38388;&#28388;&#27874;&#22120;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20351;&#20219;&#21153;-DNN&#23545;&#20110;&#29616;&#22312;&#38656;&#35201;&#22788;&#29702;&#30340;&#22810;&#31181;&#21487;&#33021;&#33410;&#28857;&#23376;&#38598;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#23558;&#26368;&#20339;&#36890;&#36947;&#30340;&#36873;&#25321;&#20998;&#24067;&#21040;WSN&#20013;&#30340;&#19981;&#21516;&#33410;&#28857;&#19978;&#12290;&#25105;&#20204;&#22312;&#36523;&#20307;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#20013;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a dynamic sensor selection approach for deep neural networks (DNNs), which is able to derive an optimal sensor subset selection for each specific input sample instead of a fixed selection for the entire dataset. This dynamic selection is jointly learned with the task model in an end-to-end way, using the Gumbel-Softmax trick to allow the discrete decisions to be learned through standard backpropagation. We then show how we can use this dynamic selection to increase the lifetime of a wireless sensor network (WSN) by imposing constraints on how often each node is allowed to transmit. We further improve performance by including a dynamic spatial filter that makes the task-DNN more robust against the fact that it now needs to be able to handle a multitude of possible node subsets. Finally, we explain how the selection of the optimal channels can be distributed across the different nodes in a WSN. We validate this method on a use case in the context of body-sensor networks, where
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#25104;&#21253;&#21547;&#36807;&#31243;&#25968;&#25454;&#21644;&#23545;&#24212;&#31243;&#24207;&#21270;&#30693;&#35782;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08371</link><description>&lt;p&gt;
PDPK: &#29992;&#20110;&#21046;&#36896;&#19994;&#30340;&#21512;&#25104;&#36807;&#31243;&#25968;&#25454;&#21644;&#23545;&#24212;&#31243;&#24207;&#21270;&#30693;&#35782;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PDPK: A Framework to Synthesise Process Data and Corresponding Procedural Knowledge for Manufacturing. (arXiv:2308.08371v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08371
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#25104;&#21253;&#21547;&#36807;&#31243;&#25968;&#25454;&#21644;&#23545;&#24212;&#31243;&#24207;&#21270;&#30693;&#35782;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#23884;&#20837;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21270;&#30693;&#35782;&#25551;&#36848;&#20102;&#22914;&#20309;&#23436;&#25104;&#20219;&#21153;&#21644;&#35299;&#20915;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#30693;&#35782;&#36890;&#24120;&#30001;&#39046;&#22495;&#19987;&#23478;&#25345;&#26377;&#65292;&#20363;&#22914;&#21046;&#36896;&#19994;&#20013;&#35843;&#25972;&#21442;&#25968;&#20197;&#36798;&#21040;&#36136;&#37327;&#30446;&#26631;&#30340;&#25805;&#20316;&#21592;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#21253;&#21547;&#36807;&#31243;&#25968;&#25454;&#21644;&#23545;&#24212;&#31243;&#24207;&#21270;&#30693;&#35782;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30446;&#21069;&#36824;&#19981;&#23384;&#22312;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#20225;&#19994;&#23545;&#30693;&#35782;&#36827;&#23637;&#30340;&#25439;&#22833;&#23384;&#22312;&#25285;&#24515;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#39046;&#22495;&#30340;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#30340;&#35774;&#35745;&#36873;&#25321;&#21463;&#21040;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#30340;&#20004;&#20010;&#23454;&#38469;&#30340;&#31243;&#24207;&#21270;&#30693;&#35782;&#25968;&#25454;&#38598;&#30340;&#21551;&#21457;&#12290;&#38500;&#20102;&#21253;&#21547;&#31526;&#21512;&#36164;&#28304;&#25551;&#36848;&#26694;&#26550;&#65288;RDF&#65289;&#26631;&#20934;&#30340;&#30693;&#35782;&#22270;&#24418;&#20013;&#30340;&#31243;&#24207;&#21270;&#30693;&#35782;&#30340;&#34920;&#31034;&#22806;&#65292;&#35813;&#26694;&#26550;&#36824;&#27169;&#25311;&#21442;&#25968;&#21270;&#36807;&#31243;&#24182;&#25552;&#20379;&#19968;&#33268;&#30340;&#36807;&#31243;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22312;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#24418;&#19978;&#30340;&#24050;&#24314;&#31435;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;&#21738;&#20123;&#24320;&#31665;&#21363;&#29992;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#26032;&#35299;&#37322;&#30693;&#35782;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural knowledge describes how to accomplish tasks and mitigate problems. Such knowledge is commonly held by domain experts, e.g. operators in manufacturing who adjust parameters to achieve quality targets. To the best of our knowledge, no real-world datasets containing process data and corresponding procedural knowledge are publicly available, possibly due to corporate apprehensions regarding the loss of knowledge advances. Therefore, we provide a framework to generate synthetic datasets that can be adapted to different domains. The design choices are inspired by two real-world datasets of procedural knowledge we have access to. Apart from containing representations of procedural knowledge in Resource Description Framework (RDF)-compliant knowledge graphs, the framework simulates parametrisation processes and provides consistent process data. We compare established embedding methods on the resulting knowledge graphs, detailing which out-of-the-box methods have the potential to rep
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#30340;&#21452;&#20998;&#25903;&#28201;&#24230;&#32553;&#25918;&#26657;&#20934;&#26041;&#27861;&#65288;Dual-TS&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#31867;&#21035;&#28201;&#24230;&#21442;&#25968;&#30340;&#19981;&#21516;&#24615;&#21644;&#38750;&#36890;&#29992;&#24615;&#65292;&#35299;&#20915;&#20102;&#26679;&#26412;&#31232;&#26377;&#38382;&#39064;&#21644;&#35757;&#32451;&#38598;&#39564;&#35777;&#38598;&#28201;&#24230;&#31995;&#25968;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.08366</link><description>&lt;p&gt;
&#38271;&#23614;&#35782;&#21035;&#30340;&#21452;&#20998;&#25903;&#28201;&#24230;&#32553;&#25918;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dual-Branch Temperature Scaling Calibration for Long-Tailed Recognition. (arXiv:2308.08366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#30340;&#21452;&#20998;&#25903;&#28201;&#24230;&#32553;&#25918;&#26657;&#20934;&#26041;&#27861;&#65288;Dual-TS&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#31867;&#21035;&#28201;&#24230;&#21442;&#25968;&#30340;&#19981;&#21516;&#24615;&#21644;&#38750;&#36890;&#29992;&#24615;&#65292;&#35299;&#20915;&#20102;&#26679;&#26412;&#31232;&#26377;&#38382;&#39064;&#21644;&#35757;&#32451;&#38598;&#39564;&#35777;&#38598;&#28201;&#24230;&#31995;&#25968;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#21644;&#30740;&#31350;&#12290;&#26657;&#20934;&#19981;&#20934;&#30830;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#12290;&#22312;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#30340;&#26465;&#20214;&#19979;&#65292;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#26356;&#20026;&#31361;&#20986;&#65292;&#22240;&#20026;&#23569;&#25968;&#31867;&#21644;&#22810;&#25968;&#31867;&#26679;&#26412;&#30340;&#32622;&#20449;&#27700;&#24179;&#19981;&#21516;&#65292;&#20250;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#36807;&#20110;&#33258;&#20449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#24403;&#21069;&#30740;&#31350;&#22522;&#20110;&#28201;&#24230;&#32553;&#25918;&#65288;TS&#65289;&#26041;&#27861;&#35774;&#35745;&#20102;&#19981;&#21516;&#31867;&#21035;&#30340;&#19981;&#21516;&#28201;&#24230;&#31995;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#25968;&#31867;&#20013;&#23384;&#22312;&#31232;&#26377;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#28201;&#24230;&#31995;&#25968;&#19981;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#19988;&#35757;&#32451;&#38598;&#21644;&#39564;&#35777;&#38598;&#20043;&#38388;&#30340;&#28201;&#24230;&#31995;&#25968;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20998;&#25903;&#28201;&#24230;&#32553;&#25918;&#26657;&#20934;&#27169;&#22411;&#65288;Dual-TS&#65289;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;&#31867;&#21035;&#30340;&#28201;&#24230;&#21442;&#25968;&#30340;&#19981;&#21516;&#24615;&#21644;&#38750;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The calibration for deep neural networks is currently receiving widespread attention and research. Miscalibration usually leads to overconfidence of the model. While, under the condition of long-tailed distribution of data, the problem of miscalibration is more prominent due to the different confidence levels of samples in minority and majority categories, and it will result in more serious overconfidence. To address this problem, some current research have designed diverse temperature coefficients for different categories based on temperature scaling (TS) method. However, in the case of rare samples in minority classes, the temperature coefficient is not generalizable, and there is a large difference between the temperature coefficients of the training set and the validation set. To solve this challenge, this paper proposes a dual-branch temperature scaling calibration model (Dual-TS), which considers the diversities in temperature parameters of different categories and the non-genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KernelWarehouse&#65292;&#23427;&#26159;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#21160;&#24577;&#21367;&#31215;&#24418;&#24335;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#21160;&#24577;&#21367;&#31215;&#20013;&#30340;&#21367;&#31215;&#26680;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#34920;&#31034;&#33021;&#21147;&#20043;&#38388;&#30340;&#26377;&#21033;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.08361</link><description>&lt;p&gt;
KernelWarehouse&#65306;&#26397;&#30528;&#21442;&#25968;&#26377;&#25928;&#30340;&#21160;&#24577;&#21367;&#31215;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
KernelWarehouse: Towards Parameter-Efficient Dynamic Convolution. (arXiv:2308.08361v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KernelWarehouse&#65292;&#23427;&#26159;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#21160;&#24577;&#21367;&#31215;&#24418;&#24335;&#65292;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#21160;&#24577;&#21367;&#31215;&#20013;&#30340;&#21367;&#31215;&#26680;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#34920;&#31034;&#33021;&#21147;&#20043;&#38388;&#30340;&#26377;&#21033;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#21367;&#31215;&#23398;&#20064;&#19968;&#31181;&#24102;&#26377;&#26679;&#26412;&#30456;&#20851;&#27880;&#24847;&#21147;&#26435;&#37325;&#30340;$n$&#20010;&#38745;&#24577;&#21367;&#31215;&#26680;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#19982;&#26222;&#36890;&#21367;&#31215;&#30456;&#27604;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35774;&#35745;&#22312;&#21442;&#25968;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#65306;&#23427;&#20204;&#23558;&#21367;&#31215;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#20102;$n$&#20493;&#12290;&#36825;&#19982;&#20248;&#21270;&#22256;&#38590;&#23548;&#33268;&#20102;&#21160;&#24577;&#21367;&#31215;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#32531;&#24930;&#65292;&#26080;&#27861;&#20351;&#29992;&#22823;&#30340;$n$&#20540;&#65288;&#20363;&#22914;$n&gt;100$&#32780;&#19981;&#26159;&#20856;&#22411;&#35774;&#32622;$n&lt;10$&#65289;&#26469;&#25512;&#21160;&#24615;&#33021;&#36793;&#30028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KernelWarehouse&#65292;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#21160;&#24577;&#21367;&#31215;&#24418;&#24335;&#65292;&#23427;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#34920;&#31034;&#33021;&#21147;&#20043;&#38388;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#24179;&#34913;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#20174;&#20943;&#23569;&#21367;&#31215;&#26680;&#32500;&#24230;&#21644;&#26174;&#33879;&#22686;&#21152;&#21367;&#31215;&#26680;&#25968;&#37327;&#30340;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#21160;&#24577;&#21367;&#31215;&#20013;&#30340;"$kernels$"&#21644;"$assembling$ $kernels$"&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;&#21407;&#21017;&#19978;&#65292;KernelWarehouse&#22686;&#24378;&#20102;&#21367;&#31215;&#21442;&#25968;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic convolution learns a linear mixture of $n$ static kernels weighted with their sample-dependent attentions, demonstrating superior performance compared to normal convolution. However, existing designs are parameter-inefficient: they increase the number of convolutional parameters by $n$ times. This and the optimization difficulty lead to no research progress in dynamic convolution that can allow us to use a significant large value of $n$ (e.g., $n&gt;100$ instead of typical setting $n&lt;10$) to push forward the performance boundary. In this paper, we propose $KernelWarehouse$, a more general form of dynamic convolution, which can strike a favorable trade-off between parameter efficiency and representation power. Its key idea is to redefine the basic concepts of "$kernels$" and "$assembling$ $kernels$" in dynamic convolution from the perspective of reducing kernel dimension and increasing kernel number significantly. In principle, KernelWarehouse enhances convolutional parameter depen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20998;&#24067;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#31169;&#26377;&#22270;&#23884;&#20837;&#30340;&#38544;&#31169;&#12290;&#36890;&#36807;&#32771;&#34385;&#20027;&#35201;&#23398;&#20064;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#38656;&#25317;&#26377;&#25152;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#38480;&#21046;&#65292;&#24182;&#35299;&#20915;&#20102;&#38544;&#31169;&#20445;&#25252;&#34920;&#31034;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08360</link><description>&lt;p&gt;
&#29420;&#31435;&#20998;&#24067;&#27491;&#21017;&#21270;&#29992;&#20110;&#31169;&#26377;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Independent Distribution Regularization for Private Graph Embedding. (arXiv:2308.08360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08360
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#20998;&#24067;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#31169;&#26377;&#22270;&#23884;&#20837;&#30340;&#38544;&#31169;&#12290;&#36890;&#36807;&#32771;&#34385;&#20027;&#35201;&#23398;&#20064;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#38656;&#25317;&#26377;&#25152;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#38480;&#21046;&#65292;&#24182;&#35299;&#20915;&#20102;&#38544;&#31169;&#20445;&#25252;&#34920;&#31034;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22270;&#23884;&#20837;&#26159;&#22270;&#25366;&#25496;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#19968;&#20010;&#26377;&#25928;&#30340;&#22270;&#23884;&#20837;&#27169;&#22411;&#21487;&#20197;&#20174;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#20302;&#32500;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#25968;&#25454;&#21457;&#24067;&#24102;&#26469;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#30340;&#22909;&#22788;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#36335;&#39044;&#27979;&#31561;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#23884;&#20837;&#23481;&#26131;&#21463;&#21040;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20174;&#23398;&#20064;&#21040;&#30340;&#22270;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#31169;&#26377;&#33410;&#28857;&#23646;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#20445;&#25252;&#38544;&#31169;&#30340;&#22270;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#21516;&#26102;&#32771;&#34385;&#20027;&#35201;&#23398;&#20064;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20551;&#35774;&#34920;&#31034;&#27169;&#22411;&#22312;&#35757;&#32451;&#38454;&#27573;&#25317;&#26377;&#25152;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#35775;&#38382;&#26435;&#65292;&#20294;&#30001;&#20110;&#19981;&#21516;&#30340;&#38544;&#31169;&#20559;&#22909;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#38544;&#31169;&#20445;&#25252;&#34920;&#31034;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#23545;&#25239;&#23398;&#20064;&#25216;&#26415;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning graph embeddings is a crucial task in graph mining tasks. An effective graph embedding model can learn low-dimensional representations from graph-structured data for data publishing benefiting various downstream applications such as node classification, link prediction, etc. However, recent studies have revealed that graph embeddings are susceptible to attribute inference attacks, which allow attackers to infer private node attributes from the learned graph embeddings. To address these concerns, privacy-preserving graph embedding methods have emerged, aiming to simultaneously consider primary learning and privacy protection through adversarial learning. However, most existing methods assume that representation models have access to all sensitive attributes in advance during the training stage, which is not always the case due to diverse privacy preferences. Furthermore, the commonly used adversarial learning technique in privacy-preserving representation learning suffers from 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20004;&#23618;&#38750;&#32447;&#24615;&#21333;&#20803;&#22238;&#24402;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;softmax ReLU&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;Hessian&#30340;&#24615;&#36136;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#36817;&#20284;&#29275;&#39039;&#27861;&#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#26368;&#21518;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08358</link><description>&lt;p&gt;
&#20004;&#23618;&#38750;&#32447;&#24615;&#21333;&#20803;&#22238;&#24402;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of Two-Layer Regression with Nonlinear Units. (arXiv:2308.08358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20004;&#23618;&#38750;&#32447;&#24615;&#21333;&#20803;&#22238;&#24402;&#30340;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;softmax ReLU&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;Hessian&#30340;&#24615;&#36136;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#36817;&#20284;&#29275;&#39039;&#27861;&#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#26368;&#21518;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT4&#65292;&#22312;&#35768;&#22810;&#20154;&#31867;&#29983;&#27963;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#27880;&#24847;&#21147;&#35745;&#31639;&#22312;&#35757;&#32451;LLMs&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;Softmax&#21333;&#20803;&#21644;ReLU&#21333;&#20803;&#26159;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#20851;&#38190;&#32467;&#26500;&#12290;&#21463;&#21040;&#23427;&#20204;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;softmax ReLU&#22238;&#24402;&#38382;&#39064;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#28041;&#21450;ReLU&#21333;&#20803;&#30340;&#22238;&#24402;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;Hessian&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#31034;&#12290;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Hessian&#30340;Lipschitz&#36830;&#32493;&#24615;&#21644;PSD&#24615;&#36136;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#36817;&#20284;&#29275;&#39039;&#27861;&#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#36317;&#31163;&#26368;&#20248;&#35299;&#30340;&#24847;&#20041;&#19979;&#25910;&#25947;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25918;&#23485;&#20102;Lipschitz&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25439;&#22833;&#20540;&#30340;&#24847;&#20041;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT and GPT4, have shown outstanding performance in many human life task. Attention computation plays an important role in training LLMs. Softmax unit and ReLU unit are the key structure in attention computation. Inspired by them, we put forward a softmax ReLU regression problem. Generally speaking, our goal is to find an optimal solution to the regression problem involving the ReLU unit. In this work, we calculate a close form representation for the Hessian of the loss function. Under certain assumptions, we prove the Lipschitz continuous and the PSDness of the Hessian. Then, we introduce an greedy algorithm based on approximate Newton method, which converges in the sense of the distance to optimal solution. Last, We relax the Lipschitz condition and prove the convergence in the sense of loss value.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#20803;&#23398;&#20064;&#25216;&#26415;&#22312;&#22788;&#29702;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#24050;&#25104;&#20026;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#24182;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#25317;&#26377;&#24222;&#22823;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#25968;&#37327;&#65292;&#19988;&#26377;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.08354</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#21542;&#26159;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#27491;&#30830;&#26041;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Meta-Learning the Right Approach for the Cold-Start Problem in Recommender Systems?. (arXiv:2308.08354v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#20803;&#23398;&#20064;&#25216;&#26415;&#22312;&#22788;&#29702;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#24050;&#25104;&#20026;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#24182;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#36825;&#20123;&#31995;&#32479;&#25317;&#26377;&#24222;&#22823;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#25968;&#37327;&#65292;&#19988;&#26377;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#22312;&#32447;&#20135;&#21697;&#21644;&#26381;&#21153;&#30340;&#22522;&#30784;&#26500;&#24314;&#27169;&#22359;&#65292;&#24182;&#23545;&#29992;&#25143;&#20307;&#39564;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21560;&#24341;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#29616;&#20195;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#20919;&#21551;&#21160;&#35774;&#32622;&#19979;&#30340;&#25512;&#33616;&#38382;&#39064;&#65292;&#20363;&#22914;&#24403;&#29992;&#25143;&#22312;&#31995;&#32479;&#20013;&#30340;&#20114;&#21160;&#26377;&#38480;&#26102;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#36828;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#65292;&#26368;&#36817;&#24050;&#25104;&#20026;&#23398;&#26415;&#30740;&#31350;&#25991;&#29486;&#20013;&#22788;&#29702;&#25512;&#33616;&#31995;&#32479;&#20013;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#25317;&#26377;&#25968;&#21313;&#20159;&#29992;&#25143;&#21644;&#29289;&#21697;&#20197;&#21450;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#30340;&#29616;&#23454;&#25512;&#33616;&#31995;&#32479;&#26469;&#35828;&#24182;&#19981;&#23454;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#29992;&#22522;&#20934;&#19978;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#39640;&#24615;&#33021;&#26159;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have become fundamental building blocks of modern online products and services, and have a substantial impact on user experience. In the past few years, deep learning methods have attracted a lot of research, and are now heavily used in modern real-world recommender systems. Nevertheless, dealing with recommendations in the cold-start setting, e.g., when a user has done limited interactions in the system, is a problem that remains far from solved. Meta-learning techniques, and in particular optimization-based meta-learning, have recently become the most popular approaches in the academic research literature for tackling the cold-start problem in deep learning models for recommender systems. However, current meta-learning approaches are not practical for real-world recommender systems, which have billions of users and items, and strict latency requirements. In this paper we show that it is possible to obtaining similar, or higher, performance on commonly used benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;OOD-GMixup&#8221;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#22270;&#30340;&#24102;&#22806;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#22270;&#21512;&#29702;&#24615;&#21644;&#29983;&#25104;&#34394;&#25311;&#26679;&#26412;&#30340;&#26041;&#24335;&#26469;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08344</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#22270;&#30340;&#24102;&#22806;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph Out-of-Distribution Generalization with Controllable Data Augmentation. (arXiv:2308.08344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;OOD-GMixup&#8221;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#22270;&#30340;&#24102;&#22806;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#22270;&#21512;&#29702;&#24615;&#21644;&#29983;&#25104;&#34394;&#25311;&#26679;&#26412;&#30340;&#26041;&#24335;&#26469;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#23646;&#24615;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#36873;&#25321;&#20559;&#24046;&#65288;&#20363;&#22914;&#65292;&#22312;&#23567;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#22823;&#22270;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25110;&#22312;&#31264;&#23494;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#31232;&#30095;&#22270;&#19978;&#36827;&#34892;&#27979;&#35797;&#65289;&#65292;&#20998;&#24067;&#20559;&#24046;&#24456;&#26222;&#36941;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#32463;&#24120;&#35266;&#23519;&#21040;&#23613;&#31649;&#26377;&#21333;&#36793;&#20559;&#21521;&#30340;&#25968;&#25454;&#20998;&#21306;&#65292;&#20294;&#21364;&#23384;&#22312;&#30528;&#21516;&#26102;&#20855;&#26377;&#35268;&#27169;&#21644;&#23494;&#24230;&#30340;&#28151;&#21512;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#12290;&#28151;&#21512;&#20998;&#24067;&#20559;&#31227;&#20013;&#30340;&#20266;&#30456;&#20851;&#24615;&#38477;&#20302;&#20102;&#20808;&#21069;GNN&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;OOD-GMixup&#8221;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#20197;&#32852;&#21512;&#25805;&#20316;&#35757;&#32451;&#20998;&#24067;&#30340;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#21462;&#22270;&#21512;&#29702;&#24615;&#26469;&#28040;&#38500;&#30001;&#20110;&#19981;&#30456;&#20851;&#20449;&#24687;&#32780;&#24341;&#36215;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#22270;&#21512;&#29702;&#24615;&#36827;&#34892;&#25200;&#21160;&#29983;&#25104;&#34394;&#25311;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network (GNN) has demonstrated extraordinary performance in classifying graph properties. However, due to the selection bias of training and testing data (e.g., training on small graphs and testing on large graphs, or training on dense graphs and testing on sparse graphs), distribution deviation is widespread. More importantly, we often observe \emph{hybrid structure distribution shift} of both scale and density, despite of one-sided biased data partition. The spurious correlations over hybrid distribution deviation degrade the performance of previous GNN methods and show large instability among different datasets. To alleviate this problem, we propose \texttt{OOD-GMixup} to jointly manipulate the training distribution with \emph{controllable data augmentation} in metric space. Specifically, we first extract the graph rationales to eliminate the spurious correlations due to irrelevant information. Secondly, we generate virtual samples with perturbation on graph rationale r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.08334</link><description>&lt;p&gt;
&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning Logic Programs by Discovering Higher-Order Abstractions. (arXiv:2308.08334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#26469;&#23398;&#20064;&#36923;&#36753;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#24182;&#20943;&#23569;&#23398;&#20064;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#26032;&#39062;&#30340;&#25277;&#35937;&#23545;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21457;&#29616;&#39640;&#38454;&#25277;&#35937;&#65288;&#20363;&#22914;map&#12289;filter&#21644;fold&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65292;&#21363;&#20174;&#31034;&#20363;&#21644;&#32972;&#26223;&#30693;&#35782;&#20013;&#24402;&#32435;&#36923;&#36753;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#38454;&#37325;&#26500;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#25277;&#35937;&#26469;&#21387;&#32553;&#36923;&#36753;&#31243;&#24207;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#22312;STEVIE&#20013;&#65292;&#23427;&#23558;&#39640;&#38454;&#37325;&#26500;&#38382;&#39064;&#24314;&#27169;&#20026;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#31243;&#24207;&#21512;&#25104;&#21644;&#35270;&#35273;&#25512;&#29702;&#65292;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#27809;&#26377;&#37325;&#26500;&#30456;&#27604;&#65292;STEVIE&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;27%&#24182;&#23558;&#23398;&#20064;&#26102;&#38388;&#20943;&#23569;47%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;STEVIE&#21487;&#20197;&#21457;&#29616;&#36866;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel abstractions is important for human-level AI. We introduce an approach to discover higher-order abstractions, such as map, filter, and fold. We focus on inductive logic programming, which induces logic programs from examples and background knowledge. We introduce the higher-order refactoring problem, where the goal is to compress a logic program by introducing higher-order abstractions. We implement our approach in STEVIE, which formulates the higher-order refactoring problem as a constraint optimisation problem. Our experimental results on multiple domains, including program synthesis and visual reasoning, show that, compared to no refactoring, STEVIE can improve predictive accuracies by 27% and reduce learning times by 47%. We also show that STEVIE can discover abstractions that transfer to different domains
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25197;&#26354;&#20960;&#20309;&#23398;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#20248;&#21270;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;&#37325;&#26032;&#23450;&#20041;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#25214;&#21040;&#20102;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.08305</link><description>&lt;p&gt;
&#22312;&#27431;&#20960;&#37324;&#24503;&#20989;&#25968;&#20248;&#21270;&#20013;&#30340;&#25197;&#26354;&#20960;&#20309;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Warped geometric information on the optimisation of Euclidean functions. (arXiv:2308.08305v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08305
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25197;&#26354;&#20960;&#20309;&#23398;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#20248;&#21270;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22312;&#37325;&#26032;&#23450;&#20041;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#25214;&#21040;&#20102;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#28508;&#22312;&#39640;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#20248;&#21270;&#23454;&#20540;&#20989;&#25968;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20363;&#22914;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#25110;&#32479;&#35745;&#25512;&#26029;&#20013;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#23545;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#25197;&#26354;&#40654;&#26364;&#20960;&#20309;&#27010;&#24565;&#65292;&#23558;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#19978;&#30340;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#24102;&#26377;&#25197;&#26354;&#24230;&#37327;&#30340;&#40654;&#26364;&#27969;&#24418;&#65292;&#24182;&#22312;&#35813;&#27969;&#24418;&#19978;&#25214;&#21040;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;&#36873;&#25321;&#29992;&#20110;&#25628;&#32034;&#22495;&#30340;&#25197;&#26354;&#24230;&#37327;&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#21451;&#22909;&#30340;&#24230;&#37327;&#24352;&#37327;&#65292;&#20351;&#24471;&#22312;&#27969;&#24418;&#19978;&#25214;&#21040;&#26368;&#20248;&#25628;&#32034;&#26041;&#21521;&#19982;&#27979;&#22320;&#32447;&#21464;&#24471;&#26356;&#23481;&#26131;&#35745;&#31639;&#12290;&#27839;&#27979;&#22320;&#32447;&#36827;&#34892;&#20248;&#21270;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#22312;&#36825;&#20010;&#29305;&#23450;&#30340;&#27969;&#24418;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#35299;&#26512;&#22320;&#24471;&#21040;&#39640;&#36798;&#19977;&#38454;&#30340;&#27888;&#21202;&#36817;&#20284;&#12290;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#23545;&#27979;&#22320;&#32447;&#30340;&#36817;&#20284;&#19981;&#20250;&#20301;&#20110;&#27969;&#24418;&#19978;&#65292;&#20294;&#25105;&#20204;&#26500;&#36896;&#20102;&#21512;&#36866;&#30340;&#22238;&#32553;&#26041;&#31243;&#23558;&#36825;&#20123;&#36817;&#20284;&#37325;&#26032;&#26144;&#23556;&#21040;&#27969;&#24418;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the fundamental task of optimizing a real-valued function defined in a potentially high-dimensional Euclidean space, such as the loss function in many machine-learning tasks or the logarithm of the probability distribution in statistical inference. We use the warped Riemannian geometry notions to redefine the optimisation problem of a function on Euclidean space to a Riemannian manifold with a warped metric, and then find the function's optimum along this manifold. The warped metric chosen for the search domain induces a computational friendly metric-tensor for which optimal search directions associate with geodesic curves on the manifold becomes easier to compute. Performing optimization along geodesics is known to be generally infeasible, yet we show that in this specific manifold we can analytically derive Taylor approximations up to third-order. In general these approximations to the geodesic curve will not lie on the manifold, however we construct suitable retraction m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RoBOS&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#28385;&#36275;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#19978;&#19979;&#25991;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#26102;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#19968;&#23450;&#30340;&#20998;&#24067;&#20559;&#31227;&#37327;&#19979;&#20445;&#35777;&#20111;&#24471;&#19981;&#20005;&#37325;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#20998;&#24067;&#20559;&#31227;&#37327;&#26080;&#20851;&#30340;&#36739;&#24369;&#36951;&#25022;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.08291</link><description>&lt;p&gt;
&#40065;&#26834;&#36125;&#21494;&#26031;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Bayesian Satisficing. (arXiv:2308.08291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RoBOS&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#28385;&#36275;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#19978;&#19979;&#25991;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#26102;&#30340;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#19968;&#23450;&#30340;&#20998;&#24067;&#20559;&#31227;&#37327;&#19979;&#20445;&#35777;&#20111;&#24471;&#19981;&#20005;&#37325;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#20998;&#24067;&#20559;&#31227;&#37327;&#26080;&#20851;&#30340;&#36739;&#24369;&#36951;&#25022;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#23545;&#20110;&#23454;&#29616;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#40065;&#26834;&#28385;&#36275;&#65288;RS&#65289;&#22312;&#23454;&#29616;&#36229;&#36807;&#26399;&#26395;&#38408;&#20540;&#30340;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#23547;&#27714;&#23545;&#20110;&#26410;&#25351;&#23450;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#40065;&#26834;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#19978;&#19979;&#25991;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#23384;&#22312;&#30495;&#23454;&#21644;&#21442;&#32771;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#26102;&#30340;&#40065;&#26834;&#28385;&#36275;&#65288;RS&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RoBOS&#30340;&#26032;&#22411;&#22122;&#22768;&#40657;&#31665;&#20248;&#21270;&#30340;&#40065;&#26834;&#36125;&#21494;&#26031;&#28385;&#36275;&#31639;&#27861;&#12290;&#22312;&#26576;&#20123;&#20851;&#20110;&#20998;&#24067;&#20559;&#31227;&#37327;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#20111;&#24471;&#19981;&#20005;&#37325;&#30340;&#23376;&#32447;&#24615;&#36951;&#25022;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#36739;&#24369;&#30340;&#36951;&#25022;&#27010;&#24565;&#65292;&#31216;&#20026;&#40065;&#26834;&#28385;&#36275;&#36951;&#25022;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19982;&#20998;&#24067;&#20559;&#31227;&#37327;&#26080;&#20851;&#30340;&#23376;&#32447;&#24615;&#19978;&#30028;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22914;&#20998;&#24067;&#20132;&#25442;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional shifts pose a significant challenge to achieving robustness in contemporary machine learning. To overcome this challenge, robust satisficing (RS) seeks a robust solution to an unspecified distributional shift while achieving a utility above a desired threshold. This paper focuses on the problem of RS in contextual Bayesian optimization when there is a discrepancy between the true and reference distributions of the context. We propose a novel robust Bayesian satisficing algorithm called RoBOS for noisy black-box optimization. Our algorithm guarantees sublinear lenient regret under certain assumptions on the amount of distribution shift. In addition, we define a weaker notion of regret called robust satisficing regret, in which our algorithm achieves a sublinear upper bound independent of the amount of distribution shift. To demonstrate the effectiveness of our method, we apply it to various learning problems and compare it to other approaches, such as distributionally rob
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;DFedADMM&#21644;&#20854;&#25913;&#36827;&#29256;&#26412;DFedADMM-SAM&#65292;&#29992;&#20110;&#35299;&#20915;&#23616;&#37096;&#19981;&#19968;&#33268;&#24615;&#21644;&#23616;&#37096;&#24322;&#26500;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08290</link><description>&lt;p&gt;
DFedADMM&#65306;&#29992;&#20110;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#21452;&#37325;&#32422;&#26463;&#25511;&#21046;&#27169;&#22411;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
DFedADMM: Dual Constraints Controlled Model Inconsistency for Decentralized Federated Learning. (arXiv:2308.08290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08290
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;DFedADMM&#21644;&#20854;&#25913;&#36827;&#29256;&#26412;DFedADMM-SAM&#65292;&#29992;&#20110;&#35299;&#20915;&#23616;&#37096;&#19981;&#19968;&#33268;&#24615;&#21644;&#23616;&#37096;&#24322;&#26500;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#36127;&#25285;&#38382;&#39064;&#65292;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#33293;&#24323;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#24314;&#31435;&#20998;&#25955;&#24335;&#36890;&#20449;&#32593;&#32476;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#20165;&#19982;&#30456;&#37051;&#23458;&#25143;&#31471;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DFL&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23616;&#37096;&#19981;&#19968;&#33268;&#24615;&#21644;&#23616;&#37096;&#24322;&#26500;&#36807;&#25311;&#21512;&#65292;&#36825;&#20123;&#38382;&#39064;&#23578;&#26410;&#20174;&#26681;&#26412;&#19978;&#24471;&#21040;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;DFL&#31639;&#27861;DFedADMM&#21450;&#20854;&#25913;&#36827;&#29256;&#26412;DFedADMM-SAM&#65292;&#20197;&#25552;&#39640;DFL&#30340;&#24615;&#33021;&#12290;DFedADMM&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#21452;&#21464;&#37327;&#26469;&#25511;&#21046;&#30001;&#20998;&#25955;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#23548;&#33268;&#30340;&#27169;&#22411;&#19981;&#19968;&#33268;&#24615;&#26469;&#20351;&#29992;&#21407;&#22987;-&#23545;&#20598;&#20248;&#21270;&#65288;ADMM&#65289;&#12290;DFedADMM-SAM&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#25200;&#21160;&#26469;&#29983;&#25104;&#23616;&#37096;&#24179;&#22374;&#27169;&#22411;&#24182;&#23547;&#25214;&#27169;&#22411;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;DFedADMM&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the communication burden issues associated with federated learning (FL), decentralized federated learning (DFL) discards the central server and establishes a decentralized communication network, where each client communicates only with neighboring clients. However, existing DFL methods still suffer from two major challenges: local inconsistency and local heterogeneous overfitting, which have not been fundamentally addressed by existing DFL methods. To tackle these issues, we propose novel DFL algorithms, DFedADMM and its enhanced version DFedADMM-SAM, to enhance the performance of DFL. The DFedADMM algorithm employs primal-dual optimization (ADMM) by utilizing dual variables to control the model inconsistency raised from the decentralized heterogeneous data distributions. The DFedADMM-SAM algorithm further improves on DFedADMM by employing a Sharpness-Aware Minimization (SAM) optimizer, which uses gradient perturbations to generate locally flat models and searches for models
&lt;/p&gt;</description></item><item><title>CARE&#26159;&#19968;&#31181;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30452;&#32928;&#30284;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20020;&#24202;&#36866;&#29992;&#30340;&#22522;&#20934;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27491;&#24120;&#21644;&#30284;&#24615;&#30452;&#32928;&#30340;&#20687;&#32032;&#32423;&#27880;&#37322;&#65292;&#20026;&#31639;&#27861;&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#24320;&#21457;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.08283</link><description>&lt;p&gt;
CARE: &#19968;&#31181;&#29992;&#20110;&#30452;&#32928;&#30284;&#20998;&#21106;&#30340;&#22823;&#35268;&#27169;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20020;&#24202;&#36866;&#29992;&#30340;&#22522;&#20934;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CARE: A Large Scale CT Image Dataset and Clinical Applicable Benchmark Model for Rectal Cancer Segmentation. (arXiv:2308.08283v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08283
&lt;/p&gt;
&lt;p&gt;
CARE&#26159;&#19968;&#31181;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30452;&#32928;&#30284;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20020;&#24202;&#36866;&#29992;&#30340;&#22522;&#20934;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#27491;&#24120;&#21644;&#30284;&#24615;&#30452;&#32928;&#30340;&#20687;&#32032;&#32423;&#27880;&#37322;&#65292;&#20026;&#31639;&#27861;&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#24320;&#21457;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CT&#22270;&#20687;&#30340;&#30452;&#32928;&#30284;&#20998;&#21106;&#22312;&#21450;&#26102;&#20020;&#24202;&#35786;&#26029;&#12289;&#25918;&#30103;&#27835;&#30103;&#21644;&#38543;&#35775;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#30446;&#21069;&#30340;&#20998;&#21106;&#26041;&#27861;&#22312;&#25551;&#32472;&#30284;&#32452;&#32455;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#19968;&#23450;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#28982;&#22312;&#36798;&#21040;&#39640;&#20998;&#21106;&#31934;&#24230;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#38590;&#39064;&#28304;&#20110;&#30452;&#32928;&#22797;&#26434;&#30340;&#35299;&#21078;&#32467;&#26500;&#20197;&#21450;&#36827;&#34892;&#30452;&#32928;&#30284;&#37492;&#21035;&#35786;&#26029;&#30340;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#29992;&#20110;&#30452;&#32928;&#30284;&#20998;&#21106;&#30340;&#22823;&#35268;&#27169;&#12289;&#31934;&#32454;&#26631;&#27880;&#30340;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22823;&#35268;&#27169;&#30452;&#32928;&#30284;CT&#22270;&#20687;&#25968;&#25454;&#38598;CARE&#65292;&#24182;&#25552;&#20379;&#20102;&#27491;&#24120;&#21644;&#30284;&#24615;&#30452;&#32928;&#30340;&#20687;&#32032;&#32423;&#27880;&#37322;&#65292;&#36825;&#20026;&#31639;&#27861;&#30740;&#31350;&#21644;&#20020;&#24202;&#24212;&#29992;&#24320;&#21457;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;U-SAM&#30340;&#26032;&#22411;&#21307;&#23398;&#30284;&#30151;&#30149;&#21464;&#20998;&#21106;&#22522;&#20934;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24212;&#23545;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rectal cancer segmentation of CT image plays a crucial role in timely clinical diagnosis, radiotherapy treatment, and follow-up. Although current segmentation methods have shown promise in delineating cancerous tissues, they still encounter challenges in achieving high segmentation precision. These obstacles arise from the intricate anatomical structures of the rectum and the difficulties in performing differential diagnosis of rectal cancer. Additionally, a major obstacle is the lack of a large-scale, finely annotated CT image dataset for rectal cancer segmentation. To address these issues, this work introduces a novel large scale rectal cancer CT image dataset CARE with pixel-level annotations for both normal and cancerous rectum, which serves as a valuable resource for algorithm research and clinical application development. Moreover, we propose a novel medical cancer lesion segmentation benchmark model named U-SAM. The model is specifically designed to tackle the challenges posed b
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;OOD&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#31070;&#31192;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27867;&#21270;&#25968;&#23383;&#36816;&#31639;&#26102;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.08268</link><description>&lt;p&gt;
&#23427;&#20854;&#23454;&#19981;&#37027;&#20040;&#31967;&#31957;&#65306;&#29702;&#35299;&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#23545;OOD&#27867;&#21270;&#30340;&#31070;&#31192;&#24615;&#33021;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
It Ain't That Bad: Understanding the Mysterious Performance Drop in OOD Generalization for Generative Transformer Models. (arXiv:2308.08268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08268
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;OOD&#27867;&#21270;&#26041;&#38754;&#23384;&#22312;&#31070;&#31192;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27867;&#21270;&#25968;&#23383;&#36816;&#31639;&#26102;&#30340;&#34892;&#20026;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65292;&#24182;&#23581;&#35797;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#21464;&#25442;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#27809;&#26377;&#23436;&#20840;&#34987;&#29702;&#35299;&#65292;&#24182;&#19988;&#24182;&#19981;&#24635;&#26159;&#20196;&#20154;&#28385;&#24847;&#12290;&#30740;&#31350;&#20154;&#21592;&#20174;&#22522;&#26412;&#30340;&#25968;&#23398;&#20219;&#21153;&#65288;&#22914;n&#20301;&#25968;&#30340;&#21152;&#27861;&#25110;&#20056;&#27861;&#65289;&#24320;&#22987;&#65292;&#20316;&#20026;&#37325;&#35201;&#35270;&#35282;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#35266;&#23519;&#21040;&#24403;&#27169;&#22411;&#22312;n&#20301;&#25968;&#36816;&#31639;&#65288;&#20363;&#22914;&#21152;&#27861;&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#38271;&#24230;&#20026;n&#20301;&#30340;&#36755;&#20837;&#19978;&#21487;&#20197;&#25104;&#21151;&#27867;&#21270;&#65288;&#21363;&#20869;&#20998;&#24067;&#27867;&#21270;&#65289;&#65292;&#20294;&#22312;&#38271;&#24230;&#26356;&#38271;&#12289;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#19979;&#65288;&#21363;&#22806;&#20998;&#24067;&#27867;&#21270;&#65289;&#20250;&#22833;&#36133;&#24182;&#19988;&#34920;&#29616;&#31070;&#31192;&#12290;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#20462;&#25913;&#20301;&#32622;&#23884;&#20837;&#12289;&#24494;&#35843;&#21644;&#24341;&#20837;&#26356;&#24191;&#27867;&#25110;&#26356;&#26377;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#35299;&#20915;&#26412;&#36136;&#26426;&#21046;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#31283;&#20581;&#24615;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Transformer-based models have achieved remarkable proficiency on solving diverse problems. However, their generalization ability is not fully understood and not always satisfying. Researchers take basic mathematical tasks like n-digit addition or multiplication as important perspectives for investigating their generalization behaviors. Curiously, it is observed that when training on n-digit operations (e.g., additions) in which both input operands are n-digit in length, models generalize successfully on unseen n-digit inputs (in-distribution (ID) generalization), but fail miserably and mysteriously on longer, unseen cases (out-of-distribution (OOD) generalization). Studies try to bridge this gap with workarounds such as modifying position embedding, fine-tuning, and priming with more extensive or instructive data. However, without addressing the essential mechanism, there is hardly any guarantee regarding the robustness of these solutions. We bring this unexplained performan
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#22270;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#26159;&#22914;&#20309;&#22788;&#29702;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#20197;&#21450;&#22914;&#20309;&#22312;&#19981;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#20256;&#36755;&#36328;&#22270;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08259</link><description>&lt;p&gt;
&#22270;&#20851;&#31995;&#24863;&#30693;&#30340;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Relation Aware Continual Learning. (arXiv:2308.08259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08259
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22270;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#26159;&#22914;&#20309;&#22788;&#29702;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#20197;&#21450;&#22914;&#20309;&#22312;&#19981;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#20256;&#36755;&#36328;&#22270;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22270;&#23398;&#20064;&#65288;CGL&#65289;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#26080;&#38480;&#30340;&#22270;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#65292; consolidainge&#21644;&#23558;&#21382;&#21490;&#30693;&#35782;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#20219;&#21153;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21482;&#26377;&#24403;&#21069;&#30340;&#22270;&#25968;&#25454;&#21487;&#29992;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#19968;&#20123;&#23581;&#35797;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#28508;&#22312;&#30340;&#25361;&#25112;&#65306;1&#65289;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20316;&#21697;&#21482;&#22312;&#20013;&#38388;&#22270;&#23884;&#20837;&#19978;&#25805;&#20316;&#65292;&#24573;&#30053;&#20102;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#12290;&#36328;&#22270;&#20256;&#36755;&#20449;&#24687;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;2&#65289;&#26368;&#36817;&#30340;&#23581;&#35797;&#37319;&#29992;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#65292;&#22312;&#26102;&#38388;&#27493;&#19978;&#20256;&#36755;&#30693;&#35782;&#65292;&#25110;&#32773;&#26681;&#25454;&#36716;&#31227;&#30340;&#22270;&#20998;&#24067;&#36880;&#27493;&#25193;&#23637;&#26032;&#30340;&#26550;&#26500;&#12290;&#23398;&#20064;&#21333;&#19968;&#27169;&#22411;&#21487;&#33021;&#20250;&#20002;&#22833;&#27599;&#20010;&#22270;&#20219;&#21153;&#30340;&#26377;&#21306;&#21035;&#20449;&#24687;&#65292;&#32780;&#27169;&#22411;&#25193;&#23637;&#26041;&#26696;&#21017;&#38754;&#20020;&#27169;&#22411;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25351;&#20986;&#22270;&#36793;&#32536;&#21518;&#38754;&#30340;&#28508;&#22312;&#20851;&#31995;&#21487;&#20197;&#24402;&#22240;&#20110;&#21457;&#23637;&#20013;&#22270;&#30340;&#19968;&#20010;&#19981;&#21464;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual graph learning (CGL) studies the problem of learning from an infinite stream of graph data, consolidating historical knowledge, and generalizing it to the future task. At once, only current graph data are available. Although some recent attempts have been made to handle this task, we still face two potential challenges: 1) most of existing works only manipulate on the intermediate graph embedding and ignore intrinsic properties of graphs. It is non-trivial to differentiate the transferred information across graphs. 2) recent attempts take a parameter-sharing policy to transfer knowledge across time steps or progressively expand new architecture given shifted graph distribution. Learning a single model could loss discriminative information for each graph task while the model expansion scheme suffers from high model complexity. In this paper, we point out that latent relations behind graph edges can be attributed as an invariant factor for the evolving graphs and the statistica
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#32553;&#25918;&#24459;&#21487;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#36805;&#36895;&#20943;&#23567;&#65307;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#35823;&#24046;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#32531;&#24930;&#20943;&#23567;&#12290;&#36825;&#34920;&#26126;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#22312;&#25968;&#25454;&#20998;&#24067;&#33391;&#22909;&#26102;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#65292;&#32780;&#19981;&#26159;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.08247</link><description>&lt;p&gt;
&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#20004;&#20010;&#38454;&#27573;&#30340;&#32553;&#25918;&#24459;
&lt;/p&gt;
&lt;p&gt;
Two Phases of Scaling Laws for Nearest Neighbor Classifiers. (arXiv:2308.08247v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08247
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#32553;&#25918;&#24459;&#21487;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#36805;&#36895;&#20943;&#23567;&#65307;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#35823;&#24046;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#32531;&#24930;&#20943;&#23567;&#12290;&#36825;&#34920;&#26126;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#22312;&#25968;&#25454;&#20998;&#24067;&#33391;&#22909;&#26102;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#65292;&#32780;&#19981;&#26159;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#24459;&#26159;&#25351;&#24403;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#27169;&#22411;&#30340;&#27979;&#35797;&#24615;&#33021;&#20250;&#25552;&#39640;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#24555;&#36895;&#30340;&#32553;&#25918;&#24459;&#24847;&#21619;&#30528;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#21644;&#27169;&#22411;&#22823;&#23567;&#23601;&#33021;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#22686;&#21152;&#26356;&#22810;&#25968;&#25454;&#30340;&#22909;&#22788;&#21487;&#33021;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#32553;&#25918;&#24459;&#12290;&#25105;&#20204;&#21457;&#29616;&#32553;&#25918;&#24459;&#21487;&#33021;&#26377;&#20004;&#20010;&#38454;&#27573;&#65306;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#19988;&#24555;&#36895;&#20943;&#23567;&#65307;&#32780;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#35823;&#24046;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#24182;&#19988;&#20943;&#23567;&#24471;&#24930;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#26174;&#20102;&#25968;&#25454;&#20998;&#24067;&#22312;&#20915;&#23450;&#27867;&#21270;&#35823;&#24046;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;&#24403;&#25968;&#25454;&#20998;&#24067;&#33391;&#22909;&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#35823;&#24046;&#22810;&#39033;&#24335;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#65292;&#32780;&#19981;&#26159;&#25351;&#25968;&#22320;&#20381;&#36182;&#20110;&#25968;&#25454;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A scaling law refers to the observation that the test performance of a model improves as the number of training data increases. A fast scaling law implies that one can solve machine learning problems by simply boosting the data and the model sizes. Yet, in many cases, the benefit of adding more data can be negligible. In this work, we study the rate of scaling laws of nearest neighbor classifiers. We show that a scaling law can have two phases: in the first phase, the generalization error depends polynomially on the data dimension and decreases fast; whereas in the second phase, the error depends exponentially on the data dimension and decreases slowly. Our analysis highlights the complexity of the data distribution in determining the generalization error. When the data distributes benignly, our result suggests that nearest neighbor classifier can achieve a generalization error that depends polynomially, instead of exponentially, on the data dimension.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#24418;&#24335;&#23450;&#20041;&#19979;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;&#22270;&#29305;&#24449;&#22686;&#24378;&#12289;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;GNNs&#26550;&#26500;&#22686;&#24378;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.08235</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Graph Neural Networks: A Survey. (arXiv:2308.08235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#24418;&#24335;&#23450;&#20041;&#19979;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;&#22270;&#29305;&#24449;&#22686;&#24378;&#12289;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;GNNs&#26550;&#26500;&#22686;&#24378;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#35768;&#22810;&#19982;&#22270;&#30456;&#20851;&#30340;&#24212;&#29992;&#20013;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#38598;&#20013;&#22312;GNNs&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#21363;&#20854;&#34920;&#36798;&#33021;&#21147;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;GNNs&#30340;&#22270;&#21516;&#26500;&#35782;&#21035;&#33021;&#21147;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#21033;&#29992;&#23376;&#22270;&#35745;&#25968;&#21644;&#36830;&#25509;&#23398;&#20064;&#31561;&#23646;&#24615;&#26469;&#25551;&#36848;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36825;&#26356;&#21152;&#23454;&#38469;&#19988;&#26356;&#25509;&#36817;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20851;&#20110;&#27492;&#26041;&#21521;&#30340;&#32508;&#36848;&#35770;&#25991;&#21644;&#24320;&#28304;&#20179;&#24211;&#32508;&#21512;&#24635;&#32467;&#21644;&#35752;&#35770;&#36825;&#20123;&#27169;&#22411;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#19981;&#21516;&#24418;&#24335;&#23450;&#20041;&#19979;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#19977;&#20010;&#31867;&#21035;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#36848;&#65292;&#21363;&#22270;&#29305;&#24449;&#22686;&#24378;&#12289;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;GNNs&#26550;&#26500;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are effective machine learning models for many graph-related applications. Despite their empirical success, many research efforts focus on the theoretical limitations of GNNs, i.e., the GNNs expressive power. Early works in this domain mainly focus on studying the graph isomorphism recognition ability of GNNs, and recent works try to leverage the properties such as subgraph counting and connectivity learning to characterize the expressive power of GNNs, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for models for enhancing expressive power under different forms of definition. Concretely, the models are reviewed based on three categories, i.e., Graph feature enhancement, Graph topology enhancement, and GNNs architecture enhancement.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#36890;&#36807;&#23545;NLP&#20013;&#22522;&#20110;Transformer&#30340;MTL&#26041;&#27861;&#20197;&#21450;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#38454;&#27573;&#30340;&#25361;&#25112;&#36827;&#34892;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;&#30456;&#20851;&#39046;&#22495;&#30340;&#27010;&#36848;&#21644;&#21160;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.08234</link><description>&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Challenges and Opportunities of Using Transformer-Based Multi-Task Learning in NLP Through ML Lifecycle: A Survey. (arXiv:2308.08234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#36890;&#36807;&#23545;NLP&#20013;&#22522;&#20110;Transformer&#30340;MTL&#26041;&#27861;&#20197;&#21450;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#21508;&#38454;&#27573;&#30340;&#25361;&#25112;&#36827;&#34892;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;&#30456;&#20851;&#39046;&#22495;&#30340;&#27010;&#36848;&#21644;&#21160;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20174;&#35757;&#32451;&#21040;&#22312;&#29983;&#20135;&#20013;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#26377;&#25928;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#26356;&#26032;&#22810;&#20010;&#27169;&#22411;&#21487;&#33021;&#22797;&#26434;&#12289;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#20316;&#20026;&#25913;&#36827;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26412;&#35843;&#30740;&#39318;&#20808;&#27010;&#36848;&#20102;NLP&#20013;&#22522;&#20110;Transformer&#30340;MTL&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#20013;&#20351;&#29992;MTL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#24037;&#31243;&#12289;&#27169;&#22411;&#24320;&#21457;&#12289;&#37096;&#32626;&#21644;&#30417;&#25511;&#38454;&#27573;&#30340;&#25361;&#25112;&#12290;&#26412;&#39033;&#35843;&#30740;&#38598;&#20013;&#20110;&#22522;&#20110;Transformer&#30340;MTL&#26550;&#26500;&#65292;&#24182;&#25454;&#25105;&#20204;&#25152;&#30693;&#26159;&#39318;&#21019;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing adoption of natural language processing (NLP) models across industries has led to practitioners' need for machine learning systems to handle these models efficiently, from training to serving them in production. However, training, deploying, and updating multiple models can be complex, costly, and time-consuming, mainly when using transformer-based pre-trained language models. Multi-Task Learning (MTL) has emerged as a promising approach to improve efficiency and performance through joint training, rather than training separate models. Motivated by this, we first provide an overview of transformer-based MTL approaches in NLP. Then, we discuss the challenges and opportunities of using MTL approaches throughout typical ML lifecycle phases, specifically focusing on the challenges related to data engineering, model development, deployment, and monitoring phases. This survey focuses on transformer-based MTL architectures and, to the best of our knowledge, is novel in that it 
&lt;/p&gt;</description></item><item><title>SCQPTH&#26159;&#19968;&#31181;&#29992;&#20110;&#20984;&#20108;&#27425;&#35268;&#21010;&#30340;&#39640;&#25928;&#21487;&#24494;&#20998;&#30340;&#20998;&#35010;&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#26041;&#27861;&#65288;ADMM&#65289;&#21644;&#25805;&#20316;&#25286;&#20998;&#27714;&#35299;&#22120;&#23454;&#29616;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.08232</link><description>&lt;p&gt;
SCQPTH:&#29992;&#20110;&#20984;&#20108;&#27425;&#35268;&#21010;&#30340;&#39640;&#25928;&#21487;&#24494;&#20998;&#20998;&#35010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SCQPTH: an efficient differentiable splitting method for convex quadratic programming. (arXiv:2308.08232v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08232
&lt;/p&gt;
&lt;p&gt;
SCQPTH&#26159;&#19968;&#31181;&#29992;&#20110;&#20984;&#20108;&#27425;&#35268;&#21010;&#30340;&#39640;&#25928;&#21487;&#24494;&#20998;&#30340;&#20998;&#35010;&#31639;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#26041;&#27861;&#65288;ADMM&#65289;&#21644;&#25805;&#20316;&#25286;&#20998;&#27714;&#35299;&#22120;&#23454;&#29616;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SCQPTH&#65306;&#19968;&#31181;&#29992;&#20110;&#20984;&#20108;&#27425;&#35268;&#21010;&#30340;&#21487;&#24494;&#20998;&#19968;&#38454;&#20998;&#35010;&#26041;&#27861;&#12290;SCQPTH&#26694;&#26550;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#26041;&#27861;&#65288;ADMM&#65289;&#65292;&#36719;&#20214;&#23454;&#29616;&#21463;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20984;&#20108;&#27425;&#35268;&#21010;&#27714;&#35299;&#22120;OSQP&#30340;&#21551;&#21457;&#65292;&#35813;&#27714;&#35299;&#22120;&#26159;&#19968;&#31181;&#29992;&#20110;&#20984;&#20108;&#27425;&#35268;&#21010;&#30340;&#25805;&#20316;&#25286;&#20998;&#27714;&#35299;&#22120;&#12290;SCQPTH&#36719;&#20214;&#20316;&#20026;&#24320;&#28304;Python&#21253;&#25552;&#20379;&#65292;&#24182;&#21253;&#21547;&#35768;&#22810;&#31867;&#20284;&#30340;&#21151;&#33021;&#65292;&#21253;&#25324;&#26377;&#25928;&#37325;&#29992;&#30697;&#38453;&#20998;&#35299;&#12289;&#19981;&#21487;&#34892;&#24615;&#26816;&#27979;&#12289;&#33258;&#21160;&#32553;&#25918;&#21644;&#21442;&#25968;&#36873;&#25321;&#12290;&#21069;&#21521;&#20256;&#25773;&#31639;&#27861;&#22312;&#21407;&#22987;&#38382;&#39064;&#31354;&#38388;&#30340;&#32500;&#24230;&#19978;&#36827;&#34892;&#31639;&#23376;&#20998;&#35010;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#20855;&#26377;100-1000&#20010;&#20915;&#31574;&#21464;&#37327;&#21644;&#25104;&#21315;&#19978;&#19975;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#22823;&#35268;&#27169;&#20984;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;ADMM&#22266;&#23450;&#28857;&#26144;&#23556;&#36827;&#34892;&#38544;&#24335;&#24494;&#20998;&#26469;&#25191;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#20984;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#65292;SCQPTH&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#21487;&#20197;&#25552;&#20379;1&#20493;&#21040;10&#20493;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SCQPTH: a differentiable first-order splitting method for convex quadratic programs. The SCQPTH framework is based on the alternating direction method of multipliers (ADMM) and the software implementation is motivated by the state-of-the art solver OSQP: an operating splitting solver for convex quadratic programs (QPs). The SCQPTH software is made available as an open-source python package and contains many similar features including efficient reuse of matrix factorizations, infeasibility detection, automatic scaling and parameter selection. The forward pass algorithm performs operator splitting in the dimension of the original problem space and is therefore suitable for large scale QPs with $100-1000$ decision variables and thousands of constraints. Backpropagation is performed by implicit differentiation of the ADMM fixed-point mapping. Experiments demonstrate that for large scale QPs, SCQPTH can provide a $1\times - 10\times$ improvement in computational efficiency in com
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;Winograd&#21367;&#31215;&#22312;&#31070;&#32463;&#32593;&#32476;&#23481;&#38169;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#20174;&#19981;&#21516;&#31890;&#24230;&#65288;&#27169;&#22411;&#12289;&#23618;&#12289;&#25805;&#20316;&#31867;&#22411;&#65289;&#36827;&#34892;&#30340;&#32508;&#21512;&#23481;&#38169;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;Winograd&#21367;&#31215;&#33021;&#22815;&#38477;&#20302;&#23481;&#38169;&#35774;&#35745;&#24320;&#38144;&#65292;&#24182;&#19982;&#32463;&#20856;&#30340;&#23481;&#38169;&#35774;&#35745;&#26041;&#27861;&#26377;&#25928;&#32467;&#21512;&#65292;&#23454;&#29616;&#23545;&#36719;&#38169;&#35823;&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#31070;&#32463;&#32593;&#32476;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2308.08230</link><description>&lt;p&gt;
&#25506;&#32034;Winograd&#21367;&#31215;&#29992;&#20110;&#25104;&#26412;&#25928;&#30410;&#30340;&#31070;&#32463;&#32593;&#32476;&#23481;&#38169;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Winograd Convolution for Cost-effective Neural Network Fault Tolerance. (arXiv:2308.08230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;Winograd&#21367;&#31215;&#22312;&#31070;&#32463;&#32593;&#32476;&#23481;&#38169;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#35780;&#20272;&#20102;&#20174;&#19981;&#21516;&#31890;&#24230;&#65288;&#27169;&#22411;&#12289;&#23618;&#12289;&#25805;&#20316;&#31867;&#22411;&#65289;&#36827;&#34892;&#30340;&#32508;&#21512;&#23481;&#38169;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;Winograd&#21367;&#31215;&#33021;&#22815;&#38477;&#20302;&#23481;&#38169;&#35774;&#35745;&#24320;&#38144;&#65292;&#24182;&#19982;&#32463;&#20856;&#30340;&#23481;&#38169;&#35774;&#35745;&#26041;&#27861;&#26377;&#25928;&#32467;&#21512;&#65292;&#23454;&#29616;&#23545;&#36719;&#38169;&#35823;&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#31070;&#32463;&#32593;&#32476;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Winograd&#36890;&#24120;&#29992;&#20110;&#20248;&#21270;&#21367;&#31215;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20943;&#23569;&#20102;&#20056;&#27861;&#36816;&#31639;&#65292;&#20294;&#36890;&#24120;&#24573;&#30053;&#20102;Winograd&#24102;&#26469;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;Winograd&#21367;&#31215;&#22312;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23481;&#38169;&#24615;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;Winograd&#21367;&#31215;&#23481;&#38169;&#24615;&#65292;&#20174;&#27169;&#22411;&#12289;&#23618;&#21644;&#25805;&#20316;&#31867;&#22411;&#31561;&#19981;&#21516;&#31890;&#24230;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;Winograd&#21367;&#31215;&#30340;&#20869;&#22312;&#23481;&#38169;&#24615;&#22914;&#20309;&#19982;&#32463;&#20856;&#30340;&#23481;&#38169;&#35774;&#35745;&#26041;&#27861;&#65288;&#21253;&#25324;&#19977;&#37325;&#27169;&#22359;&#20887;&#20313;&#12289;&#23481;&#38169;&#37325;&#35757;&#32451;&#21644;&#32422;&#26463;&#28608;&#27963;&#20989;&#25968;&#65289;&#26377;&#25928;&#32467;&#21512;&#65292;&#23454;&#29616;&#23545;&#36719;&#38169;&#35823;&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#31070;&#32463;&#32593;&#32476;&#20445;&#25252;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;Winograd&#21367;&#31215;&#21487;&#20197;&#38477;&#20302;&#23481;&#38169;&#35774;&#35745;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Winograd is generally utilized to optimize convolution performance and computational efficiency because of the reduced multiplication operations, but the reliability issues brought by winograd are usually overlooked. In this work, we observe the great potential of winograd convolution in improving neural network (NN) fault tolerance. Based on the observation, we evaluate winograd convolution fault tolerance comprehensively from different granularities ranging from models, layers, and operation types for the first time. Then, we explore the use of inherent fault tolerance of winograd convolution for cost-effective NN protection against soft errors. Specifically, we mainly investigate how winograd convolution can be effectively incorporated with classical fault-tolerant design approaches including triple modular redundancy (TMR), fault-aware retraining, and constrained activation functions. According to our experiments, winograd convolution can reduce the fault-tolerant design overhead b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22266;&#26377;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32423;&#31354;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#21033;&#29992;&#36825;&#31181;&#20887;&#20313;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#24110;&#21161;&#25552;&#39640;&#20102;&#21442;&#25968;&#21033;&#29992;&#30340;&#25928;&#29575;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#22122;&#22768;&#33033;&#20914;&#12290;</title><link>http://arxiv.org/abs/2308.08227</link><description>&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22266;&#26377;&#20887;&#20313;&#24615;
&lt;/p&gt;
&lt;p&gt;
Inherent Redundancy in Spiking Neural Networks. (arXiv:2308.08227v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22266;&#26377;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32423;&#31354;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#21033;&#29992;&#36825;&#31181;&#20887;&#20313;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#24110;&#21161;&#25552;&#39640;&#20102;&#21442;&#25968;&#21033;&#29992;&#30340;&#25928;&#29575;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#22122;&#22768;&#33033;&#20914;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476; (SNNs) &#26159;&#19968;&#31181;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#24076;&#26395;&#30340;&#39640;&#25928;&#33021;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545; SNNs &#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#31232;&#30095;&#28608;&#27963;&#30340;&#20808;&#20837;&#20026;&#20027;&#30340;&#21360;&#35937;&#65292;&#23545;&#20110; SNNs &#20013;&#22266;&#26377;&#20887;&#20313;&#24615;&#30340;&#20998;&#26512;&#21644;&#20248;&#21270;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#36825;&#26679;&#23601;&#24178;&#25200;&#20102;&#22522;&#20110;&#33033;&#20914;&#30340;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#22312;&#20934;&#30830;&#24615;&#21644;&#33021;&#37327;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#20851;&#27880;&#20102;&#20851;&#20110; SNNs &#20013;&#22266;&#26377;&#20887;&#20313;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#20887;&#20313;&#24615;&#26159;&#30001;&#20110; SNNs &#30340;&#26102;&#31354;&#19981;&#21464;&#24615;&#24341;&#36215;&#30340;&#65292;&#36825;&#22686;&#24378;&#20102;&#21442;&#25968;&#21033;&#29992;&#30340;&#25928;&#29575;&#65292;&#20294;&#20063;&#24341;&#26469;&#20102;&#22823;&#37327;&#30340;&#22122;&#22768;&#33033;&#20914;&#12290;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26102;&#31354;&#19981;&#21464;&#24615;&#23545; SNNs &#30340;&#26102;&#31354;&#21160;&#24577;&#21644;&#33033;&#20914;&#28608;&#27963;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#21463;&#21040;&#36825;&#20123;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#32423;&#31354;&#38388;&#27880;&#24847;&#21147; (ASA) &#27169;&#22359;&#65292;&#20197;&#21033;&#29992; SNNs &#30340;&#20887;&#20313;&#24615;&#65292;&#35813;&#27169;&#22359;&#21487;&#20197;&#36890;&#36807;&#19968;&#23545;&#36866;&#24212;&#24615;&#20248;&#21270;&#33180;&#30005;&#21183;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are well known as a promising energy-efficient alternative to conventional artificial neural networks. Subject to the preconceived impression that SNNs are sparse firing, the analysis and optimization of inherent redundancy in SNNs have been largely overlooked, thus the potential advantages of spike-based neuromorphic computing in accuracy and energy efficiency are interfered. In this work, we pose and focus on three key questions regarding the inherent redundancy in SNNs. We argue that the redundancy is induced by the spatio-temporal invariance of SNNs, which enhances the efficiency of parameter utilization but also invites lots of noise spikes. Further, we analyze the effect of spatio-temporal invariance on the spatio-temporal dynamics and spike firing of SNNs. Then, motivated by these analyses, we propose an Advance Spatial Attention (ASA) module to harness SNNs' redundancy, which can adaptively optimize their membrane potential distribution by a pair 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#29616;&#23454;&#25968;&#25454;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#65292;&#20998;&#21035;&#26159;&#31867;&#38388;&#19981;&#24179;&#34913;&#12289;&#31867;&#20869;&#19981;&#24179;&#34913;&#21644;&#31867;&#38388;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#25968;&#25454;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#38543;&#26426;&#37319;&#26679;&#19981;&#33021;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.08224</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#20811;&#26381;&#21322;&#30417;&#30563;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#30830;&#35748;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
How To Overcome Confirmation Bias in Semi-Supervised Image Classification By Active Learning. (arXiv:2308.08224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#29616;&#23454;&#25968;&#25454;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#65292;&#20998;&#21035;&#26159;&#31867;&#38388;&#19981;&#24179;&#34913;&#12289;&#31867;&#20869;&#19981;&#24179;&#34913;&#21644;&#31867;&#38388;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#25968;&#25454;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#38543;&#26426;&#37319;&#26679;&#19981;&#33021;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26159;&#21542;&#38656;&#35201;&#20027;&#21160;&#23398;&#20064;&#65311;&#24378;&#22823;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#26041;&#27861;&#30340;&#23835;&#36215;&#23545;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#35774;&#32622;&#20013;&#20027;&#21160;&#23398;&#20064;&#30340;&#21487;&#29992;&#24615;&#25552;&#20986;&#20102;&#30097;&#38382;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19982;&#38543;&#26426;&#36873;&#25321;&#36827;&#34892;&#26631;&#35760;&#30456;&#32467;&#21512;&#21487;&#20197;&#32988;&#36807;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#26469;&#33258;&#20110;&#22312;&#20844;&#35748;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#21487;&#33021;&#39640;&#20272;&#20102;&#22806;&#37096;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23545;&#20027;&#21160;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#38469;&#25968;&#25454;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#32570;&#20047;&#36275;&#22815;&#30340;&#30740;&#31350;&#65292;&#36825;&#22312;&#25105;&#20204;&#30340;&#29702;&#35299;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#31354;&#30333;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#25361;&#25112;&#65306;&#31867;&#38388;&#19981;&#24179;&#34913;&#12289;&#31867;&#20869;&#19981;&#24179;&#34913;&#21644;&#31867;&#38388;&#30456;&#20284;&#24615;&#12290;&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#30001;&#20110;&#30830;&#35748;&#20559;&#24046;&#32780;&#25439;&#23475;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#25361;&#25112;&#19978;&#36827;&#34892;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#38543;&#26426;&#37319;&#26679;&#19981;&#33021;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do we need active learning? The rise of strong deep semi-supervised methods raises doubt about the usability of active learning in limited labeled data settings. This is caused by results showing that combining semi-supervised learning (SSL) methods with a random selection for labeling can outperform existing active learning (AL) techniques. However, these results are obtained from experiments on well-established benchmark datasets that can overestimate the external validity. However, the literature lacks sufficient research on the performance of active semi-supervised learning methods in realistic data scenarios, leaving a notable gap in our understanding. Therefore we present three data challenges common in real-world applications: between-class imbalance, within-class imbalance, and between-class similarity. These challenges can hurt SSL performance due to confirmation bias. We conduct experiments with SSL and AL on simulated data challenges and find that random sampling does not mi
&lt;/p&gt;</description></item><item><title>HyperSNN&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#25511;&#21046;&#24212;&#29992;&#30340;&#39640;&#25928;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#33267;1.36%-9.96%&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#23427;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#12289;&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#20419;&#36827;&#20102;&#33021;&#37327;&#39640;&#25928;&#21644;&#31283;&#20581;&#30340;&#31995;&#32479;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.08222</link><description>&lt;p&gt;
HyperSNN&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#25511;&#21046;&#24212;&#29992;&#30340;&#39640;&#25928;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HyperSNN: A new efficient and robust deep learning model for resource constrained control applications. (arXiv:2308.08222v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08222
&lt;/p&gt;
&lt;p&gt;
HyperSNN&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#25511;&#21046;&#24212;&#29992;&#30340;&#39640;&#25928;&#31283;&#20581;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#21644;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#33267;1.36%-9.96%&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#23427;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#12289;&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#20419;&#36827;&#20102;&#33021;&#37327;&#39640;&#25928;&#21644;&#31283;&#20581;&#30340;&#31995;&#32479;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#36793;&#32536;&#35745;&#31639;&#22312;&#26234;&#33021;&#23478;&#20855;&#12289;&#26426;&#22120;&#20154;&#21644;&#26234;&#33021;&#23478;&#23621;&#31561;&#39046;&#22495;&#30340;&#26085;&#30410;&#37319;&#29992;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;HyperSNN&#65292;&#23427;&#20351;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#39640;&#32500;&#35745;&#31639;&#30456;&#32467;&#21512;&#12290; HyperSNN&#29992;8&#20301;&#25972;&#25968;&#21152;&#27861;&#26367;&#20195;&#20102;&#26114;&#36149;&#30340;32&#20301;&#28014;&#28857;&#20056;&#27861;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#33021;&#37327;&#28040;&#32791;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#31283;&#20581;&#24615;&#21644;&#21487;&#33021;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;AI Gym&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;Cartpole&#12289;Acrobot&#12289;MountainCar&#21644;Lunar Lander&#12290; HyperSNN&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#65292;&#20294;&#20165;&#28040;&#32791;1.36&#65285;&#33267;9.96&#65285;&#30340;&#33021;&#37327;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#20351;&#29992;HyperSNN&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;HyperSNN&#29305;&#21035;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#12289;&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#20419;&#36827;&#20102;&#33021;&#37327;&#39640;&#25928;&#21644;&#31283;&#20581;&#30340;&#31995;&#32479;&#35774;&#35745;&#12290;&#32780;&#19988;&#65292;&#23427;&#20026;.....
&lt;/p&gt;
&lt;p&gt;
In light of the increasing adoption of edge computing in areas such as intelligent furniture, robotics, and smart homes, this paper introduces HyperSNN, an innovative method for control tasks that uses spiking neural networks (SNNs) in combination with hyperdimensional computing. HyperSNN substitutes expensive 32-bit floating point multiplications with 8-bit integer additions, resulting in reduced energy consumption while enhancing robustness and potentially improving accuracy. Our model was tested on AI Gym benchmarks, including Cartpole, Acrobot, MountainCar, and Lunar Lander. HyperSNN achieves control accuracies that are on par with conventional machine learning methods but with only 1.36% to 9.96% of the energy expenditure. Furthermore, our experiments showed increased robustness when using HyperSNN. We believe that HyperSNN is especially suitable for interactive, mobile, and wearable devices, promoting energy-efficient and robust system design. Furthermore, it paves the way for th
&lt;/p&gt;</description></item><item><title>Epicure&#26159;&#19968;&#31181;&#23558;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#36716;&#21270;&#20026;&#31616;&#21333;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#39640;&#29109;&#24207;&#21015;&#20998;&#24067;&#20013;&#30340;&#21517;&#31216;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#20989;&#25968;&#21517;&#31216;&#21644;&#26816;&#27979;&#24322;&#24120;&#21517;&#31216;&#30340;&#20219;&#21153;&#20013;&#30456;&#36739;&#20110;&#26368;&#39640;&#27010;&#29575;&#27169;&#22411;&#39044;&#27979;&#26356;&#20934;&#30830;&#21305;&#37197;&#23454;&#38469;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.08203</link><description>&lt;p&gt;
Epicure: &#23558;&#24207;&#21015;&#27169;&#22411;&#39044;&#27979;&#36716;&#21270;&#25104;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Epicure: Distilling Sequence Model Predictions into Patterns. (arXiv:2308.08203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08203
&lt;/p&gt;
&lt;p&gt;
Epicure&#26159;&#19968;&#31181;&#23558;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#36716;&#21270;&#20026;&#31616;&#21333;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#39640;&#29109;&#24207;&#21015;&#20998;&#24067;&#20013;&#30340;&#21517;&#31216;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#20989;&#25968;&#21517;&#31216;&#21644;&#26816;&#27979;&#24322;&#24120;&#21517;&#31216;&#30340;&#20219;&#21153;&#20013;&#30456;&#36739;&#20110;&#26368;&#39640;&#27010;&#29575;&#27169;&#22411;&#39044;&#27979;&#26356;&#20934;&#30830;&#21305;&#37197;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#26159;&#20851;&#20110;&#20855;&#20307;&#36755;&#20986;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#19988;&#24456;&#38590;&#20934;&#30830;&#39044;&#27979;&#39640;&#29109;&#24207;&#21015;&#20998;&#24067;&#20013;&#30340;&#21517;&#31216;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23547;&#25214;&#36825;&#20123;&#39044;&#27979;&#20013;&#30340;&#25277;&#35937;&#12289;&#39640;&#31934;&#24230;&#27169;&#24335;&#65292;&#20197;&#20415;&#36827;&#34892;&#26377;&#29992;&#22320;&#25429;&#25417;&#32597;&#35265;&#24207;&#21015;&#30340;&#25277;&#35937;&#39044;&#27979;&#12290;&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Epicure&#65292;&#19968;&#31181;&#23558;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#65288;&#20363;&#22914;beam search&#30340;&#36755;&#20986;&#65289;&#36716;&#21270;&#20026;&#31616;&#21333;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;Epicure&#23558;&#27169;&#22411;&#30340;&#39044;&#27979;&#26144;&#23556;&#21040;&#19968;&#20010;&#34920;&#31034;&#36234;&#26469;&#36234;&#19968;&#33324;&#27169;&#24335;&#30340;&#26684;&#28857;&#19978;&#65292;&#36825;&#20123;&#27169;&#24335;&#21253;&#21547;&#20102;&#20855;&#20307;&#27169;&#22411;&#39044;&#27979;&#30340;&#20869;&#23481;&#12290;&#22312;&#39044;&#27979;&#32473;&#23450;&#20989;&#25968;&#30340;&#28304;&#20195;&#30721;&#26102;&#36873;&#25321;&#25551;&#36848;&#24615;&#21517;&#31216;&#21644;&#26816;&#27979;&#32473;&#23450;&#20989;&#25968;&#20013;&#24322;&#24120;&#21517;&#31216;&#30340;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Epicure&#30456;&#36739;&#20110;&#20165;&#20351;&#29992;&#26368;&#39640;&#27010;&#29575;&#27169;&#22411;&#39044;&#27979;&#26356;&#20934;&#30830;&#22320;&#21629;&#21517;&#27169;&#24335;&#19982;&#23454;&#38469;&#24773;&#20917;&#30456;&#21305;&#37197;&#12290;&#22312;&#35823;&#25253;&#29575;&#20026;10%&#30340;&#24773;&#20917;&#19979;&#65292;Epicure&#39044;&#27979;&#30340;&#27169;&#24335;&#19982;&#23454;&#38469;&#24773;&#20917;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning models predict a probability distribution over concrete outputs and struggle to accurately predict names over high entropy sequence distributions. Here, we explore finding abstract, high-precision patterns intrinsic to these predictions in order to make abstract predictions that usefully capture rare sequences. In this short paper, we present Epicure, a method that distils the predictions of a sequence model, such as the output of beam search, into simple patterns. Epicure maps a model's predictions into a lattice that represents increasingly more general patterns that subsume the concrete model predictions.  On the tasks of predicting a descriptive name of a function given the source code of its body and detecting anomalous names given a function, we show that Epicure yields accurate naming patterns that match the ground truth more often compared to just the highest probability model prediction. For a false alarm rate of 10%, Epicure predicts patterns that match 
&lt;/p&gt;</description></item><item><title>DeSCo&#26159;&#19968;&#20010;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23376;&#22270;&#35745;&#25968;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#31070;&#32463;&#35745;&#25968;&#26041;&#27861;&#22312;&#35745;&#25968;&#20934;&#30830;&#24615;&#12289;&#22270;&#24418;&#21306;&#20998;&#21644;&#20986;&#29616;&#20301;&#32622;&#39044;&#27979;&#26041;&#38754;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08198</link><description>&lt;p&gt;
DeSCo:&#38754;&#21521;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23376;&#22270;&#35745;&#25968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeSCo: Towards Generalizable and Scalable Deep Subgraph Counting. (arXiv:2308.08198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08198
&lt;/p&gt;
&lt;p&gt;
DeSCo&#26159;&#19968;&#20010;&#36890;&#29992;&#19988;&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#23376;&#22270;&#35745;&#25968;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#29616;&#26377;&#31070;&#32463;&#35745;&#25968;&#26041;&#27861;&#22312;&#35745;&#25968;&#20934;&#30830;&#24615;&#12289;&#22270;&#24418;&#21306;&#20998;&#21644;&#20986;&#29616;&#20301;&#32622;&#39044;&#27979;&#26041;&#38754;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#35745;&#25968;&#26159;&#22312;&#22823;&#22411;&#30446;&#26631;&#22270;&#20013;&#35745;&#31639;&#32473;&#23450;&#26597;&#35810;&#22270;&#30340;&#20986;&#29616;&#27425;&#25968;&#30340;&#38382;&#39064;&#12290;&#22823;&#35268;&#27169;&#30340;&#23376;&#22270;&#35745;&#25968;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#24456;&#26377;&#29992;&#65292;&#27604;&#22914;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#20013;&#30340;&#22270;&#26696;&#35745;&#25968;&#21644;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#21453;&#27927;&#38065;&#26816;&#27979;&#20013;&#30340;&#24490;&#29615;&#35745;&#25968;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#35299;&#20915;&#21487;&#25193;&#23637;&#23376;&#22270;&#35745;&#25968;&#30340;&#25351;&#25968;&#32423;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#35745;&#25968;&#26041;&#27861;&#22312;&#19977;&#20010;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#39318;&#20808;&#65292;&#30456;&#21516;&#26597;&#35810;&#30340;&#35745;&#25968;&#22312;&#19981;&#21516;&#30446;&#26631;&#22270;&#19978;&#21487;&#20197;&#20174;&#38646;&#21040;&#25968;&#30334;&#19975;&#65292;&#27604;&#22823;&#22810;&#25968;&#22270;&#22238;&#24402;&#20219;&#21153;&#37117;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#20854;&#27425;&#65292;&#30446;&#21069;&#30340;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26080;&#27861;&#39640;&#25928;&#22320;&#21306;&#20998;&#35745;&#25968;&#39044;&#27979;&#20013;&#30340;&#22270;&#24418;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#26041;&#27861;&#26080;&#27861;&#39044;&#27979;&#26597;&#35810;&#22312;&#30446;&#26631;&#22270;&#20013;&#30340;&#20986;&#29616;&#20301;&#32622;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DeSCo&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#28145;&#24230;&#23376;&#22270;&#35745;&#25968;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#20934;&#30830;&#22320;&#39044;&#27979;&#23376;&#22270;&#30340;&#35745;&#25968;&#21644;&#20986;&#29616;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph counting is the problem of counting the occurrences of a given query graph in a large target graph. Large-scale subgraph counting is useful in various domains, such as motif counting for social network analysis and loop counting for money laundering detection on transaction networks. Recently, to address the exponential runtime complexity of scalable subgraph counting, neural methods are proposed. However, existing neural counting approaches fall short in three aspects. Firstly, the counts of the same query can vary from zero to millions on different target graphs, posing a much larger challenge than most graph regression tasks. Secondly, current scalable graph neural networks have limited expressive power and fail to efficiently distinguish graphs in count prediction. Furthermore, existing neural approaches cannot predict the occurrence position of queries in the target graph.  Here we design DeSCo, a scalable neural deep subgraph counting pipeline, which aims to accurately p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#31639;&#27861;&#34917;&#25937;&#20013;&#30340;&#20869;&#29983;&#21160;&#21147;&#23398;&#21644;&#23545;&#31574;&#24433;&#21709;&#20854;&#20182;&#20010;&#20307;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#23545;&#31574;&#30340;&#38544;&#34255;&#25104;&#26412;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08187</link><description>&lt;p&gt;
&#31639;&#27861;&#34917;&#25937;&#20013;&#30340;&#20869;&#29983;&#23439;&#35266;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Endogenous Macrodynamics in Algorithmic Recourse. (arXiv:2308.08187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08187
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#31639;&#27861;&#34917;&#25937;&#20013;&#30340;&#20869;&#29983;&#21160;&#21147;&#23398;&#21644;&#23545;&#31574;&#24433;&#21709;&#20854;&#20182;&#20010;&#20307;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#25581;&#31034;&#20102;&#23545;&#31574;&#30340;&#38544;&#34255;&#25104;&#26412;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;&#23545;&#31574;&#35299;&#37322;&#21644;&#31639;&#27861;&#34917;&#25937;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#30340;&#21333;&#20010;&#20010;&#20307;&#19978;&#65306;&#22312;&#32473;&#23450;&#19968;&#20123;&#20272;&#35745;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#30340;&#21333;&#20010;&#23454;&#20363;&#30340;&#26377;&#25928;&#23545;&#31574;&#35299;&#37322;&#12290;&#36825;&#20123;&#23545;&#31574;&#35299;&#37322;&#30340;&#33021;&#21147;&#22788;&#29702;&#25968;&#25454;&#21644;&#27169;&#22411;&#28418;&#31227;&#31561;&#21160;&#24577;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#23569;&#30740;&#31350;&#30340;&#25361;&#25112;&#12290;&#19982;&#27492;&#30456;&#20851;&#30340;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#19968;&#20010;&#20010;&#20307;&#23545;&#31574;&#23454;&#26045;&#30340;&#23454;&#38469;&#24433;&#21709;&#20854;&#20182;&#20010;&#20307;&#30340;&#38382;&#39064;&#65292;&#20851;&#20110;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#30456;&#24403;&#23569;&#12290;&#36890;&#36807;&#36825;&#39033;&#24037;&#20316;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#29616;&#26377;&#30340;&#35768;&#22810;&#26041;&#27861;&#21487;&#20197;&#34987;&#32479;&#19968;&#25551;&#36848;&#20026;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#26694;&#26550;&#27809;&#26377;&#32771;&#34385;&#21040;&#19968;&#20010;&#38544;&#34255;&#30340;&#23545;&#31574;&#25104;&#26412;&#65292;&#22312;&#30740;&#31350;&#32676;&#20307;&#23618;&#38754;&#19978;&#30340;&#20869;&#29983;&#21160;&#21147;&#23398;&#26102;&#25165;&#20250;&#26174;&#29616;&#20986;&#26469;&#12290;&#36890;&#36807;&#28041;&#21450;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#23545;&#31574;&#35299;&#37322;&#26041;&#27861;&#30340;&#20223;&#30495;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Existing work on Counterfactual Explanations (CE) and Algorithmic Recourse (AR) has largely focused on single individuals in a static environment: given some estimated model, the goal is to find valid counterfactuals for an individual instance that fulfill various desiderata. The ability of such counterfactuals to handle dynamics like data and model drift remains a largely unexplored research challenge. There has also been surprisingly little work on the related question of how the actual implementation of recourse by one individual may affect other individuals. Through this work, we aim to close that gap. We first show that many of the existing methodologies can be collectively described by a generalized framework. We then argue that the existing framework does not account for a hidden external cost of recourse, that only reveals itself when studying the endogenous dynamics of recourse at the group level. Through simulation experiments involving various state-of the-art counterfactual
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#21516;&#35774;&#35745;&#26550;&#26500;&#12289;&#32534;&#35793;&#22120;&#21644;&#20998;&#21306;&#26041;&#27861;&#26469;&#35299;&#20915;GNN&#27169;&#22411;&#30340;&#39640;&#24102;&#23485;&#38656;&#27714;&#21644;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.08174</link><description>&lt;p&gt;
&#36890;&#36807;&#26550;&#26500;&#12289;&#32534;&#35793;&#22120;&#21644;&#20998;&#21306;&#26041;&#27861;&#30340;&#21327;&#21516;&#35774;&#35745;&#21152;&#36895;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerating Generic Graph Neural Networks via Architecture, Compiler, Partition Method Co-Design. (arXiv:2308.08174v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08174
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21327;&#21516;&#35774;&#35745;&#26550;&#26500;&#12289;&#32534;&#35793;&#22120;&#21644;&#20998;&#21306;&#26041;&#27861;&#26469;&#35299;&#20915;GNN&#27169;&#22411;&#30340;&#39640;&#24102;&#23485;&#38656;&#27714;&#21644;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#65292;&#24341;&#21457;&#20102;&#30456;&#24403;&#22823;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#20934;&#30830;&#24615;&#25913;&#36827;&#36716;&#21270;&#20026;&#23454;&#38469;&#24212;&#29992;&#65292;&#24320;&#21457;&#39640;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#23545;&#20110;GNN&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;GNN&#21152;&#36895;&#22120;&#38754;&#20020;&#20004;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;GNN&#27169;&#22411;&#30340;&#39640;&#24102;&#23485;&#38656;&#27714;&#21644;GNN&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#26356;&#26114;&#36149;&#30340;&#20869;&#23384;&#25509;&#21475;&#26469;&#23454;&#29616;&#26356;&#39640;&#30340;&#24102;&#23485;&#26469;&#35299;&#20915;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#32780;&#23545;&#20110;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#35201;&#20040;&#25903;&#25345;&#29305;&#23450;&#30340;GNN&#27169;&#22411;&#65292;&#35201;&#20040;&#20855;&#26377;&#30828;&#20214;&#21033;&#29992;&#29575;&#20302;&#30340;&#36890;&#29992;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#20998;&#21306;&#32423;&#36816;&#31639;&#31526;&#34701;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#22312;&#20869;&#37096;&#20943;&#23569;GNN&#30340;&#39640;&#24102;&#23485;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#21306;&#32423;&#22810;&#32447;&#31243;&#26469;&#35843;&#24230;&#24182;&#21457;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown significant accuracy improvements in a variety of graph learning domains, sparking considerable research interest. To translate these accuracy improvements into practical applications, it is essential to develop high-performance and efficient hardware acceleration for GNN models. However, designing GNN accelerators faces two fundamental challenges: the high bandwidth requirement of GNN models and the diversity of GNN models. Previous works have addressed the first challenge by using more expensive memory interfaces to achieve higher bandwidth. For the second challenge, existing works either support specific GNN models or have generic designs with poor hardware utilization.  In this work, we tackle both challenges simultaneously. First, we identify a new type of partition-level operator fusion, which we utilize to internally reduce the high bandwidth requirement of GNNs. Next, we introduce partition-level multi-threading to schedule the concurrent
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#20256;&#32479;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#35777;&#26126;&#20102;&#26356;&#24378;&#22823;&#30340;GNNs&#26080;&#27861;&#27867;&#21270;&#21040;&#23567;&#25200;&#21160;&#30340;&#22270;&#32467;&#26500;&#21644;&#20998;&#24067;&#19981;&#19968;&#26679;&#30340;&#22270;&#12290;</title><link>http://arxiv.org/abs/2308.08173</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#65292;&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness. (arXiv:2308.08173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08173
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#20256;&#32479;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#35777;&#26126;&#20102;&#26356;&#24378;&#22823;&#30340;GNNs&#26080;&#27861;&#27867;&#21270;&#21040;&#23567;&#25200;&#21160;&#30340;&#22270;&#32467;&#26500;&#21644;&#20998;&#24067;&#19981;&#19968;&#26679;&#30340;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#65292;&#35777;&#26126;&#23427;&#20204;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#27604;&#20256;&#32479;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26356;&#24378;&#22823;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#40065;&#26834;&#24615;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#26469;&#25581;&#31034;&#23427;&#20204;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#21644;&#32463;&#39564;&#19978;&#23454;&#38469;&#36798;&#21040;&#30340;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20851;&#27880;GNNs&#35745;&#25968;&#29305;&#23450;&#30340;&#23376;&#22270;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#24050;&#24314;&#31435;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#65292;&#23558;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#36825;&#20010;&#20219;&#21153;&#19978;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#26469;&#36827;&#34892;&#23376;&#22270;&#35745;&#25968;&#65292;&#24182;&#23637;&#31034;&#26356;&#24378;&#22823;&#30340;GNNs&#21363;&#20351;&#22312;&#23545;&#22270;&#32467;&#26500;&#30340;&#23567;&#25200;&#21160;&#19979;&#20063;&#26080;&#27861;&#27867;&#21270;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#36825;&#26679;&#30340;&#26550;&#26500;&#22312;&#22788;&#29702;&#20998;&#24067;&#19981;&#19968;&#26679;&#30340;&#22270;&#26102;&#20063;&#26080;&#27861;&#35745;&#25968;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We perform the first adversarial robustness study into Graph Neural Networks (GNNs) that are provably more powerful than traditional Message Passing Neural Networks (MPNNs). In particular, we use adversarial robustness as a tool to uncover a significant gap between their theoretically possible and empirically achieved expressive power. To do so, we focus on the ability of GNNs to count specific subgraph patterns, which is an established measure of expressivity, and extend the concept of adversarial robustness to this task. Based on this, we develop efficient adversarial attacks for subgraph counting and show that more powerful GNNs fail to generalize even to small perturbations to the graph's structure. Expanding on this, we show that such architectures also fail to count substructures on out-of-distribution graphs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20934;&#22791;&#24182;&#21457;&#24067;&#20102;&#19968;&#27454;&#21517;&#20026;AATCT-IDS&#30340;&#22522;&#20934;&#33145;&#37096;&#33026;&#32938;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#25918;&#23556;&#23398;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#20013;&#27880;&#37322;&#30340;&#33026;&#32938;&#32452;&#32455;&#21306;&#22495;&#36827;&#34892;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#21508;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#25968;&#25454;&#38598;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08172</link><description>&lt;p&gt;
AATCT-IDS: &#19968;&#27454;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#25918;&#23556;&#23398;&#35780;&#20272;&#30340;&#33145;&#37096;&#33026;&#32938;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AATCT-IDS: A Benchmark Abdominal Adipose Tissue CT Image Dataset for Image Denoising, Semantic Segmentation, and Radiomics Evaluation. (arXiv:2308.08172v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08172
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20934;&#22791;&#24182;&#21457;&#24067;&#20102;&#19968;&#27454;&#21517;&#20026;AATCT-IDS&#30340;&#22522;&#20934;&#33145;&#37096;&#33026;&#32938;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#25918;&#23556;&#23398;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#25968;&#25454;&#38598;&#20013;&#27880;&#37322;&#30340;&#33026;&#32938;&#32452;&#32455;&#21306;&#22495;&#36827;&#34892;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#21508;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#25968;&#25454;&#38598;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20934;&#22791;&#24182;&#21457;&#24067;&#20102;&#19968;&#27454;&#21517;&#20026;AATCT-IDS&#30340;&#22522;&#20934;&#33145;&#37096;&#33026;&#32938;CT&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;300&#20010;&#21463;&#35797;&#32773;&#12290;AATCT-IDS&#20844;&#24320;&#20102;13,732&#20010;&#21407;&#22987;CT&#20999;&#29255;&#65292;&#24182;&#19988;&#30740;&#31350;&#20154;&#21592;&#23545;&#20854;&#20013;3,213&#20010;&#20855;&#26377;&#30456;&#21516;&#20999;&#29255;&#36317;&#31163;&#30340;&#20999;&#29255;&#20998;&#21035;&#36827;&#34892;&#20102;&#30382;&#19979;&#21644;&#33145;&#33108;&#20869;&#33026;&#32938;&#32452;&#32455;&#21306;&#22495;&#30340;&#27880;&#37322;&#65292;&#20197;&#39564;&#35777;&#21435;&#22122;&#26041;&#27861;&#65292;&#35757;&#32451;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#25918;&#23556;&#23398;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#21487;&#35270;&#21270;&#32467;&#26524;&#21644;&#35780;&#20272;&#25968;&#25454;&#65292;&#27604;&#36739;&#21644;&#20998;&#26512;&#20102;&#21508;&#31181;&#26041;&#27861;&#22312;AATCT-IDS&#19978;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#35813;&#25968;&#25454;&#38598;&#22312;&#19978;&#36848;&#19977;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods: In this study, a benchmark \emph{Abdominal Adipose Tissue CT Image Dataset} (AATTCT-IDS) containing 300 subjects is prepared and published. AATTCT-IDS publics 13,732 raw CT slices, and the researchers individually annotate the subcutaneous and visceral adipose tissue regions of 3,213 of those slices that have the same slice distance to validate denoising methods, train semantic segmentation models, and study radiomics. For different tasks, this paper compares and analyzes the performance of various methods on AATTCT-IDS by combining the visualization results and evaluation data. Thus, verify the research potential of this data set in the above three types of tasks.  Results: In the comparative study of image denoising, algorithms using a smoothing strategy suppress mixed noise at the expense of image details and obtain better evaluation data. Methods such as BM3D preserve the original image structure better, although the evaluation data are slightly lower. The results show sig
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#23376;&#36924;&#36817;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#32463;&#20856;&#30340;k-Means&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#26041;&#26696;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#20855;&#26377;&#22810;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#39640;&#27010;&#29575;&#19979;&#36755;&#20986;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#22810;&#23545;&#25968;&#36816;&#34892;&#26102;&#38388;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;&#36924;&#36817;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.08167</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;k-Means&#30340;&#37327;&#23376;&#36924;&#36817;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Quantum Approximation Scheme for k-Means. (arXiv:2308.08167v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#23376;&#36924;&#36817;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#32463;&#20856;&#30340;k-Means&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#26041;&#26696;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#20855;&#26377;&#22810;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#39640;&#27010;&#29575;&#19979;&#36755;&#20986;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#22810;&#23545;&#25968;&#36816;&#34892;&#26102;&#38388;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;&#36924;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;QRAM&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#37327;&#23376;&#36924;&#36817;&#26041;&#26696;&#65288;&#21363;&#23545;&#20110;&#20219;&#24847;&#949; &gt; 0, &#37117;&#26159; (1 + &#949;)-&#36924;&#36817;&#65289;&#65292;&#29992;&#20110;&#32463;&#20856;&#30340;k-Means&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20165;&#19982;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#20855;&#26377;&#22810;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#22312;QRAM&#25968;&#25454;&#32467;&#26500;&#20013;&#23384;&#20648;&#30340;&#20855;&#26377;N&#20010;&#28857;&#30340;&#25968;&#25454;&#38598;V&#65292;&#36825;&#20010;&#37327;&#23376;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20026;O&#771;(2^(O&#771;(k/&#949;))&#951;^2d)&#65292;&#24182;&#19988;&#20197;&#39640;&#27010;&#29575;&#36755;&#20986;&#19968;&#20010;&#21253;&#21547;k&#20010;&#20013;&#24515;&#30340;&#38598;&#21512;C&#65292;&#28385;&#36275;cost(V, C) &#8804; (1+&#949;) &#183; cost(V, C_OPT)&#12290;&#36825;&#37324;C_OPT&#34920;&#31034;&#26368;&#20248;&#30340;k&#20010;&#20013;&#24515;&#65292;cost(.)&#34920;&#31034;&#26631;&#20934;&#30340;k-Means&#20195;&#20215;&#20989;&#25968;&#65288;&#21363;&#28857;&#21040;&#26368;&#36817;&#20013;&#24515;&#30340;&#24179;&#26041;&#36317;&#31163;&#20043;&#21644;&#65289;&#65292;&#32780;&#951;&#26159;&#32437;&#27178;&#27604;&#65288;&#21363;&#26368;&#36828;&#36317;&#31163;&#19982;&#26368;&#36817;&#36317;&#31163;&#30340;&#27604;&#20540;&#65289;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#22810;&#23545;&#25968;&#36816;&#34892;&#26102;&#38388;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;(1+&#949;)&#36924;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give a quantum approximation scheme (i.e., $(1 + \varepsilon)$-approximation for every $\varepsilon &gt; 0$) for the classical $k$-means clustering problem in the QRAM model with a running time that has only polylogarithmic dependence on the number of data points. More specifically, given a dataset $V$ with $N$ points in $\mathbb{R}^d$ stored in QRAM data structure, our quantum algorithm runs in time $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ and with high probability outputs a set $C$ of $k$ centers such that $cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$. Here $C_{OPT}$ denotes the optimal $k$-centers, $cost(.)$ denotes the standard $k$-means cost function (i.e., the sum of the squared distance of points to the closest center), and $\eta$ is the aspect ratio (i.e., the ratio of maximum distance to minimum distance). This is the first quantum algorithm with a polylogarithmic running time that gives a provable approximation guarantee of $(1+\varep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#26680;&#21270;&#30340;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#27492;&#31639;&#27861;&#29983;&#25104;&#30340;&#32593;&#32476;&#30340;&#29305;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26680;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#31639;&#27861;&#21487;&#20197;&#23558;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#26080;&#21521;&#22270;&#65292;&#24182;&#25552;&#21462;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#20316;&#20026;&#22270;&#24418;&#12290;&#20116;&#31181;&#26680;&#20989;&#25968;&#34987;&#29992;&#20110;&#27492;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.08163</link><description>&lt;p&gt;
&#30001;&#26680;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#29983;&#25104;&#30340;&#32593;&#32476;&#30340;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Characteristics of networks generated by kernel growing neural gas. (arXiv:2308.08163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#26680;&#21270;&#30340;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#27492;&#31639;&#27861;&#29983;&#25104;&#30340;&#32593;&#32476;&#30340;&#29305;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26680;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#31639;&#27861;&#21487;&#20197;&#23558;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#26080;&#21521;&#22270;&#65292;&#24182;&#25552;&#21462;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#20316;&#20026;&#22270;&#24418;&#12290;&#20116;&#31181;&#26680;&#20989;&#25968;&#34987;&#29992;&#20110;&#27492;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#26680;&#21270;&#30340;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#65288;GNG&#65289;&#31639;&#27861;&#30340;&#26680;GNG&#65292;&#24182;&#30740;&#31350;&#30001;&#26680;GNG&#29983;&#25104;&#30340;&#32593;&#32476;&#30340;&#29305;&#24449;&#12290;GNG&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#23558;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#26080;&#21521;&#22270;&#65292;&#20174;&#32780;&#25552;&#21462;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#20316;&#20026;&#22270;&#24418;&#12290;GNG&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21521;&#37327;&#37327;&#21270;&#12289;&#32858;&#31867;&#21644;&#19977;&#32500;&#22270;&#24418;&#20013;&#12290;&#26680;&#26041;&#27861;&#24120;&#29992;&#20110;&#23558;&#25968;&#25454;&#38598;&#26144;&#23556;&#21040;&#29305;&#24449;&#31354;&#38388;&#65292;&#20854;&#20013;&#25903;&#25345;&#21521;&#37327;&#26426;&#26159;&#26368;&#37325;&#35201;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26680;GNG&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#30001;&#26680;GNG&#29983;&#25104;&#30340;&#32593;&#32476;&#30340;&#29305;&#24615;&#12290;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#20116;&#31181;&#26680;&#20989;&#25968;&#65292;&#21253;&#25324;&#39640;&#26031;&#26680;&#12289;&#25289;&#26222;&#25289;&#26031;&#26680;&#12289;&#26607;&#35199;&#26680;&#12289;&#21453;&#22810;&#39033;&#24335;&#26680;&#21644;&#23545;&#25968;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to develop kernel GNG, a kernelized version of the growing neural gas (GNG) algorithm, and to investigate the features of the networks generated by the kernel GNG. The GNG is an unsupervised artificial neural network that can transform a dataset into an undirected graph, thereby extracting the features of the dataset as a graph. The GNG is widely used in vector quantization, clustering, and 3D graphics. Kernel methods are often used to map a dataset to feature space, with support vector machines being the most prominent application. This paper introduces the kernel GNG approach and explores the characteristics of the networks generated by kernel GNG. Five kernels, including Gaussian, Laplacian, Cauchy, inverse multiquadric, and log kernels, are used in this study.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21407;&#22411;&#37096;&#20214;&#35299;&#37322;&#30340;&#31354;&#38388;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#37322;&#24615;&#22522;&#20934;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#34917;&#20607;&#19981;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#34920;&#26126;&#20102;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#34917;&#20607;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08162</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#21407;&#22411;&#37096;&#20214;&#35299;&#37322;&#30340;&#31354;&#38388;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#37322;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations. (arXiv:2308.08162v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21407;&#22411;&#37096;&#20214;&#35299;&#37322;&#30340;&#31354;&#38388;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#37322;&#24615;&#22522;&#20934;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#34917;&#20607;&#19981;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#34920;&#26126;&#20102;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#34917;&#20607;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24544;&#23454;&#30340;&#33258;&#35299;&#37322;&#24615;&#65292;&#21407;&#22411;&#37096;&#20214;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#22270;&#35745;&#31639;&#22312;&#20498;&#25968;&#31532;&#20108;&#23618;&#32593;&#32476;&#20013;&#12290;&#22240;&#27492;&#65292;&#21407;&#22411;&#28608;&#27963;&#21306;&#22495;&#30340;&#24863;&#21463;&#37326;&#36890;&#24120;&#20381;&#36182;&#20110;&#22270;&#20687;&#21306;&#22495;&#22806;&#30340;&#37096;&#20998;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#31216;&#20026;&#31354;&#38388;&#35299;&#37322;&#19981;&#19968;&#33268;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35299;&#37322;&#24615;&#22522;&#20934;&#65292;&#25552;&#20379;&#19968;&#22871;&#19987;&#29992;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#19968;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34917;&#20607;&#19981;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#22522;&#20934;&#30340;&#34920;&#36798;&#33021;&#21147;&#20197;&#21450;&#25552;&#20986;&#30340;&#34917;&#20607;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototypical parts-based networks are becoming increasingly popular due to their faithful self-explanations. However, their similarity maps are calculated in the penultimate network layer. Therefore, the receptive field of the prototype activation region often depends on parts of the image outside this region, which can lead to misleading interpretations. We name this undesired behavior a spatial explanation misalignment and introduce an interpretability benchmark with a set of dedicated metrics for quantifying this phenomenon. In addition, we propose a method for misalignment compensation and apply it to existing state-of-the-art models. We show the expressiveness of our benchmark and the effectiveness of the proposed compensation methodology through extensive empirical studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#36807;&#31243;&#20013;&#65292;&#35009;&#21098;&#27169;&#22411;&#20173;&#33021;&#20445;&#25345;&#22522;&#30784;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.08160</link><description>&lt;p&gt;
&#21387;&#32553;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Adversarial Robustness of Compressed Deep Learning Models. (arXiv:2308.08160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#36807;&#31243;&#20013;&#65292;&#35009;&#21098;&#27169;&#22411;&#20173;&#33021;&#20445;&#25345;&#22522;&#30784;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#23545;&#27169;&#22411;&#21387;&#32553;&#30340;&#38656;&#27714;&#26085;&#30410;&#36843;&#20999;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#20351;&#29992;&#26102;&#12290;&#21516;&#26102;&#65292;DNNs&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#25935;&#24863;&#24615;&#20063;&#26159;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#23613;&#31649;&#23545;&#20110;&#27169;&#22411;&#21387;&#32553;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#20805;&#20998;&#65292;&#20294;&#23427;&#20204;&#30340;&#32852;&#21512;&#30740;&#31350;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#26088;&#22312;&#20102;&#35299;&#38024;&#23545;&#22522;&#30784;&#27169;&#22411;&#21046;&#20316;&#30340;&#23545;&#25239;&#36755;&#20837;&#23545;&#20854;&#35009;&#21098;&#29256;&#26412;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20851;&#31995;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#27969;&#34892;&#30340;DNN&#27169;&#22411;&#12290;&#25105;&#20204;&#29420;&#29305;&#22320;&#20851;&#27880;&#20102;&#20043;&#21069;&#26410;&#32463;&#36807;&#23545;&#25239;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20102;&#38024;&#23545;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#20248;&#21270;&#30340;&#35009;&#21098;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#35009;&#21098;&#24102;&#26469;&#20102;&#22686;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#21387;&#32553;&#25928;&#26524;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#20294;&#23545;&#25239;&#40065;&#26834;&#24615;&#19982;&#22522;&#30784;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing size of Deep Neural Networks (DNNs) poses a pressing need for model compression, particularly when employed on resource constrained devices. Concurrently, the susceptibility of DNNs to adversarial attacks presents another significant hurdle. Despite substantial research on both model compression and adversarial robustness, their joint examination remains underexplored. Our study bridges this gap, seeking to understand the effect of adversarial inputs crafted for base models on their pruned versions. To examine this relationship, we have developed a comprehensive benchmark across diverse adversarial attacks and popular DNN models. We uniquely focus on models not previously exposed to adversarial training and apply pruning schemes optimized for accuracy and performance. Our findings reveal that while the benefits of pruning enhanced generalizability, compression, and faster inference times are preserved, adversarial robustness remains comparable to the base model. This sug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38750;&#38543;&#26426;&#32570;&#22833;&#25968;&#25454;&#30340;&#28145;&#24230;&#29983;&#25104;&#22635;&#20805;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26032;&#30340;&#35282;&#24230;&#21435;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#30452;&#25509;&#23558;&#32479;&#35745;&#26041;&#27861;&#32435;&#20837;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#27425;&#20248;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2308.08158</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#22635;&#20805;&#27169;&#22411;&#29992;&#20110;&#38750;&#38543;&#26426;&#32570;&#22833;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Imputation Model for Missing Not At Random Data. (arXiv:2308.08158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38750;&#38543;&#26426;&#32570;&#22833;&#25968;&#25454;&#30340;&#28145;&#24230;&#29983;&#25104;&#22635;&#20805;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26032;&#30340;&#35282;&#24230;&#21435;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#30452;&#25509;&#23558;&#32479;&#35745;&#26041;&#27861;&#32435;&#20837;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#27425;&#20248;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20998;&#26512;&#36890;&#24120;&#38754;&#20020;&#38750;&#38543;&#26426;&#32570;&#22833;&#65288;MNAR&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#32570;&#22833;&#20540;&#30340;&#21407;&#22240;&#27809;&#26377;&#23436;&#20840;&#35266;&#23519;&#21040;&#12290;&#19982;&#31616;&#21333;&#30340;&#23436;&#20840;&#38543;&#26426;&#32570;&#22833;&#65288;MCAR&#65289;&#38382;&#39064;&#30456;&#27604;&#65292;&#36825;&#26356;&#31526;&#21512;&#23454;&#38469;&#24773;&#20917;&#65292;&#20063;&#26356;&#22797;&#26434;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#32479;&#35745;&#26041;&#27861;&#36890;&#36807;&#19981;&#21516;&#30340;&#23436;&#25972;&#25968;&#25454;&#21644;&#32570;&#22833;&#33945;&#29256;&#30340;&#32852;&#21512;&#20998;&#24067;&#20998;&#35299;&#26469;&#24314;&#27169;MNAR&#26426;&#21046;&#12290;&#20294;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#21457;&#29616;&#65292;&#30452;&#25509;&#23558;&#36825;&#20123;&#32479;&#35745;&#26041;&#27861;&#32435;&#20837;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26159;&#27425;&#20248;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20250;&#24573;&#35270;MNAR&#22635;&#20805;&#36807;&#31243;&#20013;&#37325;&#26500;&#33945;&#29256;&#30340;&#32622;&#20449;&#24230;&#65292;&#23548;&#33268;&#20449;&#24687;&#25552;&#21462;&#19981;&#36275;&#21644;&#22635;&#20805;&#36136;&#37327;&#19981;&#22815;&#21487;&#38752;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;MNAR&#38382;&#39064;&#65292;&#21363;&#23436;&#25972;&#25968;&#25454;&#21644;&#32570;&#22833;&#33945;&#29256;&#26159;&#20004;&#20010;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#27169;&#24577;&#12290;&#27839;&#30528;&#36825;&#26465;&#32447;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#29305;&#23450;&#30340;&#32852;&#21512;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Data analysis usually suffers from the Missing Not At Random (MNAR) problem, where the cause of the value missing is not fully observed. Compared to the naive Missing Completely At Random (MCAR) problem, it is more in line with the realistic scenario whereas more complex and challenging. Existing statistical methods model the MNAR mechanism by different decomposition of the joint distribution of the complete data and the missing mask. But we empirically find that directly incorporating these statistical methods into deep generative models is sub-optimal. Specifically, it would neglect the confidence of the reconstructed mask during the MNAR imputation process, which leads to insufficient information extraction and less-guaranteed imputation quality. In this paper, we revisit the MNAR problem from a novel perspective that the complete data and missing mask are two modalities of incomplete data on an equal footing. Along with this line, we put forward a generative-model-specific joint pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HurricaneSARC&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;15,000&#26465;&#27880;&#37322;&#20026;&#35773;&#21050;&#24847;&#22270;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35773;&#21050;&#26816;&#27979;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20013;&#38388;&#20219;&#21153;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;0.70&#30340;F1&#20540;&#12290;</title><link>http://arxiv.org/abs/2308.08156</link><description>&lt;p&gt;
&#28798;&#38590;&#32972;&#26223;&#19979;&#30340;&#35773;&#21050;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sarcasm Detection in a Disaster Context. (arXiv:2308.08156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HurricaneSARC&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;15,000&#26465;&#27880;&#37322;&#20026;&#35773;&#21050;&#24847;&#22270;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35773;&#21050;&#26816;&#27979;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20013;&#38388;&#20219;&#21153;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;0.70&#30340;F1&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#28798;&#23475;&#26399;&#38388;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;Twitter&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#23547;&#27714;&#24110;&#21161;&#12289;&#25552;&#20379;&#20851;&#20110;&#28798;&#24773;&#30340;&#20449;&#24687;&#65292;&#25110;&#34920;&#36798;&#23545;&#28798;&#24773;&#28436;&#21464;&#25110;&#20844;&#20849;&#25919;&#31574;&#21644;&#25351;&#23548;&#26041;&#38024;&#30340;&#34065;&#35270;&#12290;&#22312;&#28798;&#38590;&#32972;&#26223;&#20013;&#29702;&#35299;&#36825;&#31181;&#35328;&#35770;&#24418;&#24335;&#23545;&#25913;&#36827;&#23545;&#28798;&#23475;&#30456;&#20851;&#25512;&#25991;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HurricaneSARC&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;15,000&#26465;&#26631;&#35760;&#20102;&#35773;&#21050;&#24847;&#22270;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#35773;&#21050;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#36798;&#21040;0.70&#30340;F1&#20540;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#20013;&#38388;&#20219;&#21153;&#30340;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;HurricaneSARC&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;https://github.com/tsosea2/HurricaneSarc&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15,000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning. We release our data and code at https://github.com/tsosea2/HurricaneSarc.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#30446;&#30340;&#26159;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#22312;&#29983;&#25104;&#20887;&#20313;&#36793;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.08148</link><description>&lt;p&gt;
&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#30340;&#23618;&#27425;&#21270;&#25299;&#25169;&#25490;&#24207;&#19982;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Topological Ordering with Conditional Independence Test for Limited Time Series. (arXiv:2308.08148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#30446;&#30340;&#26159;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#22312;&#29983;&#25104;&#20887;&#20313;&#36793;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20197;&#35782;&#21035;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#26082;&#33267;&#20851;&#37325;&#35201;&#21448;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25299;&#25169;&#30340;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#20004;&#27493;&#27861;&#26469;&#21457;&#29616;DAG&#65292;&#39318;&#20808;&#23398;&#20064;&#21464;&#37327;&#30340;&#25299;&#25169;&#25490;&#24207;&#65292;&#28982;&#21518;&#28040;&#38500;&#20887;&#20313;&#36793;&#65292;&#21516;&#26102;&#30830;&#20445;&#22270;&#24418;&#20445;&#25345;&#26080;&#29615;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20010;&#38480;&#21046;&#26159;&#36825;&#20123;&#26041;&#27861;&#20250;&#29983;&#25104;&#22823;&#37327;&#30340;&#34394;&#20551;&#36793;&#65292;&#38656;&#35201;&#21518;&#32493;&#20462;&#21098;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#20013;&#20165;&#21253;&#21547;&#20004;&#20010;&#19981;&#24517;&#30456;&#37051;&#19988;&#20855;&#26377;&#28789;&#27963;&#26102;&#38388;&#30340;&#27178;&#25130;&#38754;&#35760;&#24405;&#12290;&#36890;&#36807;&#23558;&#26465;&#20214;&#24037;&#20855;&#21464;&#37327;&#20316;&#20026;&#22806;&#29983;&#24178;&#39044;&#21464;&#37327;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#27599;&#20010;&#21464;&#37327;&#35782;&#21035;&#21518;&#20195;&#33410;&#28857;&#12290;&#22312;&#36825;&#19968;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#23618;&#27425;&#21270;&#25299;&#25169;&#25490;&#24207;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning directed acyclic graphs (DAGs) to identify causal relations underlying observational data is crucial but also poses significant challenges. Recently, topology-based methods have emerged as a two-step approach to discovering DAGs by first learning the topological ordering of variables and then eliminating redundant edges, while ensuring that the graph remains acyclic. However, one limitation is that these methods would generate numerous spurious edges that require subsequent pruning. To overcome this limitation, in this paper, we propose an improvement to topology-based methods by introducing limited time series data, consisting of only two cross-sectional records that need not be adjacent in time and are subject to flexible timing. By incorporating conditional instrumental variables as exogenous interventions, we aim to identify descendant nodes for each variable. Following this line, we propose a hierarchical topological ordering algorithm with conditional independence test (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#25511;&#21046;&#32447;&#24615;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35782;&#21035;&#31995;&#32479;&#27169;&#22411;&#65292;&#32780;&#26159;&#36890;&#36807;&#32047;&#31215;&#25200;&#21160;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#24615;&#33021;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2308.08138</link><description>&lt;p&gt;
&#22312;&#32447;&#25511;&#21046;&#32447;&#24615;&#21160;&#21147;&#23398;&#65306;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Control for Linear Dynamics: A Data-Driven Approach. (arXiv:2308.08138v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#25511;&#21046;&#32447;&#24615;&#21160;&#21147;&#23398;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35782;&#21035;&#31995;&#32479;&#27169;&#22411;&#65292;&#32780;&#26159;&#36890;&#36807;&#32047;&#31215;&#25200;&#21160;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#24615;&#33021;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#22312;&#32447;&#25511;&#21046;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#20855;&#26377;&#26410;&#30693;&#21160;&#21147;&#23398;&#12289;&#26377;&#30028;&#25200;&#21160;&#21644;&#23545;&#25239;&#25104;&#26412;&#30340;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#26469;&#38477;&#20302;&#25511;&#21046;&#22120;&#30340;&#21518;&#24724;&#12290;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#35782;&#21035;&#31995;&#32479;&#27169;&#22411;&#65292;&#32780;&#26159;&#21033;&#29992;&#21333;&#20010;&#26080;&#22122;&#22768;&#36712;&#36857;&#26469;&#35745;&#31639;&#25200;&#21160;&#30340;&#32047;&#31215;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#35774;&#35745;&#30340;&#32047;&#31215;&#25200;&#21160;&#21160;&#20316;&#25511;&#21046;&#22120;&#20570;&#20986;&#20915;&#31574;&#65292;&#20854;&#21442;&#25968;&#36890;&#36807;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#26356;&#26032;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#31639;&#27861;&#30340;&#21518;&#24724;&#26159;$\mathcal{O}(\sqrt{T})$&#30340;&#65292;&#36825;&#34920;&#26126;&#23427;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers an online control problem over a linear time-invariant system with unknown dynamics, bounded disturbance, and adversarial cost. We propose a data-driven strategy to reduce the regret of the controller. Unlike model-based methods, our algorithm does not identify the system model, instead, it leverages a single noise-free trajectory to calculate the accumulation of disturbance and makes decisions using the accumulated disturbance action controller we design, whose parameters are updated by online gradient descent. We prove that the regret of our algorithm is $\mathcal{O}(\sqrt{T})$ under mild assumptions, suggesting that its performance is on par with model-based methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#35746;&#21333;&#27969;&#25968;&#25454;&#20013;&#26377;&#25928;&#25552;&#21462;&#20851;&#38190;&#22240;&#23376;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.08135</link><description>&lt;p&gt;
&#24494;&#32467;&#26500;&#24378;&#21270;&#30340;&#32929;&#31080;&#22240;&#23376;&#25552;&#21462;&#19982;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Microstructure-Empowered Stock Factor Extraction and Utilization. (arXiv:2308.08135v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#35746;&#21333;&#27969;&#25968;&#25454;&#20013;&#26377;&#25928;&#25552;&#21462;&#20851;&#38190;&#22240;&#23376;&#65292;&#29992;&#20110;&#22810;&#26679;&#21270;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#39057;&#37327;&#21270;&#25237;&#36164;&#26159;&#32929;&#31080;&#25237;&#36164;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#29305;&#21035;&#26159;&#35746;&#21333;&#27969;&#25968;&#25454;&#22312;&#39640;&#39057;&#20132;&#26131;&#25968;&#25454;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#26368;&#35814;&#32454;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#35746;&#21333;&#31807;&#21644;&#36880;&#31508;&#20132;&#26131;&#35760;&#24405;&#30340;&#20840;&#38754;&#25968;&#25454;&#12290;&#35746;&#21333;&#27969;&#25968;&#25454;&#23545;&#24066;&#22330;&#20998;&#26512;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#20026;&#20132;&#26131;&#32773;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#27934;&#23519;&#21147;&#65292;&#24110;&#21161;&#20182;&#20204;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#20256;&#32479;&#22240;&#23376;&#25366;&#25496;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#65292;&#35746;&#21333;&#27969;&#25968;&#25454;&#30340;&#25552;&#21462;&#21644;&#26377;&#25928;&#21033;&#29992;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#20123;&#25216;&#26415;&#20027;&#35201;&#26159;&#20026;&#31895;&#31890;&#24230;&#32929;&#31080;&#25968;&#25454;&#35774;&#35745;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#35746;&#21333;&#27969;&#25968;&#25454;&#20013;&#26377;&#25928;&#25552;&#21462;&#20851;&#38190;&#22240;&#23376;&#65292;&#29992;&#20110;&#19981;&#21516;&#31890;&#24230;&#21644;&#22330;&#26223;&#30340;&#22810;&#26679;&#21270;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#22240;&#23376;&#25552;&#21462;&#22120;&#12290;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#23398;&#20064;&#19968;&#20010;&#23884;&#20837;f
&lt;/p&gt;
&lt;p&gt;
High-frequency quantitative investment is a crucial aspect of stock investment. Notably, order flow data plays a critical role as it provides the most detailed level of information among high-frequency trading data, including comprehensive data from the order book and transaction records at the tick level. The order flow data is extremely valuable for market analysis as it equips traders with essential insights for making informed decisions. However, extracting and effectively utilizing order flow data present challenges due to the large volume of data involved and the limitations of traditional factor mining techniques, which are primarily designed for coarser-level stock data. To address these challenges, we propose a novel framework that aims to effectively extract essential factors from order flow data for diverse downstream tasks across different granularities and scenarios. Our method consists of a Context Encoder and an Factor Extractor. The Context Encoder learns an embedding f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#25105;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#32467;&#26524;&#21457;&#29616;&#21363;&#20351;&#22312;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#30340;&#22806;&#25512;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08129</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#23545;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#22806;&#25512;&#26159;&#21542;&#26377;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Self-Supervised Pretraining Good for Extrapolation in Molecular Property Prediction?. (arXiv:2308.08129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#25105;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#32467;&#26524;&#21457;&#29616;&#21363;&#20351;&#22312;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26465;&#20214;&#19979;&#65292;&#20934;&#30830;&#30340;&#22806;&#25512;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#24615;&#36136;&#30340;&#39044;&#27979;&#22312;&#30005;&#27744;&#12289;&#21322;&#23548;&#20307;&#12289;&#20652;&#21270;&#21058;&#21644;&#33647;&#29289;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32467;&#21512;&#24120;&#35268;&#29702;&#35770;&#35745;&#31639;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#65292;&#26410;&#35266;&#27979;&#30340;&#20540;&#65292;&#36890;&#24120;&#31216;&#20026;&#22806;&#25512;&#65292;&#22312;&#24615;&#36136;&#39044;&#27979;&#20013;&#23588;&#20854;&#20851;&#38190;&#65292;&#22240;&#20026;&#23427;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#28145;&#20837;&#20102;&#35299;&#36229;&#20986;&#29616;&#26377;&#25968;&#25454;&#38480;&#21046;&#33539;&#22260;&#30340;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#19979;&#65292;&#20934;&#30830;&#30340;&#22806;&#25512;&#20173;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#25105;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#27169;&#22411;&#39318;&#20808;&#36890;&#36807;&#20351;&#29992;&#30456;&#23545;&#31616;&#21333;&#30340;&#21069;&#25552;&#20219;&#21153;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#30446;&#26631;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of material properties plays a crucial role in the development and discovery of materials in diverse applications, such as batteries, semiconductors, catalysts, and pharmaceuticals. Recently, there has been a growing interest in employing data-driven approaches by using machine learning technologies, in combination with conventional theoretical calculations. In material science, the prediction of unobserved values, commonly referred to as extrapolation, is particularly critical for property prediction as it enables researchers to gain insight into materials beyond the limits of available data. However, even with the recent advancements in powerful machine learning models, accurate extrapolation is still widely recognized as a significantly challenging problem. On the other hand, self-supervised pretraining is a machine learning technique where a model is first trained on unlabeled data using relatively simple pretext tasks before being trained on labeled data for target 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#31995;&#32479;&#21270;&#32534;&#30721;&#21644;&#21452;&#37325;&#36974;&#34109;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08128</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#36827;&#34892;&#36974;&#34109;&#65306;&#31995;&#32479;&#21270;&#19982;&#21452;&#37325;&#36974;&#34109;
&lt;/p&gt;
&lt;p&gt;
How to Mask in Error Correction Code Transformer: Systematic and Double Masking. (arXiv:2308.08128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08128
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#31995;&#32479;&#21270;&#32534;&#30721;&#21644;&#21452;&#37325;&#36974;&#34109;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#20449;&#21644;&#23384;&#20648;&#31995;&#32479;&#20013;&#65292;&#32416;&#38169;&#30721;&#65288;ECC&#65289;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#24191;&#27867;&#25193;&#23637;&#65292;&#31070;&#32463;&#32593;&#32476;&#35299;&#30721;&#22120;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#65292;&#36229;&#36234;&#20256;&#32479;&#35299;&#30721;&#31639;&#27861;&#12290;&#22312;&#36825;&#20123;&#31070;&#32463;&#35299;&#30721;&#22120;&#20013;&#65292;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#65288;ECCT&#65289;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;ECCT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;ECC&#30340;&#31995;&#32479;&#32534;&#30721;&#25216;&#26415;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36974;&#34109;&#30697;&#38453;&#26469;&#25913;&#21892;ECCT&#30340;&#24615;&#33021;&#24182;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ECCT&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#31216;&#20026;&#21452;&#37325;&#36974;&#34109;&#30340;ECCT&#12290;&#35813;&#26550;&#26500;&#20197;&#24182;&#34892;&#26041;&#24335;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#36974;&#34109;&#30697;&#38453;&#65292;&#20197;&#23398;&#20064;&#36974;&#34109;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#32534;&#30721;&#23383;&#20301;&#20043;&#38388;&#26356;&#22810;&#26679;&#30340;&#29305;&#24449;&#20851;&#31995;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive si
&lt;/p&gt;</description></item><item><title>S-Mixup&#26159;&#19968;&#31181;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#32467;&#26500;&#20449;&#24687;&#30340;&#26032;&#22411;Mixup&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#21644;&#36793;&#26799;&#24230;&#26469;&#26500;&#24314;Mixup&#27744;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.08097</link><description>&lt;p&gt;
S-Mixup: &#32467;&#26500;Mixup&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
S-Mixup: Structural Mixup for Graph Neural Networks. (arXiv:2308.08097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08097
&lt;/p&gt;
&lt;p&gt;
S-Mixup&#26159;&#19968;&#31181;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#32467;&#26500;&#20449;&#24687;&#30340;&#26032;&#22411;Mixup&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#21644;&#36793;&#26799;&#24230;&#26469;&#26500;&#24314;Mixup&#27744;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24212;&#29992;Mixup&#25216;&#26415;&#20110;&#22270;&#20013;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#32780;&#33410;&#28857;&#20998;&#31867;&#30340;&#30740;&#31350;&#20173;&#26410;&#28145;&#20837;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33410;&#28857;&#20998;&#31867;Mixup&#22686;&#24378;&#26041;&#27861;&#65292;&#31216;&#20026;&#32467;&#26500;Mixup&#65288;S-Mixup&#65289;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#28151;&#21512;&#33410;&#28857;&#26102;&#32771;&#34385;&#32467;&#26500;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;S-Mixup&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20998;&#31867;&#22120;&#33719;&#24471;&#22270;&#20013;&#26410;&#26631;&#35760;&#33410;&#28857;&#30340;&#20266;&#26631;&#31614;&#21644;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;&#36825;&#20123;&#20266;&#26631;&#31614;&#21644;&#32622;&#20449;&#24230;&#20316;&#20026;&#26500;&#25104;&#36328;&#31867;&#21644;&#20869;&#31867;Mixup&#30340;Mixup&#27744;&#30340;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;GNN&#35757;&#32451;&#20013;&#33719;&#24471;&#30340;&#36793;&#26799;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36793;&#36873;&#25321;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#36830;&#25509;&#21040;Mixup&#29983;&#25104;&#30340;&#33410;&#28857;&#30340;&#36793;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;S-Mixup&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;S-Mixup&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing studies for applying the mixup technique on graphs mainly focus on graph classification tasks, while the research in node classification is still under-explored. In this paper, we propose a novel mixup augmentation for node classification called Structural Mixup (S-Mixup). The core idea is to take into account the structural information while mixing nodes. Specifically, S-Mixup obtains pseudo-labels for unlabeled nodes in a graph along with their prediction confidence via a Graph Neural Network (GNN) classifier. These serve as the criteria for the composition of the mixup pool for both inter and intra-class mixups. Furthermore, we utilize the edge gradient obtained from the GNN training and propose a gradient-based edge selection strategy for selecting edges to be attached to the nodes generated by the mixup. Through extensive experiments on real-world benchmark datasets, we demonstrate the effectiveness of S-Mixup evaluated on the node classification task. We observe that S-M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20984;&#20248;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#35774;&#35745;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#36807;&#28388;&#22120;&#33021;&#22815;&#25429;&#33719;&#24314;&#27169;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26469;&#25628;&#32034;&#21487;&#20197;&#20445;&#35777;&#32422;&#26463;&#28385;&#36275;&#30340;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#22312;&#38750;&#32447;&#24615;&#25670;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08086</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20248;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#35774;&#35745;&#23433;&#20840;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Safety Filter Design for Neural Network Systems via Convex Optimization. (arXiv:2308.08086v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20984;&#20248;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#35774;&#35745;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#36807;&#28388;&#22120;&#33021;&#22815;&#25429;&#33719;&#24314;&#27169;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26469;&#25628;&#32034;&#21487;&#20197;&#20445;&#35777;&#32422;&#26463;&#28385;&#36275;&#30340;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#22312;&#38750;&#32447;&#24615;&#25670;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#22686;&#21152;&#65292;&#24050;&#32463;&#24191;&#27867;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#33021;&#22815;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20934;&#30830;&#25429;&#33719;&#22797;&#26434;&#30340;&#31995;&#32479;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;NN&#30340;&#26550;&#26500;&#22797;&#26434;&#24615;&#21644;&#38750;&#32447;&#24615;&#20351;&#24471;&#23545;&#20854;&#21512;&#25104;&#19968;&#20010;&#33021;&#22815;&#34987;&#35777;&#26126;&#26159;&#23433;&#20840;&#30340;&#25511;&#21046;&#22120;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#23427;&#20381;&#38752;&#20984;&#20248;&#21270;&#26469;&#30830;&#20445;NN&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#33021;&#22815;&#25429;&#33719;&#24314;&#27169;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;NN&#39564;&#35777;&#24037;&#20855;&#26469;&#20351;&#29992;&#19968;&#32452;&#32447;&#24615;&#36793;&#30028;&#26469;&#36817;&#20284;NN&#30340;&#21160;&#24577;&#65292;&#28982;&#21518;&#24212;&#29992;&#40065;&#26834;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;robust linear MPC&#65289;&#26469;&#23547;&#25214;&#21487;&#20197;&#20445;&#35777;&#40065;&#26834;&#32422;&#26463;&#28385;&#36275;&#30340;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#22312;&#38750;&#32447;&#24615;&#25670;&#31995;&#32479;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increase in data availability, it has been widely demonstrated that neural networks (NN) can capture complex system dynamics precisely in a data-driven manner. However, the architectural complexity and nonlinearity of the NNs make it challenging to synthesize a provably safe controller. In this work, we propose a novel safety filter that relies on convex optimization to ensure safety for a NN system, subject to additive disturbances that are capable of capturing modeling errors. Our approach leverages tools from NN verification to over-approximate NN dynamics with a set of linear bounds, followed by an application of robust linear MPC to search for controllers that can guarantee robust constraint satisfaction. We demonstrate the efficacy of the proposed framework numerically on a nonlinear pendulum system.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22320;&#19979;&#25968;&#25454;&#38598;&#30340;&#31283;&#23450;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21018;&#24615;&#21464;&#25442;&#23454;&#29616;&#20102;&#27431;&#20960;&#37324;&#24503;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#37327;&#21270;&#22320;&#19979;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#19988;&#36866;&#24212;&#22806;&#26679;&#26412;&#28857;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2308.08079</link><description>&lt;p&gt;
&#29992;&#20110;&#25903;&#25345;&#22320;&#19979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#35299;&#37322;&#30340;&#31283;&#23450;&#20302;&#32500;&#31354;&#38388;&#21018;&#24615;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Rigid Transformations for Stabilized Lower Dimensional Space to Support Subsurface Uncertainty Quantification and Interpretation. (arXiv:2308.08079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08079
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22320;&#19979;&#25968;&#25454;&#38598;&#30340;&#31283;&#23450;&#38477;&#32500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21018;&#24615;&#21464;&#25442;&#23454;&#29616;&#20102;&#27431;&#20960;&#37324;&#24503;&#19981;&#21464;&#34920;&#31034;&#65292;&#33021;&#22815;&#37327;&#21270;&#22320;&#19979;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#19988;&#36866;&#24212;&#22806;&#26679;&#26412;&#28857;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#19979;&#25968;&#25454;&#38598;&#22825;&#28982;&#22320;&#20855;&#26377;&#22823;&#25968;&#25454;&#29305;&#24449;&#65292;&#22914;&#24222;&#22823;&#30340;&#20307;&#31215;&#12289;&#22810;&#26679;&#30340;&#29305;&#24449;&#21644;&#39640;&#36895;&#37319;&#26679;&#36895;&#24230;&#65292;&#21463;&#21040;&#21508;&#31181;&#29289;&#29702;&#12289;&#24037;&#31243;&#21644;&#22320;&#36136;&#36755;&#20837;&#24341;&#36215;&#30340;&#32500;&#25968;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#22312;&#29616;&#26377;&#30340;&#38477;&#32500;&#26041;&#27861;&#20013;&#65292;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24230;&#37327;&#22810;&#32500;&#32553;&#25918;&#65288;MDS&#65289;&#65292;&#26159;&#22320;&#19979;&#25968;&#25454;&#38598;&#20013;&#39318;&#36873;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#34429;&#28982;MDS&#20445;&#30041;&#20102;&#20869;&#22312;&#30340;&#25968;&#25454;&#32467;&#26500;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21253;&#25324;&#19981;&#31283;&#23450;&#30340;&#21807;&#19968;&#35299;&#65292;&#19981;&#21464;&#20110;&#27431;&#20960;&#37324;&#24503;&#21464;&#25442;&#65292;&#24182;&#19988;&#27809;&#26377;&#22806;&#26679;&#26412;&#28857;&#65288;OOSP&#65289;&#25193;&#23637;&#12290;&#20026;&#20102;&#22686;&#24378;&#22320;&#19979;&#25512;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#65292;&#24517;&#39035;&#23558;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#31283;&#23450;&#30340;&#12289;&#38477;&#32500;&#30340;&#34920;&#31034;&#65292;&#20197;&#23481;&#32435;OOSP&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#21018;&#24615;&#21464;&#25442;&#23454;&#29616;&#20102;LDS&#30340;&#31283;&#23450;&#27431;&#20960;&#37324;&#24503;&#19981;&#21464;&#34920;&#31034;&#12290;&#36890;&#36807;&#35745;&#31639;MDS&#36755;&#20837;&#30340;&#19981;&#30456;&#20284;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
Subsurface datasets inherently possess big data characteristics such as vast volume, diverse features, and high sampling speeds, further compounded by the curse of dimensionality from various physical, engineering, and geological inputs. Among the existing dimensionality reduction (DR) methods, nonlinear dimensionality reduction (NDR) methods, especially Metric-multidimensional scaling (MDS), are preferred for subsurface datasets due to their inherent complexity. While MDS retains intrinsic data structure and quantifies uncertainty, its limitations include unstabilized unique solutions invariant to Euclidean transformations and an absence of out-of-sample points (OOSP) extension. To enhance subsurface inferential and machine learning workflows, datasets must be transformed into stable, reduced-dimension representations that accommodate OOSP.  Our solution employs rigid transformations for a stabilized Euclidean invariant representation for LDS. By computing an MDS input dissimilarity m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGREC&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#65292;&#20854;&#20013;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#20844;&#24320;&#20182;&#20204;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22270;&#26500;&#24314;&#12289;&#23616;&#37096;&#26799;&#24230;&#35745;&#31639;&#21644;&#20840;&#23616;&#26799;&#24230;&#20256;&#36882;&#19977;&#20010;&#38454;&#27573;&#23454;&#29616;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21517;&#20026;&#23433;&#20840;&#26799;&#24230;&#20849;&#20139;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#20445;&#25252;&#29992;&#25143;&#30340;&#31169;&#23494;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08072</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Decentralized Graph Neural Network for Privacy-Preserving Recommendation. (arXiv:2308.08072v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGREC&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#65292;&#20854;&#20013;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#20844;&#24320;&#20182;&#20204;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22270;&#26500;&#24314;&#12289;&#23616;&#37096;&#26799;&#24230;&#35745;&#31639;&#21644;&#20840;&#23616;&#26799;&#24230;&#20256;&#36882;&#19977;&#20010;&#38454;&#27573;&#23454;&#29616;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#21517;&#20026;&#23433;&#20840;&#26799;&#24230;&#20849;&#20139;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#20445;&#25252;&#29992;&#25143;&#30340;&#31169;&#23494;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#36829;&#21453;&#29992;&#25143;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#32852;&#37030;GNN&#21644;&#21435;&#20013;&#24515;&#21270;GNN&#20004;&#31181;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23384;&#22312;&#38382;&#39064;&#65292;&#22914;&#36890;&#20449;&#25928;&#29575;&#20302;&#21644;&#38544;&#31169;&#27844;&#38706;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#20013;&#24515;&#21270;GNN&#26694;&#26550;&#65292;&#21517;&#20026;DGREC&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#20844;&#24320;&#20182;&#20204;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65292;&#21363;&#22270;&#26500;&#24314;&#12289;&#23616;&#37096;&#26799;&#24230;&#35745;&#31639;&#21644;&#20840;&#23616;&#26799;&#24230;&#20256;&#36882;&#12290;&#31532;&#19968;&#38454;&#27573;&#20026;&#27599;&#20010;&#29992;&#25143;&#26500;&#24314;&#20102;&#19968;&#20010;&#26412;&#22320;&#20869;&#37096;&#29289;&#21697;&#36229;&#22270;&#21644;&#19968;&#20010;&#20840;&#23616;&#29992;&#25143;&#38388;&#22270;&#12290;&#31532;&#20108;&#38454;&#27573;&#23545;&#29992;&#25143;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#22312;&#27599;&#20010;&#26412;&#22320;&#35774;&#22791;&#19978;&#35745;&#31639;&#26799;&#24230;&#12290;&#31532;&#19977;&#38454;&#27573;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;&#26799;&#24230;&#20849;&#20139;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#30340;&#31169;&#23494;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#19968;&#36143;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a graph neural network (GNN)-based recommender system without violating user privacy proves challenging. Existing methods can be divided into federated GNNs and decentralized GNNs. But both methods have undesirable effects, i.e., low communication efficiency and privacy leakage. This paper proposes DGREC, a novel decentralized GNN for privacy-preserving recommendations, where users can choose to publicize their interactions. It includes three stages, i.e., graph construction, local gradient calculation, and global gradient passing. The first stage builds a local inner-item hypergraph for each user and a global inter-user graph. The second stage models user preference and calculates gradients on each local device. The third stage designs a local differential privacy mechanism named secure gradient-sharing, which proves strong privacy-preserving of users' private data. We conduct extensive experiments on three public datasets to validate the consistent superiority of our framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26032;&#40092;&#24230;&#21644;&#26631;&#31614;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.08071</link><description>&lt;p&gt;
&#40092;&#24230;&#25110;&#20934;&#30830;&#24615;&#65292;&#20026;&#20160;&#20040;&#19981;&#33021;&#20004;&#32773;&#20860;&#24471;&#65311;&#36890;&#36807;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Freshness or Accuracy, Why Not Both? Addressing Delayed Feedback via Dynamic Graph Neural Networks. (arXiv:2308.08071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26032;&#40092;&#24230;&#21644;&#26631;&#31614;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#21830;&#19994;&#31995;&#32479;&#20013;&#65292;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;&#26159;&#39044;&#27979;&#36716;&#21270;&#29575;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#25361;&#25112;&#20043;&#19968;&#65292;&#22240;&#20026;&#29992;&#25143;&#30340;&#36716;&#21270;&#24635;&#26159;&#24310;&#36831;&#21457;&#29983;&#12290;&#34429;&#28982;&#26032;&#25968;&#25454;&#26377;&#30410;&#20110;&#25345;&#32493;&#35757;&#32451;&#65292;&#20294;&#26159;&#27809;&#26377;&#23436;&#25972;&#30340;&#21453;&#39304;&#20449;&#24687;&#65292;&#21363;&#36716;&#21270;&#26631;&#31614;&#65292;&#35757;&#32451;&#31639;&#27861;&#21487;&#33021;&#20250;&#36973;&#21463;&#22823;&#37327;&#30340;&#20551;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25110;&#35774;&#35745;&#25968;&#25454;&#27969;&#31243;&#26469;&#35299;&#20915;&#24310;&#36831;&#21453;&#39304;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#25968;&#25454;&#26032;&#40092;&#24230;&#21644;&#26631;&#31614;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#25240;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24310;&#36831;&#21453;&#39304;&#24314;&#27169; &#65288;DGDFEM&#65289;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65292;&#21363;&#20934;&#22791;&#25968;&#25454;&#27969;&#31243;&#12289;&#26500;&#24314;&#21160;&#24577;&#22270;&#20197;&#21450;&#35757;&#32451;CVR&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HLGCN&#30340;&#26032;&#39062;&#22270;&#21367;&#31215;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#39640;&#36890;&#21644;&#20302;&#36890;&#28388;&#27874;&#22120;&#26469;&#22788;&#29702;&#36716;&#21270;&#21644;&#38750;&#36716;&#21270;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25968;&#25454;&#26032;&#40092;&#24230;&#21644;&#26631;&#31614;&#20934;&#30830;&#24615;&#30340;&#21452;&#37325;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The delayed feedback problem is one of the most pressing challenges in predicting the conversion rate since users' conversions are always delayed in online commercial systems. Although new data are beneficial for continuous training, without complete feedback information, i.e., conversion labels, training algorithms may suffer from overwhelming fake negatives. Existing methods tend to use multitask learning or design data pipelines to solve the delayed feedback problem. However, these methods have a trade-off between data freshness and label accuracy. In this paper, we propose Delayed Feedback Modeling by Dynamic Graph Neural Network (DGDFEM). It includes three stages, i.e., preparing a data pipeline, building a dynamic graph, and training a CVR prediction model. In the model training, we propose a novel graph convolutional method named HLGCN, which leverages both high-pass and low-pass filters to deal with conversion and non-conversion relationships. The proposed method achieves both 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#20223;&#23556;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#20998;&#26512;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#35266;&#27979;&#27425;&#25968;&#36739;&#23569;&#26102;&#37117;&#33021;&#21462;&#24471;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08070</link><description>&lt;p&gt;
&#26368;&#22823;&#20223;&#23556;&#22238;&#24402;&#21450;&#20854;&#19968;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Max-affine regression via first-order methods. (arXiv:2308.08070v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#20223;&#23556;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#20998;&#26512;&#26041;&#27861;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#35266;&#27979;&#27425;&#25968;&#36739;&#23569;&#26102;&#37117;&#33021;&#21462;&#24471;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26368;&#22823;&#20223;&#23556;&#27169;&#22411;&#30340;&#22238;&#24402;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#26368;&#22823;&#21270;&#20989;&#25968;&#23558;&#20223;&#23556;&#27169;&#22411;&#32452;&#21512;&#25104;&#20998;&#27573;&#32447;&#24615;&#27169;&#22411;&#12290;&#26368;&#22823;&#20223;&#23556;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#20449;&#21495;&#22788;&#29702;&#21644;&#32479;&#35745;&#23398;&#20013;&#65292;&#21253;&#25324;&#22810;&#31867;&#21035;&#20998;&#31867;&#12289;&#25293;&#21334;&#38382;&#39064;&#21644;&#20984;&#22238;&#24402;&#12290;&#23427;&#36824;&#25512;&#24191;&#20102;&#30456;&#20301;&#24674;&#22797;&#21644;&#23398;&#20064;&#25972;&#27969;&#32447;&#24615;&#21333;&#20803;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#23545;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#23567;&#25209;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#22312;&#26368;&#22823;&#20223;&#23556;&#22238;&#24402;&#20013;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20551;&#35774;&#27169;&#22411;&#20197;&#38543;&#26426;&#20301;&#32622;&#35266;&#27979;&#65292;&#36981;&#24490;&#27425;&#39640;&#26031;&#20998;&#24067;&#21644;&#20855;&#26377;&#21152;&#24615;&#27425;&#39640;&#26031;&#22122;&#22768;&#30340;&#21453;&#27987;&#24230;&#12290;&#22312;&#36825;&#20123;&#20551;&#35774;&#19979;&#65292;&#36866;&#24403;&#21021;&#22987;&#21270;&#30340;GD&#21644;SGD&#33021;&#22815;&#32447;&#24615;&#25910;&#25947;&#21040;&#30001;&#30456;&#24212;&#35823;&#24046;&#30028;&#38480;&#30830;&#23450;&#30340;&#30446;&#26631;&#21306;&#22495;&#38468;&#36817;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19982;&#29702;&#35770;&#21457;&#29616;&#30456;&#19968;&#33268;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SGD&#19981;&#20165;&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#25910;&#25947;&#26356;&#24555;&#65292;&#32780;&#19988;&#22312;&#35266;&#27979;&#27425;&#25968;&#36739;&#23569;&#26102;&#20063;&#33021;&#33719;&#24471;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider regression of a max-affine model that produces a piecewise linear model by combining affine models via the max function. The max-affine model ubiquitously arises in applications in signal processing and statistics including multiclass classification, auction problems, and convex regression. It also generalizes phase retrieval and learning rectifier linear unit activation functions. We present a non-asymptotic convergence analysis of gradient descent (GD) and mini-batch stochastic gradient descent (SGD) for max-affine regression when the model is observed at random locations following the sub-Gaussianity and an anti-concentration with additive sub-Gaussian noise. Under these assumptions, a suitably initialized GD and SGD converge linearly to a neighborhood of the ground truth specified by the corresponding error bound. We provide numerical results that corroborate the theoretical finding. Importantly, SGD not only converges faster in run time with fewer observations than alt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20113;&#35745;&#31639;&#33410;&#28857;&#19978;&#35774;&#35745;&#20102;&#19968;&#20010;&#33021;&#22815;&#38480;&#21046;&#21151;&#32791;&#32780;&#19981;&#24433;&#21709;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.08069</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#20013;&#24515;&#35745;&#31639;&#33410;&#28857;&#21151;&#32791;&#38477;&#20302;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Approach for Performance-aware Reduction in Power Consumption of Data Center Compute Nodes. (arXiv:2308.08069v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20113;&#35745;&#31639;&#33410;&#28857;&#19978;&#35774;&#35745;&#20102;&#19968;&#20010;&#33021;&#22815;&#38480;&#21046;&#21151;&#32791;&#32780;&#19981;&#24433;&#21709;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36229;&#32423;&#35745;&#31639;&#25104;&#20026;&#29616;&#23454;&#65292;&#20113;&#25968;&#25454;&#20013;&#24515;&#30340;&#35745;&#31639;&#33410;&#28857;&#33021;&#28304;&#38656;&#27714;&#23558;&#32487;&#32493;&#22686;&#38271;&#12290;&#20943;&#23569;&#33021;&#28304;&#38656;&#27714;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#22312;&#31995;&#32479;&#20854;&#20182;&#37096;&#20998;&#38754;&#20020;&#29942;&#39048;&#26102;&#38480;&#21046;&#30828;&#20214;&#32452;&#20214;&#30340;&#21151;&#32791;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24182;&#38480;&#21046;&#21151;&#32791;&#30340;&#36164;&#28304;&#25511;&#21046;&#22120;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#36824;&#21487;&#33021;&#23545;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35774;&#35745;&#20113;&#35745;&#31639;&#33410;&#28857;&#19978;&#30340;&#21151;&#29575;&#38480;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#35266;&#23519;&#24403;&#21069;&#21151;&#32791;&#21644;&#30636;&#26102;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#65288;&#24515;&#36339;&#65289;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#21033;&#29992;Argo Node&#36164;&#28304;&#31649;&#29702;&#65288;NRM&#65289;&#36719;&#20214;&#22534;&#26632;&#21644;&#33521;&#29305;&#23572;&#36816;&#34892;&#24179;&#22343;&#21151;&#32791;&#38480;&#21046;&#65288;RAPL&#65289;&#30828;&#20214;&#25511;&#21046;&#26426;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20195;&#29702;&#26469;&#25511;&#21046;&#20379;&#24212;&#32473;&#22788;&#29702;&#22120;&#30340;&#26368;&#22823;&#21151;&#29575;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#12290;&#37319;&#29992;&#19968;&#31181;&#25509;&#36817;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65288;Proximal&#65289;
&lt;/p&gt;
&lt;p&gt;
As Exascale computing becomes a reality, the energy needs of compute nodes in cloud data centers will continue to grow. A common approach to reducing this energy demand is to limit the power consumption of hardware components when workloads are experiencing bottlenecks elsewhere in the system. However, designing a resource controller capable of detecting and limiting power consumption on-the-fly is a complex issue and can also adversely impact application performance. In this paper, we explore the use of Reinforcement Learning (RL) to design a power capping policy on cloud compute nodes using observations on current power consumption and instantaneous application performance (heartbeats). By leveraging the Argo Node Resource Management (NRM) software stack in conjunction with the Intel Running Average Power Limit (RAPL) hardware control mechanism, we design an agent to control the maximum supplied power to processors without compromising on application performance. Employing a Proximal
&lt;/p&gt;</description></item><item><title>&#22312;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#38656;&#35201;&#20851;&#27880;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#26368;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#24314;&#27169;&#26694;&#26550;&#65292;&#24110;&#21161;&#20225;&#19994;&#28145;&#20837;&#20102;&#35299;&#21644;&#35780;&#20272;&#36825;&#20123;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.08061</link><description>&lt;p&gt;
&#39640;&#25104;&#26412;&#22256;&#22659;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#26368;&#20248;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Costly Dilemma: Generalization, Evaluation and Cost-Optimal Deployment of Large Language Models. (arXiv:2308.08061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08061
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#38656;&#35201;&#20851;&#27880;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#26368;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#24314;&#27169;&#26694;&#26550;&#65292;&#24110;&#21161;&#20225;&#19994;&#28145;&#20837;&#20102;&#35299;&#21644;&#35780;&#20272;&#36825;&#20123;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20026;&#20219;&#20309;&#20135;&#21697;/&#24212;&#29992;&#31243;&#24207;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#36890;&#24120;&#24076;&#26395;&#20855;&#22791;&#19977;&#20010;&#23646;&#24615;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#24212;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#22312;&#25105;&#20204;&#23545;&#39046;&#22495;&#30693;&#35782;&#30340;&#21457;&#23637;&#20013;&#21487;&#20197;&#25193;&#23637;&#20854;&#29992;&#36884;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#24212;&#35813;&#26159;&#21487;&#35780;&#20272;&#30340;&#65292;&#36825;&#26679;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#21487;&#20197;&#26377;&#28165;&#26224;&#30340;&#24615;&#33021;&#25351;&#26631;&#21644;&#35745;&#31639;&#36825;&#20123;&#25351;&#26631;&#30340;&#21487;&#34892;&#26041;&#24335;&#12290;&#26368;&#21518;&#65292;&#37096;&#32626;&#24212;&#23613;&#21487;&#33021;&#22320;&#25104;&#26412;&#26368;&#20248;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36825;&#19977;&#20010;&#30446;&#26631;&#65288;&#21363;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#26368;&#20248;&#21270;&#65289;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24448;&#24448;&#26159;&#30456;&#23545;&#29420;&#31435;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23613;&#31649;&#20854;&#22312;&#20256;&#32479;NLP&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20225;&#19994;&#22312;&#23545;&#36825;&#39033;&#25216;&#26415;&#36827;&#34892;&#37325;&#22823;&#25237;&#36164;&#20043;&#21069;&#38656;&#35201;&#20180;&#32454;&#35780;&#20272;&#25152;&#26377;&#19977;&#20010;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#12289;&#35780;&#20272;&#21644;&#25104;&#26412;&#24314;&#27169;&#26694;&#26550;&#65292;&#20026;&#20225;&#19994;&#25552;&#20379;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#22797;&#26434;&#22240;&#32032;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying machine learning models in production for any product/application, there are three properties that are commonly desired. First, the models should be generalizable, in that we can extend it to further use cases as our knowledge of the domain area develops. Second they should be evaluable, so that there are clear metrics for performance and the calculation of those metrics in production settings are feasible. Finally, the deployment should be cost-optimal as far as possible. In this paper we propose that these three objectives (i.e. generalization, evaluation and cost-optimality) can often be relatively orthogonal and that for large language models, despite their performance over conventional NLP models, enterprises need to carefully assess all the three factors before making substantial investments in this technology. We propose a framework for generalization, evaluation and cost-modeling specifically tailored to large language models, offering insights into the intricaci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#36125;&#21494;&#26031;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#33192;&#32960;&#27850;&#26494;&#27169;&#22411;&#26469;&#22788;&#29702;&#21253;&#21547;&#36807;&#22810;&#38646;&#20540;&#30340;&#39640;&#32500;&#35745;&#25968;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#38543;&#26426;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#33268;&#32858;&#21512;&#30340;&#26041;&#27861;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08060</link><description>&lt;p&gt;
&#40065;&#26834;&#36125;&#21494;&#26031;&#24352;&#37327;&#20998;&#35299;&#19982;&#38646;&#33192;&#32960;&#27850;&#26494;&#27169;&#22411;&#21644;&#19968;&#33268;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Robust Bayesian Tensor Factorization with Zero-Inflated Poisson Model and Consensus Aggregation. (arXiv:2308.08060v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#36125;&#21494;&#26031;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#20351;&#29992;&#38646;&#33192;&#32960;&#27850;&#26494;&#27169;&#22411;&#26469;&#22788;&#29702;&#21253;&#21547;&#36807;&#22810;&#38646;&#20540;&#30340;&#39640;&#32500;&#35745;&#25968;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#38543;&#26426;&#24615;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#33268;&#32858;&#21512;&#30340;&#26041;&#27861;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#20998;&#35299;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#34920;&#31034;&#21644;&#20998;&#26512;&#22810;&#32500;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#21253;&#21547;&#36807;&#22810;&#38646;&#20540;&#30340;&#35745;&#25968;&#25968;&#25454;&#65288;&#22914;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;&#65289;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#24352;&#37327;&#20998;&#35299;&#30340;&#38543;&#26426;&#24615;&#23548;&#33268;&#22240;&#23376;&#22312;&#22810;&#27425;&#36816;&#34892;&#20013;&#21464;&#21270;&#65292;&#20351;&#24471;&#32467;&#26524;&#30340;&#35299;&#37322;&#21644;&#37325;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#38646;&#33192;&#32960;&#27850;&#26494;&#24352;&#37327;&#20998;&#35299;&#65288;ZIPTF&#65289;&#65292;&#29992;&#20110;&#20998;&#35299;&#20855;&#26377;&#36807;&#22810;&#38646;&#20540;&#30340;&#39640;&#32500;&#35745;&#25968;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#38543;&#26426;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#19968;&#33268;&#38646;&#33192;&#32960;&#27850;&#26494;&#24352;&#37327;&#20998;&#35299;&#65288;C-ZIPTF&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;ZIPTF&#19982;&#22522;&#20110;&#19968;&#33268;&#24615;&#30340;&#20803;&#20998;&#26512;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#30340;&#38646;&#33192;&#32960;&#35745;&#25968;&#25968;&#25454;&#21644;&#21512;&#25104;&#30340;&#30495;&#23454;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#25968;&#25454;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;ZIPTF&#21644;C-ZIPTF&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ZIPTF&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;&#30697;&#38453;&#21644;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor factorizations (TF) are powerful tools for the efficient representation and analysis of multidimensional data. However, classic TF methods based on maximum likelihood estimation underperform when applied to zero-inflated count data, such as single-cell RNA sequencing (scRNA-seq) data. Additionally, the stochasticity inherent in TFs results in factors that vary across repeated runs, making interpretation and reproducibility of the results challenging. In this paper, we introduce Zero Inflated Poisson Tensor Factorization (ZIPTF), a novel approach for the factorization of high-dimensional count data with excess zeros. To address the challenge of stochasticity, we introduce Consensus Zero Inflated Poisson Tensor Factorization (C-ZIPTF), which combines ZIPTF with a consensus-based meta-analysis. We evaluate our proposed ZIPTF and C-ZIPTF on synthetic zero-inflated count data and synthetic and real scRNA-seq data. ZIPTF consistently outperforms baseline matrix and tensor factorizatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#19988;&#25928;&#26524;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#23384;&#22312;&#19968;&#20010;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08055</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#36827;&#34892;&#31616;&#21333;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple online learning with consistency oracle. (arXiv:2308.08055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#19988;&#25928;&#26524;&#26356;&#22909;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#23384;&#22312;&#19968;&#20010;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21482;&#33021;&#36890;&#36807;&#19968;&#33268;&#24615;&#39044;&#35328;&#26426;&#35775;&#38382;&#31867;&#30340;&#27169;&#22411;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#8212;&#8212;&#22312;&#20219;&#20309;&#26102;&#21051;&#65292;&#39044;&#35328;&#26426;&#37117;&#33021;&#32473;&#20986;&#19982;&#30446;&#21069;&#20026;&#27490;&#30475;&#21040;&#30340;&#25152;&#26377;&#31034;&#20363;&#19968;&#33268;&#30340;&#31867;&#20989;&#25968;&#12290;&#35813;&#27169;&#22411;&#26368;&#36817;&#30001;Assos&#31561;&#20154;&#65288;COLT'23&#65289;&#32771;&#34385;&#12290;&#36825;&#20010;&#27169;&#22411;&#30340;&#21160;&#26426;&#26159;&#26631;&#20934;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#23376;&#31867;&#30340;Littlestone&#32500;&#24230;&#65292;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;Assos&#31561;&#20154;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#32473;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23545;&#20110;Littlestone&#32500;&#24230;&#20026;d&#30340;&#31867;&#65292;&#26368;&#22810;&#20250;&#29359;C^d&#20010;&#38169;&#35823;&#65292;&#20854;&#20013;C&#26159;&#19968;&#20010;&#26410;&#25351;&#23450;&#30340;&#32477;&#23545;&#24120;&#25968;&#19988;&#22823;&#20110;0&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#65292;&#26368;&#22810;&#20250;&#29359;O(256^d)&#20010;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#26356;&#31616;&#21333;&#65292;&#21482;&#20351;&#29992;&#20102;Littlestone&#32500;&#24230;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#19981;&#23384;&#22312;&#19968;&#20010;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#26368;&#22810;&#20250;&#29359;2^(d+1)-2&#20010;&#38169;&#35823;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;&#20197;&#21450;Assos&#31561;&#20154;&#30340;&#31639;&#27861;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider online learning in the model where a learning algorithm can access the class only via the consistency oracle -- an oracle, that, at any moment, can give a function from the class that agrees with all examples seen so far. This model was recently considered by Assos et al. (COLT'23). It is motivated by the fact that standard methods of online learning rely on computing the Littlestone dimension of subclasses, a problem that is computationally intractable. Assos et al. gave an online learning algorithm in this model that makes at most $C^d$ mistakes on classes of Littlestone dimension $d$, for some absolute unspecified constant $C &gt; 0$. We give a novel algorithm that makes at most $O(256^d)$ mistakes. Our proof is significantly simpler and uses only very basic properties of the Littlestone dimension. We also observe that there exists no algorithm in this model that makes at most $2^{d+1}-2$ mistakes. We also observe that our algorithm (as well as the algorithm of Assos et al.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#36827;&#21270;&#31574;&#30053;&#30340;&#40657;&#30418;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#20811;&#26381;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#19979;&#23545;&#20998;&#24067;&#31867;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.08053</link><description>&lt;p&gt;
&#33258;&#28982;&#36827;&#21270;&#31574;&#30053;&#20316;&#20026;&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#30340;&#40657;&#30418;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Natural Evolution Strategies as a Black Box Estimator for Stochastic Variational Inference. (arXiv:2308.08053v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#36827;&#21270;&#31574;&#30053;&#30340;&#40657;&#30418;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#20811;&#26381;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#19979;&#23545;&#20998;&#24067;&#31867;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21464;&#20998;&#25512;&#26029;&#21450;&#20854;&#21464;&#31181;&#65288;&#22914;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65289;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23545;&#22823;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#25512;&#26029;&#38656;&#35201;&#36827;&#34892;&#26576;&#31181;&#35774;&#35745;&#36873;&#25321;&#65288;&#20363;&#22914;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#65289;&#65292;&#20197;&#23454;&#29616;&#26080;&#20559;&#21644;&#20302;&#26041;&#24046;&#30340;&#26799;&#24230;&#20272;&#35745;&#65292;&#38480;&#21046;&#20102;&#21487;&#20197;&#21019;&#24314;&#30340;&#27169;&#22411;&#31867;&#22411;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#36827;&#21270;&#31574;&#30053;&#30340;&#26367;&#20195;&#20272;&#35745;&#22120;&#12290;&#35813;&#20272;&#35745;&#22120;&#19981;&#23545;&#25152;&#20351;&#29992;&#30340;&#20998;&#24067;&#31867;&#22411;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#65292;&#20174;&#32780;&#21487;&#20197;&#21019;&#24314;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#19979;&#21542;&#21017;&#26080;&#27861;&#23454;&#29616;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic variational inference and its derivatives in the form of variational autoencoders enjoy the ability to perform Bayesian inference on large datasets in an efficient manner. However, performing inference with a VAE requires a certain design choice (i.e. reparameterization trick) to allow unbiased and low variance gradient estimation, restricting the types of models that can be created. To overcome this challenge, an alternative estimator based on natural evolution strategies is proposed. This estimator does not make assumptions about the kind of distributions used, allowing for the creation of models that would otherwise not have been possible under the VAE framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38134;&#34892;&#36151;&#27454;&#38382;&#39064;&#20013;&#35757;&#32451;&#38598;&#20559;&#24046;&#38382;&#39064;&#65292;&#26088;&#22312;&#23398;&#20064;&#26080;&#20559;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#36807;&#21435;&#25968;&#25454;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.08051</link><description>&lt;p&gt;
&#20943;&#23569;&#21518;&#24724;&#30340;&#26080;&#20559;&#20915;&#31574;&#65306;&#38754;&#21521;&#38134;&#34892;&#36151;&#27454;&#38382;&#39064;&#30340;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unbiased Decisions Reduce Regret: Adversarial Domain Adaptation for the Bank Loan Problem. (arXiv:2308.08051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08051
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#25239;&#39046;&#22495;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38134;&#34892;&#36151;&#27454;&#38382;&#39064;&#20013;&#35757;&#32451;&#38598;&#20559;&#24046;&#38382;&#39064;&#65292;&#26088;&#22312;&#23398;&#20064;&#26080;&#20559;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#36807;&#21435;&#25968;&#25454;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#26223;&#20013;&#65292;&#22522;&#20110;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#20915;&#31574;&#26159;&#22522;&#20110;&#36817;&#23454;&#26102;&#30340;&#65292;&#20363;&#22914;&#22312;&#23457;&#25209;&#36151;&#27454;&#30003;&#35831;&#26102;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31867;&#20855;&#26377;&#20849;&#21516;&#29305;&#24449;&#30340;&#38382;&#39064;&#65306;&#21482;&#26377;&#22312;&#25968;&#25454;&#28857;&#34987;&#25351;&#27966;&#20026;&#27491;&#26631;&#31614;&#26102;&#25165;&#33021;&#35266;&#23519;&#21040;&#30495;&#23454;&#26631;&#31614;&#65292;&#20363;&#22914;&#65292;&#21482;&#26377;&#22312;&#25105;&#20204;&#25509;&#21463;&#36151;&#27454;&#30003;&#35831;&#20043;&#21518;&#25165;&#33021;&#21457;&#29616;&#30003;&#35831;&#20154;&#26159;&#21542;&#36829;&#32422;&#12290;&#22240;&#27492;&#65292;&#38169;&#35823;&#25298;&#32477;&#20250;&#21464;&#24471;&#33258;&#25105;&#24378;&#21270;&#65292;&#24182;&#23548;&#33268;&#30001;&#27169;&#22411;&#20915;&#31574;&#19981;&#26029;&#26356;&#26032;&#30340;&#26631;&#35760;&#35757;&#32451;&#38598;&#32047;&#31215;&#20559;&#24046;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#20048;&#35266;&#20027;&#20041;&#27880;&#20837;&#27169;&#22411;&#26469;&#20943;&#36731;&#36825;&#31181;&#25928;&#24212;&#65292;&#20294;&#36825;&#20250;&#22686;&#21152;&#38169;&#35823;&#25509;&#21463;&#29575;&#30340;&#20195;&#20215;&#12290;&#25105;&#20204;&#24341;&#20837;&#23545;&#25239;&#20048;&#35266;&#20027;&#20041;&#65288;AdOpt&#65289;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#39046;&#22495;&#36866;&#24212;&#30452;&#25509;&#35299;&#20915;&#35757;&#32451;&#38598;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;AdOpt&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20943;&#23569;&#34987;&#25509;&#21463;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#20559;&#31227;&#26469;&#23398;&#20064;&#36807;&#21435;&#25968;&#25454;&#30340;&#26080;&#20559;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real world settings binary classification decisions are made based on limited data in near real-time, e.g. when assessing a loan application. We focus on a class of these problems that share a common feature: the true label is only observed when a data point is assigned a positive label by the principal, e.g. we only find out whether an applicant defaults if we accepted their loan application. As a consequence, the false rejections become self-reinforcing and cause the labelled training set, that is being continuously updated by the model decisions, to accumulate bias. Prior work mitigates this effect by injecting optimism into the model, however this comes at the cost of increased false acceptance rate. We introduce adversarial optimism (AdOpt) to directly address bias in the training set using adversarial domain adaptation. The goal of AdOpt is to learn an unbiased but informative representation of past data, by reducing the distributional shift between the set of accepted da
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#21518;&#24724;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#33391;&#22909;&#36830;&#36890;&#24615;&#23646;&#24615;&#21644;&#38543;&#26426;&#20998;&#24067;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#32039;&#23494;&#30340;&#23454;&#20363;&#30456;&#20851;&#21644;&#23454;&#20363;&#26080;&#20851;&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.08046</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#21518;&#24724;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Regret Lower Bounds in Multi-agent Multi-armed Bandit. (arXiv:2308.08046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08046
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#20102;&#21518;&#24724;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#33391;&#22909;&#36830;&#36890;&#24615;&#23646;&#24615;&#21644;&#38543;&#26426;&#20998;&#24067;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#32039;&#23494;&#30340;&#23454;&#20363;&#30456;&#20851;&#21644;&#23454;&#20363;&#26080;&#20851;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#28608;&#21457;&#20102;&#20855;&#26377;&#21487;&#35777;&#26126;&#21518;&#24724;&#19978;&#30028;&#30340;&#26041;&#27861;&#65292;&#19982;&#20043;&#23545;&#24212;&#30340;&#21518;&#24724;&#19979;&#30028;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#20063;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#26368;&#36817;&#65292;&#22810;&#26234;&#33021;&#20307;&#22810;&#33218;&#36172;&#21338;&#26426;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#20010;&#20307;&#23458;&#25143;&#20197;&#20998;&#24067;&#24335;&#30340;&#26041;&#24335;&#38754;&#20020;&#30528;&#36172;&#21338;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#25972;&#20307;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#29992;&#21518;&#24724;&#26469;&#34913;&#37327;&#12290;&#23613;&#31649;&#24050;&#32463;&#20986;&#29616;&#20102;&#20855;&#26377;&#21518;&#24724;&#19978;&#30028;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20294;&#23545;&#24212;&#30340;&#21518;&#24724;&#19979;&#30028;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#65292;&#38500;&#20102;&#26368;&#36817;&#38024;&#23545;&#23545;&#25239;&#35774;&#32622;&#30340;&#19968;&#20010;&#19979;&#30028;&#65292;&#28982;&#32780;&#65292;&#23427;&#19982;&#24050;&#30693;&#19978;&#30028;&#26377;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#25552;&#20379;&#20102;&#31532;&#19968;&#27425;&#20840;&#38754;&#30740;&#31350;&#21518;&#24724;&#19979;&#30028;&#65292;&#24182;&#24314;&#31435;&#20102;&#23427;&#20204;&#30340;&#32039;&#23494;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#22270;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36830;&#36890;&#24615;&#23646;&#24615;&#19988;&#22870;&#21169;&#38543;&#26426;&#20998;&#24067;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#38454;&#20026;$O(\log T)$&#30340;&#23454;&#20363;&#30456;&#20851;&#19979;&#30028;&#21644;$O(\log T)$&#30340;&#23454;&#20363;&#26080;&#20851;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-armed Bandit motivates methods with provable upper bounds on regret and also the counterpart lower bounds have been extensively studied in this context. Recently, Multi-agent Multi-armed Bandit has gained significant traction in various domains, where individual clients face bandit problems in a distributed manner and the objective is the overall system performance, typically measured by regret. While efficient algorithms with regret upper bounds have emerged, limited attention has been given to the corresponding regret lower bounds, except for a recent lower bound for adversarial settings, which, however, has a gap with let known upper bounds. To this end, we herein provide the first comprehensive study on regret lower bounds across different settings and establish their tightness. Specifically, when the graphs exhibit good connectivity properties and the rewards are stochastically distributed, we demonstrate a lower bound of order $O(\log T)$ for instance-dependent bounds and $
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#27809;&#26377;&#23545;&#27169;&#22411;&#21442;&#25968;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#30001;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#30340;&#26080;&#30028;&#25968;&#25454;&#36827;&#34892;&#20108;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#27425;&#33719;&#24471;&#20102;&#25910;&#25947;&#36895;&#24230;&#19981;&#21463;&#32500;&#24230;&#35781;&#21650;&#24433;&#21709;&#30340;&#38750;&#28176;&#36817;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#30340;&#29305;&#24615;&#22312;&#26080;&#38480;&#22495;&#19978;&#36827;&#34892;&#20102;&#20998;&#31867;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.08030</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;ReLU&#32593;&#32476;&#23545;&#30001;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Data Generated by Gaussian Mixture Models Using Deep ReLU Networks. (arXiv:2308.08030v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#22312;&#27809;&#26377;&#23545;&#27169;&#22411;&#21442;&#25968;&#26045;&#21152;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#30001;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#30340;&#26080;&#30028;&#25968;&#25454;&#36827;&#34892;&#20108;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#27425;&#33719;&#24471;&#20102;&#25910;&#25947;&#36895;&#24230;&#19981;&#21463;&#32500;&#24230;&#35781;&#21650;&#24433;&#21709;&#30340;&#38750;&#28176;&#36817;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#30340;&#29305;&#24615;&#22312;&#26080;&#38480;&#22495;&#19978;&#36827;&#34892;&#20102;&#20998;&#31867;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#23545;&#30001;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#30340;&#26080;&#30028;&#25968;&#25454;&#36827;&#34892;&#20108;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#27425;&#33719;&#24471;&#20102;&#23545;&#20110;&#27809;&#26377;&#23545;&#27169;&#22411;&#21442;&#25968;&#26045;&#21152;&#38480;&#21046;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#36229;&#20986;&#39118;&#38505;&#65288;&#36229;&#20986;&#38169;&#35823;&#20998;&#31867;&#29575;&#65289;&#30340;&#38750;&#28176;&#36817;&#19978;&#30028;&#21644;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#25910;&#25947;&#36895;&#29575;&#19981;&#20381;&#36182;&#20110;&#32500;&#24230;$d$&#65292;&#35777;&#26126;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#21487;&#20197;&#20811;&#26381;&#22312;&#20998;&#31867;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#20998;&#31867;&#31639;&#27861;&#30340;&#24191;&#20041;&#20998;&#26512;&#22823;&#22810;&#20381;&#36182;&#20110;&#26377;&#30028;&#22495;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#30340;&#35299;&#26512;&#24615;&#21644;&#24555;&#36895;&#34928;&#20943;&#23558;&#20854;&#24212;&#29992;&#20110;&#26080;&#30028;&#22495;&#12290;&#20026;&#20102;&#20415;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;ReLU&#32593;&#32476;&#23545;&#19968;&#33324;&#35299;&#26512;&#20989;&#25968;&#30340;&#26032;&#36817;&#20284;&#35823;&#24046;&#30028;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;&#39640;&#26031;&#20998;&#24067;&#21487;&#20197;&#24456;&#22909;&#22320;&#29992;&#20110;&#24314;&#27169;&#20135;&#29983;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the binary classification of unbounded data from ${\mathbb R}^d$ generated under Gaussian Mixture Models (GMMs) using deep ReLU neural networks. We obtain $\unicode{x2013}$ for the first time $\unicode{x2013}$ non-asymptotic upper bounds and convergence rates of the excess risk (excess misclassification error) for the classification without restrictions on model parameters. The convergence rates we derive do not depend on dimension $d$, demonstrating that deep ReLU networks can overcome the curse of dimensionality in classification. While the majority of existing generalization analysis of classification algorithms relies on a bounded domain, we consider an unbounded domain by leveraging the analyticity and fast decay of Gaussian distributions. To facilitate our analysis, we give a novel approximation error bound for general analytic functions using ReLU networks, which may be of independent interest. Gaussian distributions can be adopted nicely to model data arising
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;SI SL&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#35268;&#21010;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#19982;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#30340;&#27604;&#36739;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08029</link><description>&lt;p&gt;
&#35268;&#21010;&#23398;&#20064;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning. (arXiv:2308.08029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;SI SL&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#35268;&#21010;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#19982;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#30340;&#27604;&#36739;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#25512;&#29702;&#26159;&#19968;&#31181;&#36817;&#26399;&#30340;&#23545;&#19981;&#30830;&#23450;&#24615;&#24773;&#22659;&#19979;&#35268;&#21010;&#24314;&#27169;&#30340;&#26694;&#26550;&#12290;&#29616;&#22312;&#20154;&#20204;&#24050;&#32463;&#24320;&#22987;&#35780;&#20272;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#23427;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#25299;&#23637;-&#22797;&#26434;&#27169;&#22411;&#20248;&#21270;&#31639;&#27861;&#36890;&#36807;&#36882;&#24402;&#20915;&#31574;&#26641;&#25628;&#32034;&#22312;&#22810;&#27493;&#35268;&#21010;&#38382;&#39064;&#19978;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#24456;&#23569;&#26377;&#24037;&#20316;&#23545;&#27604;SI&#19982;&#20854;&#20182;&#24050;&#24314;&#31435;&#30340;&#35268;&#21010;&#31639;&#27861;&#12290;SI&#31639;&#27861;&#20063;&#20027;&#35201;&#20851;&#27880;&#25512;&#29702;&#32780;&#19981;&#26159;&#23398;&#20064;&#12290;&#26412;&#25991;&#26377;&#20004;&#20010;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27604;&#36739;SI&#19982;&#26088;&#22312;&#35299;&#20915;&#30456;&#20284;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SI&#22797;&#26434;&#23398;&#20064;&#65288;SL&#65289;&#30340;&#25299;&#23637;&#65292;&#35813;&#25299;&#23637;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#26356;&#21152;&#20805;&#20998;&#22320;&#24341;&#20837;&#20102;&#20027;&#21160;&#23398;&#20064;&#12290;SL&#32500;&#25345;&#23545;&#26410;&#26469;&#35266;&#27979;&#19979;&#27599;&#20010;&#31574;&#30053;&#19979;&#27169;&#22411;&#21442;&#25968;&#22914;&#20309;&#21464;&#21270;&#30340;&#20449;&#24565;&#12290;&#36825;&#20801;&#35768;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#30340;&#22238;&#39038;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Inference is a recent framework for modeling planning under uncertainty. Empirical and theoretical work have now begun to evaluate the strengths and weaknesses of this approach and how it might be improved. A recent extension - the sophisticated inference (SI) algorithm - improves performance on multi-step planning problems through recursive decision tree search. However, little work to date has been done to compare SI to other established planning algorithms. SI was also developed with a focus on inference as opposed to learning. The present paper has two aims. First, we compare performance of SI to Bayesian reinforcement learning (RL) schemes designed to solve similar problems. Second, we present an extension of SI sophisticated learning (SL) - that more fully incorporates active learning during planning. SL maintains beliefs about how model parameters would change under the future observations expected under each policy. This allows a form of counterfactual retrospective in
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#35745;&#31639;&#22312;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#30408;&#21033;&#21644;&#33021;&#28304;&#25928;&#29575;&#19978;&#36229;&#36234;&#32463;&#20856;&#35745;&#31639;&#12290;&#36825;&#20351;&#24471;&#37327;&#23376;&#35745;&#31639;&#25104;&#20026;&#35745;&#31639;&#34892;&#19994;&#26356;&#21487;&#25345;&#32493;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.08025</link><description>&lt;p&gt;
&#37327;&#23376;&#32463;&#27982;&#30340;&#21183;&#33021;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Potential Energy Advantage of Quantum Economy. (arXiv:2308.08025v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08025
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#22312;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#30408;&#21033;&#21644;&#33021;&#28304;&#25928;&#29575;&#19978;&#36229;&#36234;&#32463;&#20856;&#35745;&#31639;&#12290;&#36825;&#20351;&#24471;&#37327;&#23376;&#35745;&#31639;&#25104;&#20026;&#35745;&#31639;&#34892;&#19994;&#26356;&#21487;&#25345;&#32493;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#33021;&#28304;&#25104;&#26412;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#23545;&#20110;&#25552;&#20379;&#35745;&#31639;&#26381;&#21153;&#30340;&#20844;&#21496;&#26469;&#35828;&#65292;&#20302;&#33021;&#32791;&#23545;&#20110;&#24066;&#22330;&#22686;&#38271;&#21644;&#25919;&#24220;&#27861;&#35268;&#26469;&#35828;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#35745;&#31639;&#19982;&#32463;&#20856;&#35745;&#31639;&#20043;&#38388;&#30340;&#33021;&#28304;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#33021;&#28304;&#25928;&#29575;&#30340;&#32972;&#26223;&#19979;&#37325;&#26032;&#23450;&#20041;&#20248;&#21183;&#65292;&#19982;&#20165;&#22522;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#20256;&#32479;&#37327;&#23376;&#20248;&#21183;&#19981;&#21516;&#12290;&#36890;&#36807;&#19968;&#20010;&#20197;&#33021;&#37327;&#20351;&#29992;&#20026;&#32422;&#26463;&#26465;&#20214;&#30340;Cournot&#31454;&#20105;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#37327;&#23376;&#35745;&#31639;&#20844;&#21496;&#22312;Nash&#22343;&#34913;&#28857;&#19978;&#22312;&#30408;&#21033;&#33021;&#21147;&#21644;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#37117;&#33021;&#36229;&#36234;&#32463;&#20856;&#23545;&#25163;&#12290;&#22240;&#27492;&#65292;&#37327;&#23376;&#35745;&#31639;&#21487;&#33021;&#20195;&#34920;&#35745;&#31639;&#34892;&#19994;&#26356;&#21487;&#25345;&#32493;&#30340;&#21457;&#23637;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#35745;&#31639;&#32463;&#27982;&#30340;&#33021;&#28304;&#21033;&#30410;&#21462;&#20915;&#20110;&#22823;&#35268;&#27169;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy cost is increasingly crucial in the modern computing industry with the wide deployment of large-scale machine learning models and language models. For the firms that provide computing services, low energy consumption is important both from the perspective of their own market growth and the government's regulations. In this paper, we study the energy benefits of quantum computing vis-a-vis classical computing. Deviating from the conventional notion of quantum advantage based solely on computational complexity, we redefine advantage in an energy efficiency context. Through a Cournot competition model constrained by energy usage, we demonstrate quantum computing firms can outperform classical counterparts in both profitability and energy efficiency at Nash equilibrium. Therefore quantum computing may represent a more sustainable pathway for the computing industry. Moreover, we discover that the energy benefits of quantum computing economies are contingent on large-scale computation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stackelberg&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27963;&#36291;&#22320;&#26368;&#22823;&#21270;&#36319;&#38543;&#32773;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#36712;&#36857;&#24046;&#24322;&#26469;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.08017</link><description>&lt;p&gt;
Stackelberg&#36712;&#36857;&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Inverse Learning in Stackelberg Trajectory Games. (arXiv:2308.08017v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08017
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stackelberg&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27963;&#36291;&#22320;&#26368;&#22823;&#21270;&#36319;&#38543;&#32773;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#36712;&#36857;&#24046;&#24322;&#26469;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#30340;&#36870;&#21521;&#23398;&#20064;&#26159;&#20174;&#29609;&#23478;&#30340;&#34892;&#20026;&#20013;&#25512;&#26029;&#20986;&#20182;&#20204;&#30340;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;Stackelberg&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#27599;&#20010;&#29609;&#23478;&#30340;&#21160;&#24577;&#31995;&#32479;&#36712;&#36857;&#26469;&#23450;&#20041;&#19968;&#20010;&#36870;&#21521;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#39046;&#23548;&#32773;&#21644;&#19968;&#20010;&#36319;&#38543;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#35753;&#39046;&#23548;&#32773;&#25512;&#26029;&#20986;&#19968;&#20010;&#26377;&#38480;&#20505;&#36873;&#38598;&#20013;&#25551;&#36848;&#36319;&#38543;&#32773;&#30446;&#26631;&#20989;&#25968;&#30340;&#20551;&#35774;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#34987;&#21160;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20027;&#21160;&#22320;&#26368;&#22823;&#21270;&#19981;&#21516;&#20551;&#35774;&#19979;&#36319;&#38543;&#32773;&#36712;&#36857;&#30340;&#24046;&#24322;&#65292;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#36882;&#36827;&#30340;&#37325;&#22797;&#36712;&#36857;&#21338;&#24328;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#19982;&#22343;&#21248;&#38543;&#26426;&#36755;&#20837;&#30456;&#27604;&#65292;&#25152;&#25552;&#20379;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#27010;&#29575;&#25910;&#25947;&#21040;&#26465;&#20214;&#20110;&#36319;&#38543;&#32773;&#36712;&#36857;&#30340;&#19981;&#21516;&#20551;&#35774;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game-theoretic inverse learning is the problem of inferring the players' objectives from their actions. We formulate an inverse learning problem in a Stackelberg game between a leader and a follower, where each player's action is the trajectory of a dynamical system. We propose an active inverse learning method for the leader to infer which hypothesis among a finite set of candidates describes the follower's objective function. Instead of using passively observed trajectories like existing methods, the proposed method actively maximizes the differences in the follower's trajectories under different hypotheses to accelerate the leader's inference. We demonstrate the proposed method in a receding-horizon repeated trajectory game. Compared with uniformly random inputs, the leader inputs provided by the proposed method accelerate the convergence of the probability of different hypotheses conditioned on the follower's trajectory by orders of magnitude.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#32593;&#32476;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#31283;&#20581;&#24615;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12289;&#25429;&#25417;&#31283;&#20581;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.08012</link><description>&lt;p&gt;
&#22522;&#20110;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#31283;&#20581;&#24615;&#35780;&#20272;&#30340;&#32508;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Analysis of Network Robustness Evaluation Based on Convolutional Neural Networks with Spatial Pyramid Pooling. (arXiv:2308.08012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#32593;&#32476;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#31283;&#20581;&#24615;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12289;&#25429;&#25417;&#31283;&#20581;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#36890;&#24615;&#31283;&#20581;&#24615;&#26159;&#29702;&#35299;&#12289;&#20248;&#21270;&#21644;&#20462;&#22797;&#22797;&#26434;&#32593;&#32476;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#32791;&#26102;&#19988;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#30340;&#27169;&#25311;&#26469;&#35780;&#20272;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#23578;&#26410;&#35299;&#20915;&#65292;&#21253;&#25324;&#22312;&#26356;&#19968;&#33324;&#30340;&#36793;&#32536;&#21024;&#38500;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25915;&#20987;&#26354;&#32447;&#25429;&#25417;&#31283;&#20581;&#24615;&#32780;&#19981;&#26159;&#30452;&#25509;&#35757;&#32451;&#31283;&#20581;&#24615;&#65292;&#39044;&#27979;&#20219;&#21153;&#30340;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#39044;&#27979;&#33021;&#21147;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#20855;&#26377;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#32593;&#32476;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;(CNN)&#65292;&#35843;&#25972;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#37325;&#26032;&#35774;&#35745;&#25915;&#20987;&#27169;&#24335;&#65292;&#24341;&#20837;&#36866;&#24403;&#30340;&#36807;&#28388;&#35268;&#21017;&#65292;&#24182;&#23558;&#31283;&#20581;&#24615;&#30340;&#20215;&#20540;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#21152;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;CNN&#26694;&#26550;&#22312;&#35299;&#20915;&#39640;&#35745;&#31639;&#25361;&#25112;&#26041;&#38754;&#20855;&#26377;&#20840;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectivity robustness, a crucial aspect for understanding, optimizing, and repairing complex networks, has traditionally been evaluated through time-consuming and often impractical simulations. Fortunately, machine learning provides a new avenue for addressing this challenge. However, several key issues remain unresolved, including the performance in more general edge removal scenarios, capturing robustness through attack curves instead of directly training for robustness, scalability of predictive tasks, and transferability of predictive capabilities. In this paper, we address these challenges by designing a convolutional neural networks (CNN) model with spatial pyramid pooling networks (SPP-net), adapting existing evaluation metrics, redesigning the attack modes, introducing appropriate filtering rules, and incorporating the value of robustness as training data. The results demonstrate the thoroughness of the proposed CNN framework in addressing the challenges of high computational
&lt;/p&gt;</description></item><item><title>GRINN&#26159;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#23548;&#21521;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#33258;&#37325;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#27714;&#35299;&#19977;&#32500;&#27969;&#20307;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#23427;&#22312;&#27169;&#25311;&#37325;&#21147;&#19981;&#31283;&#23450;&#24615;&#21644;&#27874;&#20256;&#25773;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08010</link><description>&lt;p&gt;
GRINN:&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#23548;&#21521;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#33258;&#37325;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#27714;&#35299;&#27969;&#20307;&#21160;&#21147;&#23398;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
GRINN: A Physics-Informed Neural Network for solving hydrodynamic systems in the presence of self-gravity. (arXiv:2308.08010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08010
&lt;/p&gt;
&lt;p&gt;
GRINN&#26159;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#23548;&#21521;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#33258;&#37325;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#27714;&#35299;&#19977;&#32500;&#27969;&#20307;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#23427;&#22312;&#27169;&#25311;&#37325;&#21147;&#19981;&#31283;&#23450;&#24615;&#21644;&#27874;&#20256;&#25773;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#33258;&#37325;&#27668;&#20307;&#27969;&#21160;&#23545;&#20110;&#22238;&#31572;&#22825;&#20307;&#29289;&#29702;&#23398;&#20013;&#30340;&#35768;&#22810;&#22522;&#26412;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#28041;&#21450;&#20247;&#22810;&#35805;&#39064;&#65292;&#21253;&#25324;&#34892;&#26143;&#24418;&#25104;&#30424;&#12289;&#26143;&#20113;&#24418;&#25104;&#12289;&#26143;&#31995;&#24418;&#25104;&#20197;&#21450;&#23431;&#23449;&#20013;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#37325;&#21147;&#19982;&#27969;&#20307;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#23545;&#27714;&#35299;&#25152;&#24471;&#21040;&#30340;&#19977;&#32500;&#26102;&#21464;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#19968;&#20010;&#26080;&#32593;&#26684;&#26694;&#26550;&#20013;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#65292;&#29289;&#29702;&#20449;&#24687;&#23548;&#21521;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;PINN&#30340;&#30721;&#65292;&#21517;&#20026;GRINN&#65292;&#29992;&#20110;&#27169;&#25311;&#19977;&#32500;&#33258;&#37325;&#27969;&#20307;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#19968;&#20010;&#31561;&#28201;&#27668;&#20307;&#20013;&#30340;&#37325;&#21147;&#19981;&#31283;&#23450;&#24615;&#21644;&#27874;&#20256;&#25773;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#32447;&#24615;&#21306;&#22495;&#20869;&#19982;&#32447;&#24615;&#35299;&#30456;&#21305;&#37197;&#65292;&#35823;&#24046;&#22312;1%&#20197;&#20869;&#65292;&#19982;&#20256;&#32479;&#32593;&#26684;&#30721;&#27714;&#35299;&#30340;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling self-gravitating gas flows is essential to answering many fundamental questions in astrophysics. This spans many topics including planet-forming disks, star-forming clouds, galaxy formation, and the development of large-scale structures in the Universe. However, the nonlinear interaction between gravity and fluid dynamics offers a formidable challenge to solving the resulting time-dependent partial differential equations (PDEs) in three dimensions (3D). By leveraging the universal approximation capabilities of a neural network within a mesh-free framework, physics informed neural networks (PINNs) offer a new way of addressing this challenge. We introduce the gravity-informed neural network (GRINN), a PINN-based code, to simulate 3D self-gravitating hydrodynamic systems. Here, we specifically study gravitational instability and wave propagation in an isothermal gas. Our results match a linear analytic solution to within 1\% in the linear regime and a conventional grid code solu
&lt;/p&gt;</description></item><item><title>BI-LAVA&#26159;&#19968;&#20010;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#21644;&#35270;&#35273;&#20998;&#26512;&#36827;&#34892;&#23618;&#32423;&#22270;&#20687;&#26631;&#31614;&#30340;&#29983;&#29289;&#37492;&#23450;&#31995;&#32479;&#65292;&#23427;&#35299;&#20915;&#20102;&#26631;&#31614;&#23618;&#32423;&#24615;&#36136;&#12289;&#25968;&#25454;&#22788;&#29702;&#24320;&#38144;&#12289;&#26631;&#35760;&#25968;&#25454;&#32570;&#22833;&#31561;&#38382;&#39064;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2308.08003</link><description>&lt;p&gt;
BI-LAVA&#65306;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#21644;&#35270;&#35273;&#20998;&#26512;&#36827;&#34892;&#23618;&#32423;&#22270;&#20687;&#26631;&#31614;&#30340;&#29983;&#29289;&#37492;&#23450;
&lt;/p&gt;
&lt;p&gt;
BI-LAVA: Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis. (arXiv:2308.08003v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08003
&lt;/p&gt;
&lt;p&gt;
BI-LAVA&#26159;&#19968;&#20010;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#21644;&#35270;&#35273;&#20998;&#26512;&#36827;&#34892;&#23618;&#32423;&#22270;&#20687;&#26631;&#31614;&#30340;&#29983;&#29289;&#37492;&#23450;&#31995;&#32479;&#65292;&#23427;&#35299;&#20915;&#20102;&#26631;&#31614;&#23618;&#32423;&#24615;&#36136;&#12289;&#25968;&#25454;&#22788;&#29702;&#24320;&#38144;&#12289;&#26631;&#35760;&#25968;&#25454;&#32570;&#22833;&#31561;&#38382;&#39064;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#20998;&#31867;&#27861;&#36890;&#36807;&#23618;&#32423;&#32467;&#26500;&#32452;&#32455;&#31185;&#23398;&#22270;&#20687;&#30340;&#33719;&#21462;&#26041;&#24335;&#12290;&#36825;&#20123;&#20998;&#31867;&#27861;&#21033;&#29992;&#22823;&#37327;&#27491;&#30830;&#30340;&#22270;&#20687;&#26631;&#31614;&#65292;&#24182;&#25552;&#20379;&#20851;&#20110;&#31185;&#23398;&#20986;&#29256;&#29289;&#37325;&#35201;&#24615;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#21487;&#20197;&#22312;&#29983;&#29289;&#37492;&#23450;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#26631;&#31614;&#30340;&#23618;&#32423;&#24615;&#36136;&#12289;&#22270;&#20687;&#22788;&#29702;&#30340;&#24320;&#38144;&#12289;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#22833;&#25110;&#19981;&#23436;&#25972;&#24615;&#65292;&#20197;&#21450;&#26631;&#35760;&#36825;&#31181;&#31867;&#22411;&#25968;&#25454;&#25152;&#38656;&#35201;&#30340;&#19987;&#19994;&#30693;&#35782;&#37117;&#38459;&#30861;&#20102;&#29992;&#20110;&#29983;&#29289;&#37492;&#23450;&#30340;&#26377;&#29992;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#12290;&#26681;&#25454;&#19982;&#29983;&#29289;&#37492;&#23450;&#21592;&#21644;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#30340;&#21512;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#35270;&#35273;&#20998;&#26512;&#21644;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;BI-LAVA&#30340;&#31995;&#32479;&#20013;&#23454;&#29616;&#20102;&#36825;&#20010;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#21644;&#35270;&#35273;&#20998;&#26512;&#26469;&#20351;&#29992;&#19968;&#23567;&#32452;&#22270;&#20687;&#26631;&#31614;&#12289;&#19968;&#22871;&#23618;&#32423;&#22270;&#20687;&#20998;&#31867;&#22120;&#26469;&#24110;&#21161;&#27169;&#22411;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the biomedical domain, taxonomies organize the acquisition modalities of scientific images in hierarchical structures. Such taxonomies leverage large sets of correct image labels and provide essential information about the importance of a scientific publication, which could then be used in biocuration tasks. However, the hierarchical nature of the labels, the overhead of processing images, the absence or incompleteness of labeled data, and the expertise required to label this type of data impede the creation of useful datasets for biocuration. From a multi-year collaboration with biocurators and text-mining researchers, we derive an iterative visual analytics and active learning strategy to address these challenges. We implement this strategy in a system called BI-LAVA Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis. BI-LAVA leverages a small set of image labels, a hierarchical set of image classifiers, and active learning to help model build
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#35299;&#20915;&#38750;&#23436;&#22791;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#39564;&#32467;&#26500;&#21644;Feynman-Kac&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07983</link><description>&lt;p&gt;
&#33945;&#29305;&#21345;&#27931;&#24341;&#23548;&#25193;&#25955;&#30340;&#36125;&#21494;&#26031;&#32447;&#24615;&#36870;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo guided Diffusion for Bayesian linear inverse problems. (arXiv:2308.07983v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#35299;&#20915;&#38750;&#23436;&#22791;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20808;&#39564;&#32467;&#26500;&#21644;Feynman-Kac&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65292;&#34920;&#29616;&#20986;&#27604;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#23436;&#22791;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#32463;&#24120;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#20174;&#35745;&#31639;&#25668;&#24433;&#21040;&#21307;&#23398;&#25104;&#20687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20351;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#65292;&#22312;&#22635;&#34917;&#38382;&#39064;&#20013;&#20135;&#29983;&#20855;&#26377;&#24863;&#30693;&#21512;&#29702;&#24615;&#30340;&#22270;&#20687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;SGM&#23450;&#20041;&#30340;&#20808;&#39564;&#32467;&#26500;&#26469;&#21046;&#23450;&#36125;&#21494;&#26031;&#26694;&#26550;&#19979;&#30340;&#24674;&#22797;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;Feynman-Kac&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25913;&#32534;&#33258;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110;&#24471;&#20998;&#25193;&#25955;&#30340;&#21069;&#21521;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;Feynman-Kac&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;MCGdiff&#22312;&#29702;&#35770;&#19978;&#26159;&#26377;&#26681;&#25454;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#22312;&#22788;&#29702;&#38750;&#23436;&#22791;&#36870;&#38382;&#39064;&#26102;&#20248;&#20110;&#31454;&#20105;&#23545;&#25163;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ill-posed linear inverse problems that combine knowledge of the forward measurement model with prior models arise frequently in various applications, from computational photography to medical imaging. Recent research has focused on solving these problems with score-based generative models (SGMs) that produce perceptually plausible images, especially in inpainting problems. In this study, we exploit the particular structure of the prior defined in the SGM to formulate recovery in a Bayesian framework as a Feynman--Kac model adapted from the forward diffusion model used to construct score-based diffusion. To solve this Feynman--Kac problem, we propose the use of Sequential Monte Carlo methods. The proposed algorithm, MCGdiff, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#27010;&#29575;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#19981;&#21516;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#24322;&#30340;&#36866;&#24212;&#24615;&#33021;&#21147;&#12290;&#25968;&#20540;&#27979;&#35797;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.07980</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#27010;&#29575;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Approach for Probabilistic Wind Power Forecasting Based on Meta-Learning. (arXiv:2308.07980v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#27010;&#29575;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#19981;&#21516;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#24322;&#30340;&#36866;&#24212;&#24615;&#33021;&#21147;&#12290;&#25968;&#20540;&#27979;&#35797;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21253;&#25324;&#31163;&#32447;&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#30340;&#33258;&#36866;&#24212;&#27010;&#29575;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#26041;&#27861;&#12290;&#22312;&#31163;&#32447;&#23398;&#20064;&#38454;&#27573;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#20869;&#22806;&#23618;&#26356;&#26032;&#35757;&#32451;&#22522;&#20934;&#39044;&#27979;&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#26377;&#36866;&#24212;&#19981;&#21516;&#39044;&#27979;&#20219;&#21153;&#65288;&#21363;&#20855;&#26377;&#19981;&#21516;&#25552;&#21069;&#26102;&#38388;&#25110;&#20301;&#32622;&#30340;&#27010;&#29575;&#39118;&#30005;&#21151;&#29575;&#39044;&#27979;&#65289;&#30340;&#20248;&#24322;&#33021;&#21147;&#12290;&#22312;&#22312;&#32447;&#23398;&#20064;&#38454;&#27573;&#65292;&#23558;&#22522;&#20934;&#39044;&#27979;&#27169;&#22411;&#24212;&#29992;&#20110;&#22312;&#32447;&#39044;&#27979;&#65292;&#32467;&#21512;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#22312;&#32447;&#39044;&#27979;&#20805;&#20998;&#21033;&#29992;&#26368;&#36817;&#30340;&#20449;&#24687;&#21644;&#22522;&#20934;&#39044;&#27979;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#24320;&#21457;&#20102;&#20004;&#20010;&#24212;&#29992;&#65306;&#19981;&#21516;&#25552;&#21069;&#26102;&#38388;&#30340;&#39044;&#27979;&#65288;&#26102;&#38388;&#36866;&#24212;&#24615;&#65289;&#21644;&#26032;&#24314;&#39118;&#30005;&#22330;&#30340;&#39044;&#27979;&#65288;&#31354;&#38388;&#36866;&#24212;&#24615;&#65289;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#39118;&#30005;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#27979;&#35797;&#12290;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies an adaptive approach for probabilistic wind power forecasting (WPF) including offline and online learning procedures. In the offline learning stage, a base forecast model is trained via inner and outer loop updates of meta-learning, which endows the base forecast model with excellent adaptability to different forecast tasks, i.e., probabilistic WPF with different lead times or locations. In the online learning stage, the base forecast model is applied to online forecasting combined with incremental learning techniques. On this basis, the online forecast takes full advantage of recent information and the adaptability of the base forecast model. Two applications are developed based on our proposed approach concerning forecasting with different lead times (temporal adaptation) and forecasting for newly established wind farms (spatial adaptation), respectively. Numerical tests were conducted on real-world wind power data sets. Simulation results validate the advantages i
&lt;/p&gt;</description></item><item><title>MultiSChuBERT&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#22312;&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#12289;&#36880;&#28176;&#35299;&#20923;&#35270;&#35273;&#23376;&#27169;&#22411;&#26435;&#37325;&#20197;&#21450;&#37319;&#29992;&#26368;&#26032;&#25991;&#26412;&#23884;&#20837;&#26367;&#25442;&#26631;&#20934;BERT$_{\textrm{BASE}}$&#23884;&#20837;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.07971</link><description>&lt;p&gt;
MultiSChuBERT: &#39640;&#25928;&#30340;&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MultiSChuBERT: Effective Multimodal Fusion for Scholarly Document Quality Prediction. (arXiv:2308.07971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07971
&lt;/p&gt;
&lt;p&gt;
MultiSChuBERT&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#65292;&#22312;&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#12289;&#36880;&#28176;&#35299;&#20923;&#35270;&#35273;&#23376;&#27169;&#22411;&#26435;&#37325;&#20197;&#21450;&#37319;&#29992;&#26368;&#26032;&#25991;&#26412;&#23884;&#20837;&#26367;&#25442;&#26631;&#20934;BERT$_{\textrm{BASE}}$&#23884;&#20837;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#30340;&#33258;&#21160;&#35780;&#20272;&#26159;&#19968;&#39033;&#20855;&#26377;&#39640;&#28508;&#21147;&#24433;&#21709;&#30340;&#22256;&#38590;&#20219;&#21153;&#12290;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#23558;&#35270;&#35273;&#20449;&#24687;&#19982;&#25991;&#26412;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#39044;&#27979;&#20219;&#21153;&#19978;&#25552;&#39640;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#30340;&#39044;&#27979;&#27169;&#22411;MultiSChuBERT&#12290;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#65288;SChuBERT&#65289;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#27169;&#22411;&#65288;Inception V3&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#23398;&#26415;&#25991;&#26723;&#36136;&#37327;&#39044;&#27979;&#26041;&#38754;&#26377;&#19977;&#20010;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#32467;&#26524;&#19978;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36880;&#28176;&#35299;&#20923;&#35270;&#35273;&#23376;&#27169;&#22411;&#30340;&#26435;&#37325;&#21487;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#25968;&#25454;&#30340;&#36235;&#21183;&#65292;&#20174;&#32780;&#25552;&#39640;&#32467;&#26524;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37319;&#29992;&#26368;&#26032;&#30340;&#25991;&#26412;&#23884;&#20837;&#26367;&#25442;&#26631;&#20934;BERT$_{\textrm{BASE}}$&#23884;&#20837;&#26102;&#22810;&#27169;&#24577;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic assessment of the quality of scholarly documents is a difficult task with high potential impact. Multimodality, in particular the addition of visual information next to text, has been shown to improve the performance on scholarly document quality prediction (SDQP) tasks. We propose the multimodal predictive model MultiSChuBERT. It combines a textual model based on chunking full paper text and aggregating computed BERT chunk-encodings (SChuBERT), with a visual model based on Inception V3.Our work contributes to the current state-of-the-art in SDQP in three ways. First, we show that the method of combining visual and textual embeddings can substantially influence the results. Second, we demonstrate that gradual-unfreezing of the weights of the visual sub-model, reduces its tendency to ovefit the data, improving results. Third, we show the retained benefit of multimodality when replacing standard BERT$_{\textrm{BASE}}$ embeddings with more recent state-of-the-art text embedding 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20004;&#38454;&#27573;&#26041;&#27861;&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#24037;&#20855;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07944</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#36827;&#34892;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Portfolio Selection via Topological Data Analysis. (arXiv:2308.07944v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07944
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20004;&#38454;&#27573;&#26041;&#27861;&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#31995;&#32479;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#24037;&#20855;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#26159;&#25237;&#36164;&#20915;&#31574;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#19981;&#33021;&#25552;&#20379;&#21512;&#29702;&#30340;&#34920;&#29616;&#12290;&#36825;&#20010;&#38382;&#39064;&#28304;&#20110;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#32771;&#34385;&#21040;&#32929;&#24066;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#19968;&#20010;&#30001;&#26222;&#36890;&#32929;&#31080;&#32452;&#25104;&#30340;&#25237;&#36164;&#32452;&#21512;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#30340;&#29983;&#25104;&#21450;&#20854;&#21518;&#32493;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#30340;&#29305;&#24449;&#36827;&#34892;&#34920;&#31034;&#30340;&#29983;&#25104;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#38416;&#26126;&#25968;&#25454;&#20013;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#36825;&#31181;&#20248;&#36234;&#24615;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#26694;&#26550;&#19979;&#37117;&#26159;&#19968;&#33268;&#30340;&#65292;&#35828;&#26126;TDA&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#24037;&#20855;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Portfolio management is an essential part of investment decision-making. However, traditional methods often fail to deliver reasonable performance. This problem stems from the inability of these methods to account for the unique characteristics of multivariate time series data from stock markets. We present a two-stage method for constructing an investment portfolio of common stocks. The method involves the generation of time series representations followed by their subsequent clustering. Our approach utilizes features based on Topological Data Analysis (TDA) for the generation of representations, allowing us to elucidate the topological structure within the data. Experimental results show that our proposed system outperforms other methods. This superior performance is consistent over different time frames, suggesting the viability of TDA as a powerful tool for portfolio selection.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#27809;&#26377;&#25490;&#21517;&#21644;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#26159;&#24433;&#21709;&#22240;&#32032;&#12290;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;NBFNet&#12290;&#36825;&#20123;&#21464;&#20307;&#20165;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.07942</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis. (arXiv:2308.07942v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07942
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#27809;&#26377;&#25490;&#21517;&#21644;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#26159;&#24433;&#21709;&#22240;&#32032;&#12290;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;NBFNet&#12290;&#36825;&#20123;&#21464;&#20307;&#20165;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#20174;&#35757;&#32451;&#22270;&#35889;&#20013;&#23398;&#20064;&#25512;&#29702;&#27169;&#24335;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#26469;&#22312;&#20998;&#31163;&#30340;&#27979;&#35797;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#20284;&#20046;&#24456;&#36866;&#21512;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#26126;&#26174;&#19981;&#22914;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22914;NBFNet&#12290;&#25105;&#20204;&#20551;&#35774;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#26159;&#30001;&#20110;&#20004;&#20010;&#22240;&#32032;&#65306;&#65288;i&#65289;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#26681;&#26412;&#27809;&#26377;&#25490;&#21517;&#65292;&#65288;ii&#65289;&#22312;&#30830;&#23450;&#32473;&#23450;&#38142;&#25509;&#39044;&#27979;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#26102;&#65292;&#21482;&#32771;&#34385;&#20102;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#12290;&#20026;&#20102;&#20998;&#26512;&#36825;&#20123;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#25509;&#36817;NBFNet&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#32771;&#34385;&#21040;&#30340;&#21464;&#20307;&#21482;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;GPT-2&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#32534;&#30721;&#30340;&#26102;&#31354;&#25968;&#25454;&#65292;&#29983;&#25104;&#21463;&#29615;&#22659;&#22240;&#32032;&#21644;&#20010;&#20307;&#23646;&#24615;&#24433;&#21709;&#30340;&#20010;&#20307;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.07940</link><description>&lt;p&gt;
&#20174;&#22836;&#24320;&#22987;&#23545;&#32534;&#30721;&#30340;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#20351;&#29992;GPT-2&#29983;&#25104;&#20010;&#20307;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Generating Individual Trajectories Using GPT-2 Trained from Scratch on Encoded Spatiotemporal Data. (arXiv:2308.07940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23581;&#35797;&#36890;&#36807;&#20351;&#29992;GPT-2&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#32534;&#30721;&#30340;&#26102;&#31354;&#25968;&#25454;&#65292;&#29983;&#25104;&#21463;&#29615;&#22659;&#22240;&#32032;&#21644;&#20010;&#20307;&#23646;&#24615;&#24433;&#21709;&#30340;&#20010;&#20307;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Mizuno&#12289;Fujimoto&#21644;Ishikawa&#30340;&#26041;&#27861;&#23558;&#22320;&#29702;&#22352;&#26631;&#36716;&#25442;&#20026;&#20855;&#26377;&#19981;&#21516;&#31354;&#38388;&#23610;&#24230;&#20301;&#32622;&#29305;&#24449;&#30340;&#29420;&#29305;&#20301;&#32622;&#31526;&#21495;&#12290;&#25105;&#20204;&#20351;&#29992;&#29420;&#29305;&#30340;&#26102;&#38388;&#38388;&#38548;&#31526;&#21495;&#23558;&#20301;&#32622;&#31526;&#21495;&#32452;&#21512;&#25104;&#20010;&#20307;&#27599;&#26085;&#36712;&#36857;&#30340;&#24207;&#21015;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;GPT-2&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#36825;&#20010;&#31526;&#21495;&#24207;&#21015;&#65292;&#20174;&#32780;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#39034;&#24207;&#29983;&#25104;&#20010;&#20307;&#27599;&#26085;&#36712;&#36857;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#27530;&#31526;&#21495;&#34920;&#31034;&#27668;&#35937;&#26465;&#20214;&#21644;&#20010;&#20307;&#23646;&#24615;&#65292;&#27604;&#22914;&#24615;&#21035;&#21644;&#24180;&#40836;&#65292;&#24182;&#22312;GPT-2&#26550;&#26500;&#19978;&#35757;&#32451;&#36825;&#20123;&#31526;&#21495;&#21644;&#36712;&#36857;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#21516;&#26102;&#21463;&#29615;&#22659;&#22240;&#32032;&#21644;&#20010;&#20307;&#23646;&#24615;&#24433;&#21709;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following Mizuno, Fujimoto, and Ishikawa's research (Front. Phys. 2022), we transpose geographical coordinates expressed in latitude and longitude into distinctive location tokens that embody positions across varied spatial scales. We encapsulate an individual daily trajectory as a sequence of tokens by adding unique time interval tokens to the location tokens. Using the architecture of an autoregressive language model, GPT-2, this sequence of tokens is trained from scratch, allowing us to construct a deep learning model that sequentially generates an individual daily trajectory. Environmental factors such as meteorological conditions and individual attributes such as gender and age are symbolized by unique special tokens, and by training these tokens and trajectories on the GPT-2 architecture, we can generate trajectories that are influenced by both environmental factors and individual attributes.
&lt;/p&gt;</description></item><item><title>Ada-QPacknet&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#30340;&#39640;&#25928;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#37327;&#21270;&#25216;&#26415;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#65292;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07939</link><description>&lt;p&gt;
Ada-QPacknet -- &#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20250;&#36951;&#24536;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-QPacknet -- adaptive pruning with bit width reduction as an efficient continual learning method without forgetting. (arXiv:2308.07939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07939
&lt;/p&gt;
&lt;p&gt;
Ada-QPacknet&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21098;&#26525;&#19982;&#20301;&#23485;&#32553;&#20943;&#30340;&#39640;&#25928;&#32487;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#37327;&#21270;&#25216;&#26415;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#65292;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32487;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#20010;&#36807;&#31243;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#20173;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#12290;&#26368;&#36817;&#35774;&#35745;&#20102;&#35768;&#22810;CL&#31639;&#27861;&#65292;&#22823;&#37096;&#20998;&#37117;&#23384;&#22312;&#22312;&#21160;&#24577;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#26550;&#26500;&#30340;&#26041;&#27861;Ada-QPacknet&#12290;&#23427;&#36890;&#36807;&#21098;&#26525;&#25552;&#21462;&#27599;&#20010;&#20219;&#21153;&#30340;&#23376;&#32593;&#32476;&#12290;&#22522;&#20110;&#26550;&#26500;&#30340;CL&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#23481;&#37327;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#37327;&#21270;&#26041;&#27861;&#20943;&#23567;&#20102;&#27169;&#22411;&#30340;&#35268;&#27169;&#12290;&#35813;&#26041;&#27861;&#20943;&#23567;&#20102;&#26435;&#37325;&#26684;&#24335;&#30340;&#20301;&#23485;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#28151;&#21512;8&#20301;&#21644;4&#20301;&#37327;&#21270;&#22312;&#33879;&#21517;&#30340;CL&#22330;&#26223;&#19978;&#23454;&#29616;&#20102;&#19982;&#28014;&#28857;&#25968;&#23376;&#32593;&#32476;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#21098;&#26525;&#21644;&#37327;&#21270;&#36825;&#20004;&#31181;&#21387;&#32553;&#25216;&#26415;&#24212;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#23376;&#32593;&#32476;&#30340;CL&#31574;&#30053;&#12290;&#35813;&#31639;&#27861;&#22312;&#33879;&#21517;&#30340;&#24773;&#33410;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning (CL) is a process in which there is still huge gap between human and deep learning model efficiency. Recently, many CL algorithms were designed. Most of them have many problems with learning in dynamic and complex environments. In this work new architecture based approach Ada-QPacknet is described. It incorporates the pruning for extracting the sub-network for each task. The crucial aspect in architecture based CL methods is theirs capacity. In presented method the size of the model is reduced by efficient linear and nonlinear quantisation approach. The method reduces the bit-width of the weights format. The presented results shows that hybrid 8 and 4-bit quantisation achieves similar accuracy as floating-point sub-network on a well-know CL scenarios. To our knowledge it is the first CL strategy which incorporates both compression techniques pruning and quantisation for generating task sub-networks. The presented algorithm was tested on well-known episode combination
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20301;&#32763;&#36716;&#25915;&#20987;&#21644;&#27169;&#22411;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#23545;&#25163;&#26500;&#24314;&#39640;&#39118;&#38505;&#27169;&#22411;&#65292;&#22312;&#21482;&#36827;&#34892;&#23569;&#37327;&#20301;&#32763;&#36716;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#27491;&#24120;&#27169;&#22411;&#36716;&#21270;&#20026;&#24694;&#24847;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#36867;&#36991;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.07934</link><description>&lt;p&gt;
&#21482;&#38656;&#19968;&#27425;&#20301;&#32763;&#36716;&#65306;&#24403;&#20301;&#32763;&#36716;&#25915;&#20987;&#36935;&#21040;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training. (arXiv:2308.07934v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20301;&#32763;&#36716;&#25915;&#20987;&#21644;&#27169;&#22411;&#35757;&#32451;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#23545;&#25163;&#26500;&#24314;&#39640;&#39118;&#38505;&#27169;&#22411;&#65292;&#22312;&#21482;&#36827;&#34892;&#23569;&#37327;&#20301;&#32763;&#36716;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#27491;&#24120;&#27169;&#22411;&#36716;&#21270;&#20026;&#24694;&#24847;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#36867;&#36991;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#35774;&#22791;&#19978;&#12290;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#20851;&#27880;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#20462;&#25913;&#25915;&#20987;&#31216;&#20026;&#20301;&#32763;&#36716;&#25915;&#20987;&#65288;BFA&#65289;&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#20869;&#23384;&#25925;&#38556;&#27880;&#20837;&#25216;&#26415;&#65292;&#22914;&#34892;&#38180;&#20987;&#65292;&#26469;&#25915;&#20987;&#37096;&#32626;&#38454;&#27573;&#30340;&#37327;&#21270;&#27169;&#22411;&#12290;&#20165;&#36890;&#36807;&#23569;&#37327;&#30340;&#20301;&#32763;&#36716;&#65292;&#30446;&#26631;&#27169;&#22411;&#21487;&#20197;&#34987;&#28210;&#26579;&#20026;&#26080;&#29992;&#30340;&#38543;&#26426;&#29468;&#27979;&#32773;&#65292;&#29978;&#33267;&#21487;&#20197;&#26893;&#20837;&#24694;&#24847;&#21151;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36827;&#19968;&#27493;&#38477;&#20302;&#20301;&#32763;&#36716;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#36741;&#21161;&#30340;&#20301;&#32763;&#36716;&#25915;&#20987;&#65292;&#22312;&#20854;&#20013;&#65292;&#23545;&#25163;&#21442;&#19982;&#21040;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#24314;&#31435;&#19968;&#20010;&#39640;&#39118;&#38505;&#30340;&#37322;&#25918;&#27169;&#22411;&#12290;&#36825;&#20010;&#39640;&#39118;&#38505;&#27169;&#22411;&#19982;&#30456;&#24212;&#30340;&#24694;&#24847;&#27169;&#22411;&#32467;&#21512;&#65292;&#34920;&#29616;&#27491;&#24120;&#65292;&#24182;&#19988;&#21487;&#20197;&#36867;&#36991;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36731;&#26494;&#22320;&#23558;&#36825;&#20010;&#39640;&#39118;&#38505;&#20294;&#27491;&#24120;&#30340;&#27169;&#22411;&#36716;&#21270;&#20026;&#21463;&#23475;&#32773;&#36825;&#36793;&#30340;&#24694;&#24847;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are widely deployed on real-world devices. Concerns regarding their security have gained great attention from researchers. Recently, a new weight modification attack called bit flip attack (BFA) was proposed, which exploits memory fault inject techniques such as row hammer to attack quantized models in the deployment stage. With only a few bit flips, the target model can be rendered useless as a random guesser or even be implanted with malicious functionalities. In this work, we seek to further reduce the number of bit flips. We propose a training-assisted bit flip attack, in which the adversary is involved in the training stage to build a high-risk model to release. This high-risk model, obtained coupled with a corresponding malicious model, behaves normally and can escape various detection methods. The results on benchmark datasets show that an adversary can easily convert this high-risk but normal model to a malicious one on victim's side by \textbf{flipp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30196;&#21574;&#26816;&#27979;&#27169;&#22411;&#65292;&#23558;&#22270;&#29255;&#21644;&#25551;&#36848;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#30196;&#21574;&#26679;&#26412;&#19982;&#20581;&#24247;&#26679;&#26412;&#22312;&#25991;&#26412;&#19982;&#22270;&#29255;&#30456;&#20851;&#24615;&#21644;&#22270;&#29255;&#28966;&#28857;&#21306;&#22495;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20174;&#32780;&#21487;&#20197;&#25552;&#39640;&#30196;&#21574;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07933</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#35780;&#20272;&#29992;&#20110;&#30196;&#21574;&#26816;&#27979;&#30340;&#22270;&#29255;&#25551;&#36848;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment. (arXiv:2308.07933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30196;&#21574;&#26816;&#27979;&#27169;&#22411;&#65292;&#23558;&#22270;&#29255;&#21644;&#25551;&#36848;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#30196;&#21574;&#26679;&#26412;&#19982;&#20581;&#24247;&#26679;&#26412;&#22312;&#25991;&#26412;&#19982;&#22270;&#29255;&#30456;&#20851;&#24615;&#21644;&#22270;&#29255;&#28966;&#28857;&#21306;&#22495;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20174;&#32780;&#21487;&#20197;&#25552;&#39640;&#30196;&#21574;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#29255;&#25551;&#36848;&#35821;&#38899;&#36827;&#34892;&#30196;&#21574;&#26816;&#27979;&#24050;&#32463;&#30740;&#31350;&#20102;30&#24180;&#12290;&#23613;&#31649;&#26377;&#36825;&#20040;&#38271;&#30340;&#21382;&#21490;&#65292;&#20808;&#21069;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#27491;&#24120;&#20154;&#21644;&#24739;&#26377;&#30196;&#21574;&#30151;&#30340;&#24739;&#32773;&#20043;&#38388;&#35821;&#38899;&#27169;&#24335;&#30340;&#24046;&#24322;&#65292;&#20294;&#27809;&#26377;&#30452;&#25509;&#21033;&#29992;&#22270;&#29255;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#23558;&#22270;&#29255;&#21644;&#25551;&#36848;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#34701;&#20837;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#30340;&#30196;&#21574;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#30196;&#21574;&#26679;&#26412;&#21644;&#20581;&#24247;&#26679;&#26412;&#22312;&#25991;&#26412;&#19982;&#22270;&#29255;&#30456;&#20851;&#24615;&#20197;&#21450;&#22270;&#29255;&#30340;&#28966;&#28857;&#21306;&#22495;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#30196;&#21574;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25991;&#26412;&#19982;&#22270;&#29255;&#30340;&#30456;&#20851;&#24615;&#23545;&#26679;&#26412;&#30340;&#21477;&#23376;&#36827;&#34892;&#25490;&#24207;&#21644;&#36807;&#28388;&#12290;&#25105;&#20204;&#36824;&#26681;&#25454;&#22270;&#29255;&#30340;&#28966;&#28857;&#21306;&#22495;&#30830;&#23450;&#20102;&#35805;&#39064;&#65292;&#24182;&#26681;&#25454;&#28966;&#28857;&#21306;&#22495;&#23545;&#21477;&#23376;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using picture description speech for dementia detection has been studied for 30 years. Despite the long history, previous models focus on identifying the differences in speech patterns between healthy subjects and patients with dementia but do not utilize the picture information directly. In this paper, we propose the first dementia detection models that take both the picture and the description texts as inputs and incorporate knowledge from large pre-trained image-text alignment models. We observe the difference between dementia and healthy samples in terms of the text's relevance to the picture and the focused area of the picture. We thus consider such a difference could be used to enhance dementia detection accuracy. Specifically, we use the text's relevance to the picture to rank and filter the sentences of the samples. We also identified focused areas of the picture as topics and categorized the sentences according to the focused areas. We propose three advanced models that pre-pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340;3D&#20960;&#20309;&#19982;2D&#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07931</link><description>&lt;p&gt;
&#31934;&#31616;&#29305;&#24449;&#22330;&#20351;&#24471;&#35821;&#35328;&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation. (arXiv:2308.07931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340;3D&#20960;&#20309;&#19982;2D&#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#23569;&#26679;&#26412;&#25805;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#21644;&#35821;&#35328;&#30417;&#30563;&#30340;&#22270;&#20687;&#27169;&#22411;&#21253;&#21547;&#20102;&#19990;&#30028;&#30340;&#20016;&#23500;&#30693;&#35782;&#65292;&#23545;&#20110;&#27867;&#21270;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#38656;&#35201;&#23545; 3D &#20960;&#20309;&#30340;&#35814;&#32454;&#29702;&#35299;&#65292;&#36825;&#22312; 2D &#22270;&#20687;&#29305;&#24449;&#20013;&#24448;&#24448;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#31934;&#31616;&#29305;&#24449;&#22330;&#65292;&#23558;&#31934;&#30830;&#30340; 3D &#20960;&#20309;&#19982; 2D &#22522;&#30784;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#26469;&#24357;&#21512;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#30340; 2D &#21040; 3D &#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545; 6 &#33258;&#30001;&#24230;&#25235;&#21462;&#21644;&#25918;&#32622;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20808;&#39564;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#30340;&#33258;&#28982;&#27867;&#21270;&#12290;&#36890;&#36807;&#20174;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; CLIP &#20013;&#31934;&#31616;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#30001;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#26032;&#39062;&#23545;&#35937;&#36827;&#34892;&#25805;&#20316;&#30340;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26410;&#35265;&#36807;&#30340;&#34920;&#36798;&#21644;&#26032;&#39062;&#31867;&#21035;&#30340;&#29289;&#20307;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised and language-supervised image models contain rich knowledge of the world that is important for generalization. Many robotic tasks, however, require a detailed understanding of 3D geometry, which is often lacking in 2D image features. This work bridges this 2D-to-3D gap for robotic manipulation by leveraging distilled feature fields to combine accurate 3D geometry with rich semantics from 2D foundation models. We present a few-shot learning method for 6-DOF grasping and placing that harnesses these strong spatial and semantic priors to achieve in-the-wild generalization to unseen objects. Using features distilled from a vision-language model, CLIP, we present a way to designate novel objects for manipulation via free-text natural language, and demonstrate its ability to generalize to unseen expressions and novel categories of objects.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#21160;MDP&#23398;&#20064;&#26469;&#36827;&#34892;&#27010;&#29575;&#40657;&#30418;&#26816;&#26597;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#21508;&#20010;&#38454;&#27573;&#36816;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#20256;&#32479;&#40657;&#30418;&#26816;&#26597;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07930</link><description>&lt;p&gt;
&#36816;&#29992;&#20027;&#21160;MDP&#23398;&#20064;&#30340;&#27010;&#29575;&#40657;&#30418;&#26816;&#26597;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Black-Box Checking via Active MDP Learning. (arXiv:2308.07930v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07930
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20027;&#21160;MDP&#23398;&#20064;&#26469;&#36827;&#34892;&#27010;&#29575;&#40657;&#30418;&#26816;&#26597;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#21508;&#20010;&#38454;&#27573;&#36816;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#20256;&#32479;&#40657;&#30418;&#26816;&#26597;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#27979;&#35797;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#38543;&#26426;&#40657;&#30418;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#24050;&#26377;&#30340;&#40657;&#30418;&#26816;&#26597;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#38543;&#26426;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#40657;&#30418;&#26816;&#26597;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#20197;&#19979;&#19977;&#20010;&#38454;&#27573;&#26469;&#36845;&#20195;&#22320;&#30830;&#23450;&#19968;&#20010;&#36829;&#21453;&#31995;&#32479;&#35268;&#33539;&#30340;&#36755;&#20837;&#65306;&#23398;&#20064;&#38454;&#27573;&#29992;&#20110;&#26500;&#24314;&#36924;&#36817;&#40657;&#30418;&#34892;&#20026;&#30340;&#33258;&#21160;&#26426;&#65292;&#21512;&#25104;&#38454;&#27573;&#29992;&#20110;&#20174;&#23398;&#20064;&#30340;&#33258;&#21160;&#26426;&#20013;&#30830;&#23450;&#19968;&#20010;&#20505;&#36873;&#30340;&#21453;&#20363;&#65292;&#39564;&#35777;&#38454;&#27573;&#29992;&#20110;&#39564;&#35777;&#33719;&#24471;&#30340;&#20505;&#36873;&#21453;&#20363;&#21644;&#23398;&#20064;&#30340;&#33258;&#21160;&#26426;&#26159;&#21542;&#31526;&#21512;&#21407;&#22987;&#40657;&#30418;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;ProbBBC&#65292;&#22312;&#23398;&#20064;&#38454;&#27573;&#37319;&#29992;&#20102;&#20027;&#21160;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#21512;&#25104;&#38454;&#27573;&#32467;&#21512;&#20102;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#65292;&#39564;&#35777;&#38454;&#27573;&#24212;&#29992;&#20102;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel methodology for testing stochastic black-box systems, frequently encountered in embedded systems. Our approach enhances the established black-box checking (BBC) technique to address stochastic behavior. Traditional BBC primarily involves iteratively identifying an input that breaches the system's specifications by executing the following three phases: the learning phase to construct an automaton approximating the black box's behavior, the synthesis phase to identify a candidate counterexample from the learned automaton, and the validation phase to validate the obtained candidate counterexample and the learned automaton against the original black-box system. Our method, ProbBBC, refines the conventional BBC approach by (1) employing an active Markov Decision Process (MDP) learning method during the learning phase, (2) incorporating probabilistic model checking in the synthesis phase, and (3) applying statistical hypothesis testing in the validation phase. ProbBBC un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#65292;&#36890;&#36807;&#24456;&#23569;&#30340;&#31034;&#20363;&#21644;&#26368;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#39640;&#25928;&#22320;&#24494;&#35843;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#31526;&#21512;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07929</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation. (arXiv:2308.07929v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#65292;&#36890;&#36807;&#24456;&#23569;&#30340;&#31034;&#20363;&#21644;&#26368;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#39640;&#25928;&#22320;&#24494;&#35843;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20351;&#20854;&#26356;&#31526;&#21512;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;Stable Diffusion&#22312;&#22522;&#30784;&#29702;&#35770;&#21644;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#21644;&#35745;&#31639;&#35201;&#27714;&#22686;&#21152;&#65292;&#29992;&#25143;&#20026;&#29305;&#23450;&#20219;&#21153;&#25110;&#20559;&#22909;&#20010;&#24615;&#21270;&#23427;&#20204;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#20043;&#21069;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#29305;&#23450;&#20154;&#31867;&#20559;&#22909;&#38598;&#21512;&#30340;&#38382;&#39064;&#65292;&#23558;&#26816;&#32034;&#25110;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#29992;&#25143;&#30340;&#20559;&#22909;&#23545;&#40784;&#12290;&#25105;&#20204;&#21033;&#29992;Bradley-Terry&#20559;&#22909;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24456;&#23569;&#30340;&#31034;&#20363;&#21644;&#26368;&#23567;&#30340;&#35745;&#31639;&#36164;&#28304;&#39640;&#25928;&#22320;&#24494;&#35843;&#21407;&#22987;&#27169;&#22411;&#12290;&#36890;&#36807;&#19982;&#22810;&#27169;&#24577;&#25991;&#26412;&#21644;&#22270;&#20687;&#29702;&#35299;&#30456;&#20851;&#30340;&#19981;&#21516;&#39046;&#22495;&#30340;&#23454;&#39564;&#35777;&#25454;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20010;&#26694;&#26550;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large multimodal models, such as CLIP and Stable Diffusion have experimented tremendous successes in both foundations and applications. However, as these models increase in parameter size and computational requirements, it becomes more challenging for users to personalize them for specific tasks or preferences. In this work, we address the problem of adapting the previous models towards sets of particular human preferences, aligning the retrieved or generated images with the preferences of the user. We leverage the Bradley-Terry preference model to develop a fast adaptation method that efficiently fine-tunes the original model, with few examples and with minimal computing resources. Extensive evidence of the capabilities of this framework is provided through experiments in different domains related to multimodal text and image understanding, including preference prediction as a reward model, and generation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25506;&#32034;&#20102;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#30340;&#24320;&#22987;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.07927</link><description>&lt;p&gt;
"&#26376;&#32463;&#21608;&#26399;&#38271;&#24230;&#30340;&#39044;&#27979;&#24314;&#27169;&#65306;&#19968;&#31181;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;"
&lt;/p&gt;
&lt;p&gt;
Predictive Modeling of Menstrual Cycle Length: A Time Series Forecasting Approach. (arXiv:2308.07927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25506;&#32034;&#20102;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#30340;&#24320;&#22987;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#30830;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#23545;&#22899;&#24615;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#35753;&#20010;&#20307;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#26469;&#20943;&#23569;&#19982;&#21608;&#26399;&#30456;&#20851;&#30340;&#19981;&#36866;&#12290;&#27492;&#22806;&#65292;&#31934;&#30830;&#30340;&#39044;&#27979;&#23545;&#20110;&#35268;&#21010;&#22899;&#24615;&#29983;&#27963;&#20013;&#30340;&#37325;&#35201;&#20107;&#20214;&#65292;&#22914;&#35745;&#21010;&#29983;&#32946;&#65292;&#20063;&#26159;&#26377;&#29992;&#30340;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#39044;&#27979;&#35268;&#24459;&#21644;&#19981;&#35268;&#24459;&#26376;&#32463;&#21608;&#26399;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20123;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#65292;&#22914;&#33258;&#22238;&#24402;&#32508;&#21512;&#31227;&#21160;&#24179;&#22343;&#12289;Huber&#22238;&#24402;&#12289;Lasso&#22238;&#24402;&#12289;&#27491;&#20132;&#21305;&#37197;&#36861;&#36394;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#21512;&#25104;&#25968;&#25454;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#30340;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26376;&#32463;&#21608;&#26399;&#30340;&#24320;&#22987;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
A proper forecast of the menstrual cycle is meaningful for women's health, as it allows individuals to take preventive actions to minimize cycle-associated discomforts. In addition, precise prediction can be useful for planning important events in a woman's life, such as family planning. In this work, we explored the use of machine learning techniques to predict regular and irregular menstrual cycles. We implemented some time series forecasting algorithm approaches, such as AutoRegressive Integrated Moving Average, Huber Regression, Lasso Regression, Orthogonal Matching Pursuit, and Long Short-Term Memory Network. Moreover, we generated synthetic data to achieve our purposes. The results showed that it is possible to accurately predict the onset and duration of menstrual cycles using machine learning techniques.
&lt;/p&gt;</description></item><item><title>SciRE-Solver&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#37319;&#26679;&#36807;&#31243;&#32531;&#24930;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07896</link><description>&lt;p&gt;
SciRE-Solver: &#29992;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation. (arXiv:2308.07896v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07896
&lt;/p&gt;
&lt;p&gt;
SciRE-Solver&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;&#21644;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#37319;&#26679;&#36807;&#31243;&#32531;&#24930;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;(DPMs)&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20854;&#29983;&#25104;&#39640;&#20445;&#30495;&#22270;&#20687;&#26679;&#26412;&#30340;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;DPMs&#30340;&#23454;&#29616;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#37319;&#26679;&#36807;&#31243;&#32531;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;DPMs&#37319;&#26679;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38024;&#23545;&#19982;DPMs&#37319;&#26679;&#36807;&#31243;&#23545;&#24212;&#30340;&#25193;&#25955;ODE&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24471;&#20998;&#30340;&#31934;&#30830;&#35299;&#20915;&#26041;&#26696;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#20026;&#27714;&#35299;&#25193;&#25955;ODE&#30340;&#25968;&#20540;&#31639;&#27861;&#24320;&#21457;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#30340;&#37319;&#26679;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#23548;&#25968;&#20272;&#35745;(RDE)&#26041;&#27861;&#26469;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#33539;&#24335;&#21644;RDE&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#25910;&#25947;&#39034;&#24207;&#20445;&#35777;&#30340;&#24471;&#20998;&#31215;&#20998;&#27714;&#35299;&#22120;(SciRE-Solver)&#26469;&#35299;&#20915;&#25193;&#25955;ODEs&#12290;SciRE-Solver&#22312;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;DPMs&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#24615;&#33021;&#65292;&#24182;&#19988;&#20165;&#38656;&#26377;&#38480;&#25968;&#37327;&#30340;&#24471;&#20998;&#20989;&#25968;&#35780;&#20272;(NFE)&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) are a powerful class of generative models known for their ability to generate high-fidelity image samples. A major challenge in the implementation of DPMs is the slow sampling process. In this work, we bring a high-efficiency sampler for DPMs. Specifically, we propose a score-based exact solution paradigm for the diffusion ODEs corresponding to the sampling process of DPMs, which introduces a new perspective on developing numerical algorithms for solving diffusion ODEs. To achieve an efficient sampler, we propose a recursive derivative estimation (RDE) method to reduce the estimation error. With our proposed solution paradigm and RDE method, we propose the score-integrand solver with the convergence order guarantee as efficient solver (SciRE-Solver) for solving diffusion ODEs. The SciRE-Solver attains state-of-the-art (SOTA) sampling performance with a limited number of score function evaluations (NFE) on both discrete-time and continuous-time DPMs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffGuard&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DiffGuard&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.07687</link><description>&lt;p&gt;
DiffGuard&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models. (arXiv:2308.07687v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffGuard&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DiffGuard&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#35821;&#20041;&#24102;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#19982;&#21512;&#27861;&#31867;&#21035;&#20869;&#23481;&#22312;&#35821;&#20041;&#19978;&#30340;&#19981;&#21305;&#37197;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;DiffGuard&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;DiffGuard&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#19981;&#21305;&#37197;&#24341;&#23548;&#65292;&#30456;&#36739;&#20110;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#25193;&#25955;&#27169;&#22411;&#26356;&#26131;&#20110;&#35757;&#32451;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#26465;&#20214;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#65292;DiffGuard&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24102;&#22806;&#20998;&#24067;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20351;&#29992;&#22270;&#20687;&#21644;&#26631;&#31614;&#20316;&#20026;&#26465;&#20214;&#35757;&#32451;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#22256;&#38590;&#24615;&#65292;DiffGuard&#22312;ImageNet&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a classifier, the inherent property of semantic Out-of-Distribution (OOD) samples is that their contents differ from all legal classes in terms of semantics, namely semantic mismatch. There is a recent work that directly applies it to OOD detection, which employs a conditional Generative Adversarial Network (cGAN) to enlarge semantic mismatch in the image space. While achieving remarkable OOD detection performance on small datasets, it is not applicable to ImageNet-scale datasets due to the difficulty in training cGANs with both input images and labels as conditions. As diffusion models are much easier to train and amenable to various conditions compared to cGANs, in this work, we propose to directly use pre-trained diffusion models for semantic mismatch-guided OOD detection, named DiffGuard. Specifically, given an OOD input image and the predicted label from the classifier, we try to enlarge the semantic difference between the reconstructed OOD image under these conditions and t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#20010;&#24615;&#21270;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.07439</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#20114;&#24863;&#30693;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interaction-Aware Personalized Vehicle Trajectory Prediction Using Temporal Graph Neural Networks. (arXiv:2308.07439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#20010;&#24615;&#21270;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#36710;&#36742;&#36712;&#36857;&#23545;&#20110;&#20808;&#36827;&#39550;&#39542;&#36741;&#21161;&#31995;&#32479;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#30340;&#36890;&#29992;&#36712;&#36857;&#39044;&#27979;&#65292;&#24573;&#35270;&#20102;&#20010;&#21035;&#39550;&#39542;&#21592;&#30340;&#20010;&#24615;&#21270;&#39550;&#39542;&#27169;&#24335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24863;&#30693;&#30340;&#20010;&#24615;&#21270;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26469;&#24314;&#27169;&#30446;&#26631;&#36710;&#36742;&#19982;&#21608;&#22260;&#20132;&#36890;&#20043;&#38388;&#30340;&#26102;&#31354;&#20132;&#20114;&#12290;&#20026;&#20102;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#27969;&#31243;&#65306;&#27169;&#22411;&#39318;&#20808;&#22312;&#22823;&#35268;&#27169;&#36712;&#36857;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#23450;&#39550;&#39542;&#25968;&#25454;&#38024;&#23545;&#27599;&#20010;&#39550;&#39542;&#21592;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#37319;&#29992;&#20154;&#26426;&#21327;&#21516;&#20223;&#30495;&#26469;&#25910;&#38598;&#20010;&#24615;&#21270;&#30340;&#33258;&#28982;&#39550;&#39542;&#36712;&#36857;&#21450;&#20854;&#30456;&#24212;&#30340;&#21608;&#22260;&#36710;&#36742;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of vehicle trajectories is vital for advanced driver assistance systems and autonomous vehicles. Existing methods mainly rely on generic trajectory predictions derived from large datasets, overlooking the personalized driving patterns of individual drivers. To address this gap, we propose an approach for interaction-aware personalized vehicle trajectory prediction that incorporates temporal graph neural networks. Our method utilizes Graph Convolution Networks (GCN) and Long Short-Term Memory (LSTM) to model the spatio-temporal interactions between target vehicles and their surrounding traffic. To personalize the predictions, we establish a pipeline that leverages transfer learning: the model is initially pre-trained on a large-scale trajectory dataset and then fine-tuned for each driver using their specific driving data. We employ human-in-the-loop simulation to collect personalized naturalistic driving trajectories and corresponding surrounding vehicle trajectories
&lt;/p&gt;</description></item><item><title>LCE&#26159;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;Python&#21253;&#65292;&#23427;&#23454;&#29616;&#20102;&#23616;&#37096;&#32423;&#32852;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#30340;&#20248;&#21183;&#65292;&#37319;&#29992;&#22810;&#26679;&#21270;&#30340;&#26041;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07250</link><description>&lt;p&gt;
LCE: Python&#20013;&#22686;&#24378;&#29256;&#30340;Bagging&#21644;Boosting&#30340;&#32467;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LCE: An Augmented Combination of Bagging and Boosting in Python. (arXiv:2308.07250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07250
&lt;/p&gt;
&lt;p&gt;
LCE&#26159;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;Python&#21253;&#65292;&#23427;&#23454;&#29616;&#20102;&#23616;&#37096;&#32423;&#32852;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#30340;&#20248;&#21183;&#65292;&#37319;&#29992;&#22810;&#26679;&#21270;&#30340;&#26041;&#27861;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
lcensemble&#26159;&#19968;&#20010;&#39640;&#24615;&#33021;&#12289;&#21487;&#25193;&#23637;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#24120;&#35265;&#20219;&#21153;&#12290;&#35813;&#21253;&#23454;&#29616;&#20102;&#23616;&#37096;&#32423;&#32852;&#38598;&#25104;(LCE)&#65292;&#36825;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22914;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;LCE&#23558;&#23427;&#20204;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#20114;&#34917;&#30340;&#22810;&#26679;&#21270;&#26041;&#27861;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#39044;&#27979;&#22120;&#12290;&#35813;&#21253;&#19982;scikit-learn&#20860;&#23481;, &#21487;&#20197;&#19982;scikit-learn&#30340;&#27969;&#27700;&#32447;&#21644;&#27169;&#22411;&#36873;&#25321;&#24037;&#20855;&#20132;&#20114;&#12290;&#23427;&#22312;Apache 2.0&#35768;&#21487;&#19979;&#20998;&#21457;&#65292;&#24182;&#19988;&#20854;&#28304;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/LocalCascadeEnsemble/LCE&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
lcensemble is a high-performing, scalable and user-friendly Python package for the general tasks of classification and regression. The package implements Local Cascade Ensemble (LCE), a machine learning method that further enhances the prediction performance of the current state-of-the-art methods Random Forest and XGBoost. LCE combines their strengths and adopts a complementary diversification approach to obtain a better generalizing predictor. The package is compatible with scikit-learn, therefore it can interact with scikit-learn pipelines and model selection tools. It is distributed under the Apache 2.0 license, and its source code is available at https://github.com/LocalCascadeEnsemble/LCE.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#22312;&#24037;&#19994;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#22270;&#20687;&#26469;&#23398;&#20064;3D&#22330;&#26223;&#34920;&#31034;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#24037;&#19994;3D&#34920;&#31034;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.07118</link><description>&lt;p&gt;
&#24037;&#19994;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#65306;&#24212;&#29992;&#65292;&#30740;&#31350;&#26426;&#20250;&#21644;&#20351;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Neural radiance fields in the industrial and robotics domain: applications, research opportunities and use cases. (arXiv:2308.07118v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#22312;&#24037;&#19994;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#22270;&#20687;&#26469;&#23398;&#20064;3D&#22330;&#26223;&#34920;&#31034;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#24037;&#19994;3D&#34920;&#31034;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35832;&#22914;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#20043;&#31867;&#30340;&#25216;&#26415;&#30340;&#26222;&#21450;&#22686;&#21152;&#20102;&#23545;&#39640;&#36136;&#37327;&#19977;&#32500;&#22270;&#24418;&#34920;&#31034;&#30340;&#38656;&#27714;&#12290;&#24037;&#19994;3D&#24212;&#29992;&#21253;&#25324;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;CAD&#65289;&#65292;&#26377;&#38480;&#20803;&#20998;&#26512;&#65288;FEA&#65289;&#65292;&#25195;&#25551;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#24037;&#19994;3D&#34920;&#31034;&#30340;&#26041;&#27861;&#23384;&#22312;&#23454;&#26045;&#25104;&#26412;&#39640;&#21644;&#23545;&#20934;&#30830;3D&#24314;&#27169;&#30340;&#25163;&#24037;&#36755;&#20837;&#20381;&#36182;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#20379;&#30340;&#35757;&#32451;2D&#22270;&#20687;&#26469;&#23398;&#20064;3D&#22330;&#26223;&#34920;&#31034;&#12290;&#23613;&#31649;&#23545;NeRFs&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#20294;&#23427;&#20204;&#22312;&#21508;&#31181;&#24037;&#19994;&#23376;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;NeRF&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#27010;&#24565;&#39564;&#35777;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;NeRF&#22312;&#24037;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of technologies, such as extended reality (XR), has increased the demand for high-quality three-dimensional (3D) graphical representations. Industrial 3D applications encompass computer-aided design (CAD), finite element analysis (FEA), scanning, and robotics. However, current methods employed for industrial 3D representations suffer from high implementation costs and reliance on manual human input for accurate 3D modeling. To address these challenges, neural radiance fields (NeRFs) have emerged as a promising approach for learning 3D scene representations based on provided training 2D images. Despite a growing interest in NeRFs, their potential applications in various industrial subdomains are still unexplored. In this paper, we deliver a comprehensive examination of NeRF industrial applications while also providing direction for future research endeavors. We also present a series of proof-of-concept experiments that demonstrate the potential of NeRFs in the industri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#20301;&#23485;&#21644;&#23618;&#23485;&#26469;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#20248;&#21270;&#20102;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06422</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26641;&#29366;Parzen&#20272;&#35745;&#30340;&#25935;&#24863;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21644;&#23485;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation. (arXiv:2308.06422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#20301;&#23485;&#21644;&#23618;&#23485;&#26469;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#20248;&#21270;&#20102;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#25552;&#39640;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#26377;&#25928;&#20248;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25628;&#32034;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#36873;&#25321;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26368;&#20339;&#20301;&#23485;&#21644;&#23618;&#23485;&#12290;&#36825;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25928;&#29575;&#30340;&#26126;&#26174;&#25552;&#39640;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;Hessian&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26377;&#36873;&#25321;&#22320;&#20943;&#23569;&#25628;&#32034;&#22495;&#65292;&#30830;&#20445;&#31227;&#38500;&#38750;&#20851;&#38190;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#24320;&#21457;&#26377;&#21033;&#21644;&#19981;&#21033;&#32467;&#26524;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#36825;&#31181;&#31574;&#30053;&#20801;&#35768;&#23545;&#26550;&#26500;&#21487;&#33021;&#24615;&#36827;&#34892;&#31616;&#21270;&#30340;&#25506;&#32034;&#65292;&#24182;&#36805;&#36895;&#30830;&#23450;&#34920;&#29616;&#26368;&#22909;&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#30693;&#21517;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#26126;&#26174;&#20248;&#21183;&#12290;&#19982;&#39046;&#20808;&#30340;&#21387;&#32553;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;PeRP&#65292;&#29992;&#20110;&#32531;&#35299;&#25317;&#22581;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#26500;&#21270;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26681;&#25454;&#39550;&#39542;&#21592;&#30340;&#29305;&#24449;&#20026;&#20854;&#25552;&#20379;&#34892;&#21160;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;</title><link>http://arxiv.org/abs/2308.00864</link><description>&lt;p&gt;
PeRP&#65306;&#36890;&#36807;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;&#23454;&#29616;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#20197;&#32531;&#35299;&#25317;&#22581;
&lt;/p&gt;
&lt;p&gt;
PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems. (arXiv:2308.00864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;PeRP&#65292;&#29992;&#20110;&#32531;&#35299;&#25317;&#22581;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32467;&#26500;&#21270;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26681;&#25454;&#39550;&#39542;&#21592;&#30340;&#29305;&#24449;&#20026;&#20854;&#25552;&#20379;&#34892;&#21160;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#39550;&#39542;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#34892;&#21160;&#26469;&#32531;&#35299;&#25317;&#22581;&#65292;&#20174;&#32780;&#25913;&#21892;&#36890;&#21220;&#26102;&#38388;&#21644;&#29123;&#27833;&#25104;&#26412;&#31561;&#20247;&#22810;&#31038;&#20250;&#32463;&#27982;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20551;&#35774;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#38431;&#20855;&#26377;&#31934;&#30830;&#30340;&#25511;&#21046;&#65292;&#22240;&#27492;&#22312;&#23454;&#38469;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#26410;&#33021;&#32771;&#34385;&#21040;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20998;&#27573;&#24120;&#25968;&#65288;PC&#65289;&#31574;&#30053;&#36890;&#36807;&#32467;&#26500;&#24314;&#27169;&#20154;&#31867;&#39550;&#39542;&#30340;&#30456;&#20284;&#24615;&#26469;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#65292;&#20197;&#25552;&#20379;&#32473;&#20154;&#31867;&#39550;&#39542;&#21592;&#36981;&#24490;&#30340;&#34892;&#21160;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;PC&#31574;&#30053;&#20551;&#35774;&#25152;&#26377;&#39550;&#39542;&#21592;&#34892;&#20026;&#30456;&#20284;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;PC&#31574;&#30053;&#30340;&#21512;&#20316;&#21672;&#35810;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#31181;&#26032;&#22411;&#30340;&#39550;&#39542;&#21592;&#29305;&#24449;&#30456;&#20851;&#30340;&#20010;&#24615;&#21270;&#21097;&#20313;&#31574;&#30053;&#65292;&#21363;PeRP&#12290;PeRP&#24314;&#35758;&#39550;&#39542;&#21592;&#20197;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#30340;&#26041;&#24335;&#34892;&#39542;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26080;&#30417;&#30563;&#22320;&#25512;&#26029;&#39550;&#39542;&#21592;&#22914;&#20309;&#36981;&#24490;&#25351;&#20196;&#30340;&#20869;&#22312;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#31574;&#30053;&#19982;&#39550;&#39542;&#21592;&#29305;&#24449;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#34892;&#21160;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent driving systems can be used to mitigate congestion through simple actions, thus improving many socioeconomic factors such as commute time and gas costs. However, these systems assume precise control over autonomous vehicle fleets, and are hence limited in practice as they fail to account for uncertainty in human behavior. Piecewise Constant (PC) Policies address these issues by structurally modeling the likeness of human driving to reduce traffic congestion in dense scenarios to provide action advice to be followed by human drivers. However, PC policies assume that all drivers behave similarly. To this end, we develop a co-operative advisory system based on PC policies with a novel driver trait conditioned Personalized Residual Policy, PeRP. PeRP advises drivers to behave in ways that mitigate traffic congestion. We first infer the driver's intrinsic traits on how they follow instructions in an unsupervised manner with a variational autoencoder. Then, a policy conditioned o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.16120</link><description>&lt;p&gt;
&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#19982;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#29992;&#20110;&#38750;&#32447;&#24615;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Deep Unrolling Networks with Recurrent Momentum Acceleration for Nonlinear Inverse Problems. (arXiv:2307.16120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#22522;&#20110;&#27169;&#22411;&#30340;&#36845;&#20195;&#31639;&#27861;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;(DuNets)&#24050;&#25104;&#20026;&#35299;&#20915;&#36870;&#21521;&#25104;&#20687;&#38382;&#39064;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#34429;&#28982;DuNets&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#65292;&#20294;&#38750;&#32447;&#24615;&#38382;&#39064;&#24448;&#24448;&#20250;&#24433;&#21709;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#21463;&#20248;&#21270;&#31639;&#27861;&#20013;&#24120;&#29992;&#30340;&#21160;&#37327;&#21152;&#36895;&#25216;&#26415;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24490;&#29615;&#21160;&#37327;&#21152;&#36895;(RMA)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(LSTM-RNN)&#26469;&#27169;&#25311;&#21160;&#37327;&#21152;&#36895;&#36807;&#31243;&#12290;RMA&#27169;&#22359;&#21033;&#29992;LSTM-RNN&#23398;&#20064;&#21644;&#20445;&#30041;&#20808;&#21069;&#26799;&#24230;&#30340;&#30693;&#35782;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;RMA&#24212;&#29992;&#20110;&#20004;&#31181;&#27969;&#34892;&#30340;DuNets&#8212;&#8212;&#23398;&#20064;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;(LPGD)&#21644;&#23398;&#20064;&#30340;&#21407;&#22987;-&#23545;&#20598;(LPD)&#26041;&#27861;&#65292;&#20998;&#21035;&#24471;&#21040;LPGD-RMA&#21644;LPD-RMA&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38750;&#32447;&#24615;&#36870;&#21521;&#38382;&#39064;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65306;&#38750;&#32447;&#24615;&#21435;&#21367;&#31215;&#38382;&#39064;&#12289;
&lt;/p&gt;
&lt;p&gt;
Combining the strengths of model-based iterative algorithms and data-driven deep learning solutions, deep unrolling networks (DuNets) have become a popular tool to solve inverse imaging problems. While DuNets have been successfully applied to many linear inverse problems, nonlinear problems tend to impair the performance of the method. Inspired by momentum acceleration techniques that are often used in optimization algorithms, we propose a recurrent momentum acceleration (RMA) framework that uses a long short-term memory recurrent neural network (LSTM-RNN) to simulate the momentum acceleration process. The RMA module leverages the ability of the LSTM-RNN to learn and retain knowledge from the previous gradients. We apply RMA to two popular DuNets -- the learned proximal gradient descent (LPGD) and the learned primal-dual (LPD) methods, resulting in LPGD-RMA and LPD-RMA respectively. We provide experimental results on two nonlinear inverse problems: a nonlinear deconvolution problem, an
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.13565</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65306;&#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#22522;&#20934;&#21644;&#26410;&#26469;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13565
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#20197;&#20248;&#21270;&#20915;&#31574;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20010;&#33539;&#24335;&#26377;&#26395;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20915;&#31574;&#21046;&#23450;&#65292;&#36825;&#20123;&#24212;&#29992;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20123;&#20915;&#31574;&#27169;&#22411;&#20013;&#20272;&#35745;&#26410;&#30693;&#21442;&#25968;&#32463;&#24120;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#23545;DFL&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#23427;&#23545;&#21508;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26681;&#25454;&#20854;&#29420;&#29305;&#29305;&#24449;&#26469;&#21306;&#20998;DFL&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;DFL&#30340;&#21512;&#36866;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;DFL&#30740;&#31350;&#20013;&#24403;&#21069;&#21644;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2307.11787</link><description>&lt;p&gt;
LLM&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#26377;&#25152;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#20225;&#19994;&#21644;&#28040;&#36153;&#32773;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#34429;&#28982;&#36825;&#31867;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#23427;&#20204;&#20316;&#20026;&#35748;&#30693;&#20027;&#20307;&#30340;&#35843;&#26597;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#23545;GPT-3&#21644;ChatGPT&#22312;&#19968;&#20010;&#26469;&#33258;&#35748;&#30693;&#31185;&#23398;&#25991;&#29486;&#30340;&#26377;&#38480;&#25968;&#25454;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#20462;&#21098;&#30456;&#20851;&#23454;&#20307;&#65292;&#20174;&#32780;&#24341;&#23548;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#39046;&#22495;&#21644;&#24322;&#36136;&#31181;&#23376;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24212;&#29992;&#31867;&#27604;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.16296</link><description>&lt;p&gt;
&#30456;&#20851;&#23454;&#20307;&#36873;&#25321;&#65306;&#36890;&#36807;&#38646;&#26679;&#26412;&#31867;&#27604;&#20462;&#21098;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Relevant Entity Selection: Knowledge Graph Bootstrapping via Zero-Shot Analogical Pruning. (arXiv:2306.16296v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#21644;&#20462;&#21098;&#30456;&#20851;&#23454;&#20307;&#65292;&#20174;&#32780;&#24341;&#23548;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#39046;&#22495;&#21644;&#24322;&#36136;&#31181;&#23376;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24212;&#29992;&#31867;&#27604;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#39640;&#36136;&#37327;&#30340;&#26680;&#24515;&#24320;&#22987;&#65292;&#36890;&#36807;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#19981;&#26029;&#25913;&#36827;&#12290;&#36825;&#26679;&#30340;&#26680;&#24515;&#21487;&#20197;&#20174;&#20687;Wikidata&#36825;&#26679;&#30340;&#24320;&#25918;&#24335;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#24471;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#31181;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#35268;&#27169;&#65292;&#23558;&#20854;&#20316;&#20026;&#25972;&#20307;&#38598;&#25104;&#21487;&#33021;&#20250;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#65292;&#20174;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#24863;&#20852;&#36259;&#31181;&#23376;&#23454;&#20307;&#24320;&#22987;&#65292;&#24182;&#20445;&#30041;&#25110;&#20462;&#21098;&#20854;&#30456;&#37051;&#23454;&#20307;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598; &#22312;Wikidata&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#39046;&#22495;&#21516;&#36136;&#25110;&#24322;&#36136;&#30340;&#31181;&#23376;&#23454;&#20307;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#31867;&#27604;&#30340;&#26041;&#27861;&#20248;&#20110;LSTM&#65292;&#38543;&#26426;&#26862;&#26519;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#19988;&#21442;&#25968;&#25968;&#37327;&#22823;&#22823;&#20943;&#23569;&#12290;&#25105;&#20204;&#36824;&#22312;&#36801;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#36827;&#19968;&#27493;&#23558;&#22522;&#20110;&#31867;&#27604;&#30340;&#25512;&#29702;&#38598;&#25104;&#21040;&#30456;&#20851;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Construction (KGC) can be seen as an iterative process starting from a high quality nucleus that is refined by knowledge extraction approaches in a virtuous loop. Such a nucleus can be obtained from knowledge existing in an open KG like Wikidata. However, due to the size of such generic KGs, integrating them as a whole may entail irrelevant content and scalability issues. We propose an analogy-based approach that starts from seed entities of interest in a generic KG, and keeps or prunes their neighboring entities. We evaluate our approach on Wikidata through two manually labeled datasets that contain either domain-homogeneous or -heterogeneous seed entities. We empirically show that our analogy-based approach outperforms LSTM, Random Forest, SVM, and MLP, with a drastically lower number of parameters. We also evaluate its generalization potential in a transfer learning setting. These results advocate for the further integration of analogy-based inference in tasks relate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65288;APCONs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#26102;&#21464;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#22810;&#20010;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#12289;&#27744;&#21270;&#21644;&#28608;&#27963;&#25805;&#20316;&#26469;&#25429;&#25417;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#30340;&#25193;&#25955;&#34892;&#20026;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15891</link><description>&lt;p&gt;
&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#25429;&#25417;&#22810;&#23610;&#24230;&#32447;&#24615;&#36755;&#36816;&#26041;&#31243;&#30340;&#25193;&#25955;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Asymptotic-Preserving Convolutional DeepONets Capture the Diffusive Behavior of the Multiscale Linear Transport Equations. (arXiv:2306.15891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65288;APCONs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#26102;&#21464;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#22810;&#20010;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#12289;&#27744;&#21270;&#21644;&#28608;&#27963;&#25805;&#20316;&#26469;&#25429;&#25417;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#30340;&#25193;&#25955;&#34892;&#20026;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65288;APCONs&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#23610;&#24230;&#26102;&#21464;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#20462;&#25913;&#36807;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#22522;&#26412;&#29289;&#29702;&#32422;&#26463;DeepONets&#21487;&#33021;&#22312;&#20445;&#25345;&#26399;&#26395;&#30340;&#23439;&#35266;&#34892;&#20026;&#19978;&#34920;&#29616;&#20986;&#19981;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20351;&#29992;&#28176;&#36817;&#20445;&#25345;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#21463;&#25193;&#25955;&#26041;&#31243;&#20013;&#30340;&#28909;&#26680;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65292;&#23427;&#22312;&#27599;&#20010;&#28388;&#27874;&#22120;&#23618;&#20013;&#37319;&#29992;&#22810;&#20010;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#32780;&#19981;&#26159;&#20840;&#23616;&#28909;&#26680;&#65292;&#24182;&#32467;&#21512;&#27744;&#21270;&#21644;&#28608;&#27963;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;APCON&#26041;&#27861;&#30340;&#21442;&#25968;&#25968;&#37327;&#19982;&#32593;&#26684;&#22823;&#23567;&#26080;&#20851;&#65292;&#24182;&#33021;&#22815;&#25429;&#25417;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#30340;&#25193;&#25955;&#34892;&#20026;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce two types of novel Asymptotic-Preserving Convolutional Deep Operator Networks (APCONs) designed to address the multiscale time-dependent linear transport problem. We observe that the vanilla physics-informed DeepONets with modified MLP may exhibit instability in maintaining the desired limiting macroscopic behavior. Therefore, this necessitates the utilization of an asymptotic-preserving loss function. Drawing inspiration from the heat kernel in the diffusion equation, we propose a new architecture called Convolutional Deep Operator Networks, which employ multiple local convolution operations instead of a global heat kernel, along with pooling and activation operations in each filter layer. Our APCON methods possess a parameter count that is independent of the grid size and are capable of capturing the diffusive behavior of the linear transport problem. Finally, we validate the effectiveness of our methods through several numerical examples.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10698</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#35760;&#24518;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Multitask Episodic Memory Based on Task-Conditioned Hypernetwork. (arXiv:2306.10698v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10698
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#19968;&#20010;&#26032;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#32593;&#32476;&#21442;&#25968;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36873;&#25321;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#24182;&#23558;&#20854;&#34701;&#21512;&#21040;&#26082;&#26377;&#20915;&#31574;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#21463;&#21040;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38480;&#21046;&#65292;&#20005;&#37325;&#20381;&#36182;&#19982;&#29615;&#22659;&#30340;&#22810;&#27425;&#20132;&#20114;&#25165;&#33021;&#33719;&#24471;&#20934;&#30830;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#20284;&#20046;&#20381;&#36182;&#28023;&#39532;&#20307;&#20174;&#36807;&#21435;&#26377;&#20851;&#20219;&#21153;&#30340;&#32463;&#21382;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#25351;&#23548;&#20854;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20026;&#20195;&#29702;&#35774;&#35745;&#31867;&#20284;&#28023;&#39532;&#20307;&#30340;&#27169;&#22359;&#20197;&#23558;&#36807;&#21435;&#30340;&#32463;&#21382;&#34701;&#20837;&#26082;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#28041;&#21450;&#36873;&#25321;&#24403;&#21069;&#20219;&#21153;&#26368;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#31532;&#20108;&#20010;&#26159;&#23558;&#36825;&#20123;&#32463;&#39564;&#19982;&#20915;&#31574;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#21270;&#36229;&#32593;&#32476;&#30340;&#26816;&#32034;&#32593;&#32476;&#65292;&#26681;&#25454;&#20219;&#21153;&#35843;&#25972;&#26816;&#32034;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning algorithms are usually impeded by sampling inefficiency, heavily depending on multiple interactions with the environment to acquire accurate decision-making capabilities. In contrast, humans seem to rely on their hippocampus to retrieve relevant information from past experiences of relevant tasks, which guides their decision-making when learning a new task, rather than exclusively depending on environmental interactions. Nevertheless, designing a hippocampus-like module for an agent to incorporate past experiences into established reinforcement learning algorithms presents two challenges. The first challenge involves selecting the most relevant past experiences for the current task, and the second is integrating such experiences into the decision network. To address these challenges, we propose a novel algorithm that utilizes a retrieval network based on a task-conditioned hypernetwork, which adapts the retrieval network's parameters depending on the task. A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411; LDIDPs&#65292;&#21033;&#29992;&#38544;&#24335;&#25193;&#25955;&#36807;&#31243;&#20174;&#21160;&#24577;&#28508;&#22312;&#36807;&#31243;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#28982;&#21518;&#29983;&#25104;&#30456;&#24212;&#30340;&#39034;&#24207;&#35266;&#23519;&#26679;&#26412;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#39034;&#24207;&#29983;&#25104;&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07077</link><description>&lt;p&gt;
&#28508;&#22312;&#21160;&#24577;&#38544;&#24335;&#25193;&#25955;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Latent Dynamical Implicit Diffusion Processes. (arXiv:2306.07077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411; LDIDPs&#65292;&#21033;&#29992;&#38544;&#24335;&#25193;&#25955;&#36807;&#31243;&#20174;&#21160;&#24577;&#28508;&#22312;&#36807;&#31243;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#28982;&#21518;&#29983;&#25104;&#30456;&#24212;&#30340;&#39034;&#24207;&#35266;&#23519;&#26679;&#26412;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#30340;&#39034;&#24207;&#29983;&#25104;&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#24120;&#34987;&#29992;&#26469;&#23398;&#20064;&#20195;&#34920;&#19968;&#31995;&#21015;&#22122;&#22768;&#25968;&#25454;&#26679;&#26412;&#30340;&#28508;&#22312;&#21160;&#24577;&#36807;&#31243;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28508;&#22312;&#30340;&#21644;&#35266;&#27979;&#21160;&#24577;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#65292;&#20135;&#29983;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#30340;&#26679;&#26412;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;DDPM&#21644;NCSN&#65289;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20174;&#20808;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24207;&#21015;&#26679;&#26412;&#65292;&#30456;&#36739;&#20110;&#20808;&#36827;&#30340;&#28508;&#22312;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#31070;&#32463;ODE&#12289;RNN&#21644;&#27491;&#21017;&#21270;&#27969;&#32593;&#32476;&#65289;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#24207;&#21015;&#25968;&#25454;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#21160;&#24577;&#38544;&#24335;&#25193;&#25955;&#36807;&#31243;&#65288;LDIDPs&#65289;&#30340;&#26032;&#22411;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65292;&#21033;&#29992;&#38544;&#24335;&#25193;&#25955;&#36807;&#31243;&#20174;&#21160;&#24577;&#28508;&#22312;&#36807;&#31243;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#28982;&#21518;&#29983;&#25104;&#30456;&#24212;&#30340;&#39034;&#24207;&#35266;&#23519;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#27169;&#25311;&#31070;&#32463;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;LDIDPs&#65292;&#24182;&#35777;&#26126;&#23427;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#39034;&#24207;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent dynamical models are commonly used to learn the distribution of a latent dynamical process that represents a sequence of noisy data samples. However, producing samples from such models with high fidelity is challenging due to the complexity and variability of latent and observation dynamics. Recent advances in diffusion-based generative models, such as DDPM and NCSN, have shown promising alternatives to state-of-the-art latent generative models, such as Neural ODEs, RNNs, and Normalizing flow networks, for generating high-quality sequential samples from a prior distribution. However, their application in modeling sequential data with latent dynamical models is yet to be explored. Here, we propose a novel latent variable model named latent dynamical implicit diffusion processes (LDIDPs), which utilizes implicit diffusion processes to sample from dynamical latent processes and generate sequential observation samples accordingly. We tested LDIDPs on synthetic and simulated neural d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QBSD&#30340;&#23454;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.05989</link><description>&lt;p&gt;
&#22522;&#20110;&#22235;&#20998;&#20301;&#25968;&#30340;&#23395;&#33410;&#24615;&#20998;&#35299;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quartile-Based Seasonality Decomposition for Time Series Forecasting and Anomaly Detection. (arXiv:2306.05989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QBSD&#30340;&#23454;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#20449;&#39046;&#22495;&#65292;&#21450;&#26102;&#26816;&#27979;&#24322;&#24120;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#26377;&#21161;&#20110;&#35782;&#21035;&#21644;&#34920;&#24449;&#19981;&#35268;&#21017;&#27169;&#24335;&#12289;&#24322;&#24120;&#34892;&#20026;&#21644;&#32593;&#32476;&#24322;&#24120;&#65292;&#20174;&#32780;&#25552;&#39640;&#26381;&#21153;&#36136;&#37327;&#21644;&#25805;&#20316;&#25928;&#29575;&#12290;&#31934;&#30830;&#22320;&#39044;&#27979;&#21644;&#28040;&#38500;&#21487;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#26159;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#22235;&#20998;&#20301;&#25968;&#30340;&#23395;&#33410;&#24615;&#20998;&#35299;&#65288;QBSD&#65289;&#30340;&#23454;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#39044;&#27979;&#20934;&#30830;&#29575;&#20043;&#38388;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;QBSD&#19982;&#29616;&#26377;&#39044;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#21450;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The timely detection of anomalies is essential in the telecom domain as it facilitates the identification and characterization of irregular patterns, abnormal behaviors, and network anomalies, contributing to enhanced service quality and operational efficiency. Precisely forecasting and eliminating predictable time series patterns constitutes a vital component of time series anomaly detection. While the state-of-the-art methods aim to maximize forecasting accuracy, the computational performance takes a hit. In a system composed of a large number of time series variables, e.g., cell Key Performance Indicators (KPIs), the time and space complexity of the forecasting employed is of crucial importance. Quartile-Based Seasonality Decomposition (QBSD) is a live forecasting method proposed in this paper to make an optimal trade-off between computational complexity and forecasting accuracy. This paper compares the performance of QBSD to the state-of-the-art forecasting methods and their applic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#33410;&#28857;&#20043;&#38388;&#25104;&#23545;&#20132;&#20114;&#30340;&#27700;&#24179;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20855;&#26377;&#19968;&#23450;&#23481;&#37327;&#30340;MPNN&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#33410;&#28857;&#29305;&#24449;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20102;&#20445;&#35777;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#20805;&#20998;&#36890;&#20449;&#65292;MPNN&#30340;&#23481;&#37327;&#24517;&#39035;&#26159;...</title><link>http://arxiv.org/abs/2306.03589</link><description>&lt;p&gt;
&#36807;&#24230;&#21387;&#32553;&#22914;&#20309;&#24433;&#21709;GNN&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How does over-squashing affect the power of GNNs?. (arXiv:2306.03589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27979;&#37327;&#33410;&#28857;&#20043;&#38388;&#25104;&#23545;&#20132;&#20114;&#30340;&#27700;&#24179;&#65292;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20855;&#26377;&#19968;&#23450;&#23481;&#37327;&#30340;MPNN&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#33410;&#28857;&#29305;&#24449;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20102;&#20445;&#35777;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#20805;&#20998;&#36890;&#20449;&#65292;MPNN&#30340;&#23481;&#37327;&#24517;&#39035;&#26159;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#26368;&#27969;&#34892;&#30340;GNN&#31867;&#21035;&#26159;&#36890;&#36807;&#30456;&#37051;&#33410;&#28857;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#26469;&#25805;&#20316;&#30340;&#65292;&#31216;&#20026;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#12290;&#37492;&#20110;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20102;&#35299;MPNN&#30340;&#34920;&#36798;&#33021;&#21147;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#32467;&#26524;&#36890;&#24120;&#32771;&#34385;&#20855;&#26377;&#26080;&#20449;&#24687;&#33410;&#28857;&#29305;&#24449;&#30340;&#29615;&#22659;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#30830;&#23450;&#20855;&#26377;&#19968;&#23450;&#23481;&#37327;&#30340;MPNN&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#33410;&#28857;&#29305;&#24449;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;MPNN&#20801;&#35768;&#30340;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20132;&#20114;&#27700;&#24179;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;&#35813;&#27979;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#29305;&#24615;&#65292;&#21363;&#25152;&#35859;&#30340;&#36807;&#24230;&#21387;&#32553;&#25928;&#24212;&#65292;&#35813;&#25928;&#24212;&#34987;&#35266;&#23519;&#21040;&#26159;&#24403;&#22823;&#37327;&#30340;&#20449;&#24687;&#32858;&#21512;&#25104;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#26102;&#21457;&#29983;&#30340;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#27979;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#20026;&#20102;&#20445;&#35777;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#20805;&#20998;&#36890;&#20449;&#65292;MPNN&#30340;&#23481;&#37327;&#24517;&#39035;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01631</link><description>&lt;p&gt;
Gode -- &#23558;&#29983;&#29289;&#21270;&#23398;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21040;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#21457;&#23637;&#21644;&#29702;&#35299;&#21270;&#23398;&#29289;&#36136;&#21644;&#29983;&#29289;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889; (KG) &#30340;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#20998;&#23376;&#32423;&#21644; KG &#32423;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312; 11 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18477</link><description>&lt;p&gt;
&#36229;&#36234;&#20803;&#25968;&#25454;&#65306;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#36827;&#34892;&#36328;&#29256;&#26412;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic Esport Analytics. (arXiv:2305.18477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#29256;&#26412;&#30340;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28216;&#25103;&#35774;&#35745;&#21442;&#25968;&#24182;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21019;&#24314;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#26469;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#30701;&#23551;&#21629;&#30340;&#38382;&#39064;&#12290;&#20197;Dota 2&#20026;&#20363;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#26159;&#20840;&#29699;&#28216;&#25103;&#24066;&#22330;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#19988;&#26159;&#22686;&#38271;&#26368;&#24555;&#30340;&#28216;&#25103;&#32454;&#20998;&#39046;&#22495;&#12290;&#36825;&#23548;&#33268;&#20102;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#30340;&#39046;&#22495;&#20135;&#29983;&#65292;&#20854;&#20351;&#29992;&#28216;&#25103;&#25552;&#21462;&#30340;&#36965;&#27979;&#25968;&#25454;&#26469;&#20026;&#29609;&#23478;&#12289;&#25945;&#32451;&#12289;&#25773;&#38899;&#21592;&#21644;&#20854;&#20182;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#20449;&#24687;&#12290;&#19982;&#20256;&#32479;&#30340;&#20307;&#32946;&#27604;&#36187;&#30456;&#27604;&#65292;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#30340;&#26426;&#21046;&#21644;&#35268;&#21017;&#32463;&#24120;&#21457;&#29983;&#24555;&#36895;&#21464;&#21270;&#12290;&#30001;&#20110;&#28216;&#25103;&#21442;&#25968;&#30340;&#39057;&#32321;&#26356;&#25913;&#65292;&#30005;&#23376;&#31454;&#25216;&#20998;&#26512;&#27169;&#22411;&#30340;&#20351;&#29992;&#23551;&#21629;&#21487;&#33021;&#24456;&#30701;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#30053;&#20102;&#12290;&#26412;&#25991;&#25552;&#21462;&#28216;&#25103;&#35774;&#35745;&#20449;&#24687;&#65288;&#21363;&#34917;&#19969;&#35828;&#26126;&#65289;&#65292;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#24418;&#24335;&#12290;&#20197;Dota 2&#28216;&#25103;&#20013;&#20987;&#26432;&#27425;&#25968;&#30340;&#39044;&#27979;&#20026;&#26696;&#20363;&#65292;&#21033;&#29992;&#36825;&#31181;&#21019;&#26032;&#30340;&#35282;&#33394;&#34920;&#24449;&#25216;&#26415;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#27492;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21253;&#25324;&#24120;&#35268;&#25216;&#26415;&#22312;&#20869;&#30340;&#20004;&#20010;&#19981;&#21516;&#22522;&#32447;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#27700;&#24179;&#65292;&#36824;&#20811;&#26381;&#20102;&#30005;&#23376;&#31454;&#25216;&#28216;&#25103;&#20013;&#29256;&#26412;&#26356;&#36845;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Esport games comprise a sizeable fraction of the global games market, and is the fastest growing segment in games. This has given rise to the domain of esports analytics, which uses telemetry data from games to inform players, coaches, broadcasters and other stakeholders. Compared to traditional sports, esport titles change rapidly, in terms of mechanics as well as rules. Due to these frequent changes to the parameters of the game, esport analytics models can have a short life-spam, a problem which is largely ignored within the literature. This paper extracts information from game design (i.e. patch notes) and utilises clustering techniques to propose a new form of character representation. As a case study, a neural network model is trained to predict the number of kills in a Dota 2 match utilising this novel character representation technique. The performance of this model is then evaluated against two distinct baselines, including conventional techniques. Not only did the model signi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25968;&#20540;&#32534;&#30721;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#21644;SRG&#31639;&#27861;&#26469;&#29983;&#25104;&#35299;&#37322;&#24615;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18437</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#22312;&#31867;&#21035;&#21644;&#28151;&#21512;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65306;&#26080;&#25439;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Explainable Machine Learning for Categorical and Mixed Data with Lossless Visualization. (arXiv:2305.18437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#25968;&#20540;&#32534;&#30721;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#21644;SRG&#31639;&#27861;&#26469;&#29983;&#25104;&#35299;&#37322;&#24615;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#28151;&#21512;&#25968;&#25454;&#26500;&#24314;&#20934;&#30830;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19968;&#30452;&#26159;&#31639;&#27861;&#38754;&#23545;&#38750;&#25968;&#20540;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#20540;&#32534;&#30721;&#26041;&#26696;&#21644;&#26080;&#25439;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25903;&#25345;&#20934;&#30830;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#22810;&#20998;&#31867;&#27169;&#22411;&#21644;&#28436;&#31034;&#20854;&#37325;&#35201;&#20316;&#29992;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#28151;&#21512;&#25968;&#25454;&#31867;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#21253;&#65292;&#20197;&#23545;&#28151;&#21512;&#25968;&#25454;&#30340;&#25152;&#26377;&#20869;&#37096;&#25805;&#20316;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#39034;&#24207;&#35268;&#21017;&#29983;&#25104;&#65288;SRG&#65289;&#8221;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#35745;&#31639;&#23454;&#39564;&#20013;&#25104;&#21151;&#35780;&#20272;&#35813;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building accurate and interpretable Machine Learning (ML) models for heterogeneous/mixed data is a long-standing challenge for algorithms designed for numeric data. This work focuses on developing numeric coding schemes for non-numeric attributes for ML algorithms to support accurate and explainable ML models, methods for lossless visualization of n-D non-numeric categorical data with visual rule discovery in these visualizations, and accurate and explainable ML models for categorical data. This study proposes a classification of mixed data types and analyzes their important role in Machine Learning. It presents a toolkit for enforcing interpretability of all internal operations of ML algorithms on mixed data with a visual data exploration on mixed data. A new Sequential Rule Generation (SRG) algorithm for explainable rule generation with categorical data is proposed and successfully evaluated in multiple computational experiments. This work is one of the steps to the full scope ML alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15121</link><description>&lt;p&gt;
&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#36827;&#34892;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#36229;&#36234;&#20010;&#20307;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Individual Input for Deep Anomaly Detection on Tabular Data. (arXiv:2305.15121v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#32593;&#32476;&#23433;&#20840;&#31561;&#21508;&#20010;&#39046;&#22495;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38750;&#21442;&#25968;&#36716;&#25442;&#22120;&#65288;NPTs&#65289;&#30340;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#25429;&#25417;&#29305;&#24449;&#19982;&#29305;&#24449;&#20043;&#38388;&#20197;&#21450;&#26679;&#26412;&#19982;&#26679;&#26412;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#22522;&#20110;&#37325;&#26500;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;NPT&#26469;&#37325;&#26500;&#27491;&#24120;&#26679;&#26412;&#30340;&#36974;&#34109;&#29305;&#24449;&#12290;&#20197;&#38750;&#21442;&#25968;&#21270;&#26041;&#24335;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;&#25972;&#20010;&#35757;&#32451;&#38598;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#22312;&#29983;&#25104;&#24322;&#24120;&#24471;&#20998;&#26102;&#37325;&#26500;&#36974;&#34109;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#25104;&#21151;&#32467;&#21512;&#29305;&#24449;&#20043;&#38388;&#21644;&#26679;&#26412;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;31&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#24471;&#20998;&#21644;AUROC&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is crucial in various domains, such as finance, healthcare, and cybersecurity. In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies. In a reconstruction-based framework, we train the NPT to reconstruct masked features of normal samples. In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features during to generate an anomaly score. To the best of our knowledge, our proposed method is the first to successfully combine feature-feature and sample-sample dependencies for anomaly detection on tabular datasets. We evaluate our method on an extensive benchmark of 31 tabular datasets and demonstrate that our approach outperforms existing state-of-the-art methods based on the F1-score and AUROC by a signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25581;&#31034;&#20102;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#21644;&#20196;&#20154;&#24974;&#24694;&#30340;&#27169;&#22240;&#65292;&#24182;&#19988;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#30456;&#24403;&#22823;&#27604;&#20363;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#12290;&#20316;&#32773;&#37492;&#23450;&#20102;&#19968;&#20123;&#25991;&#26412;&#25552;&#31034;&#22240;&#32032;&#21644;&#27169;&#22411;&#20542;&#21521;&#22240;&#32032;&#65292;&#20197;&#25581;&#31034;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#29983;&#25104;&#26426;&#29702;&#65292;&#24182;&#19988;&#20984;&#26174;&#20102;&#38656;&#35201;&#32487;&#32493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13873</link><description>&lt;p&gt;
&#19981;&#23433;&#20840;&#25193;&#25955;&#65306;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#21644;&#20196;&#20154;&#24974;&#24694;&#30340;&#27169;&#22240;
&lt;/p&gt;
&lt;p&gt;
Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models. (arXiv:2305.13873v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25581;&#31034;&#20102;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#21644;&#20196;&#20154;&#24974;&#24694;&#30340;&#27169;&#22240;&#65292;&#24182;&#19988;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#30456;&#24403;&#22823;&#27604;&#20363;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#12290;&#20316;&#32773;&#37492;&#23450;&#20102;&#19968;&#20123;&#25991;&#26412;&#25552;&#31034;&#22240;&#32032;&#21644;&#27169;&#22411;&#20542;&#21521;&#22240;&#32032;&#65292;&#20197;&#25581;&#31034;&#19981;&#23433;&#20840;&#20869;&#23481;&#30340;&#29983;&#25104;&#26426;&#29702;&#65292;&#24182;&#19988;&#20984;&#26174;&#20102;&#38656;&#35201;&#32487;&#32493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stable Diffusion&#21644;DALLE&#183;2&#31561;&#26368;&#26032;&#30340;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#20154;&#20204;&#30340;&#35270;&#35273;&#20869;&#23481;&#29983;&#25104;&#26041;&#24335;&#12290;&#21516;&#26102;&#65292;&#31038;&#20250;&#23545;&#23545;&#25163;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#21644;&#20196;&#20154;&#25285;&#24551;&#30340;&#27169;&#22240;&#23384;&#22312;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#25581;&#31034;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#21644;&#20196;&#20154;&#24974;&#24694;&#30340;&#27169;&#22240;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20116;&#31181;&#31867;&#21035;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#20998;&#31867;&#27861;(&#24615;&#26292;&#21147;&#12289;&#26292;&#21147;&#12289;&#20196;&#20154;&#19981;&#23433;&#12289;&#20196;&#20154;&#24974;&#24694;&#21644;&#25919;&#27835;)&#65292;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#25552;&#31034;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#22235;&#31181;&#20808;&#36827;&#30340;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#29983;&#25104;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#27604;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#30456;&#24403;&#22823;&#27604;&#20363;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#65307;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#22235;&#20010;&#25552;&#31034;&#25968;&#25454;&#38598;&#20013;&#65292;&#25152;&#26377;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#26377;14.56%&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#22312;&#27604;&#36739;&#36825;&#22235;&#31181;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#19981;&#21516;&#30340;&#39118;&#38505;&#27700;&#24179;&#65292;&#20854;&#20013;Stable Diffusion&#26159;&#29983;&#25104;&#19981;&#23433;&#20840;&#20869;&#23481;&#26368;&#23481;&#26131;&#30340;(&#25152;&#26377;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#26377;18.92%&#26159;&#19981;&#23433;&#20840;&#30340;)&#12290;&#37492;&#20110;Stable Diffusion&#30340;&#27969;&#34892;&#21644;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;Stable Diffusion&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#25581;&#31034;&#20854;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#20542;&#21521;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#32463;&#24120;&#23548;&#33268;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#21450;&#27169;&#22411;&#29983;&#25104;&#26576;&#20123;&#31867;&#22411;&#20869;&#23481;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24378;&#35843;&#20102;&#38656;&#35201;&#32487;&#32493;&#30740;&#31350;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#19981;&#23433;&#20840;&#22270;&#20687;&#29983;&#25104;&#20197;&#21450;&#24320;&#21457;&#24378;&#26377;&#21147;&#30340;&#23545;&#31574;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art Text-to-Image models like Stable Diffusion and DALLE$\cdot$2 are revolutionizing how people generate visual content. At the same time, society has serious concerns about how adversaries can exploit such models to generate unsafe images. In this work, we focus on demystifying the generation of unsafe images and hateful memes from Text-to-Image models. We first construct a typology of unsafe images consisting of five categories (sexually explicit, violent, disturbing, hateful, and political). Then, we assess the proportion of unsafe images generated by four advanced Text-to-Image models using four prompt datasets. We find that these models can generate a substantial percentage of unsafe images; across four models and four prompt datasets, 14.56% of all generated images are unsafe. When comparing the four models, we find different risk levels, with Stable Diffusion being the most prone to generating unsafe content (18.92% of all generated images are unsafe). Given Stable 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11095</link><description>&lt;p&gt;
&#28608;&#21457;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#23454;&#29616;&#38646;-shot&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;Whisper&#30340;&#26032;&#20852;&#21151;&#33021;&#65292;&#22312;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;&#27169;&#22411;&#21518;&#65292;&#36866;&#24212;&#20102;&#26410;&#35265;&#36807;&#30340;AVSR&#65292;CS-ASR&#21644;ST&#19977;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#35201;&#20040;&#21033;&#29992;&#21478;&#19968;&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25805;&#20316;&#40664;&#35748;&#25552;&#31034;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#20351;&#36825;&#19977;&#20010;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#21040;45&#65285;&#65292;&#29978;&#33267;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;SotA&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;Whisper&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#21475;&#38899;&#30340;&#20559;&#22909;&#20197;&#21450;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/jasonppy/PromptingWhisper&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
&lt;/p&gt;</description></item><item><title>SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.09781</link><description>&lt;p&gt;
SpecInfer&#65306;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification. (arXiv:2305.09781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09781
&lt;/p&gt;
&lt;p&gt;
SpecInfer&#26159;&#19968;&#31181;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#26469;&#21152;&#36895;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36807;&#31243;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#65292;&#22240;&#27492;&#24555;&#36895;&#21644;&#24265;&#20215;&#22320;&#20026;&#23427;&#20204;&#25552;&#20379;&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;SpecInfer&#65292;&#19968;&#20010;LLM&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#25512;&#27979;&#25512;&#26029;&#21644;&#20196;&#29260;&#26641;&#39564;&#35777;&#21152;&#36895;&#29983;&#25104;&#24335;LLM&#25512;&#26029;&#12290;SpecInfer&#32972;&#21518;&#30340;&#20851;&#38190;&#26159;&#23558;&#21508;&#31181;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#25552;&#21319;&#35843;&#25972;&#65292;&#20849;&#21516;&#39044;&#27979;LLM&#30340;&#36755;&#20986;&#65307; &#39044;&#27979;&#32467;&#26524;&#32452;&#32455;&#25104;&#19968;&#20010;&#20196;&#29260;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#37117;&#34920;&#31034;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26641;&#30340;&#24182;&#34892;&#35299;&#30721;&#26426;&#21046;&#65292;&#20197;LMM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#20196;&#29260;&#26641;&#25152;&#20195;&#34920;&#30340;&#25152;&#26377;&#20505;&#36873;&#20196;&#29260;&#24207;&#21015;&#30340;&#27491;&#30830;&#24615;&#12290;SpecInfer&#20351;&#29992;LLM&#20316;&#20026;&#20196;&#29260;&#26641;&#39564;&#35777;&#22120;&#65292;&#32780;&#19981;&#26159;&#22686;&#37327;&#35299;&#30721;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#20026;&#29983;&#25104;&#24335;LLM&#25552;&#20379;&#26381;&#21153;&#25152;&#38656;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#21516;&#26102;&#21487;&#30830;&#20445;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high computational and memory requirements of generative large language models (LLMs) make it challenging to serve them quickly and cheaply. This paper introduces SpecInfer, an LLM serving system that accelerates generative LLM inference with speculative inference and token tree verification. A key insight behind SpecInfer is to combine various collectively boost-tuned small language models to jointly predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified by the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality.
&lt;/p&gt;</description></item><item><title>CUTS+&#26159;&#19968;&#31181;&#22522;&#20110;Granger&#22240;&#26524;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(MPGNN)&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31895;&#21040;&#32454;&#21457;&#29616;&#65288;C2FD&#65289;&#25216;&#26415;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CUTS+&#22312;&#38750;&#35268;&#21017;&#39640;&#32500;&#25968;&#25454;&#19978;&#30340;&#22240;&#26524;&#21457;&#29616;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.05890</link><description>&lt;p&gt;
CUTS+&#65306;&#22522;&#20110;&#38750;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#39640;&#32500;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
CUTS+: High-dimensional Causal Discovery from Irregular Time-series. (arXiv:2305.05890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05890
&lt;/p&gt;
&lt;p&gt;
CUTS+&#26159;&#19968;&#31181;&#22522;&#20110;Granger&#22240;&#26524;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(MPGNN)&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31895;&#21040;&#32454;&#21457;&#29616;&#65288;C2FD&#65289;&#25216;&#26415;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CUTS+&#22312;&#38750;&#35268;&#21017;&#39640;&#32500;&#25968;&#25454;&#19978;&#30340;&#22240;&#26524;&#21457;&#29616;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#24773;&#22659;&#19979;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982; Granger &#22240;&#26524;&#30456;&#32467;&#21512;&#26469;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#65292;&#20294;&#26159;&#24403;&#36935;&#21040;&#39640;&#32500;&#25968;&#25454;&#26102;&#65292;&#30001;&#20110;&#39640;&#24230;&#20887;&#20313;&#30340;&#32593;&#32476;&#35774;&#35745;&#21644;&#24222;&#22823;&#30340;&#22240;&#26524;&#22270;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#20005;&#37325;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#20540;&#20013;&#30340;&#32570;&#22833;&#26465;&#30446;&#36827;&#19968;&#27493;&#38459;&#30861;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986; CUTS+&#65292;&#23427;&#24314;&#31435;&#22312;&#22522;&#20110; Granger &#22240;&#26524;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861; CUTS &#19978;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#31895;&#21040;&#32454;&#21457;&#29616;&#65288;C2FD&#65289;&#30340;&#25216;&#26415;&#21644;&#21033;&#29992;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPGNN&#65289;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#12289;&#20934;&#23454;&#38469;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; CUTS+ &#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#38750;&#35268;&#21017;&#39640;&#32500;&#25968;&#25454;&#19978;&#22823;&#22823;&#25552;&#39640;&#20102;&#22240;&#26524;&#21457;&#29616;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery in time-series is a fundamental problem in the machine learning community, enabling causal reasoning and decision-making in complex scenarios. Recently, researchers successfully discover causality by combining neural networks with Granger causality, but their performances degrade largely when encountering high-dimensional data because of the highly redundant network design and huge causal graphs. Moreover, the missing entries in the observations further hamper the causal structural learning. To overcome these limitations, We propose CUTS+, which is built on the Granger-causality-based causal discovery method CUTS and raises the scalability by introducing a technique called Coarse-to-fine-discovery (C2FD) and leveraging a message-passing-based graph neural network (MPGNN). Compared to previous methods on simulated, quasi-real, and real datasets, we show that CUTS+ largely improves the causal discovery performance on high-dimensional data with different types of irregula
&lt;/p&gt;</description></item><item><title>Echoes&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20559;&#24046;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#22788;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04043</link><description>&lt;p&gt;
Echoes: &#22522;&#20110;&#20266;&#20559;&#24046;&#26631;&#35760;&#30340;&#27169;&#20223;&#24335;&#22238;&#22768;&#23460;&#26080;&#30417;&#30563;&#21435;&#20559;
&lt;/p&gt;
&lt;p&gt;
Echoes: Unsupervised Debiasing via Pseudo-bias Labeling in an Echo Chamber. (arXiv:2305.04043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04043
&lt;/p&gt;
&lt;p&gt;
Echoes&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#25454;&#38598;&#20013;&#20559;&#24046;&#29305;&#24449;&#30340;&#19968;&#33268;&#24615;&#22788;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#26292;&#38706;&#20110;&#26377;&#20559;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#36890;&#24120;&#20250;&#23398;&#20064;&#21040;&#19981;&#27491;&#30830;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#25299;&#23637;&#39046;&#22495;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;Echoes&#8221;&#30340;&#31616;&#21333;&#39640;&#25928;&#26041;&#27861;&#65292;&#23427;&#29983;&#25104;&#20559;&#24046;&#23545;&#31435;&#26679;&#26412;&#30340;&#20266;&#20559;&#24046;&#26631;&#31614;&#65292;&#20197;&#24378;&#21046;&#20351;&#20266;&#26631;&#31614;&#19982;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#24046;&#29305;&#24449;&#19968;&#33268;&#65292;&#24182;&#29992;&#20110;&#21435;&#20559;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;Echoes&#23454;&#29616;&#20102;&#21508;&#39033;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks often learn spurious correlations when exposed to biased training data, leading to poor performance on out-of-distribution data. A biased dataset can be divided, according to biased features, into bias-aligned samples (i.e., with biased features) and bias-conflicting samples (i.e., without biased features). Recent debiasing works typically assume that no bias label is available during the training phase, as obtaining such information is challenging and labor-intensive. Following this unsupervised assumption, existing methods usually train two models: a biased model specialized to learn biased features and a target model that uses information from the biased model for debiasing. This paper first presents experimental analyses revealing that the existing biased models overfit to bias-conflicting samples in the training data, which negatively impacts the debiasing performance of the target models. To address this issue, we propose a straightforward and effective method cal
&lt;/p&gt;</description></item><item><title>&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.01397</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#19981;&#21464;&#27169;&#22411;&#21644;&#34920;&#31034;&#26159;&#21542;&#20844;&#24179;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are demographically invariant models and representations in medical imaging fair?. (arXiv:2305.01397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01397
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#21307;&#23398;&#25104;&#20687;&#27169;&#22411;&#22312;&#20854;&#28508;&#22312;&#34920;&#31034;&#20013;&#32534;&#30721;&#20102;&#26377;&#20851;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#20449;&#24687;&#65288;&#24180;&#40836;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#65292;&#36825;&#24341;&#21457;&#20102;&#26377;&#20851;&#20854;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#21487;&#34892;&#21644;&#20540;&#24471;&#35757;&#32451;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#19981;&#21516;&#31867;&#22411;&#30340;&#19982;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#30340;&#19981;&#21464;&#24615;&#65292;&#21363;&#36793;&#38469;&#12289;&#31867;&#26465;&#20214;&#21644;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#24182;&#35828;&#26126;&#23427;&#20204;&#19982;&#31639;&#27861;&#20844;&#24179;&#30340;&#26631;&#20934;&#27010;&#24565;&#30340;&#31561;&#20215;&#24615;&#12290;&#26681;&#25454;&#29616;&#26377;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#38469;&#21644;&#31867;&#26465;&#20214;&#30340;&#19981;&#21464;&#24615;&#21487;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#26576;&#20123;&#20844;&#24179;&#27010;&#24565;&#30340;&#36807;&#24230;&#38480;&#21046;&#26041;&#27861;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#25439;&#22833;&#12290;&#20851;&#20110;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#65292;&#23450;&#20041;&#21307;&#23398;&#22270;&#20687;&#21453;&#20107;&#23454;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#29978;&#33267;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging models have been shown to encode information about patient demographics (age, race, sex) in their latent representation, raising concerns about their potential for discrimination. Here, we ask whether it is feasible and desirable to train models that do not encode demographic attributes. We consider different types of invariance with respect to demographic attributes marginal, class-conditional, and counterfactual model invariance - and lay out their equivalence to standard notions of algorithmic fairness. Drawing on existing theory, we find that marginal and class-conditional invariance can be considered overly restrictive approaches for achieving certain fairness notions, resulting in significant predictive performance losses. Concerning counterfactual model invariance, we note that defining medical image counterfactuals with respect to demographic attributes is fraught with complexities. Finally, we posit that demographic encoding may even be considered advantageou
&lt;/p&gt;</description></item><item><title>DuETT&#26159;&#19968;&#20010;&#29992;&#20110;EHR&#30340;&#21452;&#37325;&#20107;&#20214;&#26102;&#38388;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30456;&#21516;&#26102;&#38388;&#27493;&#30340;&#19981;&#21516;&#34920;&#31034;&#65292;&#36991;&#20813;&#30001;&#20110;&#26102;&#38388;&#27493;&#20240;&#22823;&#32780;&#20135;&#29983;&#30340;&#20108;&#27425;&#25918;&#32553;&#38382;&#39064;&#65292;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;EHR&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#24448;&#30340;&#21333;&#19968;&#20219;&#21153;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13017</link><description>&lt;p&gt;
DuETT: &#21452;&#37325;&#20107;&#20214;&#26102;&#38388;&#21464;&#25442;&#22120;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;
&lt;/p&gt;
&lt;p&gt;
DuETT: Dual Event Time Transformer for Electronic Health Records. (arXiv:2304.13017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13017
&lt;/p&gt;
&lt;p&gt;
DuETT&#26159;&#19968;&#20010;&#29992;&#20110;EHR&#30340;&#21452;&#37325;&#20107;&#20214;&#26102;&#38388;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30456;&#21516;&#26102;&#38388;&#27493;&#30340;&#19981;&#21516;&#34920;&#31034;&#65292;&#36991;&#20813;&#30001;&#20110;&#26102;&#38388;&#27493;&#20240;&#22823;&#32780;&#20135;&#29983;&#30340;&#20108;&#27425;&#25918;&#32553;&#38382;&#39064;&#65292;&#24182;&#22312;&#22235;&#20010;&#22522;&#20934;EHR&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#24448;&#30340;&#21333;&#19968;&#20219;&#21153;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#38498;&#35774;&#32622;&#20013;&#35760;&#24405;&#30340;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#36890;&#24120;&#21253;&#21547;&#24191;&#27867;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#29305;&#24449;&#26159;&#39640;&#31232;&#30095;&#24615;&#21644;&#19981;&#35268;&#21017;&#35266;&#23519;&#12290;&#36825;&#20123;&#25968;&#25454;&#30340;&#26377;&#25928;&#24314;&#27169;&#24517;&#39035;&#21033;&#29992;&#20854;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;&#65292;&#19981;&#21516;&#31867;&#22411;&#35266;&#23519;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#20197;&#21450;&#25968;&#25454;&#20013;&#31232;&#30095;&#24615;&#32467;&#26500;&#20013;&#30340;&#20449;&#24687;&#12290;&#33258;&#30417;&#30563;&#21464;&#21387;&#22120;&#22312;NLP&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#31181;&#32467;&#26500;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21253;&#21547;&#20004;&#20010;&#32500;&#24230;&#19978;&#30340;&#32467;&#26500;&#21270;&#20851;&#31995;&#65306;&#26102;&#38388;&#21644;&#35760;&#24405;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#32780;&#30452;&#25509;&#23558;&#21464;&#21387;&#22120;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21017;&#19981;&#33021;&#21033;&#29992;&#36825;&#31181;&#29420;&#29305;&#30340;&#32467;&#26500;&#12290;&#33258;&#25105;&#27880;&#24847;&#21147;&#23618;&#30340;&#20108;&#27425;&#25918;&#32553;&#36824;&#21487;&#20197;&#26174;&#30528;&#38480;&#21046;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#27809;&#26377;&#36866;&#24403;&#30340;&#36755;&#20837;&#24037;&#31243;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DuETT&#26550;&#26500;&#65292;&#36825;&#26159;&#21464;&#21387;&#22120;&#30340;&#25193;&#23637;&#65292;&#35774;&#35745;&#29992;&#20110;&#22312;EHR&#30340;&#26102;&#38388;&#21644;&#20107;&#20214;&#31867;&#22411;&#32500;&#24230;&#19978;&#20851;&#27880;&#12290;DuETT&#20351;&#29992;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30456;&#21516;&#26102;&#38388;&#27493;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20869;&#26680;&#26041;&#27861;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#65292;DuETT&#36991;&#20813;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#25918;&#32553;&#65292;&#24182;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#26102;&#38388;&#27493;&#39588;&#12290;DuETT&#22312;&#22235;&#20010;&#22522;&#20934;EHR&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#21333;&#19968;&#20219;&#21153;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) recorded in hospital settings typically contain a wide range of numeric time series data that is characterized by high sparsity and irregular observations. Effective modelling for such data must exploit its time series nature, the semantic relationship between different types of observations, and information in the sparsity structure of the data. Self-supervised Transformers have shown outstanding performance in a variety of structured tasks in NLP and computer vision. But multivariate time series data contains structured relationships over two dimensions: time and recorded event type, and straightforward applications of Transformers to time series data do not leverage this distinct structure. The quadratic scaling of self-attention layers can also significantly limit the input sequence length without appropriate input engineering. We introduce the DuETT architecture, an extension of Transformers designed to attend over both time and event type dimensio
&lt;/p&gt;</description></item><item><title>STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08503
&lt;/p&gt;
&lt;p&gt;
STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;(STO)&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#26088;&#22312;&#21033;&#29992;&#20648;&#23384;&#22312;&#25968;&#25454;&#24211;&#20013;&#20197;&#21069;&#27714;&#35299;&#30340;&#20248;&#21270;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31639;&#27861;&#35774;&#35745;&#24050;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;STO&#20013;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#24182;&#19981;&#23436;&#21892;&#12290;&#23427;&#20204;&#24448;&#24448;&#26159;&#30001;&#20854;&#20182;&#22522;&#20934;&#20989;&#25968;&#38543;&#26426;&#32452;&#21512;&#32780;&#25104;&#65292;&#36825;&#20123;&#22522;&#20934;&#20989;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20339;&#20540;&#65292;&#25110;&#32773;&#29983;&#25104;&#33258;&#34920;&#29616;&#20986;&#26377;&#38480;&#21464;&#21270;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#25163;&#21160;&#37197;&#32622;&#30340;&#65292;&#22240;&#27492;&#21333;&#35843;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#34920;&#24449;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20123;&#34920;&#24449;STO&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#40065;&#26834;&#26041;&#27861;&#23454;&#29616;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.06783</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#40065;&#26834;&#26041;&#27861;&#23454;&#29616;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Distributionally Robust Approach to Regret Optimal Control using the Wasserstein Distance. (arXiv:2304.06783v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06783
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#40065;&#26834;&#26041;&#27861;&#23454;&#29616;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#26041;&#27861;&#30340;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#35813;&#31995;&#32479;&#21463;&#21040;&#38543;&#26426;&#21152;&#24615;&#25200;&#21160;&#24433;&#21709;&#65292;&#25104;&#26412;&#20026;&#20108;&#27425;&#22411;&#12290;&#25200;&#21160;&#36807;&#31243;&#30340;&#27010;&#29575;&#20998;&#24067;&#26410;&#30693;&#65292;&#20294;&#20551;&#23450;&#20301;&#20110;&#32473;&#23450;&#30340;&#20998;&#24067;&#29699;&#20869;&#65292;&#23450;&#20041;&#20026;&#20108;&#38454;Wasserstein&#36317;&#31163;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#35774;&#35745;&#20855;&#26377;&#20005;&#26684;&#22240;&#26524;&#32447;&#24615;&#25200;&#21160;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#20197;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39044;&#26399;&#21518;&#24724;&#12290;&#25511;&#21046;&#22120;&#25152;&#20135;&#29983;&#30340;&#21518;&#24724;&#26159;&#25351;&#23427;&#23545;&#25239;&#25200;&#21160;&#26102;&#20135;&#29983;&#30340;&#25104;&#26412;&#19982;&#26368;&#20248;&#38750;&#22240;&#26524;&#25511;&#21046;&#22120;&#22312;&#19968;&#24320;&#22987;&#23601;&#26377;&#23436;&#20840;&#20102;&#35299;&#25200;&#21160;&#36807;&#31243;&#21518;&#20135;&#29983;&#30340;&#25104;&#26412;&#20043;&#24046;&#12290;&#24314;&#31435;&#22312;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#23545;&#20598;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#31561;&#20215;&#22320;&#25913;&#20889;&#36825;&#20010;&#26497;&#23567;&#26497;&#22823;&#30340;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a distributionally robust approach to regret optimal control of discrete-time linear dynamical systems with quadratic costs subject to stochastic additive disturbance on the state process. The underlying probability distribution of the disturbance process is unknown, but assumed to lie in a given ball of distributions defined in terms of the type-2 Wasserstein distance. In this framework, strictly causal linear disturbance feedback controllers are designed to minimize the worst-case expected regret. The regret incurred by a controller is defined as the difference between the cost it incurs in response to a realization of the disturbance process and the cost incurred by the optimal noncausal controller which has perfect knowledge of the disturbance process realization at the outset. Building on a well-established duality theory for optimal transport problems, we show how to equivalently reformulate this minimax regret optimal control problem as a tractable semidefini
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30721;&#29575;-&#22833;&#30495;&#29702;&#35770;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#32423;&#20998;&#31867;&#20013;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#23398;&#20064;&#25968;&#25454;&#26679;&#26412;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.04137</link><description>&lt;p&gt;
RD-DPP: &#30721;&#29575;-&#22833;&#30495;&#29702;&#35770;&#19982;&#20998;&#24067;&#24335;&#28857;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#22810;&#26679;&#21270;&#23398;&#20064;&#25968;&#25454;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
RD-DPP: Rate-Distortion Theory Meets Determinantal Point Process to Diversify Learning Data Samples. (arXiv:2304.04137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30721;&#29575;-&#22833;&#30495;&#29702;&#35770;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#28857;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#32423;&#20998;&#31867;&#20013;&#36873;&#25321;&#22810;&#26679;&#21270;&#30340;&#23398;&#20064;&#25968;&#25454;&#26679;&#26412;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20123;&#23454;&#38469;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22914;&#20132;&#36890;&#35270;&#39057;&#20998;&#26512;&#65292;&#21487;&#29992;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#21463;&#21040;&#19981;&#21516;&#22240;&#32032;&#30340;&#38480;&#21046;&#65292;&#22914;&#26377;&#38480;&#30340;&#36890;&#20449;&#24102;&#23485;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#36873;&#25321;&#23545;&#23398;&#20064;&#31995;&#32479;&#36136;&#37327;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26679;&#26412;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36873;&#25321;&#22810;&#26679;&#21270;&#26679;&#26412;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#20998;&#24067;&#24335;&#28857;&#36807;&#31243; (DPP)&#12290;&#28982;&#32780;&#65292;DPP&#23384;&#22312;&#19968;&#20123;&#24050;&#30693;&#30340;&#32570;&#28857;&#65292;&#20363;&#22914;&#23558;&#26679;&#26412;&#25968;&#37327;&#38480;&#21046;&#20026;&#30456;&#20284;&#24615;&#30697;&#38453;&#30340;&#31209;&#65292;&#24182;&#19988;&#19981;&#33021;&#20026;&#29305;&#23450;&#30340;&#23398;&#20064;&#20219;&#21153;&#65288;&#20363;&#22914;&#22810;&#32423;&#20998;&#31867;&#20219;&#21153;&#65289;&#23450;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30721;&#29575;-&#22833;&#30495; (RD) &#29702;&#35770;&#30340;&#34913;&#37327;&#20219;&#21153;&#23450;&#21521;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22810;&#32423;&#20998;&#31867;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;DPP&#21644;RD&#29702;&#35770;&#20043;&#38388;&#30340;&#22522;&#26412;&#20851;&#31995;&#65292;&#35774;&#35745;&#20102;RD-DPP&#65292;&#19968;&#31181;&#22522;&#20110;RD&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#22686;&#30410;&#12290;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;&#19968;&#20123;&#29305;&#23450;&#24773;&#20917;&#19979;RD-DPP&#30340;&#38381;&#21512;&#35299;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#26679;&#24615;&#20248;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some practical learning tasks, such as traffic video analysis, the number of available training samples is restricted by different factors, such as limited communication bandwidth and computation power; therefore, it is imperative to select diverse data samples that contribute the most to the quality of the learning system. One popular approach to selecting diverse samples is Determinantal Point Process (DPP). However, it suffers from a few known drawbacks, such as restriction of the number of samples to the rank of the similarity matrix, and not being customizable for specific learning tasks (e.g., multi-level classification tasks). In this paper, we propose a new way of measuring task-oriented diversity based on the Rate-Distortion (RD) theory, appropriate for multi-level classification. To this end, we establish a fundamental relationship between DPP and RD theory, which led to designing RD-DPP, an RD-based value function to evaluate the diversity gain of data samples. We also ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#20307;&#31995;&#32467;&#26500; V-&#22810;&#38754;&#20307;&#21487;&#35777;&#26126;&#20462;&#22797;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#20462;&#22797;&#21482;&#20462;&#25913; DNN &#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#25903;&#25345;&#22810;&#31181;&#31867;&#22411;&#30340;&#23618;&#65292;&#24182;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2304.03496</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20445;&#20307;&#31995;&#32467;&#26500;&#21487;&#35777;&#26126;&#20462;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Architecture-Preserving Provable Repair of Deep Neural Networks. (arXiv:2304.03496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#20307;&#31995;&#32467;&#26500; V-&#22810;&#38754;&#20307;&#21487;&#35777;&#26126;&#20462;&#22797;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#20462;&#22797;&#21482;&#20462;&#25913; DNN &#30340;&#21442;&#25968;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#25903;&#25345;&#22810;&#31181;&#31867;&#22411;&#30340;&#23618;&#65292;&#24182;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#25104;&#20026;&#20102;&#36719;&#20214;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#35768;&#22810;&#38382;&#39064;&#65288;&#22914;&#22270;&#20687;&#35782;&#21035;&#65289;&#30340;&#26368;&#20808;&#36827;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;DNN &#36828;&#38750;&#19981;&#21487;&#38169;&#35823;&#65292;DNN &#30340;&#19981;&#27491;&#30830;&#34892;&#20026;&#21487;&#33021;&#20250;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36896;&#25104;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20445;&#20307;&#31995;&#32467;&#26500; V-&#22810;&#38754;&#20307;&#21487;&#35777;&#26126;&#20462;&#22797; DNNs &#30340;&#38382;&#39064;&#12290;V-&#22810;&#38754;&#20307;&#20351;&#29992;&#20854;&#39030;&#28857;&#34920;&#31034;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010;&#20984;&#32422;&#26463;&#22810;&#38754;&#20307;&#12290;V-&#22810;&#38754;&#20307;&#21487;&#35777;&#26126;&#20462;&#22797;&#20445;&#35777;&#20462;&#22797;&#21518;&#30340; DNN &#28385;&#36275;&#32473;&#23450; V-&#22810;&#38754;&#20307;&#20013;&#26080;&#38480;&#28857;&#38598;&#19978;&#30340;&#35268;&#33539;&#12290;&#20307;&#31995;&#32467;&#26500;&#20445;&#25345;&#20462;&#22797;&#20165;&#20462;&#25913; DNN &#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#20307;&#31995;&#32467;&#26500;&#12290;&#20462;&#22797;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#20462;&#25913; DNN &#30340;&#22810;&#20010;&#23618;&#65292;&#24182;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;&#23427;&#25903;&#25345;&#20855;&#26377;&#19968;&#20123;&#32447;&#24615;&#37096;&#20998;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#21450;&#23436;&#20840;&#36830;&#25509;&#30340;&#12289;&#21367;&#31215;&#30340;&#12289;&#27744;&#21270;&#30340;&#21644;&#27531;&#20313;&#23618;&#30340; DNNs&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#25552;&#20379; DNN &#20307;&#31995;&#32467;&#26500;&#20445;&#25345;&#21487;&#35777;&#26126;&#20462;&#22797;&#30340;&#27491;&#24335;&#26694;&#26550;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are becoming increasingly important components of software, and are considered the state-of-the-art solution for a number of problems, such as image recognition. However, DNNs are far from infallible, and incorrect behavior of DNNs can have disastrous real-world consequences. This paper addresses the problem of architecture-preserving V-polytope provable repair of DNNs. A V-polytope defines a convex bounded polytope using its vertex representation. V-polytope provable repair guarantees that the repaired DNN satisfies the given specification on the infinite set of points in the given V-polytope. An architecture-preserving repair only modifies the parameters of the DNN, without modifying its architecture. The repair has the flexibility to modify multiple layers of the DNN, and runs in polynomial time. It supports DNNs with activation functions that have some linear pieces, as well as fully-connected, convolutional, pooling and residual layers. To the best our 
&lt;/p&gt;</description></item><item><title>ERM++&#26159;&#19968;&#20010;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#26631;&#20934;ERM&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#34920;&#29616;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01973</link><description>&lt;p&gt;
ERM++&#65306;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ERM++: An Improved Baseline for Domain Generalization. (arXiv:2304.01973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01973
&lt;/p&gt;
&lt;p&gt;
ERM++&#26159;&#19968;&#20010;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#26631;&#20934;ERM&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#34920;&#29616;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#22495;&#36890;&#29992;&#24615;&#65288;DG&#65289;&#34913;&#37327;&#20998;&#31867;&#22120;&#23545;&#20110;&#23427;&#27809;&#26377;&#25509;&#21463;&#36807;&#35757;&#32451;&#30340;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#20010;&#35757;&#32451;&#22495;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22810;&#28304;DG&#26041;&#27861;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22495;&#26631;&#31614;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#36807;&#31243;&#65292;&#21363;&#22312;&#28304;&#22495;&#19978;&#31616;&#21333;&#22320;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#21487;&#20197;&#32988;&#36807;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;DG&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#20505;&#36873;&#25216;&#26415;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ERM&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#31216;&#20026;ERM ++&#65292;&#24182;&#23637;&#31034;&#23427;&#30456;&#23545;&#20110;&#26631;&#20934;ERM&#22312;&#20116;&#20010;&#22810;&#28304;&#25968;&#25454;&#38598;&#19978;&#23558;DG&#30340;&#24615;&#33021;&#26174;&#30528;&#25552;&#39640;&#20102;5&#65285;&#20197;&#19978;&#65292;&#24182;&#19988;&#23613;&#31649;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#20294;&#20987;&#36133;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;ERM ++&#22312;WILDS-FMOW&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-source Domain Generalization (DG) measures a classifier's ability to generalize to new distributions of data it was not trained on, given several training domains. While several multi-source DG methods have been proposed, they incur additional complexity during training by using domain labels. Recent work has shown that a well-tuned Empirical Risk Minimization (ERM) training procedure, that is simply minimizing the empirical risk on the source domains, can outperform most existing DG methods. We identify several key candidate techniques to further improve ERM performance, such as better utilization of training data, model parameter selection, and weight-space regularization. We call the resulting method ERM++, and show it significantly improves the performance of DG on five multi-source datasets by over 5% compared to standard ERM, and beats state-of-the-art despite being less computationally expensive. Additionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.01752</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#23569;&#26679;&#26412;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Black Box Few-Shot Adaptation for Vision-Language models. (arXiv:2304.01752v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#36719;&#25552;&#31034;&#23398;&#20064;&#26159;&#23569;&#26679;&#26412;&#39046;&#22495;&#36866;&#29992;&#30340;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26032;&#39046;&#22495;&#24341;&#21457;&#30340;&#20998;&#24067;&#20559;&#31227;&#26469;&#32553;&#23567;&#27169;&#24577;&#24046;&#36317;&#12290;&#34429;&#28982;&#35813;&#26041;&#27861;&#24615;&#33021;&#39640;&#25928;&#65292;&#20294;&#20173;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#27169;&#22411;&#19978;&#21487;&#33021;&#20250;&#23548;&#33268;&#35745;&#31639;&#19978;&#30340;&#19981;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#21283;&#23376;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#20808;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#30340; V-L &#23569;&#26679;&#26412;&#36866;&#24212;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65292;&#35757;&#32451;&#36895;&#24230;&#24555;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#36866;&#29992;&#20110;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#19988;&#36824;&#21487;&#20197;&#29992;&#20110;&#23545;&#21333;&#27169;&#22411;&#35745;&#31639;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language (V-L) models trained with contrastive learning to align the visual and language modalities have been shown to be strong few-shot learners. Soft prompt learning is the method of choice for few-shot downstream adaption aiming to bridge the modality gap caused by the distribution shift induced by the new domain. While parameter-efficient, prompt learning still requires access to the model weights and can be computationally infeasible for large models with billions of parameters. To address these shortcomings, in this work, we describe a black-box method for V-L few-shot adaptation that (a) operates on pre-computed image and text features and hence works without access to the model's weights, (b) it is orders of magnitude faster at training time, (c) it is amenable to both supervised and unsupervised training, and (d) it can be even used to align image and text features computed from uni-modal models. To achieve this, we propose Linear Feature Alignment (LFA), a simple line
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25429;&#33719;&#20102;&#39318;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#21830;&#29992;&#35774;&#22791;&#30495;&#23454;&#23556;&#39057;&#25351;&#32441;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#20110;&#35782;&#21035;&#38750;&#27861;&#25110;&#26410;&#25480;&#26435;&#21457;&#23556;&#22120;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.13538</link><description>&lt;p&gt;
&#29992;&#20110;&#21830;&#29992;&#35774;&#22791;&#30495;&#23454;&#23556;&#39057;&#25351;&#32441;&#30340;&#34013;&#29273;&#21644;WiFi&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Bluetooth and WiFi Dataset for Real World RF Fingerprinting of Commercial Devices. (arXiv:2303.13538v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25429;&#33719;&#20102;&#39318;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#21830;&#29992;&#35774;&#22791;&#30495;&#23454;&#23556;&#39057;&#25351;&#32441;&#25968;&#25454;&#38598;&#65292;&#36825;&#23545;&#20110;&#35782;&#21035;&#38750;&#27861;&#25110;&#26410;&#25480;&#26435;&#21457;&#23556;&#22120;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#25351;&#32441;&#20316;&#20026;&#19968;&#31181;&#29289;&#29702;&#23618;&#23433;&#20840;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#22312;&#20849;&#20139;RF&#39057;&#35889;&#30340;&#38750;&#27861;&#25110;&#26410;&#25480;&#26435;&#21457;&#23556;&#22120;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#36719;&#20214;&#23450;&#20041;&#26080;&#32447;&#30005;&#65288;SDR&#65289;&#29983;&#25104;&#21512;&#25104;&#27874;&#24418;&#65292;&#36825;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#20165;&#20851;&#27880;&#29983;&#25104;&#19968;&#31181;&#27874;&#24418;&#30340;&#33455;&#29255;&#32452;&#12290;&#21830;&#29992;&#29616;&#25104;&#65288;COTS&#65289;&#32452;&#21512;&#33455;&#29255;&#32452;&#25903;&#25345;&#20351;&#29992;&#20849;&#20139;&#21452;&#39057;&#27573;&#22825;&#32447;&#30340;&#20004;&#31181;&#26080;&#32447;&#26631;&#20934;&#65288;&#20363;&#22914;WiFi&#21644;&#34013;&#29273;&#65289;&#65292;&#22914;&#22312;&#31508;&#35760;&#26412;&#30005;&#33041;&#12289;&#36866;&#37197;&#22120;&#12289;&#26080;&#32447;&#20805;&#30005;&#22120;&#12289;&#26641;&#33683;&#27966;&#31561;IoT&#35774;&#22791;&#20013;&#24191;&#27867;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#36319;&#19978;&#29616;&#20195;IoT&#29615;&#22659;&#30340;&#27493;&#20240;&#65292;&#36843;&#20999;&#38656;&#35201;&#25429;&#33719;&#36825;&#20123;&#32452;&#21512;&#33455;&#29255;&#32452;&#21457;&#23556;&#24322;&#26500;&#36890;&#20449;&#21327;&#35758;&#30340;&#30495;&#23454;&#19990;&#30028;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25429;&#33719;&#20102;&#31532;&#19968;&#20010;&#24050;&#30693;&#30340;&#27492;&#31867;&#20844;&#24320;&#21487;&#35775;&#38382;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#21830;&#19994;&#35774;&#22791;&#30340;COTS Wi-Fi&#21644;&#34013;&#29273;&#32452;&#21512;&#33455;&#29255;&#32452;&#30340;RF&#21457;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
RF fingerprinting is emerging as a physical layer security scheme to identify illegitimate and/or unauthorized emitters sharing the RF spectrum. However, due to the lack of publicly accessible real-world datasets, most research focuses on generating synthetic waveforms with software-defined radios (SDRs) which are not suited for practical deployment settings. On other hand, the limited datasets that are available focus only on chipsets that generate only one kind of waveform. Commercial off-the-shelf (COTS) combo chipsets that support two wireless standards (for example WiFi and Bluetooth) over a shared dual-band antenna such as those found in laptops, adapters, wireless chargers, Raspberry Pis, among others are becoming ubiquitous in the IoT realm. Hence, to keep up with the modern IoT environment, there is a pressing need for real-world open datasets capturing emissions from these combo chipsets transmitting heterogeneous communication protocols. To this end, we capture the first kno
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#28040;&#34701;&#65292;&#21487;&#20197;&#28040;&#38500;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#21644;&#26679;&#26412;&#35760;&#24518;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13516</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#28040;&#34701;
&lt;/p&gt;
&lt;p&gt;
Ablating Concepts in Text-to-Image Diffusion Models. (arXiv:2303.13516v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#28040;&#34701;&#65292;&#21487;&#20197;&#28040;&#38500;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#21644;&#26679;&#26412;&#35760;&#24518;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#32452;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#29255;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#25968;&#37327;&#24222;&#22823;&#30340;&#32593;&#32476;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24448;&#24448;&#21253;&#21547;&#26377;&#29256;&#26435;&#26448;&#26009;&#12289;&#25480;&#26435;&#22270;&#20687;&#21644;&#20010;&#20154;&#29031;&#29255;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#34987;&#21457;&#29616;&#33021;&#22815;&#27169;&#20223;&#19981;&#21516;&#33402;&#26415;&#23478;&#30340;&#39118;&#26684;&#25110;&#35760;&#20303;&#20934;&#30830;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#22914;&#20309;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#36825;&#20123;&#29256;&#26435;&#27010;&#24565;&#25110;&#22270;&#20687;&#65311;&#20026;&#20102;&#36798;&#25104;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23454;&#29616;&#27010;&#24565;&#28040;&#34701;&#65292;&#21363;&#38450;&#27490;&#29983;&#25104;&#30446;&#26631;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23398;&#20064;&#22914;&#20309;&#21305;&#37197;&#19968;&#20010;&#38170;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#22270;&#20687;&#20998;&#24067;&#21644;&#19982;&#30446;&#26631;&#39118;&#26684;&#12289;&#23454;&#20363;&#25110;&#25991;&#26412;&#25552;&#31034;&#30456;&#20851;&#30340;&#22270;&#20687;&#20998;&#24067;&#65292;&#20197;&#38450;&#27490;&#27169;&#22411;&#26681;&#25454;&#20854;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#30446;&#26631;&#27010;&#24565;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#38450;&#27490;&#28040;&#34701;&#27010;&#24565;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated conce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#29255;&#21040;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.10310</link><description>&lt;p&gt;
&#20266;&#30417;&#30563;&#24230;&#37327;&#65306;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pseudo Supervised Metrics: Evaluating Unsupervised Image to Image Translation Models In Unsupervised Cross-Domain Classification Frameworks. (arXiv:2303.10310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#22270;&#29255;&#21040;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#21462;&#20915;&#20110;&#35775;&#38382;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#30340;&#30456;&#21516;&#39046;&#22495;&#19978;&#27979;&#35797;&#25968;&#25454;&#12290;&#24403;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26032;&#25968;&#25454;&#26102;&#65292;&#20998;&#31867;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25910;&#38598;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#24182;&#20174;&#22836;&#35757;&#32451;&#26032;&#20998;&#31867;&#22120;&#32791;&#26102;&#12289;&#26114;&#36149;&#65292;&#26377;&#26102;&#26159;&#19981;&#21487;&#34892;&#25110;&#19981;&#21487;&#33021;&#30340;&#12290;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#22270;&#20687;&#23545;&#22270;&#20687; (UI2I) &#32763;&#35793;&#27169;&#22411;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26410;&#26631;&#35760;&#30340;&#22495;&#36716;&#25442;&#20026;&#26631;&#35760;&#22495;&#26469;&#22788;&#29702;&#36825;&#20010;&#25968;&#25454;&#22495;&#28418;&#31227;&#38382;&#39064;&#12290;&#36825;&#20123;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#38382;&#39064;&#22312;&#20110;&#23427;&#20204;&#26159;&#26080;&#30417;&#30563;&#30340;&#12290;&#30001;&#20110;&#32570;&#23569;&#27880;&#37322;&#65292;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#30340;&#30417;&#30563;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#32763;&#35793;&#27169;&#22411;&#20197;&#36873;&#25321;&#26368;&#20339;&#30340;&#26816;&#26597;&#28857;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20266;&#30417;&#30563;&#24230;&#37327;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#26080;&#30417;&#30563;&#36328;&#22495;&#20998;&#31867;&#26694;&#26550;&#20013; UI2I &#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to classify images accurately and efficiently is dependent on having access to large labeled datasets and testing on data from the same domain that the model is trained on. Classification becomes more challenging when dealing with new data from a different domain, where collecting a large labeled dataset and training a new classifier from scratch is time-consuming, expensive, and sometimes infeasible or impossible. Cross-domain classification frameworks were developed to handle this data domain shift problem by utilizing unsupervised image-to-image (UI2I) translation models to translate an input image from the unlabeled domain to the labeled domain. The problem with these unsupervised models lies in their unsupervised nature. For lack of annotations, it is not possible to use the traditional supervised metrics to evaluate these translation models to pick the best-saved checkpoint model. In this paper, we introduce a new method called Pseudo Supervised Metrics that was desig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#25991;&#26412;&#21040;&#26757;&#23572;&#39057;&#35889;&#29983;&#25104;&#22120;&#30340;&#26080;&#26631;&#35760;&#36716;&#20889;&#39046;&#22495;&#33258;&#36866;&#24212;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#29983;&#25104;&#26757;&#23572;&#39057;&#35889;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26032;&#39046;&#22495;&#30340;&#20165;&#25991;&#26412;&#25968;&#25454;&#26469;&#36866;&#24212;ASR&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;ASR&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#33258;&#36866;&#24212;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#19978;&#36229;&#36807;&#20102;&#32423;&#32852;TTS&#31995;&#32479;&#19982;&#22768;&#30721;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.14036</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#30340;&#25991;&#26412;&#21040;&#26757;&#23572;&#39057;&#35889;&#29983;&#25104;&#22120;&#30340;&#26080;&#26631;&#35760;&#36716;&#20889;&#39046;&#22495;&#33258;&#36866;&#24212;&#31471;&#21040;&#31471;ASR
&lt;/p&gt;
&lt;p&gt;
Text-only domain adaptation for end-to-end ASR using integrated text-to-mel-spectrogram generator. (arXiv:2302.14036v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#25991;&#26412;&#21040;&#26757;&#23572;&#39057;&#35889;&#29983;&#25104;&#22120;&#30340;&#26080;&#26631;&#35760;&#36716;&#20889;&#39046;&#22495;&#33258;&#36866;&#24212;&#31471;&#21040;&#31471;ASR&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#29983;&#25104;&#26757;&#23572;&#39057;&#35889;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26032;&#39046;&#22495;&#30340;&#20165;&#25991;&#26412;&#25968;&#25454;&#26469;&#36866;&#24212;ASR&#27169;&#22411;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;ASR&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#33258;&#36866;&#24212;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#19978;&#36229;&#36807;&#20102;&#32423;&#32852;TTS&#31995;&#32479;&#19982;&#22768;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#29992;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#12289;&#20165;&#26377;&#25991;&#26412;&#30340;&#25968;&#25454;&#25110;&#32773;&#20108;&#32773;&#30340;&#28151;&#21512;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#31181;&#38598;&#25104;&#30340;&#25991;&#26412;&#22522;&#30784;&#35757;&#32451;&#36741;&#21161;&#27169;&#22359;&#12290;&#35813;&#27169;&#22359;&#32467;&#21512;&#20102;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#35828;&#35805;&#20154;&#25991;&#26412;&#21040;&#26757;&#23572;&#39057;&#35889;&#29983;&#25104;&#22120;&#21644;&#22522;&#20110;GAN&#30340;&#22686;&#24378;&#22120;&#65292;&#20197;&#25552;&#39640;&#39057;&#35889;&#36136;&#37327;&#12290;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#29983;&#25104;&#26757;&#23572;&#39057;&#35889;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#26032;&#39046;&#22495;&#30340;&#20165;&#25991;&#26412;&#25968;&#25454;&#65292;&#21487;&#20197;&#23558;ASR&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#19982;&#20165;&#35757;&#32451;&#20110;&#36716;&#24405;&#35821;&#38899;&#30340;&#31995;&#32479;&#30456;&#27604;&#26174;&#33879;&#25552;&#39640;&#20102;ASR&#20934;&#30830;&#24615;&#12290;&#23427;&#36824;&#22312;&#33258;&#36866;&#24212;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#19978;&#36229;&#36807;&#20102;&#32423;&#32852;TTS&#31995;&#32479;&#19982;&#22768;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an end-to-end Automatic Speech Recognition (ASR) system that can be trained on transcribed speech data, text-only data, or a mixture of both. The proposed model uses an integrated auxiliary block for text-based training. This block combines a non-autoregressive multi-speaker text-to-mel-spectrogram generator with a GAN-based enhancer to improve the spectrogram quality. The proposed system can generate a mel-spectrogram dynamically during training. It can be used to adapt the ASR model to a new domain by using text-only data from this domain. We demonstrate that the proposed training method significantly improves ASR accuracy compared to the system trained on transcribed speech only. It also surpasses cascade TTS systems with the vocoder in the adaptation quality and training speed.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;NeRF&#30340;3D&#24863;&#30693;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;3D&#24863;&#30693;&#23545;&#40784;&#21644;&#34701;&#21512;&#26469;&#35299;&#20915;&#36755;&#20837;&#22270;&#20687;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;FFHQ&#21644;AFHQ-Cat&#19978;&#39564;&#35777;&#20102;&#20248;&#20110;&#29616;&#26377;2D&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06608</link><description>&lt;p&gt;
&#20855;&#26377;&#29983;&#25104;&#24335;NeRF&#30340;3D&#24863;&#30693;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
3D-aware Blending with Generative NeRFs. (arXiv:2302.06608v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06608
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;NeRF&#30340;3D&#24863;&#30693;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;3D&#24863;&#30693;&#23545;&#40784;&#21644;&#34701;&#21512;&#26469;&#35299;&#20915;&#36755;&#20837;&#22270;&#20687;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;FFHQ&#21644;AFHQ-Cat&#19978;&#39564;&#35777;&#20102;&#20248;&#20110;&#29616;&#26377;2D&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#34701;&#21512;&#26088;&#22312;&#26080;&#32541;&#22320;&#21512;&#24182;&#22810;&#20010;&#22270;&#20687;&#12290;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;2D&#30340;&#26041;&#27861;&#26469;&#35828;&#65292;&#22914;&#26524;&#36755;&#20837;&#22270;&#20687;&#30001;&#20110;3D&#30456;&#26426;&#23039;&#24577;&#21644;&#29289;&#20307;&#24418;&#29366;&#30340;&#24046;&#24322;&#32780;&#19981;&#23545;&#40784;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;3D&#24863;&#30693;&#34701;&#21512;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;3D&#24863;&#30693;&#23545;&#40784;&#21644;3D&#24863;&#30693;&#34701;&#21512;&#12290;&#23545;&#20110;3D&#24863;&#30693;&#23545;&#40784;&#65292;&#25105;&#20204;&#39318;&#20808;&#20272;&#35745;&#19982;&#29983;&#25104;&#24335;NeRF&#30456;&#20851;&#30340;&#21442;&#32771;&#22270;&#20687;&#30340;&#30456;&#26426;&#23039;&#24577;&#65292;&#28982;&#21518;&#23545;&#27599;&#20010;&#37096;&#20998;&#36827;&#34892;3D&#23616;&#37096;&#23545;&#40784;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21033;&#29992;&#29983;&#25104;&#24335;NeRF&#30340;3D&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;3D&#24863;&#30693;&#30340;&#34701;&#21512;&#65292;&#23427;&#30452;&#25509;&#22312;NeRF&#30340;&#28508;&#22312;&#34920;&#31034;&#31354;&#38388;&#19978;&#36827;&#34892;&#22270;&#20687;&#34701;&#21512;&#65292;&#32780;&#19981;&#26159;&#22312;&#21407;&#22987;&#20687;&#32032;&#31354;&#38388;&#19978;&#36827;&#34892;&#12290;&#36890;&#36807;&#23545;FFHQ&#21644;AFHQ-Cat&#36827;&#34892;&#24191;&#27867;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;2D&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image blending aims to combine multiple images seamlessly. It remains challenging for existing 2D-based methods, especially when input images are misaligned due to differences in 3D camera poses and object shapes. To tackle these issues, we propose a 3D-aware blending method using generative Neural Radiance Fields (NeRF), including two key components: 3D-aware alignment and 3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of the reference image with respect to generative NeRFs and then perform 3D local alignment for each part. To further leverage 3D information of the generative NeRF, we propose 3D-aware blending that directly blends images on the NeRF's latent representation space, rather than raw pixel space. Collectively, our method outperforms existing 2D baselines, as validated by extensive quantitative and qualitative evaluations with FFHQ and AFHQ-Cat.
&lt;/p&gt;</description></item><item><title>&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11562</link><description>&lt;p&gt;
&#39044;&#27979;&#26159;&#21542;&#38543;&#24847;&#65311;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#35780;&#20272;&#33258;&#27965;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290; &#23454;&#35777;&#34920;&#26126;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30340;&#26041;&#24046;&#24046;&#24322;&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#20915;&#31574;&#23454;&#38469;&#19978;&#26159;&#38543;&#24847;&#30340;&#12290; &#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20570;&#20986;&#20102;&#22235;&#20010;&#24635;&#20307;&#36129;&#29486;&#65306;&#25105;&#20204;1&#65289;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#33258;&#27965;&#24615;&#65292;&#22312;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#26102;&#20351;&#29992;&#65307; 2&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#24403;&#39044;&#27979;&#26080;&#27861;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#21487;&#20197;&#25918;&#24323;&#20998;&#31867;&#65307; 3&#65289;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26377;&#20851;&#20844;&#24179;&#20998;&#31867;&#20013;&#26041;&#24046;&#65288;&#30456;&#23545;&#20110;&#33258;&#27965;&#24615;&#21644;&#38543;&#24847;&#24615;&#65289;&#20316;&#29992;&#30340;&#26368;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65307; 4&#65289;&#25512;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#20351;&#32654;&#22269;&#20303;&#25151;&#25269;&#25276;&#36151;&#27454;&#25259;&#38706;&#27861;&#26696;&#65288;HMDA&#65289;&#25968;&#25454;&#38598;&#26131;&#20110;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#12290; &#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25581;&#31034;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#20196;&#20154;&#38663;&#24778;&#30340;&#35265;&#35299;&#12290;&#24403;&#32771;&#34385;&#21040;&#26041;&#24046;&#21644;&#38543;&#24847;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#22823;&#22810;&#25968;&#20844;&#24179;&#20998;&#31867;&#22522;&#20934;&#25509;&#36817;&#20844;&#24179;&#12290; &#20294;&#26159;&#65292;&#19968;&#23567;&#37096;&#20998;&#23454;&#20363;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#38543;&#24847;&#24615;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
&lt;/p&gt;</description></item><item><title>Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.11118</link><description>&lt;p&gt;
Box$^2$EL: EL++&#25551;&#36848;&#36923;&#36753;&#20013;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#26041;&#27861;&#21450;&#20854;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11118
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#36923;&#36753;&#26412;&#20307;&#35770;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#35889;&#19982;&#27010;&#24565;&#20449;&#24687;&#21644;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#36825;&#31181;&#26412;&#20307;&#35770;&#30340;&#24402;&#32435;&#25512;&#29702;&#25216;&#26415;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#36825;&#20123;&#25216;&#26415;&#26377;&#26395;&#34917;&#20805;&#20256;&#32479;&#30340;&#28436;&#32462;&#25512;&#29702;&#31639;&#27861;&#12290;&#31867;&#20284;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23436;&#21892;&#65292;&#29616;&#26377;&#30340;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#26412;&#20307;&#35770;&#23884;&#20837;&#65292;&#21516;&#26102;&#30830;&#20445;&#36825;&#20123;&#23884;&#20837;&#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#24213;&#23618;&#25551;&#36848;&#36923;&#36753;&#30340;&#36923;&#36753;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21463;&#38480;&#30340;&#35282;&#33394;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Box$^2$EL&#26041;&#27861;&#65292;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#37117;&#34920;&#31034;&#20026;&#30418;&#23376;&#65288;&#21363;&#36724;&#23545;&#40784;&#36229;&#30697;&#24418;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20811;&#26381;&#20043;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;&#20316;&#20026;&#25105;&#20204;&#35780;&#20272;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29366;&#24577;&#20272;&#35745;&#22120;&#65288;DeNSE&#65289;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#32508;&#21512;&#21033;&#29992;&#24930;&#36895;&#20294;&#24191;&#27867;&#23384;&#22312;&#30340;SCADA&#25968;&#25454;&#21644;&#24555;&#36895;&#20294;&#23616;&#37096;&#30340;PMU&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#31995;&#32479;&#30340;&#20122;&#31186;&#32423;&#29366;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#32771;&#34385;&#23454;&#38469;&#25361;&#25112;&#65292;&#22914;&#25299;&#25169;&#21464;&#21270;&#12289;&#38750;&#39640;&#26031;&#27979;&#37327;&#22122;&#22768;&#21644;&#38169;&#35823;&#25968;&#25454;&#26816;&#27979;&#19982;&#26657;&#27491;&#65292;&#35777;&#26126;&#20102;DeNSE&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.01729</link><description>&lt;p&gt;
&#32771;&#34385;&#23454;&#38469;&#23454;&#26045;&#25361;&#25112;&#30340;&#21516;&#27493;&#20840;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Time-Synchronized Full System State Estimation Considering Practical Implementation Challenges. (arXiv:2212.01729v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29366;&#24577;&#20272;&#35745;&#22120;&#65288;DeNSE&#65289;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#32508;&#21512;&#21033;&#29992;&#24930;&#36895;&#20294;&#24191;&#27867;&#23384;&#22312;&#30340;SCADA&#25968;&#25454;&#21644;&#24555;&#36895;&#20294;&#23616;&#37096;&#30340;PMU&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#31995;&#32479;&#30340;&#20122;&#31186;&#32423;&#29366;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#32771;&#34385;&#23454;&#38469;&#25361;&#25112;&#65292;&#22914;&#25299;&#25169;&#21464;&#21270;&#12289;&#38750;&#39640;&#26031;&#27979;&#37327;&#22122;&#22768;&#21644;&#38169;&#35823;&#25968;&#25454;&#26816;&#27979;&#19982;&#26657;&#27491;&#65292;&#35777;&#26126;&#20102;DeNSE&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30456;&#37327;&#27979;&#37327;&#35013;&#32622;&#65288;PMUs&#65289;&#36890;&#24120;&#25918;&#32622;&#22312;&#26368;&#39640;&#30005;&#21387;&#30340;&#27597;&#32447;&#19978;&#65292;&#22240;&#27492;&#35768;&#22810;&#20302;&#30005;&#21387;&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#30005;&#21147;&#31995;&#32479;&#26080;&#27861;&#34987;&#20854;&#35266;&#27979;&#21040;&#12290;&#36825;&#31181;&#35266;&#27979;&#32570;&#22833;&#20351;&#24471;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29366;&#24577;&#20272;&#35745;&#22120;&#65288;DeNSE&#65289;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;DeNSE&#37319;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#38388;&#25509;&#32467;&#21512;&#26469;&#33258;&#24930;&#26102;&#38388;&#23610;&#24230;&#20294;&#26222;&#36941;&#23384;&#22312;&#30340;&#30417;&#30563;&#25511;&#21046;&#19982;&#25968;&#25454;&#37319;&#38598;&#65288;SCADA&#65289;&#25968;&#25454;&#19982;&#24555;&#26102;&#38388;&#23610;&#24230;&#20294;&#23616;&#37096;&#30340;PMU&#25968;&#25454;&#30340;&#25512;&#26029;&#65292;&#23454;&#29616;&#23545;&#25972;&#20010;&#31995;&#32479;&#30340;&#20122;&#31186;&#32423;&#24773;&#26223;&#24863;&#30693;&#12290;&#36890;&#36807;&#32771;&#34385;&#25299;&#25169;&#21464;&#21270;&#12289;&#38750;&#39640;&#26031;&#27979;&#37327;&#22122;&#22768;&#21644;&#38169;&#35823;&#25968;&#25454;&#26816;&#27979;&#21644;&#26657;&#27491;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#20351;&#29992;IEEE 118-bus&#31995;&#32479;&#24471;&#21040;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;DeNSE&#22312;&#32431;SCADA&#29366;&#24577;&#20272;&#35745;&#22120;&#12289;SCADA-PMU&#28151;&#21512;&#29366;&#24577;&#20272;&#35745;&#22120;&#21644;&#20165;PMU&#32447;&#24615;&#29366;&#24577;&#20272;&#35745;&#22120;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As phasor measurement units (PMUs) are usually placed on the highest voltage buses, many lower voltage levels of the bulk power system are not observed by them. This lack of visibility makes time-synchronized state estimation of the full system a challenging problem. We propose a Deep Neural network-based State Estimator (DeNSE) to overcome this problem. The DeNSE employs a Bayesian framework to indirectly combine inferences drawn from slow timescale but widespread supervisory control and data acquisition (SCADA) data with fast timescale but local PMU data to attain sub-second situational awareness of the entire system. The practical utility of the proposed approach is demonstrated by considering topology changes, non-Gaussian measurement noise, and bad data detection and correction. The results obtained using the IEEE 118-bus system show the superiority of the DeNSE over a purely SCADA state estimator, a SCADA-PMU hybrid state estimator, and a PMU-only linear state estimator from a te
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38598;&#25104;VisNet&#12289;Transformer-M&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#32473;&#23450;&#20998;&#23376;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2211.12791</link><description>&lt;p&gt;
&#12298;VisNet&#12289;Transformer-M&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;OGB&#22823;&#35268;&#27169;&#25361;&#25112;&#20013;&#30340;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#30340;&#38598;&#25104;&#12299;
&lt;/p&gt;
&lt;p&gt;
An ensemble of VisNet, Transformer-M, and pretraining models for molecular property prediction in OGB Large-Scale Challenge @ NeurIPS 2022. (arXiv:2211.12791v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38598;&#25104;VisNet&#12289;Transformer-M&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#32473;&#23450;&#20998;&#23376;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25216;&#26415;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#22312;OGB-LSC 2022&#22270;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;PCQM4Mv2&#25968;&#25454;&#38598;&#20013;&#32473;&#23450;&#20998;&#23376;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21363;HOMO-LUMO&#33021;&#38553;&#12290;&#22312;&#27604;&#36187;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#27169;&#22411;&#65306;Transformer-M-ViSNet&#65288;&#19968;&#31181;&#22686;&#24378;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#21644;Pretrained-3D-ViSNet&#65288;&#36890;&#36807;&#25552;&#21462;&#20248;&#21270;&#32467;&#26500;&#30340;&#20960;&#20309;&#20449;&#24687;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;ViSNet&#65289;&#12290;&#36890;&#36807;22&#31181;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;ViSNet&#22242;&#38431;&#22312;&#27979;&#35797;&#25361;&#25112;&#38598;&#19978;&#23454;&#29616;&#20102;0.0723 eV&#30340;MAE&#65292;&#30456;&#36739;&#20110;&#21435;&#24180;&#27604;&#36187;&#20013;&#26368;&#20339;&#26041;&#27861;&#65292;&#38169;&#35823;&#20943;&#23569;&#20102;39.75%&#12290;
&lt;/p&gt;
&lt;p&gt;
In the technical report, we provide our solution for OGB-LSC 2022 Graph Regression Task. The target of this task is to predict the quantum chemical property, HOMO-LUMO gap for a given molecule on PCQM4Mv2 dataset. In the competition, we designed two kinds of models: Transformer-M-ViSNet which is an geometry-enhanced graph neural network for fully connected molecular graphs and Pretrained-3D-ViSNet which is a pretrained ViSNet by distilling geomeotric information from optimized structures. With an ensemble of 22 models, ViSNet Team achieved the MAE of 0.0723 eV on the test-challenge set, dramatically reducing the error by 39.75% compared with the best method in the last year competition.
&lt;/p&gt;</description></item><item><title>&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.11695</link><description>&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning. (arXiv:2211.11695v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11695
&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65288;DRL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#21487;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#22240;&#32032;&#30340;&#27169;&#22411;&#12290;&#23558;&#21464;&#21270;&#30340;&#28508;&#22312;&#35201;&#32032;&#20998;&#31163;&#25104;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#21464;&#37327;&#30340;&#36807;&#31243;&#26377;&#21161;&#20110;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#27169;&#20223;&#20154;&#31867;&#35266;&#23519;&#23545;&#35937;&#25110;&#20851;&#31995;&#26102;&#30340;&#26377;&#24847;&#20041;&#29702;&#35299;&#36807;&#31243;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;DRL&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25968;&#25454;&#25366;&#25496;&#31561;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;DRL&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#23450;&#20041;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#12289;&#24212;&#29992;&#21644;&#27169;&#22411;&#35774;&#35745;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#20004;&#20010;&#20844;&#35748;&#23450;&#20041;&#65288;&#30452;&#35266;&#23450;&#20041;&#21644;&#32676;&#35770;&#23450;&#20041;&#65289;&#30340;DRL&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;DRL&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#20808;&#35753;&#27169;&#22411;&#23398;&#20064;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#19981;&#26029;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2211.09703</link><description>&lt;p&gt;
&#39640;&#25928;&#35757;&#32451;&#65306;&#25506;&#32034;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26469;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones. (arXiv:2211.09703v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#20808;&#35753;&#27169;&#22411;&#23398;&#20064;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#19981;&#26029;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#30340;&#21331;&#36234;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#26114;&#36149;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65288;&#20363;&#22914;&#35270;&#35273;Transformer&#65289;&#12290;&#26412;&#25991;&#21551;&#21457;&#20110;&#28145;&#24230;&#32593;&#32476;&#30340;&#20869;&#22312;&#23398;&#20064;&#21160;&#21147;&#23398;&#65306;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#22312;&#36739;&#26089;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#27169;&#22411;&#20027;&#35201;&#23398;&#20064;&#22312;&#27599;&#20010;&#31034;&#20363;&#20013;&#35782;&#21035;&#19968;&#20123;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#21028;&#21035;&#27169;&#24335;&#65292;&#20363;&#22914;&#22270;&#20687;&#30340;&#20302;&#39057;&#25104;&#20998;&#21644;&#25968;&#25454;&#22686;&#24191;&#20043;&#21069;&#30340;&#21407;&#22987;&#20449;&#24687;&#12290;&#22522;&#20110;&#27492;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#65292;&#20854;&#20013;&#27169;&#22411;&#24635;&#26159;&#22312;&#27599;&#20010;&#26102;&#26399;&#21033;&#29992;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#35838;&#31243;&#22987;&#20110;&#20165;&#26292;&#38706;&#27599;&#20010;&#31034;&#20363;&#30340;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#24182;&#36880;&#28176;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;1&#65289;&#22312;&#36755;&#20837;&#30340;&#20613;&#37324;&#21494;&#35889;&#20013;&#24341;&#20837;&#19968;&#20010;&#35009;&#21098;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#21482;&#33021;&#20174;&#20302;&#39057;&#32452;&#20998;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The superior performance of modern deep networks usually comes with a costly training procedure. This paper presents a new curriculum learning approach for the efficient training of visual backbones (e.g., vision Transformers). Our work is inspired by the inherent learning dynamics of deep networks: we experimentally show that at an earlier training stage, the model mainly learns to recognize some 'easier-to-learn' discriminative patterns within each example, e.g., the lower-frequency components of images and the original information before data augmentation. Driven by this phenomenon, we propose a curriculum where the model always leverages all the training data at each epoch, while the curriculum starts with only exposing the 'easier-to-learn' patterns of each example, and introduces gradually more difficult patterns. To implement this idea, we 1) introduce a cropping operation in the Fourier spectrum of the inputs, which enables the model to learn from only the lower-frequency compo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#22312;&#27424;&#21442;&#25968;&#21644;&#36807;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24314;&#31435;&#20102;&#27424;&#21442;&#25968;DCNNs&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#21152;&#28145;&#26041;&#26696;&#33719;&#24471;&#20102;&#25554;&#20540;DCNN&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#36807;&#25311;&#21512;&#30340;DCNN&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.14184</link><description>&lt;p&gt;
&#25554;&#20540;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning Ability of Interpolating Deep Convolutional Neural Networks. (arXiv:2210.14184v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#22312;&#27424;&#21442;&#25968;&#21644;&#36807;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24314;&#31435;&#20102;&#27424;&#21442;&#25968;DCNNs&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#21152;&#28145;&#26041;&#26696;&#33719;&#24471;&#20102;&#25554;&#20540;DCNN&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#36807;&#25311;&#21512;&#30340;DCNN&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#29702;&#35770;&#24037;&#20316;&#20027;&#35201;&#30740;&#31350;&#20102;&#32447;&#24615;&#35774;&#32622;&#25110;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#37325;&#35201;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21363;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#22312;&#27424;&#21442;&#25968;&#21644;&#36807;&#21442;&#25968;&#35774;&#32622;&#19979;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#25991;&#29486;&#20013;&#39318;&#27425;&#24314;&#31435;&#20102;&#26080;&#21442;&#25968;&#25110;&#20989;&#25968;&#21464;&#37327;&#32467;&#26500;&#38480;&#21046;&#30340;&#27424;&#21442;&#25968;DCNNs&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#21521;&#38750;&#25554;&#20540;DCNN&#28155;&#21152;&#33391;&#23450;&#20041;&#30340;&#23618;&#65292;&#21487;&#20197;&#33719;&#24471;&#19968;&#20123;&#20445;&#25345;&#38750;&#25554;&#20540;DCNN&#33391;&#22909;&#23398;&#20064;&#36895;&#24230;&#30340;&#25554;&#20540;DCNN&#12290;&#36825;&#19968;&#32467;&#26524;&#26159;&#36890;&#36807;&#20026;DCNN&#35774;&#35745;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#21152;&#28145;&#26041;&#26696;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;&#36807;&#25311;&#21512;&#30340;DCNN&#22914;&#20309;&#33391;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is frequently observed that overparameterized neural networks generalize well. Regarding such phenomena, existing theoretical work mainly devotes to linear settings or fully-connected neural networks. This paper studies the learning ability of an important family of deep neural networks, deep convolutional neural networks (DCNNs), under both underparameterized and overparameterized settings. We establish the first learning rates of underparameterized DCNNs without parameter or function variable structure restrictions presented in the literature. We also show that by adding well-defined layers to a non-interpolating DCNN, we can obtain some interpolating DCNNs that maintain the good learning rates of the non-interpolating DCNN. This result is achieved by a novel network deepening scheme designed for DCNNs. Our work provides theoretical verification of how overfitted DCNNs generalize well.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#21160;&#24577;&#22270;&#30340;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;DyTed&#65292;&#36890;&#36807;&#35774;&#35745;&#26102;&#38388;&#29255;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#21644;&#32467;&#26500;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20998;&#31163;&#24863;&#30693;&#21028;&#21035;&#22120;&#20197;&#22686;&#24378;&#20998;&#31163;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.10592</link><description>&lt;p&gt;
DyTed:&#31163;&#25955;&#26102;&#21160;&#24577;&#22270;&#30340;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DyTed: Disentangled Representation Learning for Discrete-time Dynamic Graph. (arXiv:2210.10592v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#21160;&#24577;&#22270;&#30340;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;DyTed&#65292;&#36890;&#36807;&#35774;&#35745;&#26102;&#38388;&#29255;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#21644;&#32467;&#26500;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20998;&#31163;&#24863;&#30693;&#21028;&#21035;&#22120;&#20197;&#22686;&#24378;&#20998;&#31163;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26080;&#30417;&#30563;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20851;&#27880;&#12290;&#19982;&#38745;&#24577;&#22270;&#30456;&#27604;&#65292;&#21160;&#24577;&#22270;&#26082;&#20307;&#29616;&#20102;&#33410;&#28857;&#30340;&#20869;&#22312;&#31283;&#23450;&#29305;&#24449;&#65292;&#21448;&#20307;&#29616;&#20102;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#21160;&#24577;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#36825;&#20004;&#31181;&#20449;&#24687;&#28151;&#21512;&#21040;&#19968;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35299;&#37322;&#24615;&#24046;&#12289;&#40065;&#26834;&#24615;&#24046;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#26102;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#26102;&#21160;&#24577;&#22270;&#30340;&#26032;&#22411;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;DyTed&#12290;&#25105;&#20204;&#29305;&#21035;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#29255;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#20102;&#32467;&#26500;&#23545;&#27604;&#23398;&#20064;&#65292;&#20998;&#21035;&#26377;&#25928;&#22320;&#35782;&#21035;&#26102;&#38388;&#19981;&#21464;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#20004;&#31181;&#34920;&#31034;&#30340;&#20998;&#31163;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31163;&#24863;&#30693;&#21028;&#21035;&#22120;...
&lt;/p&gt;
&lt;p&gt;
Unsupervised representation learning for dynamic graphs has attracted a lot of research attention in recent years. Compared with static graph, the dynamic graph is a comprehensive embodiment of both the intrinsic stable characteristics of nodes and the time-related dynamic preference. However, existing methods generally mix these two types of information into a single representation space, which may lead to poor explanation, less robustness, and a limited ability when applied to different downstream tasks. To solve the above problems, in this paper, we propose a novel disenTangled representation learning framework for discrete-time Dynamic graphs, namely DyTed. We specially design a temporal-clips contrastive learning task together with a structure contrastive learning to effectively identify the time-invariant and time-varying representations respectively. To further enhance the disentanglement of these two types of representation, we propose a disentanglement-aware discriminator unde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-former&#30340;&#26032;&#22411;transformer&#26550;&#26500;&#65292;&#29992;&#20110;COVID-19&#26399;&#38388;&#30340;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#23458;&#27969;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#33258;&#36866;&#24212;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#23458;&#27969;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21644;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.09043</link><description>&lt;p&gt;
ST-former&#22312;COVID-19&#26399;&#38388;&#29992;&#20110;&#30701;&#26399;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#23458;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ST-former for short-term passenger flow prediction during COVID-19 in urban rail transit system. (arXiv:2210.09043v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-former&#30340;&#26032;&#22411;transformer&#26550;&#26500;&#65292;&#29992;&#20110;COVID-19&#26399;&#38388;&#30340;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#23458;&#27969;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#33258;&#36866;&#24212;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#21487;&#20197;&#20934;&#30830;&#24314;&#27169;&#23458;&#27969;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#21644;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#30340;&#23458;&#27969;&#23545;&#20110;&#25552;&#39640;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#24615;&#33021;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#30123;&#24773;&#26399;&#38388;&#12290;&#22914;&#20309;&#21160;&#24577;&#24314;&#27169;&#23458;&#27969;&#30340;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26159;&#23454;&#29616;&#20934;&#30830;&#23458;&#27969;&#39044;&#27979;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;ST-former&#65292;&#19987;&#38376;&#29992;&#20110;COVID-19&#26399;&#38388;&#30340;&#23458;&#27969;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21629;&#21517;&#20026;&#22240;&#26524;&#21367;&#31215;ProbSparse&#33258;&#27880;&#24847;&#65288;CPSA&#65289;&#65292;&#20197;&#20302;&#35745;&#31639;&#25104;&#26412;&#24314;&#27169;&#23458;&#27969;&#30340;&#22810;&#20010;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#25429;&#25417;&#22797;&#26434;&#19988;&#21160;&#24577;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#22810;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;AMGCN&#65289;&#65292;&#36890;&#36807;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#21033;&#29992;&#22810;&#20010;&#22270;&#26469;&#36827;&#34892;&#24314;&#27169;&#12290;&#21478;&#22806;&#65292;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#27169;&#22359;&#23558;&#23458;&#27969;&#25968;&#25454;&#12289;COVID-19&#30830;&#35786;&#30149;&#20363;&#31561;&#22810;&#31181;&#25968;&#25454;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate passenger flow prediction of urban rail transit is essential for improving the performance of intelligent transportation systems, especially during the epidemic. How to dynamically model the complex spatiotemporal dependencies of passenger flow is the main issue in achieving accurate passenger flow prediction during the epidemic. To solve this issue, this paper proposes a brand-new transformer-based architecture called STformer under the encoder-decoder framework specifically for COVID-19. Concretely, we develop a modified self-attention mechanism named Causal-Convolution ProbSparse Self-Attention (CPSA) to model the multiple temporal dependencies of passenger flow with low computational costs. To capture the complex and dynamic spatial dependencies, we introduce a novel Adaptive Multi-Graph Convolution Network (AMGCN) by leveraging multiple graphs in a self-adaptive manner. Additionally, the Multi-source Data Fusion block fuses the passenger flow data, COVID-19 confirmed case
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Kullback Leibler&#25955;&#24230;&#32422;&#26463;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#30340;&#22797;&#26434;&#24230;&#65292;&#27599;&#27425;&#36845;&#20195;&#21482;&#38656;&#35201;&#24658;&#23450;&#30340;&#25209;&#27425;&#22823;&#23567;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#38750;&#20984;&#21644;&#20984;&#32422;&#26463;DRO&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.05740</link><description>&lt;p&gt;
&#38543;&#26426;&#32422;&#26463;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#30340;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Stochastic Constrained DRO with a Complexity Independent of Sample Size. (arXiv:2210.05740v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Kullback Leibler&#25955;&#24230;&#32422;&#26463;&#30340;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#30340;&#22797;&#26434;&#24230;&#65292;&#27599;&#27425;&#36845;&#20195;&#21482;&#38656;&#35201;&#24658;&#23450;&#30340;&#25209;&#27425;&#22823;&#23567;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#38750;&#20984;&#21644;&#20984;&#32422;&#26463;DRO&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;(DRO)&#20316;&#20026;&#19968;&#31181;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#36827;&#34892;&#20998;&#24067;&#20559;&#31227;&#35757;&#32451;&#40065;&#26834;&#27169;&#22411;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;Kullback Leibler&#25955;&#24230;&#32422;&#26463;DRO&#38382;&#39064;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#38750;&#20984;&#21644;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38543;&#26426;&#31639;&#27861;&#19981;&#20165;&#20855;&#26377;&#19982;&#26679;&#26412;&#22823;&#23567;&#26080;&#20851;&#30340;&#31454;&#20105;&#24615;&#29978;&#33267;&#26356;&#22909;&#30340;&#22797;&#26434;&#24230;&#65292;&#32780;&#19988;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#21482;&#38656;&#35201;&#24658;&#23450;&#30340;&#25209;&#27425;&#22823;&#23567;&#65292;&#36825;&#23545;&#20110;&#24191;&#27867;&#24212;&#29992;&#26356;&#21152;&#23454;&#29992;&#12290;&#25105;&#20204;&#20026;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#25214;&#21040;&#20102;&#19968;&#20010;$\epsilon$&#31283;&#23450;&#35299;&#30340;&#36817;&#20046;&#26368;&#20248;&#30340;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#20026;&#20984;&#25439;&#22833;&#20989;&#25968;&#25214;&#21040;&#20102;&#19968;&#20010;$\epsilon$&#26368;&#20248;&#35299;&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#25152;&#25552;&#31639;&#27861;&#22312;&#35299;&#20915;&#38750;&#20984;&#21644;&#20984;&#32422;&#26463;DRO&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Optimization (DRO), as a popular method to train robust models against distribution shift between training and test sets, has received tremendous attention in recent years. In this paper, we propose and analyze stochastic algorithms that apply to both non-convex and convex losses for solving Kullback Leibler divergence constrained DRO problem. Compared with existing methods solving this problem, our stochastic algorithms not only enjoy competitive if not better complexity independent of sample size but also just require a constant batch size at every iteration, which is more practical for broad applications. We establish a nearly optimal complexity bound for finding an $\epsilon$ stationary solution for non-convex losses and an optimal complexity for finding an $\epsilon$ optimal solution for convex losses. Empirical studies demonstrate the effectiveness of the proposed algorithms for solving non-convex and convex constrained DRO problems.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#36873;&#25321;&#26159;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#26500;&#24314;&#23567;&#22411;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#20998;&#24067;&#32780;&#38750;&#27979;&#35797;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#20256;&#32479;&#22522;&#20934;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.03921</link><description>&lt;p&gt;
&#25968;&#25454;&#36873;&#25321;&#65306;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#26500;&#24314;&#23567;&#22411;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Selection: A Surprisingly Effective and General Principle for Building Small Interpretable Models. (arXiv:2210.03921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03921
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#36873;&#25321;&#26159;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#26500;&#24314;&#23567;&#22411;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#35757;&#32451;&#20998;&#24067;&#32780;&#38750;&#27979;&#35797;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#20256;&#32479;&#22522;&#20934;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#19968;&#31181;&#26500;&#24314;&#20934;&#30830;&#23567;&#22411;&#27169;&#22411;&#30340;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#31574;&#30053;&#12290;&#36825;&#31181;&#27169;&#22411;&#23545;&#20110;&#21487;&#35299;&#37322;&#24615;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#24182;&#19988;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#20063;&#26377;&#29992;&#36884;&#12290;&#35813;&#31574;&#30053;&#26159;&#23398;&#20064;&#35757;&#32451;&#20998;&#24067;&#32780;&#19981;&#26159;&#20351;&#29992;&#27979;&#35797;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#20998;&#24067;&#23398;&#20064;&#31639;&#27861;&#19981;&#26159;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#65307;&#25105;&#20204;&#24378;&#35843;&#36825;&#31181;&#31616;&#21333;&#31574;&#30053;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#24182;&#19988;&#22522;&#20110;&#20005;&#26684;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#25105;&#20204;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#20197;&#19979;&#20219;&#21153;&#65306;&#65288;1&#65289;&#26500;&#24314;&#32858;&#31867;&#35299;&#37322;&#26641;&#65292;&#65288;2&#65289;&#22522;&#20110;&#21407;&#22411;&#30340;&#20998;&#31867;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#23427;&#25552;&#39640;&#20102;&#24369;&#20256;&#32479;&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#23427;&#20204;&#20196;&#20154;&#24778;&#35766;&#22320;&#19982;&#19987;&#19994;&#30340;&#29616;&#20195;&#25216;&#26415;&#30456;&#31454;&#20105;&#12290;&#27492;&#31574;&#30053;&#20063;&#36866;&#29992;&#20110;&#27169;&#22411;&#22823;&#23567;&#30340;&#27010;&#24565;&#12290;&#22312;&#21069;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#27169;&#22411;&#22823;&#23567;&#36890;&#36807;&#26641;&#20013;&#21494;&#23376;&#33410;&#28857;&#30340;&#25968;&#37327;&#26469;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present convincing empirical evidence for an effective and general strategy for building accurate small models. Such models are attractive for interpretability and also find use in resource-constrained environments. The strategy is to learn the training distribution instead of using data from the test distribution. The distribution learning algorithm is not a contribution of this work; we highlight the broad usefulness of this simple strategy on a diverse set of tasks, and as such these rigorous empirical results are our contribution. We apply it to the tasks of (1) building cluster explanation trees, (2) prototype-based classification, and (3) classification using Random Forests, and show that it improves the accuracy of weak traditional baselines to the point that they are surprisingly competitive with specialized modern techniques.  This strategy is also versatile wrt the notion of model size. In the first two tasks, model size is identified by number of leaves in the tree and th
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36741;&#21161;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#31934;&#24230;&#21644;&#25910;&#25947;&#26102;&#38388;</title><link>http://arxiv.org/abs/2210.02614</link><description>&lt;p&gt;
&#24102;&#26377;&#26381;&#21153;&#22120;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#25552;&#39640;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Server Learning: Enhancing Performance for Non-IID Data. (arXiv:2210.02614v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02614
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36741;&#21161;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#30340;&#27169;&#22411;&#31934;&#24230;&#21644;&#25910;&#25947;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#20351;&#29992;&#23458;&#25143;&#31471;&#23384;&#20648;&#30340;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#19968;&#31181;&#25163;&#27573;&#65292;&#20854;&#20013;&#21327;&#35843;&#26381;&#21153;&#22120;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#35757;&#32451;&#23458;&#25143;&#31471;&#25968;&#25454;&#19981;&#29420;&#31435;&#21516;&#20998;&#24067;&#26102;&#65292;FL&#21487;&#33021;&#20250;&#36973;&#21463;&#24615;&#33021;&#19979;&#38477;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#26032;&#30340;&#34917;&#20805;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#24615;&#33021;&#19979;&#38477;&#65292;&#21363;&#20801;&#35768;&#26381;&#21153;&#22120;&#20174;&#23567;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#36741;&#21161;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26381;&#21153;&#22120;&#25968;&#25454;&#38598;&#24456;&#23567;&#19988;&#20854;&#20998;&#24067;&#19982;&#25152;&#26377;&#23458;&#25143;&#31471;&#32858;&#21512;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#20063;&#21487;&#20197;&#22312;&#27169;&#22411;&#31934;&#24230;&#21644;&#25910;&#25947;&#26102;&#38388;&#26041;&#38754;&#23454;&#29616;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a means of distributed learning using local data stored at clients with a coordinating server. Recent studies showed that FL can suffer from poor performance and slower convergence when training data at clients are not independent and identically distributed. Here we consider a new complementary approach to mitigating this performance degradation by allowing the server to perform auxiliary learning from a small dataset. Our analysis and experiments show that this new approach can achieve significant improvements in both model accuracy and convergence time even when the server dataset is small and its distribution differs from that of the aggregated data from all clients.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29983;&#25104;&#31163;&#25955;&#22270;&#26102;&#20351;&#29992;&#31163;&#25955;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#31163;&#25955;&#22122;&#22768;&#21487;&#20197;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#37319;&#26679;&#36807;&#31243;&#36895;&#24230;&#25552;&#39640;&#20102;30&#20493;&#12290;</title><link>http://arxiv.org/abs/2210.01549</link><description>&lt;p&gt;
&#22270;&#30340;&#25193;&#25955;&#27169;&#22411;&#21463;&#30410;&#20110;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Graphs Benefit From Discrete State Spaces. (arXiv:2210.01549v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29983;&#25104;&#31163;&#25955;&#22270;&#26102;&#20351;&#29992;&#31163;&#25955;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#31163;&#25955;&#22122;&#22768;&#21487;&#20197;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#37319;&#26679;&#36807;&#31243;&#36895;&#24230;&#25552;&#39640;&#20102;30&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#21644;&#35780;&#20998;&#21305;&#37197;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#38750;&#24120;&#24378;&#22823;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#20063;&#34987;&#24212;&#29992;&#20110;&#31163;&#25955;&#22270;&#30340;&#29983;&#25104;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#23427;&#20204;&#20173;&#20381;&#36182;&#20110;&#36830;&#32493;&#39640;&#26031;&#25200;&#21160;&#12290;&#30456;&#21453;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#31163;&#25955;&#22122;&#22768;&#36827;&#34892;&#21069;&#21521;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#36825;&#30830;&#20445;&#22312;&#27599;&#20010;&#20013;&#38388;&#27493;&#39588;&#20013;&#65292;&#22270;&#20445;&#25345;&#31163;&#25955;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26550;&#26500;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#31163;&#25955;&#22122;&#22768;&#22788;&#29702;&#36807;&#31243;&#29983;&#25104;&#30340;&#26679;&#26412;&#36136;&#37327;&#26356;&#39640;&#65292;&#24179;&#22343; MMDs &#38477;&#20302;&#20102;1.5&#20493;&#12290;&#27492;&#22806;&#65292;&#21435;&#22122;&#27493;&#39588;&#30340;&#25968;&#37327;&#20174;1000&#20943;&#23569;&#21040;32&#27493;&#65292;&#23548;&#33268;&#37319;&#26679;&#36807;&#31243;&#36895;&#24230;&#25552;&#39640;&#20102;30&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion probabilistic models and score-matching models have proven to be very powerful for generative tasks. While these approaches have also been applied to the generation of discrete graphs, they have, so far, relied on continuous Gaussian perturbations. Instead, in this work, we suggest using discrete noise for the forward Markov process. This ensures that in every intermediate step the graph remains discrete. Compared to the previous approach, our experimental results on four datasets and multiple architectures show that using a discrete noising process results in higher quality generated samples indicated with an average MMDs reduced by a factor of 1.5. Furthermore, the number of denoising steps is reduced from 1000 to 32 steps, leading to a 30 times faster sampling procedure.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#36741;&#21161;AI&#21457;&#29616;&#23450;&#37327;&#21644;&#24418;&#24335;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25193;&#23637;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20174;&#30495;&#23454;&#25968;&#25454;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#30340;&#38750;&#32447;&#24615;&#21644;&#21160;&#24577;&#20851;&#31995;&#65292;&#24110;&#21161;&#31185;&#23398;&#23478;&#22312;&#30740;&#31350;&#36807;&#31243;&#20013;&#25581;&#31034;&#26032;&#30340;&#20851;&#31995;&#21644;&#25506;&#32034;&#23545;&#29031;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.00563</link><description>&lt;p&gt;
&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#36741;&#21161;AI&#21457;&#29616;&#23450;&#37327;&#21644;&#24418;&#24335;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AI-Assisted Discovery of Quantitative and Formal Models in Social Science. (arXiv:2210.00563v3 [cs.SC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#36741;&#21161;AI&#21457;&#29616;&#23450;&#37327;&#21644;&#24418;&#24335;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25193;&#23637;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#20174;&#30495;&#23454;&#25968;&#25454;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#30340;&#38750;&#32447;&#24615;&#21644;&#21160;&#24577;&#20851;&#31995;&#65292;&#24110;&#21161;&#31185;&#23398;&#23478;&#22312;&#30740;&#31350;&#36807;&#31243;&#20013;&#25581;&#31034;&#26032;&#30340;&#20851;&#31995;&#21644;&#25506;&#32034;&#23545;&#29031;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#65292;&#20351;&#29992;&#24418;&#24335;&#21644;&#23450;&#37327;&#27169;&#22411;&#65292;&#22914;&#25551;&#36848;&#32463;&#27982;&#22686;&#38271;&#21644;&#38598;&#20307;&#34892;&#21160;&#30340;&#27169;&#22411;&#65292;&#20197;&#21046;&#23450;&#26426;&#26800;&#35299;&#37322;&#65292;&#25552;&#20379;&#39044;&#27979;&#65292;&#24182;&#25581;&#31034;&#26377;&#20851;&#35266;&#23519;&#21040;&#29616;&#35937;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36741;&#21161;&#21457;&#29616;&#33021;&#22815;&#25429;&#25417;&#31038;&#20250;&#31185;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#38750;&#32447;&#24615;&#21644;&#21160;&#24577;&#20851;&#31995;&#30340;&#31526;&#21495;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#25193;&#23637;&#21040;&#22024;&#26434;&#21644;&#32437;&#21521;&#25968;&#25454;&#20013;&#25214;&#21040;&#31616;&#27905;&#20989;&#25968;&#21644;&#24494;&#20998;&#26041;&#31243;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20174;&#32463;&#27982;&#23398;&#21644;&#31038;&#20250;&#23398;&#30340;&#30495;&#23454;&#25968;&#25454;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#31526;&#21495;&#22238;&#24402;&#19982;&#29616;&#26377;&#24037;&#20316;&#27969;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#24110;&#21161;&#21457;&#29616;&#26032;&#30340;&#20851;&#31995;&#65292;&#24182;&#22312;&#31185;&#23398;&#36807;&#31243;&#20013;&#25506;&#32034;&#23545;&#29031;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#36825;&#31181;AI&#36741;&#21161;&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#31995;&#32479;&#22320;&#25506;&#32034;&#38750;&#32447;&#24615;&#27169;&#22411;&#31354;&#38388;&#26469;&#26725;&#25509;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#21442;&#25968;&#21270;&#21644;&#38750;&#21442;&#25968;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In social science, formal and quantitative models, such as ones describing economic growth and collective action, are used to formulate mechanistic explanations, provide predictions, and uncover questions about observed phenomena. Here, we demonstrate the use of a machine learning system to aid the discovery of symbolic models that capture nonlinear and dynamical relationships in social science datasets. By extending neuro-symbolic methods to find compact functions and differential equations in noisy and longitudinal data, we show that our system can be used to discover interpretable models from real-world data in economics and sociology. Augmenting existing workflows with symbolic regression can help uncover novel relationships and explore counterfactual models during the scientific process. We propose that this AI-assisted framework can bridge parametric and non-parametric models commonly employed in social science research by systematically exploring the space of nonlinear models an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#25512;&#26029;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#24178;&#39044;&#23545;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#24555;&#36895;&#30830;&#23450;&#26368;&#20248;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#22823;&#24178;&#39044;&#31354;&#38388;&#19979;&#36827;&#34892;&#23454;&#39564;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.04744</link><description>&lt;p&gt;
&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#24178;&#39044;&#35774;&#35745;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Optimal Intervention Design in Causal Models. (arXiv:2209.04744v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#25512;&#26029;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#24178;&#39044;&#23545;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#24555;&#36895;&#30830;&#23450;&#26368;&#20248;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#22823;&#24178;&#39044;&#31354;&#38388;&#19979;&#36827;&#34892;&#23454;&#39564;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#12289;&#24037;&#31243;&#21644;&#20844;&#20849;&#25919;&#31574;&#31561;&#21508;&#20010;&#39046;&#22495;&#65292;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#20197;&#21457;&#29616;&#23454;&#29616;&#26399;&#26395;&#32467;&#26524;&#30340;&#24178;&#39044;&#31574;&#30053;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#24403;&#21487;&#33021;&#30340;&#24178;&#39044;&#31354;&#38388;&#24456;&#22823;&#26102;&#65292;&#23436;&#20840;&#25628;&#32034;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#38656;&#35201;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32534;&#30721;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#24178;&#39044;&#23545;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#26356;&#26377;&#25928;&#22320;&#30830;&#23450;&#29702;&#24819;&#30340;&#24178;&#39044;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#21518;&#24178;&#39044;&#20998;&#24067;&#30340;&#22343;&#20540;&#19982;&#26399;&#26395;&#30446;&#26631;&#22343;&#20540;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#30830;&#23450;&#26368;&#20248;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#26356;&#26032;&#36827;&#34892;&#22240;&#26524;&#27169;&#22411;&#30340;&#24314;&#31435;&#65292;&#24182;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#65292;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#33719;&#21462;&#20989;&#25968;&#26469;&#20248;&#20808;&#32771;&#34385;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#33719;&#21462;&#20989;&#25968;&#21487;&#20197;&#20197;&#38381;&#21512;&#24418;&#24335;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#20248;&#21270;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential experimental design to discover interventions that achieve a desired outcome is a key problem in various domains including science, engineering and public policy. When the space of possible interventions is large, making an exhaustive search infeasible, experimental design strategies are needed. In this context, encoding the causal relationships between the variables, and thus the effect of interventions on the system, is critical for identifying desirable interventions more efficiently. Here, we develop a causal active learning strategy to identify interventions that are optimal, as measured by the discrepancy between the post-interventional mean of the distribution and a desired target mean. The approach employs a Bayesian update for the causal model and prioritizes interventions using a carefully designed, causally informed acquisition function. This acquisition function is evaluated in closed form, allowing for fast optimization. The resulting algorithms are theoreticall
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21160;&#24577;&#29366;&#24577;&#20272;&#35745;&#31639;&#27861;&#65292;&#29992;&#20110;&#32593;&#32476;&#21270;&#24494;&#30005;&#32593;&#20013;&#30340;&#26410;&#30693;&#23376;&#31995;&#32479;&#12290;&#20855;&#20307;&#36129;&#29486;&#21253;&#25324;&#65306;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;Neuro-DSE&#31639;&#27861;&#12289;&#33258;&#25105;&#23436;&#21892;&#30340;Neuro-DSE+&#31639;&#27861;&#12289;&#31070;&#32463;KalmanNet-DSE&#31639;&#27861;&#20197;&#21450;&#29992;&#20110;&#32852;&#21512;&#20272;&#35745;&#24494;&#30005;&#32593;&#29366;&#24577;&#21644;&#26410;&#30693;&#21442;&#25968;&#30340;&#22686;&#24378;Neuro-DSE&#31639;&#27861;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#12289;&#25511;&#21046;&#27169;&#24335;&#12289;&#30005;&#28304;&#21644;&#21487;&#35266;&#27979;&#24615;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.12288</link><description>&lt;p&gt;
&#32593;&#32476;&#21270;&#24494;&#30005;&#32593;&#30340;&#31070;&#32463;&#21160;&#24577;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neuro-Dynamic State Estimation for Networked Microgrids. (arXiv:2208.12288v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12288
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21160;&#24577;&#29366;&#24577;&#20272;&#35745;&#31639;&#27861;&#65292;&#29992;&#20110;&#32593;&#32476;&#21270;&#24494;&#30005;&#32593;&#20013;&#30340;&#26410;&#30693;&#23376;&#31995;&#32479;&#12290;&#20855;&#20307;&#36129;&#29486;&#21253;&#25324;&#65306;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;Neuro-DSE&#31639;&#27861;&#12289;&#33258;&#25105;&#23436;&#21892;&#30340;Neuro-DSE+&#31639;&#27861;&#12289;&#31070;&#32463;KalmanNet-DSE&#31639;&#27861;&#20197;&#21450;&#29992;&#20110;&#32852;&#21512;&#20272;&#35745;&#24494;&#30005;&#32593;&#29366;&#24577;&#21644;&#26410;&#30693;&#21442;&#25968;&#30340;&#22686;&#24378;Neuro-DSE&#31639;&#27861;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#12289;&#25511;&#21046;&#27169;&#24335;&#12289;&#30005;&#28304;&#21644;&#21487;&#35266;&#27979;&#24615;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;&#31070;&#32463;&#21160;&#24577;&#29366;&#24577;&#20272;&#35745;&#65288;Neuro-DSE&#65289;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21160;&#24577;&#29366;&#24577;&#20272;&#35745;&#65288;DSE&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#26410;&#30693;&#23376;&#31995;&#32479;&#30340;&#32593;&#32476;&#21270;&#24494;&#30005;&#32593;&#65288;NM&#65289;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;1&#65289;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;Neuro-DSE&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#37096;&#20998;&#26410;&#30693;&#21160;&#24577;&#27169;&#22411;&#30340;NM DSE&#65292;&#35813;&#31639;&#27861;&#23558;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE-Net&#65289;&#19982;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30456;&#32467;&#21512;&#65307;2&#65289;&#19968;&#31181;&#33258;&#25105;&#23436;&#21892;&#30340;Neuro-DSE&#31639;&#27861;&#65288;Neuro-DSE+&#65289;&#65292;&#36890;&#36807;&#24314;&#31435;&#33258;&#21160;&#36807;&#28388;&#12289;&#22686;&#21152;&#21644;&#26657;&#27491;&#26694;&#26550;&#65292;&#20351;&#24471;&#22312;&#26377;&#38480;&#19988;&#23384;&#22312;&#22122;&#22768;&#27979;&#37327;&#24773;&#20917;&#19979;&#21487;&#20197;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;DSE&#65307;3&#65289;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;KalmanNet-DSE&#30340;&#31639;&#27861;&#65292;&#36827;&#19968;&#27493;&#23558;KalmanNet&#19982;Neuro-DSE&#30456;&#32467;&#21512;&#65292;&#20197;&#32531;&#35299;&#31070;&#32463;&#21644;&#29289;&#29702;&#21160;&#24577;&#27169;&#22411;&#20043;&#38388;&#30340;&#27169;&#22411;&#19981;&#21305;&#37197;&#65307;4&#65289;&#19968;&#31181;&#22686;&#24378;&#30340;Neuro-DSE&#31639;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#20272;&#35745;NM&#29366;&#24577;&#21644;&#26410;&#30693;&#21442;&#25968;&#65288;&#22914;&#24815;&#24615;&#65289;&#12290;&#22823;&#37327;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;Neuro-DSE&#21450;&#20854;&#21464;&#20307;&#22312;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#12289;&#25511;&#21046;&#27169;&#24335;&#12289;&#30005;&#28304;&#21644;&#21487;&#35266;&#27979;&#24615;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We devise neuro-dynamic state estimation (Neuro-DSE), a learning-based dynamic state estimation (DSE) algorithm for networked microgrids (NMs) under unknown subsystems. Our contributions include: 1) a data-driven Neuro-DSE algorithm for NMs DSE with partially unidentified dynamic models, which incorporates the neural-ordinary-differential-equations (ODE-Net) into Kalman filters; 2) a self-refining Neuro-DSE algorithm (Neuro-DSE+) which enables data-driven DSE under limited and noisy measurements by establishing an automatic filtering, augmenting and correcting framework; 3) a Neuro-KalmanNet-DSE algorithm which further integrates KalmanNet with Neuro-DSE to relieve the model mismatch of both neural- and physics-based dynamic models; and 4) an augmented Neuro-DSE for joint estimation of NMs states and unknown parameters (e.g., inertia). Extensive case studies demonstrate the efficacy of Neuro-DSE and its variants under different noise levels, control modes, power sources, observabilitie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#32467;&#21512;&#21508;&#31181;&#20840;&#23616;&#21442;&#32771;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#22320;&#22270;&#19978;&#23545;&#20219;&#24847;&#26597;&#35810;&#20301;&#32622;&#36827;&#34892;&#20934;&#30830;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2208.11061</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#34701;&#21512;&#21644;&#34920;&#31034;&#26144;&#23556;&#30340;&#22823;&#35268;&#27169;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Traffic Congestion Prediction based on Multimodal Fusion and Representation Mapping. (arXiv:2208.11061v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#32467;&#21512;&#21508;&#31181;&#20840;&#23616;&#21442;&#32771;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#35268;&#27169;&#22320;&#22270;&#19978;&#23545;&#20219;&#24847;&#26597;&#35810;&#20301;&#32622;&#36827;&#34892;&#20934;&#30830;&#30340;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22478;&#24066;&#21270;&#36827;&#31243;&#30340;&#25512;&#36827;&#65292;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#23545;&#22478;&#24066;&#21457;&#23637;&#21644;&#23621;&#27665;&#29983;&#27963;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#20998;&#26512;&#25317;&#22581;&#22240;&#32032;&#26469;&#21028;&#26029;&#20132;&#36890;&#25317;&#22581;&#26159;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24341;&#20837;&#20102;&#21508;&#31181;&#20256;&#32479;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#20132;&#36890;&#25317;&#22581;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#35201;&#20040;&#23545;&#22823;&#35268;&#27169;&#25317;&#22581;&#22240;&#32032;&#32858;&#21512;&#19981;&#23436;&#21892;&#65292;&#35201;&#20040;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#22823;&#35268;&#27169;&#31354;&#38388;&#20013;&#27599;&#20010;&#31934;&#30830;&#20301;&#32622;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#31471;&#21040;&#31471;&#26694;&#26550;&#12290;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#65292;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#26144;&#23556;&#27169;&#22359;&#65292;&#32467;&#21512;&#21508;&#31181;&#20840;&#23616;&#21442;&#32771;&#20449;&#24687;&#65292;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#22320;&#22270;&#19978;&#23545;&#20219;&#24847;&#26597;&#35810;&#20301;&#32622;&#36827;&#34892;&#20132;&#36890;&#25317;&#22581;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the progress of the urbanisation process, the urban transportation system is extremely critical to the development of cities and the quality of life of the citizens. Among them, it is one of the most important tasks to judge traffic congestion by analysing the congestion factors. Recently, various traditional and machine-learning-based models have been introduced for predicting traffic congestion. However, these models are either poorly aggregated for massive congestion factors or fail to make accurate predictions for every precise location in large-scale space. To alleviate these problems, a novel end-to-end framework based on convolutional neural networks is proposed in this paper. With learning representations, the framework proposes a novel multimodal fusion module and a novel representation mapping module to achieve traffic congestion predictions on arbitrary query locations on a large-scale map, combined with various global reference information. The proposed framework achie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#23884;&#20837;&#21040;Hilbert Simplex&#20960;&#20309;&#20013;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21457;&#29616;&#35813;&#20960;&#20309;&#32467;&#26500;&#22312;&#23884;&#20837;&#20219;&#21153;&#20013;&#19982;&#20854;&#20182;&#20960;&#20309;&#32467;&#26500;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#21644;&#25968;&#20540;&#31283;&#20581;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2203.11434</link><description>&lt;p&gt;
Hilbert Simplex&#20960;&#20309;&#20013;&#30340;&#38750;&#32447;&#24615;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Non-linear Embeddings in Hilbert Simplex Geometry. (arXiv:2203.11434v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#23884;&#20837;&#21040;Hilbert Simplex&#20960;&#20309;&#20013;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21457;&#29616;&#35813;&#20960;&#20309;&#32467;&#26500;&#22312;&#23884;&#20837;&#20219;&#21153;&#20013;&#19982;&#20854;&#20182;&#20960;&#20309;&#32467;&#26500;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#21644;&#25968;&#20540;&#31283;&#20581;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#23558;&#31163;&#25955;&#21152;&#26435;&#22270;&#23884;&#20837;&#21040;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#21518;&#32493;&#22788;&#29702;&#12290;&#22312;&#20998;&#23618;&#32467;&#26500;&#23884;&#20837;&#21040;&#21452;&#26354;&#20960;&#20309;&#20013;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#22240;&#20026;&#24050;&#32463;&#35777;&#26126;&#20219;&#20309;&#21152;&#26435;&#26641;&#37117;&#21487;&#20197;&#20197;&#20219;&#24847;&#20302;&#30340;&#25197;&#26354;&#31243;&#24230;&#23884;&#20837;&#21040;&#35813;&#20960;&#20309;&#20013;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#23884;&#20837;&#21040;Hilbert Simplex&#20960;&#20309;&#20013;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;Hilbert Simplex&#20960;&#20309;&#22312;&#23884;&#20837;&#20219;&#21153;&#20013;&#19982;&#20854;&#20182;&#20960;&#20309;&#32467;&#26500;&#65288;&#22914;Poincar&#233;&#21452;&#26354;&#29699;&#25110;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#65289;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#21644;&#25968;&#20540;&#31283;&#20581;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key technique of machine learning and computer vision is to embed discrete weighted graphs into continuous spaces for further downstream processing. Embedding discrete hierarchical structures in hyperbolic geometry has proven very successful since it was shown that any weighted tree can be embedded in that geometry with arbitrary low distortion. Various optimization methods for hyperbolic embeddings based on common models of hyperbolic geometry have been studied. In this paper, we consider Hilbert geometry for the standard simplex which is isometric to a vector space equipped with the variation polytope norm. We study the representation power of this Hilbert simplex geometry by embedding distance matrices of graphs. Our findings demonstrate that Hilbert simplex geometry is competitive to alternative geometries such as the Poincar\'e hyperbolic ball or the Euclidean geometry for embedding tasks while being fast and numerically robust.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31354;&#26102;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#26469;&#39044;&#27979;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#31995;&#32479;&#33410;&#20551;&#26085;&#30701;&#26399;&#23458;&#27969;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22810;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#25552;&#21462;&#23458;&#27969;&#30340;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;&#36890;&#36807;&#21367;&#31215;-&#27880;&#24847;&#21147;&#22359;&#25552;&#21462;&#23458;&#27969;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#12290;&#36825;&#20010;&#26041;&#27861;&#23545;&#20110;&#20132;&#36890;&#31649;&#29702;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2203.00007</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#26102;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#30340;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#31995;&#32479;&#33410;&#20551;&#26085;&#30701;&#26399;&#23458;&#27969;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Attention Fusion Network for short-term passenger flow prediction on holidays in urban rail transit systems. (arXiv:2203.00007v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00007
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#31354;&#26102;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#26469;&#39044;&#27979;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#31995;&#32479;&#33410;&#20551;&#26085;&#30701;&#26399;&#23458;&#27969;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22810;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#25552;&#21462;&#23458;&#27969;&#30340;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;&#36890;&#36807;&#21367;&#31215;-&#27880;&#24847;&#21147;&#22359;&#25552;&#21462;&#23458;&#27969;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#12290;&#36825;&#20010;&#26041;&#27861;&#23545;&#20110;&#20132;&#36890;&#31649;&#29702;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#31995;&#32479;&#26469;&#35828;&#65292;&#30701;&#26399;&#23458;&#27969;&#39044;&#27979;&#23545;&#20132;&#36890;&#36816;&#33829;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#27169;&#22411;&#20027;&#35201;&#39044;&#27979;&#26222;&#36890;&#24037;&#20316;&#26085;&#25110;&#21608;&#26411;&#30340;&#23458;&#27969;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#33410;&#20551;&#26085;&#23458;&#27969;&#30340;&#39044;&#27979;&#65292;&#32780;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#31361;&#21457;&#24615;&#21644;&#19981;&#35268;&#24459;&#24615;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;&#31354;&#26102;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#65292;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#12289;&#21367;&#31215;-&#27880;&#24847;&#21147;&#22359;&#21644;&#29305;&#24449;&#34701;&#21512;&#22359;&#65292;&#29992;&#20110;&#33410;&#20551;&#26085;&#30701;&#26399;&#23458;&#27969;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The short term passenger flow prediction of the urban rail transit system is of great significance for traffic operation and management. The emerging deep learning-based models provide effective methods to improve prediction accuracy. However, most of the existing models mainly predict the passenger flow on general weekdays or weekends. There are only few studies focusing on predicting the passenger flow on holidays, which is a significantly challenging task for traffic management because of its suddenness and irregularity. To this end, we propose a deep learning-based model named Spatial Temporal Attention Fusion Network comprising a novel Multi-Graph Attention Network, a Conv-Attention Block, and Feature Fusion Block for short-term passenger flow prediction on holidays. The multi-graph attention network is applied to extract the complex spatial dependencies of passenger flow dynamically and the conv-attention block is applied to extract the temporal dependencies of passenger flow fro
&lt;/p&gt;</description></item><item><title>STG-GAN&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#31354;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#30701;&#26399;&#23458;&#27969;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#20869;&#23384;&#21344;&#29992;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.06727</link><description>&lt;p&gt;
STG-GAN&#65306;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#31995;&#32479;&#30701;&#26399;&#23458;&#27969;&#39044;&#27979;&#30340;&#26102;&#31354;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
STG-GAN: A spatiotemporal graph generative adversarial networks for short-term passenger flow prediction in urban rail transit systems. (arXiv:2202.06727v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06727
&lt;/p&gt;
&lt;p&gt;
STG-GAN&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#31354;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#30701;&#26399;&#23458;&#27969;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#20869;&#23384;&#21344;&#29992;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#26399;&#23458;&#27969;&#39044;&#27979;&#26159;&#26356;&#22909;&#31649;&#29702;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#31995;&#32479;&#30340;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#19968;&#20123;&#26032;&#20852;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20026;&#25913;&#21892;&#30701;&#26399;&#39044;&#27979;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#31995;&#32479;&#20013;&#23384;&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#21482;&#23558;&#30495;&#23454;&#20540;&#21644;&#39044;&#27979;&#20540;&#20043;&#38388;&#30340;&#32477;&#23545;&#35823;&#24046;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#26410;&#33021;&#32771;&#34385;&#21040;&#23545;&#39044;&#27979;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#39044;&#27979;&#27169;&#22411;&#24341;&#20837;&#20102;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#38477;&#20302;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26102;&#31354;&#22270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;STG-GAN&#65289;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#26356;&#20302;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#29992;&#20110;&#39044;&#27979;&#30701;&#26399;&#23458;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term passenger flow prediction is an important but challenging task for better managing urban rail transit (URT) systems. Some emerging deep learning models provide good insights to improve short-term prediction accuracy. However, there exist many complex spatiotemporal dependencies in URT systems. Most previous methods only consider the absolute error between ground truth and predictions as the optimization objective, which fails to account for spatial and temporal constraints on the predictions. Furthermore, a large number of existing prediction models introduce complex neural network layers to improve accuracy while ignoring their training efficiency and memory occupancy, decreasing the chances to be applied to the real world. To overcome these limitations, we propose a novel deep learning-based spatiotemporal graph generative adversarial network (STG-GAN) model with higher prediction accuracy, higher efficiency, and lower memory occupancy to predict short-term passenger flows
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#20998;&#36776;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;SMGRL&#65289;&#65292;&#36890;&#36807;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#24212;&#29992;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#22810;&#20998;&#36776;&#29575;&#33410;&#28857;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2201.12670</link><description>&lt;p&gt;
SMGRL&#65306;&#21487;&#25193;&#23637;&#30340;&#22810;&#20998;&#36776;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SMGRL: Scalable Multi-resolution Graph Representation Learning. (arXiv:2201.12670v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#20998;&#36776;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;SMGRL&#65289;&#65292;&#36890;&#36807;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#24212;&#29992;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#22810;&#20998;&#36776;&#29575;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#25299;&#25169;&#24863;&#30693;&#33021;&#21147;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23545;&#20110;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#25429;&#25417;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#38500;&#38750;&#28155;&#21152;&#39069;&#22806;&#30340;&#23618;&#27425;&#8212;&#8212;&#32780;&#36825;&#21448;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#21644;&#26102;&#38388;&#31354;&#38388;&#22797;&#26434;&#24230;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#20351;&#24471;&#23567;&#25209;&#37327;&#22788;&#29702;&#21464;&#24471;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#22411;&#22270;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#20998;&#36776;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;SMGRL&#65289;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#22810;&#20998;&#36776;&#29575;&#33410;&#28857;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;GCN&#27169;&#22411;&#12290;&#36890;&#36807;&#20165;&#22312;&#21407;&#22987;&#22270;&#30340;&#38477;&#32500;&#31895;&#21270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#21033;&#29992;&#33258;&#30456;&#20284;&#24615;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#24212;&#29992;&#25152;&#24471;&#31639;&#27861;&#65292;&#25105;&#20204;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;&#26368;&#32456;&#30340;&#22810;&#20998;&#36776;&#29575;&#23884;&#20837;&#21487;&#20197;&#32858;&#21512;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) allow us to learn topologically-aware node embeddings, which can be useful for classification or link prediction. However, they are unable to capture long-range dependencies between nodes without adding additional layers -- which in turn leads to over-smoothing and increased time and space complexity. Further, the complex dependencies between nodes make mini-batching challenging, limiting their applicability to large graphs. We propose a Scalable Multi-resolution Graph Representation Learning (SMGRL) framework that enables us to learn multi-resolution node embeddings efficiently. Our framework is model-agnostic and can be applied to any existing GCN model. We dramatically reduce training costs by training only on a reduced-dimension coarsening of the original graph, then exploit self-similarity to apply the resulting algorithm at multiple resolutions. The resulting multi-resolution embeddings can be aggregated to yield high-quality node embeddings th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STS-GAN&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#32473;&#23450;&#30340;2D&#31034;&#20363;&#25193;&#23637;&#21040;&#20219;&#24847;3D&#23454;&#20307;&#32441;&#29702;&#65292;&#24182;&#21512;&#25104;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#21644;&#30456;&#20284;&#29305;&#24449;&#30340;&#23454;&#20307;&#32441;&#29702;&#12290;</title><link>http://arxiv.org/abs/2102.03973</link><description>&lt;p&gt;
STS-GAN: &#25105;&#20204;&#33021;&#21542;&#20174;&#20219;&#24847;2D&#31034;&#20363;&#21512;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#23454;&#20307;&#32441;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
STS-GAN: Can We Synthesize Solid Texture with High Fidelity from Arbitrary 2D Exemplar?. (arXiv:2102.03973v7 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.03973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STS-GAN&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#32473;&#23450;&#30340;2D&#31034;&#20363;&#25193;&#23637;&#21040;&#20219;&#24847;3D&#23454;&#20307;&#32441;&#29702;&#65292;&#24182;&#21512;&#25104;&#20855;&#26377;&#39640;&#20445;&#30495;&#24230;&#21644;&#30456;&#20284;&#29305;&#24449;&#30340;&#23454;&#20307;&#32441;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#32441;&#29702;&#21512;&#25104;&#65288;STS&#65289;&#26159;&#23558;2D&#31034;&#20363;&#25193;&#23637;&#21040;3D&#23454;&#20307;&#20307;&#31215;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#22312;&#35745;&#31639;&#25668;&#24433;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#20934;&#30830;&#23398;&#20064;&#20219;&#24847;&#32441;&#29702;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21512;&#25104;&#39640;&#20445;&#30495;&#24230;&#23454;&#20307;&#32441;&#29702;&#30340;&#22833;&#36133;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26694;&#26550;&#65288;STS-GAN&#65289;&#65292;&#23558;&#32473;&#23450;&#30340;2D&#31034;&#20363;&#25193;&#23637;&#21040;&#20219;&#24847;3D&#23454;&#20307;&#32441;&#29702;&#12290;&#22312;STS-GAN&#20013;&#65292;&#22810;&#23610;&#24230;2D&#32441;&#29702;&#37492;&#21035;&#22120;&#35780;&#20272;&#32473;&#23450;&#30340;2D&#31034;&#20363;&#19982;&#29983;&#25104;&#30340;3D&#32441;&#29702;&#20999;&#29255;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20419;&#36827;3D&#32441;&#29702;&#29983;&#25104;&#22120;&#21512;&#25104;&#36924;&#30495;&#30340;&#23454;&#20307;&#32441;&#29702;&#12290;&#26368;&#21518;&#65292;&#23454;&#39564;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#19982;2D&#31034;&#20363;&#31867;&#20284;&#30340;&#35270;&#35273;&#29305;&#24449;&#30340;&#39640;&#20445;&#30495;&#24230;&#23454;&#20307;&#32441;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solid texture synthesis (STS), an effective way to extend a 2D exemplar to a 3D solid volume, exhibits advantages in computational photography. However, existing methods generally fail to accurately learn arbitrary textures, which may result in the failure to synthesize solid textures with high fidelity. In this paper, we propose a novel generative adversarial nets-based framework (STS-GAN) to extend the given 2D exemplar to arbitrary 3D solid textures. In STS-GAN, multi-scale 2D texture discriminators evaluate the similarity between the given 2D exemplar and slices from the generated 3D texture, promoting the 3D texture generator synthesizing realistic solid textures. Finally, experiments demonstrate that the proposed method can generate high-fidelity solid textures with similar visual characteristics to the 2D exemplar.
&lt;/p&gt;</description></item></channel></rss>