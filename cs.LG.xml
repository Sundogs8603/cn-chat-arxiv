<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#23398;&#20064;&#29575;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#20135;&#29983;&#30340;&#38544;&#24615;&#20559;&#24046;&#65292;&#21253;&#25324;&#31283;&#23450;&#30340;&#36793;&#30028;&#12289;&#24179;&#34913;&#21644;&#24377;&#23556;&#65292;&#24182;&#36890;&#36807;&#21457;&#23637;&#26032;&#30340;&#20840;&#23616;&#25910;&#25947;&#29702;&#35770;&#21644;&#30740;&#31350;&#33391;&#22909;&#35268;&#21017;&#24615;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#36825;&#20123;&#29616;&#35937;&#24402;&#32435;&#20026;&#21516;&#19968;&#29616;&#35937;&#30340;&#19981;&#21516;&#34920;&#29616;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.17087</link><description>&lt;p&gt;
&#33391;&#22909;&#30340;&#35268;&#21017;&#24615;&#21019;&#36896;&#20102;&#22823;&#23398;&#20064;&#29575;&#30340;&#38544;&#24615;&#20559;&#24046;&#65306;&#31283;&#23450;&#30340;&#36793;&#30028;&#65292;&#24179;&#34913;&#21644;&#24377;&#23556;
&lt;/p&gt;
&lt;p&gt;
Good regularity creates large learning rate implicit biases: edge of stability, balancing, and catapult. (arXiv:2310.17087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17087
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#23398;&#20064;&#29575;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#20135;&#29983;&#30340;&#38544;&#24615;&#20559;&#24046;&#65292;&#21253;&#25324;&#31283;&#23450;&#30340;&#36793;&#30028;&#12289;&#24179;&#34913;&#21644;&#24377;&#23556;&#65292;&#24182;&#36890;&#36807;&#21457;&#23637;&#26032;&#30340;&#20840;&#23616;&#25910;&#25947;&#29702;&#35770;&#21644;&#30740;&#31350;&#33391;&#22909;&#35268;&#21017;&#24615;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#36825;&#20123;&#29616;&#35937;&#24402;&#32435;&#20026;&#21516;&#19968;&#29616;&#35937;&#30340;&#19981;&#21516;&#34920;&#29616;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#26102;&#65292;&#22823;&#23398;&#20064;&#29575;&#20250;&#20135;&#29983;&#21508;&#31181;&#38544;&#24615;&#20559;&#24046;&#65292;&#21253;&#25324;&#31283;&#23450;&#30340;&#36793;&#30028;&#12289;&#24179;&#34913;&#21644;&#24377;&#23556;&#12290;&#36825;&#20123;&#29616;&#35937;&#26080;&#27861;&#29992;&#32463;&#20856;&#30340;&#20248;&#21270;&#29702;&#35770;&#24456;&#22909;&#22320;&#35299;&#37322;&#12290;&#23613;&#31649;&#22312;&#29702;&#35299;&#36825;&#20123;&#38544;&#24615;&#20559;&#24046;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#29702;&#35770;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#19981;&#28165;&#26970;&#23427;&#20204;&#22312;&#21738;&#20123;&#30446;&#26631;&#20989;&#25968;&#19978;&#20250;&#21457;&#29983;&#12290;&#26412;&#25991;&#23545;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#21021;&#22987;&#30340;&#27493;&#39588;&#65292;&#21363;&#36825;&#20123;&#38544;&#24615;&#20559;&#24046;&#23454;&#38469;&#19978;&#26159;&#21516;&#19968;&#20912;&#23665;&#30340;&#21508;&#31181;&#23574;&#31471;&#12290;&#24403;&#20248;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#19968;&#23450;&#30340;&#33391;&#22909;&#35268;&#21017;&#24615;&#65292;&#24182;&#19982;&#22823;&#23398;&#20064;&#29575;&#26799;&#24230;&#19979;&#38477;&#23545;&#21521;&#26356;&#24179;&#22374;&#21306;&#22495;&#31227;&#21160;&#30340;&#21487;&#35777;&#26126;&#20559;&#22909;&#30456;&#32467;&#21512;&#26102;&#65292;&#23601;&#20250;&#20135;&#29983;&#36825;&#20123;&#38750;&#24179;&#20961;&#30340;&#21160;&#21147;&#23398;&#29616;&#35937;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#23398;&#20064;&#29575;&#20840;&#23616;&#25910;&#25947;&#29702;&#35770;&#65292;&#38024;&#23545;&#19968;&#26063;&#38750;&#20984;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large learning rates, when applied to gradient descent for nonconvex optimization, yield various implicit biases including the edge of stability (Cohen et al., 2021), balancing (Wang et al., 2022), and catapult (Lewkowycz et al., 2020). These phenomena cannot be well explained by classical optimization theory. Though significant theoretical progress has been made in understanding these implicit biases, it remains unclear for which objective functions would they occur. This paper provides an initial step in answering this question, namely that these implicit biases are in fact various tips of the same iceberg. They occur when the objective function of optimization has some good regularity, which, in combination with a provable preference of large learning rate gradient descent for moving toward flatter regions, results in these nontrivial dynamical phenomena. To establish this result, we develop a new global convergence theory under large learning rates, for a family of nonconvex functi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;CMB&#35266;&#27979;&#20013;&#30340;&#23576;&#22467;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#27169;&#25311;&#23576;&#22467;&#21069;&#26223;&#24182;&#36827;&#34892;&#25104;&#20998;&#20998;&#31163;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;CMB&#23431;&#23449;&#23398;&#26465;&#20214;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#22312;&#25104;&#20998;&#20998;&#31163;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16285</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;CMB&#35266;&#27979;&#20013;&#30340;&#23576;&#22467;
&lt;/p&gt;
&lt;p&gt;
Removing Dust from CMB Observations with Diffusion Models. (arXiv:2310.16285v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;CMB&#35266;&#27979;&#20013;&#30340;&#23576;&#22467;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#27169;&#25311;&#23576;&#22467;&#21069;&#26223;&#24182;&#36827;&#34892;&#25104;&#20998;&#20998;&#31163;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19982;CMB&#23431;&#23449;&#23398;&#26465;&#20214;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#22312;&#25104;&#20998;&#20998;&#31163;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23431;&#23449;&#23398;&#20013;&#65292;&#36861;&#23547;&#23431;&#23449;&#24494;&#27874;&#32972;&#26223;(CMB)&#35266;&#27979;&#20013;&#30340;&#21407;&#22987;$B$-mode&#65292;&#31361;&#20986;&#20102;&#23545;&#38134;&#27827;&#23576;&#22467;&#21069;&#26223;&#30340;&#31934;&#30830;&#24314;&#27169;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#23576;&#22467;&#21069;&#26223;&#24314;&#27169;&#21450;&#20854;&#22312;&#25104;&#20998;&#20998;&#31163;&#20013;&#30340;&#24847;&#20041;&#12290;&#22312;&#20551;&#35774;&#20855;&#26377;&#24050;&#30693;&#23431;&#23449;&#23398;(&#25110;&#21327;&#26041;&#24046;&#30697;&#38453;)&#30340;&#39640;&#26031;CMB&#30340;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#22312;&#23576;&#22467;&#36752;&#23556;&#22320;&#22270;&#30340;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#20854;&#37319;&#26679;&#36807;&#31243;&#30452;&#25509;&#19982;&#25104;&#20998;&#20998;&#31163;&#19978;&#19979;&#25991;&#20013;&#30340;&#21518;&#39564;&#37319;&#26679;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#29992;&#27169;&#25311;&#30340;&#23576;&#22467;&#36752;&#23556;&#21644;CMB&#30340;&#28151;&#21512;&#29289;&#26469;&#35828;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#24456;&#22909;&#22320;&#24674;&#22797;&#32452;&#20998;&#30340;&#24120;&#35265;&#25688;&#35201;&#32479;&#35745;&#37327;(&#21151;&#29575;&#35889;&#12289;&#38389;&#21487;&#22827;&#26031;&#22522;&#20989;&#25968;)&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19982;CMB&#23431;&#23449;&#23398;&#26465;&#20214;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#27604;&#21333;&#19968;&#23431;&#23449;&#23398;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25104;&#20998;&#20998;&#31163;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#23558;&#22312;&#26410;&#26469;&#30340;&#24037;&#20316;&#20013;&#29992;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#23431;&#23449;&#23398;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cosmology, the quest for primordial $B$-modes in cosmic microwave background (CMB) observations has highlighted the critical need for a refined model of the Galactic dust foreground. We investigate diffusion-based modeling of the dust foreground and its interest for component separation. Under the assumption of a Gaussian CMB with known cosmology (or covariance matrix), we show that diffusion models can be trained on examples of dust emission maps such that their sampling process directly coincides with posterior sampling in the context of component separation. We illustrate this on simulated mixtures of dust emission and CMB. We show that common summary statistics (power spectrum, Minkowski functionals) of the components are well recovered by this process. We also introduce a model conditioned by the CMB cosmology that outperforms models trained using a single cosmology on component separation. Such a model will be used in future work for diffusion-based cosmological inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.10998</link><description>&lt;p&gt;
&#20351;&#29992;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;
&lt;/p&gt;
&lt;p&gt;
Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation. (arXiv:2310.10998v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#35770;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#22270;&#30340;&#35268;&#27169;&#20351;&#24471;GNNs&#30340;&#23454;&#26102;&#25512;&#35770;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#21487;&#25193;&#23637;GNNs&#21033;&#29992;&#32447;&#24615;&#20256;&#25773;&#23545;&#29305;&#24449;&#36827;&#34892;&#39044;&#22788;&#29702;&#24182;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#35770;&#36807;&#31243;&#65292;&#20294;&#22312;&#23545;&#26410;&#30693;&#33410;&#28857;&#36827;&#34892;&#25512;&#35770;&#26102;&#20173;&#28982;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#38656;&#35201;&#24050;&#30693;&#19988;&#22266;&#23450;&#30340;&#22270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#36895;&#36825;&#31181;&#24402;&#32435;&#35774;&#32622;&#19979;&#30340;&#21487;&#25193;&#23637;GNNs&#25512;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#20256;&#25773;&#26694;&#26550;&#21644;&#20004;&#31181;&#26032;&#30340;&#33410;&#28857;&#33258;&#36866;&#24212;&#20256;&#25773;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#33410;&#28857;&#30340;&#25299;&#25169;&#20449;&#24687;&#33258;&#23450;&#20041;&#27599;&#20010;&#33410;&#28857;&#30340;&#26368;&#20339;&#20256;&#25773;&#28145;&#24230;&#65292;&#20174;&#32780;&#36991;&#20813;&#20887;&#20313;&#29305;&#24449;&#20256;&#25773;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#31649;&#29702;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#24310;&#36831;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#34917;&#20607;&#25439;&#22833;&#30340;&#31934;&#24230;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#34917;&#20607;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#20801;&#35768;&#20256;&#25773;&#30340;&#23618;&#25968;&#36229;&#36807;&#25152;&#36873;&#25321;&#30340;&#28145;&#24230;&#65292;&#20197;&#25552;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have exhibited exceptional efficacy in a diverse array of applications. However, the sheer size of large-scale graphs presents a significant challenge to real-time inference with GNNs. Although existing Scalable GNNs leverage linear propagation to preprocess the features and accelerate the training and inference procedure, these methods still suffer from scalability issues when making inferences on unseen nodes, as the feature preprocessing requires the graph to be known and fixed. To further accelerate Scalable GNNs inference in this inductive setting, we propose an online propagation framework and two novel node-adaptive propagation methods that can customize the optimal propagation depth for each node based on its topological information and thereby avoid redundant feature propagation. The trade-off between accuracy and latency can be flexibly managed through simple hyper-parameters to accommodate various latency constraints. Moreover, to compensate for 
&lt;/p&gt;</description></item><item><title>SAFE is a novel line notation for chemical structures that reimagines SMILES strings as an unordered sequence of interconnected fragment blocks, streamlining complex generative tasks and facilitating fragment-constrained design without the need for intricate decoding or graph-based models. It has been demonstrated to be effective through extensive experimentation.</title><link>http://arxiv.org/abs/2310.10773</link><description>&lt;p&gt;
&#12298;&#24517;&#39035;&#23433;&#20840;&#65306;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#35774;&#35745;&#26694;&#26550;&#12299;
&lt;/p&gt;
&lt;p&gt;
Gotta be SAFE: A New Framework for Molecular Design. (arXiv:2310.10773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10773
&lt;/p&gt;
&lt;p&gt;
SAFE is a novel line notation for chemical structures that reimagines SMILES strings as an unordered sequence of interconnected fragment blocks, streamlining complex generative tasks and facilitating fragment-constrained design without the need for intricate decoding or graph-based models. It has been demonstrated to be effective through extensive experimentation.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20998;&#23376;&#23383;&#31526;&#20018;&#34920;&#31034;&#65292;&#22914;SMILES&#65292;&#22312;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20998;&#23376;&#35774;&#35745;&#20013;&#32463;&#24120;&#38754;&#20020;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20197;&#38750;&#36830;&#32493;&#30340;&#26041;&#24335;&#25551;&#36848;&#20102;&#20998;&#23376;&#30340;&#20122;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#39034;&#24207;&#36830;&#25509;&#30340;&#29255;&#27573;&#23884;&#20837;&#65288;SAFE&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#39062;&#30340;&#21270;&#23398;&#32467;&#26500;&#32447;&#24615;&#31526;&#21495;&#34920;&#31034;&#27861;&#12290;SAFE&#23558;SMILES&#23383;&#31526;&#20018;&#37325;&#26032;&#26500;&#24819;&#20026;&#19968;&#20010;&#26080;&#24207;&#30340;&#20114;&#36830;&#29255;&#27573;&#22359;&#24207;&#21015;&#65292;&#21516;&#26102;&#19982;&#29616;&#26377;&#30340;SMILES&#35299;&#26512;&#22120;&#23436;&#20840;&#20860;&#23481;&#12290;&#23427;&#31616;&#21270;&#20102;&#22797;&#26434;&#30340;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;&#39592;&#26550;&#35013;&#39280;&#12289;&#29255;&#27573;&#36830;&#25509;&#12289;&#32858;&#21512;&#29289;&#29983;&#25104;&#21644;&#39592;&#26550;&#36339;&#36291;&#65292;&#21516;&#26102;&#20419;&#36827;&#20102;&#21463;&#29255;&#27573;&#32422;&#26463;&#30340;&#35774;&#35745;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#32321;&#29712;&#30340;&#35299;&#30721;&#25110;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21253;&#21547;11&#20159;&#20010;SAFE&#34920;&#31034;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;8700&#19975;&#21442;&#25968;&#30340;&#31867;GPT2&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;SAFE&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;SAFE-GPT&#27169;&#22411;&#23637;&#31034;&#20102;&#22810;&#21151;&#33021;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Traditional molecular string representations, such as SMILES, often pose challenges for AI-driven molecular design due to their non-sequential depiction of molecular substructures. To address this issue, we introduce Sequential Attachment-based Fragment Embedding (SAFE), a novel line notation for chemical structures. SAFE reimagines SMILES strings as an unordered sequence of interconnected fragment blocks while maintaining full compatibility with existing SMILES parsers. It streamlines complex generative tasks, including scaffold decoration, fragment linking, polymer generation, and scaffold hopping, while facilitating autoregressive generation for fragment-constrained design, thereby eliminating the need for intricate decoding or graph-based models. We demonstrate the effectiveness of SAFE by training an 87-million-parameter GPT2-like model on a dataset containing 1.1 billion SAFE representations. Through extensive experimentation, we show that our SAFE-GPT model exhibits versatile an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Score dynamics (SD) &#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;1 ps&#30340;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#30701;&#38142;&#28919;&#28867;&#26696;&#20363;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01678</link><description>&lt;p&gt;
Score dynamics: &#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#30382;&#31186;&#26102;&#38388;&#27493;&#25552;&#39640;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#35268;&#27169;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score dynamics: scaling molecular dynamics with picosecond timesteps via conditional diffusion model. (arXiv:2310.01678v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01678
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Score dynamics (SD) &#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;1 ps&#30340;&#26102;&#38388;&#27493;&#38271;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#30701;&#38142;&#28919;&#28867;&#26696;&#20363;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Score dynamics (SD) &#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#28436;&#21270;&#31639;&#23376;&#65292;&#29992;&#20110;&#21407;&#23376;&#32423;&#21644;&#31895;&#31890;&#21270;&#21160;&#21147;&#23398;&#12290;SD&#20197;&#20998;&#25968;&#20026;&#20013;&#24515;&#65292;&#21363;&#19982;&#21160;&#24577;&#33258;&#30001;&#24230;&#30340;&#36716;&#25442;&#23545;&#25968;&#27010;&#29575;&#23548;&#25968;&#30456;&#20851;&#30340;&#37327;&#12290;&#21518;&#32773;&#22312;&#20998;&#25968;&#26102;&#38388;&#27493;&#20013;&#36215;&#21040;&#19982;MD&#20013;&#21147;&#22330;&#30456;&#21516;&#30340;&#20316;&#29992;&#65292;&#20294;&#22312;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20013;&#29992;&#20110;&#29983;&#25104;&#21160;&#24577;&#21464;&#37327;&#30340;&#31163;&#25955;&#36716;&#21464;&#12290;&#36825;&#31181;&#26102;&#38388;&#27493;&#38271;&#21487;&#20197;&#27604;&#20856;&#22411;&#30340;MD&#26102;&#38388;&#27493;&#38271;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Score dynamics&#27169;&#22411;&#65292;&#29992;&#20110;&#28436;&#21270;&#20197;1~ps&#26102;&#38388;&#27493;&#38271;&#30340;&#29616;&#23454;&#20998;&#23376;&#20307;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#19993;&#27688;&#37240;&#20108;&#32957;&#21644;&#27700;&#28342;&#28082;&#20013;&#30340;&#30701;&#38142;&#28919;&#28867;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;Score dynamics&#30340;&#25928;&#33021;&#12290;&#36890;&#36807;&#20174;&#26465;&#20214;&#27010;&#29575;&#30340;&#24179;&#31283;&#20998;&#24067;&#20013;&#25512;&#23548;&#20986;&#30340;&#24179;&#34913;&#39044;&#27979;&#21644;&#23545;&#36716;&#25442;&#36895;&#29575;&#21644;&#36716;&#25442;&#30340;&#21160;&#21147;&#23398;&#39044;&#27979;&#36827;&#34892;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose score dynamics (SD), a general framework for learning effective evolution operators for atomistic as well as coarse-grained dynamics from molecular-dynamics (MD) simulations. SD is centered around scores, or derivatives of the transition log-probability with respect to the dynamical degrees of freedom. The latter play the same role as force fields in MD but are used in denoising diffusion probability models to generate discrete transitions of the dynamical variables in an SD timestep, which can be orders of magnitude larger than a typical MD timestep. In this work, we construct graph neural network based score dynamics models of realistic molecular systems that are evolved with 1~ps timesteps. We demonstrate the efficacy of score dynamics with case studies of alanine dipeptide and short alkanes in aqueous solution. Both equilibrium predictions derived from the stationary distributions of the conditional probability and kinetic predictions for the transition rates and transit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#21644;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#24335;&#26816;&#27979;&#65292;&#20272;&#35745;&#20102;&#20010;&#21035;&#31038;&#21306;&#30340;&#20154;&#21475;&#23494;&#24230;&#12289;&#23478;&#24237;&#25910;&#20837;&#20013;&#20301;&#25968;&#21644;&#25945;&#32946;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.16808</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#31890;&#24230;&#65306;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#27491;&#23556;&#24433;&#20687;&#21644;&#28151;&#21512;&#23398;&#20064;&#20272;&#35745;&#23621;&#27665;&#31038;&#21306;&#30340;&#31119;&#31049;
&lt;/p&gt;
&lt;p&gt;
Granularity at Scale: Estimating Neighborhood Well-Being from High-Resolution Orthographic Imagery and Hybrid Learning. (arXiv:2309.16808v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#21644;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#24335;&#26816;&#27979;&#65292;&#20272;&#35745;&#20102;&#20010;&#21035;&#31038;&#21306;&#30340;&#20154;&#21475;&#23494;&#24230;&#12289;&#23478;&#24237;&#25910;&#20837;&#20013;&#20301;&#25968;&#21644;&#25945;&#32946;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#19990;&#30028;&#19978;&#35768;&#22810;&#22320;&#21306;&#32570;&#20047;&#26377;&#20851;&#23621;&#27665;&#31119;&#31049;&#30340;&#22522;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#36965;&#24863;&#33719;&#21462;&#30340;&#39640;&#31354;&#24433;&#20687;&#65292;&#22914;&#21355;&#26143;&#25110;&#39134;&#26426;&#65292;&#21487;&#20197;&#20316;&#20026;&#31397;&#35270;&#22320;&#38754;&#19978;&#29983;&#27963;&#29366;&#20917;&#30340;&#31383;&#21475;&#65292;&#24182;&#24110;&#21161;&#22635;&#34917;&#31038;&#21306;&#20449;&#24687;&#31232;&#32570;&#30340;&#22320;&#26041;&#65292;&#36739;&#23567;&#22320;&#29702;&#23610;&#24230;&#30340;&#20272;&#35745;&#38656;&#35201;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#20256;&#24863;&#22120;&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#20998;&#36776;&#29575;&#30340;&#25552;&#39640;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#24555;&#36895;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#26816;&#27979;&#27169;&#24335;&#65292;&#20174;&#32780;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#20854;&#20182;&#20449;&#24687;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;&#30417;&#30563;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#35270;&#35273;&#35789;&#34955;&#30340;&#21322;&#30417;&#30563;&#32858;&#31867;&#65289;&#22914;&#20309;&#20174;&#20844;&#24320;&#21487;&#29992;&#30340;&#39640;&#20998;&#36776;&#29575;&#24433;&#20687;&#20013;&#20272;&#35745;&#20010;&#21035;&#31038;&#21306;&#30340;&#20154;&#21475;&#23494;&#24230;&#12289;&#23478;&#24237;&#25910;&#20837;&#20013;&#20301;&#25968;&#21644;&#25945;&#32946;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many areas of the world are without basic information on the well-being of the residing population due to limitations in existing data collection methods. Overhead images obtained remotely, such as from satellite or aircraft, can help serve as windows into the state of life on the ground and help "fill in the gaps" where community information is sparse, with estimates at smaller geographic scales requiring higher resolution sensors. Concurrent with improved sensor resolutions, recent advancements in machine learning and computer vision have made it possible to quickly extract features from and detect patterns in image data, in the process correlating these features with other information. In this work, we explore how well two approaches, a supervised convolutional neural network and semi-supervised clustering based on bag-of-visual-words, estimate population density, median household income, and educational attainment of individual neighborhoods from publicly available high-resolution 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16512</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#20998;&#26512;&#34920;&#36798;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#21270;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#30001;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#38382;&#39064;&#21487;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#22312;&#20854;&#20013;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#20197;&#25968;&#25454;&#21521;&#37327;&#29983;&#25104;&#30340;&#19977;&#35282;&#24418;&#21644;&#24179;&#34892;&#20307;&#30340;&#26377;&#31526;&#21495;&#20307;&#31215;&#34920;&#31034;&#12290;&#20984;&#38382;&#39064;&#36890;&#36807;$\ell_1$&#27491;&#21017;&#21270;&#25214;&#21040;&#26679;&#26412;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#20197;&#21457;&#29616;&#20165;&#30456;&#20851;&#30340;&#26964;&#31215;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
&lt;/p&gt;</description></item><item><title>DifAttack&#26159;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#39640;&#25928;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#23545;&#25239;&#29305;&#24449;&#24182;&#21033;&#29992;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#26597;&#35810;&#21453;&#39304;&#29983;&#25104;&#25104;&#21151;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.14585</link><description>&lt;p&gt;
DifAttack: &#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#39640;&#25928;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DifAttack: Query-Efficient Black-Box Attack via Disentangled Feature Space. (arXiv:2309.14585v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14585
&lt;/p&gt;
&lt;p&gt;
DifAttack&#26159;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#39640;&#25928;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#23545;&#25239;&#29305;&#24449;&#24182;&#21033;&#29992;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#26597;&#35810;&#21453;&#39304;&#29983;&#25104;&#25104;&#21151;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#20855;&#26377;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#39640;&#25928;&#22522;&#20110;&#20998;&#25968;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32806;&#29305;&#24449;&#31354;&#38388;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;DifAttack&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#22312;&#25972;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DifAttack&#39318;&#20808;&#23558;&#22270;&#20687;&#30340;&#28508;&#22312;&#29305;&#24449;&#35299;&#32806;&#20026;&#23545;&#25239;&#29305;&#24449;&#21644;&#35270;&#35273;&#29305;&#24449;&#65292;&#20854;&#20013;&#21069;&#32773;&#20027;&#23548;&#22270;&#20687;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#32780;&#21518;&#32773;&#20027;&#35201;&#20915;&#23450;&#20854;&#35270;&#35273;&#22806;&#35266;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#21487;&#29992;&#26367;&#20195;&#27169;&#22411;&#36890;&#36807;&#30333;&#30418;&#25915;&#20987;&#26041;&#27861;&#29983;&#25104;&#30340;&#24178;&#20928;&#22270;&#20687;&#21644;&#20854;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#35757;&#32451;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#35299;&#32806;&#12290;&#26368;&#32456;&#65292;DifAttack&#26681;&#25454;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#26597;&#35810;&#21453;&#39304;&#65292;&#36845;&#20195;&#20248;&#21270;&#23545;&#25239;&#29305;&#24449;&#65292;&#30452;&#21040;&#25104;&#21151;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#29305;&#24449;&#19981;&#21464;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#36991;&#20813;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
This work investigates efficient score-based black-box adversarial attacks with a high Attack Success Rate (ASR) and good generalizability. We design a novel attack method based on a Disentangled Feature space, called DifAttack, which differs significantly from the existing ones operating over the entire feature space. Specifically, DifAttack firstly disentangles an image's latent feature into an adversarial feature and a visual feature, where the former dominates the adversarial capability of an image, while the latter largely determines its visual appearance. We train an autoencoder for the disentanglement by using pairs of clean images and their Adversarial Examples (AEs) generated from available surrogate models via white-box attack methods. Eventually, DifAttack iteratively optimizes the adversarial feature according to the query feedback from the victim model until a successful AE is generated, while keeping the visual feature unaltered. In addition, due to the avoidance of using
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.03648</link><description>&lt;p&gt;
GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;Lipschitz&#29305;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03648
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;GNN&#30340;Lipschitz&#30028;&#38480;&#34920;&#24449;&#20102;GNN&#23545;&#20844;&#24179;&#24615;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#21644;&#22266;&#26377;&#20559;&#20506;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#23545;&#20110;&#38480;&#21046;GNN&#36755;&#20986;&#30340;&#25200;&#21160;&#20197;&#20445;&#38556;&#20844;&#24179;&#24615;&#35757;&#32451;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lipschitz&#30028;&#38480;&#26159;&#20174;&#40065;&#26834;&#32479;&#35745;&#23398;&#20013;&#20511;&#37492;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#38480;&#21046;&#36755;&#20986;&#30456;&#23545;&#20110;&#36755;&#20837;&#30340;&#26368;&#22823;&#21464;&#21270;&#65292;&#32771;&#34385;&#21040;&#30456;&#20851;&#30340;&#38750;&#20851;&#38190;&#20559;&#20506;&#22240;&#32032;&#12290;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#26597;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#31283;&#23450;&#24615;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#19978;&#25805;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#27809;&#26377;&#30740;&#31350;&#35843;&#26597;GNN&#30340;Lipschitz&#30028;&#38480;&#20197;&#25581;&#31034;&#27169;&#22411;&#36755;&#20986;&#30340;&#31283;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#20855;&#26377;&#22266;&#26377;&#20559;&#20506;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#12290;&#30001;&#20110;&#24120;&#35265;&#22270;&#24418;&#25968;&#25454;&#22312;GNN&#35757;&#32451;&#20013;&#23384;&#22312;&#22266;&#26377;&#20559;&#24046;&#65292;&#36825;&#32473;&#38480;&#21046;&#30001;&#36755;&#20837;&#20559;&#24046;&#24341;&#36215;&#30340;GNN&#36755;&#20986;&#25200;&#21160;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#38556;&#20844;&#24179;&#24615;&#65292;&#24102;&#26469;&#20102;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23613;&#31649;Lipschitz&#24120;&#25968;&#22312;&#25511;&#21046;&#27431;&#20960;&#37324;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#26377;&#25152;&#24212;&#29992;&#65292;&#20294;&#31934;&#30830;Lipschitz&#24120;&#25968;&#30340;&#35745;&#31639;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. It is an efficient and provable method for examining the output stability of machine learning models without incurring additional computation costs. Recently, Graph Neural Networks (GNNs), which operate on non-Euclidean data, have gained significant attention. However, no previous research has investigated the GNN Lipschitz bounds to shed light on stabilizing model outputs, especially when working on non-Euclidean data with inherent biases. Given the inherent biases in common graph data used for GNN training, it poses a serious challenge to constraining the GNN output perturbations induced by input biases, thereby safeguarding fairness during training. Recently, despite the Lipschitz constant's use in controlling the stability of Euclideanneural networks, the calculation of the precise Lipschitz constant rem
&lt;/p&gt;</description></item><item><title>MedShapeNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#23545;&#20110;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2308.16139</link><description>&lt;p&gt;
MedShapeNet - &#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22823;&#35268;&#27169;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer Vision. (arXiv:2308.16139v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16139
&lt;/p&gt;
&lt;p&gt;
MedShapeNet&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#21307;&#23398;&#24418;&#29366;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#31181;&#23545;&#20110;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MedShapeNet&#65292;&#19968;&#20010;&#21253;&#21547;&#20102;&#35299;&#21078;&#24418;&#29366;&#65288;&#22914;&#39592;&#39612;&#12289;&#22120;&#23448;&#12289;&#34880;&#31649;&#65289;&#21644;&#19977;&#32500;&#25163;&#26415;&#22120;&#26800;&#27169;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#20043;&#21069;&#65292;&#32479;&#35745;&#24418;&#29366;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#35777;&#26126;&#20102;&#24418;&#29366;&#24120;&#34987;&#29992;&#26469;&#25551;&#36848;&#21307;&#23398;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#26159;&#22522;&#20110;&#20307;&#32032;&#30340;&#12290;&#30456;&#21453;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#24418;&#29366;&#65288;&#21253;&#25324;&#20307;&#32032;&#21344;&#25454;&#32593;&#26684;&#12289;&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#38544;&#24335;&#34920;&#38754;&#27169;&#22411;&#65289;&#26159;&#19977;&#32500;&#25968;&#25454;&#30340;&#39318;&#36873;&#34920;&#31034;&#26041;&#27861;&#65292;&#36825;&#19968;&#28857;&#21487;&#20197;&#20174;&#22823;&#37327;&#20851;&#20110;&#24418;&#29366;&#30340;&#25991;&#31456;&#21450;&#22312;&#39030;&#32423;&#35745;&#31639;&#26426;&#35270;&#35273;&#20250;&#35758;&#65288;&#22914;IEEE/CVF&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#27169;&#24335;&#35782;&#21035;&#20250;&#35758;&#65288;CVPR&#65289;&#65289;&#20013;&#35265;&#21040;&#65292;&#21516;&#26102;ShapeNet&#65288;&#32422;51300&#20010;&#27169;&#22411;&#65289;&#21644;&#26222;&#26519;&#26031;&#39039;ModelNet&#65288;127,915&#20010;&#27169;&#22411;&#65289;&#30340;&#27969;&#34892;&#24230;&#20063;&#22312;&#19981;&#26029;&#22686;&#21152;&#12290;MedShapeNet&#30340;&#21019;&#24314;&#26159;&#20026;&#20102;&#20316;&#20026;&#36825;&#20123;&#24120;&#29992;&#24418;&#29366;&#22522;&#20934;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MedShapeNet, a large collection of anatomical shapes (e.g., bones, organs, vessels) and 3D surgical instrument models. Prior to the deep learning era, the broad application of statistical shape models (SSMs) in medical image analysis is evidence that shapes have been commonly used to describe medical data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in medical imaging are predominantly voxel-based. In computer vision, on the contrary, shapes (including, voxel occupancy grids, meshes, point clouds and implicit surface models) are preferred data representations in 3D, as seen from the numerous shape-related publications in premier vision conferences, such as the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as well as the increasing popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is created as an alternative to these commonly used shape benchmarks to f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.14815</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Statistical Verification with Imprecise Neural Networks. (arXiv:2308.14815v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#40065;&#26834;&#32479;&#35745;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#65292;&#21487;&#20197;&#22312;&#22823;&#37327;&#30340;&#20998;&#24067;&#19978;&#25552;&#20379;&#23545;&#40657;&#30418;&#31995;&#32479;&#34892;&#20026;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AI&#23433;&#20840;&#39046;&#22495;&#65292;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#22312;&#39640;&#32500;&#33258;&#20027;&#31995;&#32479;&#30340;&#34892;&#20026;&#19978;&#25552;&#20379;&#20445;&#35777;&#12290;&#20197;&#21487;&#36798;&#24615;&#20998;&#26512;&#20026;&#20013;&#24515;&#30340;&#39564;&#35777;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#32780;&#32431;&#31929;&#30340;&#32479;&#35745;&#26041;&#27861;&#21463;&#21040;&#23545;&#37319;&#26679;&#36807;&#31243;&#30340;&#20998;&#24067;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#40657;&#30418;&#31995;&#32479;&#30340;&#20998;&#24067;&#40065;&#26834;&#29256;&#26412;&#30340;&#32479;&#35745;&#39564;&#35777;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#24615;&#33021;&#20445;&#35777;&#36866;&#29992;&#20110;&#22823;&#37327;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26680;&#24515;&#37096;&#20998;&#26159;&#19968;&#31181;&#31216;&#20026;&#19981;&#31934;&#30830;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#23427;&#25552;&#20379;&#20102;&#19981;&#30830;&#23450;&#24615;&#20197;&#25351;&#23548;&#20027;&#21160;&#23398;&#20064;&#12290;&#20027;&#21160;&#23398;&#20064;&#20351;&#29992;&#20102;&#19968;&#31181;&#31216;&#20026;Sherlock&#30340;&#20840;&#38754;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24037;&#20855;&#26469;&#25910;&#38598;&#26679;&#26412;&#12290;&#22312;openAI gym Mujoco&#29615;&#22659;&#20013;&#20351;&#29992;&#22810;&#20010;&#29289;&#29702;&#27169;&#25311;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A particularly challenging problem in AI safety is providing guarantees on the behavior of high-dimensional autonomous systems. Verification approaches centered around reachability analysis fail to scale, and purely statistical approaches are constrained by the distributional assumptions about the sampling process. Instead, we pose a distributionally robust version of the statistical verification problem for black-box systems, where our performance guarantees hold over a large family of distributions. This paper proposes a novel approach based on a combination of active learning, uncertainty quantification, and neural network verification. A central piece of our approach is an ensemble technique called Imprecise Neural Networks, which provides the uncertainty to guide active learning. The active learning uses an exhaustive neural-network verification tool Sherlock to collect samples. An evaluation on multiple physical simulators in the openAI gym Mujoco environments with reinforcement-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12696</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#23398;&#20064;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Disentanglement Learning via Topology. (arXiv:2308.12696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25439;&#22833;&#23454;&#29616;&#35299;&#32544;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#35770;&#25991;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;TopDis&#65288;&#25299;&#25169;&#35299;&#32544;&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;&#22810;&#23610;&#24230;&#25299;&#25169;&#25439;&#22833;&#39033;&#23398;&#20064;&#35299;&#32544;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#35299;&#32544;&#26159;&#25968;&#25454;&#34920;&#31034;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#32423;&#35748;&#30693;&#30340;&#23454;&#29616;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#22522;&#20110;VAE&#30340;&#26368;&#26032;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#28508;&#21464;&#37327;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#24635;&#20307;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#35299;&#32544;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#23646;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#35299;&#32544;&#65292;&#29305;&#21035;&#26159;&#20248;&#21270;&#25968;&#25454;&#27969;&#24418;&#36941;&#21382;&#30340;&#25299;&#25169;&#30456;&#20284;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#29992;&#20110;&#35299;&#32544;&#30340;&#21487;&#24494;&#25299;&#25169;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25299;&#25169;&#25439;&#22833;&#30456;&#23545;&#20110;&#26368;&#26032;&#32467;&#26524;&#25913;&#36827;&#20102;&#35299;&#32544;&#24471;&#20998;&#65292;&#22914;MIG&#12289;FactorVAE&#24471;&#20998;&#12289;SAP&#24471;&#20998;&#21644;DCI&#35299;&#32544;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TopDis (Topological Disentanglement), a method for learning disentangled representations via adding multi-scale topological loss term. Disentanglement is a crucial property of data representations substantial for the explainability and robustness of deep learning models and a step towards high-level cognition. The state-of-the-art method based on VAE minimizes the total correlation of the joint distribution of latent variables. We take a different perspective on disentanglement by analyzing topological properties of data manifolds. In particular, we optimize the topological similarity for data manifolds traversals. To the best of our knowledge, our paper is the first one to propose a differentiable topological loss for disentanglement. Our experiments have shown that the proposed topological loss improves disentanglement scores such as MIG, FactorVAE score, SAP score and DCI disentanglement score with respect to state-of-the-art results. Our method works in an unsupervised m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21517;&#20026;MultiPL-T&#65292;&#36890;&#36807;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.09895</link><description>&lt;p&gt;
&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#29992;&#20110;&#20195;&#30721;LLMs
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21517;&#20026;MultiPL-T&#65292;&#36890;&#36807;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20195;&#30721;LLMs&#65288;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#24320;&#22987;&#23545;&#32534;&#31243;&#23454;&#36341;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20195;&#30721;LLMs&#36824;&#25104;&#20026;&#32534;&#31243;&#35821;&#35328;&#21644;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20195;&#30721;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#22312;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20195;&#30721;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#20805;&#20998;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;Java&#12289;Python&#25110;JavaScript&#65289;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20687;OCaml&#21644;Racket&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#25552;&#39640;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#21487;&#29992;&#20110;&#24494;&#35843;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;LLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;MultiPL-T&#65292;&#23427;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#21313;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#28789;&#27963;&#21644;&#22266;&#23450;&#38271;&#24230;&#32422;&#26463;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.08641</link><description>&lt;p&gt;
&#38750;&#21333;&#35843;&#36882;&#22686;&#30340;&#39034;&#24207;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-monotone Sequential Submodular Maximization. (arXiv:2308.08641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#32771;&#34385;&#20102;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#28789;&#27963;&#21644;&#22266;&#23450;&#38271;&#24230;&#32422;&#26463;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23376;&#27169;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#39034;&#24207;&#23376;&#27169;&#26368;&#22823;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#19968;&#20010;&#24453;&#36873;&#38598;&#21512;V&#20013;&#36873;&#25321;&#24182;&#23545;&#19968;&#32452;k&#20010;&#39033;&#36827;&#34892;&#25490;&#24207;&#65292;&#20351;&#24471;k&#20010;&#65288;&#21487;&#33021;&#20026;&#38750;&#21333;&#35843;&#36882;&#22686;&#30340;&#65289;&#23376;&#27169;&#20989;&#25968;f1&#65292;...&#65292;fk&#65288;&#20854;&#20013;&#27599;&#20010;&#20989;&#25968;fj&#23558;&#36825;&#20010;&#24207;&#21015;&#30340;&#21069;j&#20010;&#39033;&#20316;&#20026;&#36755;&#20837;&#65289;&#30340;&#21152;&#26435;&#27714;&#21644;&#26368;&#22823;&#21270;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#35843;&#35774;&#32622;&#19978;&#65292;&#20551;&#35774;&#23376;&#27169;&#20989;&#25968;&#26159;&#38750;&#36882;&#20943;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#27604;&#22914;&#22810;&#26679;&#24615;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#21521;&#29616;&#26377;&#38598;&#21512;&#28155;&#21152;&#39033;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#25928;&#29992;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#28789;&#27963;&#21644;&#22266;&#23450;&#38271;&#24230;&#32422;&#26463;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study a fundamental problem in submodular optimization, which is called sequential submodular maximization. Specifically, we aim to select and rank a group of $k$ items from a ground set $V$ such that the weighted summation of $k$ (possibly non-monotone) submodular functions $f_1, \cdots ,f_k: 2^V \rightarrow \mathbb{R}^+$ is maximized, here each function $f_j$ takes the first $j$ items from this sequence as input. The existing research on sequential submodular maximization has predominantly concentrated on the monotone setting, assuming that the submodular functions are non-decreasing. However, in various real-world scenarios, like diversity-aware recommendation systems, adding items to an existing set might negatively impact the overall utility. In response, this paper pioneers the examination of the aforementioned problem with non-monotone submodular functions and offers effective solutions for both flexible and fixed length constraints, as well as a special case w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#21512;&#25104;&#21644;&#28210;&#26579;NeRF&#23545;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#28145;&#24230;&#22330;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21160;&#24577;&#38452;&#24433;&#24182;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2308.04669</link><description>&lt;p&gt;
&#24555;&#36895;NeRF&#21512;&#25104;&#21644;&#28210;&#26579;&#30340;&#36890;&#29992;&#38544;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#26694;&#26550;&#65292;&#21487;&#20197;&#24555;&#36895;&#21512;&#25104;&#21644;&#28210;&#26579;NeRF&#23545;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#28145;&#24230;&#22330;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21160;&#24577;&#38452;&#24433;&#24182;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21508;&#31181;&#31070;&#32463;&#36752;&#23556;&#22330;&#26041;&#27861;&#22312;&#39640;&#28210;&#26579;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21152;&#36895;&#26041;&#27861;&#19987;&#38376;&#21270;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#21508;&#31181;&#38544;&#24335;&#26041;&#27861;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;NeRF&#20316;&#21697;&#36827;&#34892;&#23454;&#26102;&#21512;&#25104;&#12290;&#30001;&#20110;NeRF&#20381;&#36182;&#20110;&#27839;&#20809;&#32447;&#37319;&#26679;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#20379;&#19968;&#33324;&#24615;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38544;&#24335;&#27969;&#27700;&#32447;&#26469;&#24555;&#36895;&#21512;&#25104;NeRF&#23545;&#35937;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#20351;&#24471;&#21160;&#24577;&#38452;&#24433;&#21487;&#20197;&#20351;&#29992;&#35299;&#26512;&#20809;&#28304;&#22312;&#23545;&#35937;&#20869;&#37096;&#25110;&#23545;&#35937;&#20043;&#38388;&#36827;&#34892;&#25237;&#23556;&#65292;&#21516;&#26102;&#20801;&#35768;&#22810;&#20010;NeRF&#23545;&#35937;&#20197;&#20219;&#24847;&#21018;&#20307;&#21464;&#25442;&#26080;&#32541;&#22320;&#25918;&#32622;&#21644;&#28210;&#26579;&#22312;&#19968;&#36215;&#12290;&#20027;&#35201;&#22320;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#28145;&#24230;&#22330;&#65288;NeDF&#65289;&#30340;&#26032;&#30340;&#34920;&#38754;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20801;&#35768;&#20809;&#32447;&#21644;&#38544;&#24335;&#34920;&#38754;&#20043;&#38388;&#30340;&#30452;&#25509;&#30456;&#20132;&#35745;&#31639;&#26469;&#24555;&#36895;&#30830;&#23450;&#23545;&#35937;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#23427;&#21033;&#29992;&#19968;&#20010;&#20132;&#28857;&#31070;&#32463;&#32593;&#32476;&#26469;&#21152;&#36895;&#26597;&#35810;NeRF&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a variety of Neural radiance fields methods have garnered remarkable success in high render speed. However, current accelerating methods is specialized and not compatible for various implicit method, which prevent a real-time composition over different kinds of NeRF works. Since NeRF relies on sampling along rays, it's possible to provide a guidance generally. We propose a general implicit pipeline to rapidly compose NeRF objects. This new method enables the casting of dynamic shadows within or between objects using analytical light sources while allowing multiple NeRF objects to be seamlessly placed and rendered together with any arbitrary rigid transformations. Mainly, our work introduces a new surface representation known as Neural Depth Fields (NeDF) that quickly determines the spatial relationship between objects by allowing direct intersection computation between rays and implicit surfaces. It leverages an intersection neural network to query NeRF for acceleration inste
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#24182;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;&#20854;&#26041;&#27861;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.16164</link><description>&lt;p&gt;
&#22312;RKHS&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#23494;&#24230;&#27604;&#29575;
&lt;/p&gt;
&lt;p&gt;
Adaptive learning of density ratios in RKHS. (arXiv:2307.16164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#24182;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;&#20854;&#26041;&#27861;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#23494;&#24230;&#35266;&#27979;&#20013;&#20272;&#35745;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#30340;&#27604;&#29575;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#24212;&#29992;&#21253;&#25324;&#21452;&#26679;&#26412;&#26816;&#39564;&#12289;&#20998;&#27495;&#20272;&#35745;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#21327;&#21464;&#37327;&#36716;&#31227;&#36866;&#24212;&#12289;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#21644;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#22823;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#20204;&#36890;&#36807;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#26368;&#23567;&#21270;&#30495;&#23454;&#23494;&#24230;&#27604;&#29575;&#19982;&#27169;&#22411;&#20043;&#38388;&#30340;&#27491;&#21017;Bregman&#36317;&#31163;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26032;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Lepskii&#31867;&#22411;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#22312;&#19981;&#30693;&#36947;&#23494;&#24230;&#27604;&#29575;&#30340;&#27491;&#21017;&#24615;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#35823;&#24046;&#30028;&#12290;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#20540;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.
&lt;/p&gt;</description></item><item><title>INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08131</link><description>&lt;p&gt;
INFLECT-DGNN: &#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24433;&#21709;&#32773;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks. (arXiv:2307.08131v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08131
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#32593;&#32476;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#12290;&#22312;&#25512;&#33616;&#21644;&#23450;&#21521;&#33829;&#38144;&#39046;&#22495;&#20013;&#65292;&#24433;&#21709;&#32773;&#26816;&#27979;&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#21160;&#24577;&#32593;&#32476;&#34920;&#31034;&#22823;&#22823;&#21463;&#30410;&#30340;&#39046;&#22495;&#65292;&#21407;&#22240;&#26159;&#19981;&#26029;&#21457;&#23637;&#30340;&#23458;&#25143;-&#21697;&#29260;&#20851;&#31995;&#12290;&#20026;&#20102;&#38416;&#36848;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;INFLECT-DGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#20197;&#21450;&#31934;&#24515;&#35774;&#35745;&#30340;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#35780;&#20272;&#39044;&#27979;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#22478;&#24066;&#32593;&#32476;&#30340;&#29420;&#29305;&#20225;&#19994;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#20197;&#21033;&#28070;&#20026;&#39537;&#21160;&#30340;&#24433;&#21709;&#32773;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#20197;&#21450;GNN&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships. To elaborate this idea, we introduce INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural Networks (RNN) with weighted loss functions, the Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted rolling-window strategy. To evaluate predictive performance, we utilize a unique corporate data set with networks of three cities and derive a profit-driven evaluation methodology for influencer prediction. Our results show how using RNN to encode temporal attributes alongside GNNs significantly improves predictive perform
&lt;/p&gt;</description></item><item><title>PIGEON&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#31934;&#21270;&#65292;&#20197;&#21450;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;StreetCLIP&#12290;</title><link>http://arxiv.org/abs/2307.05845</link><description>&lt;p&gt;
PIGEON: &#39044;&#27979;&#22270;&#20687;&#22320;&#29702;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05845
&lt;/p&gt;
&lt;p&gt;
PIGEON&#26159;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#31934;&#21270;&#65292;&#20197;&#21450;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;StreetCLIP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;PIGEON&#65292;&#19968;&#20010;&#29992;&#20110;&#20840;&#29699;&#35268;&#27169;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#22312;&#22806;&#37096;&#22522;&#20934;&#27979;&#35797;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32467;&#21512;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#30340;&#21019;&#24314;&#21644;&#26631;&#31614;&#24179;&#28369;&#65292;&#23545;&#20855;&#26377;&#22320;&#29702;&#20449;&#24687;&#30340;&#22270;&#20687;&#36827;&#34892;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;ProtoNets&#22312;&#20505;&#36873;&#22320;&#29702;&#21333;&#20803;&#38598;&#21512;&#20013;&#25913;&#36827;&#20301;&#32622;&#39044;&#27979;&#12290;PIGEON&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;&#25968;&#25454;&#30340;&#35821;&#20041;&#22320;&#29702;&#21333;&#20803;&#21019;&#24314;&#21644;&#20998;&#21106;&#31639;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#38598;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22320;&#29702;&#21333;&#20803;&#20869;&#37096;&#31934;&#21270;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;ProtoNets&#22312;&#35813;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#39044;&#35757;&#32451;&#30340;CLIP&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;StreetCLIP&#65292;&#20844;&#24320;&#25552;&#20379;&#65292;&#21487;&#29992;&#20110;&#19982;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#22478;&#24066;&#20065;&#26449;&#22330;&#26223;&#29702;&#35299;&#30456;&#20851;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.
&lt;/p&gt;</description></item><item><title>DragDiffusion&#26159;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14435</link><description>&lt;p&gt;
DragDiffusion: &#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14435
&lt;/p&gt;
&lt;p&gt;
DragDiffusion&#26159;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21487;&#25511;&#30340;&#22270;&#20687;&#32534;&#36753;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;DragGAN&#23454;&#29616;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#24182;&#20197;&#20687;&#32032;&#32423;&#31934;&#24230;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35813;&#26041;&#27861;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#20854;&#36890;&#29992;&#24615;&#21463;&#38480;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;GAN&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#32534;&#36753;&#26694;&#26550;&#25193;&#23637;&#21040;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;DragDiffusion&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20132;&#20114;&#24335;&#22522;&#20110;&#28857;&#30340;&#32534;&#36753;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#22522;&#20110;&#25991;&#26412;&#23884;&#20837;&#65292;DragDiffusion&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#20197;&#36845;&#20195;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#65292;&#20294;&#25105;&#20204;&#20973;&#32463;&#39564;&#34920;&#26126;&#65292;&#22312;&#19968;&#20010;&#21333;&#29420;&#30340;&#27493;&#39588;&#20013;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#24050;&#36275;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#20351;&#24471;&#35813;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabl
&lt;/p&gt;</description></item><item><title>PLASTIC&#31639;&#27861;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#26631;&#31614;&#21487;&#22609;&#24615;&#65292;&#25552;&#39640;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10711</link><description>&lt;p&gt;
PLASTIC: &#25913;&#21892;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#30340;&#36755;&#20837;&#21644;&#26631;&#31614;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning. (arXiv:2306.10711v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10711
&lt;/p&gt;
&lt;p&gt;
PLASTIC&#31639;&#27861;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#26631;&#31614;&#21487;&#22609;&#24615;&#65292;&#25552;&#39640;&#26679;&#26412;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#39640;&#26114;&#19988;&#39118;&#38505;&#39640;&#30340;&#24773;&#20917;&#19979;&#12290;&#21407;&#21017;&#19978;&#65292;&#31163;&#31574;&#30053;RL&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#20801;&#35768;&#27599;&#20010;&#29615;&#22659;&#20132;&#20114;&#36827;&#34892;&#22810;&#27425;&#26356;&#26032;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22810;&#27425;&#26356;&#26032;&#24448;&#24448;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20043;&#21069;&#30340;&#20132;&#20114;&#65292;&#36825;&#34987;&#31216;&#20026;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#21487;&#22609;&#24615;&#20998;&#20026;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#35843;&#26597;&#12290;&#36755;&#20837;&#21487;&#22609;&#24615;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#23545;&#21464;&#21270;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#26631;&#31614;&#21487;&#22609;&#24615;&#65292;&#25351;&#30340;&#26159;&#27169;&#22411;&#23545;&#19981;&#26029;&#28436;&#21270;&#30340;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#23545;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#21512;&#25104;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25439;&#22833;&#27010;&#35272;&#20013;&#23547;&#25214;&#26356;&#24179;&#28369;&#30340;&#26368;&#23567;&#20540;&#21487;&#20197;&#22686;&#24378;&#36755;&#20837;&#21487;&#22609;&#24615;&#65292;&#32780;&#32454;&#21270;&#30340;&#26799;&#24230;&#20256;&#25773;&#21487;&#20197;&#25552;&#39640;&#26631;&#31614;&#21487;&#22609;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PLASTIC&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#36825;&#20004;&#26041;&#38754;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier interactions, which is referred to as the loss of plasticity. Our study investigates the underlying causes of this phenomenon by dividing plasticity into two aspects. Input plasticity, which denotes the model's adaptability to changing input data, and label plasticity, which denotes the model's adaptability to evolving input-output relationships. Synthetic experiments on the CIFAR-10 dataset reveal that finding smoother minima of loss landscape enhances input plasticity, whereas refined gradient propagation improves label plasticity. Leveraging these findings, we introduce the PLASTIC algorithm, which harmoniously combines techniques to address both
&lt;/p&gt;</description></item><item><title>DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09124</link><description>&lt;p&gt;
DIFFender&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#29992;&#20110;&#25269;&#24481;Patch&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09124
&lt;/p&gt;
&lt;p&gt;
DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;Patch&#25915;&#20987;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#24320;&#21457;&#21487;&#38752;&#30340;&#38450;&#24481;&#26041;&#27861;&#20197;&#25269;&#24481;Patch&#25915;&#20987;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24403;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIFFender&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#12290;DIFFender&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;Patch&#23450;&#20301;&#21644;Patch&#24674;&#22797;&#12290;&#22312;&#23450;&#20301;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#36259;&#29305;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#23545;&#25239;&#24615;Patch&#30340;&#20301;&#32622;&#12290;&#22312;&#24674;&#22797;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#22270;&#20687;&#20013;&#30340;&#23545;&#25239;&#24615;&#21306;&#22495;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20004;&#20010;&#38454;&#27573;&#37117;&#21463;&#21040;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#31934;&#24515;&#24341;&#23548;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#32039;&#23494;&#30456;&#20114;&#20316;&#29992;&#26469;&#25552;&#39640;&#25972;&#20010;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is not satisfactory. In this paper, we propose DIFFender, a novel defense method that leverages a text-guided diffusion model to defend against adversarial patches. DIFFender includes two main stages: patch localization and patch restoration. In the localization stage, we find and exploit an intriguing property of the diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ the diffusion model to reconstruct the adversarial regions in the images while preserving the integrity of the visual content. Importantly, these two stages are carefully guided by a unified diffusion model, thus we can utilize the close interaction between them to improve the whole defense performance. Mor
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06871</link><description>&lt;p&gt;
&#25552;&#21319;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;Q-Ensembles&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Offline-to-Online Reinforcement Learning with Q-Ensembles. (arXiv:2306.06871v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06871
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#20195;&#29702;&#26681;&#25454;&#22266;&#23450;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20165;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#33021;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#22240;&#20026;&#32570;&#20047;&#25506;&#32034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#31163;&#32447;&#39044;&#35757;&#32451;&#19982;&#22312;&#32447;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#35753;&#20195;&#29702;&#19982;&#29615;&#22659;&#23454;&#26102;&#20132;&#20114;&#65292;&#36827;&#19968;&#27493;&#23436;&#21892;&#20854;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#22312;&#32447;&#38454;&#27573;&#25913;&#36827;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-Ensembles&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#22686;&#21152;Q&#32593;&#32476;&#30340;&#25968;&#37327;&#65292;&#26080;&#32541;&#22320;&#36830;&#25509;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21152;&#24555;&#22312;&#32447;&#24615;&#33021;&#25552;&#21319;&#65292;&#25105;&#20204;&#36866;&#24403;&#25918;&#23485;Q&#20540;&#20272;&#35745;&#30340;&#24754;&#35266;&#24615;&#65292;&#24182;&#23558;&#22522;&#20110;&#38598;&#21512;&#30340;&#25506;&#32034;&#26426;&#21046;&#34701;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is a learning paradigm where an agent learns from a fixed dataset of experience. However, learning solely from a static dataset can limit the performance due to the lack of exploration. To overcome it, offline-to-online RL combines offline pre-training with online fine-tuning, which enables the agent to further refine its policy by interacting with the environment in real-time. Despite its benefits, existing offline-to-online RL methods suffer from performance degradation and slow improvement during the online phase. To tackle these challenges, we propose a novel framework called Ensemble-based Offline-to-Online (E2O) RL. By increasing the number of Q-networks, we seamlessly bridge offline pre-training and online fine-tuning without degrading performance. Moreover, to expedite online performance enhancement, we appropriately loosen the pessimism of Q-value estimation and incorporate ensemble-based exploration mechanisms into our framework. Experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03241</link><description>&lt;p&gt;
&#29702;&#35299;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#23545;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models. (arXiv:2306.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#20215;&#39640;&#26114;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33267;&#25910;&#25947;&#24182;&#19981;&#39640;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27839;&#30528;&#36712;&#36857;&#36827;&#34892;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#65292;&#20197;&#22312;&#27169;&#22411;&#25910;&#25947;&#20043;&#21069;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#26399;&#38388;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;10&#20159;&#21040;120&#20159;&#21442;&#25968;&#30340;Pythia LLM&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#24182;&#35777;&#26126;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#21644;&#20013;&#26399;&#38454;&#27573;&#65292;&#36825;&#31181;&#24819;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#24182;&#25552;&#39640;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#12290;&#25439;&#22833;&#27874;&#21160;&#26159;LLM&#35757;&#32451;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65307;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#20004;&#31181;&#22522;&#30784;&#36712;&#36857;&#30340;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#24179;&#22343;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#20004;&#31181;&#24773;&#20917;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#19968;&#20010;&#25317;&#26377;69&#20159;&#21442;&#25968;&#30340;LLM&#65292;&#25105;&#20204;&#30340;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#37197;&#26041;&#21487;&#20197;&#33410;&#30465;&#39640;&#36798;4200&#23567;&#26102;&#30340;GPU&#26102;&#38388;&#65292;&#36825;&#23545;&#20113;&#35745;&#31639;&#25104;&#26412;&#26469;&#35828;&#26159;&#26174;&#33879;&#30340;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training LLMs is expensive, and recent evidence indicates training all the way to convergence is inefficient. In this paper, we investigate the ability of a simple idea, checkpoint averaging along the trajectory of a training run to improve the quality of models before they have converged. This approach incurs no extra cost during training or inference. Specifically, we analyze the training trajectories of Pythia LLMs with 1 to 12 billion parameters and demonstrate that, particularly during the early to mid stages of training, this idea accelerates convergence and improves both test and zero-shot generalization. Loss spikes are a well recognized problem in LLM training; in our analysis we encountered two instances of this in the underlying trajectories, and both instances were mitigated by our averaging.  For a 6.9B parameter LLM, for example, our early weight averaging recipe can save upto 4200 hours of GPU time, which corresponds to significant savings in cloud compute costs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#37319;&#29992;&#26041;&#24046;&#32553;&#20943;&#21644;&#33258;&#36866;&#24212;&#25191;&#34892;&#31574;&#30053;&#36716;&#25442;&#25216;&#26415;&#65292;&#22312;&#30701;&#28903;&#21270;&#26102;&#38388;MDPs&#19978;&#23454;&#29616;&#20102;&#36951;&#25022;&#26368;&#20248;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#24615;&#21644;&#38656;&#35201;&#20184;&#20986;&#39640;&#26114;&#20869;&#23384;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15546</link><description>&lt;p&gt;
&#30701;&#28903;&#21270;&#26102;&#38388;MDPs&#19978;&#20855;&#26377;&#36951;&#25022;&#26368;&#20248;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time. (arXiv:2305.15546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15546
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#37319;&#29992;&#26041;&#24046;&#32553;&#20943;&#21644;&#33258;&#36866;&#24212;&#25191;&#34892;&#31574;&#30053;&#36716;&#25442;&#25216;&#26415;&#65292;&#22312;&#30701;&#28903;&#21270;&#26102;&#38388;MDPs&#19978;&#23454;&#29616;&#20102;&#36951;&#25022;&#26368;&#20248;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#26080;&#27861;&#23454;&#29616;&#26368;&#20248;&#24615;&#21644;&#38656;&#35201;&#20184;&#20986;&#39640;&#26114;&#20869;&#23384;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22312;&#32447;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#26080;&#38480;&#26102;&#27573;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#12290;&#29616;&#26377;&#31639;&#27861;&#35201;&#20040;&#26080;&#27861;&#23454;&#29616;&#36951;&#25022;&#26368;&#20248;&#24615;&#65292;&#35201;&#20040;&#38656;&#35201;&#20184;&#20986;&#39640;&#26114;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#22312;&#29616;&#26377;&#30340;&#26368;&#20248;&#31639;&#27861;&#20013;&#65292;&#20026;&#20102;&#23454;&#29616;&#26368;&#20248;&#26679;&#26412;&#25928;&#29575;&#65292;&#25152;&#26377;&#31639;&#27861;&#37117;&#35201;&#32463;&#36807;&#36739;&#38271;&#30340;&#28903;&#21270;&#26102;&#38388;&#65292;&#21363;&#21482;&#26377;&#26679;&#26412;&#23481;&#37327;&#36229;&#36807;&#19968;&#20010;&#39640;&#38408;&#20540;&#25165;&#33021;&#20445;&#35777;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#26041;&#24046;&#32553;&#20943;&#21644;&#19968;&#31181;&#24930;&#32780;&#33258;&#36866;&#24212;&#30340;&#25191;&#34892;&#31574;&#30053;&#36716;&#25442;&#25216;&#26415;&#12290;&#36825;&#26159;&#25240;&#25187;&#35774;&#32622;&#19979;&#31532;&#19968;&#20010;&#20855;&#26377;&#36951;&#25022;&#26368;&#20248;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#20302;&#28903;&#21270;&#26102;&#38388;&#30340;&#39069;&#22806;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial problem in reinforcement learning is learning the optimal policy. We study this in tabular infinite-horizon discounted Markov decision processes under the online setting. The existing algorithms either fail to achieve regret optimality or have to incur a high memory and computational cost. In addition, existing optimal algorithms all require a long burn-in time in order to achieve optimal sample efficiency, i.e., their optimality is not guaranteed unless sample size surpasses a high threshold. We address both open problems by introducing a model-free algorithm that employs variance reduction and a novel technique that switches the execution policy in a slow-yet-adaptive manner. This is the first regret-optimal model-free algorithm in the discounted setting, with the additional benefit of a low burn-in time.
&lt;/p&gt;</description></item><item><title>SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;</title><link>http://arxiv.org/abs/2305.15486</link><description>&lt;p&gt;
SPRING: GPT-4&#36890;&#36807;&#23398;&#20064;&#35770;&#25991;&#21644;&#25512;&#29702;&#22312;&#28216;&#25103;&#20013;&#34920;&#29616;&#36229;&#36807;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning. (arXiv:2305.15486v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15486
&lt;/p&gt;
&lt;p&gt;
SPRING&#26159;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#30001;&#20110;&#20854;&#22810;&#20219;&#21153;&#12289;&#28145;&#24230;&#25506;&#32034;&#21644;&#30446;&#26631;&#20248;&#20808;&#32423;&#35201;&#27714;&#65292;&#23545;AI&#31639;&#27861;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#35299;&#20915;&#28216;&#25103;&#26041;&#38754;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#23427;&#22312;&#20687;Crafter&#25110;Minecraft&#36825;&#26679;&#22797;&#26434;&#30340;&#24320;&#25918;&#19990;&#30028;&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SPRING&#65292;&#36890;&#36807;&#38405;&#35835;&#28216;&#25103;&#30340;&#21407;&#22987;&#23398;&#26415;&#35770;&#25991;&#24182;&#20351;&#29992;&#25152;&#23398;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#24182;&#29609;&#28216;&#25103;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#32473;&#23450;LaTeX&#28304;&#20316;&#20026;&#28216;&#25103;&#35821;&#22659;&#21644;&#20195;&#29702;&#24403;&#21069;&#35266;&#23519;&#30340;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;SPRING&#26694;&#26550;&#21033;&#29992;&#20855;&#26377;&#28216;&#25103;&#30456;&#20851;&#38382;&#39064;&#30340;&#23450;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20316;&#20026;&#33410;&#28857;&#21644;&#20381;&#36182;&#20851;&#31995;&#20316;&#20026;&#36793;&#12290;&#36890;&#36807;&#25353;&#25299;&#25169;&#39034;&#24207;&#36941;&#21382;DAG&#24182;&#35745;&#31639;&#27599;&#20010;&#33410;&#28857;&#30340;LLM&#21709;&#24212;&#26469;&#30830;&#23450;&#22312;&#29615;&#22659;&#20013;&#37319;&#21462;&#30340;&#26368;&#20248;&#34892;&#21160;&#65292;LLM&#23545;&#26368;&#32456;&#33410;&#28857;&#30340;&#31572;&#26696;&#30452;&#25509;&#36716;&#21270;&#20026;&#29615;&#22659;&#34892;&#21160;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;
&lt;/p&gt;
&lt;p&gt;
Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39063;&#31890;&#27969;&#26367;&#20195;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#22797;&#26434;&#34892;&#20026;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05218</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39063;&#31890;&#27969;&#26367;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network-based surrogate model for granular flows. (arXiv:2305.05218v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05218
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39063;&#31890;&#27969;&#26367;&#20195;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#22797;&#26434;&#34892;&#20026;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#27169;&#25311;&#39063;&#31890;&#27969;&#21160;&#21147;&#23398;&#23545;&#20110;&#35780;&#20272;&#21508;&#31181;&#23721;&#22303;&#24037;&#31243;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#23665;&#20307;&#28369;&#22369;&#21644;&#27877;&#30707;&#27969;&#12290;&#39063;&#31890;&#27969;&#28041;&#21450;&#39063;&#31890;&#21160;&#24577;&#37325;&#32452;&#65292;&#34920;&#29616;&#20986;&#20174;&#22266;&#20307;&#26679;&#26412;&#21040;&#28082;&#20307;&#26679;&#26412;&#30340;&#22797;&#26434;&#36716;&#21464;&#12290;&#20256;&#32479;&#30340;&#36830;&#32493;&#20307;&#21644;&#31163;&#25955;&#25968;&#20540;&#26041;&#27861;&#22312;&#27169;&#25311;&#22823;&#35268;&#27169;&#31995;&#32479;&#26102;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#22522;&#20110;&#32479;&#35745;&#25110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#20197;&#26377;&#38480;&#30340;&#19968;&#32452;&#21442;&#25968;&#20026;&#22522;&#30784;&#30340;&#32463;&#39564;&#24615;&#27169;&#22411;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#27867;&#21270;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#23398;&#20064;&#20250;&#20381;&#36182;&#20110;&#25490;&#21015;&#30340;&#39034;&#24207;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#8212;&#8212;&#19968;&#31181;&#23398;&#20064;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#12290;&#22270;&#34920;&#31034;&#20102;&#21160;&#24577;&#21464;&#21270;&#30340;&#39063;&#31890;&#27969;&#30340;&#29366;&#24577;&#21644;&#30456;&#20114;&#20316;&#29992;&#23450;&#24459;&#65292;&#20363;&#22914;&#39063;&#31890;&#20043;&#38388;&#30340;&#33021;&#37327;&#21644;&#21160;&#37327;&#20132;&#25442;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39063;&#31890;&#27969;&#27169;&#25311;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#25417;&#39640;&#24230;&#21160;&#24577;&#30340;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate simulation of granular flow dynamics is crucial for assessing various geotechnical risks, including landslides and debris flows. Granular flows involve a dynamic rearrangement of particles exhibiting complex transitions from solid-like to fluid-like responses. Traditional continuum and discrete numerical methods are limited by their computational cost in simulating large-scale systems. Statistical or machine learning-based models offer an alternative. Still, they are largely empirical, based on a limited set of parameters. Due to their permutation-dependent learning, traditional machine learning-based models require huge training data to generalize. To resolve these problems, we use a graph neural network, a state-of-the-art machine learning architecture that learns local interactions. Graphs represent the state of dynamically changing granular flows and the interaction laws, such as energy and momentum exchange between grains. We develop a graph neural network-based simulator
&lt;/p&gt;</description></item><item><title>&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14660</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#8220;Segment Anything Model&#8221;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14660
&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25512;&#24191;&#20998;&#21106;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#20004;&#31181;&#27169;&#24335;&#23454;&#29616;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#27169;&#24577;&#12289;&#32454;&#24494;&#30340;&#35299;&#21078;&#32467;&#26500;&#12289;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#23545;&#35937;&#36793;&#30028;&#21644;&#24191;&#27867;&#30340;&#23545;&#35937;&#23610;&#24230;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65288;MIS&#65289;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#38646;&#26679;&#26412;&#21644;&#39640;&#25928;&#30340;MIS&#21487;&#20197;&#24456;&#22909;&#22320;&#20943;&#23569;&#27880;&#37322;&#26102;&#38388;&#24182;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;SAM&#20284;&#20046;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#24037;&#20855;&#65292;&#24182;&#19988;&#20854;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24212;&#35813;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#25972;&#29702;&#20102;52&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;16&#20010;&#27169;&#24577;&#21644;68&#20010;&#23545;&#35937;&#30340;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#25552;&#21462;&#20154;&#20307;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#26102;&#38388;-selective&#29305;&#24449;&#25552;&#21462;&#31639;&#27861;&#21306;&#20998;&#26377;&#30452;&#35273;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.13107</link><description>&lt;p&gt;
&#22522;&#20110;WiFi CSI&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#30340;&#26102;&#38388;&#36873;&#25321;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Time-Selective RNN for Device-Free Multi-Room Human Presence Detection Using WiFi CSI. (arXiv:2304.13107v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#25552;&#21462;&#20154;&#20307;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#26102;&#38388;-selective&#29305;&#24449;&#25552;&#21462;&#31639;&#27861;&#21306;&#20998;&#26377;&#30452;&#35273;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23384;&#22312;&#26816;&#27979;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#21253;&#25324;&#23478;&#23621;&#33258;&#21160;&#21270;&#12289;&#23433;&#20840;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#37319;&#29992;&#22522;&#20110;&#25668;&#20687;&#26426;&#30340;&#31995;&#32479;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#20294;&#20250;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21830;&#29992;WiFi&#25509;&#20837;&#28857;&#25552;&#20379;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#26041;&#27861;&#65292;&#25552;&#20379;&#35814;&#32454;&#30340;&#20449;&#36947;&#29305;&#24449;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#36873;&#25321;&#24615;&#26465;&#20214;&#21452;&#29305;&#24449;&#25552;&#21462;&#36882;&#24402;&#32593;&#32476;(TCD-FERN)&#30340;&#26080;&#35774;&#22791;&#22810;&#25151;&#38388;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26088;&#22312;&#20351;&#29992;&#21160;&#24577;&#21644;&#38745;&#24577;(DaS)&#25968;&#25454;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#22312;&#26465;&#20214;&#20154;&#20307;&#29305;&#24449;&#19979;&#25429;&#25417;&#37325;&#35201;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#25552;&#21462;&#20154;&#30340;&#31227;&#21160;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#24182;&#21306;&#20998;&#26377;&#30452;&#25509;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#21644;&#26080;&#35270;&#32447;&#36335;&#24452;&#38459;&#22622;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#20943;&#23569;&#25151;&#38388;&#38548;&#26029;&#36896;&#25104;&#30340;&#29305;&#24449;&#34928;&#20943;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110; LSTM &#30340; NCoV-DaS &#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human presence detection is a crucial technology for various applications, including home automation, security, and healthcare. While camera-based systems have traditionally been used for this purpose, they raise privacy concerns. To address this issue, recent research has explored the use of channel state information (CSI) approaches that can be extracted from commercial WiFi access points (APs) and provide detailed channel characteristics. In this thesis, we propose a device-free human presence detection system for multi-room scenarios using a time-selective conditional dual feature extract recurrent Network (TCD-FERN). Our system is designed to capture significant time features with the condition on current human features using a dynamic and static (DaS) data preprocessing technique to extract moving and spatial features of people and differentiate between line-of-sight (LoS) path blocking and non-blocking cases. To mitigate the feature attenuation problem caused by room partitions,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20302;&#20998;&#36776;&#29575;LES&#25968;&#25454;&#37325;&#24314;&#36830;&#32493;&#30340;DNS&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#28237;&#27969;&#27969;&#22330;&#30340;&#26102;&#31354;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12130</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#24863;&#30693;&#30340;&#26102;&#31354;&#21160;&#21147;&#23398;&#21644;&#27979;&#35797;&#26102;&#38388;&#32454;&#21270;&#37325;&#24314;&#28237;&#27969;&#27969;&#22330;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal Dynamics and Test-Time Refinement. (arXiv:2304.12130v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20302;&#20998;&#36776;&#29575;LES&#25968;&#25454;&#37325;&#24314;&#36830;&#32493;&#30340;DNS&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#28237;&#27969;&#27969;&#22330;&#30340;&#26102;&#31354;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#28237;&#27969;&#23545;&#20110;&#33322;&#31354;&#33322;&#22825;&#24037;&#31243;&#12289;&#29615;&#22659;&#31185;&#23398;&#12289;&#33021;&#28304;&#34892;&#19994;&#21644;&#29983;&#29289;&#21307;&#23398;&#31561;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#28065;&#27169;&#25311;&#65288;LES&#65289;&#30001;&#20110;&#20854;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#34987;&#24191;&#27867;&#29992;&#20316;&#27169;&#25311;&#28237;&#27969;&#27969;&#22330;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#65288;DNS&#65289;&#12290;&#28982;&#32780;&#65292;LES&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#28237;&#27969;&#36816;&#36755;&#30340;&#25152;&#26377;&#23610;&#24230;&#12290;&#20174;&#20302;&#20998;&#36776;&#29575;LES&#37325;&#24314;DNS&#23545;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#23398;&#31185;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#28237;&#27969;&#27969;&#22330;&#30340;&#26102;&#31354;&#22797;&#26434;&#24615;&#65292;&#36825;&#32473;&#29616;&#26377;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#25552;&#20986;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#20302;&#20998;&#36776;&#29575;LES&#25968;&#25454;&#37325;&#24314;&#36830;&#32493;&#30340;DNS&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#27969;&#21160;&#21160;&#21147;&#23398;&#24213;&#23618;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#22312;&#26102;&#31354;&#27169;&#22411;&#26550;&#26500;&#30340;&#35774;&#35745;&#20013;&#36827;&#34892;&#24314;&#27169;&#12290;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38477;&#32423;&#30340;&#32454;&#21270;&#26041;&#27861;&#65292;&#20197;&#24378;&#21046;&#23454;&#26045;p
&lt;/p&gt;
&lt;p&gt;
Simulating turbulence is critical for many societally important applications in aerospace engineering, environmental science, the energy industry, and biomedicine. Large eddy simulation (LES) has been widely used as an alternative to direct numerical simulation (DNS) for simulating turbulent flows due to its reduced computational cost. However, LES is unable to capture all of the scales of turbulent transport accurately. Reconstructing DNS from low-resolution LES is critical for many scientific and engineering disciplines, but it poses many challenges to existing super-resolution methods due to the spatio-temporal complexity of turbulent flows. In this work, we propose a new physics-guided neural network for reconstructing the sequential DNS from low-resolution LES data. The proposed method leverages the partial differential equation that underlies the flow dynamics in the design of spatio-temporal model architecture. A degradation-based refinement method is also developed to enforce p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#31639;&#27861;&#65288;SDEC&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2304.03907</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#32500;&#35889;&#21160;&#24577;&#23884;&#20837;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding. (arXiv:2304.03907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#31639;&#27861;&#65288;SDEC&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#19968;&#30452;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;Ren&#31561;&#20154;&#24341;&#20837;&#20102;&#35889;&#21160;&#24577;&#23884;&#20837;&#26469;&#24320;&#21457;&#25511;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#26080;&#31351;&#32500;&#29305;&#24449;&#26469;&#32447;&#24615;&#34920;&#31034;&#29366;&#24577;&#20540;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#32500;&#30340;&#25130;&#26029;&#36924;&#36817;&#36827;&#34892;&#23454;&#38469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#24050;&#30693;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25511;&#21046;&#20013;&#30340;&#26377;&#38480;&#32500;&#36924;&#36817;&#24615;&#36136;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#25511;&#21046;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#65288;SDEC&#65289;&#65292;&#24182;&#36827;&#34892;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#34920;&#24449;&#30001;&#26377;&#38480;&#32500;&#25130;&#26029;&#24341;&#36215;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#30001;&#26377;&#38480;&#26679;&#26412;&#36924;&#36817;&#24341;&#36215;&#30340;&#32479;&#35745;&#35823;&#24046;&#65292;&#21516;&#26102;&#36827;&#34892;&#25919;&#31574;&#35780;&#20272;&#21644;&#25919;&#31574;&#20248;&#21270;&#30340;&#23454;&#39564;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal control is notoriously difficult for stochastic nonlinear systems. Ren et al. introduced Spectral Dynamics Embedding for developing reinforcement learning methods for controlling an unknown system. It uses an infinite-dimensional feature to linearly represent the state-value function and exploits finite-dimensional truncation approximation for practical implementation. However, the finite-dimensional approximation properties in control have not been investigated even when the model is known. In this paper, we provide a tractable stochastic nonlinear control algorithm that exploits the nonlinear dynamics upon the finite-dimensional feature approximation, Spectral Dynamics Embedding Control (SDEC), with an in-depth theoretical analysis to characterize the approximation error induced by the finite-dimension truncation and statistical error induced by finite-sample approximation in both policy evaluation and policy optimization. We also empirically test the algorithm and compare th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; DeepAccident &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#20107;&#25925;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01168</link><description>&lt;p&gt;
DeepAccident&#65306;V2X&#33258;&#21160;&#39550;&#39542;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving. (arXiv:2304.01168v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340; DeepAccident &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#20107;&#25925;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#26159;&#33258;&#21160;&#39550;&#39542;&#30340;&#39318;&#35201;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#24050;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#25903;&#25345;&#33258;&#21160;&#39550;&#39542;&#30340;&#30452;&#25509;&#21644;&#21487;&#35299;&#37322;&#30340;&#23433;&#20840;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DeepAccident&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#29616;&#23454;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32463;&#24120;&#22312;&#29616;&#23454;&#39550;&#39542;&#20013;&#21457;&#29983;&#30340;&#21508;&#31181;&#20107;&#25925;&#22330;&#26223;&#12290;DeepAccident &#25968;&#25454;&#38598;&#21253;&#21547; 57k &#20010;&#24102;&#27880;&#37322;&#24103;&#21644; 285k &#20010;&#24102;&#27880;&#37322;&#30340;&#26679;&#26412;&#65292;&#36825;&#20960;&#20046;&#26159;&#22823;&#35268;&#27169; nuScenes &#25968;&#25454;&#38598;&#30340; 7 &#20493;&#65292;&#20854;&#26679;&#26412;&#25968;&#20026; 40k&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#31471;&#21040;&#31471;&#30340;&#36816;&#21160;&#21644;&#20107;&#25925;&#39044;&#27979;&#65292;&#21487;&#29992;&#20110;&#30452;&#25509;&#35780;&#20272;&#19981;&#21516;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#20107;&#25925;&#39044;&#27979;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27599;&#31181;&#22330;&#26223;&#65292;&#25105;&#20204;&#35774;&#32622;&#20102;&#22235;&#36742;&#36710;&#21644;&#19968;&#20010;&#22522;&#30784;&#35774;&#26045;&#26469;&#35760;&#24405;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#20107;&#25925;&#22330;&#26223;&#25552;&#20379;&#20102;&#22810;&#31181;&#35270;&#35282;&#65292;&#24182;&#20351; V2X&#65288;&#36710;&#36742;&#23545;&#19968;&#20999;&#65289;&#24863;&#30693;&#21644;&#39044;&#27979;&#30740;&#31350;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is the primary priority of autonomous driving. Nevertheless, no published dataset currently supports the direct and explainable safety evaluation for autonomous driving. In this work, we propose DeepAccident, a large-scale dataset generated via a realistic simulator containing diverse accident scenarios that frequently occur in real-world driving. The proposed DeepAccident dataset contains 57K annotated frames and 285K annotated samples, approximately 7 times more than the large-scale nuScenes dataset with 40k annotated samples. In addition, we propose a new task, end-to-end motion and accident prediction, based on the proposed dataset, which can be used to directly evaluate the accident prediction ability for different autonomous driving algorithms. Furthermore, for each scenario, we set four vehicles along with one infrastructure to record data, thus providing diverse viewpoints for accident scenarios and enabling V2X (vehicle-to-everything) research on perception and predicti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; Iterative Markovian Fitting&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230; Schr\"odinger&#26725;&#65288;SBs&#65289;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#29616;&#20986;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.16852</link><description>&lt;p&gt;
&#25193;&#25955;Schr\"odinger&#26725;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Diffusion Schr\"odinger Bridge Matching. (arXiv:2303.16852v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; Iterative Markovian Fitting&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#24230; Schr\"odinger&#26725;&#65288;SBs&#65289;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#29616;&#20986;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#30340;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#36816;&#36755;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#30528;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#26032;&#22411;&#30340;&#36136;&#37327;&#20256;&#36755;&#26041;&#27861;&#65292;&#22914;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#21644;&#27969;&#21305;&#37197;&#27169;&#22411;&#65288;FMMs&#65289;&#65292;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#23454;&#29616;&#36825;&#26679;&#30340;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#36817;&#20284;&#30830;&#23450;&#24615;&#21160;&#24577;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#26144;&#23556;&#26159;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#24615;&#36136;&#65292;&#20294; DDMs &#21644; FMMs &#24182;&#19981;&#33021;&#20445;&#35777;&#25552;&#20379;&#25509;&#36817; OT &#26144;&#23556;&#30340;&#20256;&#36755;&#12290;&#30456;&#21453;&#65292;Schr\"odinger&#26725;&#65288;SBs&#65289;&#35745;&#31639;&#38543;&#26426;&#21160;&#24577;&#26144;&#23556;&#65292;&#21487;&#20197;&#24674;&#22797;&#27491;&#21017;&#29109;&#29256;&#26412;&#30340; OT&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#25968;&#20540;&#26041;&#27861;&#36817;&#20284; SBs &#30340;&#32500;&#24230;&#32553;&#25918;&#24046;&#25110;&#22312;&#36845;&#20195;&#20013;&#31215;&#32047;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36845;&#20195;&#39532;&#23572;&#31185;&#22827;&#25311;&#21512;&#65292;&#19968;&#31181;&#35299;&#20915;&#39640;&#32500;&#24230; SB &#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#35774;&#35745;&#20026;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#23558;&#32622;&#20449;&#20256;&#25773;&#25193;&#23637;&#21040; KL &#25955;&#24230;&#65292;&#21033;&#29992;&#26465;&#20214;&#29420;&#31435;&#24615;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#25104;&#26524;&#26041;&#27861;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving transport problems, i.e. finding a map transporting one given distribution to another, has numerous applications in machine learning. Novel mass transport methods motivated by generative modeling have recently been proposed, e.g. Denoising Diffusion Models (DDMs) and Flow Matching Models (FMMs) implement such a transport through a Stochastic Differential Equation (SDE) or an Ordinary Differential Equation (ODE). However, while it is desirable in many applications to approximate the deterministic dynamic Optimal Transport (OT) map which admits attractive properties, DDMs and FMMs are not guaranteed to provide transports close to the OT map. In contrast, Schr\"odinger bridges (SBs) compute stochastic dynamic mappings which recover entropy-regularized versions of OT. Unfortunately, existing numerical methods approximating SBs either scale poorly with dimension or accumulate errors across iterations. In this work, we introduce Iterative Markovian Fitting, a new methodology for solv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20809;&#20239;&#30005;&#27744;&#32452;&#28151;&#21512;&#29289;&#29702;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#21327;&#21464;&#37327;&#65292;&#25913;&#21892;&#20102;&#20809;&#20239;&#31995;&#32479;&#21151;&#29575;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#21487;&#20197;&#36798;&#21040;7.54&#65285;&#30340;&#25216;&#33021;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2303.08459</link><description>&lt;p&gt;
&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20809;&#20239;&#30005;&#27744;&#32452;&#30340;&#28151;&#21512;&#29289;&#29702;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hybrid-Physical Probabilistic Forecasting for a Set of Photovoltaic Systems using Recurrent Neural Networks. (arXiv:2303.08459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#20809;&#20239;&#30005;&#27744;&#32452;&#28151;&#21512;&#29289;&#29702;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#21327;&#21464;&#37327;&#65292;&#25913;&#21892;&#20102;&#20809;&#20239;&#31995;&#32479;&#21151;&#29575;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#21487;&#20197;&#36798;&#21040;7.54&#65285;&#30340;&#25216;&#33021;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20809;&#20239;&#31995;&#32479;&#30340;&#21151;&#29575;&#36755;&#20986;&#23545;&#20110;&#25913;&#21892;&#33021;&#28304;&#20998;&#24067;&#32593;&#32476;&#30340;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;-&#29289;&#29702;&#27169;&#22411;&#65292;&#22312;&#25968;&#20540;&#22825;&#27668;&#39044;&#27979;&#30340;&#24110;&#21161;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#20854;&#20316;&#20026;&#21327;&#21464;&#37327;&#30340;PV&#24615;&#33021;&#27169;&#22411;&#21644;&#33258;&#22238;&#24402;&#36882;&#24402;&#31070;&#32463;&#27169;&#22411;&#26469;&#25913;&#36827;&#30830;&#23450;&#24615;&#30340;&#30701;&#26102;&#39044;&#27979;&#12290;&#25105;&#20204;&#37325;&#26032;&#35774;&#35745;&#20102;&#26368;&#21021;&#29992;&#20110;&#38646;&#21806;&#39046;&#22495;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25130;&#26029;&#39640;&#26031;&#36755;&#20986;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#35768;&#22810;&#27169;&#22411;&#21464;&#37327;&#19982;&#25991;&#29486;&#20013;&#30340;&#26367;&#20195;&#26041;&#26696;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#65292;&#24182;&#19988;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#26368;&#20339;&#24615;&#33021;&#21464;&#20307;&#20013;&#30340;&#32452;&#20214;&#21327;&#21516;&#24037;&#20316;&#20197;&#36798;&#21040;&#19982;NWP&#39537;&#21160;&#30340;PV&#24615;&#33021;&#27169;&#22411;&#22522;&#32447;&#30456;&#27604;&#30340;&#25216;&#33021;&#35780;&#20998;&#20026;7.54&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate intra-day forecasts of the power output by PhotoVoltaic (PV) systems are critical to improve the operation of energy distribution grids. We describe a hybrid-physical model, which aims at improving deterministic intra-day forecasts, issued by a PV performance model fed by Numerical Weather Predictions (NWP), by using them as covariates in the context of an autoregressive recurrent neural model. Our proposal repurposes a neural model initially used in the retail sector, and discloses a novel truncated Gaussian output distribution. We experimentally compare many model variants to alternatives from the literature, and an ablation study shows that the components in the best performing variant work synergistically to reach a skill score of 7.54% with respect to the NWP-driven PV performance model baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#36807;&#21442;&#25968;&#27169;&#22411;&#20013;&#20351;&#29992;&#39640;&#26031;-&#29275;&#39039;&#27861;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;GN&#22312;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#26041;&#38754;&#27604;GD&#26356;&#24555;&#65292;&#20294;&#23398;&#20064;&#29575;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#26435;&#37325;&#26041;&#24046;&#23545;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#26356;&#23567;&#30340;&#26041;&#24046;&#21021;&#22987;&#21270;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#19982;GD&#19981;&#21516;&#30340;&#26159;&#65292;GN&#22312;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#26041;&#38754;&#20351;&#29992;&#26356;&#23567;&#30340;&#23398;&#20064;&#29575;&#33021;&#22815;&#21462;&#24471;&#25104;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.02904</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rethinking Gauss-Newton for learning over-parameterized models. (arXiv:2302.02904v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#36807;&#21442;&#25968;&#27169;&#22411;&#20013;&#20351;&#29992;&#39640;&#26031;-&#29275;&#39039;&#27861;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;GN&#22312;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#26041;&#38754;&#27604;GD&#26356;&#24555;&#65292;&#20294;&#23398;&#20064;&#29575;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#26435;&#37325;&#26041;&#24046;&#23545;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#26356;&#23567;&#30340;&#26041;&#24046;&#21021;&#22987;&#21270;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#32780;&#19982;GD&#19981;&#21516;&#30340;&#26159;&#65292;GN&#22312;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#26041;&#38754;&#20351;&#29992;&#26356;&#23567;&#30340;&#23398;&#20064;&#29575;&#33021;&#22815;&#21462;&#24471;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#65292;&#20351;&#29992;&#39640;&#26031;-&#29275;&#39039;&#27861;&#65288;GN&#65289;&#23545;&#19968;&#23618;&#38544;&#34255;&#23618;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#26102;&#30340;&#20840;&#23616;&#25910;&#25947;&#21644;&#27867;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#19979;&#30830;&#23450;&#20102;GN&#30340;&#20840;&#23616;&#25910;&#25947;&#32467;&#26524;&#65292;&#30001;&#20110;&#25913;&#21892;&#20102;&#26465;&#20214;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#27604;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#26356;&#24555;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#22238;&#24402;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;GN&#26041;&#27861;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;GN&#22987;&#32456;&#27604;GD&#26356;&#24555;&#22320;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#20294;&#23398;&#20064;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#21463;&#21040;&#23398;&#20064;&#29575;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#32593;&#32476;&#26435;&#37325;&#26041;&#24046;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#26356;&#23567;&#30340;&#26041;&#24046;&#21021;&#22987;&#21270;&#32467;&#26524;&#20250;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#65292;&#36825;&#20063;&#26159;GD&#30340;&#19968;&#31181;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19982;GD&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#26356;&#23567;&#30340;&#23398;&#20064;&#29575;&#21487;&#20197;&#20351;GN&#22312;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#26041;&#38754;&#21462;&#24471;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the global convergence and generalization properties of Gauss Newton's (GN) when optimizing one-hidden layer networks in the over-parameterized regime. We first establish a global convergence result for GN in the continuous-time limit exhibiting a faster convergence rate compared to GD due to improved conditioning. We then perform an empirical study on a synthetic regression task to investigate the implicit bias of GN's method. We find that, while GN is consistently faster than GD in finding a global optimum, the performance of the learned model on a test dataset is heavily influenced by both the learning rate and the variance of the randomly initialized network's weights. Specifically, we find that initializing with a smaller variance results in a better generalization, a behavior also observed for GD. However, in contrast to GD where larger learning rates lead to the best generalization, we find that GN achieves an improved generalization when using smaller learning
&lt;/p&gt;</description></item><item><title>&#31526;&#21512;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#24212;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#38598;&#65292;&#21487;&#20197;&#32416;&#27491;&#30001;&#20110;&#27169;&#22411;&#35268;&#33539;&#19981;&#24403;&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#24102;&#26469;&#30340;&#20027;&#35266;&#19978;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#21644;&#34920;&#26684;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.12496</link><description>&lt;p&gt;
&#24102;&#26377;&#31526;&#21512;&#24615;&#39044;&#27979;&#38598;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Conformal Prediction Sets. (arXiv:2210.12496v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12496
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#24212;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#38598;&#65292;&#21487;&#20197;&#32416;&#27491;&#30001;&#20110;&#27169;&#22411;&#35268;&#33539;&#19981;&#24403;&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#24102;&#26469;&#30340;&#20027;&#35266;&#19978;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#21644;&#34920;&#26684;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#20570;&#20986;&#20915;&#31574;&#30340;&#26222;&#36941;&#26041;&#27861;&#65292;&#24212;&#29992;&#21253;&#25324;&#22810;&#33218;&#32769;&#34382;&#26426;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#40657;&#30418;&#20248;&#21270;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#36873;&#25321;&#20855;&#26377;&#26368;&#22823;&#39044;&#26399;&#25928;&#29992;&#30340;&#20915;&#31574;(&#21363;&#30446;&#26631;&#20989;&#25968;&#26597;&#35810;)&#65292;&#35813;&#21518;&#39564;&#20998;&#24067;&#37327;&#21270;&#20102;&#26597;&#35810;&#32467;&#26524;&#30340;&#21487;&#20943;&#23569;&#30340;&#20808;&#39564;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22240;&#27169;&#22411;&#35268;&#33539;&#19981;&#24403;&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#21407;&#22240;&#65292;&#20027;&#35266;&#19978;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#21487;&#33021;&#32463;&#24120;&#21457;&#29983;&#12290;&#31526;&#21512;&#24615;&#39044;&#27979;&#26159;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#21363;&#20351;&#23545;&#20110;&#35268;&#33539;&#19981;&#33391;&#30340;&#27169;&#22411;&#20063;&#20855;&#26377;&#35206;&#30422;&#20445;&#35777;&#65292;&#24182;&#19988;&#20855;&#26377;&#32416;&#27491;&#21327;&#21464;&#37327;&#36716;&#31227;&#30340;&#31616;&#21333;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31526;&#21512;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#23558;&#26597;&#35810;&#24341;&#23548;&#21040;&#27169;&#22411;&#39044;&#27979;&#20855;&#26377;&#20445;&#35777;&#26377;&#25928;&#24615;&#30340;&#25628;&#32034;&#31354;&#38388;&#21306;&#22495;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#22312;&#19968;&#32452;&#40657;&#30418;&#20248;&#21270;&#20219;&#21153;&#21644;&#34920;&#26684;&#25490;&#21517;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#31526;&#21512;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#20248;&#20110;&#26631;&#20934;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a coherent, ubiquitous approach to decision-making under uncertainty, with applications including multi-arm bandits, active learning, and black-box optimization. Bayesian optimization selects decisions (i.e. objective function queries) with maximal expected utility with respect to the posterior distribution of a Bayesian model, which quantifies reducible, epistemic uncertainty about query outcomes. In practice, subjectively implausible outcomes can occur regularly for two reasons: 1) model misspecification and 2) covariate shift. Conformal prediction is an uncertainty quantification method with coverage guarantees even for misspecified models and a simple mechanism to correct for covariate shift. We propose conformal Bayesian optimization, which directs queries towards regions of search space where the model predictions have guaranteed validity, and investigate its behavior on a suite of black-box optimization tasks and tabular ranking tasks. In many cases we f
&lt;/p&gt;</description></item><item><title>&#36328;&#23458;&#25143;&#26631;&#31614;&#20256;&#25773;&#65288;XCLP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#35774;&#22791;&#21644;&#21322;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.06434</link><description>&lt;p&gt;
&#36328;&#23458;&#25143;&#26631;&#31614;&#20256;&#25773;&#29992;&#20110;&#36328;&#35774;&#22791;&#21644;&#21322;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-client Label Propagation for Transductive and Semi-Supervised Federated Learning. (arXiv:2210.06434v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06434
&lt;/p&gt;
&lt;p&gt;
&#36328;&#23458;&#25143;&#26631;&#31614;&#20256;&#25773;&#65288;XCLP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#35774;&#22791;&#21644;&#21322;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65306;&#36328;&#23458;&#25143;&#26631;&#31614;&#20256;&#25773;&#65288;XCLP&#65289;&#12290;XCLP&#36890;&#36807;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#20849;&#21516;&#20272;&#35745;&#25968;&#25454;&#22270;&#65292;&#24182;&#36890;&#36807;&#22312;&#22270;&#19978;&#20256;&#25773;&#26631;&#31614;&#20449;&#24687;&#26469;&#35745;&#31639;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#36991;&#20813;&#23458;&#25143;&#31471;&#38656;&#35201;&#19982;&#20182;&#20154;&#20849;&#20139;&#25968;&#25454;&#65292;XCLP&#37319;&#29992;&#20102;&#20004;&#20010;&#23494;&#30721;&#23398;&#23433;&#20840;&#21327;&#35758;&#65306;&#23433;&#20840;&#30340;&#27721;&#26126;&#36317;&#31163;&#35745;&#31639;&#21644;&#23433;&#20840;&#27714;&#21644;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;XCLP&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#22312;&#31532;&#19968;&#20010;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#20197;&#19968;&#27425;&#24615;&#30340;&#26041;&#24335;&#39044;&#27979;&#26410;&#35265;&#27979;&#35797;&#28857;&#30340;&#26631;&#31614;&#12290;&#22312;&#31532;&#20108;&#20010;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#22312;&#32852;&#37030;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#37325;&#22797;&#20266;&#26631;&#35760;&#26080;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#30495;&#23454;&#30340;&#32852;&#37030;&#21644;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;XCLP&#22312;&#36825;&#20004;&#20010;&#24212;&#29992;&#20013;&#27604;&#26367;&#20195;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Cross-Client Label Propagation(XCLP), a new method for transductive federated learning. XCLP estimates a data graph jointly from the data of multiple clients and computes labels for the unlabeled data by propagating label information across the graph. To avoid clients having to share their data with anyone, XCLP employs two cryptographically secure protocols: secure Hamming distance computation and secure summation. We demonstrate two distinct applications of XCLP within federated learning. In the first, we use it in a one-shot way to predict labels for unseen test points. In the second, we use it to repeatedly pseudo-label unlabeled training data in a federated semi-supervised setting. Experiments on both real federated and standard benchmark datasets show that in both applications XCLP achieves higher classification accuracy than alternative approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#65292;&#20351;&#34928;&#24369;&#30340;&#20449;&#21495;&#20986;&#29616;&#20855;&#26377;&#37327;&#21270;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#26230;&#20307;&#26448;&#26009;&#30340;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20013;&#12290;</title><link>http://arxiv.org/abs/2209.09247</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21435;&#22122;&#25552;&#21462;&#34928;&#20943;&#20449;&#21495;&#30340;&#24212;&#29992;&#30740;&#31350;&#8212;&#8212;&#20197;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Weak-signal extraction enabled by deep-neural-network denoising of diffraction data. (arXiv:2209.09247v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#65292;&#20351;&#34928;&#24369;&#30340;&#20449;&#21495;&#20986;&#29616;&#20855;&#26377;&#37327;&#21270;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#26230;&#20307;&#26448;&#26009;&#30340;X&#23556;&#32447;&#34893;&#23556;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#38500;&#22122;&#38899;&#22312;&#25104;&#20687;&#21644;&#22768;&#23398;&#26041;&#38754;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#26085;&#24120;&#24212;&#29992;&#20013;&#65292;&#21435;&#22122;&#21487;&#33021;&#29978;&#33267;&#21253;&#21547;&#19982;&#30495;&#23454;&#24773;&#20917;&#19981;&#31526;&#30340;&#29983;&#25104;&#26041;&#38754;&#12290;&#20294;&#26159;&#65292;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#65292;&#21435;&#22122;&#24517;&#39035;&#20934;&#30830;&#22320;&#20877;&#29616;&#30495;&#23454;&#24773;&#20917;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#65292;&#20197;&#20351;&#34928;&#24369;&#30340;&#20449;&#21495;&#20986;&#29616;&#20855;&#26377;&#37327;&#21270;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;X&#23556;&#32447;&#34893;&#23556;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21435;&#22122;&#25968;&#25454;&#20013;&#65292;&#28304;&#33258;&#30005;&#33655;&#25490;&#24207;&#30340;&#24494;&#24369;&#20449;&#21495;&#65292;&#22312;&#22122;&#38899;&#25968;&#25454;&#20013;&#19981;&#26174;&#33879;&#65292;&#20294;&#22312;&#21435;&#22122;&#21518;&#21464;&#24471;&#28165;&#26224;&#32780;&#20934;&#30830;&#21487;&#35265;&#12290;&#36825;&#31181;&#25104;&#21151;&#24471;&#30410;&#20110;&#20351;&#29992;&#25152;&#27979;&#37327;&#30340;&#20302;&#22122;&#22768;&#25968;&#25454;&#21644;&#39640;&#22122;&#22768;&#25968;&#25454;&#30340;&#25104;&#23545;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30417;&#30563;&#35757;&#32451;&#12290;&#36825;&#26679;&#65292;&#31070;&#32463;&#32593;&#32476;&#23601;&#21487;&#20197;&#23398;&#20064;&#22122;&#22768;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#20154;&#24037;&#22122;&#22768;&#26080;&#27861;&#24471;&#21040;&#22914;&#27492;&#37327;&#21270;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38416;&#26126;&#20102;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21435;&#22122;&#25552;&#21462;&#22122;&#38899;&#25968;&#25454;&#20013;&#30340;&#34928;&#20943;&#20449;&#21495;&#30340;&#23454;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Removal or cancellation of noise has wide-spread applications for imaging and acoustics. In every-day-life applications, denoising may even include generative aspects which are unfaithful to the ground truth. For scientific applications, however, denoising must reproduce the ground truth accurately. Here, we show how data can be denoised via a deep convolutional neural network such that weak signals appear with quantitative accuracy. In particular, we study X-ray diffraction on crystalline materials. We demonstrate that weak signals stemming from charge ordering, insignificant in the noisy data, become visible and accurate in the denoised data. This success is enabled by supervised training of a deep neural network with pairs of measured low- and high-noise data. This way, the neural network learns about the statistical properties of the noise. We demonstrate that using artificial noise does not yield such quantitatively accurate results. Our approach thus illustrates a practical strat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#20572;&#31574;&#30053;&#26469;&#35299;&#20915;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#21644;DIP&#21464;&#20307;&#20013;&#25345;&#32493;&#26816;&#27979;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;DIP&#23454;&#29992;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2112.06074</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#30340;&#26089;&#20572;&#27861;
&lt;/p&gt;
&lt;p&gt;
Early Stopping for Deep Image Prior. (arXiv:2112.06074v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.06074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#20572;&#31574;&#30053;&#26469;&#35299;&#20915;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#21644;DIP&#21464;&#20307;&#20013;&#25345;&#32493;&#26816;&#27979;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;DIP&#23454;&#29992;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;(DIP)&#21450;&#20854;&#21464;&#20307;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#35299;&#20915;&#36870;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#38469;&#30340;DIP&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#21040;&#22823;&#37096;&#20998;&#26399;&#26395;&#30340;&#35270;&#35273;&#20869;&#23481;&#65292;&#28982;&#21518;&#36880;&#28176;&#25429;&#25417;&#21040;&#28508;&#22312;&#30340;&#24314;&#27169;&#21644;&#35266;&#27979;&#22122;&#22768;&#65292;&#21363;&#36807;&#25311;&#21512;&#12290;&#22240;&#27492;&#65292;DIP&#30340;&#23454;&#29992;&#24615;&#24448;&#24448;&#20851;&#38190;&#21462;&#20915;&#20110;&#33391;&#22909;&#30340;&#26089;&#20572;&#31574;&#30053;&#65292;&#20197;&#25429;&#25417;&#36807;&#28193;&#26399;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;DIP&#24037;&#20316;&#21482;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#25253;&#21578;&#20102;&#19982;&#30495;&#23454;&#32467;&#26524;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#22914;&#20309;&#22312;&#27809;&#26377;&#30495;&#23454;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#25805;&#20316;&#24615;&#22320;&#33719;&#24471;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#24182;&#27809;&#26377;&#32473;&#20986;&#32447;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#31361;&#30772;DIP&#30340;&#23454;&#29992;&#24615;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26089;&#20572;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#21644;DIP&#21464;&#20307;&#20013;&#25345;&#32493;&#22320;&#26816;&#27979;&#21040;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep image prior (DIP) and its variants have showed remarkable potential for solving inverse problems in computer vision, without any extra training data. Practical DIP models are often substantially overparameterized. During the fitting process, these models learn mostly the desired visual content first, and then pick up the potential modeling and observational noise, i.e., overfitting. Thus, the practicality of DIP often depends critically on good early stopping (ES) that captures the transition period. In this regard, the majority of DIP works for vision tasks only demonstrates the potential of the models -- reporting the peak performance against the ground truth, but provides no clue about how to operationally obtain near-peak performance without access to the groundtruth. In this paper, we set to break this practicality barrier of DIP, and propose an efficient ES strategy, which consistently detects near-peak performance across several vision tasks and DIP variants. Based on a sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#20989;&#25968;&#22797;&#26434;&#24615;&#65288;LFC&#65289;&#30340;&#20272;&#35745;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#23616;&#37096;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21457;&#23637;&#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#30340;&#23616;&#37096;&#22810;&#39033;&#24335;&#24179;&#28369;&#65288;LPS&#65289;&#27169;&#22411;&#30340;&#31867;&#27604;&#65292;&#20351;&#24471;&#35813;&#26694;&#26550;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;</title><link>http://arxiv.org/abs/1902.10664</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#28151;&#21512;&#23454;&#29616;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#23616;&#37096;&#20989;&#25968;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Local Function Complexity for Active Learning via Mixture of Gaussian Processes. (arXiv:1902.10664v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1902.10664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#20989;&#25968;&#22797;&#26434;&#24615;&#65288;LFC&#65289;&#30340;&#20272;&#35745;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#23616;&#37096;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#21457;&#23637;&#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#30340;&#23616;&#37096;&#22810;&#39033;&#24335;&#24179;&#28369;&#65288;LPS&#65289;&#27169;&#22411;&#30340;&#31867;&#27604;&#65292;&#20351;&#24471;&#35813;&#26694;&#26550;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#30340;&#19981;&#22343;&#21248;&#24615;&#65292;&#20363;&#22914;&#35266;&#27979;&#22122;&#22768;&#27700;&#24179;&#30340;&#21464;&#21270;&#25110;&#28304;&#20989;&#25968;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#21464;&#21270;&#65292;&#32473;&#32479;&#35745;&#25512;&#26029;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#22240;&#32032;&#21487;&#20197;&#22312;&#29289;&#29702;&#36164;&#28304;&#25110;&#35745;&#31639;&#26102;&#38388;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#25991;&#20511;&#37492;&#20102;&#26368;&#36817;&#20851;&#20110;&#23616;&#37096;&#22810;&#39033;&#24335;&#24179;&#28369;&#65288;LPS&#65289;&#39046;&#22495;&#20013;&#23616;&#37096;&#20989;&#25968;&#22797;&#26434;&#24615;&#65288;LFC&#65289;&#30340;&#20272;&#35745;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#23616;&#37096;&#32467;&#26500;&#22797;&#26434;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#29992;&#23427;&#26469;&#24320;&#21457;&#19968;&#20010;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26694;&#26550;&#12290;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#28857;&#20272;&#35745;&#65292;LPS&#27169;&#22411;&#31867;&#22312;&#22788;&#29702;&#36890;&#24120;&#20276;&#38543;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#22823;&#36755;&#20837;&#31354;&#38388;&#32500;&#24230;&#26102;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#21644;&#20272;&#35745;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;GPR&#65289;&#30340;LPS-based LFC&#30340;&#31867;&#27604;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20197;&#19978;&#26694;&#26550;&#30340;&#26367;&#20195;&#65292;&#20351;&#20043;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inhomogeneities in real-world data, e.g., due to changes in the observation noise level or variations in the structural complexity of the source function, pose a unique set of challenges for statistical inference. Accounting for them can greatly improve predictive power when physical resources or computation time is limited. In this paper, we draw on recent theoretical results on the estimation of local function complexity (LFC), derived from the domain of local polynomial smoothing (LPS), to establish a notion of local structural complexity, which is used to develop a model-agnostic active learning (AL) framework. Due to its reliance on pointwise estimates, the LPS model class is not robust and scalable concerning large input space dimensions that typically come along with real-world problems. Here, we derive and estimate the Gaussian process regression (GPR)-based analog of the LPS-based LFC and use it as a substitute in the above framework to make it robust and scalable. We assess t
&lt;/p&gt;</description></item></channel></rss>