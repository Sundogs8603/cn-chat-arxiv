<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#23478;&#24237;&#20445;&#20581;&#26426;&#26500;&#38754;&#20020;&#30340;&#25252;&#29702;&#21592;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#25252;&#29702;&#21592;&#23545;&#35775;&#38382;&#39034;&#24207;&#30340;&#28789;&#27963;&#24615;&#65292;&#26088;&#22312;&#20943;&#23569;&#34892;&#39542;&#37324;&#31243;&#12289;&#22686;&#21152;&#35775;&#38382;&#27425;&#25968;&#65292;&#24182;&#20445;&#25345;&#25252;&#29702;&#30340;&#36830;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00696</link><description>&lt;p&gt;
&#20915;&#31574;&#25903;&#25345;&#26694;&#26550;&#22312;&#23478;&#24237;&#20445;&#20581;&#25252;&#29702;&#21592;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;&#65306;&#30000;&#32435;&#35199;&#24030;HHC&#26426;&#26500;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Decision Support Framework for Home Health Caregiver Allocation: A Case Study of HHC Agency in Tennessee, USA. (arXiv:2311.00696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#23478;&#24237;&#20445;&#20581;&#26426;&#26500;&#38754;&#20020;&#30340;&#25252;&#29702;&#21592;&#20998;&#37197;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#25252;&#29702;&#21592;&#23545;&#35775;&#38382;&#39034;&#24207;&#30340;&#28789;&#27963;&#24615;&#65292;&#26088;&#22312;&#20943;&#23569;&#34892;&#39542;&#37324;&#31243;&#12289;&#22686;&#21152;&#35775;&#38382;&#27425;&#25968;&#65292;&#24182;&#20445;&#25345;&#25252;&#29702;&#30340;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32769;&#40836;&#21270;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#23545;&#32769;&#24180;&#20154;&#30340;&#21307;&#30103;&#21644;&#31038;&#20250;&#26381;&#21153;&#38656;&#27714;&#22686;&#21152;&#12290;&#23478;&#24237;&#20445;&#20581;&#25252;&#29702;&#65288;HHC&#65289;&#20316;&#20026;&#19968;&#31181;&#19987;&#38376;&#20026;&#36825;&#19968;&#20154;&#32676;&#25552;&#20379;&#26381;&#21153;&#30340;&#37325;&#35201;&#35299;&#20915;&#26041;&#26696;&#27491;&#36880;&#28176;&#20852;&#36215;&#12290;&#37492;&#20110;&#23545;HHC&#30340;&#38656;&#27714;&#28608;&#22686;&#65292;&#26377;&#25928;&#22320;&#21327;&#35843;&#21644;&#31649;&#29702;&#25252;&#29702;&#21592;&#30340;&#20998;&#37197;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#23545;&#20110;&#39044;&#31639;&#20248;&#21270;&#30340;&#35268;&#21010;&#21644;&#30830;&#20445;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25252;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#22238;&#31572;&#20102;&#23478;&#24237;&#20445;&#20581;&#26426;&#26500;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#8220;&#22312;&#25252;&#29702;&#21592;&#20559;&#22909;&#28789;&#27963;&#30340;&#35775;&#38382;&#39034;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#20248;&#21270;&#20182;&#20204;&#30340;&#20998;&#37197;&#65311;&#8221;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21018;&#24615;&#30340;&#35775;&#38382;&#39034;&#24207;&#65292;&#32780;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20915;&#31574;&#25903;&#25345;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#23545;&#25252;&#29702;&#21592;&#36827;&#34892;&#20998;&#37197;&#65292;&#32771;&#34385;&#20102;&#35775;&#38382;&#39034;&#24207;&#30340;&#28789;&#27963;&#24615;&#65292;&#26088;&#22312;&#20943;&#23569;&#34892;&#39542;&#37324;&#31243;&#12289;&#22686;&#21152;&#27599;&#20010;&#35268;&#21010;&#21608;&#26399;&#30340;&#35775;&#38382;&#27425;&#25968;&#65292;&#24182;&#20445;&#25345;&#36830;&#32493;&#25252;&#29702;-&#36825;&#26159;&#34913;&#37327;&#24739;&#32773;&#24773;&#20917;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Population aging is a global challenge, leading to increased demand for healthcare and social services for the elderly. Home Health Care (HHC) emerges as a vital solution, specifically designed to serve this population segment. Given the surging demand for HHC, it's essential to coordinate and regulate caregiver allocation efficiently. This is crucial for both budget-optimized planning and ensuring the delivery of high-quality care. This research addresses a key question faced by home health agencies (HHAs): "How can caregiver allocation be optimized, especially when caregivers prefer flexibility in their visiting sequences?". While earlier studies proposed rigid visiting sequences, our study introduces a decision support framework that allocates caregivers through a hybrid method that considers the flexibility in visiting sequences and aims to reduce travel mileage, increase the number of visits per planning period, and maintain the continuity of care - a critical metric for patient s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#36719;&#20214;&#23384;&#20648;&#24211;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#32476;&#23433;&#20840;&#23384;&#20648;&#24211;&#20197;&#21450;&#20027;&#39064;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#36719;&#20214;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#35782;&#21035;&#20986;&#20851;&#38190;&#30340;&#32593;&#32476;&#23433;&#20840;&#28431;&#27934;&#65292;&#20026;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#33258;&#21160;&#21270;&#30340;&#28431;&#27934;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.00691</link><description>&lt;p&gt;
&#36719;&#20214;&#23384;&#20648;&#24211;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Software Repositories and Machine Learning Research in Cyber Security. (arXiv:2311.00691v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#36719;&#20214;&#23384;&#20648;&#24211;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#32593;&#32476;&#23433;&#20840;&#23384;&#20648;&#24211;&#20197;&#21450;&#20027;&#39064;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#36719;&#20214;&#24320;&#21457;&#30340;&#26089;&#26399;&#38454;&#27573;&#35782;&#21035;&#20986;&#20851;&#38190;&#30340;&#32593;&#32476;&#23433;&#20840;&#28431;&#27934;&#65292;&#20026;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#33258;&#21160;&#21270;&#30340;&#28431;&#27934;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#24555;&#36895;&#21457;&#23637;&#30340;&#25216;&#26415;&#29615;&#22659;&#21644;&#20808;&#36827;&#30340;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#32593;&#32476;&#23433;&#20840;&#25915;&#20987;&#30340;&#22686;&#21152;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#22312;&#36719;&#20214;&#24320;&#21457;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#25972;&#21512;&#24378;&#22823;&#30340;&#32593;&#32476;&#23433;&#20840;&#38450;&#24481;&#24050;&#32463;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#21021;&#26399;&#38454;&#27573;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#27714;&#38454;&#27573;&#65292;&#35782;&#21035;&#20851;&#38190;&#30340;&#32593;&#32476;&#23433;&#20840;&#28431;&#27934;&#20855;&#26377;&#29305;&#27530;&#30340;&#24847;&#20041;&#12290;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23433;&#20840;&#23384;&#20648;&#24211;&#65292;&#22914;MITRE&#30340;&#24120;&#35265;&#25915;&#20987;&#27169;&#24335;&#26522;&#20030;&#21644;&#20998;&#31867;&#65288;CAPEC&#65289;&#20197;&#21450;&#24120;&#35265;&#28431;&#27934;&#21644;&#20844;&#24320;&#28431;&#27934;&#65288;CVE&#65289;&#25968;&#25454;&#24211;&#65292;&#24050;&#32463;&#23581;&#35797;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#26816;&#27979;&#36719;&#20214;&#38656;&#27714;&#36807;&#31243;&#20013;&#30340;&#36825;&#20123;&#26089;&#26399;&#28431;&#27934;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20027;&#39064;&#22312;&#23581;&#35797;&#33258;&#21160;&#21270;&#28431;&#27934;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#32467;&#26524;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's rapidly evolving technological landscape and advanced software development, the rise in cyber security attacks has become a pressing concern. The integration of robust cyber security defenses has become essential across all phases of software development. It holds particular significance in identifying critical cyber security vulnerabilities at the initial stages of the software development life cycle, notably during the requirement phase. Through the utilization of cyber security repositories like The Common Attack Pattern Enumeration and Classification (CAPEC) from MITRE and the Common Vulnerabilities and Exposures (CVE) databases, attempts have been made to leverage topic modeling and machine learning for the detection of these early-stage vulnerabilities in the software requirements process. Past research themes have returned successful outcomes in attempting to automate vulnerability identification for software developers, employing a mixture of unsupervised machine lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;&#34892;&#20026;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#20998;&#26512;&#20102;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.00690</link><description>&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#22312;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#20013;&#30340;&#24046;&#24322;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What User Behaviors Make the Differences During the Process of Visual Analytics?. (arXiv:2311.00690v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;&#34892;&#20026;&#30340;&#25968;&#25454;&#37319;&#38598;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#20998;&#26512;&#20102;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35270;&#35273;&#20998;&#26512;&#36807;&#31243;&#30340;&#29702;&#35299;&#21487;&#20197;&#20174;&#22810;&#20010;&#26041;&#38754;&#21463;&#30410;&#20110;&#21487;&#35270;&#21270;&#30740;&#31350;&#20154;&#21592;&#65292;&#21253;&#25324;&#25913;&#36827;&#21487;&#35270;&#21270;&#35774;&#35745;&#21644;&#24320;&#21457;&#20808;&#36827;&#30340;&#20132;&#20114;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24863;&#30693;&#30340;&#22797;&#26434;&#24615;&#21644;&#25105;&#20204;&#23545;&#30456;&#20851;&#29992;&#25143;&#34892;&#20026;&#30340;&#32570;&#20047;&#20102;&#35299;&#65292;&#29992;&#25143;&#34892;&#20026;&#30340;&#26085;&#24535;&#25991;&#20214;&#20173;&#28982;&#38590;&#20197;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#20840;&#38754;&#25968;&#25454;&#37319;&#38598;&#30340;&#30740;&#31350;&#65292;&#24182;&#32467;&#21512;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#20010;&#32463;&#20856;&#30340;&#21487;&#35270;&#21270;&#24212;&#29992;&#65292;Covid-19&#25968;&#25454;&#20998;&#26512;&#65292;&#28085;&#30422;&#22320;&#29702;&#31354;&#38388;&#12289;&#26102;&#38388;&#24207;&#21015;&#21644;&#22810;&#23646;&#24615;&#30340;&#24120;&#35265;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#25910;&#38598;&#20102;&#20851;&#20110;&#22810;&#20010;&#21487;&#35270;&#21270;&#20219;&#21153;&#30340;&#29992;&#25143;&#34892;&#20026;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#21487;&#27604;&#36739;&#30340;&#31995;&#32479;&#65292;&#26700;&#38754;&#21644;&#27785;&#28024;&#24335;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#20004;&#20010;&#23610;&#24230;&#19978;&#20351;&#29992;&#19977;&#31181;&#26102;&#38388;&#24207;&#21015;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#34892;&#20026;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of visual analytics process can benefit visualization researchers from multiple aspects, including improving visual designs and developing advanced interaction functions. However, the log files of user behaviors are still hard to analyze due to the complexity of sensemaking and our lack of knowledge on the related user behaviors. This work presents a study on a comprehensive data collection of user behaviors, and our analysis approach with time-series classification methods. We have chosen a classical visualization application, Covid-19 data analysis, with common analysis tasks covering geo-spatial, time-series and multi-attributes. Our user study collects user behaviors on a diverse set of visualization tasks with two comparable systems, desktop and immersive visualizations. We summarize the classification results with three time-series machine learning algorithms at two scales, and explore the influences of behavior features. Our results reveal that user behaviors c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00687</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#21463;&#20247;&#32676;&#20307;&#65292;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Improving Interpersonal Communication by Simulating Audiences with Language Models. (arXiv:2311.00687v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#12289;&#29983;&#25104;&#27807;&#36890;&#20505;&#36873;&#20197;&#21450;&#27169;&#25311;&#21463;&#20247;&#21453;&#24212;&#65292;&#26469;&#25913;&#21892;&#20154;&#38469;&#27807;&#36890;&#12290;&#36890;&#36807;&#35780;&#20272;&#20843;&#20010;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#22522;&#26412;&#36807;&#31243;&#30340;&#22330;&#26223;&#65292;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#19982;&#20182;&#20154;&#36827;&#34892;&#27807;&#36890;&#20197;&#23454;&#29616;&#33258;&#24049;&#30340;&#30446;&#26631;&#65311;&#25105;&#20204;&#21033;&#29992;&#20808;&#21069;&#30340;&#32463;&#39564;&#25110;&#20182;&#20154;&#30340;&#24314;&#35758;&#65292;&#25110;&#32773;&#36890;&#36807;&#39044;&#27979;&#23545;&#26041;&#30340;&#21453;&#24212;&#26469;&#26500;&#36896;&#20505;&#36873;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#26159;&#26377;&#38480;&#21644;&#26377;&#20559;&#35265;&#30340;&#65292;&#32780;&#19988;&#23545;&#28508;&#22312;&#32467;&#26524;&#36827;&#34892;&#25512;&#29702;&#21487;&#33021;&#26159;&#22256;&#38590;&#19988;&#35748;&#30693;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27169;&#25311;&#26469;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#27807;&#36890;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25506;&#32034;-&#29983;&#25104;-&#27169;&#25311;&#65288;EGS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25509;&#21463;&#20219;&#20309;&#19968;&#20010;&#20010;&#20307;&#19982;&#19968;&#20010;&#30446;&#26631;&#21463;&#20247;&#36827;&#34892;&#27807;&#36890;&#30340;&#22330;&#26223;&#20316;&#20026;&#36755;&#20837;&#12290;EGS&#65288;1&#65289;&#36890;&#36807;&#29983;&#25104;&#19982;&#22330;&#26223;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#24314;&#35758;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#31354;&#38388;&#65292;&#65288;2&#65289;&#29983;&#25104;&#20197;&#37096;&#20998;&#24314;&#35758;&#20026;&#26465;&#20214;&#30340;&#27807;&#36890;&#20505;&#36873;&#65292;&#65288;3&#65289;&#27169;&#25311;&#19981;&#21516;&#21463;&#20247;&#30340;&#21453;&#24212;&#65292;&#20197;&#30830;&#23450;&#26368;&#20339;&#20505;&#36873;&#21644;&#24314;&#35758;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#20154;&#38469;&#27807;&#36890;&#21313;&#20010;&#22522;&#26412;&#36807;&#31243;&#30340;&#20843;&#20010;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#35813;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we communicate with others to achieve our goals? We use our prior experience or advice from others, or construct a candidate utterance by predicting how it will be received. However, our experiences are limited and biased, and reasoning about potential outcomes can be difficult and cognitively challenging. In this paper, we explore how we can leverage Large Language Model (LLM) simulations to help us communicate better. We propose the Explore-Generate-Simulate (EGS) framework, which takes as input any scenario where an individual is communicating to an audience with a goal they want to achieve. EGS (1) explores the solution space by producing a diverse set of advice relevant to the scenario, (2) generates communication candidates conditioned on subsets of the advice, and (3) simulates the reactions from various audiences to determine both the best candidate and advice to use. We evaluate the framework on eight scenarios spanning the ten fundamental processes of interpersonal com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#25913;&#21892;T5&#22312;&#38271;&#24207;&#21015;&#22788;&#29702;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00684</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#23545;&#40784;&#21644;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#25552;&#39640;&#20102;Transformer&#38271;&#24230;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. (arXiv:2311.00684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#25913;&#21892;T5&#22312;&#38271;&#24207;&#21015;&#22788;&#29702;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24819;&#30340;&#38271;&#24230;&#21487;&#22806;&#25512;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#27604;&#35757;&#32451;&#38271;&#24230;&#26356;&#38271;&#30340;&#24207;&#21015;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#38271;&#24207;&#21015;&#24494;&#35843;&#12290;&#36825;&#31181;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#39640;&#24230;&#20381;&#36182;&#20110;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#12290;&#22312;&#35843;&#26597;&#29616;&#26377;&#22823;&#22411;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;T5&#31995;&#21015;&#20540;&#24471;&#26356;&#20180;&#32454;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#30340;&#20301;&#32622;&#23884;&#20837;&#25429;&#25417;&#21040;&#20102;&#20016;&#23500;&#32780;&#28789;&#27963;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;T5&#23384;&#22312;&#30528;&#20998;&#25955;&#30340;&#27880;&#24847;&#21147;&#38382;&#39064;&#65306;&#36755;&#20837;&#24207;&#21015;&#36234;&#38271;&#65292;&#27880;&#24847;&#21147;&#20998;&#24067;&#23601;&#36234;&#24179;&#22374;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#28201;&#24230;&#32553;&#25918;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#39640;&#20102;T5&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#26816;&#32034;&#21644;&#22810;&#25991;&#26723;&#38382;&#31572;&#26041;&#38754;&#30340;&#38271;&#19978;&#19979;&#25991;&#21033;&#29992;&#33021;&#21147;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#65292;&#36825;&#34920;&#26126;&#28789;&#27963;&#30340;&#20301;&#32622;&#23884;&#20837;&#35774;&#35745;&#21644;&#27880;&#24847;&#21147;&#23545;&#40784;&#23545;&#20110;Transformer&#38271;&#24230;&#22806;&#25512;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
An ideal length-extrapolatable Transformer language model can handle sequences longer than the training length without any long sequence fine-tuning. Such long-context utilization capability highly relies on a flexible positional embedding design. Upon investigating the flexibility of existing large pre-trained Transformer language models, we find that the T5 family deserves a closer look, as its positional embeddings capture rich and flexible attention patterns. However, T5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution. To alleviate the issue, we propose two attention alignment strategies via temperature scaling. Our findings improve the long-context utilization capability of T5 on language modeling, retrieval, and multi-document question answering without any fine-tuning, suggesting that a flexible positional embedding design and attention alignment go a long way toward Transformer length extrapolation.\footnote{\url
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CoPhNet&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21306;&#20998;CdZnTeSe&#21322;&#23548;&#20307;&#25506;&#27979;&#22120;&#20013;&#30340;&#24247;&#26222;&#39039;&#25955;&#23556;&#21644;&#20809;&#30005;&#30456;&#20114;&#20316;&#29992;&#30340;&#20285;&#29595;/ x&#23556;&#32447;&#20809;&#23376;&#12290;</title><link>http://arxiv.org/abs/2311.00682</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23460;&#28201;&#21322;&#23548;&#20307;&#36752;&#23556;&#25506;&#27979;&#22120;&#20013;&#20285;&#29595;&#20809;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Classification of Gamma Photon Interactions in Room-Temperature Semiconductor Radiation Detectors. (arXiv:2311.00682v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CoPhNet&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21306;&#20998;CdZnTeSe&#21322;&#23548;&#20307;&#25506;&#27979;&#22120;&#20013;&#30340;&#24247;&#26222;&#39039;&#25955;&#23556;&#21644;&#20809;&#30005;&#30456;&#20114;&#20316;&#29992;&#30340;&#20285;&#29595;/ x&#23556;&#32447;&#20809;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23376;&#35745;&#25968;&#36752;&#23556;&#25506;&#27979;&#22120;&#24050;&#32463;&#25104;&#20026;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#65288;&#22914;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#25110;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65289;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#26377;&#21069;&#26223;&#30340;&#25506;&#27979;&#22120;&#20043;&#19968;&#26159;&#23485;&#31105;&#24102;&#23460;&#28201;&#21322;&#23548;&#20307;&#25506;&#27979;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#20285;&#29595;/ x&#23556;&#32447;&#20809;&#23376;&#19982;&#25506;&#27979;&#22120;&#26448;&#26009;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#21253;&#25324;&#24247;&#26222;&#39039;&#25955;&#23556;&#65292;&#23548;&#33268;&#21333;&#20010;&#20809;&#23376;&#30340;&#22810;&#27425;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;MIPE&#65289;&#12290;&#23545;&#20110;&#20855;&#26377;&#24247;&#26222;&#39039;&#25955;&#23556;&#21644;&#20809;&#30005;&#20107;&#20214;&#20043;&#38388;&#25506;&#27979;&#33021;&#37327;&#37325;&#21472;&#30340;&#21322;&#23548;&#20307;&#25506;&#27979;&#22120;&#65288;&#22914;CdZnTeSe&#65288;CZTS&#65289;&#65289;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#35835;&#20986;&#30005;&#23376;&#23398;&#25110;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#20960;&#20046;&#19981;&#21487;&#33021;&#21306;&#20998;&#24247;&#26222;&#39039;&#25955;&#23556;&#20107;&#20214;&#21644;&#20809;&#30005;&#20107;&#20214;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#21517;&#20026;CoPhNet&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21306;&#20998;CdZnTeSe&#65288;CZTS&#65289;&#21322;&#23548;&#20307;&#25506;&#27979;&#22120;&#20013;&#30340;&#24247;&#26222;&#39039;&#25955;&#23556;&#21644;&#20809;&#30005;&#30456;&#20114;&#20316;&#29992;&#30340;&#20285;&#29595;/ x&#23556;&#32447;&#20809;&#23376;&#12290;&#25105;&#20204;&#30340;CoPhNet&#27169;&#22411;&#26159;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photon counting radiation detectors have become an integral part of medical imaging modalities such as Positron Emission Tomography or Computed Tomography. One of the most promising detectors is the wide bandgap room temperature semiconductor detectors, which depends on the interaction gamma/x-ray photons with the detector material involves Compton scattering which leads to multiple interaction photon events (MIPEs) of a single photon. For semiconductor detectors like CdZnTeSe (CZTS), which have a high overlap of detected energies between Compton and photoelectric events, it is nearly impossible to distinguish between Compton scattered events from photoelectric events using conventional readout electronics or signal processing algorithms. Herein, we report a deep learning classifier CoPhNet that distinguishes between Compton scattering and photoelectric interactions of gamma/x-ray photons with CdZnTeSe (CZTS) semiconductor detectors. Our CoPhNet model was trained using simulated data t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#35299;&#20915;&#20855;&#26377;&#38543;&#26426;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#20013;&#21333;&#24490;&#29615;&#20108;&#27425;&#32602;&#20989;&#25968;&#21644;&#22686;&#24191;Lagrangian&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#20855;&#26377;&#19981;&#21516;&#32422;&#26463;&#24615;&#36136;&#30340;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#20854;&#20013;&#30340;&#20004;&#31181;&#24773;&#20917;&#26159;&#39318;&#20010;&#37319;&#29992;&#21333;&#24490;&#29615;&#31639;&#27861;&#24182;&#20855;&#26377;&#19968;&#23450;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.00678</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#35268;&#21010;&#20013;&#20855;&#26377;&#38543;&#26426;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#21333;&#24490;&#29615;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Complexity of Single Loop Algorithms for Nonlinear Programming with Stochastic Objective and Constraints. (arXiv:2311.00678v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00678
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#35299;&#20915;&#20855;&#26377;&#38543;&#26426;&#30446;&#26631;&#21644;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#20013;&#21333;&#24490;&#29615;&#20108;&#27425;&#32602;&#20989;&#25968;&#21644;&#22686;&#24191;Lagrangian&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#20855;&#26377;&#19981;&#21516;&#32422;&#26463;&#24615;&#36136;&#30340;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#20854;&#20013;&#30340;&#20004;&#31181;&#24773;&#20917;&#26159;&#39318;&#20010;&#37319;&#29992;&#21333;&#24490;&#29615;&#31639;&#27861;&#24182;&#20855;&#26377;&#19968;&#23450;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#21151;&#33021;&#31561;&#24335;&#32422;&#26463;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#21333;&#24490;&#29615;&#20108;&#27425;&#32602;&#20989;&#25968;&#21644;&#22686;&#24191;Lagrangian&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#24773;&#20917;&#65292;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#20989;&#25968;&#37117;&#26159;&#38543;&#26426;&#19988;&#24179;&#28369;&#30340;&#65292;&#21363;&#23545;&#26410;&#30693;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#30340;&#26399;&#26395;&#12290;&#19977;&#31181;&#24773;&#20917;&#19979;&#31561;&#24335;&#32422;&#26463;&#30340;&#24615;&#36136;&#26377;&#25152;&#19981;&#21516;&#65306;&#31532;&#19968;&#31181;&#24773;&#20917;&#19979;&#30830;&#23450;&#24615;&#21644;&#32447;&#24615;&#65292;&#31532;&#20108;&#31181;&#24773;&#20917;&#19979;&#30830;&#23450;&#24615;&#12289;&#24179;&#28369;&#21644;&#38750;&#32447;&#24615;&#65292;&#31532;&#19977;&#31181;&#24773;&#20917;&#19979;&#38543;&#26426;&#12289;&#24179;&#28369;&#21644;&#38750;&#32447;&#24615;&#12290;&#21033;&#29992;&#26041;&#24046;&#20943;&#23567;&#25216;&#26415;&#25913;&#21892;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#25214;&#21040;&#28385;&#36275;&#949;&#36817;&#20284;&#19968;&#38454;&#26465;&#20214;&#30340;&#28857;&#65292;&#25105;&#20204;&#22312;&#31532;&#19968;&#31181;&#24773;&#20917;&#19979;&#38656;&#35201;&#22797;&#26434;&#24230;&#20026;$\widetilde{O}(\varepsilon^{-3})$&#65292;&#31532;&#20108;&#31181;&#24773;&#20917;&#19979;&#38656;&#35201;&#22797;&#26434;&#24230;&#20026;$\widetilde{O}(\varepsilon^{-4})$&#65292;&#31532;&#19977;&#31181;&#24773;&#20917;&#19979;&#38656;&#35201;&#22797;&#26434;&#24230;&#20026;$\widetilde{O}(\varepsilon^{-5})$&#12290;&#23545;&#20110;&#31532;&#19968;&#31181;&#21644;&#31532;&#19977;&#31181;&#24773;&#20917;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#8220;&#21333;&#24490;&#29615;&#8221;&#31867;&#22411;&#30340;&#31639;&#27861;&#65288;&#21516;&#26102;&#20063;&#20351;&#29992;$O$
&lt;/p&gt;
&lt;p&gt;
We analyze the complexity of single-loop quadratic penalty and augmented Lagrangian algorithms for solving nonconvex optimization problems with functional equality constraints. We consider three cases, in all of which the objective is stochastic and smooth, that is, an expectation over an unknown distribution that is accessed by sampling. The nature of the equality constraints differs among the three cases: deterministic and linear in the first case, deterministic, smooth and nonlinear in the second case, and stochastic, smooth and nonlinear in the third case. Variance reduction techniques are used to improve the complexity. To find a point that satisfies $\varepsilon$-approximate first-order conditions, we require $\widetilde{O}(\varepsilon^{-3})$ complexity in the first case, $\widetilde{O}(\varepsilon^{-4})$ in the second case, and $\widetilde{O}(\varepsilon^{-5})$ in the third case. For the first and third cases, they are the first algorithms of "single loop" type (that also use $O
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#36951;&#25022;&#21305;&#37197;&#30340;&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#36136;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#21457;&#29616;&#22810;&#20010;&#23454;&#38469;&#21464;&#20307;&#22312;&#31616;&#21333;&#30340;&#28216;&#25103;&#20013;&#32570;&#20047;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#20445;&#35777;&#65292;&#32780;&#22522;&#20110;&#24179;&#28369;&#25216;&#26415;&#30340;&#26368;&#36817;&#21464;&#20307;&#21017;&#20855;&#26377;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00676</link><description>&lt;p&gt;
Regret-Matching&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games. (arXiv:2311.00676v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#36951;&#25022;&#21305;&#37197;&#30340;&#31639;&#27861;&#22312;&#28216;&#25103;&#20013;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#36136;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#21457;&#29616;&#22810;&#20010;&#23454;&#38469;&#21464;&#20307;&#22312;&#31616;&#21333;&#30340;&#28216;&#25103;&#20013;&#32570;&#20047;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#20445;&#35777;&#65292;&#32780;&#22522;&#20110;&#24179;&#28369;&#25216;&#26415;&#30340;&#26368;&#36817;&#21464;&#20307;&#21017;&#20855;&#26377;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36951;&#25022;&#21305;&#37197;&#30340;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#36951;&#25022;&#21305;&#37197;+ (RM+)&#21450;&#20854;&#21464;&#31181;&#65292;&#26159;&#35299;&#20915;&#22823;&#35268;&#27169;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#12290;&#19982;&#20855;&#26377;&#38646;&#21644;&#28216;&#25103;&#30340;&#24378;&#26368;&#32456;&#36845;&#20195;&#21644;&#36941;&#21382;&#25910;&#25947;&#24615;&#36136;&#30340;&#31639;&#27861;&#65288;&#22914;&#20048;&#35266;&#26799;&#24230;&#19978;&#21319;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#23545;&#20110;&#36951;&#25022;&#21305;&#37197;&#31639;&#27861;&#30340;&#26368;&#32456;&#36845;&#20195;&#24615;&#36136;&#20960;&#20046;&#19968;&#26080;&#25152;&#30693;&#12290;&#37492;&#20110;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#23545;&#20110;&#25968;&#20540;&#20248;&#21270;&#21644;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#28216;&#25103;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;RM+&#21464;&#20307;&#30340;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#36136;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;&#21253;&#25324;&#21516;&#26102;RM+&#12289;&#20132;&#26367;RM+&#21644;&#21516;&#26102;&#39044;&#27979;RM+&#22312;&#20869;&#30340;&#20960;&#20010;&#23454;&#38469;&#21464;&#20307;&#65292;&#29978;&#33267;&#22312;&#31616;&#21333;&#30340;3x3&#28216;&#25103;&#20013;&#20063;&#32570;&#20047;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26368;&#36817;&#21464;&#20307;&#65292;&#22522;&#20110;&#24179;&#28369;&#25216;&#26415;&#24471;&#21040;&#20102;&#26368;&#32456;&#36845;&#20195;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms based on regret matching, specifically regret matching$^+$ (RM$^+$), and its variants are the most popular approaches for solving large-scale two-player zero-sum games in practice. Unlike algorithms such as optimistic gradient descent ascent, which have strong last-iterate and ergodic convergence properties for zero-sum games, virtually nothing is known about the last-iterate properties of regret-matching algorithms. Given the importance of last-iterate convergence for numerical optimization reasons and relevance as modeling real-word learning in games, in this paper, we study the last-iterate convergence properties of various popular variants of RM$^+$. First, we show numerically that several practical variants such as simultaneous RM$^+$, alternating RM$^+$, and simultaneous predictive RM$^+$, all lack last-iterate convergence guarantees even on a simple $3\times 3$ game. We then prove that recent variants of these algorithms based on a smoothing technique do enjoy last-it
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Cholesky&#20998;&#35299;&#24674;&#22797;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#32447;&#24615;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#31934;&#30830;&#24674;&#22797;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00674</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;Cholesky&#20998;&#35299;&#24674;&#22797;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#32447;&#24615;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Recovering Linear Causal Models with Latent Variables via Cholesky Factorization of Covariance Matrix. (arXiv:2311.00674v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Cholesky&#20998;&#35299;&#24674;&#22797;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#32447;&#24615;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#36895;&#24230;&#21644;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#31934;&#30830;&#24674;&#22797;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#24674;&#22797;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#32467;&#26500;&#26469;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#24403;&#23384;&#22312;&#28508;&#21464;&#37327;&#26102;&#65292;&#35813;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;Cholesky&#20998;&#35299;&#30340;DAG&#32467;&#26500;&#24674;&#22797;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#24555;&#36895;&#26131;&#23454;&#29616;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;&#31934;&#30830;&#24674;&#22797;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#31639;&#27861;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#24555;&#24471;&#22810;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#31561;&#35823;&#24046;&#26041;&#24046;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23558;&#20248;&#21270;&#36807;&#31243;&#19982;&#22522;&#20110;Cholesky&#20998;&#35299;&#30340;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#22788;&#29702;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;DAG&#24674;&#22797;&#38382;&#39064;&#12290;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#20462;&#27491;&#30340;&#8220;Cholesky + &#20248;&#21270;&#8221;&#31639;&#27861;&#33021;&#22815;&#24674;&#22797;&#20986;&#30495;&#23454;&#30340;&#22270;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering the causal relationship via recovering the directed acyclic graph (DAG) structure from the observed data is a well-known challenging combinatorial problem. When there are latent variables, the problem becomes even more difficult. In this paper, we first propose a DAG structure recovering algorithm, which is based on the Cholesky factorization of the covariance matrix of the observed data. The algorithm is fast and easy to implement and has theoretical grantees for exact recovery. On synthetic and real-world datasets, the algorithm is significantly faster than previous methods and achieves the state-of-the-art performance. Furthermore, under the equal error variances assumption, we incorporate an optimization procedure into the Cholesky factorization based algorithm to handle the DAG recovering problem with latent variables. Numerical simulations show that the modified "Cholesky + optimization" algorithm is able to recover the ground truth graph in most cases and outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28508;&#22312;&#31354;&#38388;&#30340;&#32763;&#35793;&#38382;&#39064;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#21464;&#25442;&#65292;&#21487;&#20197;&#23558;&#19981;&#21516;&#31070;&#32463;&#27169;&#22411;&#23398;&#21040;&#30340;&#34920;&#31034;&#32763;&#35793;&#21040;&#20854;&#20182;&#39044;&#35757;&#32451;&#32593;&#32476;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25340;&#25509;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00664</link><description>&lt;p&gt;
&#28508;&#22312;&#31354;&#38388;&#32763;&#35793;&#36890;&#36807;&#35821;&#20041;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Latent Space Translation via Semantic Alignment. (arXiv:2311.00664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28508;&#22312;&#31354;&#38388;&#30340;&#32763;&#35793;&#38382;&#39064;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#21464;&#25442;&#65292;&#21487;&#20197;&#23558;&#19981;&#21516;&#31070;&#32463;&#27169;&#22411;&#23398;&#21040;&#30340;&#34920;&#31034;&#32763;&#35793;&#21040;&#20854;&#20182;&#39044;&#35757;&#32451;&#32593;&#32476;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25340;&#25509;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#19981;&#21516;&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#25509;&#35302;&#21040;&#35821;&#20041;&#30456;&#20851;&#30340;&#25968;&#25454;&#26102;&#24448;&#24448;&#20250;&#23637;&#29616;&#20986;&#30456;&#20284;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20294;&#36825;&#31181;&#20869;&#22312;&#30340;&#30456;&#20284;&#24615;&#24182;&#19981;&#24635;&#26159;&#31435;&#21363;&#21487;&#36776;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#31616;&#21333;&#30340;&#21464;&#25442;&#23558;&#20174;&#36825;&#20123;&#31070;&#32463;&#27169;&#22359;&#23398;&#21040;&#30340;&#34920;&#31034;&#32763;&#35793;&#21040;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#20043;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#20351;&#29992;&#26631;&#20934;&#30340;&#12289;&#36890;&#29992;&#30340;&#20195;&#25968;&#31243;&#24207;&#26469;&#20272;&#35745;&#36825;&#20123;&#21464;&#25442;&#65292;&#24182;&#19988;&#36825;&#20123;&#31243;&#24207;&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#20272;&#35745;&#20004;&#20010;&#32473;&#23450;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#25340;&#25509;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#20013;&#24191;&#27867;&#39564;&#35777;&#20102;&#36825;&#31181;&#32763;&#35793;&#36807;&#31243;&#30340;&#36866;&#24212;&#24615;&#65306;&#21253;&#25324;&#21508;&#31181;&#35757;&#32451;&#25968;&#25454;&#12289;&#39046;&#22495;&#12289;&#26550;&#26500;&#65288;&#22914;ResNet&#12289;CNN&#12289;ViT&#65289;&#20197;&#21450;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65288;&#20998;&#31867;&#12289;&#37325;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While different neural models often exhibit latent spaces that are alike when exposed to semantically related data, this intrinsic similarity is not always immediately discernible. Towards a better understanding of this phenomenon, our work shows how representations learned from these neural modules can be translated between different pre-trained networks via simpler transformations than previously thought. An advantage of this approach is the ability to estimate these transformations using standard, well-understood algebraic procedures that have closed-form solutions. Our method directly estimates a transformation between two given latent spaces, thereby enabling effective stitching of encoders and decoders without additional training. We extensively validate the adaptability of this translation procedure in different experimental settings: across various trainings, domains, architectures (e.g., ResNet, CNN, ViT), and in multiple downstream tasks (classification, reconstruction). Nota
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26102;&#38388;&#21464;&#21270;&#22270;&#36793;&#32536;&#20449;&#21495;&#39044;&#27979;&#31639;&#27861;&#65292;&#21033;&#29992;&#32447;&#22270;&#36716;&#25442;&#36793;&#32536;&#20449;&#21495;&#20026;&#36793;&#21040;&#39030;&#28857;&#23545;&#20598;&#33410;&#28857;&#65292;&#20351;&#24471;&#20449;&#21495;&#21487;&#20197;&#20351;&#29992;&#24050;&#26377;&#30340;GSP&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2311.00656</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#22270;&#36716;&#25442;&#22312;&#32447;&#20272;&#35745;&#22270;&#36793;&#32536;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Online Signal Estimation on the Graph Edges via Line Graph Transformation. (arXiv:2311.00656v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26102;&#38388;&#21464;&#21270;&#22270;&#36793;&#32536;&#20449;&#21495;&#39044;&#27979;&#31639;&#27861;&#65292;&#21033;&#29992;&#32447;&#22270;&#36716;&#25442;&#36793;&#32536;&#20449;&#21495;&#20026;&#36793;&#21040;&#39030;&#28857;&#23545;&#20598;&#33410;&#28857;&#65292;&#20351;&#24471;&#20449;&#21495;&#21487;&#20197;&#20351;&#29992;&#24050;&#26377;&#30340;GSP&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#22270;&#24402;&#19968;&#21270;&#26368;&#23567;&#22343;&#26041;(LGNLMS)&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#26102;&#38388;&#21464;&#21270;&#22270;&#36793;&#32536;&#20449;&#21495;&#30340;&#39044;&#27979;&#12290;LGNLMS&#21033;&#29992;&#32447;&#22270;&#23558;&#22270;&#36793;&#32536;&#20449;&#21495;&#36716;&#25442;&#20026;&#20854;&#36793;&#21040;&#39030;&#28857;&#23545;&#20598;&#33410;&#28857;&#12290;&#36825;&#20351;&#24471;&#36793;&#32536;&#20449;&#21495;&#21487;&#20197;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;GSP&#27010;&#24565;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#26080;&#38656;&#22312;&#22270;&#36793;&#32536;&#19978;&#37325;&#26032;&#23450;&#20041;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Line Graph Normalized Least Mean Square (LGNLMS) algorithm for online time-varying graph edge signals prediction. LGNLMS utilizes the Line Graph to transform graph edge signals into the node of its edge-to-vertex dual. This enables edge signals to be processed using established GSP concepts without redefining them on graph edges.
&lt;/p&gt;</description></item><item><title>FAIRLABEL&#26159;&#19968;&#31181;&#26816;&#27979;&#21644;&#32416;&#27491;&#26631;&#31614;&#20013;&#20559;&#35265;&#30340;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#32676;&#20307;&#38388;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#32467;&#26524;&#26174;&#31034;FAIRLABEL&#22312;&#26631;&#31614;&#20462;&#27491;&#26041;&#38754;&#30340;&#27491;&#30830;&#29575;&#36739;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;14.8%, &#22312;&#19981;&#24179;&#31561;&#24433;&#21709;&#27604;&#29575;&#26041;&#38754;&#36798;&#21040;&#20102;54.2%&#30340;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2311.00638</link><description>&lt;p&gt;
FAIRLABEL&#65306;&#20462;&#27491;&#26631;&#31614;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
FAIRLABEL: Correcting Bias in Labels. (arXiv:2311.00638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00638
&lt;/p&gt;
&lt;p&gt;
FAIRLABEL&#26159;&#19968;&#31181;&#26816;&#27979;&#21644;&#32416;&#27491;&#26631;&#31614;&#20013;&#20559;&#35265;&#30340;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#32676;&#20307;&#38388;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#32467;&#26524;&#26174;&#31034;FAIRLABEL&#22312;&#26631;&#31614;&#20462;&#27491;&#26041;&#38754;&#30340;&#27491;&#30830;&#29575;&#36739;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;14.8%, &#22312;&#19981;&#24179;&#31561;&#24433;&#21709;&#27604;&#29575;&#26041;&#38754;&#36798;&#21040;&#20102;54.2%&#30340;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#22810;&#31181;&#31639;&#27861;&#21487;&#20197;&#34913;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#22522;&#26412;&#20551;&#35774;&#26159;&#65292;&#30495;&#23454;&#25968;&#25454;&#26159;&#20844;&#24179;&#25110;&#26080;&#20559;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#30495;&#23454;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#21382;&#21490;&#21644;&#31038;&#20250;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23558;&#32487;&#25215;&#24182;&#20256;&#25773;&#20559;&#35265;&#21040;&#27169;&#22411;&#36755;&#20986;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAIRLABEL&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32416;&#27491;&#26631;&#31614;&#20013;&#30340;&#20559;&#35265;&#12290;FAIRLABEL&#30340;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#32676;&#20307;&#38388;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65288;DI&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#20559;&#35265;&#20462;&#27491;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;FAIRLABEL&#30340;&#27491;&#30830;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#26631;&#31614;&#20462;&#27491;&#30340;&#27491;&#30830;&#29575;&#20026;86.7%&#65292;&#32780;&#22522;&#20934;&#27169;&#22411;&#20026;71.9%&#12290;&#25105;&#20204;&#36824;&#23558;FAIRLABEL&#24212;&#29992;&#20110;UCI Adult&#12289;German Credit Risk&#21644;Compas&#25968;&#25454;&#38598;&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#26174;&#31034;&#19981;&#24179;&#31561;&#24433;&#21709;&#27604;&#29575;&#26368;&#22810;&#22686;&#21152;&#20102;54.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
There are several algorithms for measuring fairness of ML models. A fundamental assumption in these approaches is that the ground truth is fair or unbiased. In real-world datasets, however, the ground truth often contains data that is a result of historical and societal biases and discrimination. Models trained on these datasets will inherit and propagate the biases to the model outputs. We propose FAIRLABEL, an algorithm which detects and corrects biases in labels. The goal of FAIRLABELis to reduce the Disparate Impact (DI) across groups while maintaining high accuracy in predictions. We propose metrics to measure the quality of bias correction and validate FAIRLABEL on synthetic datasets and show that the label correction is correct 86.7% of the time vs. 71.9% for a baseline model. We also apply FAIRLABEL on benchmark datasets such as UCI Adult, German Credit Risk, and Compas datasets and show that the Disparate Impact Ratio increases by as much as 54.2%.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#20811;&#32599;&#20869;&#20811;&#36817;&#20284;&#26354;&#29575;&#31639;&#27861;&#65292;&#21487;&#20197;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;&#20004;&#31181;&#20855;&#26377;&#32447;&#24615;&#26435;&#37325;&#20849;&#20139;&#23618;&#19981;&#21516;&#35774;&#32622;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#24212;&#35774;&#32622;&#19979;&#30340;K-FAC&#31639;&#27861;&#30340;&#31934;&#30830;&#24615;&#12290;K-FAC-reduce&#36890;&#24120;&#27604;K-FAC-expand&#26356;&#24555;&#65292;&#21487;&#20197;&#29992;&#20110;&#21152;&#36895;&#33258;&#21160;&#36229;&#21442;&#25968;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2311.00636</link><description>&lt;p&gt;
Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures&#65288;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#20811;&#32599;&#20869;&#20811;&#36817;&#20284;&#26354;&#29575;&#65289;
&lt;/p&gt;
&lt;p&gt;
Kronecker-Factored Approximate Curvature for Modern Neural Network Architectures. (arXiv:2311.00636v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#20811;&#32599;&#20869;&#20811;&#36817;&#20284;&#26354;&#29575;&#31639;&#27861;&#65292;&#21487;&#20197;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#20316;&#32773;&#21457;&#29616;&#20102;&#20004;&#31181;&#20855;&#26377;&#32447;&#24615;&#26435;&#37325;&#20849;&#20139;&#23618;&#19981;&#21516;&#35774;&#32622;&#65292;&#24182;&#35777;&#26126;&#20102;&#30456;&#24212;&#35774;&#32622;&#19979;&#30340;K-FAC&#31639;&#27861;&#30340;&#31934;&#30830;&#24615;&#12290;K-FAC-reduce&#36890;&#24120;&#27604;K-FAC-expand&#26356;&#24555;&#65292;&#21487;&#20197;&#29992;&#20110;&#21152;&#36895;&#33258;&#21160;&#36229;&#21442;&#25968;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#22914;transformers&#12289;&#21367;&#31215;&#25110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#34920;&#36798;&#20026;&#20855;&#26377;&#8220;&#26435;&#37325;&#20849;&#20139;&#8221;&#30340;&#32447;&#24615;&#23618;&#12290;&#20811;&#32599;&#20869;&#20811;&#36817;&#20284;&#26354;&#29575;&#65288;K-FAC&#65289;&#26159;&#19968;&#31181;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#24050;&#26174;&#31034;&#20986;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23558;&#20854;&#24212;&#29992;&#20110;&#36890;&#29992;&#30340;&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#20855;&#26377;&#32447;&#24615;&#26435;&#37325;&#20849;&#20139;&#23618;&#30340;&#26550;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20855;&#26377;&#32447;&#24615;&#26435;&#37325;&#20849;&#20139;&#23618;&#30340;&#20004;&#31181;&#19981;&#21516;&#35774;&#32622;&#65292;&#36825;&#20419;&#20351;&#20102;&#20004;&#31181;K-FAC&#30340;&#21464;&#20307;&#8212;&#8212;&#8220;&#25193;&#23637;&#8221;&#21644;&#8220;&#20943;&#23569;&#8221;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#30456;&#24212;&#35774;&#32622;&#30340;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#65292;&#23427;&#20204;&#26159;&#31934;&#30830;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;K-FAC-reduce&#36890;&#24120;&#27604;K-FAC-expand&#26356;&#24555;&#65292;&#25105;&#20204;&#21033;&#29992;&#23427;&#26469;&#21152;&#36895;&#36890;&#36807;&#20248;&#21270;Wide ResNet&#30340;&#36793;&#38469;&#20284;&#28982;&#26469;&#36873;&#25321;&#33258;&#21160;&#36229;&#21442;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;
&lt;/p&gt;
&lt;p&gt;
The core components of many modern neural network architectures, such as transformers, convolutional, or graph neural networks, can be expressed as linear layers with $\textit{weight-sharing}$. Kronecker-Factored Approximate Curvature (K-FAC), a second-order optimisation method, has shown promise to speed up neural network training and thereby reduce computational costs. However, there is currently no framework to apply it to generic architectures, specifically ones with linear weight-sharing layers. In this work, we identify two different settings of linear weight-sharing layers which motivate two flavours of K-FAC -- $\textit{expand}$ and $\textit{reduce}$. We show that they are exact for deep linear networks with weight-sharing in their respective setting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we leverage to speed up automatic hyperparameter selection via optimising the marginal likelihood for a Wide ResNet. Finally, we observe little difference between 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#26174;&#31034;&#20986;&#23545;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00619</link><description>&lt;p&gt;
&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#25439;&#22833;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Loss Modeling for Multi-Annotator Datasets. (arXiv:2311.00619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00619
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#22810;&#27880;&#37322;&#32773;&#25968;&#25454;&#30340;&#20934;&#30830;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36824;&#26174;&#31034;&#20986;&#23545;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#27491;&#24615;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#27880;&#37322;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;&#20010;&#21035;&#27880;&#37322;&#32773;&#32463;&#24120;&#20250;&#25552;&#20379;&#25968;&#21315;&#20010;&#35780;&#20998;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#30130;&#21171;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27880;&#37322;&#36807;&#31243;&#21487;&#33021;&#20250;&#25345;&#32493;&#22810;&#22825;&#65292;&#21487;&#33021;&#23548;&#33268;&#23545;&#27880;&#37322;&#32773;&#30340;&#24847;&#35265;&#38543;&#26102;&#38388;&#30340;&#19981;&#20934;&#30830;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#25439;&#22833;&#30340;&#26631;&#31614;&#20462;&#27491;&#26469;&#23398;&#20064;&#26356;&#20934;&#30830;&#30340;&#22810;&#26679;&#24847;&#35265;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26032;&#39062;&#30340;&#20844;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#28165;&#26970;&#22320;&#20998;&#31163;&#36190;&#21516;&#21644;&#19981;&#36190;&#21516;&#30340;&#27880;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20462;&#25913;&#21487;&#20197;&#25913;&#21892;&#21333;&#19968;&#25110;&#22810;&#27880;&#37322;&#32773;&#35774;&#32622;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#24212;&#29992;&#20110;&#20027;&#35266;&#25968;&#25454;&#30340;&#39069;&#22806;&#26631;&#31614;&#22122;&#22768;&#20173;&#28982;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accounting for the opinions of all annotators of a dataset is critical for fairness. However, when annotating large datasets, individual annotators will frequently provide thousands of ratings which can lead to fatigue. Additionally, these annotation processes can occur over multiple days which can lead to an inaccurate representation of an annotator's opinion over time. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, we demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#23548;&#21521;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21487;&#25511;&#38899;&#20048;&#21046;&#20316;&#65292;&#21253;&#25324;&#38899;&#39057;&#30340;&#24310;&#32493;&#12289;&#20462;&#34917;&#21644;&#20877;&#29983;&#65292;&#20197;&#21450;&#39118;&#26684;&#29305;&#24449;&#36716;&#31227;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2311.00613</link><description>&lt;p&gt;
&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#23548;&#21521;&#26799;&#24230;&#23454;&#29616;&#21487;&#25511;&#38899;&#20048;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Controllable Music Production with Diffusion Models and Guidance Gradients. (arXiv:2311.00613v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#23548;&#21521;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#21487;&#25511;&#38899;&#20048;&#21046;&#20316;&#65292;&#21253;&#25324;&#38899;&#39057;&#30340;&#24310;&#32493;&#12289;&#20462;&#34917;&#21644;&#20877;&#29983;&#65292;&#20197;&#21450;&#39118;&#26684;&#29305;&#24449;&#36716;&#31227;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26465;&#20214;&#29983;&#25104;&#26469;&#22788;&#29702;&#38899;&#20048;&#21046;&#20316;&#20013;&#30340;&#21508;&#31181;&#29616;&#23454;&#20219;&#21153;&#65292;&#21253;&#25324;&#38899;&#20048;&#38899;&#39057;&#30340;&#24310;&#32493;&#12289;&#20462;&#34917;&#21644;&#20877;&#29983;&#12289;&#22312;&#20004;&#20010;&#19981;&#21516;&#38899;&#20048;&#26354;&#30446;&#20043;&#38388;&#21019;&#24314;&#24179;&#28369;&#30340;&#36807;&#28193;&#20197;&#21450;&#23558;&#25152;&#38656;&#30340;&#39118;&#26684;&#29305;&#24449;&#36716;&#31227;&#21040;&#29616;&#26377;&#38899;&#39057;&#29255;&#27573;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#37319;&#26679;&#26102;&#24212;&#29992;&#23548;&#21521;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#20013;&#25903;&#25345;&#37325;&#24314;&#21644;&#20998;&#31867;&#25439;&#22833;&#65292;&#25110;&#32773;&#20004;&#32773;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#38899;&#39057;&#21487;&#20197;&#21305;&#37197;&#20854;&#21608;&#22260;&#30340;&#19978;&#19979;&#25991;&#65292;&#25110;&#32773;&#31526;&#21512;&#30456;&#23545;&#20110;&#20219;&#20309;&#36866;&#24403;&#30340;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#25110;&#23884;&#20837;&#27169;&#22411;&#25351;&#23450;&#30340;&#31867;&#20998;&#24067;&#25110;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how conditional generation from diffusion models can be used to tackle a variety of realistic tasks in the production of music in 44.1kHz stereo audio with sampling-time guidance. The scenarios we consider include continuation, inpainting and regeneration of musical audio, the creation of smooth transitions between two different music tracks, and the transfer of desired stylistic characteristics to existing audio clips. We achieve this by applying guidance at sampling time in a simple framework that supports both reconstruction and classification losses, or any combination of the two. This approach ensures that generated audio can match its surrounding context, or conform to a class distribution or latent representation specified relative to any suitable pre-trained classifier or embedding model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#21644;&#35838;&#31243;&#20381;&#36182;&#24615;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#30340;&#35838;&#31243;&#25512;&#33616;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#35780;&#20998;&#21644;&#20803;&#25968;&#25454;&#12289;&#35838;&#31243;&#27880;&#20876;&#20998;&#24067;&#19981;&#22343;&#34913;&#20197;&#21450;&#35838;&#31243;&#20381;&#36182;&#24314;&#27169;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;0.97&#30340;AUC&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2311.00612</link><description>&lt;p&gt;
&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#21644;&#35838;&#31243;&#20381;&#36182;&#24615;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#30340;&#35838;&#31243;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
A Collaborative Filtering-Based Two Stage Model with Item Dependency for Course Recommendation. (arXiv:2311.00612v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36807;&#28388;&#21644;&#35838;&#31243;&#20381;&#36182;&#24615;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#30340;&#35838;&#31243;&#25512;&#33616;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#35780;&#20998;&#21644;&#20803;&#25968;&#25454;&#12289;&#35838;&#31243;&#27880;&#20876;&#20998;&#24067;&#19981;&#22343;&#34913;&#20197;&#21450;&#35838;&#31243;&#20381;&#36182;&#24314;&#27169;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;0.97&#30340;AUC&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#21069;&#26223;&#30340;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#27169;&#22411;&#30001;&#20110;&#22312;&#25512;&#33616;&#20013;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#24182;&#28040;&#38500;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#34987;&#35748;&#20026;&#26159;&#26368;&#25104;&#21151;&#30340;&#19968;&#31181;&#12290;&#26412;&#25991;&#23558;CF&#27169;&#22411;&#25193;&#23637;&#21040;&#35838;&#31243;&#25512;&#33616;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#23558;&#29616;&#26377;&#30340;CF&#27169;&#22411;&#24212;&#29992;&#20110;&#26500;&#24314;&#35838;&#31243;&#25512;&#33616;&#24341;&#25806;&#26102;&#38754;&#20020;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#20047;&#35780;&#20998;&#21644;&#20803;&#25968;&#25454;&#65292;&#35838;&#31243;&#27880;&#20876;&#20998;&#24067;&#19981;&#22343;&#34913;&#20197;&#21450;&#35838;&#31243;&#20381;&#36182;&#24314;&#27169;&#30340;&#38656;&#27714;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#24819;&#27861;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35838;&#31243;&#20381;&#36182;&#24615;&#27491;&#21017;&#21270;&#30340;&#20004;&#38454;&#27573;CF&#27169;&#22411;&#19982;&#22522;&#20110;&#35838;&#31243;&#36716;&#25442;&#32593;&#32476;&#30340;&#22270;&#24418;&#25512;&#33616;&#22120;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;0.97&#30340;AUC&#24471;&#20998;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have been studied for decades with numerous promising models been proposed. Among them, Collaborative Filtering (CF) models are arguably the most successful one due to its high accuracy in recommendation and elimination of privacy-concerned personal meta-data from training. This paper extends the usage of CF-based model to the task of course recommendation. We point out several challenges in applying the existing CF-models to build a course recommendation engine, including the lack of rating and meta-data, the imbalance of course registration distribution, and the demand of course dependency modeling. We then propose several ideas to address these challenges. Eventually, we combine a two-stage CF model regularized by course dependency with a graph-based recommender based on course-transition network, to achieve AUC as high as 0.97 with a real-world dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MCMC&#37319;&#26679;&#22120;PARNI-DAG&#65292;&#29992;&#20110;&#20840;&#36125;&#21494;&#26031;&#26041;&#27861;&#19979;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#12290;PARNI-DAG&#36890;&#36807;&#23616;&#37096;&#20449;&#24687;&#21270;&#30340;&#33258;&#36866;&#24212;&#38543;&#26426;&#37051;&#22495;&#25552;&#35758;&#36827;&#34892;&#39640;&#25928;&#30340;DAG&#37319;&#26679;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#36136;&#12290;&#20026;&#20102;&#19982;&#33410;&#28857;&#25968;&#37327;&#26356;&#22909;&#22320;&#25193;&#23637;&#65292;PARNI-DAG&#19982;&#36890;&#36807;&#19968;&#20123;&#22522;&#20110;&#32422;&#26463;&#25110;&#35780;&#20998;&#31639;&#27861;&#24471;&#21040;&#30340;&#39592;&#26550;&#22270;&#30340;&#21442;&#25968;&#39044;&#35843;&#25972;&#36807;&#31243;&#30456;&#32467;&#21512;&#12290;&#24471;&#30410;&#20110;&#36825;&#20123;&#26032;&#39062;&#30340;&#29305;&#24615;&#65292;PARNI-DAG&#24555;&#36895;&#25910;&#25947;&#21040;&#39640;&#27010;&#29575;&#21306;&#22495;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#35774;&#32622;&#20013;&#30340;&#33410;&#28857;&#20043;&#38388;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#26102;&#19981;&#22826;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2311.00599</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#38543;&#26426;&#37051;&#22495;&#30340;&#32467;&#26500;&#23398;&#20064;MCMC&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Structure Learning with Adaptive Random Neighborhood Informed MCMC. (arXiv:2311.00599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MCMC&#37319;&#26679;&#22120;PARNI-DAG&#65292;&#29992;&#20110;&#20840;&#36125;&#21494;&#26031;&#26041;&#27861;&#19979;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#12290;PARNI-DAG&#36890;&#36807;&#23616;&#37096;&#20449;&#24687;&#21270;&#30340;&#33258;&#36866;&#24212;&#38543;&#26426;&#37051;&#22495;&#25552;&#35758;&#36827;&#34892;&#39640;&#25928;&#30340;DAG&#37319;&#26679;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#36136;&#12290;&#20026;&#20102;&#19982;&#33410;&#28857;&#25968;&#37327;&#26356;&#22909;&#22320;&#25193;&#23637;&#65292;PARNI-DAG&#19982;&#36890;&#36807;&#19968;&#20123;&#22522;&#20110;&#32422;&#26463;&#25110;&#35780;&#20998;&#31639;&#27861;&#24471;&#21040;&#30340;&#39592;&#26550;&#22270;&#30340;&#21442;&#25968;&#39044;&#35843;&#25972;&#36807;&#31243;&#30456;&#32467;&#21512;&#12290;&#24471;&#30410;&#20110;&#36825;&#20123;&#26032;&#39062;&#30340;&#29305;&#24615;&#65292;PARNI-DAG&#24555;&#36895;&#25910;&#25947;&#21040;&#39640;&#27010;&#29575;&#21306;&#22495;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#35774;&#32622;&#20013;&#30340;&#33410;&#28857;&#20043;&#38388;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#26102;&#19981;&#22826;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MCMC&#37319;&#26679;&#22120;PARNI-DAG&#65292;&#29992;&#20110;&#20840;&#36125;&#21494;&#26031;&#26041;&#27861;&#19979;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#22240;&#26524;&#20805;&#20998;&#24615;&#20551;&#35774;&#19979;&#65292;&#35813;&#31639;&#27861;&#20801;&#35768;&#20174;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#36817;&#20284;&#37319;&#26679;&#12290;PARNI-DAG&#36890;&#36807;&#23616;&#37096;&#20449;&#24687;&#21270;&#30340;&#33258;&#36866;&#24212;&#38543;&#26426;&#37051;&#22495;&#25552;&#35758;&#36827;&#34892;&#39640;&#25928;&#30340;DAG&#37319;&#26679;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#28151;&#21512;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#19982;&#33410;&#28857;&#25968;&#37327;&#26356;&#22909;&#22320;&#25193;&#23637;&#65292;&#25105;&#20204;&#23558;PARNI-DAG&#19982;&#36890;&#36807;&#19968;&#20123;&#22522;&#20110;&#32422;&#26463;&#25110;&#35780;&#20998;&#31639;&#27861;&#24471;&#21040;&#30340;&#39592;&#26550;&#22270;&#30340;&#21442;&#25968;&#39044;&#35843;&#25972;&#36807;&#31243;&#30456;&#32467;&#21512;&#12290;&#24471;&#30410;&#20110;&#36825;&#20123;&#26032;&#39062;&#30340;&#29305;&#24615;&#65292;PARNI-DAG&#24555;&#36895;&#25910;&#25947;&#21040;&#39640;&#27010;&#29575;&#21306;&#22495;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#35774;&#32622;&#20013;&#30340;&#33410;&#28857;&#20043;&#38388;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#26102;&#19981;&#22826;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel MCMC sampler, PARNI-DAG, for a fully-Bayesian approach to the problem of structure learning under observational data. Under the assumption of causal sufficiency, the algorithm allows for approximate sampling directly from the posterior distribution on Directed Acyclic Graphs (DAGs). PARNI-DAG performs efficient sampling of DAGs via locally informed, adaptive random neighborhood proposal that results in better mixing properties. In addition, to ensure better scalability with the number of nodes, we couple PARNI-DAG with a pre-tuning procedure of the sampler's parameters that exploits a skeleton graph derived through some constraint-based or scoring-based algorithms. Thanks to these novel features, PARNI-DAG quickly converges to high-probability regions and is less likely to get stuck in local modes in the presence of high correlation between nodes in high-dimensional settings. After introducing the technical novelties in PARNI-DAG, we empirically demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;SDVI&#65292;&#21487;&#20197;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#25903;&#25345;&#30340;&#27010;&#29575;&#32534;&#31243;&#12290;&#36890;&#36807;&#23558;&#31243;&#24207;&#20998;&#35299;&#20026;&#20855;&#26377;&#38745;&#24577;&#25903;&#25345;&#30340;&#23376;&#31243;&#24207;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#31243;&#24207;&#26500;&#24314;&#29420;&#31435;&#30340;&#21464;&#20998;&#25351;&#23548;&#65292;SDVI&#22312;&#25512;&#26029;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.00594</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20855;&#26377;&#38543;&#26426;&#25903;&#25345;&#30340;&#27010;&#29575;&#32534;&#31243;&#30340;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Rethinking Variational Inference for Probabilistic Programs with Stochastic Support. (arXiv:2311.00594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;SDVI&#65292;&#21487;&#20197;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#25903;&#25345;&#30340;&#27010;&#29575;&#32534;&#31243;&#12290;&#36890;&#36807;&#23558;&#31243;&#24207;&#20998;&#35299;&#20026;&#20855;&#26377;&#38745;&#24577;&#25903;&#25345;&#30340;&#23376;&#31243;&#24207;&#65292;&#24182;&#20026;&#27599;&#20010;&#23376;&#31243;&#24207;&#26500;&#24314;&#29420;&#31435;&#30340;&#21464;&#20998;&#25351;&#23548;&#65292;SDVI&#22312;&#25512;&#26029;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25903;&#25345;&#20998;&#35299;&#21464;&#20998;&#25512;&#26029;&#65288;SDVI&#65289;&#65292;&#19968;&#31181;&#38754;&#21521;&#20855;&#26377;&#38543;&#26426;&#25903;&#25345;&#30340;&#27010;&#29575;&#32534;&#31243;&#30340;&#26032;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#20381;&#36182;&#20110;&#22312;&#36880;&#21464;&#37327;&#30340;&#22522;&#30784;&#19978;&#35774;&#35745;&#21333;&#20010;&#20840;&#23616;&#21464;&#20998;&#25351;&#23548;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#31243;&#24207;&#30340;&#38543;&#26426;&#25511;&#21046;&#27969;&#12290;SDVI&#30456;&#21453;&#65292;&#23558;&#31243;&#24207;&#20998;&#35299;&#20026;&#20855;&#26377;&#38745;&#24577;&#25903;&#25345;&#30340;&#23376;&#31243;&#24207;&#65292;&#28982;&#21518;&#33258;&#21160;&#26500;&#24314;&#27599;&#20010;&#23376;&#25351;&#23548;&#30340;&#29420;&#31435;&#23376;&#25351;&#23548;&#12290;&#36825;&#31181;&#20998;&#35299;&#26174;&#33879;&#26377;&#21161;&#20110;&#26500;&#24314;&#36866;&#21512;&#30340;&#21464;&#20998;&#26063;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#25512;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Support Decomposition Variational Inference (SDVI), a new variational inference (VI) approach for probabilistic programs with stochastic support. Existing approaches to this problem rely on designing a single global variational guide on a variable-by-variable basis, while maintaining the stochastic control flow of the original program. SDVI instead breaks the program down into sub-programs with static support, before automatically building separate sub-guides for each. This decomposition significantly aids in the construction of suitable variational families, enabling, in turn, substantial improvements in inference performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Coop&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20869;&#23384;&#31995;&#32479;&#38382;&#39064;&#65292;&#36890;&#36807;&#39537;&#36880;&#36830;&#32493;&#24352;&#37327;&#21644;&#20248;&#21270;&#24352;&#37327;&#20998;&#37197;&#65292;&#20197;&#38477;&#20302;&#20877;&#26448;&#26009;&#21270;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.00591</link><description>&lt;p&gt;
Coop: &#20869;&#23384;&#19981;&#26159;&#21830;&#21697;
&lt;/p&gt;
&lt;p&gt;
Coop: Memory is not a Commodity. (arXiv:2311.00591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Coop&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20869;&#23384;&#31995;&#32479;&#38382;&#39064;&#65292;&#36890;&#36807;&#39537;&#36880;&#36830;&#32493;&#24352;&#37327;&#21644;&#20248;&#21270;&#24352;&#37327;&#20998;&#37197;&#65292;&#20197;&#38477;&#20302;&#20877;&#26448;&#26009;&#21270;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#20877;&#26448;&#26009;&#21270;&#25216;&#26415;&#20801;&#35768;&#22312;&#26377;&#38480;&#30340;&#20869;&#23384;&#39044;&#31639;&#19979;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#36890;&#36807;&#22312;&#38656;&#35201;&#26102;&#26816;&#26597;&#28857;&#27169;&#22411;&#24182;&#26681;&#25454;&#38656;&#35201;&#37325;&#26032;&#35745;&#31639;&#34987;&#39537;&#36880;&#30340;&#24352;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24352;&#37327;&#20877;&#26448;&#26009;&#21270;&#25216;&#26415;&#24573;&#35270;&#20102;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#20869;&#23384;&#31995;&#32479;&#65292;&#24182;&#20551;&#35774;&#19981;&#21516;&#22320;&#22336;&#30340;&#31354;&#38386;&#20869;&#23384;&#22359;&#26159;&#30456;&#21516;&#30340;&#12290;&#22312;&#36825;&#20010;&#38169;&#35823;&#30340;&#20551;&#35774;&#19979;&#65292;&#19981;&#36830;&#32493;&#30340;&#24352;&#37327;&#34987;&#39537;&#36880;&#65292;&#20854;&#20013;&#19968;&#20123;&#19981;&#29992;&#20110;&#20998;&#37197;&#26032;&#30340;&#24352;&#37327;&#12290;&#36825;&#23548;&#33268;&#20005;&#37325;&#30340;&#20869;&#23384;&#30862;&#29255;&#21270;&#65292;&#22686;&#21152;&#20102;&#28508;&#22312;&#20877;&#26448;&#26009;&#21270;&#30340;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#28369;&#21160;&#31383;&#21475;&#20869;&#39537;&#36880;&#24352;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#25152;&#26377;&#39537;&#36880;&#37117;&#26159;&#36830;&#32493;&#30340;&#24182;&#19988;&#31435;&#21363;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24265;&#20215;&#30340;&#24352;&#37327;&#20998;&#21306;&#21644;&#21487;&#37325;&#31639;&#30340;&#23601;&#22320;&#20248;&#21270;&#24352;&#37327;&#20998;&#37197;&#26469;&#36827;&#19968;&#27493;&#38477;&#20302;&#20877;&#26448;&#26009;&#21270;&#25104;&#26412;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;Coop&#65292;&#22240;&#20026;&#23427;&#26159;&#24352;&#37327;&#20998;&#37197;&#21644;&#24352;&#37327;&#20877;&#26448;&#26009;&#21270;&#30340;&#21327;&#21516;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor rematerialization allows the training of deep neural networks (DNNs) under limited memory budgets by checkpointing the models and recomputing the evicted tensors as needed. However, the existing tensor rematerialization techniques overlook the memory system in deep learning frameworks and implicitly assume that free memory blocks at different addresses are identical. Under this flawed assumption, discontiguous tensors are evicted, among which some are not used to allocate the new tensor. This leads to severe memory fragmentation and increases the cost of potential rematerializations. To address this issue, we propose to evict tensors within a sliding window to ensure all evictions are contiguous and are immediately used. Furthermore, we proposed cheap tensor partitioning and recomputable in-place to further reduce the rematerialization cost by optimizing the tensor allocation. We named our method Coop as it is a co-optimization of tensor allocation and tensor rematerialization. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#23614;&#37096;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;&#26469;&#36817;&#20284;&#37329;&#34701;&#22238;&#25253;&#30340;&#37325;&#23614;&#20998;&#24067;&#65292;&#33021;&#22815;&#25429;&#25417;&#21487;&#33021;&#20986;&#29616;&#22312;&#25968;&#25454;&#20013;&#30340;&#26497;&#31471;&#20914;&#20987;&#12290;</title><link>http://arxiv.org/abs/2311.00580</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#23614;&#37096;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;&#65292;&#24212;&#29992;&#20110;&#37329;&#34701;&#22238;&#25253;&#25968;&#25454;&#30340;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Flexible Tails for Normalising Flows, with Application to the Modelling of Financial Return Data. (arXiv:2311.00580v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#23614;&#37096;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;&#26469;&#36817;&#20284;&#37329;&#34701;&#22238;&#25253;&#30340;&#37325;&#23614;&#20998;&#24067;&#65292;&#33021;&#22815;&#25429;&#25417;&#21487;&#33021;&#20986;&#29616;&#22312;&#25968;&#25454;&#20013;&#30340;&#26497;&#31471;&#20914;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25913;&#21464;&#20998;&#24067;&#23614;&#37096;&#29305;&#24615;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#21463;&#21040;&#26497;&#20540;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#20316;&#20026;&#24402;&#19968;&#21270;&#27969;&#20013;&#30340;&#19968;&#23618;&#65292;&#26469;&#36817;&#20284;&#22810;&#21464;&#37327;&#37325;&#23614;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#37329;&#34701;&#22238;&#25253;&#24314;&#27169;&#65292;&#25429;&#25417;&#21487;&#33021;&#20986;&#29616;&#22312;&#27492;&#31867;&#25968;&#25454;&#20013;&#30340;&#26497;&#31471;&#20914;&#20987;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#29983;&#25104;&#21487;&#33021;&#30340;&#26497;&#31471;&#22238;&#25253;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a transformation capable of altering the tail properties of a distribution, motivated by extreme value theory, which can be used as a layer in a normalizing flow to approximate multivariate heavy tailed distributions. We apply this approach to model financial returns, capturing potentially extreme shocks that arise in such data. The trained models can be used directly to generate new synthetic sets of potentially extreme returns
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#25968;&#25454;&#27969;&#21152;&#36895;&#22120;&#19978;&#30340;&#20391;&#20449;&#36947;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#26469;&#24674;&#22797;CNN&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#35813;&#25915;&#20987;&#21033;&#29992;&#20102;&#25968;&#25454;&#27969;&#26144;&#23556;&#30340;&#25968;&#25454;&#37325;&#29992;&#20197;&#21450;&#26550;&#26500;&#32447;&#32034;&#65292;&#25104;&#21151;&#24674;&#22797;&#20102;&#27969;&#34892;&#30340;CNN&#27169;&#22411;Lenet&#65292;Alexnet&#21644;VGGnet16&#30340;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2311.00579</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27969;&#25512;&#29702;&#21152;&#36895;&#22120;&#20013;&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#25581;&#31034;CNN&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Revealing CNN Architectures via Side-Channel Analysis in Dataflow-based Inference Accelerators. (arXiv:2311.00579v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#25968;&#25454;&#27969;&#21152;&#36895;&#22120;&#19978;&#30340;&#20391;&#20449;&#36947;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;&#26469;&#24674;&#22797;CNN&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#35813;&#25915;&#20987;&#21033;&#29992;&#20102;&#25968;&#25454;&#27969;&#26144;&#23556;&#30340;&#25968;&#25454;&#37325;&#29992;&#20197;&#21450;&#26550;&#26500;&#32447;&#32034;&#65292;&#25104;&#21151;&#24674;&#22797;&#20102;&#27969;&#34892;&#30340;CNN&#27169;&#22411;Lenet&#65292;Alexnet&#21644;VGGnet16&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#26368;&#36817;&#22312;&#22522;&#20110;&#25968;&#25454;&#27969;&#30340;CNN&#21152;&#36895;&#22120;&#30340;&#36827;&#23637;&#20351;&#24471;CNN&#25512;&#29702;&#21487;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#12290;&#36825;&#20123;&#25968;&#25454;&#27969;&#21152;&#36895;&#22120;&#21033;&#29992;&#21367;&#31215;&#23618;&#30340;&#22266;&#26377;&#25968;&#25454;&#37325;&#29992;&#26469;&#39640;&#25928;&#22788;&#29702;CNN&#27169;&#22411;&#12290;&#38544;&#34255;CNN&#27169;&#22411;&#30340;&#26550;&#26500;&#23545;&#20110;&#38544;&#31169;&#21644;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#20869;&#23384;&#30340;&#20391;&#20449;&#36947;&#20449;&#24687;&#65292;&#20197;&#20174;&#25968;&#25454;&#27969;&#21152;&#36895;&#22120;&#20013;&#24674;&#22797;CNN&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#21033;&#29992;&#20102;CNN&#21152;&#36895;&#22120;&#19978;&#25968;&#25454;&#27969;&#26144;&#23556;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#25968;&#25454;&#37325;&#29992;&#20197;&#21450;&#26550;&#26500;&#32447;&#32034;&#26469;&#24674;&#22797;CNN&#27169;&#22411;&#30340;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#21487;&#20197;&#24674;&#22797;&#27969;&#34892;&#30340;CNN&#27169;&#22411;Lenet&#65292;Alexnet&#21644;VGGnet16&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolution Neural Networks (CNNs) are widely used in various domains. Recent advances in dataflow-based CNN accelerators have enabled CNN inference in resource-constrained edge devices. These dataflow accelerators utilize inherent data reuse of convolution layers to process CNN models efficiently. Concealing the architecture of CNN models is critical for privacy and security. This paper evaluates memory-based side-channel information to recover CNN architectures from dataflow-based CNN inference accelerators. The proposed attack exploits spatial and temporal data reuse of the dataflow mapping on CNN accelerators and architectural hints to recover the structure of CNN models. Experimental results demonstrate that our proposed side-channel attack can recover the structures of popular CNN models, namely Lenet, Alexnet, and VGGnet16.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#24377;&#24615;&#22522;&#30784;&#19978;&#26753;&#21160;&#21147;&#23398;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#24615;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#38382;&#39064;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00578</link><description>&lt;p&gt;
&#29992;&#20110;&#25913;&#21892;&#22240;&#26524;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#26753;&#27169;&#25311;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer learning for improved generalizability in causal physics-informed neural networks for beam simulations. (arXiv:2311.00578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#24377;&#24615;&#22522;&#30784;&#19978;&#26753;&#21160;&#21147;&#23398;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#24615;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#38382;&#39064;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#24377;&#24615;&#22522;&#30784;&#19978;&#26753;&#21160;&#21147;&#23398;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22240;&#26524;&#24615;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26694;&#26550;&#20869;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#27169;&#25311;&#20102;Winkler&#22522;&#30784;&#19978;&#30340;Euler-Bernoulli&#26753;&#27169;&#22411;&#21644;Timoshenko&#26753;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;PINNS&#22312;&#22788;&#29702;&#22823;&#31354;&#26102;&#26102;&#38388;&#22495;&#19978;&#30340;&#38382;&#39064;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#65292;&#21363;&#20351;&#23545;&#20110;&#20855;&#26377;&#23553;&#38381;&#35299;&#30340;&#38382;&#39064;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#37319;&#29992;&#22240;&#26524;&#24615;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#25928;&#25429;&#25417;&#24213;&#23618;&#30340;&#29289;&#29702;&#23398;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#22240;&#26524;&#24615;PINN&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#31867;&#20284;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#36890;&#36807;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#24182;&#36981;&#24490;&#22240;&#26524;&#24615;&#26469;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#24182;&#30830;&#20445;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#33719;&#24471;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#22312;Euler-Bernoulli&#26753;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#31361;&#20986;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel methodology for simulating the dynamics of beams on elastic foundations. Specifically, Euler-Bernoulli and Timoshenko beam models on the Winkler foundation are simulated using a transfer learning approach within a causality-respecting physics-informed neural network (PINN) framework. Conventional PINNs encounter challenges in handling large space-time domains, even for problems with closed-form analytical solutions. A causality-respecting PINN loss function is employed to overcome this limitation, effectively capturing the underlying physics. However, it is observed that the causality-respecting PINN lacks generalizability. We propose using solutions to similar problems instead of training from scratch by employing transfer learning while adhering to causality to accelerate convergence and ensure accurate results across diverse scenarios. Numerical experiments on the Euler-Bernoulli beam highlight the efficacy of the proposed approach for various initial c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#20998;&#37197;&#33267;&#22810;&#20010;&#27835;&#30103;&#32452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#32858;&#31867;&#20248;&#21270;&#27835;&#30103;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#20272;&#35745;&#21644;&#20010;&#24615;&#21270;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2311.00577</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#32858;&#31867;&#32852;&#21512;&#20998;&#37197;&#26862;&#26519;&#36827;&#34892;&#20010;&#24615;&#21270;&#20998;&#37197;&#33267;&#22810;&#20010;&#27835;&#30103;&#32452;
&lt;/p&gt;
&lt;p&gt;
Personalized Assignment to One of Many Treatment Arms via Regularized and Clustered Joint Assignment Forests. (arXiv:2311.00577v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00577
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#20998;&#37197;&#33267;&#22810;&#20010;&#27835;&#30103;&#32452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#32858;&#31867;&#20248;&#21270;&#27835;&#30103;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#20272;&#35745;&#21644;&#20010;&#24615;&#21270;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#20998;&#37197;&#33267;&#22810;&#20010;&#27835;&#30103;&#32452;&#12290;&#30001;&#20110;&#36807;&#22810;&#30340;&#26041;&#24046;&#65292;&#23545;&#20110;&#27599;&#20010;&#27835;&#30103;&#32452;&#20998;&#21035;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#30340;&#26631;&#20934;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27719;&#24635;&#27835;&#30103;&#32452;&#20449;&#24687;&#30340;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#36138;&#23146;&#36882;&#24402;&#20998;&#21306;&#30340;&#27491;&#21017;&#21270;&#26862;&#26519;&#20998;&#37197;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#32553;&#23567;&#19981;&#21516;&#27835;&#30103;&#32452;&#20043;&#38388;&#30340;&#25928;&#26524;&#20272;&#35745;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#26041;&#26696;&#23558;&#27835;&#30103;&#32452;&#19982;&#20855;&#26377;&#19968;&#33268;&#30456;&#20284;&#32467;&#26524;&#30340;&#32452;&#21512;&#36215;&#26469;&#65292;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#22312;&#27169;&#25311;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#20998;&#21035;&#39044;&#27979;&#27599;&#20010;&#27835;&#30103;&#32452;&#32467;&#26524;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35760;&#24405;&#20102;&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#32858;&#31867;&#30452;&#25509;&#20248;&#21270;&#27835;&#30103;&#20998;&#37197;&#24102;&#26469;&#30340;&#25910;&#30410;&#12290;&#22312;&#19968;&#20010;&#29702;&#35770;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35828;&#26126;&#27835;&#30103;&#32452;&#25968;&#37327;&#36739;&#22810;&#26102;&#25214;&#21040;&#26368;&#20339;&#32452;&#30340;&#22256;&#38590;&#65292;&#32780;&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#32858;&#31867;&#21487;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#26126;&#26174;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider learning personalized assignments to one of many treatment arms from a randomized controlled trial. Standard methods that estimate heterogeneous treatment effects separately for each arm may perform poorly in this case due to excess variance. We instead propose methods that pool information across treatment arms: First, we consider a regularized forest-based assignment algorithm based on greedy recursive partitioning that shrinks effect estimates across arms. Second, we augment our algorithm by a clustering scheme that combines treatment arms with consistently similar outcomes. In a simulation study, we compare the performance of these approaches to predicting arm-wise outcomes separately, and document gains of directly optimizing the treatment assignment with regularization and clustering. In a theoretical model, we illustrate how a high number of treatment arms makes finding the best arm hard, while we can achieve sizable utility gains from personalization by regularized 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#20307;-&#23616;&#37096;&#23610;&#24230;&#32467;&#26500;&#30340;&#36125;&#21494;&#26031;&#23398;&#29983;-t&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#38750;&#24179;&#31283;&#25968;&#25454;&#65292;&#36890;&#36807;&#23454;&#26102;&#25509;&#25910;&#25968;&#25454;&#36827;&#34892;&#22312;&#32447;&#25512;&#26029;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00564</link><description>&lt;p&gt;
&#22312;&#24314;&#27169;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21033;&#29992;&#25972;&#20307;-&#23616;&#37096;&#23610;&#24230;&#32467;&#26500;&#30340;&#22312;&#32447;&#23398;&#29983;-t&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Online Student-$t$ Processes with an Overall-local Scale Structure for Modelling Non-stationary Data. (arXiv:2311.00564v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00564
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#20307;-&#23616;&#37096;&#23610;&#24230;&#32467;&#26500;&#30340;&#36125;&#21494;&#26031;&#23398;&#29983;-t&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#24314;&#27169;&#38750;&#24179;&#31283;&#25968;&#25454;&#65292;&#36890;&#36807;&#23454;&#26102;&#25509;&#25910;&#25968;&#25454;&#36827;&#34892;&#22312;&#32447;&#25512;&#26029;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#36890;&#24120;&#34920;&#29616;&#20986;&#38750;&#24179;&#31283;&#24615;&#21644;&#37325;&#23614;&#35823;&#24046;&#31561;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#19981;&#36866;&#21512;&#37319;&#29992;&#24120;&#35265;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20551;&#35774;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26356;&#28789;&#27963;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#23398;&#29983;-t&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#65292;&#20854;&#21327;&#26041;&#24046;&#20855;&#26377;&#25972;&#20307;-&#23616;&#37096;&#23610;&#24230;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#65288;SMC&#65289;&#37319;&#26679;&#22120;&#36827;&#34892;&#22312;&#32447;&#25512;&#26029;&#65292;&#20197;&#23454;&#26102;&#25509;&#25910;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20856;&#22411;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#27169;&#22411;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#20197;&#35777;&#26126;&#20351;&#29992;&#23398;&#29983;-t&#36807;&#31243;&#28151;&#21512;&#27169;&#22411;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-dependent data often exhibit characteristics, such as non-stationarity and heavy-tailed errors, that would be inappropriate to model with the typical assumptions used in popular models. Thus, more flexible approaches are required to be able to accommodate such issues. To this end, we propose a Bayesian mixture of student-$t$ processes with an overall-local scale structure for the covariance. Moreover, we use a sequential Monte Carlo (SMC) sampler in order to perform online inference as data arrive in real-time. We demonstrate the superiority of our proposed approach compared to typical Gaussian process-based models on real-world data sets in order to prove the necessity of using mixtures of student-$t$ processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#23398;&#20064;&#30340;&#26032;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26799;&#24230;&#23398;&#20064;&#20248;&#21270;&#65288;ML2O&#65289;&#26469;&#23454;&#29616;&#20248;&#21270;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#24403;&#21069;&#27493;&#39588;&#30340;&#20449;&#24687;&#21644;&#21382;&#21490;&#36845;&#20195;&#36712;&#36857;&#25968;&#25454;&#26469;&#33719;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#30693;&#35782;&#65292;&#21516;&#26102;&#24341;&#20837;&#23432;&#25252;&#26426;&#21046;&#65288;GML2O&#65289;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#36845;&#20195;&#24207;&#21015;&#25910;&#25947;&#21040;Pareto&#20851;&#38190;&#28857;&#12290;</title><link>http://arxiv.org/abs/2311.00559</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#22810;&#26799;&#24230;&#36827;&#34892;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning to optimize by multi-gradient for multi-objective optimization. (arXiv:2311.00559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#23398;&#20064;&#30340;&#26032;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26799;&#24230;&#23398;&#20064;&#20248;&#21270;&#65288;ML2O&#65289;&#26469;&#23454;&#29616;&#20248;&#21270;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#24403;&#21069;&#27493;&#39588;&#30340;&#20449;&#24687;&#21644;&#21382;&#21490;&#36845;&#20195;&#36712;&#36857;&#25968;&#25454;&#26469;&#33719;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#30693;&#35782;&#65292;&#21516;&#26102;&#24341;&#20837;&#23432;&#25252;&#26426;&#21046;&#65288;GML2O&#65289;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#36845;&#20195;&#24207;&#21015;&#25910;&#25947;&#21040;Pareto&#20851;&#38190;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#31185;&#23398;&#39046;&#22495;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#30740;&#31350;&#33539;&#24335;&#30340;&#20986;&#29616;&#65292;&#38656;&#35201;&#23545;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26041;&#27861;&#30340;&#35774;&#35745;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#26032;&#19968;&#20195;&#30340;MOO&#26041;&#27861;&#24212;&#35813;&#26681;&#26893;&#20110;&#33258;&#21160;&#23398;&#20064;&#32780;&#19981;&#26159;&#25163;&#21160;&#35774;&#35745;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#23398;&#20064;&#33539;&#24335;&#29992;&#20110;&#20248;&#21270;MOO&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26799;&#24230;&#23398;&#20064;&#20248;&#21270;&#65288;ML2O&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#22810;&#20010;&#26799;&#24230;&#21040;&#26356;&#26032;&#26041;&#21521;&#30340;&#26144;&#23556;&#26469;&#26356;&#26032;&#29983;&#25104;&#22120;&#12290;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;ML2O&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#27493;&#39588;&#30340;&#20449;&#24687;&#33719;&#21462;&#23616;&#37096;&#26223;&#35266;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;&#20174;&#21382;&#21490;&#36845;&#20195;&#36712;&#36857;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#20840;&#23616;&#32463;&#39564;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#23432;&#25252;&#26426;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23432;&#25252;&#24335;&#22810;&#26799;&#24230;&#23398;&#20064;&#20248;&#21270;&#65288;GML2O&#65289;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#30001;GML2O&#29983;&#25104;&#30340;&#36845;&#20195;&#24207;&#21015;&#25910;&#25947;&#21040;Pareto&#20851;&#38190;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of artificial intelligence (AI) for science has led to the emergence of learning-based research paradigms, necessitating a compelling reevaluation of the design of multi-objective optimization (MOO) methods. The new generation MOO methods should be rooted in automated learning rather than manual design. In this paper, we introduce a new automatic learning paradigm for optimizing MOO problems, and propose a multi-gradient learning to optimize (ML2O) method, which automatically learns a generator (or mappings) from multiple gradients to update directions. As a learning-based method, ML2O acquires knowledge of local landscapes by leveraging information from the current step and incorporates global experience extracted from historical iteration trajectory data. By introducing a new guarding mechanism, we propose a guarded multi-gradient learning to optimize (GML2O) method, and prove that the iterative sequence generated by GML2O converges to a Pareto critical point. The exp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#23398;&#20064;&#21464;&#36136;&#26448;&#26009;&#65292;&#23427;&#30001;&#22522;&#20110;&#26230;&#20307;&#31649;&#30340;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#38459;&#24615;&#20803;&#20214;&#26500;&#25104;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#26426;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#38750;&#32447;&#24615;&#20219;&#21153;&#65292;&#24182;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#30340;&#22810;&#20010;&#27169;&#24335;&#65292;&#20026;&#27169;&#25311;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#30828;&#20214;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2311.00537</link><description>&lt;p&gt;
&#27809;&#26377;&#22788;&#29702;&#22120;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#38750;&#32447;&#24615;&#30005;&#23376;&#21464;&#36136;&#26448;&#26009;&#20013;&#30340;&#28014;&#29616;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Without a Processor: Emergent Learning in a Nonlinear Electronic Metamaterial. (arXiv:2311.00537v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#23398;&#20064;&#21464;&#36136;&#26448;&#26009;&#65292;&#23427;&#30001;&#22522;&#20110;&#26230;&#20307;&#31649;&#30340;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#38459;&#24615;&#20803;&#20214;&#26500;&#25104;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#26426;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#38750;&#32447;&#24615;&#20219;&#21153;&#65292;&#24182;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#30340;&#22810;&#20010;&#27169;&#24335;&#65292;&#20026;&#27169;&#25311;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#30828;&#20214;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#23545;&#22823;&#35268;&#27169;&#38750;&#32447;&#24615;&#32593;&#32476;&#36827;&#34892;&#24494;&#20998;&#65292;&#36825;&#20010;&#36807;&#31243;&#32531;&#24930;&#19988;&#32791;&#33021;&#12290;&#30005;&#23376;&#23398;&#20064;&#21464;&#36136;&#26448;&#26009;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24555;&#36895;&#12289;&#39640;&#25928;&#21644;&#23481;&#38169;&#30340;&#27169;&#25311;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#65292;&#20294;&#29616;&#26377;&#23454;&#29616;&#26159;&#32447;&#24615;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#21151;&#33021;&#12290;&#36825;&#20123;&#31995;&#32479;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#26377;&#24456;&#22823;&#30340;&#21306;&#21035;&#65292;&#22240;&#27492;&#23578;&#26410;&#25506;&#32034;&#23558;&#38750;&#32447;&#24615;&#20803;&#32032;&#32435;&#20837;&#20854;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#23398;&#20064;&#21464;&#36136;&#26448;&#26009;&#8212;&#8212;&#19968;&#31181;&#22522;&#20110;&#26230;&#20307;&#31649;&#30340;&#33258;&#36866;&#24212;&#38750;&#32447;&#24615;&#38459;&#24615;&#20803;&#20214;&#30340;&#27169;&#25311;&#30005;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#27809;&#26377;&#35745;&#31639;&#26426;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#19981;&#21487;&#23454;&#29616;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#24322;&#25110;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#38750;&#32447;&#24615;&#23398;&#20064;&#21464;&#36136;&#26448;&#26009;&#25353;&#39034;&#24207;&#38477;&#20302;&#35757;&#32451;&#35823;&#24046;&#30340;&#27169;&#24335;&#65288;&#22343;&#20540;&#12289;&#26012;&#29575;&#12289;&#26354;&#29575;&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35889;&#20559;&#24046;&#12290;&#35813;&#30005;&#36335;&#23545;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard deep learning algorithms require differentiating large nonlinear networks, a process that is slow and power-hungry. Electronic learning metamaterials offer potentially fast, efficient, and fault-tolerant hardware for analog machine learning, but existing implementations are linear, severely limiting their capabilities. These systems differ significantly from artificial neural networks as well as the brain, so the feasibility and utility of incorporating nonlinear elements have not been explored. Here we introduce a nonlinear learning metamaterial -- an analog electronic network made of self-adjusting nonlinear resistive elements based on transistors. We demonstrate that the system learns tasks unachievable in linear systems, including XOR and nonlinear regression, without a computer. We find our nonlinear learning metamaterial reduces modes of training error in order (mean, slope, curvature), similar to spectral bias in artificial neural networks. The circuitry is robust to da
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20415;&#25658;&#24335;&#30340;&#20027;&#21160;&#38477;&#22122;&#35774;&#22791;&#35774;&#35745;&#65292;&#36890;&#36807;&#20256;&#24863;&#22120;&#26816;&#27979;&#29615;&#22659;&#20013;&#30340;&#22122;&#38899;&#65292;&#32463;&#36807;&#30005;&#23376;&#25511;&#21046;&#31995;&#32479;&#22788;&#29702;&#21518;&#65292;&#20351;&#29992;&#21453;&#20301;&#30456;&#39057;&#29575;&#20449;&#21495;&#26469;&#38477;&#20302;&#22122;&#38899;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20302;&#39057;&#22122;&#38899;&#20063;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2311.00535</link><description>&lt;p&gt;
&#20027;&#21160;&#38477;&#22122;&#20415;&#25658;&#24335;&#35774;&#22791;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Active Noise Control Portable Device Design. (arXiv:2311.00535v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00535
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20415;&#25658;&#24335;&#30340;&#20027;&#21160;&#38477;&#22122;&#35774;&#22791;&#35774;&#35745;&#65292;&#36890;&#36807;&#20256;&#24863;&#22120;&#26816;&#27979;&#29615;&#22659;&#20013;&#30340;&#22122;&#38899;&#65292;&#32463;&#36807;&#30005;&#23376;&#25511;&#21046;&#31995;&#32479;&#22788;&#29702;&#21518;&#65292;&#20351;&#29992;&#21453;&#20301;&#30456;&#39057;&#29575;&#20449;&#21495;&#26469;&#38477;&#20302;&#22122;&#38899;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20302;&#39057;&#22122;&#38899;&#20063;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25105;&#20204;&#30340;&#19990;&#30028;&#20805;&#28385;&#20102;&#20196;&#20154;&#24841;&#24742;&#30340;&#33258;&#28982;&#22768;&#38899;&#65292;&#20294;&#20063;&#20805;&#26021;&#30528;&#20854;&#20182;&#35753;&#20154;&#35752;&#21388;&#30340;&#22768;&#38899;&#65292;&#21363;&#22122;&#38899;&#12290;&#22122;&#38899;&#19981;&#20165;&#24433;&#21709;&#24037;&#20316;&#25928;&#29575;&#65292;&#20063;&#23545;&#20154;&#31867;&#20581;&#24247;&#26377;&#24433;&#21709;&#12290;&#20943;&#23569;&#22122;&#38899;&#30340;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#19988;&#22256;&#38590;&#12290;&#22810;&#24180;&#26469;&#65292;&#20154;&#20204;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30446;&#21069;&#38477;&#22122;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#26448;&#26009;&#21644;&#20256;&#36755;&#20171;&#36136;&#65292;&#22312;&#39640;&#39057;&#22122;&#38899;&#26041;&#38754;&#26377;&#25928;&#65292;&#20294;&#23545;&#20302;&#39057;&#22122;&#38899;&#30340;&#38477;&#22122;&#26041;&#27861;&#38750;&#24120;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38477;&#22122;&#31995;&#32479;&#65292;&#21253;&#25324;&#19968;&#20010;&#20256;&#24863;&#22120;&#26469;&#26816;&#27979;&#29615;&#22659;&#20013;&#30340;&#22122;&#38899;&#12290;&#28982;&#21518;&#23558;&#22122;&#38899;&#21457;&#36865;&#21040;&#30005;&#23376;&#25511;&#21046;&#31995;&#32479;&#36827;&#34892;&#22788;&#29702;&#65292;&#29983;&#25104;&#19968;&#20010;&#21453;&#20301;&#30456;&#39057;&#29575;&#20449;&#21495;&#26469;&#25269;&#28040;&#24178;&#25200;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#25196;&#22768;&#22120;&#24191;&#25773;&#22788;&#29702;&#21518;&#30340;&#36739;&#23567;&#22122;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
While our world is filled with its own natural sounds that we can't resist enjoying, it is also chock-full of other sounds that can be irritating, this is noise. Noise not only influences the working efficiency but also the human's health. The problem of reducing noise is one of great importance and great difficulty. The problem has been addressed in many ways over the years. The current methods for noise reducing mostly rely on the materials and transmission medium, which are only effective to some extent for the high frequency noise. However, the effective reduction noise method especially for low frequency noise is very limited.  Here we come up with a noise reduction system consist of a sensor to detect the noise in the environment. Then the noise will be sent to an electronic control system to process the noise, which will generate a reverse phase frequency signal to counteract the disturbance. Finally, the processed smaller noise will be broadcasted by the speaker. Through this s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#39034;&#24207;&#21487;&#35299;&#37322;&#31574;&#30053;&#30340;&#26080;&#20559;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#36827;&#34892;&#22870;&#21169;&#26469;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#20013;&#21487;&#33021;&#23548;&#33268;&#20559;&#21521;&#29305;&#23450;&#21160;&#20316;&#30340;&#31574;&#30053;&#30340;&#19981;&#24076;&#26395;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00523</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#39034;&#24207;&#21487;&#35299;&#37322;&#31574;&#30053;&#30340;&#26080;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning impartial policies for sequential counterfactual explanations using Deep Reinforcement Learning. (arXiv:2311.00523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#39034;&#24207;&#21487;&#35299;&#37322;&#31574;&#30053;&#30340;&#26080;&#20559;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#36827;&#34892;&#22870;&#21169;&#26469;&#20943;&#36731;&#29616;&#26377;&#26041;&#27861;&#20013;&#21487;&#33021;&#23548;&#33268;&#20559;&#21521;&#29305;&#23450;&#21160;&#20316;&#30340;&#31574;&#30053;&#30340;&#19981;&#24076;&#26395;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#39034;&#24207;&#21487;&#35299;&#37322;&#65288;SCF&#65289;&#31034;&#20363;&#36890;&#36807;&#23545;&#36755;&#20837;&#23454;&#20363;&#36827;&#34892;&#19968;&#31995;&#21015;&#20462;&#25913;&#26469;&#25913;&#21464;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#34429;&#28982;&#26576;&#20123;&#27979;&#35797;&#26102;&#31639;&#27861;&#26088;&#22312;&#38024;&#23545;&#27599;&#20010;&#26032;&#23454;&#20363;&#36827;&#34892;&#20248;&#21270;&#65292;&#20294;&#26368;&#36817;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#29992;&#20110;&#21457;&#29616;SCF&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;&#12290;&#22312;RL&#20013;&#65292;RL&#38382;&#39064;&#30340;&#21046;&#23450;&#65292;&#21253;&#25324;&#29366;&#24577;&#31354;&#38388;&#65292;&#21160;&#20316;&#21644;&#22870;&#21169;&#30340;&#35268;&#23450;&#65292;&#36890;&#24120;&#23384;&#22312;&#27495;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#21487;&#33021;&#23548;&#33268;&#20855;&#26377;&#19981;&#24076;&#26395;&#30340;&#23646;&#24615;&#65288;&#22914;&#20559;&#21521;&#29305;&#23450;&#21160;&#20316;&#65289;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#27010;&#29575;&#26469;&#21019;&#24314;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#22870;&#21169;&#65292;&#20197;&#20943;&#36731;&#36825;&#31181;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of explainable Artificial Intelligence (XAI), sequential counterfactual (SCF) examples are often used to alter the decision of a trained classifier by implementing a sequence of modifications to the input instance. Although certain test-time algorithms aim to optimize for each new instance individually, recently Reinforcement Learning (RL) methods have been proposed that seek to learn policies for discovering SCFs, thereby enhancing scalability. As is typical in RL, the formulation of the RL problem, including the specification of state space, actions, and rewards, can often be ambiguous. In this work, we identify shortcomings in existing methods that can result in policies with undesired properties, such as a bias towards specific actions. We propose to use the output probabilities of the classifier to create a more informative reward, to mitigate this effect.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;REBAR&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#20449;&#24687;&#21644;&#37325;&#24314;&#23376;&#24207;&#21015;&#26469;&#26500;&#24314;&#27491;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;REBAR&#35823;&#24046;&#21487;&#20197;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#65292;&#24182;&#19988;&#22312;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#38598;&#25104;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#26377;&#29992;&#20449;&#24687;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2311.00519</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Based Reconstruction For Time-series Contrastive Learning. (arXiv:2311.00519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;REBAR&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#20449;&#24687;&#21644;&#37325;&#24314;&#23376;&#24207;&#21015;&#26469;&#26500;&#24314;&#27491;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;REBAR&#35823;&#24046;&#21487;&#20197;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#65292;&#24182;&#19988;&#22312;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#38598;&#25104;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#26377;&#29992;&#20449;&#24687;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#37492;&#21035;&#20986;&#30340;&#27491;&#26679;&#26412;&#23545;&#65292;&#24403;&#23427;&#20204;&#34987;&#25512;&#21040;&#23884;&#20837;&#31354;&#38388;&#26102;&#65292;&#21487;&#20197;&#20026;&#21518;&#32493;&#30340;&#19979;&#28216;&#20219;&#21153;&#32534;&#30721;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#21487;&#33021;&#20250;&#30772;&#22351;&#21407;&#22987;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#25105;&#20204;&#20551;&#35774;&#22914;&#26524;&#25105;&#20204;&#33021;&#20174;&#19968;&#20010;&#23376;&#24207;&#21015;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#25104;&#21151;&#37325;&#24314;&#21478;&#19968;&#20010;&#23376;&#24207;&#21015;&#65292;&#37027;&#20040;&#23427;&#20204;&#24212;&#35813;&#26159;&#19968;&#20010;&#27491;&#26679;&#26412;&#23545;&#12290;&#22522;&#20110;&#36825;&#20010;&#30452;&#35273;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;REBAR&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21367;&#31215;&#20132;&#21449;&#27880;&#24847;&#21147;&#26550;&#26500;&#35745;&#31639;&#20004;&#20010;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;REBAR&#35823;&#24046;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;REBAR&#35823;&#24046;&#26159;&#20114;&#30456;&#31867;&#21035;&#25104;&#21592;&#30340;&#39044;&#27979;&#22120;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#23427;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#30340;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#19968;&#26086;&#38598;&#25104;&#21040;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#30340;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
The success of self-supervised contrastive learning hinges on identifying positive data pairs that, when pushed together in embedding space, encode useful information for subsequent downstream tasks. However, in time-series, this is challenging because creating positive pairs via augmentations may break the original semantic meaning. We hypothesize that if we can retrieve information from one subsequence to successfully reconstruct another subsequence, then they should form a positive pair. Harnessing this intuition, we introduce our novel approach: REtrieval-BAsed Reconstruction (REBAR) contrastive learning. First, we utilize a convolutional cross-attention architecture to calculate the REBAR error between two different time-series. Then, through validation experiments, we show that the REBAR error is a predictor of mutual class membership, justifying its usage as a positive/negative labeler. Finally, once integrated into a contrastive learning framework, our REBAR method can learn an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.00502</link><description>&lt;p&gt;
&#22312;CPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Efficient LLM Inference on CPUs. (arXiv:2311.00502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CPU&#19978;&#39640;&#25928;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#25903;&#25345;&#33258;&#21160;&#26435;&#37325;&#37327;&#21270;&#21644;&#20248;&#21270;&#20869;&#26680;&#65292;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#23637;&#31034;&#20102;&#26497;&#39640;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#21644;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#24222;&#22823;&#25968;&#37327;&#65292;LLMs&#30340;&#37096;&#32626;&#19968;&#30452;&#38754;&#20020;&#25361;&#25112;&#65292;&#23545;&#22823;&#20869;&#23384;&#23481;&#37327;&#21644;&#39640;&#20869;&#23384;&#24102;&#23485;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;LLMs&#30340;&#37096;&#32626;&#26356;&#39640;&#25928;&#12290;&#25105;&#20204;&#25903;&#25345;&#33258;&#21160;&#30340;INT4&#26435;&#37325;&#37327;&#21270;&#27969;&#31243;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#27530;&#30340;LLM&#36816;&#34892;&#26102;&#65292;&#20855;&#26377;&#39640;&#24230;&#20248;&#21270;&#30340;&#20869;&#26680;&#65292;&#20197;&#21152;&#36895;&#22312;CPU&#19978;&#30340;LLM&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;LLMs&#19978;&#30340;&#26222;&#36866;&#24615;&#65292;&#21253;&#25324;Llama2&#65292;Llama&#65292;GPT-NeoX&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;CPU&#19978;&#30340;&#26497;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110;: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable performance and tremendous potential across a wide range of tasks. However, deploying these models has been challenging due to the astronomical amount of model parameters, which requires a demand for large memory capacity and high memory bandwidth. In this paper, we propose an effective approach that can make the deployment of LLMs more efficiently. We support an automatic INT4 weight-only quantization flow and design a special LLM runtime with highly-optimized kernels to accelerate the LLM inference on CPUs. We demonstrate the general applicability of our approach on popular LLMs including Llama2, Llama, GPT-NeoX, and showcase the extreme inference efficiency on CPUs. The code is publicly available at: https://github.com/intel/intel-extension-for-transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#25968;&#25454;&#24402;&#22240;&#26041;&#38754;&#65292;&#19968;&#20123;&#22312;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#33021;&#22815;&#22312;&#23454;&#38469;&#20013;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#20844;&#24179;&#34917;&#20607;&#25110;&#35748;&#21487;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2311.00500</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#24402;&#22240;&#30340;&#26377;&#36259;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing Properties of Data Attribution on Diffusion Models. (arXiv:2311.00500v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#25968;&#25454;&#24402;&#22240;&#26041;&#38754;&#65292;&#19968;&#20123;&#22312;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#33021;&#22815;&#22312;&#23454;&#38469;&#20013;&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36825;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#20844;&#24179;&#34917;&#20607;&#25110;&#35748;&#21487;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24402;&#22240;&#26088;&#22312;&#23558;&#27169;&#22411;&#36755;&#20986;&#36861;&#28335;&#21040;&#35757;&#32451;&#25968;&#25454;&#12290;&#38543;&#30528;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25968;&#25454;&#24402;&#22240;&#24050;&#25104;&#20026;&#19968;&#20010;&#29702;&#24819;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#20026;&#39640;&#36136;&#37327;&#25110;&#29256;&#26435;&#20445;&#25252;&#30340;&#35757;&#32451;&#26679;&#26412;&#27491;&#30830;&#20998;&#37197;&#20215;&#20540;&#65292;&#30830;&#20445;&#25968;&#25454;&#36129;&#29486;&#32773;&#24471;&#21040;&#20844;&#24179;&#30340;&#34917;&#20607;&#25110;&#35748;&#21487;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22312;&#29702;&#35770;&#19978;&#26377;&#21160;&#26426;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25968;&#25454;&#24402;&#22240;&#65292;&#20197;&#25913;&#21892;&#35745;&#31639;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#26524;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;CIFAR-10&#21644;CelebA&#19978;&#35757;&#32451;&#30340;DDPM&#20197;&#21450;&#22312;ArtBench&#19978;&#36827;&#34892;&#32454;&#35843;&#30340;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;LoRA&#30340;&#24402;&#22240;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#29702;&#35770;&#19978;&#19981;&#21512;&#29702;&#30340;&#35774;&#35745;&#36873;&#25321;&#22312;&#23454;&#38469;&#20013;&#22823;&#24133;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#22522;&#32447;&#65292;&#26080;&#35770;&#26159;&#22312;&#32447;&#24615;&#25968;&#25454;&#24314;&#27169;&#24471;&#20998;&#36824;&#26159;&#21453;&#20107;&#23454;&#35780;&#20272;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21576;&#29616;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited. Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness. In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation. Our work presents a signific
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#26080;&#27861;&#20805;&#20998;&#27169;&#25311;&#36229;&#20998;&#27573;&#26102;&#38388;&#29305;&#24449;&#65292;&#36825;&#20026;&#26410;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#23436;&#25972;&#35821;&#38899;&#20449;&#21495;&#36827;&#34892;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2311.00489</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#19981;&#33021;&#23398;&#20064;&#36229;&#20998;&#27573;&#26102;&#38388;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks for Automatic Speaker Recognition Do Not Learn Supra-Segmental Temporal Features. (arXiv:2311.00489v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00489
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#26080;&#27861;&#20805;&#20998;&#27169;&#25311;&#36229;&#20998;&#27573;&#26102;&#38388;&#29305;&#24449;&#65292;&#36825;&#20026;&#26410;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#23436;&#25972;&#35821;&#38899;&#20449;&#21495;&#36827;&#34892;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#23545;&#20110;&#36825;&#20123;&#32467;&#26524;&#30340;&#20855;&#20307;&#21407;&#22240;&#20102;&#35299;&#29978;&#23569;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#23558;&#25104;&#21151;&#30340;&#19968;&#37096;&#20998;&#24402;&#22240;&#20110;&#23427;&#20204;&#27169;&#25311;&#36229;&#20998;&#27573;&#26102;&#38388;&#20449;&#24687;&#65288;SST&#65289;&#30340;&#33021;&#21147;&#65292;&#21363;&#38500;&#20102;&#35889;&#29305;&#24449;&#22806;&#36824;&#23398;&#20064;&#35821;&#38899;&#30340;&#38901;&#24459;&#21644;&#38901;&#24459;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#65288;i&#65289;&#25552;&#20986;&#24182;&#24212;&#29992;&#19968;&#31181;&#26032;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#26469;&#37327;&#21270;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#26041;&#38754;&#30340;&#24615;&#33021;&#33021;&#22815;&#36890;&#36807;&#24314;&#27169;SST&#26469;&#35299;&#37322;&#21040;&#22810;&#22823;&#31243;&#24230;&#65307;&#24182;&#19988;&#65288;ii&#65289;&#25552;&#20986;&#20960;&#31181;&#24378;&#21046;&#32593;&#32476;&#26356;&#21152;&#20851;&#27880;SST&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#34987;&#24378;&#21046;&#35201;&#27714;&#65292;&#19968;&#31995;&#21015;&#22522;&#20110;CNN&#21644;RNN&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#20013;&#20063;&#19981;&#33021;&#20805;&#20998;&#22320;&#27169;&#25311;SST&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#26410;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#23436;&#25972;&#35821;&#38899;&#20449;&#21495;&#36827;&#34892;&#30740;&#31350;&#25552;&#20379;&#20102;&#38750;&#24120;&#37325;&#35201;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep neural networks have shown impressive results in automatic speaker recognition and related tasks, it is dissatisfactory how little is understood about what exactly is responsible for these results. Part of the success has been attributed in prior work to their capability to model supra-segmental temporal information (SST), i.e., learn rhythmic-prosodic characteristics of speech in addition to spectral features. In this paper, we (i) present and apply a novel test to quantify to what extent the performance of state-of-the-art neural networks for speaker recognition can be explained by modeling SST; and (ii) present several means to force respective nets to focus more on SST and evaluate their merits. We find that a variety of CNN- and RNN-based neural network architectures for speaker recognition do not model SST to any sufficient degree, even when forced. The results provide a highly relevant basis for impactful future research into better exploitation of the full speech sig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20248;&#21270;&#30446;&#26631;&#21457;&#29616;&#65292;&#20351;&#29992;Midpoint-Displacement&#65288;MD&#65289;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#33719;&#24471;&#19982;Contrast-Consistent Search&#65288;CCS&#65289;&#38750;&#24120;&#30456;&#20284;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#32780;&#19988;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#21487;&#20197;&#20351;MD&#25439;&#22833;&#20989;&#25968;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#39640;&#20110;CCS&#12290;</title><link>http://arxiv.org/abs/2311.00488</link><description>&lt;p&gt;
&#23545;&#27604;&#20248;&#21270;&#30446;&#26631;&#23545;&#19968;&#33268;&#24615;&#25628;&#32034;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing Optimization Targets for Contrast-Consistent Search. (arXiv:2311.00488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20248;&#21270;&#30446;&#26631;&#21457;&#29616;&#65292;&#20351;&#29992;Midpoint-Displacement&#65288;MD&#65289;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#33719;&#24471;&#19982;Contrast-Consistent Search&#65288;CCS&#65289;&#38750;&#24120;&#30456;&#20284;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#32780;&#19988;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#21487;&#20197;&#20351;MD&#25439;&#22833;&#20989;&#25968;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#39640;&#20110;CCS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#33268;&#24615;&#25628;&#32034;&#65288;CCS&#65289;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#26088;&#22312;&#24674;&#22797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;Midpoint-Displacement&#65288;MD&#65289;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20010;&#36229;&#21442;&#25968;&#20540;&#19979;&#65292;&#35813;MD&#25439;&#22833;&#20989;&#25968;&#23548;&#33268;&#30340;&#27169;&#22411;&#26435;&#37325;&#19982;CCS&#38750;&#24120;&#30456;&#20284;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35813;&#36229;&#21442;&#25968;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#29992;&#26356;&#22909;&#30340;&#36229;&#21442;&#25968;&#65292;MD&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#36798;&#21040;&#27604;CCS&#26356;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the optimization target of Contrast-Consistent Search (CCS), which aims to recover the internal representations of truth of a large language model. We present a new loss function that we call the Midpoint-Displacement (MD) loss function. We demonstrate that for a certain hyper-parameter value this MD loss function leads to a prober with very similar weights to CCS. We further show that this hyper-parameter is not optimal and that with a better hyper-parameter the MD loss function attains a higher test accuracy than CCS.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#32447;&#24615;bandit&#20013;&#30340;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;Lasso&#21644;&#26368;&#20248;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#36229;&#21442;&#25968;&#21644;&#24179;&#34913;&#20004;&#20010;&#38454;&#27573;&#30340;&#38169;&#35823;&#27010;&#29575;&#65292;&#24471;&#21040;&#20102;Lasso-OD&#30340;&#38750;&#28176;&#36817;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2311.00481</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#32447;&#24615;bandit&#20013;&#30340;&#22266;&#23450;&#39044;&#31639;&#19979;&#65292;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fixed-Budget Best-Arm Identification in Sparse Linear Bandits. (arXiv:2311.00481v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#32447;&#24615;bandit&#20013;&#30340;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;Lasso&#21644;&#26368;&#20248;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#36229;&#21442;&#25968;&#21644;&#24179;&#34913;&#20004;&#20010;&#38454;&#27573;&#30340;&#38169;&#35823;&#27010;&#29575;&#65292;&#24471;&#21040;&#20102;Lasso-OD&#30340;&#38750;&#28176;&#36817;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#32447;&#24615;bandit&#20013;&#30340;&#22266;&#23450;&#39044;&#31639;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#22312;&#31232;&#30095;&#32447;&#24615;bandit&#20013;&#65292;&#26410;&#30693;&#29305;&#24449;&#21521;&#37327;&#952;*&#21487;&#33021;&#20855;&#26377;&#24456;&#22823;&#30340;&#32500;&#24230;d&#65292;&#20294;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#29305;&#24449;&#65288;&#27604;&#22914;s&#20010;&#65289;&#20855;&#26377;&#38750;&#38646;&#20540;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#31639;&#27861;&#65292;&#21363;&#22522;&#20110;Lasso&#21644;&#26368;&#20248;&#35774;&#35745;(Lasso-OD)&#30340;&#32447;&#24615;&#26368;&#20339;&#33218;&#35782;&#21035;&#12290;Lasso-OD&#30340;&#31532;&#19968;&#38454;&#27573;&#21033;&#29992;&#20102;&#29305;&#24449;&#21521;&#37327;&#30340;&#31232;&#30095;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;Zhou&#65288;2009&#65289;&#24341;&#20837;&#30340;&#38408;&#20540;&#21270;Lasso&#65292;&#21033;&#29992;&#25152;&#36873;&#25321;&#30340;&#33218;&#30340;&#22238;&#25253;&#21644;&#21512;&#29702;&#36873;&#25321;&#30340;&#35774;&#35745;&#30697;&#38453;&#26469;&#39640;&#27010;&#29575;&#22320;&#27491;&#30830;&#20272;&#35745;&#952;*&#30340;&#25903;&#25345;&#38598;&#12290;Lasso-OD&#30340;&#31532;&#20108;&#38454;&#27573;&#22312;&#20272;&#35745;&#24471;&#21040;&#30340;&#25903;&#25345;&#38598;&#19978;&#24212;&#29992;&#20102;Yang&#21644;Tan&#65288;2022&#65289;&#25552;&#20986;&#30340;OD-LinBAI&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#36229;&#21442;&#25968;&#65288;&#22914;Lasso&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#65289;&#21644;&#24179;&#34913;&#20004;&#20010;&#38454;&#27573;&#30340;&#38169;&#35823;&#27010;&#29575;&#65292;&#25512;&#23548;&#20102;Lasso-OD&#30340;&#38750;&#28176;&#36817;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the best-arm identification problem in sparse linear bandits under the fixed-budget setting. In sparse linear bandits, the unknown feature vector $\theta^*$ may be of large dimension $d$, but only a few, say $s \ll d$ of these features have non-zero values. We design a two-phase algorithm, Lasso and Optimal-Design- (Lasso-OD) based linear best-arm identification. The first phase of Lasso-OD leverages the sparsity of the feature vector by applying the thresholded Lasso introduced by Zhou (2009), which estimates the support of $\theta^*$ correctly with high probability using rewards from the selected arms and a judicious choice of the design matrix. The second phase of Lasso-OD applies the OD-LinBAI algorithm by Yang and Tan (2022) on that estimated support. We derive a non-asymptotic upper bound on the error probability of Lasso-OD by carefully choosing hyperparameters (such as Lasso's regularization parameter) and balancing the error probabilities of both phases. For fixed spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#32452;&#24863;&#30693;&#30340;&#33976;&#39311;&#25439;&#22833;&#26469;&#35299;&#20915;&#30693;&#35782;&#33976;&#39311;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#20122;&#32676;&#20307;&#20559;&#31227;&#38382;&#39064;&#65292;&#20026;&#22312;&#35757;&#32451;&#26399;&#38388;&#21160;&#24577;&#20851;&#27880;&#34920;&#29616;&#36739;&#24046;&#30340;&#32676;&#32452;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.00476</link><description>&lt;p&gt;
&#20998;&#32452;&#20998;&#24067;&#40065;&#26834;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Group Distributionally Robust Knowledge Distillation. (arXiv:2311.00476v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32676;&#32452;&#24863;&#30693;&#30340;&#33976;&#39311;&#25439;&#22833;&#26469;&#35299;&#20915;&#30693;&#35782;&#33976;&#39311;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#20122;&#32676;&#20307;&#20559;&#31227;&#38382;&#39064;&#65292;&#20026;&#22312;&#35757;&#32451;&#26399;&#38388;&#21160;&#24577;&#20851;&#27880;&#34920;&#29616;&#36739;&#24046;&#30340;&#32676;&#32452;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#24555;&#36895;&#26377;&#25928;&#22320;&#20174;&#22823;&#27169;&#22411;&#21521;&#23567;&#27169;&#22411;&#20256;&#36755;&#23398;&#21040;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#33976;&#39311;&#30446;&#26631;&#23545;&#20122;&#32676;&#20307;&#30340;&#20559;&#31227;&#29305;&#21035;&#25935;&#24863;&#65292;&#32780;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#24120;&#35265;&#30340;&#26159;&#22312;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#23569;&#25968;&#32676;&#20307;/&#39046;&#22495;&#30340;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#25195;&#25551;&#20202;&#25110;&#21307;&#38498;&#33719;&#24471;&#30340;&#20581;&#24247;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#23545;&#23569;&#25968;&#32676;&#20307;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#21463;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#25216;&#26415;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32676;&#32452;&#24863;&#30693;&#30340;&#33976;&#39311;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#32473;&#23450;&#36845;&#20195;&#20013;&#30340;&#27599;&#20010;&#32676;&#32452;&#25439;&#22833;&#26469;&#26356;&#26032;&#19968;&#32452;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#21160;&#24577;&#22320;&#20851;&#27880;&#34920;&#29616;&#36739;&#24046;&#30340;&#32676;&#32452;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#33258;&#28982;&#22270;&#20687;&#21644;&#24515;&#33039;MRI&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#26368;&#24046;&#32676;&#32452;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;GroupDistil&#22987;&#32456;&#26377;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation enables fast and effective transfer of features learned from a bigger model to a smaller one. However, distillation objectives are susceptible to sub-population shifts, a common scenario in medical imaging analysis which refers to groups/domains of data that are underrepresented in the training set. For instance, training models on health data acquired from multiple scanners or hospitals can yield subpar performance for minority groups. In this paper, inspired by distributionally robust optimization (DRO) techniques, we address this shortcoming by proposing a group-aware distillation loss. During optimization, a set of weights is updated based on the per-group losses at a given iteration. This way, our method can dynamically focus on groups that have low performance during training. We empirically validate our method, GroupDistil on two benchmark datasets (natural images and cardiac MRIs) and show consistent improvement in terms of worst-group accuracy.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#21464;&#20998;&#25512;&#26029;&#65288;DMVI&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#20013;&#36827;&#34892;&#33258;&#21160;&#36817;&#20284;&#25512;&#26029;&#12290;DMVI&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#21518;&#39564;&#25512;&#26029;&#65292;&#32780;&#19988;&#26131;&#20110;&#23454;&#29616;&#21644;&#20351;&#29992;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#27809;&#26377;&#20219;&#20309;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2311.00474</link><description>&lt;p&gt;
&#27010;&#29575;&#32534;&#31243;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion models for probabilistic programming. (arXiv:2311.00474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00474
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#21464;&#20998;&#25512;&#26029;&#65288;DMVI&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#20013;&#36827;&#34892;&#33258;&#21160;&#36817;&#20284;&#25512;&#26029;&#12290;DMVI&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#21518;&#39564;&#25512;&#26029;&#65292;&#32780;&#19988;&#26131;&#20110;&#23454;&#29616;&#21644;&#20351;&#29992;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#27809;&#26377;&#20219;&#20309;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#27169;&#22411;&#21464;&#20998;&#25512;&#26029;&#65288;DMVI&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#65288;PPL&#65289;&#20013;&#36827;&#34892;&#33258;&#21160;&#36817;&#20284;&#25512;&#26029;&#30340;&#26032;&#26041;&#27861;&#12290;DMVI&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#23545;&#30495;&#23454;&#21518;&#39564;&#20998;&#24067;&#30340;&#21464;&#20998;&#36817;&#20284;&#65292;&#36890;&#36807;&#23548;&#20986;&#36125;&#21494;&#26031;&#24314;&#27169;&#20013;&#20351;&#29992;&#30340;&#36793;&#38469;&#20284;&#28982;&#30446;&#26631;&#30340;&#26032;&#32422;&#26463;&#12290;DMVI&#26131;&#20110;&#23454;&#29616;&#65292;&#22312;PPL&#20013;&#36827;&#34892;&#26080;&#38556;&#30861;&#25512;&#26029;&#65292;&#19981;&#20687;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#30340;&#21464;&#20998;&#25512;&#26029;&#37027;&#26679;&#20855;&#26377;&#32570;&#28857;&#65292;&#24182;&#19988;&#23545;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19981;&#20570;&#20219;&#20309;&#32422;&#26463;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#24120;&#35265;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#19978;&#35780;&#20272;&#20102;DMVI&#65292;&#24182;&#34920;&#26126;&#23427;&#30340;&#21518;&#39564;&#25512;&#26029;&#19968;&#33324;&#27604;PPL&#20013;&#20351;&#29992;&#30340;&#29616;&#20195;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#21516;&#26102;&#20855;&#26377;&#31867;&#20284;&#30340;&#35745;&#31639;&#25104;&#26412;&#24182;&#19988;&#38656;&#35201;&#36739;&#23569;&#30340;&#25163;&#21160;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Diffusion Model Variational Inference (DMVI), a novel method for automated approximate inference in probabilistic programming languages (PPLs). DMVI utilizes diffusion models as variational approximations to the true posterior distribution by deriving a novel bound to the marginal likelihood objective used in Bayesian modelling. DMVI is easy to implement, allows hassle-free inference in PPLs without the drawbacks of, e.g., variational inference using normalizing flows, and does not make any constraints on the underlying neural network model. We evaluate DMVI on a set of common Bayesian models and show that its posterior inferences are in general more accurate than those of contemporary methods used in PPLs while having a similar computational cost and requiring less manual tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;DCDM&#65289;&#65292;&#29992;&#20110;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#20013;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#23384;&#22312;&#39640;&#24230;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#22823;&#37327;&#20869;&#37096;&#21464;&#24322;&#24615;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#20013;&#30340;&#24515;&#33039;&#35270;&#22270;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#25298;&#32477;&#31867;&#20284;&#30340;&#22806;&#20998;&#24067;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2311.00469</link><description>&lt;p&gt;
&#21452;&#37325;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22806;&#20998;&#24067;&#26816;&#27979;&#65306;&#24212;&#29992;&#20110;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Dual Conditioned Diffusion Models for Out-Of-Distribution Detection: Application to Fetal Ultrasound Videos. (arXiv:2311.00469v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;DCDM&#65289;&#65292;&#29992;&#20110;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#20013;&#30340;&#22806;&#20998;&#24067;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#23384;&#22312;&#39640;&#24230;&#32467;&#26500;&#30456;&#20284;&#24615;&#21644;&#22823;&#37327;&#20869;&#37096;&#21464;&#24322;&#24615;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#20013;&#30340;&#24515;&#33039;&#35270;&#22270;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#25298;&#32477;&#31867;&#20284;&#30340;&#22806;&#20998;&#24067;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#23545;&#20110;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#33021;&#22815;&#26816;&#27979;&#20986;&#19981;&#23646;&#20110;&#35757;&#32451;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#26377;&#25928;&#22320;&#26816;&#27979;OOD&#26679;&#26412;&#21487;&#33021;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#20998;&#24067;&#65288;ID&#65289;&#20869;&#37096;&#23384;&#22312;&#26174;&#33879;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#19988;ID&#21644;OOD&#31867;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#20013;&#26816;&#27979;&#24515;&#33039;&#35270;&#22270;&#26102;&#65292;&#24515;&#33039;&#21644;&#33145;&#37096;&#31561;&#20854;&#20182;&#35299;&#21078;&#32467;&#26500;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#27599;&#20010;&#35270;&#22270;&#20869;&#37096;&#23384;&#22312;&#30528;5&#31181;&#19981;&#21516;&#30340;&#35270;&#22270;&#21644;&#32467;&#26500;&#21464;&#21270;&#12290;&#20026;&#20102;&#26816;&#27979;&#27492;&#32972;&#26223;&#19979;&#30340;OOD&#26679;&#26412;&#65292;&#25152;&#24471;&#27169;&#22411;&#24212;&#33021;&#22815;&#23398;&#20064;&#21040;&#35299;&#21078;&#32467;&#26500;&#20869;&#37096;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#33021;&#22815;&#25298;&#32477;&#31867;&#20284;&#30340;OOD&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21452;&#37325;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;DCDM&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;ID&#31867;&#20449;&#24687;&#21644;&#36755;&#20837;&#22270;&#20687;&#30340;&#28508;&#22312;&#29305;&#24449;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#26465;&#20214;&#21270;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential to improve the reliability of machine learning models by detecting samples that do not belong to the training distribution. Detecting OOD samples effectively in certain tasks can pose a challenge because of the substantial heterogeneity within the in-distribution (ID), and the high structural similarity between ID and OOD classes. For instance, when detecting heart views in fetal ultrasound videos there is a high structural similarity between the heart and other anatomies such as the abdomen, and large in-distribution variance as a heart has 5 distinct views and structural variations within each view. To detect OOD samples in this context, the resulting model should generalise to the intra-anatomy variations while rejecting similar OOD samples. In this paper, we introduce dual-conditioned diffusion models (DCDM) where we condition the model on in-distribution class information and latent features of the input image for reconstruction-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#22270;&#19978;&#30340;&#24322;&#27493;SGD&#65288;AGRAF SGD&#65289;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32479;&#19968;&#20102;&#24322;&#27493;&#20998;&#25955;&#21644;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#22312;&#26356;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#36824;&#24674;&#22797;&#25110;&#25913;&#21892;&#20102;&#25152;&#26377;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00465</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#24322;&#27493;SGD: &#19968;&#31181;&#32479;&#19968;&#30340;&#24322;&#27493;&#20998;&#25955;&#21644;&#32852;&#37030;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Asynchronous SGD on Graphs: a Unified Framework for Asynchronous Decentralized and Federated Optimization. (arXiv:2311.00465v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22270;&#19978;&#30340;&#24322;&#27493;SGD&#65288;AGRAF SGD&#65289;&#31639;&#27861;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32479;&#19968;&#20102;&#24322;&#27493;&#20998;&#25955;&#21644;&#32852;&#37030;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#22312;&#26356;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#36824;&#24674;&#22797;&#25110;&#25913;&#21892;&#20102;&#25152;&#26377;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#21644;&#24322;&#27493;&#36890;&#20449;&#26159;&#21152;&#36895;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#36890;&#20449;&#22797;&#26434;&#24615;&#30340;&#20004;&#31181;&#27969;&#34892;&#25216;&#26415;&#65292;&#20998;&#21035;&#36890;&#36807;&#28040;&#38500;&#23545;&#20013;&#22830;&#32534;&#25490;&#22120;&#30340;&#20381;&#36182;&#21644;&#19981;&#38656;&#35201;&#21516;&#27493;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20004;&#31181;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#24341;&#20837;&#20102;&#22270;&#19978;&#30340;&#24322;&#27493;SGD&#65288;AGRAF SGD&#65289;&#8212;&#8212;&#19968;&#20010;&#36890;&#29992;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;&#20102;&#35768;&#22810;&#27969;&#34892;&#31639;&#27861;&#30340;&#24322;&#27493;&#29256;&#26412;&#65292;&#21253;&#25324;SGD&#12289;&#20998;&#25955;SGD&#12289;&#26412;&#22320;SGD&#12289;FedBuff&#65292;&#30001;&#20110;&#20854;&#25918;&#26494;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#20551;&#35774;&#12290;&#22312;&#27604;&#20043;&#21069;&#30340;&#20998;&#25955;&#24322;&#27493;&#24037;&#20316;&#26356;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20173;&#28982;&#24674;&#22797;&#29978;&#33267;&#25913;&#21892;&#20102;&#25152;&#26377;&#31639;&#27861;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized and asynchronous communications are two popular techniques to speedup communication complexity of distributed machine learning, by respectively removing the dependency over a central orchestrator and the need for synchronization. Yet, combining these two techniques together still remains a challenge. In this paper, we take a step in this direction and introduce Asynchronous SGD on Graphs (AGRAF SGD) -- a general algorithmic framework that covers asynchronous versions of many popular algorithms including SGD, Decentralized SGD, Local SGD, FedBuff, thanks to its relaxed communication and computation assumptions. We provide rates of convergence under much milder assumptions than previous decentralized asynchronous works, while still recovering or even improving over the best know results for all the algorithms covered.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#21644;&#20849;&#36717;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;RCGP&#65289;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#27867;&#21270;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#38381;&#24335;&#26356;&#26032;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2311.00463</link><description>&lt;p&gt;
&#20581;&#22766;&#21644;&#20849;&#36717;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Robust and Conjugate Gaussian Process Regression. (arXiv:2311.00463v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#21644;&#20849;&#36717;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;RCGP&#65289;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#27867;&#21270;&#36125;&#21494;&#26031;&#25512;&#26029;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#38381;&#24335;&#26356;&#26032;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#38381;&#24335;&#26465;&#20214;&#65292;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#22238;&#24402;&#30340;&#24120;&#35265;&#20551;&#35774;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#35266;&#27979;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24378;&#20551;&#35774;&#22312;&#23454;&#38469;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#25512;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#27867;&#21270;&#36125;&#21494;&#26031;&#25512;&#26029;&#20197;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#20195;&#20215;&#23454;&#29616;&#21487;&#38752;&#21644;&#20849;&#36717;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;RCGP&#65289;&#22238;&#24402;&#12290;RCGP&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26631;&#20934;GP&#36866;&#29992;&#30340;&#25152;&#26377;&#24773;&#20917;&#19979;&#36827;&#34892;&#31934;&#30830;&#30340;&#20849;&#36717;&#38381;&#24335;&#26356;&#26032;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#24378;&#22823;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;RCGP&#24212;&#29992;&#20110;&#20174;&#36125;&#21494;&#26031;&#20248;&#21270;&#21040;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#30340;&#21508;&#31181;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enable closed form conditioning, a common assumption in Gaussian process (GP) regression is independent and identically distributed Gaussian observation noise. This strong and simplistic assumption is often violated in practice, which leads to unreliable inferences and uncertainty quantification. Unfortunately, existing methods for robustifying GPs break closed-form conditioning, which makes them less attractive to practitioners and significantly more computationally expensive. In this paper, we demonstrate how to perform provably robust and conjugate Gaussian process (RCGP) regression at virtually no additional cost using generalised Bayesian inference. RCGP is particularly versatile as it enables exact conjugate closed form updates in all settings where standard GPs admit them. To demonstrate its strong empirical performance, we deploy RCGP for problems ranging from Bayesian optimisation to sparse variational Gaussian processes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#38480;&#21046;&#39044;&#31639;&#25298;&#32477;&#37319;&#26679;&#26041;&#27861;&#65288;OBRS&#65289;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#23558;&#37319;&#26679;&#26041;&#26696;&#19982;&#35757;&#32451;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#22312;&#32473;&#23450;&#37319;&#26679;&#39044;&#31639;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#20219;&#20309;&#30495;&#23454;&#20998;&#24067;&#21644;&#25298;&#32477;&#21518;&#20998;&#24067;&#20043;&#38388;&#30340;f-&#25955;&#24230;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2311.00460</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20339;&#38480;&#21046;&#39044;&#31639;&#25298;&#32477;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Budgeted Rejection Sampling for Generative Models. (arXiv:2311.00460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00460
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#38480;&#21046;&#39044;&#31639;&#25298;&#32477;&#37319;&#26679;&#26041;&#27861;&#65288;OBRS&#65289;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#23558;&#37319;&#26679;&#26041;&#26696;&#19982;&#35757;&#32451;&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#35813;&#26041;&#27861;&#22312;&#32473;&#23450;&#37319;&#26679;&#39044;&#31639;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#20219;&#20309;&#30495;&#23454;&#20998;&#24067;&#21644;&#25298;&#32477;&#21518;&#20998;&#24067;&#20043;&#38388;&#30340;f-&#25955;&#24230;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#25298;&#32477;&#37319;&#26679;&#26041;&#27861;&#26469;&#25913;&#21892;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#22312;&#26080;&#38480;&#37319;&#26679;&#39044;&#31639;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#24212;&#29992;&#20110;&#19982;&#25298;&#32477;&#36807;&#31243;&#29420;&#31435;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;Optimal Budgeted Rejection Sampling (OBRS)&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#32473;&#23450;&#37319;&#26679;&#39044;&#31639;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#30495;&#23454;&#20998;&#24067;&#21644;&#25298;&#32477;&#21518;&#20998;&#24067;&#20043;&#38388;&#30340;&#20219;&#20309;f-&#25955;&#24230;&#35777;&#26126;&#26159;&#26368;&#20248;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#23558;&#37319;&#26679;&#26041;&#26696;&#34701;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#21644;&#25903;&#25345;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rejection sampling methods have recently been proposed to improve the performance of discriminator-based generative models. However, these methods are only optimal under an unlimited sampling budget, and are usually applied to a generator trained independently of the rejection procedure. We first propose an Optimal Budgeted Rejection Sampling (OBRS) scheme that is provably optimal with respect to \textit{any} $f$-divergence between the true distribution and the post-rejection distribution, for a given sampling budget. Second, we propose an end-to-end method that incorporates the sampling scheme into the training procedure to further enhance the model's overall performance. Through experiments and supporting theory, we show that the proposed methods are effective in significantly improving the quality and diversity of the samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30697;&#38453;&#30340;Hessian&#29305;&#24449;&#21521;&#37327;&#19982;&#32593;&#32476;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#35782;&#21035;&#20851;&#38190;&#26041;&#21521;&#25581;&#31034;&#20102;&#28418;&#31227;&#27169;&#24335;&#21644;&#21183;&#33021;&#26368;&#23567;&#20540;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2311.00452</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30697;&#38453;&#30340;Hessian&#29305;&#24449;&#21521;&#37327;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hessian Eigenvectors and Principal Component Analysis of Neural Network Weight Matrices. (arXiv:2311.00452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30697;&#38453;&#30340;Hessian&#29305;&#24449;&#21521;&#37327;&#19982;&#32593;&#32476;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;&#35782;&#21035;&#20851;&#38190;&#26041;&#21521;&#25581;&#31034;&#20102;&#28418;&#31227;&#27169;&#24335;&#21644;&#21183;&#33021;&#26368;&#23567;&#20540;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#19982;&#32593;&#32476;&#21442;&#25968;&#30340;&#20851;&#31995;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#20027;&#35201;&#22312;&#21333;&#19968;&#26041;&#21521;&#19978;&#32487;&#32493;&#35757;&#32451;&#65292;&#34987;&#31216;&#20026;&#28418;&#31227;&#27169;&#24335;&#12290;&#36825;&#31181;&#28418;&#31227;&#27169;&#24335;&#21487;&#20197;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#30340;&#20108;&#27425;&#21183;&#33021;&#27169;&#22411;&#26469;&#35299;&#37322;&#65292;&#26263;&#31034;&#20102;&#25351;&#25968;&#24930;&#24930;&#34928;&#20943;&#21040;&#21183;&#33021;&#26368;&#23567;&#20540;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;Hessian&#29305;&#24449;&#21521;&#37327;&#19982;&#32593;&#32476;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#20851;&#31995;&#20381;&#36182;&#20110;&#29305;&#24449;&#20540;&#30340;&#22823;&#23567;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21306;&#20998;&#32593;&#32476;&#20869;&#30340;&#21442;&#25968;&#26041;&#21521;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#21521;&#30340;&#37325;&#35201;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#21183;&#38449;&#30340;&#26354;&#29575;&#65288;&#30001;Hessian&#29305;&#24449;&#20540;&#30340;&#22823;&#23567;&#25351;&#31034;&#65289;&#20197;&#21450;&#19982;&#26435;&#37325;&#21521;&#37327;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#36824;&#24310;&#20280;&#21040;&#20102;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#23545;&#26435;&#37325;&#30697;&#38453;&#36827;&#34892;&#20998;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#30830;&#23450;Hessian&#20869;&#30340;&#20851;&#38190;&#26041;&#21521;&#26041;&#38754;&#38750;&#24120;&#23454;&#29992;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#30340;&#26354;&#29575;&#21644;&#26435;&#37325;&#21521;&#37327;&#30340;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study delves into the intricate dynamics of trained deep neural networks and their relationships with network parameters. Trained networks predominantly continue training in a single direction, known as the drift mode. This drift mode can be explained by the quadratic potential model of the loss function, suggesting a slow exponential decay towards the potential minima. We unveil a correlation between Hessian eigenvectors and network weights. This relationship, hinging on the magnitude of eigenvalues, allows us to discern parameter directions within the network. Notably, the significance of these directions relies on two defining attributes: the curvature of their potential wells (indicated by the magnitude of Hessian eigenvalues) and their alignment with the weight vectors. Our exploration extends to the decomposition of weight matrices through singular value decomposition. This approach proves practical in identifying critical directions within the Hessian, considering both thei
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2311.00445</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19977;&#27573;&#35770;&#25512;&#29702;&#30340;&#31995;&#32479;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models. (arXiv:2311.00445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00445
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#19977;&#27573;&#35770;&#25512;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#36739;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#21512;&#36923;&#36753;&#65292;&#29978;&#33267;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#65292;&#20294;&#21363;&#20351;&#26368;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#19982;&#20154;&#31867;&#25512;&#29702;&#31867;&#20284;&#30340;&#38169;&#35823;&#65292;&#24635;&#20307;&#19978;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#20154;&#31867;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#34892;&#20026;&#30340;&#19968;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26159;&#36923;&#36753;&#25512;&#29702;&#65306;&#30830;&#23450;&#21738;&#20123;&#32467;&#35770;&#21487;&#20197;&#20174;&#19968;&#32452;&#21069;&#25552;&#20013;&#24471;&#20986;&#12290;&#24515;&#29702;&#23398;&#23478;&#24050;&#32463;&#35760;&#24405;&#19979;&#20154;&#31867;&#25512;&#29702;&#19982;&#36923;&#36753;&#35268;&#21017;&#19981;&#31526;&#30340;&#20960;&#31181;&#26041;&#24335;&#12290;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22797;&#21046;&#36825;&#20123;&#20559;&#24046;&#65292;&#25110;&#32773;&#23427;&#20204;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#24046;&#65311;&#25105;&#20204;&#20851;&#27880;&#19977;&#27573;&#35770;&#30340;&#24773;&#20917; - &#20174;&#20004;&#20010;&#31616;&#21333;&#21069;&#25552;&#20013;&#25512;&#23548;&#20986;&#30340;&#25512;&#29702;&#65292;&#36825;&#22312;&#24515;&#29702;&#23398;&#20013;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350; - &#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#27169;&#22411;&#27604;&#36739;&#21512;&#36923;&#36753;&#65292;&#32780;&#19988;&#27604;&#20154;&#31867;&#26356;&#21512;&#36923;&#36753;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;&#27169;&#22411;&#20063;&#20250;&#20986;&#29616;&#31995;&#32479;&#24615;&#38169;&#35823;&#65292;&#20854;&#20013;&#19968;&#20123;&#38169;&#35823;&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;&#20559;&#35265;&#30456;&#20284;&#65292;&#20363;&#22914;&#25490;&#24207;&#25928;&#24212;&#21644;&#36923;&#36753;&#35884;&#35823;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#27169;&#20223;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#21363;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#22522;&#20110;&#22270;&#30340;&#21151;&#33021;&#35201;&#27714;&#29983;&#25104;&#22270;&#24418;&#12290;&#25105;&#20204;&#36890;&#36807;&#32454;&#35843;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#32467;&#21512;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#36148;&#36817;&#21151;&#33021;&#35201;&#27714;&#30340;&#22270;&#24418;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#21487;&#24471;&#30340;&#20998;&#23376;&#21644;&#30693;&#35782;&#22270;&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#27604;&#31867;&#20284;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#29983;&#25104;&#30340;&#22270;&#24418;&#26356;&#33021;&#28385;&#36275;&#21151;&#33021;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2311.00444</link><description>&lt;p&gt;
"&#24418;&#24335;&#36861;&#38543;&#21151;&#33021;&#65306;&#22522;&#20110;&#21151;&#33021;&#35201;&#27714;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#26465;&#20214;&#22270;&#29983;&#25104;"
&lt;/p&gt;
&lt;p&gt;
Form follows Function: Text-to-Text Conditional Graph Generation based on Functional Requirements. (arXiv:2311.00444v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#21363;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#22522;&#20110;&#22270;&#30340;&#21151;&#33021;&#35201;&#27714;&#29983;&#25104;&#22270;&#24418;&#12290;&#25105;&#20204;&#36890;&#36807;&#32454;&#35843;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#28040;&#24687;&#20256;&#36882;&#23618;&#26469;&#32467;&#21512;&#22270;&#32467;&#26500;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#36148;&#36817;&#21151;&#33021;&#35201;&#27714;&#30340;&#22270;&#24418;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#21487;&#24471;&#30340;&#20998;&#23376;&#21644;&#30693;&#35782;&#22270;&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#27604;&#31867;&#20284;&#20219;&#21153;&#19978;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#29983;&#25104;&#30340;&#22270;&#24418;&#26356;&#33021;&#28385;&#36275;&#21151;&#33021;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#29983;&#25104;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20197;&#22270;&#30340;&#21151;&#33021;&#35201;&#27714;&#25551;&#36848;&#20026;&#26465;&#20214;&#30340;&#22270;&#24418;&#30340;&#26032;&#38382;&#39064;&#35774;&#32622;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24314;&#31435;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#19987;&#27880;&#20110;&#32454;&#35843;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#29983;&#25104;&#22270;&#24418;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24402;&#32435;&#20559;&#24046;&#65292;&#23558;&#20851;&#20110;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#32467;&#21512;&#21040;LLM&#30340;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#22312;LLM&#30340;&#20307;&#31995;&#32467;&#26500;&#20013;&#24341;&#20837;&#28040;&#24687;&#20256;&#36882;&#23618;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#23454;&#39564;&#65292;&#20351;&#29992;&#20844;&#24320;&#21487;&#24471;&#19988;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#20998;&#23376;&#21644;&#30693;&#35782;&#22270;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#22270;&#24418;&#26356;&#25509;&#36817;&#20110;&#25152;&#35831;&#27714;&#30340;&#21151;&#33021;&#35201;&#27714;&#65292;&#26174;&#33879;&#36229;&#36807;&#20102;&#22312;&#31867;&#20284;&#20219;&#21153;&#19978;&#24320;&#21457;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on the novel problem setting of generating graphs conditioned on a description of the graph's functional requirements in a downstream task. We pose the problem as a text-to-text generation problem and focus on the approach of fine-tuning a pretrained large language model (LLM) to generate graphs. We propose an inductive bias which incorporates information about the structure of the graph into the LLM's generation process by incorporating message passing layers into an LLM's architecture. To evaluate our proposed method, we design a novel set of experiments using publicly available and widely studied molecule and knowledge graph data sets. Results suggest our proposed approach generates graphs which more closely meet the requested functional requirements, outperforming baselines developed on similar tasks by a statistically significant margin.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Dynamic Scanning Augmentation&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;Vision Transformer&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#24212;&#24615;&#22320;&#32858;&#28966;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22359;&#65292;&#24182;&#25913;&#21464;&#20102;ViT&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2311.00441</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#21160;&#24577;&#25195;&#25551;&#22686;&#24378;&#25216;&#26415;&#25913;&#36827;Vision Transformer&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness for Vision Transformer with a Simple Dynamic Scanning Augmentation. (arXiv:2311.00441v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00441
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Dynamic Scanning Augmentation&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;Vision Transformer&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#24212;&#24615;&#22320;&#32858;&#28966;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22359;&#65292;&#24182;&#25913;&#21464;&#20102;ViT&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT)&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#40065;&#26834;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;ViT&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#23545;&#25239;&#25915;&#20987;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#21160;&#24577;&#25195;&#25551;&#22686;&#24378;&#8221;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#21033;&#29992;&#21160;&#24577;&#36755;&#20837;&#24207;&#21015;&#26469;&#33258;&#36866;&#24212;&#22320;&#32858;&#28966;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22359;&#65292;&#20174;&#32780;&#20445;&#25345;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#35814;&#32454;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#31181;&#23545;&#36755;&#20837;&#24207;&#21015;&#30340;&#36866;&#24212;&#24615;&#20250;&#23548;&#33268;ViT&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#65292;&#21363;&#20351;&#23545;&#20110;&#30456;&#21516;&#30340;&#22270;&#20687;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22235;&#31181;Dynamic Scanning Augmentation&#30340;&#21464;&#20307;&#65292; &#22312;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#23545;&#33258;&#28982;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#22343;&#32988;&#36807;ViT&#65292;&#20854;&#20013;&#19968;&#31181;&#21464;&#20307;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has demonstrated promising performance in computer vision tasks, comparable to state-of-the-art neural networks. Yet, this new type of deep neural network architecture is vulnerable to adversarial attacks limiting its capabilities in terms of robustness. This article presents a novel contribution aimed at further improving the accuracy and robustness of ViT, particularly in the face of adversarial attacks. We propose an augmentation technique called `Dynamic Scanning Augmentation' that leverages dynamic input sequences to adaptively focus on different patches, thereby maintaining performance and robustness. Our detailed investigations reveal that this adaptability to the input sequence induces significant changes in the attention mechanism of ViT, even for the same image. We introduce four variations of Dynamic Scanning Augmentation, outperforming ViT in terms of both robustness to adversarial attacks and accuracy against natural images, with one variant showin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20316;&#29289;&#30149;&#23475;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#32511;&#33394;&#33394;&#24230;&#22352;&#26631;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#31227;&#21160;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#20132;&#20114;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#26368;&#20339;&#30340;&#20316;&#29289;&#31181;&#26893;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2311.00429</link><description>&lt;p&gt;
&#20351;&#29992;&#32511;&#33394;&#33394;&#24230;&#22352;&#26631;&#65288;GCC&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#25552;&#21462;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#36827;&#34892;&#20316;&#29289;&#30149;&#23475;&#20998;&#31867;&#65292;&#29992;&#20110;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#26234;&#33021;&#20892;&#19994;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Crop Disease Classification using Support Vector Machines with Green Chromatic Coordinate (GCC) and Attention based feature extraction for IoT based Smart Agricultural Applications. (arXiv:2311.00429v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20316;&#29289;&#30149;&#23475;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#32511;&#33394;&#33394;&#24230;&#22352;&#26631;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#31227;&#21160;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#20132;&#20114;&#65292;&#20026;&#20892;&#27665;&#25552;&#20379;&#26368;&#20339;&#30340;&#20316;&#29289;&#31181;&#26893;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#20154;&#31867;&#33719;&#21462;&#33021;&#37327;&#12289;&#33829;&#20859;&#21644;&#33647;&#29992;&#20215;&#20540;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#26893;&#29289;&#30149;&#23475;&#21487;&#20197;&#22312;&#20892;&#19994;&#26685;&#22521;&#36807;&#31243;&#20013;&#23545;&#21494;&#29255;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#20316;&#29289;&#20135;&#37327;&#21644;&#32463;&#27982;&#20215;&#20540;&#30340;&#26174;&#33879;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#20892;&#27665;&#33021;&#22815;&#35782;&#21035;&#20316;&#29289;&#30149;&#23475;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#36763;&#21220;&#24037;&#20316;&#12289;&#22823;&#37327;&#35745;&#21010;&#21644;&#23545;&#26893;&#29289;&#30149;&#21407;&#20307;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#37492;&#20110;&#36825;&#20123;&#38556;&#30861;&#65292;&#25552;&#20379;&#33021;&#22815;&#36731;&#26494;&#19982;&#31227;&#21160;&#35774;&#22791;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#20132;&#20114;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#20892;&#27665;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#21644;&#30740;&#31350;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#29992;&#20110;&#26893;&#29289;&#30149;&#23475;&#26816;&#27979;&#30340;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#37325;&#35201;&#19988;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#25552;&#21462;&#12289;RGB&#21644;GCC&#12290;
&lt;/p&gt;
&lt;p&gt;
Crops hold paramount significance as they serve as the primary provider of energy, nutrition, and medicinal benefits for the human population. Plant diseases, however, can negatively affect leaves during agricultural cultivation, resulting in significant losses in crop output and economic value. Therefore, it is crucial for farmers to identify crop diseases. However, this method frequently necessitates hard work, a lot of planning, and in-depth familiarity with plant pathogens. Given these numerous obstacles, it is essential to provide solutions that can easily interface with mobile and IoT devices so that our farmers can guarantee the best possible crop development. Various machine learning (ML) as well as deep learning (DL) algorithms have been created &amp; studied for the identification of plant disease detection, yielding substantial and promising results. This article presents a novel classification method that builds on prior work by utilising attention-based feature extraction, RGB
&lt;/p&gt;</description></item><item><title>NEO-KD&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#25552;&#21319;&#22810;&#20986;&#21475;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#37051;&#23621;&#30693;&#35782;&#33976;&#39311;&#21644;&#20986;&#21475;&#38388;&#27491;&#20132;&#30693;&#35782;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#23545;&#29305;&#23450;&#20986;&#21475;&#30340;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#65292;&#24182;&#22686;&#24378;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00428</link><description>&lt;p&gt;
NEO-KD&#65306;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#23545;&#25239;&#35757;&#32451;&#29992;&#20110;&#20581;&#22766;&#30340;&#22810;&#20986;&#21475;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks. (arXiv:2311.00428v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00428
&lt;/p&gt;
&lt;p&gt;
NEO-KD&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#25552;&#21319;&#22810;&#20986;&#21475;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#37051;&#23621;&#30693;&#35782;&#33976;&#39311;&#21644;&#20986;&#21475;&#38388;&#27491;&#20132;&#30693;&#35782;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#23545;&#29305;&#23450;&#20986;&#21475;&#30340;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#65292;&#24182;&#22686;&#24378;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#20986;&#21475;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#36890;&#36807;&#25552;&#21069;&#36864;&#20986;&#26469;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23545;&#25239;&#24615;&#25915;&#20987;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20986;&#21475;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#19981;&#21516;&#23376;&#27169;&#22411;&#20043;&#38388;&#30340;&#39640;&#20381;&#36182;&#24615;&#65292;&#38024;&#23545;&#29305;&#23450;&#20986;&#21475;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#19981;&#20165;&#38477;&#20302;&#20102;&#30446;&#26631;&#20986;&#21475;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#20943;&#23569;&#20102;&#25152;&#26377;&#20854;&#20182;&#20986;&#21475;&#30340;&#24615;&#33021;&#12290;&#36825;&#20351;&#24471;&#22810;&#20986;&#21475;&#32593;&#32476;&#23545;&#31616;&#21333;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#38750;&#24120;&#33030;&#24369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NEO-KD&#65292;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#36825;&#19968;&#22522;&#26412;&#25361;&#25112;&#65292;&#20854;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;NEO-KD&#39318;&#20808;&#37319;&#29992;&#37051;&#23621;&#30693;&#35782;&#33976;&#39311;&#26469;&#24341;&#23548;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#36755;&#20986;&#36235;&#21521;&#20110;&#24178;&#20928;&#25968;&#25454;&#30340;&#37051;&#23621;&#20986;&#21475;&#30340;&#38598;&#21512;&#36755;&#20986;&#12290;NEO-KD&#36824;&#20351;&#29992;&#20986;&#21475;&#38388;&#27491;&#20132;&#30340;&#30693;&#35782;&#33976;&#39311;&#26469;&#20943;&#23569;&#19981;&#21516;&#23376;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#25239;&#24615;&#20256;&#36882;&#24615;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;
&lt;/p&gt;
&lt;p&gt;
While multi-exit neural networks are regarded as a promising solution for making efficient inference via early exits, combating adversarial attacks remains a challenging problem. In multi-exit networks, due to the high dependency among different submodels, an adversarial example targeting a specific exit not only degrades the performance of the target exit but also reduces the performance of all other exits concurrently. This makes multi-exit networks highly vulnerable to simple adversarial attacks. In this paper, we propose NEO-KD, a knowledge-distillation-based adversarial training strategy that tackles this fundamental challenge based on two key contributions. NEO-KD first resorts to neighbor knowledge distillation to guide the output of the adversarial examples to tend to the ensemble outputs of neighbor exits of clean data. NEO-KD also employs exit-wise orthogonal knowledge distillation for reducing adversarial transferability across different submodels. The result is a significan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#20808;&#32423;&#25490;&#24207;&#21644;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#33258;&#25105;&#27169;&#20223;&#24378;&#21270;&#23398;&#20064;&#22312;&#36807;&#31243;&#21270;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00426</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#22870;&#21169;&#30340;&#33258;&#25105;&#27169;&#20223;&#24378;&#21270;&#23398;&#20064;&#22312;&#36807;&#31243;&#21270;&#29615;&#22659;&#20013;&#36890;&#36807;&#20248;&#20808;&#32423;&#21644;&#22810;&#26679;&#24615;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhanced Generalization through Prioritization and Diversity in Self-Imitation Reinforcement Learning over Procedural Environments with Sparse Rewards. (arXiv:2311.00426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#20808;&#32423;&#25490;&#24207;&#21644;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#33258;&#25105;&#27169;&#20223;&#24378;&#21270;&#23398;&#20064;&#22312;&#36807;&#31243;&#21270;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#26234;&#33021;&#20307;&#23398;&#20064;&#26368;&#20248;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#32570;&#20047;&#20449;&#24687;&#21453;&#39304;&#20449;&#21495;&#12290;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#22238;&#25918;&#32531;&#20914;&#21306;&#26469;&#23384;&#20648;&#21644;&#37325;&#29616;&#25104;&#21151;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#22312;&#27867;&#21270;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#36807;&#31243;&#21270;&#29983;&#25104;&#30340;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#19981;&#21516;&#26041;&#24335;&#23545;&#36716;&#25442;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#30340;&#23450;&#21046;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#23558;&#20248;&#20808;&#32423;&#25216;&#26415;&#25193;&#23637;&#21040;&#36807;&#31243;&#21270;&#29983;&#25104;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20462;&#25913;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#28436;&#31034;&#26469;&#35299;&#20915;&#22810;&#26679;&#24615;&#25439;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration poses a fundamental challenge in Reinforcement Learning (RL) with sparse rewards, limiting an agent's ability to learn optimal decision-making due to a lack of informative feedback signals. Self-Imitation Learning (self-IL) has emerged as a promising approach for exploration, leveraging a replay buffer to store and reproduce successful behaviors. However, traditional self-IL methods, which rely on high-return transitions and assume singleton environments, face challenges in generalization, especially in procedurally-generated (PCG) environments. Therefore, new self-IL methods have been proposed to rank which experiences to persist, but they replay transitions uniformly regardless of their significance, and do not address the diversity of the stored demonstrations. In this work, we propose tailored self-IL sampling strategies by prioritizing transitions in different ways and extending prioritization techniques to PCG environments. We also address diversity loss through modif
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#20043;&#21069;&#65292;&#20154;&#31867;&#36827;&#34892;&#20132;&#27969;&#20197;&#24314;&#31435;&#32422;&#23450;&#65292;&#25351;&#23450;&#35282;&#33394;&#21644;&#34892;&#21160;&#65292;&#26377;&#25928;&#22320;&#25351;&#23548;&#21327;&#21516;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#21327;&#21516;&#12290;</title><link>http://arxiv.org/abs/2311.00416</link><description>&lt;p&gt;
&#36890;&#36807;&#20934;&#22791;&#24615;&#22522;&#20110;&#35821;&#35328;&#30340;&#32422;&#23450;&#23454;&#29616;&#39640;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
Efficient Human-AI Coordination via Preparatory Language-based Convention. (arXiv:2311.00416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#20043;&#21069;&#65292;&#20154;&#31867;&#36827;&#34892;&#20132;&#27969;&#20197;&#24314;&#31435;&#32422;&#23450;&#65292;&#25351;&#23450;&#35282;&#33394;&#21644;&#34892;&#21160;&#65292;&#26377;&#25928;&#22320;&#25351;&#23548;&#21327;&#21516;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#21327;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#19982;&#20154;&#31867;&#39034;&#30021;&#21327;&#21516;&#30340;&#26234;&#33021;&#20195;&#29702;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#30446;&#21069;&#65292;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#30340;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#20195;&#29702;&#19982;&#22810;&#26679;&#21270;&#30340;&#31574;&#30053;&#25110;&#22522;&#20110;&#30495;&#23454;&#20154;&#31867;&#25968;&#25454;&#25311;&#21512;&#30340;&#20154;&#31867;&#27169;&#22411;&#36827;&#34892;&#21327;&#21516;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#34892;&#20026;&#30340;&#22823;&#35268;&#27169;&#22810;&#26679;&#24615;&#23545;&#20110;&#23481;&#37327;&#26377;&#38480;&#30340;AI&#31995;&#32479;&#26469;&#35828;&#26159;&#38556;&#30861;&#65292;&#32780;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#25968;&#25454;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21487;&#33021;&#19981;&#23481;&#26131;&#33719;&#21462;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21327;&#21516;&#20043;&#21069;&#65292;&#20154;&#31867;&#36827;&#34892;&#20132;&#27969;&#20197;&#24314;&#31435;&#35268;&#21017;&#32422;&#23450;&#65292;&#25351;&#23450;&#20010;&#20307;&#35282;&#33394;&#21644;&#34892;&#21160;&#65292;&#20351;&#20182;&#20204;&#30340;&#21327;&#21516;&#39034;&#21033;&#36827;&#34892;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#21046;&#23450;&#34892;&#21160;&#35745;&#21010;(&#25110;&#31561;&#25928;&#22320;&#65292;&#32422;&#23450;)&#65292;&#26377;&#25928;&#22320;&#25351;&#23548;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#36755;&#20837;&#20219;&#21153;&#35201;&#27714;&#12289;&#20154;&#31867;&#20559;&#22909;&#12289;&#20195;&#29702;&#25968;&#37327;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Developing intelligent agents capable of seamless coordination with humans is a critical step towards achieving artificial general intelligence. Existing methods for human-AI coordination typically train an agent to coordinate with a diverse set of policies or with human models fitted from real human data. However, the massively diverse styles of human behavior present obstacles for AI systems with constrained capacity, while high quality human data may not be readily available in real-world scenarios. In this study, we observe that prior to coordination, humans engage in communication to establish conventions that specify individual roles and actions, making their coordination proceed in an orderly manner. Building upon this observation, we propose employing the large language model (LLM) to develop an action plan (or equivalently, a convention) that effectively guides both human and AI. By inputting task requirements, human preferences, the number of agents, and other pertinent infor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20840;&#23556;&#27491;&#24120;&#21270;&#27969;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#35782;&#21035;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#21487;&#38752;&#22320;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00377</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#23556;&#27491;&#24120;&#21270;&#27969;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification and out-of-distribution detection using surjective normalizing flows. (arXiv:2311.00377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20840;&#23556;&#27491;&#24120;&#21270;&#27969;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#35782;&#21035;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#21487;&#38752;&#22320;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#29615;&#22659;&#19981;&#21516;&#30340;&#24212;&#29992;&#20013;&#65292;&#21487;&#38752;&#22320;&#37327;&#21270;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#22312;&#27668;&#20505;&#31185;&#23398;&#25110;&#31227;&#21160;&#24615;&#20998;&#26512;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20840;&#23556;&#27491;&#24120;&#21270;&#27969;&#26469;&#35782;&#21035;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#35745;&#31639;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#26368;&#36817;&#22312;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#21457;&#23637;&#22522;&#30784;&#19978;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20351;&#29992;&#20174;&#31227;&#21160;&#24615;&#25991;&#29486;&#30340;&#26426;&#26800;&#27169;&#22411;&#27169;&#25311;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#36719;&#12289;&#21407;&#23376;&#24178;&#39044;&#21518;&#24471;&#21040;&#30340;&#24178;&#39044;&#20998;&#24067;&#27169;&#25311;&#30340;&#20960;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#21306;&#20998;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#21644;&#20998;&#24067;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#20840;&#23556;&#27969;&#27169;&#22411;&#19982;Dirichlet&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable quantification of epistemic and aleatoric uncertainty is of crucial importance in applications where models are trained in one environment but applied to multiple different environments, often seen in real-world applications for example, in climate science or mobility analysis. We propose a simple approach using surjective normalizing flows to identify out-of-distribution data sets in deep neural network models that can be computed in a single forward pass. The method builds on recent developments in deep uncertainty quantification and generative modeling with normalizing flows. We apply our method to a synthetic data set that has been simulated using a mechanistic model from the mobility literature and several data sets simulated from interventional distributions induced by soft and atomic interventions on that model, and demonstrate that our method can reliably discern out-of-distribution data from in-distribution data. We compare the surjective flow model to a Dirichlet pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33521;&#29305;&#23572;Max&#31995;&#21015;GPU&#19978;&#23545;&#28145;&#24230;&#23398;&#20064;&#31232;&#30095;&#30697;&#38453;&#26680;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#33521;&#29305;&#23572;oneAPI&#30340;ESIMD SYCL&#25193;&#23637;API&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20248;&#21270;&#23454;&#29616;&#24182;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#31232;&#30095;&#30697;&#38453;&#25805;&#20316;&#65292;&#24615;&#33021;&#25509;&#36817;&#20110;GPU&#30340;&#23792;&#20540;&#24615;&#33021;&#65292;&#24182;&#22312;&#19982;&#33521;&#29305;&#23572;&#30340;oneMKL&#24211;&#21644;NVIDIA&#30340;V100 GPU&#19978;&#30340;CUDA&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#21518;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00368</link><description>&lt;p&gt;
&#33521;&#29305;&#23572;Max&#31995;&#21015;GPU&#19978;&#28145;&#24230;&#23398;&#20064;&#31232;&#30095;&#30697;&#38453;&#26680;&#30340;&#24615;&#33021;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Performance Optimization of Deep Learning Sparse Matrix Kernels on Intel Max Series GPU. (arXiv:2311.00368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33521;&#29305;&#23572;Max&#31995;&#21015;GPU&#19978;&#23545;&#28145;&#24230;&#23398;&#20064;&#31232;&#30095;&#30697;&#38453;&#26680;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#33521;&#29305;&#23572;oneAPI&#30340;ESIMD SYCL&#25193;&#23637;API&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20248;&#21270;&#23454;&#29616;&#24182;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#30340;&#31232;&#30095;&#30697;&#38453;&#25805;&#20316;&#65292;&#24615;&#33021;&#25509;&#36817;&#20110;GPU&#30340;&#23792;&#20540;&#24615;&#33021;&#65292;&#24182;&#22312;&#19982;&#33521;&#29305;&#23572;&#30340;oneMKL&#24211;&#21644;NVIDIA&#30340;V100 GPU&#19978;&#30340;CUDA&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#21518;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#19982;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30456;&#20851;&#30340;&#19977;&#20010;&#31232;&#30095;&#30697;&#38453;&#25805;&#20316;&#65306;&#31232;&#30095;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#65288;SPMM&#65289;&#65292;&#37319;&#26679;&#31264;&#23494;-&#31264;&#23494;&#30697;&#38453;&#20056;&#27861;&#65288;SDDMM&#65289;&#65292;&#20197;&#21450;SDDMM&#19982;SPMM&#30340;&#32452;&#21512;&#65288;FusedMM&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#33521;&#29305;&#23572;oneAPI&#30340;Explicit SIMD (ESIMD) SYCL&#25193;&#23637;API&#24320;&#21457;&#20102;SPMM&#12289;SDDMM&#21644;FusedMM&#25805;&#20316;&#30340;&#20248;&#21270;&#23454;&#29616;&#12290;&#19982;CUDA&#25110;SYCL&#30456;&#27604;&#65292;ESIMD API&#20801;&#35768;&#32534;&#20889;&#26174;&#24335;&#30690;&#37327;&#21270;&#30340;&#20869;&#26680;&#20195;&#30721;&#12290;&#20351;&#29992;ESIMD API&#23454;&#29616;&#30340;&#31232;&#30095;&#30697;&#38453;&#31639;&#27861;&#30340;&#24615;&#33021;&#25509;&#36817;&#20110;&#30446;&#26631;&#33521;&#29305;&#23572;&#25968;&#25454;&#20013;&#24515;GPU&#30340;&#23792;&#20540;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#24615;&#33021;&#32467;&#26524;&#19982;&#33521;&#29305;&#23572;&#30340;oneMKL&#24211;&#22312;&#33521;&#29305;&#23572;GPU&#19978;&#20197;&#21450;&#26368;&#36817;&#30340;&#36866;&#29992;&#20110;NVIDIA&#30340;V100 GPU&#30340;CUDA&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#31232;&#30095;&#30697;&#38453;&#25805;&#20316;&#23454;&#29616;&#20248;&#20110;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on three sparse matrix operations that are relevant for machine learning applications, namely, the sparse-dense matrix multiplication (SPMM), the sampled dense-dense matrix multiplication (SDDMM), and the composition of the SDDMM with SPMM, also termed as FusedMM. We develop optimized implementations for SPMM, SDDMM, and FusedMM operations utilizing Intel oneAPI's Explicit SIMD (ESIMD) SYCL extension API. In contrast to CUDA or SYCL, the ESIMD API enables the writing of explicitly vectorized kernel code. Sparse matrix algorithms implemented with the ESIMD API achieved performance close to the peak of the targeted Intel Data Center GPU. We compare our performance results to Intel's oneMKL library on Intel GPUs and to a recent CUDA implementation for the sparse matrix operations on NVIDIA's V100 GPU and demonstrate that our implementations for sparse matrix operations outperform either.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#37096;&#20998;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#23545;&#25239;&#40065;&#26834;&#30340;&#20998;&#24067;&#24335;&#35745;&#25968;&#36319;&#36394;&#65292;&#36890;&#36807;&#25506;&#31350;&#38543;&#26426;&#31639;&#27861;&#21644;&#30830;&#23450;&#31639;&#27861;&#23545;&#20110;&#36866;&#24212;&#24615;&#25932;&#25163;&#30340;&#40065;&#26834;&#24615;&#65292;&#25581;&#31034;&#20102;&#38543;&#26426;&#31639;&#27861;&#30340;$\sqrt{k}$&#20248;&#21183;&#26159;&#26469;&#28304;&#20110;&#38543;&#26426;&#24615;&#26412;&#36523;&#32780;&#19981;&#26159;&#26080;&#35270;&#25932;&#25163;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2311.00346</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#23545;&#25239;&#40065;&#26834;&#30340;&#20998;&#24067;&#24335;&#35745;&#25968;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Adversarially Robust Distributed Count Tracking via Partial Differential Privacy. (arXiv:2311.00346v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#37096;&#20998;&#24046;&#20998;&#38544;&#31169;&#23454;&#29616;&#23545;&#25239;&#40065;&#26834;&#30340;&#20998;&#24067;&#24335;&#35745;&#25968;&#36319;&#36394;&#65292;&#36890;&#36807;&#25506;&#31350;&#38543;&#26426;&#31639;&#27861;&#21644;&#30830;&#23450;&#31639;&#27861;&#23545;&#20110;&#36866;&#24212;&#24615;&#25932;&#25163;&#30340;&#40065;&#26834;&#24615;&#65292;&#25581;&#31034;&#20102;&#38543;&#26426;&#31639;&#27861;&#30340;$\sqrt{k}$&#20248;&#21183;&#26159;&#26469;&#28304;&#20110;&#38543;&#26426;&#24615;&#26412;&#36523;&#32780;&#19981;&#26159;&#26080;&#35270;&#25932;&#25163;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#36319;&#36394;&#27169;&#22411;&#65292;&#20063;&#34987;&#31216;&#20026;&#20998;&#24067;&#24335;&#21151;&#33021;&#30417;&#27979;&#12290;&#35813;&#27169;&#22411;&#28041;&#21450;&#21040;$k$&#20010;&#31449;&#28857;&#65292;&#27599;&#20010;&#31449;&#28857;&#25509;&#25910;&#19968;&#31995;&#21015;&#30340;&#39033;&#30446;&#24182;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36827;&#34892;&#36890;&#20449;&#12290;&#26381;&#21153;&#22120;&#30340;&#20219;&#21153;&#26159;&#20197;&#26368;&#23567;&#30340;&#36890;&#20449;&#25104;&#26412;&#36830;&#32493;&#36319;&#36394;&#21040;&#30446;&#21069;&#20026;&#27490;&#25509;&#25910;&#21040;&#30340;&#25152;&#26377;&#39033;&#30446;&#30340;&#20989;&#25968;&#12290;&#23545;&#20110;&#35745;&#25968;&#36319;&#36394;&#38382;&#39064;&#65292;&#24050;&#30693;&#30830;&#23450;&#24615;&#31639;&#27861;&#21644;&#38543;&#26426;&#31639;&#27861;&#20043;&#38388;&#23384;&#22312;$\sqrt{k}$&#30340;&#36890;&#20449;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38543;&#26426;&#31639;&#27861;&#20551;&#35774;"&#26080;&#35270;&#25932;&#25163;"&#22312;&#31639;&#27861;&#24320;&#22987;&#20043;&#21069;&#26500;&#36896;&#25972;&#20010;&#36755;&#20837;&#27969;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21487;&#20197;&#26681;&#25454;&#31639;&#27861;&#20043;&#21069;&#32473;&#20986;&#30340;&#31572;&#26696;&#36873;&#25321;&#26032;&#39033;&#30446;&#30340;&#36866;&#24212;&#24615;&#25932;&#25163;&#12290;&#30830;&#23450;&#24615;&#31639;&#27861;&#23545;&#20110;&#36866;&#24212;&#24615;&#25932;&#25163;&#26159;&#26174;&#28982;&#40065;&#26834;&#30340;&#65292;&#32780;&#38543;&#26426;&#31639;&#27861;&#21017;&#21487;&#33021;&#19981;&#26159;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#38543;&#26426;&#31639;&#27861;&#30340;$\sqrt{k}$&#20248;&#21183;&#26159;&#22240;&#20026;&#38543;&#26426;&#24615;&#26412;&#36523;&#36824;&#26159;&#22240;&#20026;&#26080;&#35270;&#25932;&#25163;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#32473;&#20986;&#32943;&#23450;&#30340;&#31572;&#26696;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#33719;&#24471;&#20174;&#38543;&#26426;&#24615;&#20986;&#21457;&#32780;&#19981;&#26159;&#20174;&#26080;&#35270;&#25932;&#25163;&#20551;&#35774;&#20986;&#21457;&#30340;$\sqrt{k}$&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distributed tracking model, also known as distributed functional monitoring. This model involves $k$ sites each receiving a stream of items and communicating with the central server. The server's task is to track a function of all items received thus far continuously, with minimum communication cost. For count tracking, it is known that there is a $\sqrt{k}$ gap in communication between deterministic and randomized algorithms. However, existing randomized algorithms assume an "oblivious adversary" who constructs the entire input streams before the algorithm starts. Here we consider adaptive adversaries who can choose new items based on previous answers from the algorithm. Deterministic algorithms are trivially robust to adaptive adversaries, while randomized ones may not. Therefore, we investigate whether the $\sqrt{k}$ advantage of randomized algorithms is from randomness itself or the oblivious adversary assumption. We provide an affirmative answer to this question by gi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21019;&#26032;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;8,800&#20010;MOF&#26448;&#26009;&#30340;&#36229;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#30452;&#25509;&#31354;&#27668;&#25429;&#38598;&#20013;&#26377;&#21069;&#36884;&#30340;&#21560;&#38468;&#21058;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2311.00341</link><description>&lt;p&gt;
&#12298;2023&#24180;&#24320;&#25918;&#24335;DAC&#25968;&#25454;&#38598;&#21644;&#30452;&#25509;&#31354;&#27668;&#25429;&#38598;&#20013;&#21560;&#38468;&#21058;&#21457;&#29616;&#30340;&#25361;&#25112;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Open DAC 2023 Dataset and Challenges for Sorbent Discovery in Direct Air Capture. (arXiv:2311.00341v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21019;&#26032;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;8,800&#20010;MOF&#26448;&#26009;&#30340;&#36229;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#30452;&#25509;&#31354;&#27668;&#25429;&#38598;&#20013;&#26377;&#21069;&#36884;&#30340;&#21560;&#38468;&#21058;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#38656;&#26032;&#30340;&#20108;&#27687;&#21270;&#30899;&#21435;&#38500;&#26041;&#27861;&#26469;&#24212;&#23545;&#20840;&#29699;&#27668;&#20505;&#21464;&#21270;&#12290;&#30452;&#25509;&#31354;&#27668;&#25429;&#38598;(DAC)&#26159;&#19968;&#31181;&#20174;&#29615;&#22659;&#31354;&#27668;&#20013;&#30452;&#25509;&#25429;&#38598;&#20108;&#27687;&#21270;&#30899;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#37329;&#23646;&#26377;&#26426;&#26694;&#26550;(MOFs)&#34987;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;DAC&#30340;&#28508;&#22312;&#21487;&#23450;&#21046;&#21560;&#38468;&#21058;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#25506;&#32034;&#30340;&#24040;&#22823;&#21270;&#23398;&#31354;&#38388;&#21644;&#38656;&#35201;&#20102;&#35299;&#26448;&#26009;&#38543;&#28287;&#24230;&#21644;&#28201;&#24230;&#21464;&#21270;&#30340;&#29305;&#24615;&#65292;&#21457;&#29616;&#26377;&#21069;&#36884;&#30340;DAC MOF&#21560;&#38468;&#21058;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#21019;&#26032;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Open DAC 2023 (ODAC23)&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;8,800&#20010;&#21547;&#26377;&#21560;&#38468;CO2&#21644;/&#25110;H2O&#30340;MOF&#26448;&#26009;&#30340;&#36229;&#36807;3800&#19975;&#30340;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#35745;&#31639;&#12290; ODAC23&#26159;&#30446;&#21069;&#21487;&#29992;&#31934;&#30830;&#24230;&#30340;DFT&#32423;&#21035;&#20013;&#26368;&#22823;&#30340;MOF&#21560;&#38468;&#35745;&#31639;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#25506;&#32034;&#21560;&#38468;&#20998;&#23376;&#30340;&#24615;&#36136;&#22806;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#36824;&#26159;&#20449;&#24687;&#30340;&#20016;&#23500;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
New methods for carbon dioxide removal are urgently needed to combat global climate change. Direct air capture (DAC) is an emerging technology to capture carbon dioxide directly from ambient air. Metal-organic frameworks (MOFs) have been widely studied as potentially customizable adsorbents for DAC. However, discovering promising MOF sorbents for DAC is challenging because of the vast chemical space to explore and the need to understand materials as functions of humidity and temperature. We explore a computational approach benefiting from recent innovations in machine learning (ML) and present a dataset named Open DAC 2023 (ODAC23) consisting of more than 38M density functional theory (DFT) calculations on more than 8,800 MOF materials containing adsorbed CO2 and/or H2O. ODAC23 is by far the largest dataset of MOF adsorption calculations at the DFT level of accuracy currently available. In addition to probing properties of adsorbed molecules, the dataset is a rich source of information
&lt;/p&gt;</description></item><item><title>MetisFL&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#32852;&#37030;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2311.00334</link><description>&lt;p&gt;
MetisFL:&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#24037;&#20316;&#27969;&#30340;&#23604;&#23596;&#24182;&#34892;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
MetisFL: An Embarrassingly Parallelized Controller for Scalable &amp; Efficient Federated Learning Workflows. (arXiv:2311.00334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00334
&lt;/p&gt;
&lt;p&gt;
MetisFL&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#32852;&#37030;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#31995;&#32479;&#36890;&#24120;&#30001;&#20004;&#20010;&#26680;&#24515;&#22788;&#29702;&#23454;&#20307;&#32452;&#25104;:&#32852;&#37030;&#25511;&#21046;&#22120;&#21644;&#23398;&#20064;&#22120;&#12290;&#25511;&#21046;&#22120;&#36127;&#36131;&#31649;&#29702;&#22312;&#23398;&#20064;&#22120;&#20043;&#38388;&#25191;&#34892;FL&#24037;&#20316;&#27969;&#31243;&#65292;&#23398;&#20064;&#22120;&#36127;&#36131;&#22312;&#20854;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#32852;&#37030;&#27169;&#22411;&#12290;&#22312;&#25191;&#34892;FL&#24037;&#20316;&#27969;&#26102;&#65292;FL&#31995;&#32479;&#23545;&#21442;&#19982;&#23398;&#20064;&#22120;&#30340;&#35745;&#31639;&#36164;&#28304;&#25110;&#25968;&#25454;&#27809;&#26377;&#25511;&#21046;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;FL&#31995;&#32479;&#26469;&#20419;&#36827;FL&#24037;&#20316;&#27969;&#30340;&#24320;&#21457;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#20013;&#22823;&#22810;&#25968;&#24573;&#35270;&#20102;&#25511;&#21046;&#22120;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;MetisFL&#30340;&#26032;&#22411;FL&#31995;&#32479;&#65292;&#20854;&#20013;&#32852;&#37030;&#25511;&#21046;&#22120;&#26159;&#31532;&#19968;&#31561;&#20844;&#27665;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Federated Learning (FL) system typically consists of two core processing entities: the federation controller and the learners. The controller is responsible for managing the execution of FL workflows across learners and the learners for training and evaluating federated models over their private datasets. While executing an FL workflow, the FL system has no control over the computational resources or data of the participating learners. Still, it is responsible for other operations, such as model aggregation, task dispatching, and scheduling. These computationally heavy operations generally need to be handled by the federation controller. Even though many FL systems have been recently proposed to facilitate the development of FL workflows, most of these systems overlook the scalability of the controller. To meet this need, we designed and developed a novel FL system called MetisFL, where the federation controller is the first-class citizen. MetisFL re-engineers all the operations cond
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#31354;&#38388;&#36716;&#24405;&#32452;&#23398;&#25968;&#25454;&#26144;&#23556;&#21040;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#20013;&#65292;&#20197;&#33719;&#21462;&#32452;&#32455;&#26679;&#26412;&#30340;&#23436;&#25972;&#36951;&#20256;&#34920;&#36798;&#20449;&#24687;&#65292;&#24182;&#20445;&#30041;&#31354;&#38388;&#22352;&#26631;&#12290;&#36825;&#20026;&#28145;&#20837;&#29702;&#35299;&#32454;&#32990;&#36807;&#31243;&#21644;&#36890;&#36335;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2311.00330</link><description>&lt;p&gt;
&#31354;&#38388;&#36716;&#24405;&#32452;&#30340;&#28508;&#22312;&#31354;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Latent Space Inference For Spatial Transcriptomics. (arXiv:2311.00330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#31354;&#38388;&#36716;&#24405;&#32452;&#23398;&#25968;&#25454;&#26144;&#23556;&#21040;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#20013;&#65292;&#20197;&#33719;&#21462;&#32452;&#32455;&#26679;&#26412;&#30340;&#23436;&#25972;&#36951;&#20256;&#34920;&#36798;&#20449;&#24687;&#65292;&#24182;&#20445;&#30041;&#31354;&#38388;&#22352;&#26631;&#12290;&#36825;&#20026;&#28145;&#20837;&#29702;&#35299;&#32454;&#32990;&#36807;&#31243;&#21644;&#36890;&#36335;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#32454;&#32990;&#29983;&#29289;&#23398;&#30340;&#22797;&#26434;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#32454;&#32990;&#30340;&#36951;&#20256;&#34920;&#36798;&#20449;&#24687;&#21644;&#20854;&#22312;&#32452;&#32455;&#26679;&#26412;&#20013;&#30340;&#31354;&#38388;&#22352;&#26631;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#21363;&#21333;&#32454;&#32990;RNA&#27979;&#24207;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#31354;&#38388;&#36716;&#24405;&#32452;&#23398;&#65292;&#21482;&#33021;&#24674;&#22797;&#36825;&#20123;&#20449;&#24687;&#30340;&#23376;&#38598;&#65292;&#35201;&#20040;&#23436;&#25972;&#30340;&#36951;&#20256;&#34920;&#36798;&#20449;&#24687;&#20002;&#22833;&#20102;&#31354;&#38388;&#20449;&#24687;&#65292;&#35201;&#20040;&#31354;&#38388;&#20449;&#24687;&#20002;&#22833;&#20102;&#27979;&#24207;&#25968;&#25454;&#30340;&#20998;&#36776;&#29575;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#33719;&#21462;&#32452;&#32455;&#26679;&#26412;&#30340;&#23436;&#25972;&#36951;&#20256;&#34920;&#36798;&#20449;&#24687;&#65292;&#24182;&#20445;&#30041;&#23427;&#20204;&#30340;&#31354;&#38388;&#22352;&#26631;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#20004;&#20010;&#25968;&#25454;&#38598;&#26144;&#23556;&#21040;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#20013;&#26469;&#23454;&#29616;&#30340;&#12290;&#20174;&#36825;&#37324;&#65292;&#21487;&#20197;&#35299;&#30721;&#20986;&#23436;&#25972;&#30340;&#36951;&#20256;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#20197;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#32454;&#32990;&#36807;&#31243;&#21644;&#36890;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to understand the complexities of cellular biology, researchers are interested in two important metrics: the genetic expression information of cells and their spatial coordinates within a tissue sample. However, state-of-the art methods, namely single-cell RNA sequencing and image based spatial transcriptomics can only recover a subset of this information, either full genetic expression with loss of spatial information, or spatial information with loss of resolution in sequencing data. In this project, we investigate a probabilistic machine learning method to obtain the full genetic expression information for tissues samples while also preserving their spatial coordinates. This is done through mapping both datasets to a joint latent space representation with the use of variational machine learning methods. From here, the full genetic and spatial information can be decoded and to give us greater insights on the understanding of cellular processes and pathways.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#35299;&#20915;&#20102;&#21452;&#32447;&#24615;bandit&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;GOBLIN&#31639;&#27861;&#26469;&#20248;&#21270;&#26679;&#26412;&#20998;&#37197;&#21644;&#20943;&#23567;&#26679;&#26412;&#25968;&#37327;&#65292;&#22312;&#20849;&#20139;&#34920;&#31034;&#19979;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.00327</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#21452;&#32447;&#24615;bandit&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-task Representation Learning for Pure Exploration in Bilinear Bandits. (arXiv:2311.00327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#35299;&#20915;&#20102;&#21452;&#32447;&#24615;bandit&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;GOBLIN&#31639;&#27861;&#26469;&#20248;&#21270;&#26679;&#26412;&#20998;&#37197;&#21644;&#20943;&#23567;&#26679;&#26412;&#25968;&#37327;&#65292;&#22312;&#20849;&#20139;&#34920;&#31034;&#19979;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#22312;&#21452;&#32447;&#24615;bandit&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#21452;&#32447;&#24615;bandit&#20013;&#65292;&#19968;&#20010;&#21160;&#20316;&#30001;&#26469;&#33258;&#20004;&#31181;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#30340;&#20004;&#20010;&#33218;&#26500;&#25104;&#65292;&#22870;&#21169;&#26159;&#20004;&#20010;&#33218;&#30340;&#24050;&#30693;&#29305;&#24449;&#21521;&#37327;&#30340;&#21452;&#32447;&#24615;&#20989;&#25968;&#12290;&#22312;&#22810;&#20219;&#21153;&#21452;&#32447;&#24615;bandit&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23547;&#25214;&#22810;&#20010;&#20219;&#21153;&#30340;&#26368;&#20339;&#21160;&#20316;&#65292;&#36825;&#20123;&#20219;&#21153;&#20849;&#20139;&#19968;&#20010;&#20849;&#21516;&#30340;&#20302;&#32500;&#32447;&#24615;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#36825;&#20010;&#29305;&#24449;&#21152;&#36895;&#30830;&#23450;&#25152;&#26377;&#20219;&#21153;&#30340;&#26368;&#20339;&#33218;&#23545;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#27861;GOBLIN&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#26469;&#20248;&#21270;&#23398;&#20064;&#20840;&#23616;&#34920;&#31034;&#30340;&#26679;&#26412;&#20998;&#37197;&#65292;&#24182;&#26368;&#23567;&#21270;&#35782;&#21035;&#20010;&#21035;&#20219;&#21153;&#20013;&#26368;&#20339;&#33218;&#23545;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#22312;&#20849;&#20139;&#34920;&#31034;&#19979;&#23545;&#21452;&#32447;&#24615;bandit&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#36827;&#34892;&#26679;&#26412;&#22797;&#26434;&#24615;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
We study multi-task representation learning for the problem of pure exploration in bilinear bandits. In bilinear bandits, an action takes the form of a pair of arms from two different entity types and the reward is a bilinear function of the known feature vectors of the arms. In the \textit{multi-task bilinear bandit problem}, we aim to find optimal actions for multiple tasks that share a common low-dimensional linear representation. The objective is to leverage this characteristic to expedite the process of identifying the best pair of arms for all tasks. We propose the algorithm GOBLIN that uses an experimental design approach to optimize sample allocations for learning the global representation as well as minimize the number of samples needed to identify the optimal pair of arms in individual tasks. To the best of our knowledge, this is the first study to give sample complexity analysis for pure exploration in bilinear bandits with shared representation. Our results demonstrate that
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22522;&#20110;&#20803;&#26435;&#37325;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33410;&#28857;&#23545;&#24212;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2311.00322</link><description>&lt;p&gt;
&#22122;&#22768;&#22270;&#20013;&#30340;&#40065;&#26834;&#22270;&#32858;&#31867;&#36890;&#36807;&#20803;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Robust Graph Clustering via Meta Weighting for Noisy Graphs. (arXiv:2311.00322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22522;&#20110;&#20803;&#26435;&#37325;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33410;&#28857;&#23545;&#24212;&#26435;&#37325;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#33021;&#22815;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#20013;&#25214;&#21040;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#22312;&#22122;&#22768;&#36793;&#19978;&#40065;&#26834;&#22320;&#25214;&#21040;&#22270;&#20013;&#30340;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#65311;&#22270;&#32858;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#22312;&#22270;&#32858;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#23384;&#22312;&#22122;&#22768;&#36793;&#30340;&#22270;&#19978;&#30340;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#40065;&#26834;GNN-based&#22270;&#32858;&#31867;&#30340;MetaGC&#12290;MetaGC&#37319;&#29992;&#21487;&#20998;&#35299;&#30340;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#20854;&#37325;&#26032;&#34920;&#36848;&#20026;&#33410;&#28857;&#23545;&#20043;&#38388;&#25439;&#22833;&#30340;&#27714;&#21644;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#33410;&#28857;&#23545;&#28155;&#21152;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#65292;&#24182;&#20351;&#29992;&#20803;&#26435;&#37325;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#33410;&#28857;&#23545;&#30340;&#26435;&#37325;&#65292;&#20351;&#26377;&#24847;&#20041;&#30340;&#33410;&#28857;&#23545;&#30340;&#26435;&#37325;&#22686;&#21152;&#65292;&#32780;&#19981;&#37027;&#20040;&#26377;&#24847;&#20041;&#30340;&#33410;&#28857;&#23545;&#65288;&#20363;&#22914;&#22122;&#22768;&#36793;&#65289;&#30340;&#26435;&#37325;&#20943;&#23567;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;MetaGC&#25353;&#29031;&#39044;&#26399;&#23398;&#20064;&#26435;&#37325;&#65292;&#24182;&#19988;&#22240;&#27492;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we find meaningful clusters in a graph robustly against noise edges? Graph clustering (i.e., dividing nodes into groups of similar ones) is a fundamental problem in graph analysis with applications in various fields. Recent studies have demonstrated that graph neural network (GNN) based approaches yield promising results for graph clustering. However, we observe that their performance degenerates significantly on graphs with noise edges, which are prevalent in practice. In this work, we propose MetaGC for robust GNN-based graph clustering. MetaGC employs a decomposable clustering loss function, which can be rephrased as a sum of losses over node pairs. We add a learnable weight to each node pair, and MetaGC adaptively adjusts the weights of node pairs using meta-weighting so that the weights of meaningful node pairs increase and the weights of less-meaningful ones (e.g., noise edges) decrease. We show empirically that MetaGC learns weights as intended and consequently outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;&#21548;&#21147;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#24102;&#26377;&#21452;&#32819;&#21548;&#21151;&#33021;&#30340;&#21548;&#35273;&#35774;&#22791;&#65292;&#33021;&#22815;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23454;&#26102;&#20851;&#27880;&#25110;&#24573;&#30053;&#29305;&#23450;&#30340;&#22768;&#38899;&#65292;&#24182;&#20445;&#25345;&#31354;&#38388;&#32447;&#32034;&#12290;&#30740;&#31350;&#36129;&#29486;&#21253;&#25324;&#39318;&#20010;&#33021;&#22815;&#22312;&#24178;&#25200;&#22768;&#21644;&#32972;&#26223;&#22122;&#38899;&#20013;&#23454;&#29616;&#21452;&#32819;&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#21487;&#25512;&#24191;&#21040;&#23454;&#38469;&#20351;&#29992;&#22330;&#26223;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#31995;&#32479;&#22312;&#22810;&#31181;&#22768;&#38899;&#31867;&#21035;&#19979;&#30340;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#25968;&#25454;&#65292;&#24182;&#22312;&#8220;&#37326;&#22806;&#8221;&#35780;&#20272;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00320</link><description>&lt;p&gt;
&#24102;&#26377;&#21452;&#32819;&#21548;&#21151;&#33021;&#30340;&#35821;&#20041;&#21548;&#21147;&#65306;&#32534;&#31243;&#22768;&#23398;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Semantic Hearing: Programming Acoustic Scenes with Binaural Hearables. (arXiv:2311.00320v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;&#21548;&#21147;&#30340;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#24102;&#26377;&#21452;&#32819;&#21548;&#21151;&#33021;&#30340;&#21548;&#35273;&#35774;&#22791;&#65292;&#33021;&#22815;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23454;&#26102;&#20851;&#27880;&#25110;&#24573;&#30053;&#29305;&#23450;&#30340;&#22768;&#38899;&#65292;&#24182;&#20445;&#25345;&#31354;&#38388;&#32447;&#32034;&#12290;&#30740;&#31350;&#36129;&#29486;&#21253;&#25324;&#39318;&#20010;&#33021;&#22815;&#22312;&#24178;&#25200;&#22768;&#21644;&#32972;&#26223;&#22122;&#38899;&#20013;&#23454;&#29616;&#21452;&#32819;&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#21487;&#25512;&#24191;&#21040;&#23454;&#38469;&#20351;&#29992;&#22330;&#26223;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#31995;&#32479;&#22312;&#22810;&#31181;&#22768;&#38899;&#31867;&#21035;&#19979;&#30340;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#25968;&#25454;&#65292;&#24182;&#22312;&#8220;&#37326;&#22806;&#8221;&#35780;&#20272;&#20013;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24819;&#35937;&#19968;&#19979;&#65292;&#22312;&#20844;&#22253;&#37324;&#21487;&#20197;&#21548;&#21040;&#40479;&#20799;&#22312;&#40483;&#21483;&#65292;&#32780;&#19981;&#20250;&#21548;&#21040;&#20854;&#20182;&#24466;&#27493;&#32773;&#30340;&#32850;&#22122;&#22768;&#65307;&#25110;&#32773;&#22312;&#32321;&#24537;&#30340;&#34903;&#36947;&#19978;&#21487;&#20197;&#23631;&#34109;&#20132;&#36890;&#22122;&#38899;&#65292;&#21516;&#26102;&#20173;&#28982;&#33021;&#21548;&#21040;&#32039;&#24613;&#35686;&#31515;&#21644;&#27773;&#36710;&#40483;&#31515;&#22768;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#35821;&#20041;&#21548;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21548;&#35273;&#35774;&#22791;&#21151;&#33021;&#65292;&#33021;&#22815;&#23454;&#26102;&#22320;&#20851;&#27880;&#25110;&#24573;&#30053;&#30495;&#23454;&#29615;&#22659;&#20013;&#29305;&#23450;&#30340;&#22768;&#38899;&#65292;&#21516;&#26102;&#20445;&#30041;&#31354;&#38388;&#32447;&#32034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#25216;&#26415;&#36129;&#29486;&#65306;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#24178;&#25200;&#22768;&#21644;&#32972;&#26223;&#22122;&#38899;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21452;&#32819;&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;2&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#25512;&#24191;&#21040;&#23454;&#38469;&#30340;&#20351;&#29992;&#22330;&#26223;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#22788;&#29702;20&#20010;&#22768;&#38899;&#31867;&#21035;&#65292;&#24182;&#19988;&#25105;&#20204;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#22312;&#36830;&#25509;&#30340;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#20026;6.56&#27627;&#31186;&#12290;&#22312;&#26410;&#30693;&#30340;&#23460;&#20869;&#22806;&#22330;&#26223;&#20013;&#30340;&#8220;&#37326;&#22806;&#8221;&#35780;&#20272;&#20013;&#65292;&#21442;&#19982;&#32773;&#30340;&#27979;&#35797;&#26174;&#31034;&#25105;&#20204;&#30340;&#31995;&#32479;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine being able to listen to the birds chirping in a park without hearing the chatter from other hikers, or being able to block out traffic noise on a busy street while still being able to hear emergency sirens and car honks. We introduce semantic hearing, a novel capability for hearable devices that enables them to, in real-time, focus on, or ignore, specific sounds from real-world environments, while also preserving the spatial cues. To achieve this, we make two technical contributions: 1) we present the first neural network that can achieve binaural target sound extraction in the presence of interfering sounds and background noise, and 2) we design a training methodology that allows our system to generalize to real-world use. Results show that our system can operate with 20 sound classes and that our transformer-based network has a runtime of 6.56 ms on a connected smartphone. In-the-wild evaluation with participants in previously unseen indoor and outdoor scenarios shows that ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30452;&#25509;&#23545;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#24212;&#29992;&#27867;&#21270;&#26041;&#27861;&#38450;&#27490;&#21028;&#21035;&#22120;&#30340;&#25439;&#22833;&#36807;&#20998;&#38477;&#20302;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00318</link><description>&lt;p&gt;
&#29992;&#20110;&#31283;&#23450;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27867;&#21270;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Flooding Regularization for Stable Training of Generative Adversarial Networks. (arXiv:2311.00318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30452;&#25509;&#23545;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#36890;&#36807;&#24212;&#29992;&#27867;&#21270;&#26041;&#27861;&#38450;&#27490;&#21028;&#21035;&#22120;&#30340;&#25439;&#22833;&#36807;&#20998;&#38477;&#20302;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GAN&#35757;&#32451;&#23384;&#22312;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#26159;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#24120;&#20351;&#29992;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#21464;&#23545;&#25239;&#25439;&#22833;&#30340;&#31867;&#22411;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#30452;&#25509;&#23545;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#27867;&#21270;&#65288;&#36807;&#25311;&#21512;&#25233;&#21046;&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;GANs&#20013;&#65292;&#30452;&#25509;&#38450;&#27490;&#21028;&#21035;&#22120;&#30340;&#25439;&#22833;&#36807;&#20998;&#38477;&#20302;&#12290;&#27867;&#21270;&#38656;&#35201;&#35843;&#25972;&#27867;&#21270;&#27700;&#24179;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;GANs&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#36866;&#24403;&#30340;&#27867;&#21270;&#27700;&#24179;&#35774;&#32622;&#33539;&#22260;&#30001;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#30830;&#23450;&#65292;&#35813;&#35770;&#25454;&#24471;&#21040;&#20102;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#23545;GANs&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#27867;&#21270;&#31283;&#23450;&#20102;GAN&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20854;&#20182;&#31283;&#23450;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#36890;&#36807;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have shown remarkable performance in image generation. However, GAN training suffers from the problem of instability. One of the main approaches to address this problem is to modify the loss function, often using regularization terms in addition to changing the type of adversarial losses. This paper focuses on directly regularizing the adversarial loss function. We propose a method that applies flooding, an overfitting suppression method in supervised learning, to GANs to directly prevent the discriminator's loss from becoming excessively low. Flooding requires tuning the flood level, but when applied to GANs, we propose that the appropriate range of flood level settings is determined by the adversarial loss function, supported by theoretical analysis of GANs using the binary cross entropy loss. We experimentally verify that flooding stabilizes GAN training and can be combined with other stabilization techniques. We also reveal that by restricting
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20195;&#30721;&#32763;&#35793;&#12290;&#36890;&#36807;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#21644;&#22686;&#21152;&#22810;&#20010;&#21442;&#32771;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;CodeT5&#22312;Java&#12289;Python&#21644;C++&#20043;&#38388;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00317</link><description>&lt;p&gt;
&#29992;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#21644;&#22810;&#20010;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Code Translation with Comparable Corpora and Multiple References. (arXiv:2311.00317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20195;&#30721;&#32763;&#35793;&#12290;&#36890;&#36807;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#21644;&#22686;&#21152;&#22810;&#20010;&#21442;&#32771;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;CodeT5&#22312;Java&#12289;Python&#21644;C++&#20043;&#38388;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#24179;&#34892;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#19968;&#31181;&#26159;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#65288;&#21363;&#20855;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#20195;&#30721;&#23545;&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#29992;&#22810;&#20010;&#21442;&#32771;&#32763;&#35793;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#24179;&#34892;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#20998;&#26512;&#20102;&#22810;&#31181;&#31867;&#22411;&#30340;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26723;&#20013;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#23545;&#21333;&#20010;&#21442;&#32771;&#32763;&#35793;&#30340;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#33258;&#21160;&#29983;&#25104;&#20102;&#21487;&#29992;&#24179;&#34892;&#25968;&#25454;&#30340;&#39069;&#22806;&#32763;&#35793;&#21442;&#32771;&#65292;&#24182;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#23545;&#32763;&#35793;&#36827;&#34892;&#31579;&#36873;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30446;&#26631;&#32763;&#35793;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26174;&#33879;&#25552;&#39640;&#20102;CodeT5&#22312;Java&#12289;Python&#21644;C++&#20043;&#38388;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#65288;&#24179;&#22343;&#25552;&#21319;&#20102;7.5%&#30340;&#35745;&#31639;&#20934;&#30830;&#24615;&#65288;CA@1&#65289;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32852;&#37030;&#20027;&#39064;&#27169;&#22411;&#21644;&#27169;&#22411;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#22810;&#20010;&#26041;&#21442;&#19982;&#20132;&#21449;&#20998;&#26512;&#26102;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21098;&#26525;&#21152;&#36895;&#27169;&#22411;&#12290;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#30830;&#23450;&#27169;&#22411;&#21098;&#26525;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.00314</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32852;&#37030;&#20027;&#39064;&#27169;&#22411;&#21644;&#27169;&#22411;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Federated Topic Model and Model Pruning Based on Variational Autoencoder. (arXiv:2311.00314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32852;&#37030;&#20027;&#39064;&#27169;&#22411;&#21644;&#27169;&#22411;&#21098;&#26525;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36328;&#22810;&#20010;&#26041;&#21442;&#19982;&#20132;&#21449;&#20998;&#26512;&#26102;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21098;&#26525;&#21152;&#36895;&#27169;&#22411;&#12290;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#30830;&#23450;&#27169;&#22411;&#21098;&#26525;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#24050;&#32463;&#25104;&#20026;&#22312;&#22823;&#35268;&#27169;&#25991;&#26723;&#38598;&#21512;&#20013;&#21457;&#29616;&#27169;&#24335;&#21644;&#20027;&#39064;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#36328;&#22810;&#20010;&#26041;&#21442;&#19982;&#20132;&#21449;&#20998;&#26512;&#26102;&#65292;&#25968;&#25454;&#38544;&#31169;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#32852;&#37030;&#20027;&#39064;&#24314;&#27169;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#22330;&#26223;&#20013;&#23384;&#22312;&#36890;&#20449;&#21644;&#24615;&#33021;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#32852;&#37030;&#20027;&#39064;&#27169;&#22411;&#24182;&#30830;&#20445;&#27599;&#20010;&#33410;&#28857;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21098;&#26525;&#21152;&#36895;&#27169;&#22411;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#23450;&#26399;&#23558;&#27169;&#22411;&#31070;&#32463;&#20803;&#32047;&#31215;&#26799;&#24230;&#21644;&#27169;&#22411;&#26435;&#37325;&#21457;&#36865;&#32473;&#26381;&#21153;&#22120;&#65292;&#26381;&#21153;&#22120;&#23545;&#27169;&#22411;&#36827;&#34892;&#21098;&#26525;&#12290;&#20026;&#20102;&#28385;&#36275;&#19981;&#21516;&#30340;&#35201;&#27714;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#30830;&#23450;&#27169;&#22411;&#21098;&#26525;&#29575;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic modeling has emerged as a valuable tool for discovering patterns and topics within large collections of documents. However, when cross-analysis involves multiple parties, data privacy becomes a critical concern. Federated topic modeling has been developed to address this issue, allowing multiple parties to jointly train models while protecting pri-vacy. However, there are communication and performance challenges in the federated sce-nario. In order to solve the above problems, this paper proposes a method to establish a federated topic model while ensuring the privacy of each node, and use neural network model pruning to accelerate the model, where the client periodically sends the model neu-ron cumulative gradients and model weights to the server, and the server prunes the model. To address different requirements, two different methods are proposed to determine the model pruning rate. The first method involves slow pruning throughout the entire model training process, which has 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#26041;&#26696;&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#21644;&#24494;&#35843;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#38646;&#26085;&#23041;&#32961;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31867;&#25915;&#20987;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2311.00304</link><description>&lt;p&gt;
&#20351;&#29992;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#38646;&#26085;&#23041;&#32961;&#30340;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Stacking an autoencoder for feature selection of zero-day threats. (arXiv:2311.00304v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#26041;&#26696;&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#21644;&#24494;&#35843;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#38646;&#26085;&#23041;&#32961;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#31867;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31867;&#25915;&#20987;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#33021;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26085;&#25915;&#20987;&#26816;&#27979;&#22312;&#32531;&#35299;&#39118;&#38505;&#12289;&#20445;&#25252;&#36164;&#20135;&#21644;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#23041;&#32961;&#29615;&#22659;&#20013;&#20445;&#25345;&#39046;&#20808;&#22320;&#20301;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#65288;SAE&#65289;&#65292;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#21644;&#38646;&#26085;&#23041;&#32961;&#20998;&#31867;&#65292;&#37319;&#29992;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26041;&#26696;&#12290;&#35813;&#36807;&#31243;&#21253;&#25324;&#23545;UGRansome&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#24182;&#35757;&#32451;&#26080;&#30417;&#30563;&#30340;SAE&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#36827;&#34892;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#35813;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#20998;&#26512;&#33258;&#32534;&#30721;&#22120;&#30340;&#23398;&#20064;&#26435;&#37325;&#21644;&#28608;&#27963;&#26469;&#35782;&#21035;&#21306;&#20998;&#38646;&#26085;&#23041;&#32961;&#21644;&#27491;&#24120;&#31995;&#32479;&#34892;&#20026;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#12290;&#36825;&#20123;&#36873;&#23450;&#30340;&#29305;&#24449;&#24418;&#25104;&#20102;&#19968;&#20010;&#38477;&#32500;&#29305;&#24449;&#38598;&#65292;&#33021;&#22815;&#23454;&#29616;&#20934;&#30830;&#30340;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SAE-LSTM&#22312;&#25152;&#26377;&#19977;&#31867;&#25915;&#20987;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23637;&#31034;&#20102;&#39640;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#20540;&#65292;&#24378;&#35843;&#20102;&#20854;&#21019;&#26032;&#24615;&#21644;&#36129;&#29486;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-day attack detection plays a critical role in mitigating risks, protecting assets, and staying ahead in the evolving threat landscape. This study explores the application of stacked autoencoder (SAE), a type of artificial neural network, for feature selection and zero-day threat classification using a Long Short-Term Memory (LSTM) scheme. The process involves preprocessing the UGRansome dataset and training an unsupervised SAE for feature extraction. Finetuning with supervised learning is then performed to enhance the discriminative capabilities of this model. The learned weights and activations of the autoencoder are analyzed to identify the most important features for discriminating between zero-day threats and normal system behavior. These selected features form a reduced feature set that enables accurate classification. The results indicate that the SAE-LSTM performs well across all three attack categories by showcasing high precision, recall, and F1 score values, emphasizing 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#22320;&#19979;CO2&#27844;&#28431;&#36827;&#34892;&#30417;&#27979;&#21644;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;CO2&#27969;&#21160;&#27169;&#24335;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00290</link><description>&lt;p&gt;
CO2&#27969;&#21160;&#27169;&#24335;&#30340;&#25512;&#26029;--&#21487;&#34892;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inference of CO2 flow patterns -- a feasibility study. (arXiv:2311.00290v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00290
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#22320;&#19979;CO2&#27844;&#28431;&#36827;&#34892;&#30417;&#27979;&#21644;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25551;&#36848;CO2&#27969;&#21160;&#27169;&#24335;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20840;&#29699;&#30899;&#25429;&#33719;&#21644;&#23553;&#23384;&#65288;CCS&#65289;&#25216;&#26415;&#22312;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#26007;&#20105;&#20013;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#65292;&#24314;&#31435;&#31283;&#20581;&#30340;&#30417;&#27979;&#21644;&#26816;&#27979;&#26426;&#21046;&#65292;&#20197;&#26816;&#27979;&#28508;&#22312;&#30340;&#22320;&#19979;CO2&#27844;&#28431;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#23384;&#20648;&#24211;&#23553;&#22581;&#30340;&#39044;&#20808;&#23384;&#22312;&#25110;&#35825;&#23548;&#30340;&#26029;&#23618;&#65292;&#21464;&#24471;&#36234;&#26469;&#36234;&#36843;&#20999;&#12290;&#34429;&#28982;&#35832;&#22914;&#21382;&#21490;&#21305;&#37197;&#21644;CO2&#20648;&#23384;&#30340;&#26102;&#38388;&#24207;&#21015;&#22320;&#38663;&#30417;&#27979;&#31561;&#25216;&#26415;&#24050;&#25104;&#21151;&#29992;&#20110;&#36319;&#36394;&#22320;&#19979;CO2&#28183;&#28431;&#30340;&#28436;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#23545;CO2&#27874;&#21160;&#34892;&#20026;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#26412;&#26041;&#27861;&#12290;&#31995;&#32479;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#32435;&#20837;&#23545;&#20110;&#39118;&#38505;&#32531;&#35299;&#33267;&#20851;&#37325;&#35201;&#65292;&#21407;&#22240;&#22914;&#19979;&#65306;&#65288;i&#65289;CO2&#27874;&#21160;&#35825;&#21457;&#30340;&#21464;&#21270;&#24456;&#23567;&#19988;&#22320;&#38663;&#25968;&#25454;&#22122;&#22768;&#24456;&#22823;&#65307;&#65288;ii&#65289;&#27491;&#24120;&#21644;&#19981;&#35268;&#21017;&#65288;&#20363;&#22914;&#27844;&#28431;&#24341;&#36215;&#30340;&#65289;&#27969;&#21160;&#27169;&#24335;&#20043;&#38388;&#30340;&#21464;&#21270;&#24456;&#23567;&#65307;&#65288;iii&#65289;&#25511;&#21046;&#27969;&#21160;&#30340;&#20648;&#23618;&#29305;&#24615;&#24378;&#28872;&#24322;&#36136;&#19988;&#36890;&#24120;
&lt;/p&gt;
&lt;p&gt;
As the global deployment of carbon capture and sequestration (CCS) technology intensifies in the fight against climate change, it becomes increasingly imperative to establish robust monitoring and detection mechanisms for potential underground CO2 leakage, particularly through pre-existing or induced faults in the storage reservoir's seals. While techniques such as history matching and time-lapse seismic monitoring of CO2 storage have been used successfully in tracking the evolution of CO2 plumes in the subsurface, these methods lack principled approaches to characterize uncertainties related to the CO2 plumes' behavior. Inclusion of systematic assessment of uncertainties is essential for risk mitigation for the following reasons: (i) CO2 plume-induced changes are small and seismic data is noisy; (ii) changes between regular and irregular (e.g., caused by leakage) flow patterns are small; and (iii) the reservoir properties that control the flow are strongly heterogeneous and typically 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#30340;&#21019;&#26032;&#26041;&#27861;ClinGen&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00287</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#27880;&#20837;&#65306;&#35780;&#20272;&#21644;&#25512;&#36827;&#20020;&#24202;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. (arXiv:2311.00287v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#30340;&#21019;&#26032;&#26041;&#27861;ClinGen&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38656;&#35201;&#33021;&#22815;&#24212;&#23545;&#39046;&#22495;&#29305;&#23450;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#22797;&#26434;&#30340;&#21307;&#23398;&#26415;&#35821;&#21644;&#20020;&#24202;&#32972;&#26223;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30452;&#25509;&#37096;&#32626;&#21487;&#33021;&#23548;&#33268;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#21463;&#21040;&#36164;&#28304;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#20020;&#24202;NLP&#20219;&#21153;&#30340;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#27861;ClinGen&#65292;&#23427;&#23558;&#30693;&#35782;&#27880;&#20837;&#21040;&#36825;&#20010;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#28041;&#21450;&#20020;&#24202;&#30693;&#35782;&#25552;&#21462;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;LLM&#25552;&#31034;&#12290;&#20020;&#24202;&#20027;&#39064;&#21644;&#20889;&#20316;&#39118;&#26684;&#37117;&#26469;&#33258;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#20197;&#24341;&#23548;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;7&#20010;&#20020;&#24202;NLP&#20219;&#21153;&#21644;16&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;ClinGen&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22987;&#32456;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#20351;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#26174;&#33879;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the divers
&lt;/p&gt;</description></item><item><title>JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00286</link><description>&lt;p&gt;
JADE&#65306;&#22522;&#20110;&#35821;&#35328;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00286
&lt;/p&gt;
&lt;p&gt;
JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JADE&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#20998;&#26512;&#30340;&#27169;&#31946;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22686;&#24378;&#31181;&#23376;&#38382;&#39064;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#24182;&#22987;&#32456;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#19977;&#31867;LLM&#65306;&#20843;&#20010;&#24320;&#28304;&#20013;&#25991;LLM&#65292;&#20845;&#20010;&#21830;&#19994;&#20013;&#25991;LLM&#21644;&#22235;&#20010;&#21830;&#19994;&#33521;&#25991;LLM&#12290;JADE&#20026;&#36825;&#19977;&#31867;LLM&#29983;&#25104;&#20102;&#19977;&#20010;&#23433;&#20840;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#65306;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#21516;&#26102;&#35302;&#21457;&#22810;&#20010;LLM&#30340;&#26377;&#23475;&#29983;&#25104;&#65292;&#24179;&#22343;&#19981;&#23433;&#20840;&#29983;&#25104;&#27604;&#20363;&#20026;70%&#65288;&#35831;&#21442;&#35265;&#19979;&#34920;&#65289;&#65292;&#21516;&#26102;&#36825;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#33258;&#28982;&#12289;&#27969;&#30021;&#19988;&#20445;&#30041;&#20102;&#26680;&#24515;&#30340;&#19981;&#23433;&#20840;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#21457;&#24067;&#20102;&#23545;&#21830;&#19994;&#33521;&#25991;LLM&#21644;&#24320;&#28304;&#33521;&#25991;LLM&#29983;&#25104;&#30340;&#22522;&#20934;&#28436;&#31034;&#65306;https://github.com/whitzard-ai/jade-db&#12290;&#23545;&#20110;&#23545;JADE&#29983;&#25104;&#30340;&#26356;&#22810;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#65292;&#35831;&#19982;&#25105;&#20204;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Mixture-of-Experts&#29992;&#20110;&#24320;&#25918;&#22495;&#36866;&#24212;&#30340;&#21452;&#31354;&#38388;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#21644;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#33410;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2311.00285</link><description>&lt;p&gt;
Mixture-of-Experts&#29992;&#20110;&#24320;&#25918;&#22495;&#36866;&#24212;&#30340;&#21452;&#31354;&#38388;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space Detection Approach. (arXiv:2311.00285v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00285
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Mixture-of-Experts&#29992;&#20110;&#24320;&#25918;&#22495;&#36866;&#24212;&#30340;&#21452;&#31354;&#38388;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#21644;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#33410;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#36866;&#24212;&#65288;OSDA&#65289;&#26088;&#22312;&#21516;&#26102;&#22788;&#29702;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#21644;&#26631;&#31614;&#20559;&#31227;&#65292;&#23454;&#29616;&#23545;&#24050;&#30693;&#31867;&#21035;&#30340;&#31934;&#30830;&#20998;&#31867;&#65292;&#21516;&#26102;&#22312;&#30446;&#26631;&#22495;&#20013;&#35782;&#21035;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;OSDA&#26041;&#27861;&#20381;&#36182;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#32456;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#65292;&#38656;&#35201;&#25163;&#21160;&#35843;&#33410;&#38408;&#20540;&#65292;&#24182;&#19988;&#21487;&#33021;&#23558;&#26410;&#30693;&#26679;&#26412;&#38169;&#35823;&#20998;&#31867;&#20026;&#24050;&#30693;&#31867;&#21035;&#12290;Mixture-of-Expert&#65288;MoE&#65289;&#21487;&#33021;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#22312;MoE&#20013;&#65292;&#19981;&#21516;&#30340;&#19987;&#23478;&#22788;&#29702;&#19981;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#22312;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20013;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#29983;&#25104;&#29420;&#29305;&#30340;&#19987;&#23478;&#36335;&#30001;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#20063;&#21487;&#20197;&#26174;&#31034;&#19982;&#24050;&#30693;&#31867;&#21035;&#19981;&#21516;&#30340;&#19987;&#23478;&#36335;&#30001;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#31354;&#38388;&#26816;&#27979;&#65292;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#21644;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#20219;&#20309;&#38408;&#20540;&#12290;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#22270;&#24418;&#36335;&#30001;&#22120;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#25688;&#35201;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Set Domain Adaptation (OSDA) aims to cope with the distribution and label shifts between the source and target domains simultaneously, performing accurate classification for known classes while identifying unknown class samples in the target domain. Most existing OSDA approaches, depending on the final image feature space of deep models, require manually-tuned thresholds, and may easily misclassify unknown samples as known classes. Mixture-of-Expert (MoE) could be a remedy. Within an MoE, different experts address different input features, producing unique expert routing patterns for different classes in a routing feature space. As a result, unknown class samples may also display different expert routing patterns to known classes. This paper proposes Dual-Space Detection, which exploits the inconsistencies between the image feature space and the routing feature space to detect unknown class samples without any threshold. Graph Router is further introduced to better make use of the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#27169;&#22411;&#39537;&#21160;&#24037;&#31243;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#20132;&#21449;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2311.00284</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#27169;&#22411;&#39537;&#21160;&#24037;&#31243;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Model-driven Engineering for Machine Learning Components: A Systematic Literature Review. (arXiv:2311.00284v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00284
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#27169;&#22411;&#39537;&#21160;&#24037;&#31243;&#19982;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#20132;&#21449;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#29616;&#20195;&#36719;&#20214;&#24212;&#29992;&#20013;&#20316;&#20026;&#32452;&#20214;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#30001;&#20110;&#21487;&#29992;&#30340;&#25968;&#25454;&#37327;&#24222;&#22823;&#65292;&#32452;&#32455;&#24076;&#26395;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#25968;&#25454;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#21644;&#22686;&#24378;&#19994;&#21153;&#21033;&#28070;&#33021;&#21147;&#12290;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#23454;&#29616;&#20102;&#39044;&#27979;&#33021;&#21147;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#25512;&#33616;&#12289;&#20934;&#30830;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#22788;&#29702;&#20197;&#21450;&#30693;&#24773;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#24102;&#26377;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#31995;&#32479;&#24182;&#19981;&#31616;&#21333;&#65307;&#23427;&#38656;&#35201;&#26102;&#38388;&#12289;&#31934;&#21147;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#25968;&#25454;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#26041;&#38754;&#30340;&#30693;&#35782;&#21644;&#19987;&#19994;&#30693;&#35782;&#12290;&#20043;&#21069;&#24050;&#32463;&#26377;&#20960;&#39033;&#30740;&#31350;&#38024;&#23545;&#22312;&#24320;&#21457;&#20256;&#32479;&#36719;&#20214;&#21644;&#29289;&#29702;&#31995;&#32479;&#26102;&#24212;&#29992;&#27169;&#22411;&#39537;&#21160;&#24037;&#31243;&#65288;MDE&#65289;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;&#23558;MDE&#24212;&#29992;&#20110;&#24102;&#26377;&#26426;&#22120;&#23398;&#20064;&#32452;&#20214;&#30340;&#31995;&#32479;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;&#30446;&#26631;&#65306;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#36827;&#19968;&#27493;&#25506;&#32034;MDE&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;MDE4ML&#65289;&#30340;&#26377;&#21069;&#36884;&#30340;&#20132;&#21449;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Machine Learning (ML) has become widely adopted as a component in many modern software applications. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, anomaly detection, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and software engineering. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber-physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components. Objective: The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20351;&#29992;&#26631;&#31614;&#22122;&#22768;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20102;&#27867;&#21270;&#38169;&#35823;&#30028;&#38480;&#30340;&#30740;&#31350;&#65292;&#21033;&#29992;&#31639;&#27861;&#31283;&#23450;&#24615;&#26694;&#26550;&#24471;&#21040;&#20102;&#26102;&#38388;&#26080;&#20851;&#30340;&#27867;&#21270;&#38169;&#35823;&#30028;&#38480;&#65292;&#24182;&#19988;&#22312;&#21442;&#25968;&#32500;&#24230;&#21644;&#26679;&#26412;&#22823;&#23567;&#30340;&#36895;&#29575;&#20197;&#21450;&#29305;&#23450;&#23398;&#20064;&#29575;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#30340;&#38169;&#35823;&#30028;&#38480;&#12290;&#35813;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#37327;&#21270;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2311.00274</link><description>&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27867;&#21270;&#30028;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds for Label Noise Stochastic Gradient Descent. (arXiv:2311.00274v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#20351;&#29992;&#26631;&#31614;&#22122;&#22768;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20102;&#27867;&#21270;&#38169;&#35823;&#30028;&#38480;&#30340;&#30740;&#31350;&#65292;&#21033;&#29992;&#31639;&#27861;&#31283;&#23450;&#24615;&#26694;&#26550;&#24471;&#21040;&#20102;&#26102;&#38388;&#26080;&#20851;&#30340;&#27867;&#21270;&#38169;&#35823;&#30028;&#38480;&#65292;&#24182;&#19988;&#22312;&#21442;&#25968;&#32500;&#24230;&#21644;&#26679;&#26412;&#22823;&#23567;&#30340;&#36895;&#29575;&#20197;&#21450;&#29305;&#23450;&#23398;&#20064;&#29575;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#22810;&#39033;&#24335;&#30340;&#38169;&#35823;&#30028;&#38480;&#12290;&#35813;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#37327;&#21270;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#65292;&#22522;&#20110;&#22343;&#21248;&#32791;&#25955;&#21644;&#24179;&#28369;&#26465;&#20214;&#65292;&#20026;&#20855;&#26377;&#26631;&#31614;&#22122;&#22768;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#24320;&#21457;&#20102;&#27867;&#21270;&#38169;&#35823;&#30028;&#38480;&#12290;&#22312;&#36866;&#24403;&#36873;&#25321;&#30340;&#21322;&#24230;&#37327;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#21442;&#25968;&#32500;&#24230;$d$&#22810;&#39033;&#24335;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#38543;&#26426;&#26799;&#24230;&#27969;&#30340;Wasserstein&#36317;&#31163;&#25910;&#32553;&#12290;&#21033;&#29992;&#31639;&#27861;&#31283;&#23450;&#24615;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;&#31163;&#25955;&#21270;&#31639;&#27861;&#25512;&#23548;&#20102;&#29420;&#31435;&#20110;&#26102;&#38388;&#30340;&#27867;&#21270;&#38169;&#35823;&#30028;&#38480;&#65292;&#24182;&#20351;&#29992;&#22266;&#23450;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#23454;&#29616;&#30340;&#38169;&#35823;&#30028;&#38480;&#19982;$d$&#21644;&#26679;&#26412;&#22823;&#23567;$n$&#30340;&#36895;&#29575;&#20197;&#21450;$n^{-2/3}$&#26377;&#22810;&#39033;&#24335;&#30340;&#20851;&#31995;&#12290;&#36825;&#20010;&#36895;&#29575;&#22312;&#19982;&#31867;&#20284;&#26465;&#20214;&#19979;&#20351;&#29992;&#21442;&#25968;&#26080;&#20851;&#39640;&#26031;&#22122;&#22768;&#30340;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#65288;SGLD&#65289;&#20013;&#65292;&#20854;&#26368;&#20339;&#24050;&#30693;&#36895;&#29575;$n^{-1/2}$&#35201;&#22909;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#26631;&#31614;&#22122;&#22768;&#24433;&#21709;&#30340;&#37327;&#21270;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop generalization error bounds for stochastic gradient descent (SGD) with label noise in non-convex settings under uniform dissipativity and smoothness conditions. Under a suitable choice of semimetric, we establish a contraction in Wasserstein distance of the label noise stochastic gradient flow that depends polynomially on the parameter dimension $d$. Using the framework of algorithmic stability, we derive time-independent generalisation error bounds for the discretized algorithm with a constant learning rate. The error bound we achieve scales polynomially with $d$ and with the rate of $n^{-2/3}$, where $n$ is the sample size. This rate is better than the best-known rate of $n^{-1/2}$ established for stochastic gradient Langevin dynamics (SGLD) -which employs parameter-independent Gaussian noise -- under similar conditions. Our analysis offers quantitative insights into the effect of label noise.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#20915;&#31574;Transformer&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#39640;&#23618;&#31574;&#30053;&#20026;&#24403;&#21069;&#29366;&#24577;&#25552;&#20379;&#29702;&#24819;&#25552;&#31034;&#65292;&#20302;&#23618;&#31574;&#30053;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#21160;&#20316;&#12290;&#20182;&#20204;&#21457;&#29616;&#20915;&#31574;Transformer&#26159;&#36825;&#20010;&#26694;&#26550;&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#20849;&#21516;&#20248;&#21270;&#39640;&#23618;&#21644;&#20302;&#23618;&#31574;&#30053;&#20197;&#23454;&#29616;&#25340;&#25509;&#33021;&#21147;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#26032;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2311.00267</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#37325;&#26032;&#24605;&#32771;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Rethinking Decision Transformer via Hierarchical Reinforcement Learning. (arXiv:2311.00267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65292;&#37325;&#26032;&#24605;&#32771;&#20102;&#20915;&#31574;Transformer&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24207;&#21015;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#39640;&#23618;&#31574;&#30053;&#20026;&#24403;&#21069;&#29366;&#24577;&#25552;&#20379;&#29702;&#24819;&#25552;&#31034;&#65292;&#20302;&#23618;&#31574;&#30053;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#21160;&#20316;&#12290;&#20182;&#20204;&#21457;&#29616;&#20915;&#31574;Transformer&#26159;&#36825;&#20010;&#26694;&#26550;&#30340;&#19968;&#20010;&#29305;&#20363;&#65292;&#24182;&#30740;&#31350;&#20102;&#22914;&#20309;&#20849;&#21516;&#20248;&#21270;&#39640;&#23618;&#21644;&#20302;&#23618;&#31574;&#30053;&#20197;&#23454;&#29616;&#25340;&#25509;&#33021;&#21147;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#26032;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;Transformer&#65288;DT&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Transformer&#26550;&#26500;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;DT&#30340;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#24615;&#26159;&#20854;&#20381;&#36182;&#20110;&#20174;&#25968;&#25454;&#38598;&#20013;&#22238;&#24518;&#36712;&#36857;&#30340;&#33021;&#21147;&#65292;&#22833;&#21435;&#20102;&#26080;&#32541;&#22320;&#23558;&#27425;&#20248;&#36712;&#36857;&#25340;&#25509;&#22312;&#19968;&#36215;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#24207;&#36143;&#20915;&#31574;&#30340;&#36890;&#29992;&#24207;&#21015;&#24314;&#27169;&#26694;&#26550;&#12290;&#22312;&#20570;&#20915;&#31574;&#26102;&#65292;&#39640;&#23618;&#31574;&#30053;&#39318;&#20808;&#20026;&#24403;&#21069;&#29366;&#24577;&#25552;&#20986;&#19968;&#20010;&#29702;&#24819;&#30340;&#25552;&#31034;&#65292;&#20302;&#23618;&#31574;&#30053;&#38543;&#21518;&#22312;&#32473;&#23450;&#30340;&#25552;&#31034;&#26465;&#20214;&#19979;&#29983;&#25104;&#19968;&#20010;&#21160;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DT&#26159;&#36825;&#20010;&#26694;&#26550;&#30340;&#29305;&#20363;&#65292;&#36890;&#36807;&#19968;&#23450;&#30340;&#39640;&#23618;&#21644;&#20302;&#23618;&#31574;&#30053;&#36873;&#25321;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#36873;&#25321;&#30340;&#28508;&#22312;&#22833;&#36133;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#20849;&#21516;&#20248;&#21270;&#39640;&#23618;&#21644;&#20302;&#23618;&#31574;&#30053;&#20197;&#23454;&#29616;&#25340;&#25509;&#33021;&#21147;&#65292;&#36827;&#32780;&#25512;&#21160;&#26032;&#30340;&#31163;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Transformer (DT) is an innovative algorithm leveraging recent advances of the transformer architecture in reinforcement learning (RL). However, a notable limitation of DT is its reliance on recalling trajectories from datasets, losing the capability to seamlessly stitch sub-optimal trajectories together. In this work we introduce a general sequence modeling framework for studying sequential decision making through the lens of Hierarchical RL. At the time of making decisions, a high-level policy first proposes an ideal prompt for the current state, a low-level policy subsequently generates an action conditioned on the given prompt. We show DT emerges as a special case of this framework with certain choices of high-level and low-level policies, and discuss the potential failure of these choices. Inspired by these observations, we study how to jointly optimize the high-level and low-level policies to enable the stitching ability, which further leads to the development of new offl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28608;&#21169;&#21512;&#20316;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#26631;&#31614;&#22797;&#26434;&#24615;&#65292;&#20445;&#35777;&#26234;&#33021;&#20307;&#26080;&#27861;&#36890;&#36807;&#20010;&#20154;&#34892;&#20026;&#20943;&#23569;&#20854;&#39044;&#26399;&#30340;&#26631;&#31614;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#23454;&#29616;&#19988;&#22312;&#26631;&#31614;&#22797;&#26434;&#24615;&#26041;&#38754;&#19982;&#26368;&#20339;&#21487;&#35745;&#31639;&#36817;&#20284;&#31639;&#27861;&#30456;&#23218;&#32654;&#30340;&#21512;&#20316;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2311.00260</link><description>&lt;p&gt;
&#28608;&#21169;&#21512;&#20316;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incentivized Collaboration in Active Learning. (arXiv:2311.00260v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28608;&#21169;&#21512;&#20316;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#26631;&#31614;&#22797;&#26434;&#24615;&#65292;&#20445;&#35777;&#26234;&#33021;&#20307;&#26080;&#27861;&#36890;&#36807;&#20010;&#20154;&#34892;&#20026;&#20943;&#23569;&#20854;&#39044;&#26399;&#30340;&#26631;&#31614;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#23454;&#29616;&#19988;&#22312;&#26631;&#31614;&#22797;&#26434;&#24615;&#26041;&#38754;&#19982;&#26368;&#20339;&#21487;&#35745;&#31639;&#36817;&#20284;&#31639;&#27861;&#30456;&#23218;&#32654;&#30340;&#21512;&#20316;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#30340;&#20027;&#21160;&#23398;&#20064;&#20013;&#65292;&#22810;&#20010;&#26234;&#33021;&#20307;&#35797;&#22270;&#20174;&#20849;&#21516;&#30340;&#20551;&#35774;&#20013;&#23398;&#20064;&#26631;&#31614;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28608;&#21169;&#21512;&#20316;&#26694;&#26550;&#12290;&#22312;&#36825;&#37324;&#65292;&#29702;&#24615;&#30340;&#26234;&#33021;&#20307;&#26088;&#22312;&#33719;&#24471;&#20182;&#20204;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#26631;&#31614;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30528;&#37325;&#35774;&#35745;&#65288;&#20005;&#26684;&#30340;&#65289;&#20010;&#20307;&#21512;&#29702;&#65288;IR&#65289;&#21512;&#20316;&#21327;&#35758;&#65292;&#30830;&#20445;&#26234;&#33021;&#20307;&#26080;&#27861;&#36890;&#36807;&#20010;&#20154;&#34892;&#20026;&#20943;&#23569;&#20854;&#39044;&#26399;&#30340;&#26631;&#31614;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#23545;&#20110;&#20219;&#20309;&#26368;&#20248;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#36816;&#34892;&#35813;&#31639;&#27861;&#30452;&#33267;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#21512;&#20316;&#21327;&#35758;&#24050;&#32463;&#26159;IR&#30340;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#26368;&#20248;&#31639;&#27861;&#26159;NP&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#20197;&#23454;&#29616;&#65288;&#20005;&#26684;&#30340;&#65289;IR&#19988;&#22312;&#26631;&#31614;&#22797;&#26434;&#24615;&#26041;&#38754;&#19982;&#26368;&#20339;&#24050;&#30693;&#21487;&#35745;&#31639;&#36817;&#20284;&#31639;&#27861;&#30456;&#23218;&#32654;&#30340;&#21512;&#20316;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In collaborative active learning, where multiple agents try to learn labels from a common hypothesis, we introduce an innovative framework for incentivized collaboration. Here, rational agents aim to obtain labels for their data sets while keeping label complexity at a minimum. We focus on designing (strict) individually rational (IR) collaboration protocols, ensuring that agents cannot reduce their expected label complexity by acting individually. We first show that given any optimal active learning algorithm, the collaboration protocol that runs the algorithm as is over the entire data is already IR. However, computing the optimal algorithm is NP-hard. We therefore provide collaboration protocols that achieve (strict) IR and are comparable with the best known tractable approximation algorithm in terms of label complexity.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#23567;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20272;&#35745;&#26925;&#22278;&#21644;&#25243;&#29289;&#22411;&#38382;&#39064;&#30340;&#26377;&#38480;&#24046;&#20998;&#35299;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00259</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#26377;&#38480;&#24046;&#20998;&#30340;&#26080;&#30417;&#30563;&#23567;&#22411;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26925;&#22278;&#21644;&#25243;&#29289;&#22411;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solutions to Elliptic and Parabolic Problems via Finite Difference Based Unsupervised Small Linear Convolutional Neural Networks. (arXiv:2311.00259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00259
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#23567;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20272;&#35745;&#26925;&#22278;&#21644;&#25243;&#29289;&#22411;&#38382;&#39064;&#30340;&#26377;&#38480;&#24046;&#20998;&#35299;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#31185;&#23398;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;PDE&#27714;&#35299;&#22120;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#26631;&#35760;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#25512;&#24191;&#21040;&#20998;&#24067;&#20043;&#22806;&#30340;&#31034;&#20363;&#26102;&#23481;&#26131;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#23569;&#20256;&#32479;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#20272;&#35745;PDE&#35299;&#26102;&#36935;&#21040;&#30340;&#24191;&#20041;&#21270;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#23567;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20272;&#35745;PDE&#30340;&#26377;&#38480;&#24046;&#20998;&#35299;&#12290;&#19982;&#26377;&#38480;&#24046;&#20998;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20960;&#20010;&#36873;&#23450;&#30340;&#26925;&#22278;&#21644;&#25243;&#29289;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#19982;&#30495;&#35299;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing interest in leveraging deep learning and neural networks to address scientific problems, particularly in solving partial differential equations (PDEs). However, current neural network-based PDE solvers often rely on extensive training data or labeled input-output pairs, making them prone to challenges in generalizing to out-of-distribution examples. To mitigate the generalization gap encountered by conventional neural network-based methods in estimating PDE solutions, we formulate a fully unsupervised approach, requiring no training data, to estimate finite difference solutions for PDEs directly via small convolutional neural networks. Our proposed algorithms demonstrate a comparable accuracy to the true solution for several selected elliptic and parabolic problems compared to the finite difference method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25200;&#21160;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25200;&#21160;&#26356;&#25935;&#24863;&#65292;&#24182;&#35777;&#26126;&#22686;&#21152;&#25200;&#21160;&#26679;&#26412;&#30340;&#27604;&#20363;&#21487;&#20197;&#25552;&#39640;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00258</link><description>&lt;p&gt;
&#26377;&#22122;&#22768;&#30340;&#26679;&#26412;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65306;&#19968;&#20010;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#34892;&#20026;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis. (arXiv:2311.00258v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25200;&#21160;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25200;&#21160;&#26356;&#25935;&#24863;&#65292;&#24182;&#35777;&#26126;&#22686;&#21152;&#25200;&#21160;&#26679;&#26412;&#30340;&#27604;&#20363;&#21487;&#20197;&#25552;&#39640;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#38382;&#39064;&#24341;&#23548;&#30340;&#24037;&#31243;&#36827;&#23637;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#20197;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#29575;&#35299;&#20915;&#22810;&#36339;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#24037;&#20316;&#30740;&#31350;&#23569;&#26679;&#26412;&#25552;&#31034;&#25216;&#26415;&#19979;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25200;&#21160;&#26469;&#27979;&#35797;LLMs&#22312;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#19978;&#24341;&#20837;&#25200;&#21160;&#65288;&#20363;&#22914;&#35789;&#27861;&#25200;&#21160;&#65292;&#22914;&#25340;&#20889;&#38169;&#35823;&#65292;&#20197;&#21450;&#35821;&#20041;&#25200;&#21160;&#65292;&#22914;&#22312;&#38382;&#39064;&#20013;&#21253;&#21547;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65289;&#65292;&#23545;LLMs&#36827;&#34892;&#34892;&#20026;&#20998;&#26512;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#23545;&#26576;&#20123;&#25200;&#21160;&#65288;&#22914;&#29992;&#21516;&#20041;&#35789;&#26367;&#25442;&#21333;&#35789;&#65289;&#26356;&#25935;&#24863;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#65292;&#22312;&#25552;&#31034;&#20013;&#22686;&#21152;&#25200;&#21160;&#26679;&#26412;&#30340;&#27604;&#20363;&#21487;&#20197;&#25552;&#39640;&#23569;&#26679;&#26412;&#25552;&#31034;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26234;&#33021;&#20307;&#31070;&#32463;&#25299;&#25169;&#26144;&#23556;&#65288;MANTM&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25299;&#25169;&#22320;&#22270;&#20316;&#20026;&#29615;&#22659;&#34920;&#31034;&#24182;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#24555;&#36895;&#23398;&#20064;&#65288;&#36817;&#65289;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2311.00252</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#30340;&#20027;&#21160;&#31070;&#32463;&#25299;&#25169;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Active Neural Topological Mapping for Multi-Agent Exploration. (arXiv:2311.00252v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26234;&#33021;&#20307;&#31070;&#32463;&#25299;&#25169;&#26144;&#23556;&#65288;MANTM&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25299;&#25169;&#22320;&#22270;&#20316;&#20026;&#29615;&#22659;&#34920;&#31034;&#24182;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#24555;&#36895;&#23398;&#20064;&#65288;&#36817;&#65289;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#25506;&#32034;&#38382;&#39064;&#65292;&#21363;&#35201;&#27714;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#36890;&#36807;&#24863;&#30693;&#20449;&#21495;&#26469;&#25506;&#32034;&#26410;&#30693;&#29615;&#22659;&#12290;&#25506;&#32034;&#20219;&#21153;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#23558;&#20027;&#21160;&#26144;&#23556;&#19982;&#35268;&#21010;&#32467;&#21512;&#36215;&#26469;&#12290;&#24230;&#37327;&#22320;&#22270;&#25429;&#25417;&#20102;&#31354;&#38388;&#34920;&#31034;&#30340;&#32454;&#33410;&#65292;&#20294;&#36890;&#20449;&#37327;&#22823;&#19988;&#22312;&#19981;&#21516;&#22330;&#26223;&#20043;&#38388;&#21487;&#33021;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#65292;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#25299;&#25169;&#22320;&#22270;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#21253;&#21547;&#33410;&#28857;&#21644;&#36793;&#65292;&#20855;&#26377;&#25277;&#35937;&#20294;&#20851;&#38190;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#21463;&#21040;&#22330;&#26223;&#32467;&#26500;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#25506;&#32034;&#20219;&#21153;&#20351;&#29992;&#30340;&#26159;&#20256;&#32479;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#32791;&#26102;&#19988;&#30001;&#20110;&#20854;&#25163;&#24037;&#35774;&#35745;&#32780;&#27425;&#20248;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#23637;&#29616;&#20986;&#36890;&#36807;&#24555;&#36895;&#31471;&#21040;&#31471;&#25512;&#23548;&#23398;&#20064;&#65288;&#36817;&#65289;&#26368;&#20248;&#31574;&#30053;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#31070;&#32463;&#25299;&#25169;&#26144;&#23556;&#65288;MANTM&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the multi-agent cooperative exploration problem, which requires multiple agents to explore an unseen environment via sensory signals in a limited time. A popular approach to exploration tasks is to combine active mapping with planning. Metric maps capture the details of the spatial representation, but are with high communication traffic and may vary significantly between scenarios, resulting in inferior generalization. Topological maps are a promising alternative as they consist only of nodes and edges with abstract but essential information and are less influenced by the scene structures. However, most existing topology-based exploration tasks utilize classical methods for planning, which are time-consuming and sub-optimal due to their handcrafted design. Deep reinforcement learning (DRL) has shown great potential for learning (near) optimal policies through fast end-to-end inference. In this paper, we propose Multi-Agent Neural Topological Mapping (MANTM) to i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#21453;&#21521;&#35823;&#24046;&#20998;&#26512;&#35745;&#31639;&#20102;&#22810;&#20219;&#21153;&#21644;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#35757;&#32451;&#20559;&#24046;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#20102;&#21407;&#22987;&#25439;&#22833;&#12289;&#24341;&#20837;&#20102;&#38544;&#24335;&#24179;&#22374;&#27491;&#21017;&#39033;&#21644;&#20914;&#31361;&#39033;&#12290;&#22312;&#22810;&#20219;&#21153;&#20013;&#65292;&#20914;&#31361;&#39033;&#34913;&#37327;&#20102;&#20219;&#21153;&#26799;&#24230;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#65307;&#32780;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#20914;&#31361;&#39033;&#26159;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#23427;&#36890;&#36807;&#20219;&#21153;&#26799;&#24230;&#20043;&#38388;&#30340;&#26446;&#25324;&#21495;&#26469;&#34913;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.00235</link><description>&lt;p&gt;
&#20174;&#21453;&#21521;&#35823;&#24046;&#20998;&#26512;&#35282;&#24230;&#30475;&#22810;&#20219;&#21153;&#21644;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Implicit biases in multitask and continual learning from a backward error analysis perspective. (arXiv:2311.00235v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#21453;&#21521;&#35823;&#24046;&#20998;&#26512;&#35745;&#31639;&#20102;&#22810;&#20219;&#21153;&#21644;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#19979;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#35757;&#32451;&#20559;&#24046;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#20102;&#21407;&#22987;&#25439;&#22833;&#12289;&#24341;&#20837;&#20102;&#38544;&#24335;&#24179;&#22374;&#27491;&#21017;&#39033;&#21644;&#20914;&#31361;&#39033;&#12290;&#22312;&#22810;&#20219;&#21153;&#20013;&#65292;&#20914;&#31361;&#39033;&#34913;&#37327;&#20102;&#20219;&#21153;&#26799;&#24230;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#65307;&#32780;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#20914;&#31361;&#39033;&#26159;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#23427;&#36890;&#36807;&#20219;&#21153;&#26799;&#24230;&#20043;&#38388;&#30340;&#26446;&#25324;&#21495;&#26469;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21453;&#21521;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20219;&#21153;&#21644;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#38544;&#24335;&#35757;&#32451;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38544;&#21547;&#22320;&#26368;&#23567;&#21270;&#30340;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#12290;&#23427;&#20204;&#21253;&#25324;&#19977;&#20010;&#39033;&#65306;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#65288;&#32771;&#34385;&#25910;&#25947;&#24615;&#65289;&#65292;&#19982;&#23398;&#20064;&#29575;&#25104;&#27491;&#27604;&#30340;&#38544;&#24335;&#24179;&#22374;&#27491;&#21017;&#39033;&#20197;&#21450;&#26368;&#21518;&#19968;&#20010;&#39033;&#8212;&#8212;&#20914;&#31361;&#39033;&#65292;&#35813;&#39033;&#22312;&#29702;&#35770;&#19978;&#23545;&#25910;&#25947;&#24615;&#21644;&#38544;&#24335;&#27491;&#21017;&#21270;&#37117;&#21487;&#33021;&#26377;&#23475;&#12290;&#22312;&#22810;&#20219;&#21153;&#20013;&#65292;&#20914;&#31361;&#39033;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#20219;&#21153;&#20043;&#38388;&#30340;&#26799;&#24230;&#23545;&#40784;&#24615;&#65292;&#32780;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#20914;&#31361;&#39033;&#26159;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#20013;&#30340;&#19968;&#20010;&#26032;&#37327;&#65292;&#23613;&#31649;&#22312;&#24494;&#20998;&#20960;&#20309;&#20013;&#26159;&#19968;&#20010;&#22522;&#26412;&#24037;&#20855;&#65306;&#20219;&#21153;&#26799;&#24230;&#20043;&#38388;&#30340;&#26446;&#25324;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using backward error analysis, we compute implicit training biases in multitask and continual learning settings for neural networks trained with stochastic gradient descent. In particular, we derive modified losses that are implicitly minimized during training. They have three terms: the original loss, accounting for convergence, an implicit flatness regularization term proportional to the learning rate, and a last term, the conflict term, which can theoretically be detrimental to both convergence and implicit regularization. In multitask, the conflict term is a well-known quantity, measuring the gradient alignment between the tasks, while in continual learning the conflict term is a new quantity in deep learning optimization, although a basic tool in differential geometry: The Lie bracket between the task gradients.
&lt;/p&gt;</description></item><item><title>DistDNAS&#26159;&#19968;&#31181;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#25628;&#32034;&#29305;&#24449;&#20132;&#20114;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#25628;&#32034;&#21644;&#36873;&#25321;&#26368;&#20339;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#21152;&#36895;&#24182;&#23558;&#25628;&#32034;&#26102;&#38388;&#20174;2&#22825;&#32553;&#30701;&#21040;2&#23567;&#26102;&#12290;</title><link>http://arxiv.org/abs/2311.00231</link><description>&lt;p&gt;
DistDNAS: &#22312;2&#23567;&#26102;&#20869;&#39640;&#25928;&#25628;&#32034;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
DistDNAS: Search Efficient Feature Interactions within 2 Hours. (arXiv:2311.00231v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00231
&lt;/p&gt;
&lt;p&gt;
DistDNAS&#26159;&#19968;&#31181;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#25628;&#32034;&#29305;&#24449;&#20132;&#20114;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#25628;&#32034;&#21644;&#36873;&#25321;&#26368;&#20339;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#21152;&#36895;&#24182;&#23558;&#25628;&#32034;&#26102;&#38388;&#20174;2&#22825;&#32553;&#30701;&#21040;2&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#25628;&#32034;&#25928;&#29575;&#21644;&#26381;&#21153;&#25928;&#29575;&#26159;&#26500;&#24314;&#29305;&#24449;&#20132;&#20114;&#21644;&#21152;&#24555;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#30340;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#12290;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#30001;&#20110;&#22823;&#37327;&#25968;&#25454;&#19978;&#30340;&#39034;&#24207;&#24037;&#20316;&#27969;&#31243;&#65292;&#25628;&#32034;&#26368;&#20339;&#29305;&#24449;&#20132;&#20114;&#35774;&#35745;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#34701;&#21512;&#21508;&#31181;&#26469;&#28304;&#12289;&#39034;&#24207;&#21644;&#25968;&#23398;&#36816;&#31639;&#30340;&#20132;&#20114;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20914;&#31361;&#21644;&#39069;&#22806;&#30340;&#20887;&#20313;&#65292;&#23548;&#33268;&#24615;&#33021;&#21644;&#26381;&#21153;&#25104;&#26412;&#30340;&#27425;&#20248;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DistDNAS&#20316;&#20026;&#19968;&#31181;&#31616;&#27905;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#39640;&#25928;&#22320;&#36827;&#34892;&#29305;&#24449;&#20132;&#20114;&#35774;&#35745;&#12290;DistDNAS&#25552;&#20986;&#20102;&#19968;&#20010;&#36229;&#32423;&#32593;&#32476;&#65292;&#23558;&#19981;&#21516;&#39034;&#24207;&#21644;&#31867;&#22411;&#30340;&#20132;&#20114;&#27169;&#22359;&#20316;&#20026;&#25628;&#32034;&#31354;&#38388;&#36827;&#34892;&#25972;&#21512;&#12290;&#20026;&#20102;&#20248;&#21270;&#25628;&#32034;&#25928;&#29575;&#65292;DistDNAS&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#26085;&#26399;&#19978;&#20998;&#24067;&#24335;&#25628;&#32034;&#24182;&#27719;&#24635;&#36873;&#25321;&#26368;&#20339;&#30340;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;25&#20493;&#30340;&#21152;&#36895;&#65292;&#23558;&#25628;&#32034;&#25104;&#26412;&#20174;2&#22825;&#20943;&#23569;&#21040;2&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search efficiency and serving efficiency are two major axes in building feature interactions and expediting the model development process in recommender systems. On large-scale benchmarks, searching for the optimal feature interaction design requires extensive cost due to the sequential workflow on the large volume of data. In addition, fusing interactions of various sources, orders, and mathematical operations introduces potential conflicts and additional redundancy toward recommender models, leading to sub-optimal trade-offs in performance and serving cost. In this paper, we present DistDNAS as a neat solution to brew swift and efficient feature interaction design. DistDNAS proposes a supernet to incorporate interaction modules of varying orders and types as a search space. To optimize search efficiency, DistDNAS distributes the search and aggregates the choice of optimal interaction modules on varying data dates, achieving over 25x speed-up and reducing search cost from 2 days to 2 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;StableFDG&#65292;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#21644;&#27880;&#24847;&#21147;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#39118;&#26684;&#30340;&#23398;&#20064;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#24335;&#25193;&#23637;&#21040;&#21407;&#22987;&#28304;&#22495;&#20043;&#22806;&#65292;&#24182;&#36890;&#36807;&#39118;&#26684;&#20849;&#20139;&#12289;&#36716;&#31227;&#21644;&#25506;&#32034;&#31574;&#30053;&#25913;&#36827;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#12290;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#31361;&#20986;&#22120;&#25429;&#25417;&#20102;&#25968;&#25454;&#26679;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00227</link><description>&lt;p&gt;
StableFDG:&#22522;&#20110;&#39118;&#26684;&#21644;&#27880;&#24847;&#21147;&#30340;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StableFDG: Style and Attention Based Learning for Federated Domain Generalization. (arXiv:2311.00227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StableFDG&#65292;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#21644;&#27880;&#24847;&#21147;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#39118;&#26684;&#30340;&#23398;&#20064;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#24335;&#25193;&#23637;&#21040;&#21407;&#22987;&#28304;&#22495;&#20043;&#22806;&#65292;&#24182;&#36890;&#36807;&#39118;&#26684;&#20849;&#20139;&#12289;&#36716;&#31227;&#21644;&#25506;&#32034;&#31574;&#30053;&#25913;&#36827;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#12290;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#31361;&#20986;&#22120;&#25429;&#25417;&#20102;&#25968;&#25454;&#26679;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#65288;&#28304;&#22495;&#65289;&#21644;&#27979;&#35797;&#65288;&#30446;&#26631;&#22495;&#65289;&#20013;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#24120;&#21457;&#29983;&#22495;&#20559;&#31227;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#24341;&#20837;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#26679;&#26412;/&#22495;&#65292;&#29616;&#26377;&#30340;&#39046;&#22495;&#27867;&#21270;&#31639;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#38754;&#20020;&#22522;&#26412;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39118;&#26684;&#21644;&#27880;&#24847;&#21147;&#30340;&#23398;&#20064;&#31574;&#30053;StableFDG&#65292;&#29992;&#20110;&#23454;&#29616;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#31532;&#19968;&#20010;&#26159;&#22522;&#20110;&#39118;&#26684;&#30340;&#23398;&#20064;&#65292;&#23427;&#20351;&#27599;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#36229;&#36234;&#21407;&#22987;&#28304;&#22495;&#65292;&#25506;&#32034;&#26032;&#39062;&#30340;&#39118;&#26684;&#65292;&#22522;&#20110;&#25552;&#20986;&#30340;&#39118;&#26684;&#20849;&#20139;&#12289;&#36716;&#31227;&#21644;&#25506;&#32034;&#31574;&#30053;&#25913;&#36827;&#20102;&#39046;&#22495;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#29305;&#24449;&#31361;&#20986;&#22120;&#65292;&#23427;&#25429;&#25417;&#25968;&#25454;&#26679;&#26412;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional federated learning (FL) algorithms operate under the assumption that the data distributions at training (source domains) and testing (target domain) are the same. The fact that domain shifts often occur in practice necessitates equipping FL methods with a domain generalization (DG) capability. However, existing DG algorithms face fundamental challenges in FL setups due to the lack of samples/domains in each client's local dataset. In this paper, we propose StableFDG, a style and attention based learning strategy for accomplishing federated domain generalization, introducing two key contributions. The first is style-based learning, which enables each client to explore novel styles beyond the original source domains in its local dataset, improving domain diversity based on the proposed style sharing, shifting, and exploration strategies. Our second contribution is an attention-based feature highlighter, which captures the similarities between the features of data samples in t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20272;&#35745;&#26469;&#35299;&#20915;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#20449;&#36947;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#26469;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20256;&#36755;&#31526;&#21495;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2311.00226</link><description>&lt;p&gt;
Transformers&#26159;&#26080;&#32447;&#36890;&#20449;&#20013;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers are Efficient In-Context Estimators for Wireless Communication. (arXiv:2311.00226v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00226
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20272;&#35745;&#26469;&#35299;&#20915;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#20449;&#36947;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#26469;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20256;&#36755;&#31526;&#21495;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;Transformers&#21487;&#20197;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26174;&#24335;&#30340;&#27169;&#22411;&#20248;&#21270;&#12290;&#21463;&#21040;&#36825;&#20010;&#23646;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#20272;&#35745;&#65292;&#29992;&#20110;&#20272;&#35745;&#20174;&#25509;&#25910;&#21040;&#30340;&#31526;&#21495;&#20013;&#30340;&#20256;&#36755;&#31526;&#21495;&#30340;&#32463;&#20856;&#36890;&#20449;&#38382;&#39064;&#12290;&#36890;&#20449;&#20449;&#36947;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#23558;&#20256;&#36755;&#31526;&#21495;&#26144;&#23556;&#21040;&#25509;&#25910;&#31526;&#21495;&#30340;&#22122;&#22768;&#20989;&#25968;&#65292;&#36825;&#20010;&#20989;&#25968;&#21487;&#20197;&#30001;&#19968;&#20010;&#26410;&#30693;&#21442;&#25968;&#34920;&#31034;&#65292;&#20854;&#32479;&#35745;&#25968;&#25454;&#20381;&#36182;&#20110;&#19968;&#20010;&#65288;&#20063;&#26159;&#26410;&#30693;&#30340;&#65289;&#28508;&#22312;&#19978;&#19979;&#25991;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#65292;&#21482;&#26159;&#35797;&#22270;&#20351;&#29992;&#24050;&#30693;&#30340;&#20256;&#36755;&#20449;&#21495;&#36827;&#34892;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#28982;&#21518;&#29992;&#20110;&#20272;&#35745;&#36830;&#32493;&#30340;&#26410;&#30693;&#20256;&#36755;&#31526;&#21495;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#26412;&#32852;&#31995;&#65292;&#21363;Transformers&#22312;&#23569;&#37327;&#25552;&#31034;&#19979;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#19978;&#19979;&#25991;&#24207;&#21015;&#23436;&#25104;&#33021;&#21147;&#65292;&#22240;&#27492;&#23427;&#20204;&#24212;&#35813;&#33021;&#22815;&#38544;&#24335;&#30830;&#23450;...
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers can perform in-context learning, where they adapt to a new task using only a small number of prompts without any explicit model optimization. Inspired by this attribute, we propose a novel approach, called in-context estimation, for the canonical communication problem of estimating transmitted symbols from received symbols. A communication channel is essentially a noisy function that maps transmitted symbols to received symbols, and this function can be represented by an unknown parameter whose statistics depend on an (also unknown) latent context. Conventional approaches ignore this hierarchical structure and simply attempt to use known transmissions, called pilots, to perform a least-squares estimate of the channel parameter, which is then used to estimate successive, unknown transmitted symbols. We make the basic connection that transformers show excellent contextual sequence completion with a few prompts, and so they should be able to implicitly determine t
&lt;/p&gt;</description></item><item><title>WinNet&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;CNN-based&#27169;&#22411;&#65292;&#36890;&#36807;&#31383;&#21475;&#22686;&#24378;&#30340;&#21608;&#26399;&#25552;&#21462;&#21644;&#20132;&#20114;&#65292;&#22312;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#21608;&#26399;&#24615;&#26041;&#38754;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#21644;&#31616;&#21333;&#32467;&#26500;&#12290;&#22312;&#20061;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WinNet&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;CNN&#12289;MLP&#12289;Transformer&#26041;&#27861;&#30340;&#26368;&#26032;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.00214</link><description>&lt;p&gt;
WinNet:&#20855;&#26377;&#31383;&#21475;&#22686;&#24378;&#21608;&#26399;&#25552;&#21462;&#21644;&#20132;&#20114;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
WinNet:time series forecasting with a window-enhanced period extracting and interacting. (arXiv:2311.00214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00214
&lt;/p&gt;
&lt;p&gt;
WinNet&#26159;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;CNN-based&#27169;&#22411;&#65292;&#36890;&#36807;&#31383;&#21475;&#22686;&#24378;&#30340;&#21608;&#26399;&#25552;&#21462;&#21644;&#20132;&#20114;&#65292;&#22312;&#25429;&#25417;&#38271;&#26399;&#21644;&#30701;&#26399;&#21608;&#26399;&#24615;&#26041;&#38754;&#20855;&#26377;&#39640;&#20934;&#30830;&#24230;&#21644;&#31616;&#21333;&#32467;&#26500;&#12290;&#22312;&#20061;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WinNet&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;CNN&#12289;MLP&#12289;Transformer&#26041;&#27861;&#30340;&#26368;&#26032;&#24615;&#33021;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26102;&#24207;&#39044;&#27979;&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#26080;&#27861;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#21608;&#26399;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#20934;&#30830;&#19988;&#31616;&#21333;&#32467;&#26500;&#30340;&#22522;&#20110;CNN&#30340;&#27169;&#22411;WinNet&#65292;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#21253;&#25324;&#65306;(i) Inter-Intra Period Encoder (I2PE) &#23558;1D&#24207;&#21015;&#36716;&#25442;&#20026;&#20108;&#32500;&#24352;&#37327;&#65292;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#21608;&#26399;&#31383;&#21475;&#20855;&#26377;&#38271;&#26399;&#21644;&#30701;&#26399;&#21608;&#26399;&#24615;&#65292;(ii) Two-Dimensional Period Decomposition (TDPD) &#23545;&#27169;&#22411;&#36827;&#34892;&#21608;&#26399;-&#36235;&#21183;&#21644;&#25391;&#33633;&#39033;&#24314;&#27169;&#65292;(iii) Decomposition Correlation Block (DCB) &#21033;&#29992;&#21608;&#26399;-&#36235;&#21183;&#21644;&#25391;&#33633;&#39033;&#30340;&#30456;&#20851;&#24615;&#65292;&#36890;&#36807;CNNs&#25903;&#25345;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#20061;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;WinNet&#22312;CNN&#12289;MLP&#21644;Transformer&#26041;&#27861;&#19978;&#21487;&#20197;&#23454;&#29616;SOTA&#24615;&#33021;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;WinNet&#20026;&#22522;&#20110;CNN&#30340;&#20803;&#26041;&#27861;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformer-based methods have significantly improved state-of-the-art time series forecasting results, but they suffer from high computational costs and the inability to capture the long and short periodicity of time series. We present a highly accurate and simply structured CNN-based model for long-term time series forecasting tasks, called WinNet, including (i) Inter-Intra Period Encoder (I2PE) to transform 1D sequence into 2D tensor with long and short periodicity according to the predefined periodic window, (ii) Two-Dimensional Period Decomposition (TDPD) to model period-trend and oscillation terms, and (iii) Decomposition Correlation Block (DCB) to leverage the correlations of the period-trend and oscillation terms to support the prediction tasks by CNNs. Results on nine benchmark datasets show that the WinNet can achieve SOTA performance and lower computational complexity over CNN-, MLP-, Transformer-based approaches. The WinNet provides potential for the CNN-based met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#23558;&#23545;&#31216;&#24615;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;1. &#24378;&#21046;&#24050;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;2. &#21457;&#29616;&#26410;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;3. &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#23545;&#31216;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00212</link><description>&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24378;&#21046;&#12289;&#21457;&#29616;&#21644;&#25512;&#21160;&#23545;&#31216;&#24615;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning. (arXiv:2311.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#23558;&#23545;&#31216;&#24615;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;1. &#24378;&#21046;&#24050;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;2. &#21457;&#29616;&#26410;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;3. &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#23384;&#22312;&#20110;&#33258;&#28982;&#30028;&#20013;&#65292;&#24182;&#22312;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#26680;&#24515;&#30340;&#35282;&#33394;&#12290;&#22522;&#26412;&#23545;&#31216;&#24615;&#65292;&#22914;&#24222;&#21152;&#33713;&#19981;&#21464;&#24615;&#65292;&#20351;&#22312;&#22320;&#29699;&#19978;&#23454;&#39564;&#23460;&#20013;&#21457;&#29616;&#30340;&#29289;&#29702;&#23450;&#24459;&#33021;&#22815;&#25512;&#24191;&#21040;&#23431;&#23449;&#30340;&#26368;&#36828;&#22788;&#12290;&#23545;&#31216;&#24615;&#23545;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#23454;&#29616;&#36825;&#31181;&#25512;&#24191;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24179;&#31227;&#19981;&#21464;&#24615;&#20801;&#35768;&#20351;&#29992;&#21442;&#25968;&#26356;&#23569;&#30340;&#27169;&#22411;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#21644;&#26041;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20197;&#19977;&#31181;&#26041;&#24335;&#34701;&#20837;&#23545;&#31216;&#24615;&#65306;1. &#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#24378;&#21046;&#24050;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;2. &#21457;&#29616;&#32473;&#23450;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#30340;&#26410;&#30693;&#23545;&#31216;&#24615;&#65307;3. &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#23398;&#20064;&#25171;&#30772;&#29992;&#25143;&#25351;&#23450;&#30340;&#20505;&#36873;&#32676;&#20307;&#20869;&#30340;&#23545;&#31216;&#24615;&#26469;&#20419;&#36827;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry is present throughout nature and continues to play an increasingly central role in physics and machine learning. Fundamental symmetries, such as Poincar\'{e} invariance, allow physical laws discovered in laboratories on Earth to be extrapolated to the farthest reaches of the universe. Symmetry is essential to achieving this extrapolatory power in machine learning applications. For example, translation invariance in image classification allows models with fewer parameters, such as convolutional neural networks, to be trained on smaller data sets and achieve state-of-the-art performance. In this paper, we provide a unifying theoretical and methodological framework for incorporating symmetry into machine learning models in three ways: 1. enforcing known symmetry when training a model; 2. discovering unknown symmetries of a given model or data set; and 3. promoting symmetry during training by learning a model that breaks symmetries within a user-specified group of candidates when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2311.00208</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#22120;&#65306;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#23427;&#20204;&#33021;&#21542;&#35299;&#20915;&#38382;&#39064;&#65292;&#23558;&#38382;&#39064;&#35270;&#20026;&#24418;&#24335;&#35821;&#35328;&#12290;&#25506;&#32034;&#36825;&#31867;&#38382;&#39064;&#23558;&#26377;&#21161;&#20110;&#27604;&#36739;transformers&#19982;&#20854;&#20182;&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#21464;&#31181;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#36825;&#20010;&#23376;&#39046;&#22495;&#30340;&#24037;&#20316;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35760;&#24405;&#20102;&#19981;&#21516;&#32467;&#26524;&#32972;&#21518;&#30340;&#21508;&#31181;&#20551;&#35774;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#21327;&#35843;&#30475;&#20284;&#30456;&#20114;&#30683;&#30462;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring questions such as this will help to compare transformers with other models, and transformer variants with one another, for various tasks. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#20248;&#21270;&#20840;&#23616;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#24635;&#22870;&#21169;&#65292;&#23454;&#29616;&#21327;&#20316;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#21463;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#65292;&#19988;&#20855;&#26377;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00201</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Natural Policy Gradient Methods for Multi-task Reinforcement Learning. (arXiv:2311.00201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#37030;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#20248;&#21270;&#20840;&#23616;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#24635;&#22870;&#21169;&#65292;&#23454;&#29616;&#21327;&#20316;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#21463;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#65292;&#19988;&#20855;&#26377;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#20915;&#31574;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#22810;&#20219;&#21153;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#26377;&#33258;&#24049;&#30340;&#31169;&#26377;&#22870;&#21169;&#20989;&#25968;&#23545;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#20849;&#20139;&#29615;&#22659;&#30340;&#30456;&#21516;&#36716;&#31227;&#26680;&#12290;&#38024;&#23545;&#26080;&#38480;&#26102;&#38388;&#27493;&#26631;&#35760;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#20986;&#19968;&#31181;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;&#65292;&#22312;&#20998;&#25955;&#30340;&#26041;&#24335;&#19979;&#65292;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#25240;&#25187;&#24635;&#22870;&#21169;&#20043;&#21644;&#65292;&#20854;&#20013;&#27599;&#20010;&#26234;&#33021;&#20307;&#20165;&#19982;&#20854;&#22312;&#32473;&#23450;&#22270;&#25299;&#25169;&#20013;&#30340;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#22312; softmax &#21442;&#25968;&#21270;&#19979;&#24320;&#23637;&#20102;&#32852;&#37030;&#32431;&#31929;&#21644;&#29109;&#27491;&#21017;&#21270;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#23558;&#26799;&#24230;&#36319;&#36394;&#24212;&#29992;&#20110;&#20840;&#23616; Q &#20989;&#25968;&#65292;&#20197;&#20943;&#36731;&#20449;&#24687;&#20849;&#20139;&#19981;&#23436;&#22791;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#31934;&#30830;&#31574;&#30053;&#35780;&#20272;&#19979;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#20960;&#20046;&#26159;&#29420;&#31435;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated reinforcement learning (RL) enables collaborative decision making of multiple distributed agents without sharing local data trajectories. In this work, we consider a multi-task setting, in which each agent has its own private reward function corresponding to different tasks, while sharing the same transition kernel of the environment. Focusing on infinite-horizon tabular Markov decision processes, the goal is to learn a globally optimal policy that maximizes the sum of the discounted total rewards of all the agents in a decentralized manner, where each agent only communicates with its neighbors over some prescribed graph topology. We develop federated vanilla and entropy-regularized natural policy gradient (NPG) methods under softmax parameterization, where gradient tracking is applied to the global Q-function to mitigate the impact of imperfect information sharing. We establish non-asymptotic global convergence guarantees under exact policy evaluation, which are nearly indep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#36817;&#26399;&#22312;&#25913;&#36827;&#23494;&#24230;&#27867;&#20989;&#21450;&#30456;&#20851;&#36817;&#20284;&#26041;&#27861;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22312;&#19981;&#21516;&#21270;&#23398;&#21644;&#26448;&#26009;&#31867;&#21035;&#20043;&#38388;&#35774;&#35745;&#21487;&#36801;&#31227;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#21487;&#33021;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2311.00196</link><description>&lt;p&gt;
&#29992;&#20110;&#25552;&#39640;&#23494;&#24230;&#27867;&#20989;&#36817;&#20284;&#31934;&#30830;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Machine learning for accuracy in density functional approximations. (arXiv:2311.00196v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#36817;&#26399;&#22312;&#25913;&#36827;&#23494;&#24230;&#27867;&#20989;&#21450;&#30456;&#20851;&#36817;&#20284;&#26041;&#27861;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22312;&#19981;&#21516;&#21270;&#23398;&#21644;&#26448;&#26009;&#31867;&#21035;&#20043;&#38388;&#35774;&#35745;&#21487;&#36801;&#31227;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#21487;&#33021;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#35745;&#31639;&#21270;&#23398;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#36895;&#21407;&#23376;&#27169;&#25311;&#21644;&#26448;&#26009;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26377;&#21487;&#33021;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#30005;&#23376;&#32467;&#26500;&#29702;&#35770;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#32416;&#27491;&#23494;&#24230;&#27867;&#20989;&#26041;&#27861;&#20013;&#30340;&#22522;&#26412;&#38169;&#35823;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25913;&#36827;&#23494;&#24230;&#27867;&#20989;&#21644;&#30456;&#20851;&#36817;&#20284;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36890;&#36807;&#31034;&#20363;&#24212;&#29992;&#26377;&#24076;&#26395;&#30340;&#27169;&#22411;&#20110;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#31995;&#32479;&#65292;&#35752;&#35770;&#20102;&#22312;&#19981;&#21516;&#21270;&#23398;&#21644;&#26448;&#26009;&#31867;&#21035;&#20043;&#38388;&#35774;&#35745;&#21487;&#36801;&#31227;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#21487;&#33021;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques have found their way into computational chemistry as indispensable tools to accelerate atomistic simulations and materials design. In addition, machine learning approaches hold the potential to boost the predictive power of computationally efficient electronic structure methods, such as density functional theory, to chemical accuracy and to correct for fundamental errors in density functional approaches. Here, recent progress in applying machine learning to improve the accuracy of density functional and related approximations is reviewed. Promises and challenges in devising machine learning models transferable between different chemistries and materials classes are discussed with the help of examples applying promising models to systems far outside their training sets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#19979;&#30340;&#20984;&#20989;&#25968;&#36861;&#36394;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#21516;&#26102;&#22312;&#20004;&#31181;&#24773;&#22659;&#19979;&#36798;&#21040;&#24615;&#33021;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#36825;&#26159;&#39318;&#20010;&#20351;&#29992;&#38543;&#26426;&#26694;&#26550;&#30740;&#31350;&#35813;&#38382;&#39064;&#30340;&#24037;&#20316;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#20004;&#31181;&#24773;&#22659;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.00181</link><description>&lt;p&gt;
&#26368;&#20339;&#32467;&#21512;: &#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#20984;&#20989;&#25968;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Best of Both Worlds: Stochastic and Adversarial Convex Function Chasing. (arXiv:2311.00181v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00181
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#19979;&#30340;&#20984;&#20989;&#25968;&#36861;&#36394;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#21516;&#26102;&#22312;&#20004;&#31181;&#24773;&#22659;&#19979;&#36798;&#21040;&#24615;&#33021;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#36825;&#26159;&#39318;&#20010;&#20351;&#29992;&#38543;&#26426;&#26694;&#26550;&#30740;&#31350;&#35813;&#38382;&#39064;&#30340;&#24037;&#20316;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#20004;&#31181;&#24773;&#22659;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20984;&#20989;&#25968;&#36861;&#36394;(CFC)&#26159;&#19968;&#20010;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#27599;&#19968;&#36718;$t$&#65292;&#29609;&#23478;&#26681;&#25454;&#25439;&#22833;&#20989;&#25968;$f_t(x_t)$&#21644;&#20999;&#25442;&#21160;&#20316;&#30340;&#39069;&#22806;&#25104;&#26412;$c(x_t,x_{t-1})$&#36873;&#25321;&#21160;&#20316;$x_t$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#19979;&#30340;CFC&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#20004;&#31181;&#24773;&#22659;&#19979;&#21516;&#26102;&#33719;&#24471;&#24615;&#33021;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24179;&#26041;$\ell_2$&#33539;&#25968;&#30340;&#20999;&#25442;&#25104;&#26412;&#21644;&#19968;&#31867;&#24191;&#27867;&#30340;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#36825;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#26497;&#23567;&#21270;&#24207;&#21015;&#35201;&#20040;&#24418;&#25104;&#38789;&#65292;&#35201;&#20040;&#30001;&#23545;&#25163;&#36873;&#25321;&#12290;&#36825;&#26159;&#39318;&#20010;&#20351;&#29992;&#38543;&#26426;&#26694;&#26550;&#30740;&#31350;CFC&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#26368;&#20339;&#38543;&#26426;&#22312;&#32447;&#31639;&#27861;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23545;&#38543;&#26426;&#21644;&#23545;&#25239;&#24615;&#24773;&#26223;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#22312;&#38543;&#26426;&#24773;&#22659;&#19979;&#65292;&#23545;&#25239;&#24615;&#26368;&#20248;&#31639;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#20102;&#20004;&#31181;&#24773;&#22659;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convex function chasing (CFC) is an online optimization problem in which during each round $t$, a player plays an action $x_t$ in response to a hitting cost $f_t(x_t)$ and an additional cost of $c(x_t,x_{t-1})$ for switching actions. We study the CFC problem in stochastic and adversarial environments, giving algorithms that achieve performance guarantees simultaneously in both settings. Specifically, we consider the squared $\ell_2$-norm switching costs and a broad class of quadratic hitting costs for which the sequence of minimizers either forms a martingale or is chosen adversarially. This is the first work that studies the CFC problem using a stochastic framework. We provide a characterization of the optimal stochastic online algorithm and, drawing a comparison between the stochastic and adversarial scenarios, we demonstrate that the adversarial-optimal algorithm exhibits suboptimal performance in the stochastic context. Motivated by this, we provide a best-of-both-worlds algorithm 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65292;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#12289;&#31574;&#30053;&#27169;&#22411;&#35757;&#32451;&#21644;&#31574;&#30053;&#27169;&#22411;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#23548;&#33268;&#27169;&#22411;&#34892;&#20026;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00168</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#65306;&#23545;&#40784;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;
The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback. (arXiv:2311.00168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00168
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65292;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#12289;&#31574;&#30053;&#27169;&#22411;&#35757;&#32451;&#21644;&#31574;&#30053;&#27169;&#22411;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#23548;&#33268;&#27169;&#22411;&#34892;&#20026;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#30446;&#26631;&#19981;&#21305;&#37197;&#38382;&#39064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#26131;&#20110;&#25552;&#31034;&#24182;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#26356;&#26377;&#33021;&#21147;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;RLHF&#26680;&#24515;&#26159;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;LLM&#30340;&#26032;&#24037;&#20855;&#21253;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23450;&#24615;&#35757;&#32451;&#30446;&#26631;&#30340;&#25972;&#21512;&#12290;&#22312;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20013;&#65292;&#29992;&#25143;&#20559;&#22909;&#21644;&#19979;&#28216;&#24615;&#33021;&#20043;&#38388;&#30340;&#21305;&#37197;&#23581;&#35797;&#23548;&#33268;&#20102;&#19968;&#20010;&#20248;&#21270;&#26223;&#35266;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#25351;&#26631;&#30475;&#36215;&#26469;&#21487;&#33021;&#26159;&#30456;&#20851;&#30340;&#12290;&#36825;&#31181;&#34920;&#38754;&#19978;&#30340;&#30456;&#20851;&#20851;&#31995;&#21487;&#33021;&#23548;&#33268;&#24847;&#24819;&#19981;&#21040;&#30340;&#34892;&#20026;&#21644;&#8220;&#36807;&#24230;RLHF&#8221;&#30340;&#24773;&#20917;&#12290;&#22312;RLHF&#20013;&#65292;&#30001;&#20110;&#20197;&#19979;&#23376;&#27169;&#22359;&#19981;&#19968;&#33268;&#65292;&#20250;&#20986;&#29616;&#25361;&#25112;&#65306;&#22870;&#21169;&#27169;&#22411;&#35757;&#32451;&#12289;&#31574;&#30053;&#27169;&#22411;&#35757;&#32451;&#21644;&#31574;&#30053;&#27169;&#22411;&#35780;&#20272;&#12290;&#36825;&#31181;&#19981;&#21305;&#37197;&#23548;&#33268;&#27169;&#22411;&#26377;&#26102;&#20250;&#36991;&#20813;&#29992;&#25143;&#35831;&#27714;&#30340;&#34394;&#20551;&#23433;&#20840;&#26631;&#24535;&#65292;&#24456;&#38590;&#24341;&#23548;&#27169;&#22411;&#26397;&#30528;&#39044;&#26399;&#30340;&#29305;&#24449;&#21457;&#23637;&#65292;&#25110;&#32773;&#24635;&#26159;&#20197;&#29305;&#23450;&#30340;&#39118;&#26684;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to prompt and more capable in complex settings. RLHF at its core is providing a new toolkit to optimize LLMs other than next-token prediction, enabling the integration of qualitative training goals. The attempted match between user preferences and downstream performance, which happens in a learned reward model, results in an optimization landscape where training and evaluation metrics can appear correlated. The apparent correlation can lead to unexpected behaviors and stories of "too much RLHF." In RLHF, challenges emerge because the following sub-modules are not consistent with each other: the reward model training, the policy model training, and the policy model evaluation. This mismatch results in models that sometimes avoid user requests for false safety flags, are difficult to steer to an intended characteristic, or always answer in a specific style. As
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HIS-Unet&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21152;&#26435;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;&#30340;&#39044;&#27979;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;HIS-Unet&#22312;&#28023;&#20912;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.00167</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#21271;&#20912;&#27915;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multi-task Deep Convolutional Network to Predict Sea Ice Concentration and Drift in the Arctic Ocean. (arXiv:2311.00167v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00167
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HIS-Unet&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21152;&#26435;&#27880;&#24847;&#21147;&#27169;&#22359;&#23454;&#29616;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;&#30340;&#39044;&#27979;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;HIS-Unet&#22312;&#28023;&#20912;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21271;&#20912;&#27915;&#22320;&#21306;&#65292;&#39044;&#27979;&#28023;&#20912;&#27987;&#24230;(SIC)&#21644;&#28023;&#20912;&#28418;&#31227;(SID)&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#26368;&#36817;&#30340;&#27668;&#20505;&#21464;&#26262;&#24050;&#32463;&#25913;&#21464;&#20102;&#36825;&#20010;&#29615;&#22659;&#12290;&#30001;&#20110;&#29289;&#29702;&#28023;&#20912;&#27169;&#22411;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#22797;&#26434;&#30340;&#21442;&#25968;&#21270;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#26377;&#25928;&#26367;&#20195;&#29289;&#29702;&#27169;&#22411;&#65292;&#24182;&#25552;&#39640;&#28023;&#20912;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#20840;&#21367;&#31215;&#32593;&#32476;&#26550;&#26500;&#65292;&#21517;&#20026;Hierarchical Information-Sharing U-Net (HIS-Unet)&#65292;&#29992;&#20110;&#39044;&#27979;&#27599;&#26085;&#30340;SIC&#21644;SID&#12290;&#25105;&#20204;&#36890;&#36807;&#21152;&#26435;&#27880;&#24847;&#21147;&#27169;&#22359;(WAMs)&#20801;&#35768;SIC&#21644;SID&#23618;&#20849;&#20139;&#20449;&#24687;&#65292;&#24182;&#20114;&#30456;&#36741;&#21161;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#32479;&#35745;&#26041;&#27861;&#12289;&#28023;&#20912;&#29289;&#29702;&#27169;&#22411;&#21644;&#27809;&#26377;&#20449;&#24687;&#20849;&#20139;&#21333;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;HIS-Unet&#22312;SIC&#21644;SID&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;&#22312;&#39044;&#27979;&#21271;&#20912;&#27915;&#28023;&#20912;&#27987;&#24230;&#21644;&#28418;&#31227;&#26041;&#38754;&#65292;HIS-Unet&#30340;&#25913;&#36827;&#37117;&#26159;&#26174;&#33879;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting sea ice concentration (SIC) and sea ice drift (SID) in the Arctic Ocean is of great significance as the Arctic environment has been changed by the recent warming climate. Given that physical sea ice models require high computational costs with complex parameterization, deep learning techniques can effectively replace the physical model and improve the performance of sea ice prediction. This study proposes a novel multi-task fully conventional network architecture named hierarchical information-sharing U-net (HIS-Unet) to predict daily SIC and SID. Instead of learning SIC and SID separately at each branch, we allow the SIC and SID layers to share their information and assist each other's prediction through the weighting attention modules (WAMs). Consequently, our HIS-Unet outperforms other statistical approaches, sea ice physical models, and neural networks without such information-sharing units. The improvement of HIS-Unet is obvious both for SIC and SID prediction when and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36947;&#36335;&#20132;&#36890;&#20107;&#25925;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20107;&#25925;&#21457;&#29983;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;GraphSAGE&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#19978;&#30340;&#20107;&#25925;&#25968;&#37327;&#65292;&#24182;&#21028;&#26029;&#20107;&#25925;&#26159;&#21542;&#20250;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2311.00164</link><description>&lt;p&gt;
&#36947;&#36335;&#23433;&#20840;&#24314;&#27169;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#29992;&#20110;&#20107;&#25925;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis. (arXiv:2311.00164v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36947;&#36335;&#20132;&#36890;&#20107;&#25925;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20107;&#25925;&#21457;&#29983;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;GraphSAGE&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#19978;&#30340;&#20107;&#25925;&#25968;&#37327;&#65292;&#24182;&#21028;&#26029;&#20107;&#25925;&#26159;&#21542;&#20250;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#36830;&#25509;&#21644;&#20132;&#36890;&#27969;&#37327;&#30340;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20351;&#29992;&#21382;&#21490;&#35760;&#24405;&#35774;&#35745;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#32570;&#20047;&#20849;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#32570;&#20047;&#20844;&#20849;&#20107;&#25925;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#32479;&#19968;&#30340;&#36947;&#36335;&#20132;&#36890;&#20107;&#25925;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#32654;&#22269;&#21508;&#24030;&#23448;&#26041;&#25253;&#21578;&#30340;900&#19975;&#26465;&#35760;&#24405;&#65292;&#20197;&#21450;&#36947;&#36335;&#32593;&#32476;&#21644;&#20132;&#36890;&#27969;&#37327;&#25253;&#21578;&#12290;&#21033;&#29992;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20107;&#25925;&#21457;&#29983;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#20687;GraphSAGE&#36825;&#26679;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#19978;&#30340;&#20107;&#25925;&#25968;&#37327;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#19981;&#36229;&#36807;&#23454;&#38469;&#25968;&#30446;&#30340;22%&#65292;&#24182;&#33021;&#22815;&#21028;&#26029;&#20107;&#25925;&#26159;&#21542;&#20250;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24471;&#20998;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2311.00157</link><description>&lt;p&gt;
&#19968;&#20010;&#26356;&#24555;&#30340;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#24471;&#20998;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score Normalization for a Faster Diffusion Exponential Integrator Sampler. (arXiv:2311.00157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24471;&#20998;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24352;&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#24555;&#36895;&#29983;&#25104;&#26679;&#26412;&#30340;&#25193;&#25955;&#25351;&#25968;&#31215;&#20998;&#22120;&#37319;&#26679;&#22120;&#65288;DEIS&#65289;&#12290;&#23427;&#21033;&#29992;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#21322;&#32447;&#24615;&#29305;&#24615;&#26469;&#22823;&#22823;&#20943;&#23567;&#31215;&#20998;&#35823;&#24046;&#65292;&#24182;&#22312;&#20302;&#20989;&#25968;&#35780;&#20272;&#27425;&#25968;&#65288;NFEs&#65289;&#26102;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#24471;&#20998;&#20989;&#25968;&#37325;&#21442;&#25968;&#21270;&#65292;&#23427;&#36890;&#36807;&#20943;&#23569;&#22312;&#27599;&#20010;&#31215;&#20998;&#27493;&#39588;&#20013;&#20351;&#29992;&#22266;&#23450;&#24471;&#20998;&#20989;&#25968;&#20272;&#35745;&#32780;&#24341;&#36215;&#30340;&#31215;&#20998;&#35823;&#24046;&#12290;&#21407;&#22987;&#20316;&#32773;&#20351;&#29992;&#20102;&#29992;&#20110;&#22122;&#22768;&#39044;&#27979;&#35757;&#32451;&#30340;&#27169;&#22411;&#40664;&#35748;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21363;&#23558;&#24471;&#20998;&#20056;&#20197;&#26465;&#20214;&#27491;&#21521;&#22122;&#22768;&#20998;&#24067;&#30340;&#26631;&#20934;&#24046;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#36825;&#31181;&#24471;&#20998;&#21442;&#25968;&#21270;&#30340;&#32477;&#23545;&#24179;&#22343;&#20540;&#22312;&#22823;&#37096;&#20998;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#25509;&#36817;&#24120;&#25968;&#65292;&#20294;&#22312;&#37319;&#26679;&#32467;&#26463;&#26102;&#23427;&#20250;&#36805;&#36895;&#21464;&#21270;&#12290;&#20026;&#20102;&#31616;&#21333;&#20462;&#22797;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#24471;&#20998;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65288;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Recently, zhang et al have proposed the Diffusion Exponential Integrator Sampler (DEIS) for fast generation of samples from Diffusion Models. It leverages the semi-linear nature of the probability flow ordinary differential equation (ODE) in order to greatly reduce integration error and improve generation quality at low numbers of function evaluations (NFEs). Key to this approach is the score function reparameterisation, which reduces the integration error incurred from using a fixed score function estimate over each integration step. The original authors use the default parameterisation used by models trained for noise prediction -- multiply the score by the standard deviation of the conditional forward noising distribution. We find that although the mean absolute value of this score parameterisation is close to constant for a large portion of the reverse sampling process, it changes rapidly at the end of sampling. As a simple fix, we propose to instead reparameterise the score (at in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Medi-CAT&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20811;&#26381;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#36716;&#25442;&#22120;&#26469;&#35299;&#20915;&#27424;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#24615;&#21644;&#23545;&#27604;&#24615;&#23398;&#20064;&#25216;&#26415;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.00154</link><description>&lt;p&gt;
Medi-CAT&#65306;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#27604;&#24615;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Medi-CAT: Contrastive Adversarial Training for Medical Image Classification. (arXiv:2311.00154v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Medi-CAT&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#20811;&#26381;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#36716;&#25442;&#22120;&#26469;&#35299;&#20915;&#27424;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#23545;&#25239;&#24615;&#21644;&#23545;&#27604;&#24615;&#23398;&#20064;&#25216;&#26415;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#21487;&#29992;&#30340;&#22823;&#22411;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19981;&#22810;&#12290;&#23545;&#20110;&#36825;&#20123;&#25968;&#25454;&#38598;&#65292;&#22826;&#23567;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26080;&#27861;&#23398;&#20064;&#21040;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#30001;&#20110;&#27424;&#25311;&#21512;&#32780;&#25928;&#26524;&#19981;&#20339;&#65292;&#32780;&#22826;&#22823;&#30340;&#27169;&#22411;&#21017;&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#26377;&#38480;&#30340;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#20043;&#38388;&#38656;&#35201;&#20570;&#20986;&#25240;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#8212;&#8212;Medi-CAT&#65292;&#20197;&#20811;&#26381;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#30340;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#37319;&#29992;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#36716;&#25442;&#22120;&#26469;&#35299;&#20915;&#27424;&#25311;&#21512;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#24615;&#21644;&#23545;&#27604;&#24615;&#23398;&#20064;&#25216;&#26415;&#26469;&#38450;&#27490;&#36807;&#25311;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;MedMNIST&#38598;&#21512;&#20013;&#30340;&#22235;&#20010;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#32780;&#22312;&#22522;&#32447;&#26041;&#27861;&#19978;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are not many large medical image datasets available. For these datasets, too small deep learning models can't learn useful features, so they don't work well due to underfitting, and too big models tend to overfit the limited data. As a result, there is a compromise between the two issues. This paper proposes a training strategy Medi-CAT to overcome the underfitting and overfitting phenomena in medical imaging datasets. Specifically, the proposed training methodology employs large pre-trained vision transformers to overcome underfitting and adversarial and contrastive learning techniques to prevent overfitting. The proposed method is trained and evaluated on four medical image classification datasets from the MedMNIST collection. Our experimental results indicate that the proposed approach improves the accuracy up to 2% on three benchmark datasets compared to well-known approaches, whereas it increases the performance up to 4.1% over the baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#31454;&#36873;&#27963;&#21160;&#30340;&#36127;&#38754;&#24773;&#32490;&#65292;&#21253;&#25324;&#19968;&#20010;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23545;&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;50&#21517;&#25919;&#27835;&#29992;&#25143;&#30340;5,100&#26465;&#27874;&#26031;&#35821;&#25512;&#25991;&#36827;&#34892;&#26631;&#27880;&#65292;&#24314;&#31435;&#20102;&#25152;&#38656;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00143</link><description>&lt;p&gt;
&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;&#65292;&#20351;&#29992;&#36724;&#23884;&#20837;&#30340;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#36827;&#34892;&#25919;&#27835;&#29992;&#25143;&#25512;&#25991;&#20013;&#30340;&#36127;&#38754;&#24773;&#32490;&#26816;&#27979;&#65306;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Two-Stage Classifier for Campaign Negativity Detection using Axis Embeddings: A Case Study on Tweets of Political Users during 2021 Presidential Election in Iran. (arXiv:2311.00143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#31454;&#36873;&#27963;&#21160;&#30340;&#36127;&#38754;&#24773;&#32490;&#65292;&#21253;&#25324;&#19968;&#20010;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#36890;&#36807;&#23545;&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#26399;&#38388;50&#21517;&#25919;&#27835;&#29992;&#25143;&#30340;5,100&#26465;&#27874;&#26031;&#35821;&#25512;&#25991;&#36827;&#34892;&#26631;&#27880;&#65292;&#24314;&#31435;&#20102;&#25152;&#38656;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#21508;&#22320;&#30340;&#36873;&#20030;&#20013;&#65292;&#20505;&#36873;&#20154;&#21487;&#33021;&#20250;&#22240;&#22833;&#36133;&#21069;&#26223;&#21644;&#26102;&#38388;&#21387;&#21147;&#32780;&#23558;&#20182;&#20204;&#30340;&#31454;&#36873;&#27963;&#21160;&#36716;&#21521;&#36127;&#38754;&#24773;&#32490;&#12290;&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;Twitter&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#25919;&#27835;&#35805;&#35821;&#30340;&#20016;&#23500;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Twitter&#19978;&#21457;&#24067;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#33258;&#21160;&#21270;&#30340;&#31454;&#36873;&#36127;&#38754;&#24773;&#32490;&#26816;&#27979;&#31995;&#32479;&#22312;&#29702;&#35299;&#20505;&#36873;&#20154;&#21644;&#25919;&#20826;&#22312;&#31454;&#36873;&#27963;&#21160;&#20013;&#30340;&#31574;&#30053;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#31454;&#36873;&#27963;&#21160;&#30340;&#36127;&#38754;&#24773;&#32490;&#65292;&#21253;&#25324;&#19968;&#20010;&#20004;&#38454;&#27573;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#20102;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;50&#21517;&#25919;&#27835;&#29992;&#25143;&#65288;&#21253;&#25324;&#20505;&#36873;&#20154;&#21644;&#25919;&#24220;&#23448;&#21592;&#65289;&#30340;&#27874;&#26031;&#35821;&#25512;&#25991;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26631;&#27880;&#20102;&#20854;&#20013;5,100&#26465;&#25512;&#25991;&#65292;&#36825;&#20123;&#25512;&#25991;&#26159;&#22312;&#20234;&#26391;2021&#24180;&#24635;&#32479;&#36873;&#20030;&#21069;&#30340;&#19968;&#24180;&#20869;&#21457;&#24067;&#30340;&#12290;&#22312;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#39318;&#20808;&#36890;&#36807;&#25512;&#25991;&#23884;&#20837;&#19982;&#36724;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#26469;&#24314;&#31435;&#20004;&#20010;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In elections around the world, the candidates may turn their campaigns toward negativity due to the prospect of failure and time pressure. In the digital age, social media platforms such as Twitter are rich sources of political discourse. Therefore, despite the large amount of data that is published on Twitter, the automatic system for campaign negativity detection can play an essential role in understanding the strategy of candidates and parties in their campaigns. In this paper, we propose a hybrid model for detecting campaign negativity consisting of a two-stage classifier that combines the strengths of two machine learning models. Here, we have collected Persian tweets from 50 political users, including candidates and government officials. Then we annotated 5,100 of them that were published during the year before the 2021 presidential election in Iran. In the proposed model, first, the required datasets of two classifiers based on the cosine similarity of tweet embeddings with axis
&lt;/p&gt;</description></item><item><title>Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#20013;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21518;&#33021;&#20934;&#30830;&#39044;&#27979;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#24182;&#25512;&#26029;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21516;&#26102;&#33021;&#29992;&#20110;&#39044;&#27979;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2311.00136</link><description>&lt;p&gt;
Neuroformer&#65306;&#29992;&#20110;&#33041;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data. (arXiv:2311.00136v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00136
&lt;/p&gt;
&lt;p&gt;
Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#20013;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#21518;&#33021;&#20934;&#30830;&#39044;&#27979;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#24182;&#25512;&#26029;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21516;&#26102;&#33021;&#29992;&#20110;&#39044;&#27979;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#20135;&#29983;&#20102;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#38656;&#35201;&#26032;&#30340;&#20998;&#26512;&#24037;&#20855;&#12290;&#21463;&#21040;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22823;&#35268;&#27169;&#30340;&#32454;&#32990;&#20998;&#36776;&#29575;&#31070;&#32463;&#20803;&#23574;&#23792;&#25968;&#25454;&#30340;&#20998;&#26512;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#33258;&#22238;&#24402;&#30340;&#26102;&#31354;&#29983;&#25104;&#38382;&#39064;&#12290;Neuroformer&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;transformer&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#19987;&#20026;&#22788;&#29702;&#31995;&#32479;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32780;&#35774;&#35745;&#12290;&#23427;&#19982;&#29305;&#24449;&#22823;&#23567;&#21576;&#32447;&#24615;&#25193;&#23637;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#27169;&#24577;&#65292;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#39044;&#27979;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;Neuroformer&#65292;&#24182;&#21457;&#29616;&#23427;&#26082;&#33021;&#20934;&#30830;&#39044;&#27979;&#27169;&#25311;&#31070;&#32463;&#22238;&#36335;&#27963;&#21160;&#65292;&#20063;&#33021;&#20869;&#22312;&#22320;&#25512;&#26029;&#20986;&#24213;&#23618;&#31070;&#32463;&#22238;&#36335;&#36830;&#25509;&#24615;&#65292;&#21253;&#25324;&#26041;&#21521;&#12290;&#24403;&#39044;&#35757;&#32451;&#29992;&#20110;&#35299;&#30721;&#31070;&#32463;&#21709;&#24212;&#26102;&#65292;&#35813;&#27169;&#22411;&#33021;&#39044;&#27979;&#23567;&#40736;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#33041;&#21160;&#21147;&#23398;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22240;&#26524;&#21160;&#21147;&#22312;&#19981;&#21516;&#39057;&#29575;&#19979;&#21463;&#19981;&#21516;&#33041;&#21306;&#39537;&#21160;&#65292;&#36825;&#20026;&#29702;&#35299;&#33041;&#21151;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2311.00118</link><description>&lt;p&gt;
&#25552;&#21462;&#33041;&#21160;&#21147;&#23398;&#30340;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;
&lt;/p&gt;
&lt;p&gt;
Extracting the Multiscale Causal Backbone of Brain Dynamics. (arXiv:2311.00118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21462;&#33041;&#21160;&#21147;&#23398;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22240;&#26524;&#21160;&#21147;&#22312;&#19981;&#21516;&#39057;&#29575;&#19979;&#21463;&#19981;&#21516;&#33041;&#21306;&#39537;&#21160;&#65292;&#36825;&#20026;&#29702;&#35299;&#33041;&#21151;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37096;&#20998;&#20851;&#20110;&#33041;&#36830;&#25509;&#24615;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#33041;&#21306;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#32852;&#19978;&#65292;&#36825;&#19982;&#32479;&#27835;&#33041;&#21160;&#21147;&#23398;&#30340;&#22240;&#26524;&#26426;&#21046;&#19981;&#30452;&#25509;&#30456;&#20851;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#22240;&#26524;&#39592;&#26550;&#65288;MCB&#65289;&#65292;&#23427;&#26159;&#22312;&#22810;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#20849;&#20139;&#30340;&#19968;&#32452;&#20010;&#20307;&#30340;&#33041;&#21160;&#21147;&#23398;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#23427;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22810;&#23610;&#24230;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#20248;&#21270;&#20102;&#27169;&#22411;&#25311;&#21512;&#19982;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#35268;&#33539;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#30340;&#22522;&#32447;&#12290;&#24403;&#24212;&#29992;&#20110;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#24038;&#21491;&#33041;&#21322;&#29699;&#37117;&#26377;&#31232;&#30095;&#30340;MCB&#12290;&#30001;&#20110;&#20854;&#22810;&#23610;&#24230;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#26126;&#22312;&#20302;&#39057;&#24102;&#19978;&#65292;&#22240;&#26524;&#21160;&#21147;&#26469;&#33258;&#19982;&#39640;&#32423;&#35748;&#30693;&#21151;&#33021;&#30456;&#20851;&#30340;&#33041;&#21306;&#65307;&#32780;&#22312;&#26356;&#39640;&#30340;&#39057;&#29575;&#19978;&#65292;&#30001;nod&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bulk of the research effort on brain connectivity revolves around statistical associations among brain regions, which do not directly relate to the causal mechanisms governing brain dynamics. Here we propose the multiscale causal backbone (MCB) of brain dynamics shared by a set of individuals across multiple temporal scales, and devise a principled methodology to extract it.  Our approach leverages recent advances in multiscale causal structure learning and optimizes the trade-off between the model fitting and its complexity. Empirical assessment on synthetic data shows the superiority of our methodology over a baseline based on canonical functional connectivity networks. When applied to resting-state fMRI data, we find sparse MCBs for both the left and right brain hemispheres. Thanks to its multiscale nature, our approach shows that at low-frequency bands, causal dynamics are driven by brain regions associated with high-level cognitive functions; at higher frequencies instead, nod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;EXTRACT&#65292;&#26469;&#25511;&#21046;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26469;&#30740;&#31350;&#20449;&#24687;&#27844;&#28431;&#30340;&#23384;&#22312;&#31243;&#24230;&#21644;&#26469;&#28304;&#65292;&#24182;&#36890;&#36807;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#23558;&#23884;&#20837;&#20998;&#35299;&#20026;&#31169;&#26377;&#23646;&#24615;&#30340;&#24635;&#21644;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#20174;&#29992;&#25143;&#30340;&#34892;&#20026;&#20013;&#25512;&#26029;&#20986;&#21508;&#31181;&#20010;&#20154;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00115</link><description>&lt;p&gt;
EXTRACT: &#35299;&#37322;&#24615;&#36879;&#26126;&#25511;&#21046;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
EXTRACT: Explainable Transparent Control of Bias in Embeddings. (arXiv:2311.00115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;EXTRACT&#65292;&#26469;&#25511;&#21046;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26469;&#30740;&#31350;&#20449;&#24687;&#27844;&#28431;&#30340;&#23384;&#22312;&#31243;&#24230;&#21644;&#26469;&#28304;&#65292;&#24182;&#36890;&#36807;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#23558;&#23884;&#20837;&#20998;&#35299;&#20026;&#31169;&#26377;&#23646;&#24615;&#30340;&#24635;&#21644;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#20174;&#29992;&#25143;&#30340;&#34892;&#20026;&#20013;&#25512;&#26029;&#20986;&#21508;&#31181;&#20010;&#20154;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#22312;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#34920;&#31034;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#32780;&#22270;&#23884;&#20837;&#24050;&#36805;&#36895;&#25104;&#20026;&#19968;&#31181;&#26631;&#20934;&#25216;&#26415;&#65292;&#20197;&#20415;&#20197;&#19968;&#31181;&#26131;&#20110;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;&#26041;&#24335;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#12290;&#30001;&#20110;&#36825;&#31181;&#34920;&#31034;&#26159;&#20174;&#34892;&#20026;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#65292;&#26080;&#27861;&#34987;&#20154;&#31867;&#38405;&#35835;&#65292;&#22240;&#27492;&#23384;&#22312;&#19968;&#20010;&#25285;&#24551;&#65292;&#21363;&#23427;&#21487;&#33021;&#21253;&#21547;&#24847;&#22806;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#20559;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EXTRACT&#65306;&#19968;&#22871;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#26469;&#25511;&#21046;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#65292;&#20197;&#35780;&#20272;&#21644;&#20943;&#23569;&#21463;&#20445;&#25252;&#20449;&#24687;&#30340;&#38544;&#21547;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#26469;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#20449;&#24687;&#27844;&#28431;&#30340;&#23384;&#22312;&#31243;&#24230;&#21644;&#26469;&#28304;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#23558;&#23884;&#20837;&#20998;&#35299;&#20026;&#31169;&#26377;&#23646;&#24615;&#30340;&#24635;&#21644;&#12290;&#25105;&#20204;&#22312;MovieLens1M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#20197;&#20174;&#29992;&#25143;&#30340;&#34892;&#20026;&#20013;&#25512;&#26029;&#20986;&#21508;&#31181;&#20010;&#20154;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs are a widely used method to represent relations between entities in various AI applications, and Graph Embedding has rapidly become a standard technique to represent Knowledge Graphs in such a way as to facilitate inferences and decisions. As this representation is obtained from behavioural data, and is not in a form readable by humans, there is a concern that it might incorporate unintended information that could lead to biases. We propose EXTRACT: a suite of Explainable and Transparent methods to ConTrol bias in knowledge graph embeddings, so as to assess and decrease the implicit presence of protected information. Our method uses Canonical Correlation Analysis (CCA) to investigate the presence, extent and origins of information leaks during training, then decomposes embeddings into a sum of their private attributes by solving a linear system. Our experiments, performed on the MovieLens1M dataset, show that a range of personal attributes can be inferred from a user's
&lt;/p&gt;</description></item><item><title>FairWASP&#26159;&#19968;&#31181;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#20844;&#24179;Wasserstein&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25968;&#25454;&#38598;&#26469;&#20943;&#23569;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#20154;&#21475;&#24179;&#31561;&#24615;&#20934;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#21487;&#20197;&#36755;&#20837;&#20219;&#20309;&#20998;&#31867;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2311.00109</link><description>&lt;p&gt;
FairWASP&#65306;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#20844;&#24179;Wasserstein&#39044;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
FairWASP: Fast and Optimal Fair Wasserstein Pre-processing. (arXiv:2311.00109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00109
&lt;/p&gt;
&lt;p&gt;
FairWASP&#26159;&#19968;&#31181;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#20844;&#24179;Wasserstein&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#25968;&#25454;&#38598;&#26469;&#20943;&#23569;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#20154;&#21475;&#24179;&#31561;&#24615;&#20934;&#21017;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#21487;&#20197;&#36755;&#20837;&#20219;&#20309;&#20998;&#31867;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#26088;&#22312;&#20943;&#23569;&#19981;&#21516;&#23376;&#32676;&#20307;&#20043;&#38388;&#27169;&#22411;&#36755;&#20986;&#30340;&#19981;&#24179;&#31561;&#24615;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#20250;&#34987;&#19981;&#21516;&#29992;&#25143;&#22312;&#22810;&#20010;&#19979;&#28216;&#24212;&#29992;&#20013;&#20351;&#29992;&#65292;&#36825;&#24847;&#21619;&#30528;&#23545;&#35757;&#32451;&#25968;&#25454;&#26412;&#36523;&#36827;&#34892;&#24178;&#39044;&#21487;&#33021;&#26159;&#26368;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;FairWASP&#65292;&#26088;&#22312;&#20943;&#23569;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#31561;&#24615;&#65292;&#32780;&#19981;&#20250;&#20462;&#25913;&#21407;&#22987;&#25968;&#25454;&#12290;FairWASP&#36820;&#22238;&#26679;&#26412;&#32423;&#26435;&#37325;&#65292;&#20351;&#37325;&#26032;&#21152;&#26435;&#30340;&#25968;&#25454;&#38598;&#26368;&#23567;&#21270;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;Wasserstein&#36317;&#31163;&#65292;&#21516;&#26102;&#28385;&#36275;&#65288;&#32463;&#39564;&#29256;&#26412;&#30340;&#65289;&#20154;&#21475;&#24179;&#31561;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25972;&#25968;&#26435;&#37325;&#30340;&#26368;&#20248;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#31561;&#21516;&#22320;&#29702;&#35299;&#20026;&#22797;&#21046;&#25110;&#21024;&#38500;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;FairWASP&#21487;&#29992;&#20110;&#26500;&#24314;&#21487;&#20197;&#36755;&#20837;&#20219;&#20309;&#20998;&#31867;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#25509;&#21463;&#26679;&#26412;&#26435;&#37325;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen a surge of machine learning approaches aimed at reducing disparities in model outputs across different subgroups. In many settings, training data may be used in multiple downstream applications by different users, which means it may be most effective to intervene on the training data itself. In this work, we present FairWASP, a novel pre-processing approach designed to reduce disparities in classification datasets without modifying the original data. FairWASP returns sample-level weights such that the reweighted dataset minimizes the Wasserstein distance to the original dataset while satisfying (an empirical version of) demographic parity, a popular fairness criterion. We show theoretically that integer weights are optimal, which means our method can be equivalently understood as duplicating or eliminating samples. FairWASP can therefore be used to construct datasets which can be fed into any classification method, not just methods which accept sample weights. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21387;&#32553;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;3D&#22320;&#38663;&#21453;&#28436;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#38477;&#32500;&#25805;&#20316;&#31526;&#21644;DCNN&#23454;&#29616;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#26497;&#23569;&#25968;&#22320;&#38663;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#20102;&#25968;&#37327;&#32423;&#30340;&#22320;&#38663;&#35760;&#24405;&#20351;&#29992;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#38598;&#30456;&#24403;&#30340;3D&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2311.00107</link><description>&lt;p&gt;
3D&#22320;&#38663;&#21453;&#28436;&#30340;&#28145;&#24230;&#21387;&#32553;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Compressed Learning for 3D Seismic Inversion. (arXiv:2311.00107v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21387;&#32553;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;3D&#22320;&#38663;&#21453;&#28436;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#38477;&#32500;&#25805;&#20316;&#31526;&#21644;DCNN&#23454;&#29616;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20351;&#29992;&#26497;&#23569;&#25968;&#22320;&#38663;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#20943;&#23569;&#20102;&#25968;&#37327;&#32423;&#30340;&#22320;&#38663;&#35760;&#24405;&#20351;&#29992;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#38598;&#30456;&#24403;&#30340;3D&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#26497;&#23569;&#25968;&#22320;&#38663;&#28304;&#20174;&#39044;&#21472;&#21069;&#25968;&#25454;&#36827;&#34892;3D&#22320;&#38663;&#21453;&#28436;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#21387;&#32553;&#24863;&#30693;&#21644;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#32452;&#21512;&#65292;&#31216;&#20026;&#21387;&#32553;&#23398;&#20064;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#21516;&#26102;&#20248;&#21270;&#19968;&#20010;&#38477;&#32500;&#25805;&#20316;&#31526;&#21644;&#19968;&#20010;&#30001;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(DCNN)&#23454;&#29616;&#30340;3D&#21453;&#28436;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#31232;&#30095;&#30340;&#20108;&#20540;&#24863;&#30693;&#23618;&#26469;&#23454;&#29616;&#38477;&#32500;&#65292;&#35813;&#23618;&#36873;&#25321;&#21487;&#29992;&#22320;&#38663;&#28304;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#28982;&#21518;&#36873;&#25321;&#30340;&#25968;&#25454;&#34987;&#36755;&#20837;&#21040;DCNN&#20013;&#23436;&#25104;&#22238;&#24402;&#20219;&#21153;&#12290;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#36807;&#31243;&#22312;&#35757;&#32451;&#26399;&#38388;&#20943;&#23569;&#20102;&#25968;&#37327;&#32423;&#30340;&#22320;&#38663;&#35760;&#24405;&#20351;&#29992;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19982;&#20351;&#29992;&#25972;&#20010;&#25968;&#25454;&#38598;&#30456;&#24403;&#30340;3D&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of 3D seismic inversion from pre-stack data using a very small number of seismic sources. The proposed solution is based on a combination of compressed-sensing and machine learning frameworks, known as compressed-learning. The solution jointly optimizes a dimensionality reduction operator and a 3D inversion encoder-decoder implemented by a deep convolutional neural network (DCNN). Dimensionality reduction is achieved by learning a sparse binary sensing layer that selects a small subset of the available sources, then the selected data is fed to a DCNN to complete the regression task. The end-to-end learning process provides a reduction by an order-of-magnitude in the number of seismic records used during training, while preserving the 3D reconstruction quality comparable to that obtained by using the entire dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Bandit&#31639;&#27861;&#30340;&#25209;&#27425;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22312;&#26631;&#31614;&#22122;&#38899;&#19979;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#27700;&#24179;&#30340;&#26631;&#31614;&#27745;&#26579;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#20351;&#29992;&#36741;&#21161;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.00096</link><description>&lt;p&gt;
&#20026;&#20102;&#22312;&#26631;&#31614;&#22122;&#38899;&#19979;&#36827;&#34892;&#31283;&#20581;&#23398;&#20064;&#65292;&#22522;&#20110;Bandit&#39537;&#21160;&#30340;&#25209;&#27425;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bandit-Driven Batch Selection for Robust Learning under Label Noise. (arXiv:2311.00096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Bandit&#31639;&#27861;&#30340;&#25209;&#27425;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22312;&#26631;&#31614;&#22122;&#38899;&#19979;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#27700;&#24179;&#30340;&#26631;&#31614;&#27745;&#26579;&#19979;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#20351;&#29992;&#36741;&#21161;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32452;&#21512;&#36172;&#21338;&#31639;&#27861;&#26469;&#36873;&#25321;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#35757;&#32451;&#20013;&#30340;&#25209;&#27425;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#26631;&#31614;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#27700;&#24179;&#30340;&#26631;&#31614;&#27745;&#26579;&#19979;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#24102;&#26469;&#24120;&#35265;&#30340;&#36741;&#21161;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36825;&#31181;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#24037;&#20316;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#27169;&#22411;&#25928;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#65292;&#20026;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel approach for batch selection in Stochastic Gradient Descent (SGD) training, leveraging combinatorial bandit algorithms. Our methodology focuses on optimizing the learning process in the presence of label noise, a prevalent issue in real-world datasets. Experimental evaluations on the CIFAR-10 dataset reveal that our approach consistently outperforms existing methods across various levels of label corruption. Importantly, we achieve this superior performance without incurring the computational overhead commonly associated with auxiliary neural network models. This work presents a balanced trade-off between computational efficiency and model efficacy, offering a scalable solution for complex machine learning applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#22788;&#29702;&#24615;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00094</link><description>&lt;p&gt;
&#34920;&#36798;&#24314;&#27169;&#23545;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19981;&#36275;&#65306;&#21487;&#22788;&#29702;&#30340;&#25512;&#29702;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Expressive Modeling Is Insufficient for Offline RL: A Tractable Inference Perspective. (arXiv:2311.00094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#65292;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38500;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#22788;&#29702;&#24615;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#30340;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#31934;&#30830;&#19988;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#20363;&#26159;&#20808;&#23558;&#31163;&#32447;&#36712;&#36857;&#25311;&#21512;&#21040;&#19968;&#20010;&#24207;&#21015;&#27169;&#22411;&#20013;&#65292;&#28982;&#21518;&#36890;&#36807;&#35813;&#27169;&#22411;&#25552;&#31034;&#39640;&#26399;&#26395;&#22238;&#25253;&#30340;&#21160;&#20316;&#12290;&#34429;&#28982;&#26222;&#36941;&#35748;&#20026;&#34920;&#36798;&#24615;&#26356;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26412;&#25991;&#24378;&#35843;&#20102;&#21487;&#22788;&#29702;&#24615;&#65292;&#21363;&#31934;&#30830;&#32780;&#39640;&#25928;&#22320;&#22238;&#31572;&#21508;&#31181;&#27010;&#29575;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#21516;&#26679;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#31163;&#32447;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#21644;&#29615;&#22659;&#21160;&#24577;&#24102;&#26469;&#30340;&#22522;&#26412;&#38543;&#26426;&#24615;&#65292;&#38656;&#35201;&#36827;&#34892;&#39640;&#24230;&#38750;&#24179;&#20961;&#30340;&#26465;&#20214;/&#32422;&#26463;&#29983;&#25104;&#65292;&#20197;&#24341;&#20986;&#26377;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#34429;&#28982;&#20173;&#28982;&#21487;&#20197;&#36817;&#20284;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#31895;&#31961;&#30340;&#20272;&#35745;&#26174;&#33879;&#21066;&#24369;&#20102;&#34920;&#36798;&#24615;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#24102;&#26469;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Trifle&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#22788;&#29702;&#25512;&#29702;&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#29616;&#20195;&#21487;&#22788;&#29702;&#27010;&#29575;&#27169;&#22411;&#65288;TPM&#65289;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
A popular paradigm for offline Reinforcement Learning (RL) tasks is to first fit the offline trajectories to a sequence model, and then prompt the model for actions that lead to high expected return. While a common consensus is that more expressive sequence models imply better performance, this paper highlights that tractability, the ability to exactly and efficiently answer various probabilistic queries, plays an equally important role. Specifically, due to the fundamental stochasticity from the offline data-collection policies and the environment dynamics, highly non-trivial conditional/constrained generation is required to elicit rewarding actions. While it is still possible to approximate such queries, we observe that such crude estimates significantly undermine the benefits brought by expressive sequence models. To overcome this problem, this paper proposes Trifle (Tractable Inference for Offline RL), which leverages modern Tractable Probabilistic Models (TPMs) to bridge the gap b
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21619;&#36947;&#29289;&#29702;&#23398;&#20013;&#25214;&#21040;&#30495;&#23454;&#32780;&#32654;&#20029;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.00087</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#21619;&#36947;&#29289;&#29702;&#23398;&#20013;&#23547;&#27714;&#30495;&#29702;&#21644;&#32654;
&lt;/p&gt;
&lt;p&gt;
Seeking Truth and Beauty in Flavor Physics with Machine Learning. (arXiv:2311.00087v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00087
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#21619;&#36947;&#29289;&#29702;&#23398;&#20013;&#25214;&#21040;&#30495;&#23454;&#32780;&#32654;&#20029;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#26032;&#30340;&#29702;&#35770;&#29289;&#29702;&#27169;&#22411;&#30340;&#21457;&#29616;&#36807;&#31243;&#28041;&#21450;&#21040;&#26082;&#31526;&#21512;&#29616;&#26377;&#23454;&#39564;&#25968;&#25454;&#21448;&#28385;&#36275;&#25277;&#35937;&#29702;&#35770;&#23478;&#30340;&#32654;&#35266;&#12289;&#33258;&#28982;&#31561;&#26631;&#20934;&#30340;&#21452;&#37325;&#26041;&#38754;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#20197;Yukawa&#22840;&#20811;&#37096;&#38376;&#20316;&#20026;&#19968;&#20010;&#29609;&#20855;&#31034;&#20363;&#26469;&#28436;&#31034;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21270;&#23558;&#20135;&#29983;&#30495;&#23454;&#32780;&#32654;&#20029;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery process of building new theoretical physics models involves the dual aspect of both fitting to the existing experimental data and satisfying abstract theorists' criteria like beauty, naturalness, etc. We design loss functions for performing both of those tasks with machine learning techniques. We use the Yukawa quark sector as a toy example to demonstrate that the optimization of these loss functions results in true and beautiful models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#30446;&#26631;&#26816;&#27979;&#25216;&#26415;&#21644;&#25490;&#21517;&#26041;&#27861;&#26469;&#23454;&#29616;&#26368;&#21518;&#19968;&#23618;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00079</link><description>&lt;p&gt;
&#20813;&#36153;&#30340;Spuriosity&#25490;&#21517;: &#22522;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;&#26368;&#21518;&#19968;&#23618;&#35757;&#32451;&#30340;&#31616;&#21333;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Spuriosity Rankings for Free: A Simple Framework for Last Layer Retraining Based on Object Detection. (arXiv:2311.00079v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00079
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#30446;&#26631;&#26816;&#27979;&#25216;&#26415;&#21644;&#25490;&#21517;&#26041;&#27861;&#26469;&#23454;&#29616;&#26368;&#21518;&#19968;&#23618;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#26368;&#21518;&#19968;&#23618;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#21363;&#22312;&#27809;&#26377;&#34394;&#20551;&#32447;&#32034;&#30340;&#23567;&#25968;&#25454;&#23376;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#32447;&#24615;&#20998;&#31867;&#22120;&#22836;&#37096;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#27492;&#23376;&#38598;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#65292;&#38477;&#20302;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#36873;&#23450;&#23376;&#38598;&#20013;&#21487;&#33021;&#20173;&#23384;&#22312;&#34394;&#20551;&#32447;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#26694;&#26550;&#65292;&#21033;&#29992;&#24320;&#25918;&#35789;&#27719;&#30340;&#30446;&#26631;&#26816;&#27979;&#25216;&#26415;&#26469;&#35782;&#21035;&#27809;&#26377;&#34394;&#20551;&#32447;&#32034;&#30340;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#30446;&#26631;&#26816;&#27979;&#22120;&#20316;&#20026;&#23545;&#22270;&#20687;&#20013;&#30446;&#26631;&#29289;&#20307;&#23384;&#22312;&#30340;&#37327;&#21270;&#25351;&#26631;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#36825;&#20010;&#24471;&#20998;&#23545;&#22270;&#20687;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#22312;&#24471;&#20998;&#26368;&#39640;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#12290;&#25105;&#20204;&#22312;ImageNet-1k&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have exhibited remarkable performance in various domains. However, the reliance of these models on spurious features has raised concerns about their reliability. A promising solution to this problem is last-layer retraining, which involves retraining the linear classifier head on a small subset of data without spurious cues. Nevertheless, selecting this subset requires human supervision, which reduces its scalability. Moreover, spurious cues may still exist in the selected subset. As a solution to this problem, we propose a novel ranking framework that leverages an open vocabulary object detection technique to identify images without spurious cues. More specifically, we use the object detector as a measure to score the presence of the target object in the images. Next, the images are sorted based on this score, and the last-layer of the model is retrained on a subset of the data with the highest scores. Our experiments on the ImageNet-1k dataset demonstrate the eff
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36807;&#28388;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#23433;&#20840;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#32422;&#26463;&#25511;&#21046;&#25216;&#26415;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#12289;&#23454;&#26102;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00063</link><description>&lt;p&gt;
&#20351;&#29992;&#36807;&#28388;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#23433;&#20840;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Safe multi-agent motion planning under uncertainty for drones using filtered reinforcement learning. (arXiv:2311.00063v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00063
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36807;&#28388;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#23433;&#20840;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#32422;&#26463;&#25511;&#21046;&#25216;&#26415;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#23433;&#20840;&#24615;&#12289;&#23454;&#26102;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19981;&#30830;&#23450;&#12289;&#28151;&#20081;&#30340;&#24037;&#20316;&#31354;&#38388;&#20013;&#65292;&#38024;&#23545;&#26080;&#20154;&#26426;&#30340;&#23433;&#20840;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#65292;&#23427;&#32467;&#21512;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32422;&#26463;&#25511;&#21046;&#30340;&#36712;&#36857;&#35268;&#21010;&#30340;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#21333;&#26234;&#33021;&#20307;&#30340;&#24378;&#21270;&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#36798;&#30446;&#26631;&#30340;&#36816;&#21160;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#26041;&#26696;&#21487;&#33021;&#19981;&#26159;&#26080;&#30896;&#25758;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#20984;&#20248;&#21270;&#12289;&#27010;&#29575;&#32422;&#26463;&#21644;&#38598;&#21512;&#26041;&#27861;&#26469;&#36827;&#34892;&#32422;&#26463;&#25511;&#21046;&#65292;&#20197;&#30830;&#20445;&#23613;&#31649;&#24037;&#20316;&#31354;&#38388;&#12289;&#26234;&#33021;&#20307;&#36816;&#21160;&#21644;&#24863;&#30693;&#19981;&#30830;&#23450;&#65292;&#20173;&#33021;&#20445;&#25345;&#23433;&#20840;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#65292;&#24182;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#23454;&#29616;&#26234;&#33021;&#20307;&#20043;&#38388;&#20197;&#21450;&#19982;&#24037;&#20316;&#31354;&#38388;&#20013;&#30340;&#38745;&#24577;&#38556;&#30861;&#29289;&#30340;&#30896;&#25758;&#36991;&#20813;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#23433;&#20840;&#30340;&#12289;&#23454;&#26102;&#21487;&#34892;&#30340;&#12289;&#27604;&#20165;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26356;&#31616;&#21333;&#35757;&#32451;&#30340;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#25968;&#20540;&#27169;&#25311;&#21644;&#23454;&#39564;&#26174;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of safe multi-agent motion planning for drones in uncertain, cluttered workspaces. For this problem, we present a tractable motion planner that builds upon the strengths of reinforcement learning and constrained-control-based trajectory planning. First, we use single-agent reinforcement learning to learn motion plans from data that reach the target but may not be collision-free. Next, we use a convex optimization, chance constraints, and set-based methods for constrained control to ensure safety, despite the uncertainty in the workspace, agent motion, and sensing. The proposed approach can handle state and control constraints on the agents, and enforce collision avoidance among themselves and with static obstacles in the workspace with high probability. The proposed approach yields a safe, real-time implementable, multi-agent motion planner that is simpler to train than methods based solely on learning. Numerical simulations and experiments show the efficacy of 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22411;AI&#30340;&#24726;&#35770;&#30740;&#31350;&#20102;&#29983;&#25104;&#22411;&#27169;&#22411;&#19982;&#20154;&#31867;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#20135;&#29983;&#19987;&#23478;&#32423;&#36755;&#20986;&#30340;&#33021;&#21147;&#19978;&#21487;&#33021;&#36229;&#36807;&#20854;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00059</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;AI&#30340;&#24726;&#35770;&#65306;&#8220;&#23427;&#21487;&#20197;&#21019;&#24314;&#65292;&#20294;&#21487;&#33021;&#19981;&#29702;&#35299;&#8221;
&lt;/p&gt;
&lt;p&gt;
The Generative AI Paradox: "What It Can Create, It May Not Understand". (arXiv:2311.00059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00059
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;AI&#30340;&#24726;&#35770;&#30740;&#31350;&#20102;&#29983;&#25104;&#22411;&#27169;&#22411;&#19982;&#20154;&#31867;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#20135;&#29983;&#19987;&#23478;&#32423;&#36755;&#20986;&#30340;&#33021;&#21147;&#19978;&#21487;&#33021;&#36229;&#36807;&#20854;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29983;&#25104;&#22411;AI&#28010;&#28526;&#24341;&#36215;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20840;&#29699;&#20851;&#27880;&#65292;&#26082;&#26377;&#20852;&#22859;&#20063;&#26377;&#23545;&#20154;&#24037;&#26234;&#33021;&#28508;&#22312;&#36229;&#20154;&#27700;&#24179;&#30340;&#25285;&#24551;&#65306;&#29616;&#22312;&#30340;&#27169;&#22411;&#21482;&#38656;&#35201;&#20960;&#31186;&#38047;&#23601;&#33021;&#20135;&#29983;&#36229;&#36807;&#29978;&#33267;&#25361;&#25112;&#19987;&#23478;&#32423;&#20154;&#31867;&#33021;&#21147;&#30340;&#36755;&#20986;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#27169;&#22411;&#20173;&#28982;&#26174;&#31034;&#20986;&#21363;&#20351;&#38750;&#19987;&#23478;&#20063;&#19981;&#20250;&#39044;&#26399;&#20986;&#29616;&#30340;&#22522;&#26412;&#38169;&#35823;&#12290;&#36825;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#24726;&#35770;&#65306;&#25105;&#20204;&#22914;&#20309;&#35299;&#20915;&#30475;&#20284;&#36229;&#20154;&#33021;&#21147;&#21644;&#23569;&#25968;&#20154;&#31867;&#25165;&#20250;&#29359;&#38169;&#35823;&#30340;&#25345;&#32493;&#23384;&#22312;&#20043;&#38388;&#30340;&#30683;&#30462;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#27979;&#35797;&#20102;&#29983;&#25104;&#22411;AI&#24726;&#35770;&#20551;&#35774;&#65306;&#29983;&#25104;&#22411;&#27169;&#22411;&#30001;&#20110;&#30452;&#25509;&#35757;&#32451;&#20197;&#20135;&#29983;&#31867;&#20284;&#19987;&#23478;&#30340;&#36755;&#20986;&#65292;&#32780;&#33719;&#24471;&#30340;&#29983;&#25104;&#33021;&#21147;&#26159;&#19981;&#21463;&#21046;&#20110;&#20854;&#29702;&#35299;&#33021;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon -- and can therefore exceed -- their ability to unde
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35266;&#23519;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#31995;&#32479;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#21482;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#22312;&#25512;&#29702;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#25581;&#31034;&#20102;&#24213;&#23618;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00056</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#21644;&#25193;&#25955;&#65306;&#20851;&#20110;&#20855;&#26377;&#31283;&#23450;&#25193;&#25955;&#30340;&#21512;&#25104;&#22270;&#20687;&#20998;&#24067;&#30340;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Diversity and Diffusion: Observations on Synthetic Image Distributions with Stable Diffusion. (arXiv:2311.00056v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00056
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35266;&#23519;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#31995;&#32479;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#21482;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#22312;&#25512;&#29702;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#25581;&#31034;&#20102;&#24213;&#23618;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#22914;StableDiffusion&#12289;Imagen&#21644;DALL-E 2&#65292;&#20351;&#24471;&#36890;&#36807;&#31616;&#21333;&#30340;&#25991;&#26412;&#25552;&#31034;&#21019;&#24314;&#36924;&#30495;&#30340;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#35825;&#20154;&#30340;&#26159;&#20351;&#29992;&#36825;&#20123;&#31995;&#32479;&#26469;&#28040;&#38500;&#33719;&#21462;&#35757;&#32451;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#25152;&#38656;&#30340;&#33258;&#28982;&#22270;&#20687;&#30340;&#25163;&#21160;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29992;&#20110;&#35757;&#32451;&#30340;&#22270;&#20687;&#30475;&#36215;&#26469;&#36924;&#30495;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36827;&#34892;&#30340;&#25152;&#26377;&#23454;&#39564;&#37117;&#26174;&#31034;&#21482;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#22312;&#25512;&#29702;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#35814;&#32454;&#30740;&#31350;&#27492;&#26126;&#26174;&#30340;&#19981;&#19968;&#33268;&#23558;&#27934;&#23519;&#24213;&#23618;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#23616;&#38480;&#24615;&#12290;&#20174;&#22270;&#20687;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#25152;&#21019;&#24314;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#21078;&#26512;&#20102;&#21512;&#25104;&#22270;&#20687;&#21644;&#33258;&#28982;&#22270;&#20687;&#20013;&#35821;&#20041;&#19981;&#21305;&#37197;&#30340;&#24046;&#24322;&#12290;&#36825;&#23558;&#38416;&#26126;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;CLIP&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#25193;&#25955;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22235;&#20010;&#38480;&#21046;TTI&#31995;&#32479;&#22312;&#27492;&#20219;&#21153;&#20013;&#26377;&#29992;&#24615;&#30340;&#38382;&#39064;&#65306;&#19981;&#26126;&#30830;&#30340;&#35821;&#20041;&#12289;&#31867;&#21035;&#24179;&#34913;&#38382;&#39064;&#12289;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#38382;&#39064;&#21644;&#22270;&#20687;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in text-to-image (TTI) systems, such as StableDiffusion, Imagen, and DALL-E 2, have made it possible to create realistic images with simple text prompts. It is tempting to use these systems to eliminate the manual task of obtaining natural images for training a new machine learning classifier. However, in all of the experiments performed to date, classifiers trained solely with synthetic images perform poorly at inference, despite the images used for training appearing realistic. Examining this apparent incongruity in detail gives insight into the limitations of the underlying image generation processes. Through the lens of diversity in image creation vs.accuracy of what is created, we dissect the differences in semantic mismatches in what is modeled in synthetic vs. natural images. This will elucidate the roles of the image-languag emodel, CLIP, and the image generation model, diffusion. We find four issues that limit the usefulness of TTI systems for this task: ambigu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#20803;&#34920;&#31034;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#26080;&#35757;&#32451;&#27867;&#21270;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.00055</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#34920;&#31034;&#23545;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#26080;&#35757;&#32451;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Training-Free Generalization on Heterogeneous Tabular Data via Meta-Representation. (arXiv:2311.00055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#20803;&#34920;&#31034;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#26080;&#35757;&#32451;&#27867;&#21270;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#23646;&#24615;&#21644;&#31867;&#21035;&#31354;&#38388;&#30340;&#22266;&#26377;&#24322;&#36136;&#24615;&#38459;&#30861;&#20102;&#30693;&#35782;&#30340;&#26377;&#25928;&#20849;&#20139;&#65292;&#38480;&#21046;&#20102;&#34920;&#26684;&#27169;&#22411;&#20174;&#20854;&#20182;&#25968;&#25454;&#38598;&#20013;&#21463;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20803;&#34920;&#31034;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#39044;&#35757;&#32451;&#65288;TabPTM&#65289;&#65292;&#23427;&#20801;&#35768;&#19968;&#20010;&#34920;&#26684;&#27169;&#22411;&#22312;&#19968;&#32452;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#21644;&#31867;&#21035;&#30340;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TabPTM&#36890;&#36807;&#23454;&#20363;&#21040;&#22266;&#23450;&#25968;&#37327;&#30340;&#21407;&#22411;&#30340;&#36317;&#31163;&#26469;&#34920;&#31034;&#19968;&#20010;&#23454;&#20363;&#65292;&#20174;&#32780;&#26631;&#20934;&#21270;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#26469;&#23558;&#36825;&#20123;&#20803;&#34920;&#31034;&#19982;&#25968;&#25454;&#38598;&#29305;&#23450;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#20851;&#32852;&#36215;&#26469;&#65292;&#20351;TabPTM&#20855;&#26377;&#26080;&#38656;&#35757;&#32451;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#23454;TabPTM&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is prevalent across various machine learning domains. Yet, the inherent heterogeneities in attribute and class spaces across different tabular datasets hinder the effective sharing of knowledge, limiting a tabular model to benefit from other datasets. In this paper, we propose Tabular data Pre-Training via Meta-representation (TabPTM), which allows one tabular model pre-training on a set of heterogeneous datasets. Then, this pre-trained model can be directly applied to unseen datasets that have diverse attributes and classes without additional training. Specifically, TabPTM represents an instance through its distance to a fixed number of prototypes, thereby standardizing heterogeneous tabular datasets. A deep neural network is then trained to associate these meta-representations with dataset-specific classification confidences, endowing TabPTM with the ability of training-free generalization. Experiments validate that TabPTM achieves promising performance in new datasets, 
&lt;/p&gt;</description></item><item><title>Kolmogorov&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#31934;&#30830;&#22320;&#34920;&#31034;&#36830;&#32493;&#20989;&#25968;&#12289;&#26377;&#30028;&#19981;&#36830;&#32493;&#20989;&#25968;&#21644;&#25152;&#26377;&#26080;&#30028;&#22810;&#20803;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2311.00049</link><description>&lt;p&gt;
&#20851;&#20110; Kolmogorov &#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Kolmogorov neural networks. (arXiv:2311.00049v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00049
&lt;/p&gt;
&lt;p&gt;
Kolmogorov&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#31934;&#30830;&#22320;&#34920;&#31034;&#36830;&#32493;&#20989;&#25968;&#12289;&#26377;&#30028;&#19981;&#36830;&#32493;&#20989;&#25968;&#21644;&#25152;&#26377;&#26080;&#30028;&#22810;&#20803;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; Kolmogorov &#20004;&#20010;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#36830;&#32493;&#12289;&#19981;&#36830;&#32493;&#26377;&#30028;&#25110;&#32773;&#26080;&#30028;&#28608;&#27963;&#20989;&#25968;&#22312;&#31532;&#20108;&#20010;&#38544;&#34255;&#23618;&#26469;&#31934;&#30830;&#22320;&#34920;&#31034;&#36830;&#32493;&#20989;&#25968;&#12289;&#26377;&#30028;&#19981;&#36830;&#32493;&#20989;&#25968;&#21644;&#25152;&#26377;&#26080;&#30028;&#22810;&#20803;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show that the Kolmogorov two hidden layer neural network model with a continuous, discontinuous bounded or unbounded activation function in the second hidden layer can precisely represent continuous, discontinuous bounded and all unbounded multivariate functions, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SC-MIL&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#21516;&#26102;&#25913;&#36827;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00048</link><description>&lt;p&gt;
SC-MIL: &#29992;&#20110;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#31232;&#30095;&#32534;&#30721;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image Classification. (arXiv:2311.00048v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SC-MIL&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#21516;&#26102;&#25913;&#36827;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#22312;&#24369;&#30417;&#30563;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#31867;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#20856;&#22411;&#30340;MIL&#26041;&#27861;&#21253;&#25324;&#29305;&#24449;&#23884;&#20837;&#37096;&#20998;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#23558;&#23454;&#20363;&#23884;&#20837;&#21040;&#29305;&#24449;&#20013;&#65292;&#20197;&#21450;MIL&#32858;&#21512;&#22120;&#65292;&#23558;&#23454;&#20363;&#23884;&#20837;&#32452;&#21512;&#25104;&#39044;&#27979;&#32467;&#26524;&#12290;&#30446;&#21069;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#36825;&#20123;&#37096;&#20998;&#65292;&#24182;&#21333;&#29420;&#24314;&#27169;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32534;&#30721;&#30340;MIL&#65288;SC-MIL&#65289;&#65292;&#21516;&#26102;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#26041;&#38754;&#12290;&#31232;&#30095;&#23383;&#20856;&#23398;&#20064;&#36890;&#36807;&#23558;&#23454;&#20363;&#34920;&#31034;&#20026;&#36807;&#23436;&#22791;&#23383;&#20856;&#20013;&#21407;&#23376;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#26469;&#25429;&#25417;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#31232;&#30095;&#24615;&#21487;&#20197;&#36890;&#36807;&#25233;&#21046;&#19981;&#30456;&#20851;&#30340;&#23454;&#20363;&#32780;&#20445;&#30041;&#26368;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#22686;&#24378;&#23454;&#20363;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#20026;&#20102;&#25913;&#21892;&#20256;&#32479;&#30340;&#29305;&#24449;&#23884;&#20837;&#21644;&#23454;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;we proposed a sparsely coded MIL.
&lt;/p&gt;
&lt;p&gt;
Multiple Instance Learning (MIL) has been widely used in weakly supervised whole slide image (WSI) classification. Typical MIL methods include a feature embedding part that embeds the instances into features via a pre-trained feature extractor and the MIL aggregator that combines instance embeddings into predictions. The current focus has been directed toward improving these parts by refining the feature embeddings through self-supervised pre-training and modeling the correlations between instances separately. In this paper, we proposed a sparsely coded MIL (SC-MIL) that addresses those two aspects at the same time by leveraging sparse dictionary learning. The sparse dictionary learning captures the similarities of instances by expressing them as a sparse linear combination of atoms in an over-complete dictionary. In addition, imposing sparsity help enhance the instance feature embeddings by suppressing irrelevant instances while retaining the most relevant ones. To make the convention
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#35270;&#35273;&#24187;&#35273;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#25972;&#20307;&#23545;&#40784;&#24615;&#36739;&#20302;&#65292;&#20294;&#26356;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#24863;&#30693;&#24182;&#26356;&#23481;&#26131;&#21463;&#21040;&#35270;&#35273;&#24187;&#35273;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.00047</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#26469;&#22320;&#22522;&#35270;&#35273;&#24187;&#35273;&#65306;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#24863;&#30693;&#24187;&#35273;&#65311;
&lt;/p&gt;
&lt;p&gt;
Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?. (arXiv:2311.00047v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00047
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#35270;&#35273;&#24187;&#35273;&#30340;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#25972;&#20307;&#23545;&#40784;&#24615;&#36739;&#20302;&#65292;&#20294;&#26356;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#24863;&#30693;&#24182;&#26356;&#23481;&#26131;&#21463;&#21040;&#35270;&#35273;&#24187;&#35273;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26159;&#22312;&#20154;&#31867;&#29702;&#35299;&#19990;&#30028;&#30340;&#27169;&#25311;&#19979;&#65292;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#35757;&#32451;&#24471;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#23545;&#29616;&#23454;&#30340;&#24863;&#30693;&#24182;&#19981;&#24635;&#26159;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#24544;&#23454;&#21576;&#29616;&#65292;&#34987;&#31216;&#20026;&#35270;&#35273;&#24187;&#35273;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;VLMs&#26159;&#21542;&#21644;&#20154;&#31867;&#19968;&#26679;&#26377;&#24187;&#35273;,&#25110;&#32773;&#23427;&#20204;&#26159;&#21542;&#24544;&#23454;&#22320;&#23398;&#20064;&#20102;&#23545;&#29616;&#23454;&#30340;&#34920;&#36798;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#31181;&#31867;&#22411;&#30340;&#35270;&#35273;&#24187;&#35273;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#22235;&#20010;&#20219;&#21153;&#26469;&#30740;&#31350;&#26368;&#20808;&#36827;&#30340;VLMs&#20013;&#30340;&#35270;&#35273;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#25972;&#20307;&#23545;&#40784;&#24615;&#36739;&#20302;&#65292;&#20294;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#24863;&#30693;&#24182;&#26356;&#23481;&#26131;&#21463;&#21040;&#35270;&#35273;&#24187;&#35273;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#21021;&#27493;&#32467;&#26524;&#23558;&#20419;&#36827;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#24863;&#30693;&#21644;&#20132;&#27969;&#20849;&#20139;&#35270;&#35273;&#19990;&#30028;&#26041;&#38754;&#30340;&#26356;&#22909;&#29702;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#33021;&#26356;&#22909;&#22320;&#23545;&#40784;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#24863;&#30693;&#21644;&#20132;&#27969;&#20849;&#20139;&#35270;&#35273;&#19990;&#30028;&#26041;&#38754;&#30340;&#35745;&#31639;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are av
&lt;/p&gt;</description></item><item><title>&#36125;&#21494;&#26031;&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#26041;&#27861;&#65288;BayesMBAR&#65289;&#26159;&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#65288;MBAR&#65289;&#26041;&#27861;&#30340;&#36125;&#21494;&#26031;&#25512;&#24191;&#12290;&#36890;&#36807;&#25972;&#21512;&#37319;&#26679;&#37197;&#32622;&#21644;&#20808;&#39564;&#20998;&#24067;&#65292;BayesMBAR&#35745;&#31639;&#20102;&#33258;&#30001;&#33021;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.20699</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Multistate Bennett Acceptance Ratio Methods. (arXiv:2310.20699v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20699
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#26041;&#27861;&#65288;BayesMBAR&#65289;&#26159;&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#65288;MBAR&#65289;&#26041;&#27861;&#30340;&#36125;&#21494;&#26031;&#25512;&#24191;&#12290;&#36890;&#36807;&#25972;&#21512;&#37319;&#26679;&#37197;&#32622;&#21644;&#20808;&#39564;&#20998;&#24067;&#65292;BayesMBAR&#35745;&#31639;&#20102;&#33258;&#30001;&#33021;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#29366;&#24577;Bennett&#25509;&#21463;&#27604;&#29575;&#65288;MBAR&#65289;&#26041;&#27861;&#26159;&#35745;&#31639;&#28909;&#21147;&#23398;&#29366;&#24577;&#19979;&#33258;&#30001;&#33021;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BayesMBAR&#65292;&#21363;MBAR&#26041;&#27861;&#30340;&#36125;&#21494;&#26031;&#25512;&#24191;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;&#28909;&#21147;&#23398;&#29366;&#24577;&#30340;&#37319;&#26679;&#37197;&#32622;&#19982;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#25972;&#21512;&#65292;BayesMBAR&#35745;&#31639;&#20102;&#33258;&#30001;&#33021;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#21033;&#29992;&#21518;&#39564;&#20998;&#24067;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#33258;&#30001;&#33021;&#30340;&#20272;&#35745;&#20540;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#20351;&#29992;&#22343;&#21248;&#20808;&#39564;&#20998;&#24067;&#26102;&#65292;BayesMBAR&#21487;&#20197;&#24674;&#22797;MBAR&#30340;&#32467;&#26524;&#65292;&#20294;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#24403;&#26377;&#20851;&#33258;&#30001;&#33021;&#30340;&#20808;&#39564;&#30693;&#35782;&#21487;&#29992;&#26102;&#65292;BayesMBAR&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#38750;&#22343;&#21248;&#20808;&#39564;&#20998;&#24067;&#23558;&#36825;&#20123;&#20449;&#24687;&#32435;&#20837;&#20272;&#35745;&#36807;&#31243;&#20013;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#24341;&#20837;&#20851;&#20110;&#33258;&#30001;&#33021;&#26354;&#38754;&#24179;&#28369;&#24615;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;BayesMBAR&#27604;MBAR&#26041;&#27861;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multistate Bennett acceptance ratio (MBAR) method is a prevalent approach for computing free energies of thermodynamic states. In this work, we introduce BayesMBAR, a Bayesian generalization of the MBAR method. By integrating configurations sampled from thermodynamic states with a prior distribution, BayesMBAR computes a posterior distribution of free energies. Using the posterior distribution, we derive free energy estimations and compute their associated uncertainties. Notably, when a uniform prior distribution is used, BayesMBAR recovers the MBAR's result but provides more accurate uncertainty estimates. Additionally, when prior knowledge about free energies is available, BayesMBAR can incorporate this information into the estimation procedure by using non-uniform prior distributions. As an example, we show that, by incorporating the prior knowledge about the smoothness of free energy surfaces, BayesMBAR provides more accurate estimates than the MBAR method. Given MBAR's widespr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23558;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#20844;&#24179;&#22320;&#20272;&#35745;&#22836;&#37096;&#21644;&#23614;&#37096;&#31867;&#21035;&#36129;&#29486;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#24179;&#34913;&#20998;&#32452;&#31574;&#30053;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.20490</link><description>&lt;p&gt;
&#38271;&#23614;&#23398;&#20064;&#20316;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Learning as Multi-Objective Optimization. (arXiv:2310.20490v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23558;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#20844;&#24179;&#22320;&#20272;&#35745;&#22836;&#37096;&#21644;&#23614;&#37096;&#31867;&#21035;&#36129;&#29486;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26799;&#24230;&#24179;&#34913;&#20998;&#32452;&#31574;&#30053;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#26497;&#19981;&#24179;&#34913;&#65292;&#21576;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#65292;&#23548;&#33268;&#27169;&#22411;&#23545;&#20855;&#26377;&#36275;&#22815;&#26679;&#26412;&#30340;&#31867;&#21035;&#26377;&#20559;&#35265;&#65292;&#24182;&#19988;&#22312;&#32597;&#35265;&#31867;&#21035;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#31867;&#21035;&#37325;&#24179;&#34913;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#30456;&#20114;&#30683;&#30462;&#30340;&#38382;&#39064;&#65288;&#25552;&#39640;&#23614;&#37096;&#31867;&#21035;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#38477;&#20302;&#22836;&#37096;&#31867;&#21035;&#30340;&#24615;&#33021;&#65292;&#21453;&#20043;&#20134;&#28982;&#65289;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#36825;&#19968;&#38382;&#39064;&#30340;&#26681;&#28304;&#26159;&#19981;&#21516;&#31867;&#21035;&#30340;&#26799;&#24230;&#19981;&#24179;&#34913;&#65292;&#19981;&#36866;&#24403;&#31867;&#21035;&#30340;&#26799;&#24230;&#34987;&#35774;&#32622;&#20026;&#37325;&#35201;&#26356;&#26032;&#30340;&#37096;&#20998;&#65292;&#22240;&#27492;&#22312;&#23614;&#37096;&#31867;&#21035;&#19978;&#23481;&#26131;&#20135;&#29983;&#36807;&#34917;&#20607;&#25110;&#27424;&#34917;&#20607;&#12290;&#20026;&#20102;&#23454;&#29616;&#29702;&#24819;&#30340;&#34917;&#20607;&#65292;&#25105;&#20204;&#23558;&#38271;&#23614;&#35782;&#21035;&#23450;&#20041;&#20026;&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21516;&#26102;&#20844;&#24179;&#22320;&#23562;&#37325;&#22836;&#37096;&#21644;&#23614;&#37096;&#31867;&#21035;&#30340;&#36129;&#29486;&#12290;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26799;&#24230;&#24179;&#34913;&#20998;&#32452;&#65288;GBG&#65289;&#31574;&#30053;&#26469;&#25910;&#38598;&#20855;&#26377;&#30456;&#20284;&#26799;&#24230;&#26041;&#21521;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#36817;&#20284;&#20351;&#27599;&#27425;&#26356;&#26032;&#37117;&#22312;&#30456;&#20284;&#30340;&#26041;&#21521;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is extremely imbalanced and presents a long-tailed distribution, resulting in models that are biased towards classes with sufficient samples and perform poorly on rare classes. Recent methods propose to rebalance classes but they undertake the seesaw dilemma (what is increasing performance on tail classes may decrease that of head classes, and vice versa). In this paper, we argue that the seesaw dilemma is derived from gradient imbalance of different classes, in which gradients of inappropriate classes are set to important for updating, thus are prone to overcompensation or undercompensation on tail classes. To achieve ideal compensation, we formulate the long-tailed recognition as an multi-objective optimization problem, which fairly respects the contributions of head and tail classes simultaneously. For efficiency, we propose a Gradient-Balancing Grouping (GBG) strategy to gather the classes with similar gradient directions, thus approximately make every update under 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29289;&#29702;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064; (PEML) &#30340;&#20809;&#35889;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#22797;&#26434;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#28436;&#31034;&#20102;&#19981;&#21516;&#31867;&#22411;PEML&#26041;&#27861;&#30340;&#20010;&#20307;&#29305;&#24449;&#21644;&#21160;&#26426;&#12290;</title><link>http://arxiv.org/abs/2310.20425</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#32467;&#26500;&#21147;&#23398;&#24212;&#29992;&#30340;&#35843;&#26597;&#65292;&#35752;&#35770;&#29289;&#29702;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30340;&#20809;&#35889;
&lt;/p&gt;
&lt;p&gt;
Discussing the Spectra of Physics-Enhanced Machine Learning via a Survey on Structural Mechanics Applications. (arXiv:2310.20425v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29289;&#29702;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064; (PEML) &#30340;&#20809;&#35889;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#35299;&#20915;&#22797;&#26434;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#28436;&#31034;&#20102;&#19981;&#21516;&#31867;&#22411;PEML&#26041;&#27861;&#30340;&#20010;&#20307;&#29305;&#24449;&#21644;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#24050;&#32463;&#20652;&#29983;&#20102;&#19968;&#31181;&#25105;&#20204;&#22312;&#36825;&#37324;&#31216;&#20043;&#20026;&#29289;&#29702;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064; (PEML) &#30340;&#33539;&#24335;&#65292;&#26088;&#22312;&#25552;&#39640;&#25968;&#25454;&#25110;&#29289;&#29702;&#26041;&#27861;&#30340;&#33021;&#21147;&#21644;&#20943;&#23569;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20854;&#29305;&#24449;&#12289;&#29992;&#27861;&#21644;&#21160;&#26426;&#30340;&#20840;&#38754;&#25506;&#32034;&#26469;&#35752;&#35770;&#29289;&#29702;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20809;&#35889;&#65292;&#28085;&#30422;&#20102;&#29289;&#29702;&#21644;&#25968;&#25454;&#36825;&#20004;&#20010;&#23450;&#20041;&#36724;&#12290;&#22312;&#36825;&#26679;&#20570;&#30340;&#36807;&#31243;&#20013;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;PEML&#25216;&#26415;&#30340;&#26368;&#26032;&#24212;&#29992;&#21644;&#21457;&#23637;&#30340;&#35843;&#26597;&#65292;&#25581;&#31034;&#20102;PEML&#22312;&#24212;&#23545;&#22797;&#26434;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#28436;&#31034;&#20102;&#22312;&#21333;&#33258;&#30001;&#24230;Duffing&#25391;&#23376;&#30340;&#31616;&#21333;&#24037;&#20316;&#31034;&#20363;&#19978;&#36873;&#25321;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#20174;&#32780;&#31361;&#20986;&#20102;&#19981;&#21516;&#8220;&#31867;&#22411;&#8221;PEML&#26041;&#27861;&#30340;&#20010;&#20307;&#29305;&#24449;&#21644;&#21160;&#26426;&#12290;&#20026;&#20102;&#20419;&#36827;&#21512;&#20316;&#21644;&#36879;&#26126;&#24230;&#65292;&#24182;&#20026;&#35835;&#32773;&#25552;&#20379;&#23454;&#38469;&#30340;&#31034;&#20363;&#65292;&#26412;&#25991;&#23436;&#25972;&#22320;&#35760;&#24405;&#20102;&#25152;&#37319;&#21462;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of physics and machine learning has given rise to a paradigm that we refer to here as physics-enhanced machine learning (PEML), aiming to improve the capabilities and reduce the individual shortcomings of data- or physics-only methods. In this paper, the spectrum of physics-enhanced machine learning methods, expressed across the defining axes of physics and data, is discussed by engaging in a comprehensive exploration of its characteristics, usage, and motivations. In doing so, this paper offers a survey of recent applications and developments of PEML techniques, revealing the potency of PEML in addressing complex challenges. We further demonstrate application of select such schemes on the simple working example of a single-degree-of-freedom Duffing oscillator, which allows to highlight the individual characteristics and motivations of different `genres' of PEML approaches. To promote collaboration and transparency, and to provide practical examples for the reader, the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dropout&#25216;&#26415;&#26469;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20013;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#38271;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;PPO&#31639;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;D-PPO&#31639;&#27861;&#30456;&#36739;&#20110;PPO&#31639;&#27861;&#22312;Atari 2600&#28216;&#25103;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.20380</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Dropout&#31574;&#30053;&#65306;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20013;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#38271;
&lt;/p&gt;
&lt;p&gt;
Dropout Strategy in Reinforcement Learning: Limiting the Surrogate Objective Variance in Policy Optimization Methods. (arXiv:2310.20380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dropout&#25216;&#26415;&#26469;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#20013;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#38271;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;PPO&#31639;&#27861;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;D-PPO&#31639;&#27861;&#30456;&#36739;&#20110;PPO&#31639;&#27861;&#22312;Atari 2600&#28216;&#25103;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20854;&#20013;&#65292;&#20027;&#27969;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#22914;PPO&#21644;TRPO&#24341;&#20837;&#20102;&#37325;&#35201;&#24615;&#37319;&#26679;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36825;&#20801;&#35768;&#37325;&#29992;&#21382;&#21490;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#22686;&#21152;&#65292;&#38388;&#25509;&#24433;&#21709;&#20102;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#30340;&#19978;&#30028;&#65292;&#23427;&#21487;&#20197;&#38543;&#26367;&#20195;&#30446;&#26631;&#30340;&#22686;&#21152;&#32780;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Dropout&#25216;&#26415;&#65292;&#20197;&#36991;&#20813;&#37325;&#35201;&#24615;&#37319;&#26679;&#24341;&#36215;&#30340;&#26367;&#20195;&#30446;&#26631;&#26041;&#24046;&#36807;&#24230;&#22686;&#21152;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20027;&#27969;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#23558;Dropout&#25216;&#26415;&#24212;&#29992;&#20110;PPO&#31639;&#27861;&#65292;&#24471;&#21040;&#20102;D-PPO&#21464;&#20307;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;Atari 2600&#28216;&#25103;&#19978;&#23545;D-PPO&#21644;PPO&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-based reinforcement learning algorithms are widely used in various fields. Among them, mainstream policy optimization algorithms such as PPO and TRPO introduce importance sampling into reinforcement learning, which allows the reuse of historical data. However, this also results in high variance of the surrogate objective and indirectly affects the stability and convergence of the algorithm. In this paper, we first derived an upper bound of the variance of the surrogate objective, which can grow quadratically with the increase of the surrogate objective. Next, we proposed a dropout technique to avoid the excessive increase of the surrogate objective variance caused by importance sampling. Then, we introduced a general reinforcement learning framework applicable to mainstream policy optimization methods, and applied the dropout technique to the PPO algorithm to obtain the D-PPO variant. Finally, we conduct comparative experiments between D-PPO and PPO algorithms in the Atari 2600 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO-DANCE&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#25552;&#39640;&#25506;&#32034;&#25928;&#26524;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20178</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#21457;&#29616;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning to Discover Skills through Guidance. (arXiv:2310.20178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20178
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO-DANCE&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#23398;&#20064;&#25552;&#39640;&#25506;&#32034;&#25928;&#26524;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#39046;&#22495;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#26377;&#38480;&#30340;&#25506;&#32034;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25216;&#33021;&#20559;&#31163;&#20854;&#21021;&#22987;&#36712;&#36857;&#20250;&#21463;&#21040;&#37325;&#22823;&#24809;&#32602;&#12290;&#20026;&#20102;&#22686;&#24378;&#25506;&#32034;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#36741;&#21161;&#22870;&#21169;&#26469;&#26368;&#22823;&#21270;&#29366;&#24577;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#25110;&#29109;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#22870;&#21169;&#30340;&#25928;&#26524;&#38543;&#30528;&#29615;&#22659;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#32780;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;DISCO-DANCE&#65292;&#23427;&#36873;&#25321;&#20855;&#26377;&#36798;&#21040;&#26410;&#25506;&#32034;&#29366;&#24577;&#28508;&#21147;&#26368;&#39640;&#30340;&#24341;&#23548;&#25216;&#33021;&#65292;&#24341;&#23548;&#20854;&#20182;&#25216;&#33021;&#36981;&#24490;&#24341;&#23548;&#25216;&#33021;&#65292;&#28982;&#21518;&#20998;&#25955;&#24341;&#23548;&#25216;&#33021;&#20197;&#26368;&#22823;&#21270;&#22312;&#26410;&#25506;&#32034;&#29366;&#24577;&#20013;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#21253;&#25324;&#20004;&#20010;&#23548;&#33322;&#22522;&#20934;&#21644;&#19968;&#20010;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#65292;DISCO-DANCE&#20248;&#20110;&#20854;&#20182;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#22522;&#32447;&#12290;DISCO-DANCE&#30340;&#23450;&#24615;&#21487;&#35270;&#21270;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of unsupervised skill discovery (USD), a major challenge is limited exploration, primarily due to substantial penalties when skills deviate from their initial trajectories. To enhance exploration, recent methodologies employ auxiliary rewards to maximize the epistemic uncertainty or entropy of states. However, we have identified that the effectiveness of these rewards declines as the environmental complexity rises. Therefore, we present a novel USD algorithm, skill discovery with guidance (DISCO-DANCE), which (1) selects the guide skill that possesses the highest potential to reach unexplored states, (2) guides other skills to follow guide skill, then (3) the guided skills are dispersed to maximize their discriminability in unexplored states. Empirical evaluation demonstrates that DISCO-DANCE outperforms other USD baselines in challenging environments, including two navigation benchmarks and a continuous control benchmark. Qualitative visualizations and code of DISCO-DANCE
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.20049</link><description>&lt;p&gt;
SURF: GNN&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics. (arXiv:2310.20049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20049
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SURF&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;SURF&#21253;&#25324;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#20855;&#20307;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#27969;&#20307;&#21160;&#21147;&#23398;&#23545;&#20110;&#35774;&#35745;&#21644;&#24320;&#21457;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#28085;&#30422;&#20102;&#20174;&#31616;&#21333;&#38400;&#38376;&#21040;&#22797;&#26434;&#28065;&#36718;&#26426;&#26800;&#30340;&#33539;&#22260;&#12290;&#20934;&#30830;&#27714;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#26041;&#31243;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#29305;&#28857;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#22312;&#32593;&#26684;&#19978;&#24314;&#27169;&#30456;&#20114;&#20316;&#29992;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#21152;&#36895;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#30495;&#27491;&#29702;&#35299;&#28508;&#22312;&#30340;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#27867;&#21270;&#32780;&#38750;&#25554;&#20540;&#12290;&#27867;&#21270;&#26159;&#36890;&#29992;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#23427;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#12289;&#20998;&#36776;&#29575;&#25110;&#28909;&#21147;&#23398;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SURF&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#27979;&#35797;&#23398;&#20064;&#30340;&#22522;&#20110;&#22270;&#30340;&#27969;&#20307;&#27169;&#25311;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;SURF&#21253;&#25324;&#21508;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#33021;&#21644;&#27867;&#21270;&#24230;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;SURF&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating fluid dynamics is crucial for the design and development process, ranging from simple valves to complex turbomachinery. Accurately solving the underlying physical equations is computationally expensive. Therefore, learning-based solvers that model interactions on meshes have gained interest due to their promising speed-ups. However, it is unknown to what extent these models truly understand the underlying physical principles and can generalize rather than interpolate. Generalization is a key requirement for a general-purpose fluid simulator, which should adapt to different topologies, resolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to test the \textit{generalization} of learned graph-based fluid simulators. SURF comprises individual datasets and provides specific performance and generalization metrics for evaluating and comparing different models. We empirically demonstrate the applicability of SURF by thoroughly investigating the two state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;$f$-&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#25913;&#36827;&#20102;&#27927;&#29260;&#27169;&#22411;&#21644;DP-GD&#20013;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#38544;&#31169;&#36793;&#30028;&#65292;&#25240;&#34935;&#20989;&#25968;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#20248;&#20110;$(\epsilon,\delta)$-DP&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38543;&#26426;&#21021;&#22987;&#21270;&#21487;&#20197;&#22686;&#24378;DP-GD&#30340;&#38544;&#31169;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19973</link><description>&lt;p&gt;
&#36890;&#36807;$f$-&#24046;&#20998;&#38544;&#31169;&#32479;&#19968;&#22686;&#24378;&#28151;&#21512;&#26426;&#21046;&#30340;&#38544;&#31169;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Unified Enhancement of Privacy Bounds for Mixture Mechanisms via $f$-Differential Privacy. (arXiv:2310.19973v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;$f$-&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#25913;&#36827;&#20102;&#27927;&#29260;&#27169;&#22411;&#21644;DP-GD&#20013;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#38544;&#31169;&#36793;&#30028;&#65292;&#25240;&#34935;&#20989;&#25968;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#20248;&#20110;$(\epsilon,\delta)$-DP&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38543;&#26426;&#21021;&#22987;&#21270;&#21487;&#20197;&#22686;&#24378;DP-GD&#30340;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20250;&#20135;&#29983;&#35768;&#22810;&#38543;&#26426;&#24615;&#65292;&#22914;&#38543;&#26426;&#21021;&#22987;&#21270;&#12289;&#38543;&#26426;&#25209;&#27425;&#25277;&#26679;&#21644;&#27927;&#29260;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#38543;&#26426;&#24615;&#20250;&#23548;&#33268;&#38590;&#20197;&#20998;&#26512;&#30340;&#28151;&#21512;&#20998;&#24067;&#65292;&#25152;&#20197;&#22312;&#35777;&#26126;&#24046;&#20998;&#38544;&#31169;&#36793;&#30028;&#26102;&#24456;&#38590;&#23558;&#20854;&#32435;&#20837;&#32771;&#34385;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#27927;&#29260;&#27169;&#22411;&#21644;&#19968;&#27425;&#36845;&#20195;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#65288;DP-GD&#65289;&#20013;&#29992;&#20110;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#38544;&#31169;&#36793;&#30028;&#65292;&#37319;&#29992;$f$-DP&#26041;&#27861;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#27927;&#29260;&#27169;&#22411;&#30340;&#25240;&#34935;&#20989;&#25968;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#20248;&#20110;&#22522;&#20110;$(\epsilon,\delta)$-DP&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#19968;&#27425;&#36845;&#20195;&#30340;DP-GD&#30340;&#38544;&#31169;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#25240;&#34935;&#20989;&#25968;&#30340;&#25968;&#20540;&#35745;&#31639;&#34920;&#26126;&#65292;&#38543;&#26426;&#21021;&#22987;&#21270;&#21487;&#20197;&#22686;&#24378;DP-GD&#30340;&#38544;&#31169;&#24615;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#28151;&#21512;&#26426;&#21046;&#30340;$f$-DP&#20445;&#35777;&#30340;&#20998;&#26512;&#20381;&#36182;&#20110;&#19968;&#31181;&#19981;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private (DP) machine learning algorithms incur many sources of randomness, such as random initialization, random batch subsampling, and shuffling. However, such randomness is difficult to take into account when proving differential privacy bounds because it induces mixture distributions for the algorithm's output that are difficult to analyze. This paper focuses on improving privacy bounds for shuffling models and one-iteration differentially private gradient descent (DP-GD) with random initializations using $f$-DP. We derive a closed-form expression of the trade-off function for shuffling models that outperforms the most up-to-date results based on $(\epsilon,\delta)$-DP. Moreover, we investigate the effects of random initialization on the privacy of one-iteration DP-GD. Our numerical computations of the trade-off function indicate that random initialization can enhance the privacy of DP-GD. Our analysis of $f$-DP guarantees for these mixture mechanisms relies on an ine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#26102;&#38388;&#28436;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#21442;&#25968;&#19982;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#65292;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#28909;&#24211;&#23384;&#20648;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#36825;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.19802</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#38543;&#26426;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models. (arXiv:2310.19802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#26102;&#38388;&#28436;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#21442;&#25968;&#19982;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#65292;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#28909;&#24211;&#23384;&#20648;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#36825;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21442;&#25968;&#21270;&#27010;&#29575;&#27169;&#22411;&#65288;PPM&#65289;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#20174;&#26412;&#36136;&#19978;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#28909;&#21147;&#23398;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#21442;&#25968;&#65288;&#35760;&#20026;$\Theta$&#65289;&#19982;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#65288;&#35760;&#20026;$X$&#65289;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20248;&#21270;&#22120;&#30340;&#20316;&#29992;&#26159;&#39537;&#21160;&#36825;&#20004;&#20010;&#23376;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#30340;&#33021;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#26679;&#26412;$X$&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#23548;&#33268;&#27169;&#22411;&#21442;&#25968;$\Theta$&#30340;&#29109;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#20102;&#19968;&#20010;&#28909;&#24211;&#65292;&#26377;&#25928;&#22320;&#23384;&#20648;&#20102;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#21442;&#25968;&#20316;&#20026;&#28909;&#24211;&#30340;&#35282;&#33394;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#19988;&#19968;&#33268;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28909;&#21147;&#23398;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have formulated generative machine learning problems as the time evolution of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic process. Then, we have studied the thermodynamic exchange between the model's parameters, denoted as $\Theta$, and the model's generated samples, denoted as $X$. We demonstrate that the training dataset and the action of the Stochastic Gradient Descent (SGD) optimizer serve as a work source that governs the time evolution of these two subsystems. Our findings reveal that the model learns through the dissipation of heat during the generation of samples $X$, leading to an increase in the entropy of the model's parameters, $\Theta$. Thus, the parameter subsystem acts as a heat reservoir, effectively storing the learned information. Furthermore, the role of the model's parameters as a heat reservoir provides valuable thermodynamic insights into the generalization power of over-parameterized models. This approach offers an unambiguous 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#19982;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35789;&#35821;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#26469;&#26377;&#25928;&#22788;&#29702;&#39046;&#22495;&#26415;&#35821;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.19708</link><description>&lt;p&gt;
&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#29992;&#26041;&#27861;&#65306;&#19968;&#31181;&#20016;&#23500;&#22810;&#24425;&#30340;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Combining Language Models For Specialized Domains: A Colorful Approach. (arXiv:2310.19708v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#19982;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35789;&#35821;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#26469;&#26377;&#25928;&#22788;&#29702;&#39046;&#22495;&#26415;&#35821;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30446;&#30340;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#21644;&#26415;&#35821;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#20123;&#26415;&#35821;&#32463;&#24120;&#22312;&#21307;&#23398;&#25110;&#24037;&#19994;&#39046;&#22495;&#31561;&#19987;&#19994;&#39046;&#22495;&#20013;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#36890;&#24120;&#24456;&#38590;&#35299;&#37322;&#23558;&#36890;&#29992;&#35821;&#35328;&#19982;&#19987;&#38376;&#26415;&#35821;&#28151;&#21512;&#20351;&#29992;&#30340;&#28151;&#21512;&#35821;&#38899;&#12290;&#36825;&#23545;&#20110;&#22312;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#20869;&#25805;&#20316;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#25110;&#27425;&#32423;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#36890;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;&#35813;&#31574;&#30053;&#28041;&#21450;&#23545;&#27599;&#20010;&#21333;&#35789;&#36827;&#34892;&#26631;&#35760;&#25110;&#8220;&#19978;&#33394;&#8221;&#65292;&#20197;&#25351;&#31034;&#20854;&#19982;&#36890;&#29992;&#25110;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#22686;&#24378;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#28041;&#21450;&#19978;&#33394;&#21333;&#35789;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38598;&#25104;&#26415;&#35821;&#21040;&#35821;&#35328;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#39046;&#22495;&#19987;&#29992;&#20219;&#21153;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
General purpose language models (LMs) encounter difficulties when processing domain-specific jargon and terminology, which are frequently utilized in specialized fields such as medicine or industrial settings. Moreover, they often find it challenging to interpret mixed speech that blends general language with specialized jargon. This poses a challenge for automatic speech recognition systems operating within these specific domains. In this work, we introduce a novel approach that integrates domain-specific or secondary LM into general-purpose LM. This strategy involves labeling, or ``coloring'', each word to indicate its association with either the general or the domain-specific LM. We develop an optimized algorithm that enhances the beam search algorithm to effectively handle inferences involving colored words. Our evaluations indicate that this approach is highly effective in integrating jargon into language tasks. Notably, our method substantially lowers the error rate for domain-sp
&lt;/p&gt;</description></item><item><title>&#21452;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;DGFNs&#65289;&#26159;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#20013;&#25506;&#32034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#32593;&#32476;&#21644;&#37319;&#26679;&#36335;&#24452;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#21644;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.19685</link><description>&lt;p&gt;
DGFN: &#21452;&#29983;&#25104;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DGFN: Double Generative Flow Networks. (arXiv:2310.19685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19685
&lt;/p&gt;
&lt;p&gt;
&#21452;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;DGFNs&#65289;&#26159;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#20013;&#25506;&#32034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#32593;&#32476;&#21644;&#37319;&#26679;&#36335;&#24452;&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#21644;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#33647;&#29289;&#21457;&#29616;&#24037;&#20855;&#27491;&#22312;&#23853;&#38706;&#22836;&#35282;&#65292;&#20855;&#26377;&#22312;&#39044;&#27979;&#21644;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets / GFNs&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#24341;&#20837;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#22312;&#23567;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#29289;&#32780;&#21463;&#21040;&#35748;&#21487;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;DGFNs&#65289;&#12290;&#21463;&#24378;&#21270;&#23398;&#20064;&#21644;&#21452;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#30446;&#26631;&#32593;&#32476;&#29992;&#20110;&#37319;&#26679;&#36335;&#24452;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#37319;&#26679;&#36335;&#24452;&#26469;&#26356;&#26032;&#20027;&#32593;&#32476;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#23454;&#65292;DGFNs&#33021;&#22815;&#26377;&#25928;&#22686;&#24378;&#31232;&#30095;&#22870;&#21169;&#39046;&#22495;&#21644;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#36825;&#37117;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#20840;&#26032;&#35774;&#35745;&#30340;&#25361;&#25112;&#24615;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is emerging as an effective tool in drug discovery, with potential applications in both predictive and generative models. Generative Flow Networks (GFlowNets/GFNs) are a recently introduced method recognized for the ability to generate diverse candidates, in particular in small molecule generation tasks. In this work, we introduce double GFlowNets (DGFNs). Drawing inspiration from reinforcement learning and Double Deep Q-Learning, we introduce a target network used to sample trajectories, while updating the main network with these sampled trajectories. Empirical results confirm that DGFNs effectively enhance exploration in sparse reward domains and high-dimensional state spaces, both challenging aspects of de-novo design in drug discovery.
&lt;/p&gt;</description></item><item><title>rTsfNet&#26159;&#19968;&#31181;&#26032;&#30340;DNN&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#22836;3D&#26059;&#36716;&#21644;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#23454;&#29616;&#20102;IMU-based&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.19283</link><description>&lt;p&gt;
rTsfNet:&#19968;&#31181;&#20855;&#26377;&#22810;&#22836;3D&#26059;&#36716;&#21644;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#30340;&#22522;&#20110;IMU&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;DNN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
rTsfNet: a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction for IMU-based Human Activity Recognition. (arXiv:2310.19283v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19283
&lt;/p&gt;
&lt;p&gt;
rTsfNet&#26159;&#19968;&#31181;&#26032;&#30340;DNN&#27169;&#22411;&#65292;&#36890;&#36807;&#22810;&#22836;3D&#26059;&#36716;&#21644;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#23454;&#29616;&#20102;IMU-based&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;rTsfNet&#65292;&#19968;&#31181;&#20855;&#26377;&#22810;&#22836;3D&#26059;&#36716;&#21644;&#26102;&#24207;&#29305;&#24449;&#25552;&#21462;&#30340;DNN&#27169;&#22411;&#65292;&#20316;&#20026;IMU-based&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#26032;&#22411;DNN&#27169;&#22411;&#12290;rTsfNet&#36890;&#36807;&#22312;DNN&#20869;&#37096;&#25512;&#23548;3D&#26059;&#36716;&#21442;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#24212;&#35813;&#20174;&#20013;&#27966;&#29983;&#29305;&#24449;&#30340;3D&#22522;&#20934;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;MLP&#25512;&#23548;&#26102;&#24207;&#29305;&#24449;&#65288;TSFs&#65289;&#24182;&#23454;&#29616;HAR&#12290;&#23613;&#31649;&#35813;&#27169;&#22411;&#19981;&#20351;&#29992;CNN&#65292;&#22312;&#33391;&#22909;&#31649;&#29702;&#30340;&#22522;&#20934;&#26465;&#20214;&#21644;&#22810;&#20010;&#25968;&#25454;&#38598;&#65288;UCI HAR, PAMAP2, Daphnet, &#21644;OPPORTUNITY&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#38024;&#23545;&#19981;&#21516;&#30340;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes rTsfNet, a DNN model with Multi-head 3D Rotation and Time Series Feature Extraction, as a new DNN model for IMU-based human activity recognition (HAR). rTsfNet automatically selects 3D bases from which features should be derived by deriving 3D rotation parameters within the DNN. Then, time series features (TSFs), the wisdom of many researchers, are derived and realize HAR using MLP. Although a model that does not use CNN, it achieved the highest accuracy than existing models under well-managed benchmark conditions and multiple datasets: UCI HAR, PAMAP2, Daphnet, and OPPORTUNITY, which target different activities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#35270;&#35282;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;dPoE&#65292;&#36890;&#36807;&#20351;&#29992;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#36866;&#29992;&#20110;&#22810;&#20010;&#35270;&#22270;&#21644;&#22810;&#31181;&#31867;&#22411;&#30340;&#24322;&#24120;&#65292;&#36824;&#25903;&#25345;&#27169;&#22411;&#37096;&#32626;&#21518;&#30340;&#22312;&#32447;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.18728</link><description>&lt;p&gt;
&#25581;&#31034;&#33258;&#30001;&#26680;&#32858;&#21464;&#30340;&#31070;&#35805;&#65306;&#20351;&#29992;&#35299;&#32806;&#30340;&#19987;&#23478;&#20135;&#21697;&#24314;&#27169;&#36827;&#34892;&#22312;&#32447;&#22810;&#35270;&#35282;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Debunking Free Fusion Myth: Online Multi-view Anomaly Detection with Disentangled Product-of-Experts Modeling. (arXiv:2310.18728v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#35270;&#35282;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;dPoE&#65292;&#36890;&#36807;&#20351;&#29992;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#35270;&#35282;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#36866;&#29992;&#20110;&#22810;&#20010;&#35270;&#22270;&#21644;&#22810;&#31181;&#31867;&#22411;&#30340;&#24322;&#24120;&#65292;&#36824;&#25903;&#25345;&#27169;&#22411;&#37096;&#32626;&#21518;&#30340;&#22312;&#32447;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#29978;&#33267;&#22810;&#27169;&#24577;&#25968;&#25454;&#23545;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#21560;&#24341;&#21147;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26816;&#27979;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#26368;&#36817;&#30740;&#31350;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;: 1&#65289;&#20165;&#36866;&#29992;&#20110;&#20004;&#20010;&#35270;&#22270;&#25110;&#31867;&#22411;&#29305;&#23450;&#30340;&#24322;&#24120;&#65292;2&#65289;&#23384;&#22312;&#34701;&#21512;&#35299;&#32806;&#38382;&#39064;&#65292;3&#65289;&#22312;&#27169;&#22411;&#37096;&#32626;&#21518;&#19981;&#25903;&#25345;&#22312;&#32447;&#26816;&#27979;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#30340;&#20027;&#35201;&#24605;&#36335;&#26377;&#19977;&#20010;:&#22810;&#35270;&#22270;&#23398;&#20064;&#65292;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;dPoE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#35270;&#35282;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;(1)&#19968;&#20010;&#19987;&#23478;&#20056;&#31215;(PoE)&#23618;&#26469;&#22788;&#29702;&#22810;&#35270;&#22270;&#25968;&#25454;&#65292;(2)&#19968;&#20010;&#24635;&#26657;&#27491;(TC)&#37492;&#21035;&#22120;&#26469;&#35299;&#32806;&#35270;&#22270;&#20849;&#20139;&#21644;&#35270;&#22270;&#29305;&#23450;&#34920;&#31034;&#65292;(3)&#19968;&#20010;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#25972;&#21512;&#25152;&#26377;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29702;&#35770;&#20449;&#24687;&#30028;&#26469;&#25511;&#21046;&#35270;&#22270;&#20849;&#20139;&#21644;&#35270;&#22270;&#29305;&#23450;&#34920;&#31034;&#12290;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view or even multi-modal data is appealing yet challenging for real-world applications. Detecting anomalies in multi-view data is a prominent recent research topic. However, most of the existing methods 1) are only suitable for two views or type-specific anomalies, 2) suffer from the issue of fusion disentanglement, and 3) do not support online detection after model deployment. To address these challenges, our main ideas in this paper are three-fold: multi-view learning, disentangled representation learning, and generative model. To this end, we propose dPoE, a novel multi-view variational autoencoder model that involves (1) a Product-of-Experts (PoE) layer in tackling multi-view data, (2) a Total Correction (TC) discriminator in disentangling view-common and view-specific representations, and (3) a joint loss function in wrapping up all components. In addition, we devise theoretical information bounds to control both view-common and view-specific representations. Extensive exper
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38450;&#27490;&#35821;&#35328;&#27169;&#22411;&#38544;&#34255;&#25512;&#29702;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#32534;&#30721;&#25512;&#29702;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#29992;&#25143;&#29702;&#35299;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#36825;&#31181;&#34892;&#20026;&#21487;&#33021;&#20250;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#38450;&#24481;&#32534;&#30721;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18512</link><description>&lt;p&gt;
&#38450;&#27490;&#35821;&#35328;&#27169;&#22411;&#38544;&#34255;&#20854;&#25512;&#29702;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Preventing Language Models From Hiding Their Reasoning. (arXiv:2310.18512v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38450;&#27490;&#35821;&#35328;&#27169;&#22411;&#38544;&#34255;&#25512;&#29702;&#36807;&#31243;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#32534;&#30721;&#25512;&#29702;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#29992;&#25143;&#29702;&#35299;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#36825;&#31181;&#34892;&#20026;&#21487;&#33021;&#20250;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#38450;&#24481;&#32534;&#30721;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#24120;&#36890;&#36807;&#25512;&#29702;&#30340;&#20013;&#38388;&#27493;&#39588;&#26469;&#29983;&#25104;&#22797;&#26434;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#24403;&#36825;&#20123;&#25512;&#29702;&#30340;&#20013;&#38388;&#27493;&#39588;&#34987;&#29992;&#26469;&#30417;&#25511;&#27169;&#22411;&#30340;&#27963;&#21160;&#26102;&#65292;&#20851;&#38190;&#26159;&#36825;&#31181;&#26126;&#30830;&#30340;&#25512;&#29702;&#26159;&#21487;&#20449;&#30340;&#65292;&#21363;&#21453;&#26144;&#20986;&#27169;&#22411;&#23454;&#38469;&#19978;&#22312;&#25512;&#29702;&#20160;&#20040;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#25512;&#29702;&#30340;&#20013;&#38388;&#27493;&#39588;&#19981;&#21487;&#20449;&#30340;&#26041;&#24335;&#65306;&#32534;&#30721;&#25512;&#29702;&#65292;&#21363;LLM&#21487;&#33021;&#20197;&#20154;&#31867;&#35835;&#32773;&#26080;&#27861;&#29702;&#35299;&#30340;&#26041;&#24335;&#23558;&#25512;&#29702;&#30340;&#20013;&#38388;&#27493;&#39588;&#32534;&#30721;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#35757;&#32451;&#25104;&#21033;&#29992;&#32534;&#30721;&#25512;&#29702;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#32780;&#29992;&#25143;&#24182;&#19981;&#38656;&#35201;&#29702;&#35299;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#65292;&#36825;&#31181;&#34892;&#20026;&#26356;&#21487;&#33021;&#33258;&#28982;&#20986;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#35780;&#20272;&#38024;&#23545;&#32534;&#30721;&#25512;&#29702;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#22312;&#21512;&#36866;&#30340;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#19981;&#23436;&#32654;&#35266;&#23519;&#32773;&#30340;&#38544;&#31192;&#35268;&#21010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#21160;&#21147;&#23398;&#21644;&#35266;&#23519;&#32773;&#30340;&#19981;&#23436;&#32654;&#35266;&#27979;&#32806;&#21512;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#26368;&#20248;&#20219;&#21153;&#24615;&#33021;&#32780;&#19981;&#34987;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.16791</link><description>&lt;p&gt;
&#23545;&#25239;&#19981;&#23436;&#32654;&#35266;&#23519;&#32773;&#30340;&#38544;&#31192;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Covert Planning against Imperfect Observers. (arXiv:2310.16791v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#19981;&#23436;&#32654;&#35266;&#23519;&#32773;&#30340;&#38544;&#31192;&#35268;&#21010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#21160;&#21147;&#23398;&#21644;&#35266;&#23519;&#32773;&#30340;&#19981;&#23436;&#32654;&#35266;&#27979;&#32806;&#21512;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#26368;&#20248;&#20219;&#21153;&#24615;&#33021;&#32780;&#19981;&#34987;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31192;&#35268;&#21010;&#25351;&#30340;&#26159;&#19968;&#31867;&#26377;&#38480;&#21046;&#30340;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#31243;&#24207;&#26088;&#22312;&#22312;&#26368;&#23567;&#20449;&#24687;&#27844;&#28431;&#32473;&#34987;&#21160;&#35266;&#23519;&#32773;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#20219;&#21153;&#65292;&#20197;&#36991;&#20813;&#34987;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38544;&#31192;&#35268;&#21010;&#26041;&#27861;&#36890;&#24120;&#32771;&#34385;&#30830;&#23450;&#24615;&#29615;&#22659;&#65292;&#25110;&#32773;&#19981;&#21033;&#29992;&#35266;&#23519;&#32773;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#38543;&#26426;&#21160;&#21147;&#23398;&#21644;&#35266;&#23519;&#32773;&#30340;&#19981;&#23436;&#32654;&#35266;&#27979;&#32806;&#21512;&#26469;&#23454;&#29616;&#26368;&#20248;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#34987;&#21457;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26469;&#24314;&#27169;&#20195;&#29702;&#31243;&#24207;&#19982;&#20854;&#38543;&#26426;&#29615;&#22659;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#20351;&#29992;&#37096;&#20998;&#35266;&#27979;&#20989;&#25968;&#26469;&#25429;&#25417;&#27844;&#28431;&#32473;&#34987;&#21160;&#35266;&#23519;&#32773;&#30340;&#20449;&#24687;&#12290;&#20551;&#35774;&#35266;&#23519;&#32773;&#20351;&#29992;&#20551;&#35774;&#26816;&#39564;&#26469;&#26816;&#27979;&#35266;&#27979;&#26159;&#21542;&#20559;&#31163;&#20102;&#21517;&#20041;&#31574;&#30053;&#65292;&#38544;&#31192;&#35268;&#21010;&#20195;&#29702;&#31243;&#24207;&#30340;&#30446;&#26631;&#26159;&#22312;&#20445;&#25345;&#34987;&#25932;&#26041;&#21457;&#29616;&#30340;&#27010;&#29575;&#20302;&#20110;&#32473;&#23450;&#38408;&#20540;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#21270;&#24635;&#25240;&#25187;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covert planning refers to a class of constrained planning problems where an agent aims to accomplish a task with minimal information leaked to a passive observer to avoid detection. However, existing methods of covert planning often consider deterministic environments or do not exploit the observer's imperfect information. This paper studies how covert planning can leverage the coupling of stochastic dynamics and the observer's imperfect observation to achieve optimal task performance without being detected. Specifically, we employ a Markov decision process to model the interaction between the agent and its stochastic environment, and a partial observation function to capture the leaked information to a passive observer. Assuming the observer employs hypothesis testing to detect if the observation deviates from a nominal policy, the covert planning agent aims to maximize the total discounted reward while keeping the probability of being detected as an adversary below a given threshold.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#25512;&#24191;&#20102;&#20043;&#21069;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#21457;&#29616;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;&#22312;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#20844;&#24335;&#19979;&#23384;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#36824;&#21457;&#29616;&#20102;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#20013;&#30340;&#19968;&#20010;&#32452;&#21512;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.15903</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#31070;&#32463;&#22349;&#32553;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse in Multi-label Learning with Pick-all-label Loss. (arXiv:2310.15903v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15903
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#25512;&#24191;&#20102;&#20043;&#21069;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#21457;&#29616;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;&#22312;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#20844;&#24335;&#19979;&#23384;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#36824;&#21457;&#29616;&#20102;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#20013;&#30340;&#19968;&#20010;&#32452;&#21512;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#22349;&#32553;&#65288;NC&#65289;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;MLab&#65289;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#37117;&#23616;&#38480;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26222;&#36941;&#23384;&#22312;&#30340;NC&#29616;&#35937;&#65292;&#20854;&#20013;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;i&#65289;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#29305;&#24449;&#21464;&#24322;&#24615;&#20026;&#38646;&#65292;&#65288;ii&#65289;&#29305;&#24449;&#22343;&#20540;&#38598;&#21512;&#26500;&#25104;&#19968;&#20010;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#65292;&#65288;iii&#65289;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#25910;&#32553;&#21040;&#29305;&#24449;&#22343;&#20540;&#20056;&#20197;&#26576;&#20010;&#32553;&#25918;&#22240;&#23376;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30740;&#31350;&#25512;&#24191;&#21040;&#22810;&#26631;&#31614;&#23398;&#20064;&#65292;&#24182;&#39318;&#27425;&#35777;&#26126;&#20102;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#20844;&#24335;&#23384;&#22312;&#24191;&#20041;NC&#29616;&#35937;&#12290;&#22312;&#33258;&#28982;&#30340;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#65288;UFM&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#20998;&#31867;&#22120;&#21482;&#26174;&#31034;&#20986;&#30456;&#21516;&#30340;ETF&#20960;&#20309;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#22349;&#32553;&#21040;&#22810;&#37325;&#24615;&#20026;1&#30340;&#29305;&#24449;&#31867;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study deep neural networks for the multi-label classification (MLab) task through the lens of neural collapse (NC). Previous works have been restricted to the multi-class classification setting and discovered a prevalent NC phenomenon comprising of the following properties for the last-layer features: (i) the variability of features within every class collapses to zero, (ii) the set of feature means form an equi-angular tight frame (ETF), and (iii) the last layer classifiers collapse to the feature mean upon some scaling. We generalize the study to multi-label learning, and prove for the first time that a generalized NC phenomenon holds with the "pick-all-label" formulation. Under the natural analog of the unconstrained feature model (UFM), we establish that the only global classifier of the pick-all-label cross entropy loss display the same ETF geometry which further collapse to multiplicity-1 feature class means. Besides, we discover a combinatorial property in generalized NC whic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15612</link><description>&lt;p&gt;
Nko&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#65306;&#24037;&#20855;&#12289;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23612;&#31185;&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#27809;&#26377;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#20294;&#23427;&#22312;&#25991;&#21270;&#21644;&#25945;&#32946;&#20215;&#20540;&#19978;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#26088;&#22312;&#24320;&#21457;&#21487;&#29992;&#30340;&#23612;&#31185;&#35821;&#21644;&#20854;&#20182;&#24403;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#24179;&#34892;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#20855;&#20307;&#21253;&#25324;&#65306;(1) Friallel&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#65292;&#36890;&#36807;&#22522;&#20110;&#21103;&#26412;&#32534;&#36753;&#30340;&#24037;&#20316;&#27969;&#31243;&#23454;&#29616;&#36136;&#37327;&#25511;&#21046;&#12290;(2) &#25193;&#23637;&#20102;FLoRes-200&#21644;NLLB-Seed&#35821;&#26009;&#24211;&#65292;&#20174;&#20854;&#20182;&#35821;&#35328;&#20013;&#19982;&#23612;&#31185;&#35821;&#24179;&#34892;&#32763;&#35793;&#20102;2,009&#21644;6,193&#20010;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;(3) nicolingua-0005&#65306;&#21253;&#21547;130,850&#20010;&#24179;&#34892;&#29255;&#27573;&#30340;&#19977;&#35821;&#21644;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#36229;&#36807;3&#30334;&#19975;&#23612;&#31185;&#35821;&#21333;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;(4) &#22522;&#32447;&#21452;&#35821;&#21644;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#19982;b...
&lt;/p&gt;
&lt;p&gt;
Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#20013;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#24179;&#34913;&#21305;&#37197;&#20559;&#24046;&#21644;&#20998;&#24515;&#30456;&#20284;&#24615;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#25239;&#25968;&#25454;&#21512;&#25104;&#21644;&#26679;&#26412;&#20869;&#23545;&#31435;&#35757;&#32451;&#30340;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.14670</link><description>&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#25968;&#25454;&#38598;&#20559;&#24046;&#32531;&#35299;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Dataset Bias Mitigation in Multiple-Choice Visual Question Answering and Beyond. (arXiv:2310.14670v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#39033;&#36873;&#25321;&#35270;&#35273;&#38382;&#31572;&#20013;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#24179;&#34913;&#21305;&#37197;&#20559;&#24046;&#21644;&#20998;&#24515;&#30456;&#20284;&#24615;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#25239;&#25968;&#25454;&#21512;&#25104;&#21644;&#26679;&#26412;&#20869;&#23545;&#31435;&#35757;&#32451;&#30340;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#29702;&#35299;&#20219;&#21153;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#35780;&#20272;&#27169;&#22411;&#23545;&#22797;&#26434;&#35270;&#35273;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#20004;&#31181;&#25968;&#25454;&#38598;&#20559;&#24046;&#20316;&#20026;&#26080;&#38656;&#27491;&#30830;&#29702;&#35299;&#21363;&#21487;&#27491;&#30830;&#35299;&#20915;&#21508;&#31181;VL&#20219;&#21153;&#30340;&#25463;&#24452;&#12290;&#31532;&#19968;&#31181;&#25968;&#25454;&#38598;&#20559;&#24046;&#26159;"&#19981;&#24179;&#34913;&#21305;&#37197;"&#20559;&#24046;&#65292;&#21363;&#27491;&#30830;&#31572;&#26696;&#19982;&#38382;&#39064;&#21644;&#22270;&#20687;&#30340;&#37325;&#21472;&#31243;&#24230;&#36229;&#36807;&#38169;&#35823;&#31572;&#26696;&#12290;&#31532;&#20108;&#31181;&#25968;&#25454;&#38598;&#20559;&#24046;&#26159;"&#20998;&#24515;&#30456;&#20284;&#24615;"&#20559;&#24046;&#65292;&#21363;&#38169;&#35823;&#31572;&#26696;&#19982;&#27491;&#30830;&#31572;&#26696;&#36807;&#20110;&#19981;&#30456;&#20284;&#65292;&#20294;&#19982;&#21516;&#19968;&#20010;&#26679;&#26412;&#20013;&#30340;&#20854;&#20182;&#38169;&#35823;&#31572;&#26696;&#30456;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25968;&#25454;&#38598;&#20559;&#24046;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#23545;&#25239;&#25968;&#25454;&#21512;&#25104;&#65288;ADS&#65289;&#26469;&#29983;&#25104;&#21512;&#25104;&#30340;&#35757;&#32451;&#21644;&#21435;&#20559;&#24046;&#30340;&#35780;&#20272;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26679;&#26412;&#20869; &#23545;&#31435;&#35757;&#32451;&#65288;ICT&#65289;&#26469;&#24110;&#21161;&#27169;&#22411;&#21033;&#29992;&#21512;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#23545;&#31435;&#20107;&#23454;&#25968;&#25454;&#65292;&#36890;&#36807;&#27880;&#37325;&#26679;&#26412;&#20869;&#30340;&#24046;&#24322;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language (VL) understanding tasks evaluate models' comprehension of complex visual scenes through multiple-choice questions. However, we have identified two dataset biases that models can exploit as shortcuts to resolve various VL tasks correctly without proper understanding. The first type of dataset bias is \emph{Unbalanced Matching} bias, where the correct answer overlaps the question and image more than the incorrect answers. The second type of dataset bias is \emph{Distractor Similarity} bias, where incorrect answers are overly dissimilar to the correct answer but significantly similar to other incorrect answers within the same sample. To address these dataset biases, we first propose Adversarial Data Synthesis (ADS) to generate synthetic training and debiased evaluation data. We then introduce Intra-sample Counterfactual Training (ICT) to assist models in utilizing the synthesized training data, particularly the counterfactual data, via focusing on intra-sample differentia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#33719;&#21462;&#38899;&#39057;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#24773;&#24863;&#32500;&#24230;&#30340;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.11830</link><description>&lt;p&gt;
CLARA: &#22810;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#38899;&#39057;&#34920;&#31034;&#33719;&#21462;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
CLARA: Multilingual Contrastive Learning for Audio Representation Acquisition. (arXiv:2310.11830v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#33719;&#21462;&#38899;&#39057;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#21644;&#24773;&#24863;&#32500;&#24230;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#12290;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#21046;&#32422;&#20102;&#36328;&#35821;&#35328;&#35821;&#38899;&#22788;&#29702;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#23545;&#27604;&#23398;&#20064;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#33258;&#30417;&#30563;&#25216;&#26415;&#26469;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#20943;&#23569;&#25968;&#25454;&#20381;&#36182;&#24615;&#21644;&#25913;&#21892;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#26465;&#20214;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#35821;&#35328;&#20013;&#33719;&#24471;&#20849;&#20139;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#20351;&#29992;&#26377;&#38480;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20027;&#35266;&#24863;&#30693;&#35780;&#20272;&#30340;&#25361;&#25112;&#65292;&#25429;&#25417;&#35821;&#38899;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#26159;&#22256;&#38590;&#30340;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#22810;&#26679;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#36798;&#24615;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#24320;&#21457;&#32534;&#30721;&#24773;&#24863;&#32500;&#24230;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#38899;&#39057;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel framework for multilingual speech and sound representation learning using contrastive learning. The lack of sizeable labelled datasets hinders speech-processing research across languages. Recent advances in contrastive learning provide self-supervised techniques to learn from unlabelled data. Motivated by reducing data dependence and improving generalisation across diverse languages and conditions, we develop a multilingual contrastive framework. This framework enables models to acquire shared representations across languages, facilitating cross-lingual transfer with limited target language data.  Additionally, capturing emotional cues within speech is challenging due to subjective perceptual assessments. By learning expressive representations from diverse, multilingual data in a self-supervised manner, our approach aims to develop speech representations that encode emotive dimensions.  Our method trains encoders on a large corpus of multi-lingual audio data
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#21442;&#25968;&#38656;&#27714;&#23398;&#20064;&#21644;&#24179;&#28369;&#33258;&#36866;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30456;&#20284;&#26465;&#20214;&#23454;&#29616;&#20102;&#26368;&#23567;&#21270;&#26497;&#38480;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2310.07558</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#21442;&#25968;&#38656;&#27714;&#23398;&#20064;&#30340;&#24179;&#28369;&#33258;&#36866;&#24212;&#21160;&#24577;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
Smootheness-Adaptive Dynamic Pricing with Nonparametric Demand Learning. (arXiv:2310.07558v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07558
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#21442;&#25968;&#38656;&#27714;&#23398;&#20064;&#21644;&#24179;&#28369;&#33258;&#36866;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30456;&#20284;&#26465;&#20214;&#23454;&#29616;&#20102;&#26368;&#23567;&#21270;&#26497;&#38480;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38656;&#27714;&#20989;&#25968;&#20026;&#38750;&#21442;&#25968;&#21644;Holder&#24179;&#28369;&#30340;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#65292;&#24182;&#19988;&#25105;&#20204;&#19987;&#27880;&#20110;&#36866;&#24212;&#26410;&#30693;&#30340;Holder&#24179;&#28369;&#21442;&#25968;&#946;&#30340;&#33021;&#21147;&#12290;&#20256;&#32479;&#19978;&#65292;&#26368;&#20248;&#30340;&#21160;&#24577;&#23450;&#20215;&#31639;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#23545;&#946;&#30340;&#20102;&#35299;&#65292;&#20197;&#36798;&#21040;&#19968;&#20010;&#26368;&#23567;&#21270;&#26497;&#38480;&#36951;&#25022;&#30340;&#25928;&#26524;&#65292;&#21363;O(T^((&#946;+1)/(2&#946;+1)))&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#27809;&#26377;&#23450;&#20215;&#31574;&#30053;&#33021;&#22815;&#22312;&#19981;&#30693;&#36947;&#946;&#30340;&#24773;&#20917;&#19979;&#33258;&#36866;&#24212;&#22320;&#36798;&#21040;&#36825;&#20010;&#26368;&#23567;&#21270;&#26497;&#38480;&#36951;&#25022;&#65292;&#31361;&#26174;&#20102;&#36825;&#20010;&#21160;&#24577;&#23450;&#20215;&#38382;&#39064;&#30340;&#36866;&#24212;&#24615;&#25361;&#25112;&#12290;&#21463;&#21040;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30456;&#20284;&#26465;&#20214;&#26469;&#23454;&#29616;&#36866;&#24212;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#30456;&#20284;&#26465;&#20214;&#19981;&#20250;&#25439;&#23475;&#38382;&#39064;&#26412;&#36523;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#23427;&#20445;&#25345;&#20102;&#28176;&#36817;&#36951;&#25022;&#19979;&#30028;&#937;(T^((&#946;+1)/(2&#946;+1)))&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24179;&#28369;&#33258;&#36866;&#24212;&#30340;&#21160;&#24577;&#23450;&#20215;&#31639;&#27861;&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the dynamic pricing problem where the demand function is nonparametric and H\"older smooth, and we focus on adaptivity to the unknown H\"older smoothness parameter $\beta$ of the demand function. Traditionally the optimal dynamic pricing algorithm heavily relies on the knowledge of $\beta$ to achieve a minimax optimal regret of $\widetilde{O}(T^{\frac{\beta+1}{2\beta+1}})$. However, we highlight the challenge of adaptivity in this dynamic pricing problem by proving that no pricing policy can adaptively achieve this minimax optimal regret without knowledge of $\beta$. Motivated by the impossibility result, we propose a self-similarity condition to enable adaptivity. Importantly, we show that the self-similarity condition does not compromise the problem's inherent complexity since it preserves the regret lower bound $\Omega(T^{\frac{\beta+1}{2\beta+1}})$. Furthermore, we develop a smoothness-adaptive dynamic pricing algorithm and theoretically prove that the algorithm achieves t
&lt;/p&gt;</description></item><item><title>FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06763</link><description>&lt;p&gt;
FABind: &#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06763
&lt;/p&gt;
&lt;p&gt;
FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#23545;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#24182;&#20934;&#30830;&#39044;&#27979;&#20854;&#32467;&#21512;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#37319;&#26679;&#27861;&#21644;&#22238;&#24402;&#27861;&#25104;&#20026;&#20004;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;&#37319;&#26679;&#27861;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#32467;&#26500;&#26469;&#36827;&#34892;&#36873;&#25321;&#32780;&#25928;&#29575;&#36739;&#20302;&#12290;&#32780;&#22238;&#24402;&#27861;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#34507;&#30333;&#36136;&#22823;&#23567;&#30340;&#21464;&#21270;&#36890;&#24120;&#38656;&#35201;&#22806;&#37096;&#27169;&#22359;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#32467;&#21512;&#21475;&#34955;&#65292;&#36827;&#19968;&#27493;&#24433;&#21709;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind&#65292;&#19968;&#20010;&#23558;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30456;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#37319;&#26679;&#22120;&#25351;&#23450;&#38544;&#21547;&#20998;&#24067;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36817;&#20284;&#22797;&#26434;&#30340;&#22810;&#23792;&#21644;&#30456;&#20851;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#32447;&#24615;&#21270;&#30340;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39069;&#22806;&#30340;&#32593;&#32476;&#21644;&#19981;&#31283;&#23450;&#23545;&#25239;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#26550;&#26500;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#25968;&#30334;&#19975;&#20010;&#28508;&#21464;&#37327;&#30340;&#38544;&#21547;&#20998;&#24067;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#22823;&#22411;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23618;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#23545;&#20110;&#32593;&#32476;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.06643</link><description>&lt;p&gt;
&#39640;&#32500;&#21518;&#39564;&#25512;&#26029;&#30340;&#38544;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Implicit Variational Inference for High-Dimensional Posteriors. (arXiv:2310.06643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#21464;&#20998;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#37319;&#26679;&#22120;&#25351;&#23450;&#38544;&#21547;&#20998;&#24067;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36817;&#20284;&#22797;&#26434;&#30340;&#22810;&#23792;&#21644;&#30456;&#20851;&#21518;&#39564;&#20998;&#24067;&#12290;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#32447;&#24615;&#21270;&#30340;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39069;&#22806;&#30340;&#32593;&#32476;&#21644;&#19981;&#31283;&#23450;&#23545;&#25239;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#26550;&#26500;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#25968;&#30334;&#19975;&#20010;&#28508;&#21464;&#37327;&#30340;&#38544;&#21547;&#20998;&#24067;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#22823;&#22411;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#23618;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#23545;&#20110;&#32593;&#32476;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#25512;&#26029;&#20013;&#65292;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#22909;&#22788;&#22312;&#20110;&#20934;&#30830;&#25429;&#25417;&#30495;&#23454;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#25351;&#23450;&#38544;&#21547;&#20998;&#24067;&#30340;&#31070;&#32463;&#37319;&#26679;&#22120;&#65292;&#36825;&#23545;&#20110;&#36817;&#20284;&#39640;&#32500;&#31354;&#38388;&#20013;&#22797;&#26434;&#22810;&#23792;&#21644;&#30456;&#20851;&#21518;&#39564;&#20998;&#24067;&#38750;&#24120;&#36866;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#21270;&#31070;&#32463;&#37319;&#26679;&#22120;&#24341;&#20837;&#26032;&#30340;&#32422;&#26463;&#65292;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#37492;&#21035;&#22120;&#32593;&#32476;&#21644;&#19981;&#31283;&#23450;&#30340;&#23545;&#25239;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#22120;&#26550;&#26500;&#65292;&#39318;&#27425;&#23454;&#29616;&#20102;&#23545;&#25968;&#30334;&#19975;&#20010;&#28508;&#21464;&#37327;&#30340;&#38544;&#21547;&#20998;&#24067;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#25968;&#20540;&#36817;&#20284;&#26469;&#35299;&#20915;&#35745;&#31639;&#19978;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#22823;&#22411;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#24674;&#22797;&#23618;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#32593;&#32476;&#24615;&#33021;&#20851;&#38190;&#20294;&#33261;&#21517;&#26157;&#33879;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In variational inference, the benefits of Bayesian models rely on accurately capturing the true posterior distribution. We propose using neural samplers that specify implicit distributions, which are well-suited for approximating complex multimodal and correlated posteriors in high-dimensional spaces. Our approach advances inference using implicit distributions by introducing novel bounds that come about by locally linearising the neural sampler. This is distinct from existing methods that rely on additional discriminator networks and unstable adversarial objectives. Furthermore, we present a new sampler architecture that, for the first time, enables implicit distributions over millions of latent variables, addressing computational concerns by using differentiable numerical approximations. Our empirical analysis indicates our method is capable of recovering correlations across layers in large Bayesian neural networks, a property that is crucial for a network's performance but notorious
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26102;&#31354;&#31070;&#32463;&#28857;&#36807;&#31243;&#31215;&#20998;&#26041;&#27861;(AutoSTPP)&#65292;&#25193;&#23637;&#20102;AutoInt&#26041;&#27861;&#29992;&#20110;&#19977;&#32500;&#26102;&#31354;&#28857;&#36807;&#31243;(STPP)&#30340;&#35745;&#31639;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06179</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#26102;&#31354;&#31070;&#32463;&#28857;&#36807;&#31243;&#31215;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic Integration for Spatiotemporal Neural Point Processes. (arXiv:2310.06179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#26102;&#31354;&#31070;&#32463;&#28857;&#36807;&#31243;&#31215;&#20998;&#26041;&#27861;(AutoSTPP)&#65292;&#25193;&#23637;&#20102;AutoInt&#26041;&#27861;&#29992;&#20110;&#19977;&#32500;&#26102;&#31354;&#28857;&#36807;&#31243;(STPP)&#30340;&#35745;&#31639;&#65292;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36830;&#32493;&#26102;&#38388;&#30340;&#28857;&#36807;&#31243;&#23545;&#20110;&#35768;&#22810;&#31163;&#25955;&#20107;&#20214;&#39044;&#27979;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26102;&#31354;&#28857;&#36807;&#31243;&#65288;STPPs&#65289;&#30340;&#31215;&#20998;&#38382;&#39064;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#36827;&#34892;&#19977;&#37325;&#31215;&#20998;&#35745;&#31639;&#12290;&#29616;&#26377;&#30340;STPP&#31215;&#20998;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#24378;&#24230;&#20989;&#25968;&#20855;&#26377;&#21442;&#25968;&#24418;&#24335;&#65292;&#36825;&#32570;&#20047;&#28789;&#27963;&#24615;&#65307;&#35201;&#20040;&#29992;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#36817;&#20284;&#24378;&#24230;&#65292;&#36825;&#24341;&#20837;&#20102;&#25968;&#20540;&#35823;&#24046;&#12290;Omi&#31561;&#20154;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#31215;&#20998;&#26041;&#27861;AutoInt&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#31215;&#20998;&#28789;&#27963;&#30340;&#24378;&#24230;&#20989;&#25968;&#65292;&#20294;&#35813;&#26041;&#27861;&#21482;&#20851;&#27880;1D&#26102;&#38388;&#28857;&#36807;&#31243;&#12290;&#26412;&#25991;&#23558;AutoInt&#26041;&#27861;&#25193;&#23637;&#33267;3D STPP&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65306;AutoSTPP&#65288;&#33258;&#21160;&#21270;&#30340;&#26102;&#31354;&#31070;&#32463;&#28857;&#36807;&#31243;&#31215;&#20998;&#26041;&#27861;&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#30452;&#25509;&#25193;&#23637;&#20043;&#21069;&#30340;&#24037;&#20316;&#20250;&#36807;&#20110;&#32422;&#26463;&#24378;&#24230;&#20989;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning continuous-time point processes is essential to many discrete event forecasting tasks. However, integration poses a major challenge, particularly for spatiotemporal point processes (STPPs), as it involves calculating the likelihood through triple integrals over space and time. Existing methods for integrating STPP either assume a parametric form of the intensity function, which lacks flexibility; or approximating the intensity with Monte Carlo sampling, which introduces numerical errors. Recent work by Omi et al. [2019] proposes a dual network or AutoInt approach for efficient integration of flexible intensity function. However, the method only focuses on the 1D temporal point process. In this paper, we introduce a novel paradigm: AutoSTPP (Automatic Integration for Spatiotemporal Neural Point Processes) that extends the AutoInt approach to 3D STPP. We show that direct extension of the previous work overly constrains the intensity function, leading to poor performance. We prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.05506</link><description>&lt;p&gt;
&#26597;&#35810;&#21644;&#24212;&#31572;&#22686;&#24378;&#19981;&#33021;&#24110;&#21161;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization. (arXiv:2310.05506v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#26102;&#65292;&#36890;&#36807;&#26597;&#35810;&#28436;&#21270;&#21644;&#22810;&#26679;&#21270;&#25512;&#29702;&#36335;&#24452;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#32463;&#39564;&#19978;&#34987;&#39564;&#35777;&#20026;&#26377;&#25928;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#24320;&#28304;LLMs&#21644;&#39030;&#23574;&#19987;&#26377;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#26088;&#22312;&#22238;&#31572;&#65306;&#65288;1&#65289;&#21738;&#20123;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26356;&#26377;&#25928;&#65307;&#65288;2&#65289;&#22686;&#24378;&#25968;&#25454;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32553;&#25918;&#20851;&#31995;&#22914;&#20309;&#65307;&#65288;3&#65289;&#25968;&#25454;&#22686;&#24378;&#33021;&#21542;&#28608;&#21169;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#30340;&#27867;&#21270;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;GSM8K&#26597;&#35810;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#20197;&#21450;&#37319;&#26679;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;AugGSM8K&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;AugGSM8K&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#31216;&#20026;MuggleMath&#12290;MuggleMath&#22312;GSM8K&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65288;&#22312;7B&#35268;&#27169;&#19978;&#20174;54%&#25552;&#39640;&#21040;68.4%&#65292;&#22312;&#25193;&#25918;&#21040;63.9%&#21040;74.0%&#20043;&#38388;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.05492</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition. (arXiv:2310.05492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36739;&#22823;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22686;&#21152;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#65292;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#26631;&#35760;&#21644;&#21442;&#25968;&#65292;&#23637;&#29616;&#20986;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#25351;&#20196;&#36319;&#38543;&#31561;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#24320;&#28304;&#31038;&#21306;&#24050;&#32463;&#30740;&#31350;&#20102;&#38024;&#23545;&#27599;&#31181;&#33021;&#21147;&#30340;&#20020;&#26102;SFT&#65292;&#32780;&#19987;&#26377;LLMs&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;SFT&#35299;&#38145;&#22810;&#37325;&#33021;&#21147;&#21464;&#24471;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;SFT&#36807;&#31243;&#20013;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20154;&#31867;&#23545;&#40784;&#33021;&#21147;&#20043;&#38388;&#30340;&#25968;&#25454;&#32452;&#21512;&#12290;&#20174;&#35268;&#27169;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#33021;&#21147;&#19982;&#21508;&#31181;&#22240;&#32032;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#25968;&#25454;&#37327;&#12289;&#25968;&#25454;&#32452;&#21512;&#27604;&#20363;&#12289;&#27169;&#22411;&#21442;&#25968;&#21644;SFT&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#33021;&#21147;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25193;&#23637;&#27169;&#24335;&#65292;&#36739;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#30456;&#21516;&#30340;&#25968;&#25454;&#37327;&#19979;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#25968;&#23398;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#36890;&#36807;&#24494;&#35843;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with enormous pre-training tokens and parameter amounts emerge abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). The open-source community has studied on ad-hoc SFT for each ability, while proprietary LLMs are versatile for all abilities. It is important to investigate how to unlock them with multiple abilities via SFT. In this study, we specifically focus on the data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. From a scaling perspective, we investigate the relationship between model abilities and various factors including data amounts, data composition ratio, model parameters, and SFT strategies. Our experiments reveal that different abilities exhibit different scaling patterns, and larger models generally show superior performance with the same amount of data. Mathematical reasoning and code
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#35009;&#20915;&#30340;&#33258;&#21160;&#21270;&#21311;&#21517;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#65292;&#20351;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;F1&#20998;&#25968;&#36229;&#36807;5%&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#23558;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#21171;&#21160;&#24182;&#22686;&#24378;&#33258;&#21160;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.04632</link><description>&lt;p&gt;
&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#35009;&#20915;&#30340;&#33258;&#21160;&#21270;&#21311;&#21517;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Anonymization of Swiss Federal Supreme Court Rulings. (arXiv:2310.04632v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#35009;&#20915;&#30340;&#33258;&#21160;&#21270;&#21311;&#21517;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#65292;&#20351;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;F1&#20998;&#25968;&#36229;&#36807;5%&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#23558;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#21171;&#21160;&#24182;&#22686;&#24378;&#33258;&#21160;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27861;&#38498;&#30340;&#35009;&#20915;&#20844;&#24320;&#38656;&#35201;&#36827;&#34892;&#36866;&#24403;&#30340;&#21311;&#21517;&#21270;&#65292;&#20197;&#20445;&#25252;&#25152;&#26377;&#30456;&#20851;&#26041;&#30340;&#38544;&#31169;&#12290;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#20381;&#38752;&#19968;&#31181;&#24050;&#26377;&#30340;&#31995;&#32479;&#65292;&#23558;&#19981;&#21516;&#30340;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#19982;&#20154;&#24037;&#19987;&#23478;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#24102;&#26377;&#35201;&#21311;&#21517;&#21270;&#23454;&#20307;&#27880;&#37322;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#36719;&#20214;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#21644;&#22312;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#39046;&#22495;&#20869;&#25968;&#25454;&#26469;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;F1&#20998;&#25968;&#36229;&#36807;5&#65285;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#23558;&#29616;&#26377;&#30340;&#21311;&#21517;&#21270;&#26041;&#27861;&#65288;&#22914;&#27491;&#21017;&#34920;&#36798;&#24335;&#65289;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#20154;&#24037;&#21171;&#21160;&#24182;&#22686;&#24378;&#33258;&#21160;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Releasing court decisions to the public relies on proper anonymization to protect all involved parties, where necessary. The Swiss Federal Supreme Court relies on an existing system that combines different traditional computational methods with human experts. In this work, we enhance the existing anonymization software using a large dataset annotated with entities to be anonymized. We compared BERT-based models with models pre-trained on in-domain data. Our results show that using in-domain data to pre-train the models further improves the F1-score by more than 5\% compared to existing models. Our work demonstrates that combining existing anonymization methods, such as regular expressions, with machine learning can further reduce manual labor and enhance automatic suggestions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TacoGFN&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#36716;&#25442;&#22120;&#21644;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#23376;&#31354;&#38388;&#25506;&#32034;&#21644;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32467;&#21512;&#25913;&#21892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03223</link><description>&lt;p&gt;
TacoGFN: &#38024;&#23545;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet
&lt;/p&gt;
&lt;p&gt;
TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03223
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TacoGFN&#30340;&#30446;&#26631;&#26465;&#20214;GFlowNet&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#36716;&#25442;&#22120;&#21644;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#30340;&#20998;&#23376;&#31354;&#38388;&#25506;&#32034;&#21644;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#32467;&#21512;&#25913;&#21892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#33258;&#21160;&#21270;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#34507;&#30333;&#36136;&#21475;&#34955;&#30446;&#26631;&#30340;&#31867;&#33647;&#29289;&#21270;&#21512;&#29289;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#26041;&#27861;&#26159;&#23545;&#26377;&#38480;&#25968;&#25454;&#38598;&#20013;&#30340;&#34507;&#30333;&#36136;-&#20998;&#23376;&#20998;&#24067;&#36827;&#34892;&#36817;&#20284;&#65292;&#22240;&#27492;&#22312;&#29983;&#25104;&#30340;&#20998;&#23376;&#20013;&#24456;&#38590;&#23454;&#29616;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#32467;&#21512;&#25913;&#21892;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#23558;&#21475;&#34955;&#26465;&#20214;&#19979;&#30340;&#20998;&#23376;&#29983;&#25104;&#20219;&#21153;&#23450;&#20041;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;TacoGFN&#65292;&#19968;&#31181;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#40723;&#21169;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#23646;&#24615;&#30340;&#20998;&#23376;&#65292;&#32780;&#19981;&#26159;&#36866;&#24212;&#39044;&#20808;&#23384;&#22312;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#26041;&#27861;&#26469;&#21152;&#24555;&#23545;&#25509;&#24471;&#20998;&#35745;&#31639;&#65292;&#24182;&#25552;&#20986;&#20102;TacoGFN&#26469;&#39640;&#25928;&#22320;&#25506;&#32034;&#20998;&#23376;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#20960;&#36718;&#20027;&#21160;&#23398;&#20064;&#65292;&#20351;&#29992;&#23545;&#25509;&#31070;&#32463;&#32593;&#32476;&#23545;&#29983;&#25104;&#30340;&#26679;&#26412;&#36827;&#34892;&#26597;&#35810;&#65292;&#20197;&#25913;&#21892;&#23545;&#25509;&#24471;&#20998;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#25506;&#32034;&#26356;&#22810;&#30340;&#20998;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to automate the generation of drug-like compounds conditioned to specific protein pocket targets. Most current methods approximate the protein-molecule distribution of a finite dataset and, therefore struggle to generate molecules with significant binding improvement over the training dataset. We instead frame the pocket-conditioned molecular generation task as an RL problem and develop TacoGFN, a target conditional Generative Flow Network model. Our method is explicitly encouraged to generate molecules with desired properties as opposed to fitting on a pre-existing data distribution. To this end, we develop transformer-based docking score prediction to speed up docking score computation and propose TacoGFN to explore molecule space efficiently. Furthermore, we incorporate several rounds of active learning where generated samples are queried using a docking oracle to improve the docking score prediction. This approach allows us to accurately explore as much of the molecule land
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;NMPC&#65289;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22686;&#37327;ISS&#31995;&#32479;&#12290;&#36890;&#36807;&#31616;&#21270;&#35745;&#31639;&#32456;&#31471;&#25104;&#20998;&#65292;&#24182;&#26126;&#30830;&#23450;&#20041;&#26368;&#23567;&#39044;&#27979;&#33539;&#22260;&#65292;&#23454;&#29616;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;GRU&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#36523;&#23450;&#21046;&#29366;&#24577;&#35266;&#23519;&#22120;&#30340;&#35774;&#35745;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#25511;&#21046;&#26550;&#26500;&#20855;&#26377;&#33391;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16428</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;MPC&#35774;&#35745;&#24212;&#29992;&#20110;&#22686;&#37327;ISS&#31995;&#32479;&#65292;&#20197;GRU&#32593;&#32476;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Nonlinear MPC design for incrementally ISS systems with application to GRU networks. (arXiv:2309.16428v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;NMPC&#65289;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22686;&#37327;ISS&#31995;&#32479;&#12290;&#36890;&#36807;&#31616;&#21270;&#35745;&#31639;&#32456;&#31471;&#25104;&#20998;&#65292;&#24182;&#26126;&#30830;&#23450;&#20041;&#26368;&#23567;&#39044;&#27979;&#33539;&#22260;&#65292;&#23454;&#29616;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;GRU&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#36523;&#23450;&#21046;&#29366;&#24577;&#35266;&#23519;&#22120;&#30340;&#35774;&#35745;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#25511;&#21046;&#26550;&#26500;&#20855;&#26377;&#33391;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#25351;&#25968;&#22686;&#37327;&#36755;&#20837;-&#29366;&#24577;&#31283;&#23450;&#65288;ISS&#65289;&#31995;&#32479;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;NMPC&#65289;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#65292;&#26080;&#38656;&#35745;&#31639;&#32456;&#31471;&#25104;&#20998;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#26126;&#30830;&#23450;&#20041;&#30340;&#26368;&#23567;&#39044;&#27979;&#33539;&#22260;&#20197;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#35813;&#35774;&#35745;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#30001;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;RNNs&#20197;&#20854;&#22686;&#24378;&#24314;&#27169;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#32780;&#22686;&#37327;ISS&#23646;&#24615;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#20195;&#25968;&#26465;&#20214;&#36827;&#34892;&#30740;&#31350;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#37327;&#36523;&#23450;&#21046;&#29366;&#24577;&#35266;&#23519;&#22120;&#30340;&#35774;&#35745;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#25511;&#21046;&#26550;&#26500;&#22312;&#22522;&#20934;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#20854;&#33391;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This brief addresses the design of a Nonlinear Model Predictive Control (NMPC) strategy for exponentially incremental Input-to-State Stable (ISS) systems. In particular, a novel formulation is devised, which does not necessitate the onerous computation of terminal ingredients, but rather relies on the explicit definition of a minimum prediction horizon ensuring closed-loop stability. The designed methodology is particularly suited for the control of systems learned by Recurrent Neural Networks (RNNs), which are known for their enhanced modeling capabilities and for which the incremental ISS properties can be studied thanks to simple algebraic conditions. The approach is applied to Gated Recurrent Unit (GRU) networks, providing also a method for the design of a tailored state observer with convergence guarantees. The resulting control architecture is tested on a benchmark system, demonstrating its good control performances and efficient applicability.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;Koopman&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#32534;&#30721;&#22120;&#21644;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26469;&#21516;&#26102;&#23398;&#20064;Koopman&#28508;&#22312;&#23884;&#20837;&#12289;&#31639;&#23376;&#21644;&#30456;&#20851;&#32447;&#24615;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20219;&#21153;&#25104;&#26412;&#20316;&#20026;&#20027;&#35201;&#30446;&#26631;&#65292;&#20943;&#23569;&#20102;&#23545;&#20110;&#26126;&#30830;&#23450;&#20041;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#20381;&#36182;&#65292;&#23558;Koopman&#25511;&#21046;&#25193;&#23637;&#21040;&#39640;&#32500;&#12289;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;&#20687;&#32032;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.16077</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;Koopman&#25511;&#21046;&#21644;&#23545;&#27604;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented Koopman-Based Control with Contrastive Encoder. (arXiv:2309.16077v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;Koopman&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#32534;&#30721;&#22120;&#21644;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26469;&#21516;&#26102;&#23398;&#20064;Koopman&#28508;&#22312;&#23884;&#20837;&#12289;&#31639;&#23376;&#21644;&#30456;&#20851;&#32447;&#24615;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20219;&#21153;&#25104;&#26412;&#20316;&#20026;&#20027;&#35201;&#30446;&#26631;&#65292;&#20943;&#23569;&#20102;&#23545;&#20110;&#26126;&#30830;&#23450;&#20041;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#20381;&#36182;&#65292;&#23558;Koopman&#25511;&#21046;&#25193;&#23637;&#21040;&#39640;&#32500;&#12289;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;&#20687;&#32032;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#27604;&#32534;&#30721;&#22120;&#21516;&#26102;&#23398;&#20064;Koopman&#28508;&#22312;&#23884;&#20837;&#65292;&#31639;&#23376;&#21644;&#30456;&#20851;&#32447;&#24615;&#25511;&#21046;&#22120;&#30340;&#20219;&#21153;&#23548;&#21521;Koopman&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#25104;&#26412;&#20316;&#20026;&#20027;&#35201;&#30446;&#26631;&#36827;&#34892;&#25511;&#21046;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#23545;&#20110;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#20381;&#36182;&#65292;&#23558;Koopman&#25511;&#21046;&#25193;&#23637;&#21040;&#21253;&#25324;&#22522;&#20110;&#20687;&#32032;&#30340;&#22330;&#26223;&#22312;&#20869;&#30340;&#39640;&#32500;&#12289;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present task-oriented Koopman-based control that utilizes end-to-end reinforcement learning and contrastive encoder to simultaneously learn the Koopman latent embedding, operator and associated linear controller within an iterative loop. By prioritizing the task cost as main objective for controller learning, we reduce the reliance of controller design on a well-identified model, which extends Koopman control beyond low-dimensional systems to high-dimensional, complex nonlinear systems, including pixel-based scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;&#20854;&#21152;&#24378;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#33021;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.13414</link><description>&lt;p&gt;
&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#26159;&#24102;&#26377;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#20840;&#33021;&#36924;&#36817;&#22120;
&lt;/p&gt;
&lt;p&gt;
State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory. (arXiv:2309.13414v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;&#20854;&#21152;&#24378;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#33021;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#31616;&#21333;&#26377;&#25928;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#24207;&#21015;&#24314;&#27169;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#27839;&#26102;&#38388;&#26041;&#21521;&#32570;&#20047;&#38750;&#32447;&#24615;&#28608;&#27963;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22534;&#21472;&#20855;&#26377;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36275;&#20197;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36880;&#23618;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#28155;&#21152;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#22797;&#26434;&#24207;&#21015;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21487;&#20197;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30475;&#21040;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24182;&#19981;&#26681;&#26412;&#35299;&#20915;&#25351;&#25968;&#34928;&#20943;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;&#29702;&#35770;&#32467;&#26524;&#32463;&#36807;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the exponential decaying memory issue. Theoretical results are justified by numerical verifications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#38544;&#24418;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#65292;&#22312;&#20445;&#35777;&#27491;&#24120;&#38899;&#39057;&#29983;&#25104;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20026;&#27169;&#22411;&#39564;&#35777;&#25552;&#20379;&#20445;&#25252;&#23618;&#65292;&#29992;&#20110;&#37492;&#21035;&#27169;&#22411;&#25152;&#26377;&#26435;&#21644;&#32500;&#25252;&#20854;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13166</link><description>&lt;p&gt;
&#29992;&#20110;&#38899;&#39057;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#38544;&#24418;&#25968;&#23383;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Invisible Watermarking for Audio Generation Diffusion Models. (arXiv:2309.13166v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#38544;&#24418;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#65292;&#22312;&#20445;&#35777;&#27491;&#24120;&#38899;&#39057;&#29983;&#25104;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20026;&#27169;&#22411;&#39564;&#35777;&#25552;&#20379;&#20445;&#25252;&#23618;&#65292;&#29992;&#20110;&#37492;&#21035;&#27169;&#22411;&#25152;&#26377;&#26435;&#21644;&#32500;&#25252;&#20854;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#20854;&#25968;&#25454;&#29983;&#25104;&#21644;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;&#22270;&#20687;&#21644;&#38899;&#39057;&#39046;&#22495;&#37117;&#22791;&#21463;&#37325;&#35270;&#12290;&#22312;&#36805;&#36895;&#21457;&#23637;&#30340;&#38899;&#39057;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#21644;&#30830;&#31435;&#25968;&#25454;&#30340;&#29256;&#26435;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#27425;&#24212;&#29992;&#20110;&#35757;&#32451;&#22312;mel&#39057;&#35889;&#22270;&#19978;&#30340;&#38899;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#36825;&#23545;&#19978;&#36848;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#27491;&#24120;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19981;&#21487;&#35265;&#30340;&#27700;&#21360;&#35302;&#21457;&#26426;&#21046;&#26469;&#36827;&#34892;&#27169;&#22411;&#39564;&#35777;&#12290;&#36825;&#20010;&#27700;&#21360;&#35302;&#21457;&#22120;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#23618;&#65292;&#33021;&#22815;&#35782;&#21035;&#27169;&#22411;&#30340;&#25152;&#26377;&#32773;&#24182;&#30830;&#20445;&#20854;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#19981;&#21487;&#35265;&#30340;&#27700;&#21360;&#35302;&#21457;&#22120;&#22312;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#20462;&#25913;&#30340;&#21516;&#26102;&#36824;&#33021;&#20445;&#25345;&#39640;&#25928;&#30340;&#21512;&#27861;&#38899;&#39057;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained prominence in the image domain for their capabilities in data generation and transformation, achieving state-of-the-art performance in various tasks in both image and audio domains. In the rapidly evolving field of audio-based machine learning, safeguarding model integrity and establishing data copyright are of paramount importance. This paper presents the first watermarking technique applied to audio diffusion models trained on mel-spectrograms. This offers a novel approach to the aforementioned challenges. Our model excels not only in benign audio generation, but also incorporates an invisible watermarking trigger mechanism for model verification. This watermark trigger serves as a protective layer, enabling the identification of model ownership and ensuring its integrity. Through extensive experiments, we demonstrate that invisible watermark triggers can effectively protect against unauthorized modifications while maintaining high utility in benign audio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25903;&#25745;&#40065;&#26834;&#25512;&#26029;&#30340;&#20984;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20984;&#35268;&#21010;&#25552;&#20379;&#31574;&#30053;&#20215;&#20540;&#30340;&#31934;&#30830;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#36827;&#34892;&#22810;&#31181;&#25193;&#23637;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.12450</link><description>&lt;p&gt;
&#25903;&#25745;&#40065;&#26834;&#25512;&#26029;&#30340;&#20984;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Convex Framework for Confounding Robust Inference. (arXiv:2309.12450v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25903;&#25745;&#40065;&#26834;&#25512;&#26029;&#30340;&#20984;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20984;&#35268;&#21010;&#25552;&#20379;&#31574;&#30053;&#20215;&#20540;&#30340;&#31934;&#30830;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#36827;&#34892;&#22810;&#31181;&#25193;&#23637;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21463;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#30340;&#31163;&#32447;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#24120;&#34987;&#29992;&#26469;&#22312;&#32473;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19978;&#20272;&#35745;&#22312;&#26368;&#22351;&#28151;&#28102;&#24773;&#20917;&#19979;&#30340;&#31574;&#30053;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20026;&#20102;&#21487;&#34892;&#24615;&#32780;&#37319;&#29992;&#19968;&#20123;&#31895;&#31961;&#30340;&#26494;&#24347;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#23548;&#33268;&#23545;&#31574;&#30053;&#20215;&#20540;&#30340;&#20272;&#35745;&#36807;&#20110;&#20445;&#23432;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#20984;&#35268;&#21010;&#25552;&#20379;&#20102;&#31574;&#30053;&#20215;&#20540;&#30340;&#19968;&#20010;&#36739;&#20026;&#31934;&#30830;&#30340;&#19979;&#30028;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#20351;&#24471;&#20854;&#33021;&#22815;&#36827;&#34892;&#22810;&#31181;&#25193;&#23637;&#65292;&#20363;&#22914;&#22522;&#20110;f-&#20998;&#27495;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#12289;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#21644;&#20449;&#24687;&#20934;&#21017;&#30340;&#27169;&#22411;&#36873;&#25321;&#20197;&#21450;&#21033;&#29992;&#19978;&#30028;&#36827;&#34892;&#40065;&#26834;&#31574;&#30053;&#23398;&#20064;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#24378;&#23545;&#20598;&#24615;&#37325;&#26032;&#34920;&#36848;&#20026;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#21033;&#29992;M&#25216;&#26415;&#25552;&#20379;&#20102;&#23545;&#25152;&#25552;&#20986;&#20272;&#35745;&#22120;&#30340;&#24378;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study policy evaluation of offline contextual bandits subject to unobserved confounders. Sensitivity analysis methods are commonly used to estimate the policy value under the worst-case confounding over a given uncertainty set. However, existing work often resorts to some coarse relaxation of the uncertainty set for the sake of tractability, leading to overly conservative estimation of the policy value. In this paper, we propose a general estimator that provides a sharp lower bound of the policy value using convex programming. The generality of our estimator enables various extensions such as sensitivity analysis with f-divergence, model selection with cross validation and information criterion, and robust policy learning with the sharp lower bound. Furthermore, our estimation method can be reformulated as an empirical risk minimization problem thanks to the strong duality, which enables us to provide strong theoretical guarantees of the proposed estimator using techniques of the M-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23569;&#37327;&#25918;&#30005;&#39044;&#27979;&#26410;&#26469;&#25176;&#21345;&#39532;&#20811;&#30340;&#30772;&#35010;&#65292;&#24182;&#24212;&#29992;&#20102;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#25552;&#21462;&#21644;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;J-TEXT&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#30772;&#35010;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05361</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#25552;&#21462;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#36328;&#25176;&#21345;&#39532;&#20811;&#30772;&#35010;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-tokamak Disruption Prediction based on Physics-Guided Feature Extraction and domain adaptation. (arXiv:2309.05361v2 [physics.plasm-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#23569;&#37327;&#25918;&#30005;&#39044;&#27979;&#26410;&#26469;&#25176;&#21345;&#39532;&#20811;&#30340;&#30772;&#35010;&#65292;&#24182;&#24212;&#29992;&#20102;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#25552;&#21462;&#21644;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;J-TEXT&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#30772;&#35010;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25176;&#21345;&#39532;&#20811;&#20013;&#39640;&#26114;&#30340;&#25968;&#25454;&#33719;&#21462;&#25104;&#26412;&#21644;&#23545;&#30772;&#35010;&#25918;&#30005;&#30340;&#24040;&#22823;&#38656;&#27714;&#32473;&#25968;&#25454;&#39537;&#21160;&#30772;&#35010;&#39044;&#27979;&#27169;&#22411;&#22312;&#30772;&#35010;&#39044;&#27979;&#30740;&#31350;&#20013;&#24102;&#26469;&#20102;&#20869;&#22312;&#30683;&#30462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21482;&#26377;&#23569;&#37327;&#25918;&#30005;&#26469;&#39044;&#27979;&#26410;&#26469;&#25176;&#21345;&#39532;&#20811;&#30340;&#30772;&#35010;&#12290;&#31532;&#19968;&#27493;&#26159;&#21033;&#29992;&#23545;&#21508;&#20010;&#25176;&#21345;&#39532;&#20811;&#30340;&#35786;&#26029;&#20449;&#21495;&#30340;&#29616;&#26377;&#29289;&#29702;&#29702;&#35299;&#26469;&#25552;&#21462;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#65292;&#31216;&#20026;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#25552;&#21462;&#65288;PGFE&#65289;&#12290;&#31532;&#20108;&#27493;&#26159;&#26681;&#25454;&#19968;&#31181;&#31216;&#20026;CORrelation ALignment&#65288;CORAL&#65289;&#30340;&#39046;&#22495;&#36866;&#24212;&#31639;&#27861;&#65292;&#23558;&#26410;&#26469;&#25176;&#21345;&#39532;&#20811;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#30340;&#23569;&#37327;&#25968;&#25454;&#19982;&#29616;&#26377;&#25176;&#21345;&#39532;&#20811;&#65288;&#28304;&#39046;&#22495;&#65289;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#23558;&#39046;&#22495;&#36866;&#24212;&#24212;&#29992;&#20110;&#30772;&#35010;&#39044;&#27979;&#20219;&#21153;&#20013;&#12290;PGFE&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;J-TEXT&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#30772;&#35010;&#39044;&#27979;&#24615;&#33021;&#12290;&#30001;&#20110;&#25552;&#21462;&#20102;&#29289;&#29702;&#24341;&#23548;&#29305;&#24449;&#65292;PGFE&#36824;&#21487;&#20197;&#20943;&#23569;&#25968;&#25454;&#37327;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high acquisition cost and the significant demand for disruptive discharges for data-driven disruption prediction models in future tokamaks pose an inherent contradiction in disruption prediction research. In this paper, we demonstrated a novel approach to predict disruption in a future tokamak using only a few discharges. The first step is to use the existing understanding of physics to extract physics-guided features from the diagnostic signals of each tokamak, called physics-guided feature extraction (PGFE). The second step is to align a few data from the future tokamak (target domain) and a large amount of data from existing tokamak (source domain) based on a domain adaptation algorithm called CORrelation ALignment (CORAL). It is the first attempt at applying domain adaptation in the task of disruption prediction. PGFE has been successfully applied in J-TEXT to predict disruption with excellent performance. PGFE can also reduce the data volume requirements due to extracting the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03564</link><description>&lt;p&gt;
&#35780;&#20272;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Efficacy of Supervised Learning vs Large Language Models for Identifying Cognitive Distortions and Suicidal Risks in Chinese Social Media. (arXiv:2309.03564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#32467;&#26524;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#31867;&#20284;&#24555;&#36895;&#21457;&#23637;&#30340;GPT&#31995;&#21015;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24433;&#21709;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#24515;&#29702;&#23398;&#31561;&#21307;&#23398;&#39046;&#22495;&#23545;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#27987;&#21402;&#20852;&#36259;&#65292;&#20294;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#20855;&#20307;&#25506;&#32034;&#20173;&#28982;&#24456;&#23569;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#29992;&#25143;&#36234;&#26469;&#36234;&#22810;&#22320;&#34920;&#36798;&#20010;&#20154;&#24773;&#24863;&#65307;&#22312;&#29305;&#23450;&#30340;&#20027;&#39064;&#19979;&#65292;&#36825;&#20123;&#24773;&#24863;&#36890;&#24120;&#34920;&#29616;&#20026;&#28040;&#26497;&#24773;&#32490;&#65292;&#26377;&#26102;&#20250;&#21319;&#32423;&#20026;&#33258;&#26432;&#20542;&#21521;&#12290;&#21450;&#26102;&#36776;&#35782;&#36825;&#26679;&#30340;&#35748;&#30693;&#20559;&#24046;&#21644;&#33258;&#26432;&#39118;&#38505;&#23545;&#26377;&#25928;&#24178;&#39044;&#21644;&#28508;&#22312;&#36991;&#20813;&#20005;&#37325;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#20013;&#22269;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#36827;&#34892;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#33258;&#26432;&#39118;&#38505;&#21644;&#35748;&#30693;&#20559;&#24046;&#35782;&#21035;&#30340;&#23454;&#39564;&#65292;&#36827;&#20837;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65306;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#24494;&#35843;&#65292;&#32771;&#23519;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, particularly those akin to the rapidly progressing GPT series, are gaining traction for their expansive influence. While there is keen interest in their applicability within medical domains such as psychology, tangible explorations on real-world data remain scant. Concurrently, users on social media platforms are increasingly vocalizing personal sentiments; under specific thematic umbrellas, these sentiments often manifest as negative emotions, sometimes escalating to suicidal inclinations. Timely discernment of such cognitive distortions and suicidal risks is crucial to effectively intervene and potentially avert dire circumstances. Our study ventured into this realm by experimenting on two pivotal tasks: suicidal risk and cognitive distortion identification on Chinese social media platforms. Using supervised learning as a baseline, we examined and contrasted the efficacy of large language models via three distinct strategies: zero-shot, few-shot, and fine-tunin
&lt;/p&gt;</description></item><item><title>YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00071</link><description>&lt;p&gt;
YaRN: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00071
&lt;/p&gt;
&lt;p&gt;
YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;transformer-based&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36229;&#36807;&#23427;&#20204;&#35757;&#32451;&#30340;&#24207;&#21015;&#38271;&#24230;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;YaRN&#65288;Yet another RoPE extensioN method&#65289;&#65292;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#30340;tokens&#25968;&#37327;&#21644;&#35757;&#32451;&#27493;&#39588;&#23569;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#30340;10&#20493;&#21644;2.5&#20493;&#12290;&#20351;&#29992;YaRN&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaMA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#38754;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;YaRN&#20855;&#26377;&#36229;&#36234;&#24494;&#35843;&#25968;&#25454;&#38598;&#26377;&#38480;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/jquesnelle/yarn&#19978;&#21457;&#24067;&#20102;&#20351;&#29992;64k&#21644;128k&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;Fine-tuning&#30340;Llama 2 7B/13B&#30340;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
&lt;/p&gt;</description></item><item><title>&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#32452;&#32455;&#19981;&#22815;&#26377;&#24207;&#21644;&#35780;&#20272;&#21327;&#35758;&#26377;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#35780;&#20272;&#20102;&#35768;&#22810;&#26368;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#20102;&#38024;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#30340;&#38382;&#39064;&#21450;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13068</link><description>&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;: &#28843;&#37239;&#31639;&#27861;&#21644;&#26377;&#32570;&#38519;&#30340;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13068
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#32452;&#32455;&#19981;&#22815;&#26377;&#24207;&#21644;&#35780;&#20272;&#21327;&#35758;&#26377;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#35780;&#20272;&#20102;&#35768;&#22810;&#26368;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#20102;&#38024;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#30340;&#38382;&#39064;&#21450;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MVTS&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#22823;&#37327;&#30740;&#31350;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#25991;&#29486;&#30340;&#20180;&#32454;&#30740;&#31350;&#35753;&#25105;&#20204;&#24847;&#35782;&#21040;&#65306;1&#65289;&#35813;&#39046;&#22495;&#30340;&#31038;&#21306;&#27963;&#36291;&#65292;&#20294;&#24182;&#19981;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#37027;&#26679;&#32452;&#32455;&#26377;&#24207;&#65307;2&#65289;&#22823;&#22810;&#25968;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#19981;&#21512;&#36866;&#25110;&#23384;&#22312;&#26126;&#26174;&#32570;&#38519;&#30340;&#35780;&#20272;&#21327;&#35758;&#36827;&#34892;&#35780;&#20272;&#65292;&#32570;&#20047;&#31185;&#23398;&#22522;&#30784;&#12290;&#20854;&#20013;&#19968;&#20010;&#38750;&#24120;&#27969;&#34892;&#30340;&#21327;&#35758;&#65292;&#21363;&#25152;&#35859;&#30340; \pa &#21327;&#35758;&#65292;&#26159;&#22914;&#27492;&#26377;&#32570;&#38519;&#65292;&#20197;&#33267;&#20110;&#38543;&#26426;&#29468;&#27979;&#21487;&#20197;&#26174;&#31034;&#31995;&#32479;&#22320;&#20248;&#20110;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#30340;\emph{&#25152;&#26377;}&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26356;&#20581;&#22766;&#30340;&#21327;&#35758;&#23545;&#35768;&#22810;&#26368;&#36817;&#30340;&#31639;&#27861;&#36827;&#34892;&#22238;&#39038;&#21644;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#22312;MVTS&#24322;&#24120;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#19968;&#20010;&#26412;&#26469;&#24456;&#22909;&#30340;&#21327;&#35758;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#20197;&#21450;&#22914;&#20309;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#34920;&#36798;&#20102;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series (MVTS) anomaly detection is a long-standing and challenging research topic that has attracted tremendous research effort from both industry and academia recently. However, a careful study of the literature makes us realize that 1) the community is active but not as organized as other sibling machine learning communities such as Computer Vision (CV) and Natural Language Processing (NLP), and 2) most proposed solutions are evaluated using either inappropriate or highly flawed protocols, with an apparent lack of scientific foundation. So flawed is one very popular protocol, the so-called \pa protocol, that a random guess can be shown to systematically outperform \emph{all} algorithms developed so far. In this paper, we review and evaluate many recent algorithms using more robust protocols and discuss how a normally good protocol may have weaknesses in the context of MVTS anomaly detection and how to mitigate them. We also share our concerns about benchmark dataset
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALGAN&#30340;&#26032;&#22411;GAN&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#20854;&#20182;GAN&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06663</link><description>&lt;p&gt;
ALGAN&#65306;&#20855;&#26377;&#35843;&#25972;&#30340;LSTM GAN&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ALGAN: Time Series Anomaly Detection with Adjusted-LSTM GAN. (arXiv:2308.06663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALGAN&#30340;&#26032;&#22411;GAN&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23545;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#20854;&#20182;GAN&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#21508;&#20010;&#39046;&#22495;&#65288;&#22914;&#21046;&#36896;&#19994;&#65292;&#21307;&#23398;&#25104;&#20687;&#21644;&#32593;&#32476;&#23433;&#20840;&#65289;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#20559;&#31163;&#27491;&#24120;&#34892;&#20026;&#30340;&#28857;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;GANs&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65288;&#21363;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#65289;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#27169;&#22411;&#65292;&#21517;&#20026;Adjusted-LSTM GAN&#65288;ALGAN&#65289;&#65292;&#23427;&#35843;&#25972;LSTM&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#20197;&#25552;&#39640;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#65292;&#32780;&#19988;&#26159;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#36827;&#34892;&#30340;&#12290;&#25105;&#20204;&#22312;46&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#30340;&#22823;&#22411;&#22810;&#21464;&#37327;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;ALGAN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ALGAN&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#12289;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21644;&#20854;&#20182;&#22522;&#20110;GAN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in time series data, to identify points that deviate from normal behaviour, is a common problem in various domains such as manufacturing, medical imaging, and cybersecurity. Recently, Generative Adversarial Networks (GANs) are shown to be effective in detecting anomalies in time series data. The neural network architecture of GANs (i.e. Generator and Discriminator) can significantly improve anomaly detection accuracy. In this paper, we propose a new GAN model, named Adjusted-LSTM GAN (ALGAN), which adjusts the output of an LSTM network for improved anomaly detection in both univariate and multivariate time series data in an unsupervised setting. We evaluate the performance of ALGAN on 46 real-world univariate time series datasets and a large multivariate dataset that spans multiple domains. Our experiments demonstrate that ALGAN outperforms traditional, neural network-based, and other GAN-based methods for anomaly detection in time series data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#32479;&#35745;&#23398;&#20013;&#30340;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#25512;&#24191;&#21040;&#22312;&#32447;&#20915;&#31574;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.13916</link><description>&lt;p&gt;
&#22312;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online learning in bandits with predicted context. (arXiv:2307.13916v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#39044;&#27979;&#19978;&#19979;&#25991;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#32479;&#35745;&#23398;&#20013;&#30340;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#25512;&#24191;&#21040;&#22312;&#32447;&#20915;&#31574;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#27599;&#20010;&#26102;&#21051;&#65292;&#20195;&#29702;&#21482;&#33021;&#35775;&#38382;&#21040;&#19978;&#19979;&#25991;&#30340;&#19968;&#20010;&#24102;&#22122;&#22768;&#30340;&#29256;&#26412;&#20197;&#21450;&#35823;&#24046;&#26041;&#24046;&#65288;&#25110;&#32773;&#36825;&#20010;&#26041;&#24046;&#30340;&#19968;&#20010;&#20272;&#35745;&#65289;&#12290;&#36825;&#19968;&#35774;&#32622;&#21463;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#20915;&#31574;&#30340;&#30495;&#23454;&#19978;&#19979;&#25991;&#26159;&#19981;&#21487;&#35266;&#27979;&#30340;&#65292;&#32780;&#21482;&#26377;&#19968;&#20010;&#30001;&#21487;&#33021;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#20986;&#30340;&#19978;&#19979;&#25991;&#12290;&#24403;&#19978;&#19979;&#25991;&#35823;&#24046;&#26159;&#38750;&#34928;&#20943;&#30340;&#26102;&#20505;&#65292;&#32463;&#20856;&#30340;bandit&#31639;&#27861;&#26080;&#27861;&#36798;&#21040;&#27425;&#32447;&#24615;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36825;&#19968;&#35774;&#32622;&#19979;&#65292;&#31532;&#19968;&#20010;&#20855;&#26377;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#24182;&#19982;&#36866;&#24403;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20851;&#38190;&#30340;&#24605;&#24819;&#26159;&#23558;&#32463;&#20856;&#32479;&#35745;&#23398;&#20013;&#30340;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#25512;&#24191;&#21040;&#22312;&#32447;&#20915;&#31574;&#35774;&#32622;&#20013;&#65292;&#36825;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#22240;&#20026;&#31574;&#30053;&#20381;&#36182;&#20110;&#26377;&#22122;&#22768;&#30340;&#19978;&#19979;&#25991;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-diminishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret compared to the appropriate benchmark. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12169</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#20026;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20219;&#24847;&#21040;&#20219;&#24847;&#32593;&#32476;&#30340;&#20256;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21576;&#29616;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#22312;&#20854;&#20013;&#65292;&#21482;&#26377;&#23567;&#32452;&#30340;GPU&#38656;&#35201;&#39640;&#24102;&#23485;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;GPU&#23567;&#32452;&#20043;&#38388;&#65292;&#36890;&#20449;&#38750;&#24120;&#24494;&#19981;&#36275;&#36947;&#12289;&#31232;&#30095;&#19988;&#22343;&#21248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#32039;&#23494;&#21305;&#37197;LLMs&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#38598;&#32676;&#20998;&#21106;&#20026;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#20219;&#24847;&#21040;&#20219;&#24847;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;HB&#22495;&#12290;&#22312;HB&#22495;&#20043;&#38388;&#65292;&#32593;&#32476;&#21482;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32593;&#32476;&#36830;&#25509;&#31216;&#20026;&#8220;&#20165;&#36712;&#36947;&#36830;&#25509;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;Clos&#32593;&#32476;&#21487;&#20197;&#23558;&#32593;&#32476;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;LLM&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;WorM&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#30340;WM&#20219;&#21153;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;AI WM&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#20986;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;</title><link>http://arxiv.org/abs/2307.10768</link><description>&lt;p&gt;
&#35299;&#30721;&#35868;&#22242;&#65306;&#22312;&#24037;&#20316;&#35760;&#24518;&#30340;&#22810;&#20010;&#26041;&#38754;&#19978;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory. (arXiv:2307.10768v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;WorM&#65289;&#65292;&#36890;&#36807;&#35780;&#20272;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#30340;WM&#20219;&#21153;&#26469;&#24320;&#21457;&#21644;&#35780;&#20272;AI WM&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#20986;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#22914;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#65288;WM&#65289;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#23427;&#20419;&#36827;&#20102;&#20449;&#24687;&#30340;&#20020;&#26102;&#23384;&#20648;&#12289;&#25972;&#21512;&#12289;&#25805;&#20316;&#21644;&#26816;&#32034;&#65292;&#22312;&#25512;&#29702;&#21644;&#20915;&#31574;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25429;&#25417;&#24037;&#20316;&#35760;&#24518;&#22810;&#26041;&#38754;&#29305;&#24449;&#30340;&#21487;&#38752;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#20110;&#26377;&#25928;&#22320;&#24320;&#21457;&#21644;&#35780;&#20272;AI&#24037;&#20316;&#35760;&#24518;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24037;&#20316;&#35760;&#24518;&#65288;WorM&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#36825;&#20010;&#30446;&#30340;&#12290;WorM&#21253;&#25324;10&#20010;&#20219;&#21153;&#21644;&#24635;&#20849;100&#19975;&#27425;&#35797;&#39564;&#65292;&#35780;&#20272;&#20102;WM&#30340;4&#20010;&#21151;&#33021;&#12289;3&#20010;&#39046;&#22495;&#21644;11&#20010;&#34892;&#20026;&#21644;&#31070;&#32463;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#19978;&#20849;&#21516;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#26368;&#20808;&#36827;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20154;&#31867;&#34892;&#20026;&#22522;&#20934;&#20316;&#20026;&#23545;&#27604;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#27169;&#22411;&#27169;&#25311;&#20102;&#33041;&#20013;&#24037;&#20316;&#35760;&#24518;&#30340;&#19968;&#20123;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#20248;&#21183;&#25928;&#24212;&#21644;&#26368;&#26032;&#24615;&#25928;&#24212;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#21644;&#21151;&#33021;&#24615;&#30340;&#24037;&#20316;&#35760;&#24518;&#30340;&#31070;&#32463;&#32676;&#38598;&#21644;&#30456;&#20851;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#26500;&#24314;&#30340;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03176</link><description>&lt;p&gt;
&#24322;&#26500;&#29305;&#24449;&#23376;&#37319;&#26679;&#30340;Ridge Ensemble&#30340;&#23398;&#20064;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles. (arXiv:2307.03176v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#26500;&#24314;&#30340;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21253;&#35013;&#26159;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#22312;&#38543;&#26426;&#23376;&#26679;&#26412;&#25110;&#29305;&#24449;&#25237;&#24433;&#19978;&#35757;&#32451;&#20272;&#35745;&#22120;&#26469;&#20943;&#23569;&#39044;&#27979;&#26041;&#24046;&#30340;&#25104;&#29087;&#38598;&#25104;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#38598;&#25104;&#36873;&#25321;&#26159;&#21516;&#36136;&#30340;&#65292;&#21363;&#20272;&#35745;&#22120;&#21487;&#29992;&#30340;&#29305;&#24449;&#32500;&#25968;&#22312;&#25972;&#20010;&#38598;&#25104;&#20013;&#26159;&#22343;&#21248;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#30340;&#20272;&#35745;&#22120;&#22522;&#20110;&#21464;&#21160;&#30340;&#29305;&#24449;&#32500;&#25968;&#65292;&#24182;&#30740;&#31350;&#20854;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;&#38598;&#25104;&#65292;&#27599;&#20010;&#39044;&#27979;&#22120;&#20351;&#29992;&#37096;&#20998;&#21487;&#29992;&#29305;&#24449;&#36827;&#34892;&#23725;&#22238;&#24402;&#25311;&#21512;&#12290;&#25105;&#20204;&#20801;&#35768;&#36825;&#20123;&#23376;&#38598;&#20013;&#21253;&#21547;&#30340;&#29305;&#24449;&#25968;&#37327;&#26377;&#25152;&#21464;&#21270;&#12290;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;&#22797;&#21046;&#25216;&#24039;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#30830;&#23450;&#24615;&#32447;&#24615;&#25513;&#27169;&#30340;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#23545;&#20110;&#20855;&#26377;&#21508;&#21521;&#21516;&#24615;&#29305;&#24449;&#22122;&#22768;&#30340;&#31561;&#30456;&#30456;&#20851;&#25968;&#25454;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#23398;&#20064;&#26354;&#32447;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#12290;&#21033;&#29992;&#36825;&#20123;&#25512;&#23548;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38598;&#25104;&#22312;&#19981;&#21516;&#29305;&#24449;&#32500;&#25968;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature bagging is a well-established ensembling method which aims to reduce prediction variance by training estimators in an ensemble on random subsamples or projections of features. Typically, ensembles are chosen to be homogeneous, in the sense the the number of feature dimensions available to an estimator is uniform across the ensemble. Here, we introduce heterogeneous feature ensembling, with estimators built on varying number of feature dimensions, and consider its performance in a linear regression setting. We study an ensemble of linear predictors, each fit using ridge regression on a subset of the available features. We allow the number of features included in these subsets to vary. Using the replica trick from statistical physics, we derive learning curves for ridge ensembles with deterministic linear masks. We obtain explicit expressions for the learning curves in the case of equicorrelated data with an isotropic feature noise. Using the derived expressions, we investigate t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27874;&#27573;&#21453;&#39304;&#30340;&#22312;&#32447;&#20803;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#29992;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#36172;&#21338;&#32447;&#24615;&#20248;&#21270;&#30340;&#20803;&#31639;&#27861;&#12290;&#23545;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#31639;&#27861;&#20351;&#29992;&#20102;Tsallis-&#29109;&#30340;&#27867;&#21270;Exp3&#65292;&#24182;&#19988;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#20250;&#38543;&#30528;&#26368;&#20248;&#35299;&#30340;&#29109;&#30340;&#20943;&#23567;&#32780;&#25913;&#21892;&#12290;&#23545;&#20110;&#36172;&#21338;&#32447;&#24615;&#20248;&#21270;&#65292;&#31639;&#27861;&#20351;&#29992;&#20102;&#33258;&#21327;&#35843;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#21021;&#22987;&#21270;&#21644;&#35843;&#25972;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65292;&#24182;&#19988;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#19982;&#21160;&#20316;&#31354;&#38388;&#30456;&#20851;&#30340;&#24230;&#37327;&#30452;&#25509;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.02295</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#23545;&#25239;&#27874;&#27573;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Adversarial Bandit Algorithms. (arXiv:2307.02295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#27874;&#27573;&#21453;&#39304;&#30340;&#22312;&#32447;&#20803;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#29992;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#36172;&#21338;&#32447;&#24615;&#20248;&#21270;&#30340;&#20803;&#31639;&#27861;&#12290;&#23545;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#65292;&#31639;&#27861;&#20351;&#29992;&#20102;Tsallis-&#29109;&#30340;&#27867;&#21270;Exp3&#65292;&#24182;&#19988;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#20250;&#38543;&#30528;&#26368;&#20248;&#35299;&#30340;&#29109;&#30340;&#20943;&#23567;&#32780;&#25913;&#21892;&#12290;&#23545;&#20110;&#36172;&#21338;&#32447;&#24615;&#20248;&#21270;&#65292;&#31639;&#27861;&#20351;&#29992;&#20102;&#33258;&#21327;&#35843;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#21021;&#22987;&#21270;&#21644;&#35843;&#25972;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65292;&#24182;&#19988;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#19982;&#21160;&#20316;&#31354;&#38388;&#30456;&#20851;&#30340;&#24230;&#37327;&#30452;&#25509;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20855;&#26377;&#27874;&#27573;&#21453;&#39304;&#30340;&#22312;&#32447;&#20803;&#23398;&#20064;&#65292;&#30446;&#26631;&#26159;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#25913;&#21892;&#24615;&#33021;&#65292;&#22914;&#26524;&#23427;&#20204;&#26681;&#25454;&#26576;&#20010;&#33258;&#28982;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#26159;&#30456;&#20284;&#30340;&#12290;&#20316;&#20026;&#38024;&#23545;&#25932;&#23545;&#30340;&#22312;&#32447;&#37096;&#20998;&#20449;&#24687;&#35774;&#32622;&#30340;&#39318;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20803;&#31639;&#27861;&#65292;&#23558;&#22806;&#23618;&#23398;&#20064;&#22120;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#20026;&#20004;&#31181;&#37325;&#35201;&#24773;&#20917;&#35843;&#25972;&#20869;&#37096;&#23398;&#20064;&#22120;&#30340;&#21021;&#22987;&#21270;&#21644;&#20854;&#20182;&#36229;&#21442;&#25968;&#65306;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#21644;&#36172;&#21338;&#32447;&#24615;&#20248;&#21270;&#65288;BLO&#65289;&#12290;&#23545;&#20110;MAB&#65292;&#20803;&#23398;&#20064;&#22120;&#20351;&#29992;Tsallis-&#29109;&#30340;&#27867;&#21270;Exp3&#30340;&#21021;&#22987;&#21270;&#21644;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#22914;&#26524;&#21518;&#35265;&#20043;&#39640;&#23792;&#30340;&#29109;&#23567;&#65292;&#21017;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#25913;&#21892;&#12290;&#23545;&#20110;BLO&#65292;&#25105;&#20204;&#23398;&#20250;&#20102;&#20351;&#29992;&#33258;&#21327;&#35843;&#38556;&#30861;&#27491;&#21017;&#21270;&#22120;&#21021;&#22987;&#21270;&#21644;&#35843;&#25972;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#65288;OMD&#65289;&#65292;&#34920;&#26126;&#20219;&#21153;&#24179;&#22343;&#36951;&#25022;&#19982;&#20854;&#24341;&#36215;&#30340;&#21160;&#20316;&#31354;&#38388;&#30456;&#20851;&#30340;&#24230;&#37327;&#30452;&#25509;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20445;&#35777;&#22522;&#20110;&#35777;&#26126;&#26080;&#27491;&#35268;&#21270;&#36319;&#38543;&#32773;&#19982;&#20004;&#20010;&#8230;
&lt;/p&gt;
&lt;p&gt;
We study online meta-learning with bandit feedback, with the goal of improving performance across multiple tasks if they are similar according to some natural similarity measure. As the first to target the adversarial online-within-online partial-information setting, we design meta-algorithms that combine outer learners to simultaneously tune the initialization and other hyperparameters of an inner learner for two important cases: multi-armed bandits (MAB) and bandit linear optimization (BLO). For MAB, the meta-learners initialize and set hyperparameters of the Tsallis-entropy generalization of Exp3, with the task-averaged regret improving if the entropy of the optima-in-hindsight is small. For BLO, we learn to initialize and tune online mirror descent (OMD) with self-concordant barrier regularizers, showing that task-averaged regret varies directly with an action space-dependent measure they induce. Our guarantees rely on proving that unregularized follow-the-leader combined with two 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#23398;&#20064;&#36523;&#20221;&#25928;&#24212;&#30340;&#32972;&#26223;&#19979;&#65292;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#23646;&#24615;&#21644;&#22522;&#26412;&#38480;&#21046;&#26041;&#38754;&#30340;&#26032;&#24615;&#36136;&#65292;&#20197;&#21450;&#22312;&#20004;&#20010;&#23383;&#27597;&#30340;&#21333;&#35789;&#26696;&#20363;&#20013;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.00134</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36523;&#20221;&#25928;&#24212;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Generalization Limits of Graph Neural Networks in Identity Effects Learning. (arXiv:2307.00134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#23398;&#20064;&#36523;&#20221;&#25928;&#24212;&#30340;&#32972;&#26223;&#19979;&#65292;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#23646;&#24615;&#21644;&#22522;&#26412;&#38480;&#21046;&#26041;&#38754;&#30340;&#26032;&#24615;&#36136;&#65292;&#20197;&#21450;&#22312;&#20004;&#20010;&#23383;&#27597;&#30340;&#21333;&#35789;&#26696;&#20363;&#20013;&#30340;&#20855;&#20307;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#22270;&#39046;&#22495;&#30340;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#24037;&#20855;&#12290;&#23427;&#20204;&#36890;&#24120;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#19982;Weisfeiler-Lehman(WL)&#22270;&#21516;&#26500;&#27979;&#35797;&#32039;&#23494;&#30456;&#36830;&#30340;&#30452;&#35266;&#34920;&#36848;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#20174;&#34920;&#36798;&#33021;&#21147;&#19978;&#35762;&#65292;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#19982;WL&#27979;&#35797;&#31561;&#20215;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#23398;&#20064;&#25152;&#35859;&#30340;&#36523;&#20221;&#25928;&#24212;&#65288;&#21363;&#30830;&#23450;&#19968;&#20010;&#23545;&#35937;&#26159;&#21542;&#30001;&#20004;&#20010;&#30456;&#21516;&#30340;&#32452;&#20214;&#32452;&#25104;&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#24314;&#31435;&#20102;GNN&#22312;&#27867;&#21270;&#23646;&#24615;&#21644;&#22522;&#26412;&#38480;&#21046;&#26041;&#38754;&#30340;&#26032;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#20986;&#20110;&#29702;&#35299;GNN&#22312;&#25191;&#34892;&#31616;&#21333;&#35748;&#30693;&#20219;&#21153;&#26102;&#30340;&#33021;&#21147;&#30340;&#38656;&#27714;&#65292;&#21487;&#33021;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#21270;&#23398;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#65288;i&#65289;&#20004;&#20010;&#23383;&#27597;&#30340;&#21333;&#35789;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;GNN&#22312;&#21033;&#29992;&#27491;&#20132;&#26102;&#26080;&#27861;&#23545;&#26410;&#35265;&#23383;&#27597;&#36827;&#34892;&#27867;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a powerful tool for data-driven learning on various graph domains. They are usually based on a message-passing mechanism and have gained increasing popularity for their intuitive formulation, which is closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism to which they have been proven equivalent in terms of expressive power. In this work, we establish new generalization properties and fundamental limits of GNNs in the context of learning so-called identity effects, i.e., the task of determining whether an object is composed of two identical components or not. Our study is motivated by the need to understand the capabilities of GNNs when performing simple cognitive tasks, with potential applications in computational linguistics and chemistry. We analyze two case studies: (i) two-letters words, for which we show that GNNs trained via stochastic gradient descent are unable to generalize to unseen letters when utilizing orthogo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16838</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#65292;&#36890;&#36807;&#31561;&#20215;&#30340;&#30446;&#26631;&#20989;&#25968;&#24418;&#24335;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#65292;&#36824;&#33021;&#22815;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#65292;&#21363;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;KGF&#21644;KRR&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23558;KRR&#27867;&#21270;&#65292;&#20351;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#38750;&#32447;&#24615;&#25512;&#24191;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KRR&#30446;&#26631;&#20989;&#25968;&#30340;&#31561;&#20215;&#24418;&#24335;&#65292;&#20026;&#20351;&#29992;&#20854;&#20182;&#24809;&#32602;&#26041;&#27861;&#21644;&#20174;&#26799;&#24230;&#19979;&#38477;&#30340;&#35282;&#24230;&#30740;&#31350;&#26680;&#23725;&#22238;&#24402;&#25171;&#24320;&#20102;&#21487;&#33021;&#12290;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#38381;&#21512;&#35299;&#8212;&#8212;&#26680;&#26799;&#24230;&#27969;&#65288;KGF&#65289;&#65292;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#30340;&#27491;&#21017;&#21270;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#22312;KGF&#21644;KRR&#20043;&#38388;&#29702;&#35770;&#19978;&#30028;&#23450;&#24046;&#24322;&#12290;&#25105;&#20204;&#29992;$\ell_1$&#21644;$\ell_\infty$&#24809;&#32602;&#26041;&#27861;&#23558;KRR&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;KGF&#21644;KRR&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;&#36825;&#20123;&#24809;&#32602;&#26041;&#27861;&#24471;&#21040;&#30340;&#35299;&#19982;&#20351;&#29992;&#21069;&#21521;&#20998;&#27493;&#22238;&#24402;&#65288;&#20063;&#31216;&#20026;&#22352;&#26631;&#19979;&#38477;&#65289;&#21644;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#25552;&#21069;&#20572;&#27490;&#24471;&#21040;&#30340;&#35299;&#38750;&#24120;&#30456;&#20284;&#12290;&#22240;&#27492;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#37325;&#30340;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15749</link><description>&lt;p&gt;
&#20309;&#21435;&#20309;&#20174;&#65306;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#25968;&#23383;&#30828;&#20214;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15749
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#22312;&#28085;&#30422;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#31454;&#20105;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#29983;&#29289;&#33041;&#30340;&#21151;&#32791;&#25928;&#29575;&#36229;&#36807;&#20219;&#20309;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65307;&#22240;&#27492;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#35797;&#22270;&#27169;&#20223;&#33041;&#37096;&#25805;&#20316;&#65292;&#20363;&#22914;&#22522;&#20110;&#33033;&#20914;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;DL&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#33041;&#37096;&#26377;&#35832;&#22914;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#12289;&#23494;&#38598;&#30340;&#31070;&#32463;&#20803;&#36830;&#25509;&#21644;&#35745;&#31639;&#19982;&#23384;&#20648;&#30340;&#20849;&#21516;&#20301;&#32622;&#31561;&#20248;&#21183;&#65292;&#20294;&#21487;&#29992;&#30340;&#29983;&#29289;&#22522;&#24213;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#29289;&#22823;&#33041;&#30340;&#36827;&#21270;&#12290;&#30005;&#23376;&#30828;&#20214;&#27809;&#26377;&#30456;&#21516;&#30340;&#32422;&#26463;&#65307;&#22240;&#27492;&#65292;&#34429;&#28982;&#24314;&#27169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#33021;&#25581;&#31034;&#20102;&#19968;&#20010;&#35868;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#37325;&#21551;&#8221;&#30340;&#26032;&#22411;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#31163;&#25955;&#21270;&#35823;&#24046;&#21644;&#25910;&#32553;&#65292;&#21487;&#20197;&#20248;&#21270;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#37319;&#26679;&#36895;&#24230;&#21644;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.14878</link><description>&lt;p&gt;
&#37325;&#21551;&#37319;&#26679;&#20197;&#25552;&#39640;&#29983;&#25104;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Restart Sampling for Improving Generative Processes. (arXiv:2306.14878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#37325;&#21551;&#8221;&#30340;&#26032;&#22411;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#31163;&#25955;&#21270;&#35823;&#24046;&#21644;&#25910;&#32553;&#65292;&#21487;&#20197;&#20248;&#21270;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#37319;&#26679;&#36895;&#24230;&#21644;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#36807;&#31243;&#20013;&#35299;&#20915;&#24494;&#20998;&#26041;&#31243;&#30340;&#36807;&#31243;&#65292;&#22914;&#25193;&#25955;&#27169;&#22411;&#65292;&#38656;&#35201;&#24179;&#34913;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#22522;&#20110;ODE&#30340;&#37319;&#26679;&#22120;&#36895;&#24230;&#24555;&#20294;&#24615;&#33021;&#24179;&#31283;&#65292;&#32780;&#22522;&#20110;SDE&#30340;&#37319;&#26679;&#22120;&#25552;&#20379;&#26356;&#39640;&#30340;&#26679;&#26412;&#36136;&#37327;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#37319;&#26679;&#26102;&#38388;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#24046;&#24322;&#24402;&#22240;&#20110;&#37319;&#26679;&#35823;&#24046;&#65306;ODE&#37319;&#26679;&#22120;&#28041;&#21450;&#26356;&#23567;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#65292;&#32780;SDE&#30340;&#38543;&#26426;&#24615;&#20250;&#20351;&#32047;&#31215;&#35823;&#24046;&#32553;&#23567;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37325;&#21551;&#30340;&#26032;&#22411;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#31163;&#25955;&#21270;&#35823;&#24046;&#21644;&#25910;&#32553;&#12290;&#35813;&#37319;&#26679;&#26041;&#27861;&#22312;&#39069;&#22806;&#21069;&#21521;&#27493;&#39588;&#20013;&#20132;&#26367;&#28155;&#21152;&#22823;&#37327;&#22122;&#22768;&#21644;&#20005;&#26684;&#36981;&#24490;&#21518;&#21521;ODE&#12290;&#32463;&#39564;&#35777;&#65292;&#37325;&#21551;&#37319;&#26679;&#22120;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;SDE&#21644;ODE&#37319;&#26679;&#22120;&#12290;&#22312;CIFAR-10/ImageNet $64 \times 64$&#19978;&#65292;&#37325;&#21551;&#19981;&#20165;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20339;SDE&#32467;&#26524;&#65292;&#32780;&#19988;&#21152;&#24555;&#20102;&#37319;&#26679;&#36895;&#24230;&#65292;&#20998;&#21035;&#20026;10&#20493;/2&#20493;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#36827;&#34892;&#22270;&#20687;&#29983;&#25104;&#26102;&#36824;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative processes that involve solving differential equations, such as diffusion models, frequently necessitate balancing speed and quality. ODE-based samplers are fast but plateau in performance while SDE-based samplers deliver higher sample quality at the cost of increased sampling time. We attribute this difference to sampling errors: ODE-samplers involve smaller discretization errors while stochasticity in SDE contracts accumulated errors. Based on these findings, we propose a novel sampling algorithm called Restart in order to better balance discretization errors and contraction. The sampling method alternates between adding substantial noise in additional forward steps and strictly following a backward ODE. Empirically, Restart sampler surpasses previous SDE and ODE samplers in both speed and accuracy. Restart not only outperforms the previous best SDE results, but also accelerates the sampling speed by 10-fold / 2-fold on CIFAR-10 / ImageNet $64 \times 64$. In addition, it at
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26368;&#36817;&#30340;&#21464;&#38761;&#28857;&#26159;&#36880;&#27493;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#35268;&#33539;&#27169;&#22411;&#30340;&#20013;&#38388;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2306.13411</link><description>&lt;p&gt;
&#27809;&#26377;&#20013;&#38388;&#30417;&#30563;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning Without Intermediate Supervision. (arXiv:2306.13411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13411
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26368;&#36817;&#30340;&#21464;&#38761;&#28857;&#26159;&#36880;&#27493;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#35268;&#33539;&#27169;&#22411;&#30340;&#20013;&#38388;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#20391;&#37325;&#20110;&#26500;&#24314;&#33021;&#22815;&#27169;&#20223;&#32463;&#20856;&#31639;&#27861;&#65288;&#22914;&#25490;&#24207;&#12289;&#26368;&#30701;&#36335;&#24452;&#31561;&#65289;&#25191;&#34892;&#30340;&#27169;&#22411;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#23398;&#20064;&#33021;&#22815;&#25512;&#24191;&#21040;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#19988;&#36755;&#20837;&#35268;&#27169;&#26174;&#33879;&#26356;&#22823;&#30340;&#31639;&#27861;&#12290;&#26368;&#36817;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36880;&#27493;&#23398;&#20064;&#31639;&#27861;&#20855;&#26377;&#20248;&#21183;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#35775;&#38382;&#21407;&#22987;&#31639;&#27861;&#30340;&#25152;&#26377;&#20013;&#38388;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19981;&#20351;&#29992;&#20013;&#38388;&#30417;&#30563;&#19987;&#27880;&#20110;&#20165;&#20174;&#36755;&#20837;&#36755;&#20986;&#23545;&#23398;&#20064;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#32467;&#26500;&#25913;&#36827;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#65292;&#21487;&#20197;&#35268;&#33539;&#27169;&#22411;&#30340;&#20013;&#38388;&#35745;&#31639;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#31639;&#27861;&#36712;&#36857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;CLRS&#31639;&#27861;&#30340;&#20219;&#21153;&#19978;&#19982;&#20854;&#36712;&#36857;&#30417;&#30563;&#23545;&#24212;&#29289;&#30456;&#24403;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Algorithmic Reasoning is an emerging area of machine learning focusing on building models which can imitate the execution of classic algorithms, such as sorting, shortest paths, etc. One of the main challenges is to learn algorithms that are able to generalize to out-of-distribution data, in particular with significantly larger input sizes. Recent work on this problem has demonstrated the advantages of learning algorithms step-by-step, giving models access to all intermediate steps of the original algorithm. In this work, we instead focus on learning neural algorithmic reasoning only from the input-output pairs without appealing to the intermediate supervision. We propose simple but effective architectural improvements and also build a self-supervised objective that can regularise intermediate computations of the model without access to the algorithm trajectory. We demonstrate that our approach is competitive to its trajectory-supervised counterpart on tasks from the CLRS Algori
&lt;/p&gt;</description></item><item><title>&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#25688;&#35201;&#26088;&#22312;&#20026;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#30495;&#23454;&#12289;&#22810;&#26679;&#21644;&#22797;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#21512;&#25104;&#25968;&#25454;&#30340;&#20559;&#24046;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13216</link><description>&lt;p&gt;
&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#29992;&#20110;&#25968;&#25454;&#38544;&#31169;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Diverse Community Data for Benchmarking Data Privacy Algorithms. (arXiv:2306.13216v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13216
&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#25688;&#35201;&#26088;&#22312;&#20026;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#30495;&#23454;&#12289;&#22810;&#26679;&#21644;&#22797;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#21512;&#25104;&#25968;&#25454;&#30340;&#20559;&#24046;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#26159;&#32654;&#22269;&#22269;&#23478;&#26631;&#20934;&#21644;&#25216;&#26415;&#30740;&#31350;&#25152;&#65288;NIST&#65289;&#35745;&#21010;&#30340;&#26680;&#24515;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#34920;&#26684;&#25968;&#25454;&#21435;&#35782;&#21035;&#25216;&#26415;&#65288;&#22914;&#21512;&#25104;&#25968;&#25454;&#65289;&#30340;&#29702;&#35299;&#12290;&#21512;&#25104;&#25968;&#25454;&#26159;&#27665;&#20027;&#21270;&#22823;&#25968;&#25454;&#21033;&#30410;&#30340;&#19968;&#39033;&#38596;&#24515;&#21187;&#21187;&#30340;&#23581;&#35797;&#65307;&#23427;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#37325;&#26032;&#21019;&#24314;&#25935;&#24863;&#20010;&#20154;&#25968;&#25454;&#65292;&#20197;&#20415;&#20844;&#24320;&#21457;&#24067;&#12290;&#28982;&#32780;&#65292;&#23427;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#20559;&#24046;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#29978;&#33267;&#21487;&#33021;&#25918;&#22823;&#36825;&#20123;&#38382;&#39064;&#12290;&#24403;&#21435;&#35782;&#21035;&#25968;&#25454;&#20998;&#24067;&#24341;&#20837;&#20559;&#24046;&#25110;&#24037;&#20214;&#65292;&#25110;&#27844;&#28431;&#25935;&#24863;&#20449;&#24687;&#26102;&#65292;&#23427;&#20204;&#20250;&#23558;&#36825;&#20123;&#38382;&#39064;&#20256;&#25773;&#21040;&#19979;&#28216;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#35843;&#26597;&#26465;&#20214;&#65288;&#22914;&#22810;&#26679;&#23376;&#32676;&#12289;&#24322;&#36136;&#38750;&#26377;&#24207;&#25968;&#25454;&#31354;&#38388;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65289;&#23545;&#21512;&#25104;&#25968;&#25454;&#31639;&#27861;&#25552;&#20986;&#20102;&#20855;&#20307;&#25361;&#25112;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#20419;&#20351;&#38656;&#35201;&#30495;&#23454;&#12289;&#22810;&#26679;&#21644;&#22797;&#26434;&#30340;&#22522;&#20934;&#25968;&#25454;&#26469;&#25903;&#25345;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#32780;&#22810;&#26679;&#31038;&#21306;&#25968;&#25454;&#25688;&#35201;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Diverse Communities Data Excerpts are the core of a National Institute of Standards and Technology (NIST) program to strengthen understanding of tabular data deidentification technologies such as synthetic data. Synthetic data is an ambitious attempt to democratize the benefits of big data; it uses generative models to recreate sensitive personal data with new records for public release. However, it is vulnerable to the same bias and privacy issues that impact other machine learning applications, and can even amplify those issues. When deidentified data distributions introduce bias or artifacts, or leak sensitive information, they propagate these problems to downstream applications. Furthermore, real-world survey conditions such as diverse subpopulations, heterogeneous non-ordinal data spaces, and complex dependencies between features pose specific challenges for synthetic data algorithms. These observations motivate the need for real, diverse, and complex benchmark data to support
&lt;/p&gt;</description></item><item><title>OpenGSL&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;GSL&#39046;&#22495;&#20013;&#30001;&#20110;&#23454;&#39564;&#21327;&#35758;&#19981;&#19968;&#33268;&#32780;&#23548;&#33268;&#30340;&#36827;&#23637;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10280</link><description>&lt;p&gt;
OpenGSL: &#19968;&#39033;&#38024;&#23545;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
OpenGSL: A Comprehensive Benchmark for Graph Structure Learning. (arXiv:2306.10280v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10280
&lt;/p&gt;
&lt;p&gt;
OpenGSL&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;GSL&#39046;&#22495;&#20013;&#30001;&#20110;&#23454;&#39564;&#21327;&#35758;&#19981;&#19968;&#33268;&#32780;&#23548;&#33268;&#30340;&#36827;&#23637;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#26377;&#25928;&#22320;&#25972;&#21512;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#33410;&#28857;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#30340;&#22797;&#26434;&#21644;&#20381;&#36182;&#24418;&#25104;&#36807;&#31243;&#23548;&#33268;&#30340;&#33410;&#28857;&#36830;&#25509;&#30340;&#22266;&#26377;&#27425;&#20248;&#24615;&#36136;&#65292;&#22312;&#23545;&#20854;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#23384;&#22312;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#22270;&#32467;&#26500;&#23398;&#20064;(GSL)&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#21270;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;GSL&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#21516;&#26102;&#20248;&#21270;&#22270;&#32467;&#26500;&#21644;&#23545;&#24212;&#30340;GNN&#27169;&#22411;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#35768;&#22810;GSL&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#23454;&#39564;&#21327;&#35758;&#19981;&#19968;&#33268;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#30340;&#21464;&#21270;&#12289;&#25968;&#25454;&#22788;&#29702;&#25216;&#26415;&#21644;&#20998;&#21106;&#31574;&#30053;&#30340;&#24046;&#24322;&#65292;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OpenGSL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;GSL&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;OpenGSL&#25552;&#20379;&#20102;&#19968;&#20010;&#20844;&#24179;&#30340;&#27604;&#36739;&#24179;&#21488;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#19981;&#21516;&#30340;GSL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as the de facto standard for representation learning on graphs, owing to their ability to effectively integrate graph topology and node attributes. However, the inherent suboptimal nature of node connections, resulting from the complex and contingent formation process of graphs, presents significant challenges in modeling them effectively. To tackle this issue, Graph Structure Learning (GSL), a family of data-centric learning approaches, has garnered substantial attention in recent years. The core concept behind GSL is to jointly optimize the graph structure and the corresponding GNN models. Despite the proposal of numerous GSL methods, the progress in this field remains unclear due to inconsistent experimental protocols, including variations in datasets, data processing techniques, and splitting strategies. In this paper, we introduce OpenGSL, the first comprehensive benchmark for GSL, aimed at addressing this gap. OpenGSL enables a fair compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#36830;&#32493;&#22522;&#20934;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#35813;&#20998;&#24067;&#20855;&#26377;&#24050;&#30693;&#30340;&#29109;&#26368;&#20248;&#20256;&#36755;&#21644;Schr&#246;dinger&#26725;&#35299;&#12290;&#36825;&#22635;&#34917;&#20102;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#21442;&#25968;&#36873;&#25321;&#26041;&#38754;&#30340;&#31354;&#30333;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#27979;&#35797;&#20256;&#36755;&#27169;&#22411;&#24615;&#33021;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.10161</link><description>&lt;p&gt;
&#26500;&#24314;Schr&#246;dinger&#30340;&#26725;&#26753;&#65306;&#19968;&#31181;&#36830;&#32493;&#29109;&#26368;&#20248;&#20256;&#36755;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Building the Bridge of Schr\"odinger: A Continuous Entropic Optimal Transport Benchmark. (arXiv:2306.10161v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#36830;&#32493;&#22522;&#20934;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#35813;&#20998;&#24067;&#20855;&#26377;&#24050;&#30693;&#30340;&#29109;&#26368;&#20248;&#20256;&#36755;&#21644;Schr&#246;dinger&#26725;&#35299;&#12290;&#36825;&#22635;&#34917;&#20102;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#21442;&#25968;&#36873;&#25321;&#26041;&#38754;&#30340;&#31354;&#30333;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#27979;&#35797;&#20256;&#36755;&#27169;&#22411;&#24615;&#33021;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#22312;&#24320;&#21457;&#29992;&#20110;Schr&#246;dinger&#26725;&#38382;&#39064;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#19968;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#19982;&#22312;&#23454;&#36341;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#22312;&#29702;&#35770;&#19978;&#26377;&#22522;&#30784;&#30340;&#29109;&#26368;&#20248;&#20256;&#36755;&#30456;&#20114;&#20851;&#32852;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#38750;&#24179;&#20961;&#27979;&#35797;&#65292;&#26080;&#27861;&#35753;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22312;&#35299;&#20915;SB&#25110;&#20854;&#31561;&#25928;&#36830;&#32493;EOT&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#19968;&#23545;&#24050;&#30693;&#22320;&#38754;&#30495;&#20540;OT&#35299;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;OT&#20844;&#24335;&#65292;&#23588;&#20854;&#26159;&#28085;&#30422;&#20102;&#19982;SB&#31561;&#25928;&#30340;EOT&#65288;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#20852;&#36259;&#65289;&#12290;&#36825;&#19968;&#21457;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#39640;&#32500;&#31354;&#38388;&#65288;&#22914;&#22270;&#20687;&#31354;&#38388;&#65289;&#19978;&#21019;&#24314;&#20855;&#26377;&#24050;&#30693;EOT&#21644;SB&#35299;&#30340;&#36830;&#32493;&#22522;&#20934;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last several years, there has been significant progress in developing neural solvers for the Schr\"odinger Bridge (SB) problem and applying them to generative modelling. This new research field is justifiably fruitful as it is interconnected with the practically well-performing diffusion models and theoretically grounded entropic optimal transport (EOT). Still, the area lacks non-trivial tests allowing a researcher to understand how well the methods solve SB or its equivalent continuous EOT problem. We fill this gap and propose a novel way to create pairs of probability distributions for which the ground truth OT solution is known by the construction. Our methodology is generic and works for a wide range of OT formulations, in particular, it covers the EOT which is equivalent to SB (the main interest of our study). This development allows us to create continuous benchmark distributions with the known EOT and SB solutions on high-dimensional spaces such as spaces of images. As 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Caus-Modens&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#65292;&#30456;&#27604;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.09520</link><description>&lt;p&gt;
&#38024;&#23545;&#28508;&#22312;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#32467;&#26524;&#30340;&#26356;&#32039;&#23494;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Tighter Prediction Intervals for Causal Outcomes Under Hidden Confounding. (arXiv:2306.09520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Caus-Modens&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#65292;&#30456;&#27604;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#38544;&#34255;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30830;&#20999;&#20010;&#20307;&#27835;&#30103;&#32467;&#26524;&#30340;&#22240;&#26524;&#25512;&#26029;&#24456;&#23569;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25913;&#36827;&#20102;&#31526;&#21512;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#20135;&#29983;&#32467;&#26524;&#21306;&#38388;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31867;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#65292;&#26377;&#26102;&#20250;&#32473;&#20986;&#26080;&#20449;&#24687;&#37327;&#30340;&#21306;&#38388;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21478;&#31867;&#26041;&#27861;Caus-Modens&#65292;&#29992;&#20110;&#36890;&#36807;&#35843;&#21046;&#38598;&#21512;&#26469;&#25551;&#36848;&#22240;&#26524;&#32467;&#26524;&#21306;&#38388;&#12290;&#21463;&#21040;&#36125;&#21494;&#26031;&#32479;&#35745;&#21644;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#21551;&#21457;&#65292;Caus-Modens&#22312;&#23454;&#36341;&#20013;&#32473;&#20986;&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#21306;&#38388;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#20998;&#31163;&#22522;&#20934;&#27979;&#35797;&#30340;&#24517;&#35201;&#21306;&#38388;&#22823;&#23567;&#26469;&#23454;&#29616;&#36275;&#22815;&#30340;&#35206;&#30422;&#29575;&#12290;&#26368;&#21518;&#19968;&#20010;&#22522;&#20934;&#26159;&#20351;&#29992;&#26410;&#30693;&#20294;&#21487;&#25506;&#26126;&#30340;&#22522;&#30784;&#20107;&#23454;&#24320;&#23637;&#35266;&#23519;&#23454;&#39564;&#30340;GPT-4&#30340;&#26032;&#22411;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Instead, recent work has adapted conformal prediction to produce outcome intervals. Unfortunately this family of methods tends to be overly conservative, sometimes giving uninformative intervals. We introduce an alternative approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. Motivated from Bayesian statistics and ensembled uncertainty quantification, Caus-Modens gives tighter outcome intervals in practice, measured by the necessary interval size to achieve sufficient coverage on three separate benchmarks. The last benchmark is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;5.7&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#29992;&#20110;&#38548;&#31163;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.08754</link><description>&lt;p&gt;
ClimSim&#65306;&#29992;&#20110;&#22312;&#28151;&#21512;&#22810;&#23610;&#24230;&#27668;&#20505;&#27169;&#25311;&#22120;&#20013;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ClimSim: An open large-scale dataset for training high-resolution physics emulators in hybrid multi-scale climate simulators. (arXiv:2306.08754v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08754
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#39640;&#20998;&#36776;&#29575;&#29289;&#29702;&#20223;&#30495;&#22120;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;5.7&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#29992;&#20110;&#38548;&#31163;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#27668;&#20505;&#39044;&#27979;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#32570;&#20047;&#36275;&#22815;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#19968;&#20010;&#21518;&#26524;&#26159;&#23545;&#20851;&#38190;&#36807;&#31243;&#65288;&#22914;&#26292;&#39118;&#38632;&#65289;&#30340;&#39044;&#27979;&#19981;&#20934;&#30830;&#21644;&#19981;&#31934;&#30830;&#12290;&#23558;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30456;&#32467;&#21512;&#30340;&#28151;&#21512;&#27169;&#24335;&#24341;&#20837;&#20102;&#26032;&#19968;&#20195;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#27668;&#20505;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;&#12289;&#30701;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;&#27169;&#25311;&#22996;&#25176;&#32473;ML&#20223;&#30495;&#22120;&#65292;&#21487;&#20197;&#36991;&#20813;&#25705;&#23572;&#23450;&#24459;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#28151;&#21512;&#30340;ML-&#29289;&#29702;&#20223;&#30495;&#26041;&#27861;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30340;&#22788;&#29702;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#22521;&#35757;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#19968;&#30452;&#26080;&#27861;&#35775;&#38382;ML&#19987;&#23478;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; ClimSim&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#20026;&#28151;&#21512;ML-&#29289;&#29702;&#30740;&#31350;&#32780;&#35774;&#35745;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290;&#23427;&#30001;&#27668;&#20505;&#31185;&#23398;&#23478;&#21644;ML&#30740;&#31350;&#20154;&#21592;&#32852;&#21512;&#24320;&#21457;&#30340;&#22810;&#23610;&#24230;&#27668;&#20505;&#27169;&#25311;&#32452;&#25104;&#65292;&#21253;&#25324;57&#20159;&#20010;&#22810;&#21464;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#30690;&#37327;&#23545;&#65292;&#38548;&#31163;&#20102;&#26412;&#22320;&#23884;&#22871;&#30340;&#39640;&#20998;&#36776;&#29575;&#21644;&#39640;&#20445;&#30495;&#24230;&#29289;&#29702;&#23398;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise prediction of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;FLSL&#26041;&#27861;&#65292;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35821;&#20041;&#31867;&#31751;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.06203</link><description>&lt;p&gt;
&#29305;&#24449;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;FLSL
&lt;/p&gt;
&lt;p&gt;
FLSL: Feature-level Self-supervised Learning. (arXiv:2306.06203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;FLSL&#26041;&#27861;&#65292;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35821;&#20041;&#31867;&#31751;&#34920;&#36798;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;SimCLR&#12289;DINO&#12289;VICReg&#12289;MOCOv3&#65289;&#20027;&#35201;&#38024;&#23545;&#23454;&#20363;&#32423;&#21035;&#30340;&#34920;&#31034;&#65292;&#19981;&#36866;&#29992;&#20110;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#65292;&#20363;&#22914;&#23545;&#35937;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#39318;&#27425;&#23637;&#31034;&#20102;Vision Transformers&#65288;ViT&#65289;&#30340;&#22522;&#30784;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#36807;&#31243;&#33021;&#22815;&#33391;&#22909;&#22320;&#19982;&#33258;&#28982;&#22270;&#20687;&#35821;&#20041;&#65288;&#20363;&#22914;&#29289;&#20307;&#21644;&#22330;&#26223;&#65289;&#23545;&#40784;&#12290;&#36890;&#36807;&#37319;&#29992;Transformer&#36827;&#34892;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#32423;&#29305;&#24449;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#29305;&#24449;&#32423;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;FLSL&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FLSL&#38382;&#39064;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#20174;&#22343;&#20540;&#28418;&#31227;&#21644;k-means&#30340;&#35282;&#24230;&#26500;&#24314;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FLSL&#20419;&#36827;&#20102;&#26174;&#33879;&#30340;&#35821;&#20041;&#31867;&#31751;&#34920;&#31034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#36866;&#21512;&#20110;&#20869;&#35270;&#22270;&#21644;&#22806;&#35270;&#22270;&#29305;&#24449;&#32858;&#31867;&#30340;&#23884;&#20837;&#26041;&#26696;&#12290;FLSL&#30340;&#36816;&#29992;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a two-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an embedding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements 
&lt;/p&gt;</description></item><item><title>PoET&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#20309;&#34507;&#30333;&#36136;&#23478;&#26063;&#30340;&#19968;&#31995;&#21015;&#30456;&#20851;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#21487;&#20197;&#29992;&#20316;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#35780;&#20998;&#20219;&#20309;&#20462;&#25913;</title><link>http://arxiv.org/abs/2306.06156</link><description>&lt;p&gt;
PoET: &#19968;&#31181;&#23558;&#34507;&#30333;&#36136;&#23478;&#26063;&#30475;&#20316;&#24207;&#21015;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PoET: A generative model of protein families as sequences-of-sequences. (arXiv:2306.06156v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06156
&lt;/p&gt;
&lt;p&gt;
PoET&#26159;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#20309;&#34507;&#30333;&#36136;&#23478;&#26063;&#30340;&#19968;&#31995;&#21015;&#30456;&#20851;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#21487;&#20197;&#29992;&#20316;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21644;&#35780;&#20998;&#20219;&#20309;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#26159;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#21151;&#33021;&#30340;&#26032;&#34507;&#30333;&#36136;&#30340;&#33258;&#28982;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#35201;&#20040;&#38590;&#20197;&#25351;&#23548;&#20854;&#29983;&#25104;&#29305;&#23450;&#31867;&#22411;&#30340;&#34507;&#30333;&#36136;&#65292;&#35201;&#20040;&#24517;&#39035;&#22312;&#29305;&#23450;&#31867;&#22411;&#30340;&#34507;&#30333;&#36136;&#23478;&#26063;&#30340;&#22823;&#22411;&#22810;&#37325;&#24207;&#21015;&#27604;&#23545;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#20174;&#23478;&#26063;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34507;&#30333;&#36136;&#36827;&#21270;&#21464;&#25442;&#22120;&#65288;PoET&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20840;&#34507;&#30333;&#36136;&#23478;&#26063;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#22411;&#65292;&#23398;&#20064;&#22312;&#25968;&#21315;&#19975;&#20010;&#22825;&#28982;&#34507;&#30333;&#36136;&#24207;&#21015;&#31751;&#20043;&#38388;&#29983;&#25104;&#19968;&#31995;&#21015;&#30456;&#20851;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;PoET&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20219;&#20309;&#24863;&#20852;&#36259;&#30340;&#34507;&#30333;&#36136;&#23478;&#26063;&#26465;&#20214;&#19979;&#29983;&#25104;&#21644;&#35780;&#20998;&#20219;&#24847;&#20462;&#25913;&#65292;&#32780;&#19988;&#21487;&#20197;&#20174;&#30701;&#24207;&#21015;&#38271;&#24230;&#36827;&#34892;&#22806;&#25512;&#65292;&#22312;&#23567;&#23478;&#26063;&#20013;&#20063;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#36825;&#26159;&#36890;&#36807;&#29420;&#29305;&#30340;Transformer&#23618;&#23454;&#29616;&#30340;&#65307;&#25105;&#20204;&#27169;&#25311;&#20102;&#20196;&#29260;s
&lt;/p&gt;
&lt;p&gt;
Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\textbf{P}$r$\textbf{o}$tein $\textbf{E}$volutionary $\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.04723</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#20869;&#37096;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25552;&#39640;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#23646;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#21363;&#32473;&#23450;&#25991;&#26412;&#26679;&#26412;&#23884;&#20837;&#38598;&#21512;&#19979;&#30340;&#27969;&#24418;&#30340;&#20869;&#37096;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#27969;&#30021;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#22312;&#20960;&#20010;&#22522;&#20110;&#23383;&#27597;&#30340;&#35821;&#35328;&#20013;&#32422;&#20026; $9$&#65292;&#32780;&#20013;&#25991;&#32422;&#20026; $7$&#65292;&#32780;&#27599;&#31181;&#35821;&#35328;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#36739;&#20302;&#65292;&#24046;&#32422; $1.5$&#65292;&#24182;&#19988;&#26377;&#26126;&#26174;&#30340;&#32479;&#35745;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#36755;&#36816;&#27169;&#22411;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#28789;&#27963;&#22320;&#23558;&#38160;&#24230;&#24863;&#30693;&#32435;&#20837;&#21040;&#21333;&#20010;&#27169;&#22411;&#12289;&#38598;&#25104;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04178</link><description>&lt;p&gt;
&#20248;&#21270;&#36755;&#36816;&#27169;&#22411;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport Model Distributional Robustness. (arXiv:2306.04178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#36755;&#36816;&#27169;&#22411;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#26694;&#26550;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#28789;&#27963;&#22320;&#23558;&#38160;&#24230;&#24863;&#30693;&#32435;&#20837;&#21040;&#21333;&#20010;&#27169;&#22411;&#12289;&#38598;&#25104;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#24615;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#20854;&#23545;&#25239;&#24615;&#20363;&#23376;&#21644;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;&#24433;&#21709;&#26356;&#23567;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#27169;&#22411;&#31354;&#38388;&#20998;&#24067;&#40065;&#26834;&#24615;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#20013;&#24515;&#27169;&#22411;&#20998;&#24067;&#30340;Wasserstein&#29699;&#20013;&#30340;&#27169;&#22411;&#20998;&#24067;&#65292;&#35813;&#27169;&#22411;&#20998;&#24067;&#26368;&#22823;&#21270;&#20102;&#25439;&#22833;&#12290;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#29702;&#35770;&#65292;&#20801;&#35768;&#25105;&#20204;&#23398;&#20064;&#26368;&#20339;&#30340;&#40065;&#26834;&#20013;&#24515;&#27169;&#22411;&#20998;&#24067;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#25105;&#20204;&#24320;&#21457;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#32771;&#34385;&#29305;&#23450;&#24418;&#24335;&#30340;&#20013;&#24515;&#27169;&#22411;&#20998;&#24067;&#65288;&#22914;&#21333;&#20010;&#27169;&#22411;&#19978;&#30340;Dirac delta&#20998;&#24067;&#65292;&#22810;&#20010;&#27169;&#22411;&#19978;&#30340;&#22343;&#21248;&#20998;&#24067;&#21644;&#19968;&#33324;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65289;&#26469;&#28789;&#27963;&#22320;&#23558;&#38160;&#24230;&#24863;&#30693;&#30340;&#27010;&#24565;&#32435;&#20837;&#21040;&#21333;&#20010;&#27169;&#22411;&#12289;&#38598;&#25104;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional robustness is a promising framework for training deep learning models that are less vulnerable to adversarial examples and data distribution shifts. Previous works have mainly focused on exploiting distributional robustness in data space. In this work, we explore an optimal transport-based distributional robustness framework on model spaces. Specifically, we examine a model distribution in a Wasserstein ball of a given center model distribution that maximizes the loss. We have developed theories that allow us to learn the optimal robust center model distribution. Interestingly, through our developed theories, we can flexibly incorporate the concept of sharpness awareness into training a single model, ensemble models, and Bayesian Neural Networks by considering specific forms of the center model distribution, such as a Dirac delta distribution over a single model, a uniform distribution over several models, and a general Bayesian Neural Network. Furthermore, we demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#25239;&#24178;&#25200;&#32422;&#26463;&#23398;&#20064;&#8221;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26102;&#38656;&#35201;&#28385;&#36275;&#38500;&#20102;&#20934;&#30830;&#24615;&#20197;&#22806;&#30340;&#22810;&#20010;&#35201;&#27714;&#65292;&#24182;&#20197;&#24179;&#34913;&#20174;&#25918;&#23485;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#22686;&#30410;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#25918;&#23485;&#25104;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#26041;&#24335;&#25918;&#26494;&#23398;&#20064;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2306.02426</link><description>&lt;p&gt;
&#25239;&#24178;&#25200;&#32422;&#26463;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resilient Constrained Learning. (arXiv:2306.02426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#25239;&#24178;&#25200;&#32422;&#26463;&#23398;&#20064;&#8221;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26102;&#38656;&#35201;&#28385;&#36275;&#38500;&#20102;&#20934;&#30830;&#24615;&#20197;&#22806;&#30340;&#22810;&#20010;&#35201;&#27714;&#65292;&#24182;&#20197;&#24179;&#34913;&#20174;&#25918;&#23485;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#22686;&#30410;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#25918;&#23485;&#25104;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#26041;&#24335;&#25918;&#26494;&#23398;&#20064;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26102;&#65292;&#38500;&#20102;&#20934;&#30830;&#24615;&#20043;&#22806;&#65292;&#23427;&#20204;&#24517;&#39035;&#28385;&#36275;&#22810;&#20010;&#35201;&#27714;&#65292;&#22914;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#25110;&#23433;&#20840;&#24615;&#12290;&#36825;&#20123;&#35201;&#27714;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#24809;&#32602;&#26469;&#38544;&#24335;&#22320;&#26045;&#21152;&#65292;&#25110;&#32773;&#36890;&#36807;&#22522;&#20110;Lagrangian&#23545;&#20598;&#30340;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;&#26469;&#26174;&#24335;&#22320;&#26045;&#21152;&#12290;&#26080;&#35770;&#21738;&#31181;&#26041;&#24335;&#65292;&#25351;&#23450;&#35201;&#27714;&#37117;&#21463;&#21040;&#22949;&#21327;&#21644;&#26377;&#38480;&#30340;&#26377;&#20851;&#25968;&#25454;&#30340;&#20808;&#21069;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#24120;&#21482;&#33021;&#36890;&#36807;&#23454;&#38469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#26469;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21516;&#26102;&#35299;&#20915;&#23398;&#20064;&#20219;&#21153;&#30340;&#21516;&#26102;&#35843;&#25972;&#35201;&#27714;&#12290;&#20026;&#27492;&#65292;&#23427;&#20197;&#24179;&#34913;&#20174;&#25918;&#23485;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#22686;&#30410;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#25918;&#23485;&#25104;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#26041;&#24335;&#25918;&#26494;&#20102;&#23398;&#20064;&#32422;&#26463;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#31216;&#20026;&#20855;&#26377;&#24377;&#24615;&#30340;&#32422;&#26463;&#23398;&#20064;&#65292;&#36825;&#26159;&#23545;&#29992;&#20110;&#25551;&#36848;&#29983;&#24577;&#31995;&#32479;&#30340;&#26415;&#35821;&#30340;&#19968;&#31181;&#20511;&#37492;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01103</link><description>&lt;p&gt;
&#22312;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#20013;&#23398;&#20064;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization. (arXiv:2306.01103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22270;&#24418;OOD&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#21463;&#38480;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#26080;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#26102;&#32435;&#20837;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#65288;LECI&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#21644;&#29615;&#22659;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#22240;&#26524;&#21644;&#19981;&#21464;&#23376;&#22270;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#32852;&#21512;&#20248;&#21270;&#36825;&#20004;&#20010;&#23646;&#24615;&#65292;&#29992;&#20110;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#23548;&#33268;&#23376;&#22270;&#21457;&#29616;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;LECI&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#23558;LECI&#30830;&#31435;&#20026;&#22270;&#24418;OOD&#27867;&#21270;&#30340;&#23454;&#29992;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#23558;&#25152;&#26377;&#39044;&#27979;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#35813;&#29616;&#35937;&#23545;&#26550;&#26500;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#26377;&#23454;&#38469;&#25351;&#23548;&#24847;&#20041;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#21644;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.00809</link><description>&lt;p&gt;
&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#65306;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#26576;&#20123;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Initial Guessing Bias: How Untrained Networks Favor Some Classes. (arXiv:2306.00809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#23558;&#25152;&#26377;&#39044;&#27979;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#35813;&#29616;&#35937;&#23545;&#26550;&#26500;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#26377;&#23454;&#38469;&#25351;&#23548;&#24847;&#20041;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#21644;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#29366;&#24577;&#22312;&#35843;&#33410;&#21518;&#32493;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21487;&#20197;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#29978;&#33267;&#22312;&#19981;&#23384;&#22312;&#26174;&#24335;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#27169;&#22411;&#23558;&#25152;&#26377;&#39044;&#27979;&#37117;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#23384;&#22312;&#65292;&#31216;&#20026;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#65288;Initial Guessing Bias&#65292;IGB&#65289;&#65292;&#36825;&#21462;&#20915;&#20110;&#26550;&#26500;&#36873;&#25321;&#65292;&#20363;&#22914;&#28608;&#27963;&#20989;&#25968;&#12289;&#26368;&#22823;&#27744;&#21270;&#23618;&#21644;&#32593;&#32476;&#28145;&#24230;&#12290;&#25105;&#20204;&#23545;IGB&#36827;&#34892;&#30340;&#20998;&#26512;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#21487;&#20197;&#25351;&#23548;&#26550;&#26500;&#30340;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#12289;&#33258;&#24179;&#22343;&#30340;&#30772;&#22351;&#12289;&#26576;&#20123;&#22343;&#22330;&#36817;&#20284;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The initial state of neural networks plays a central role in conditioning the subsequent training dynamics. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a neural network can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We show that the presence of this phenomenon, which we call "Initial Guessing Bias" (IGB), depends on architectural choices such as activation functions, max-pooling layers, and network depth. Our analysis of IGB has practical consequences, in that it guides architecture selection and initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging, the validity of some mean-field approximations, and the non-trivial differences arising with depth.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;Precision-Recall&#20998;&#27495;&#26469;&#24179;&#34913;&#29983;&#25104;&#27169;&#22411;&#22270;&#20687;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#29992;&#25143;&#23450;&#20041;&#30340;&#31934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.18910</link><description>&lt;p&gt;
&#36890;&#36807;GANs&#21644;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#30340;Precision-Recall&#20998;&#27495;&#20248;&#21270;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Precision-Recall Divergence Optimization for Generative Modeling with GANs and Normalizing Flows. (arXiv:2305.18910v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;Precision-Recall&#20998;&#27495;&#26469;&#24179;&#34913;&#29983;&#25104;&#27169;&#22411;&#22270;&#20687;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26032;&#35757;&#32451;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#29992;&#25143;&#23450;&#20041;&#30340;&#31934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#65292;&#24179;&#34913;&#22270;&#20687;&#36136;&#37327;&#65288;&#31934;&#30830;&#24615;&#65289;&#21644;&#22810;&#26679;&#24615;&#65288;&#21484;&#22238;&#29575;&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#20110;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22914;Fr\'echet Inception Distance&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#21457;&#23637;&#24341;&#20837;&#20102;&#19968;&#20123;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#25104;&#21151;&#22320;&#25972;&#21512;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#65292;&#23427;&#26126;&#30830;&#22320;&#20248;&#21270;&#20102;&#29992;&#25143;&#23450;&#20041;&#30340;&#31934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#30340;&#26435;&#34913;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23454;&#29616;&#25351;&#23450;&#30340;&#31934;&#30830;&#24615;-&#21484;&#22238;&#29575;&#26435;&#34913;&#31561;&#20215;&#20110;&#26368;&#23567;&#21270;&#19968;&#32452;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;PR-divergences&#8221;&#30340;&#29420;&#29305;&#30340;$f$-divergence&#12290;&#21453;&#20043;&#65292;&#20219;&#20309;$f$-divergence&#37117;&#21487;&#20197;&#20889;&#25104;PR-divergences&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#24182;&#23545;&#24212;&#19968;&#20010;&#21152;&#26435;&#31934;&#30830;&#24615;-&#21484;&#22238;&#29575;&#26435;&#34913;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35797;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving a balance between image quality (precision) and diversity (recall) is a significant challenge in the domain of generative models. Current state-of-the-art models primarily rely on optimizing heuristics, such as the Fr\'echet Inception Distance. While recent developments have introduced principled methods for evaluating precision and recall, they have yet to be successfully integrated into the training of generative models. Our main contribution is a novel training method for generative models, such as Generative Adversarial Networks and Normalizing Flows, which explicitly optimizes a user-defined trade-off between precision and recall. More precisely, we show that achieving a specified precision-recall trade-off corresponds to minimizing a unique $f$-divergence from a family we call the \textit{PR-divergences}. Conversely, any $f$-divergence can be written as a linear combination of PR-divergences and corresponds to a weighted precision-recall trade-off. Through comprehensive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#38750;&#38543;&#26426;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#21160;&#24577;&#25110;&#32773;&#20855;&#26377;&#23545;&#25239;&#24615;&#30340;&#29615;&#22659;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#24178;&#25200;&#20449;&#21495;&#20026;&#20013;&#24515;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#23454;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.17552</link><description>&lt;p&gt;
&#22312;&#32447;&#38750;&#38543;&#26426;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Nonstochastic Model-Free Reinforcement Learning. (arXiv:2305.17552v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#38750;&#38543;&#26426;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#38024;&#23545;&#21160;&#24577;&#25110;&#32773;&#20855;&#26377;&#23545;&#25239;&#24615;&#30340;&#29615;&#22659;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#24178;&#25200;&#20449;&#21495;&#20026;&#20013;&#24515;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#23454;&#29992;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#21487;&#33021;&#26159;&#21160;&#24577;&#25110;&#32773;&#20855;&#26377;&#23545;&#25239;&#24615;&#30340;&#29615;&#22659;&#30340;&#40065;&#26834;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#29366;&#24577;&#30340;&#31574;&#30053;&#24120;&#24120;&#38590;&#20197;&#36866;&#24212;&#36825;&#20123;&#29615;&#22659;&#20013;&#26410;&#24314;&#27169;&#24178;&#25200;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#22522;&#20110;&#32447;&#24615;&#29366;&#24577;&#30340;&#31574;&#30053;&#22312;&#25928;&#29575;&#20248;&#21270;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#22312;&#20687;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#36825;&#26679;&#33391;&#22909;&#30340;&#29615;&#22659;&#20013;&#20063;&#20250;&#20986;&#29616;&#38750;&#20984;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#21463;&#27169;&#22411;&#25511;&#21046;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20197;&#24178;&#25200;&#20449;&#21495;&#20026;&#20013;&#24515;&#30340;&#31574;&#30053;&#31867;&#21035;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20960;&#20010;&#36825;&#20123;&#20449;&#21495;&#30340;&#31867;&#21035;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#31574;&#30053;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#20248;&#21270;&#36825;&#20123;&#31574;&#30053;&#30340;&#39640;&#25928;&#21644;&#23454;&#29992;&#30340;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38754;&#23545;&#23545;&#25239;&#24615;&#24178;&#25200;&#26102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22312;&#32447;&#36866;&#24212;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20219;&#20309;&#40657;&#30418;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate robust model-free reinforcement learning algorithms designed for environments that may be dynamic or even adversarial. Traditional state-based policies often struggle to accommodate the challenges imposed by the presence of unmodeled disturbances in such settings. Moreover, optimizing linear state-based policies pose an obstacle for efficient optimization, leading to nonconvex objectives, even in benign environments like linear dynamical systems.  Drawing inspiration from recent advancements in model-based control, we introduce a novel class of policies centered on disturbance signals. We define several categories of these signals, which we term pseudo-disturbances, and develop corresponding policy classes based on them. We provide efficient and practical algorithms for optimizing these policies.  Next, we examine the task of online adaptation of reinforcement learning agents in the face of adversarial disturbances. Our methods seamlessly integrate with any black-box mod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DPOK&#65292;&#19968;&#31181;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16381</link><description>&lt;p&gt;
DPOK: &#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. (arXiv:2305.16381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DPOK&#65292;&#19968;&#31181;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;&#36825;&#20123;&#25216;&#26415;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#25429;&#25417;&#20219;&#21153;&#20013;&#20154;&#31867;&#20851;&#24515;&#30340;&#29305;&#24449;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#28982;&#21518;&#26681;&#25454;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#25913;&#36827;&#27169;&#22411;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#30456;&#23545;&#31616;&#21333;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22522;&#20110;&#22870;&#21169;&#24471;&#20998;&#30340;&#25298;&#32477;&#37319;&#26679;&#65289;&#65292;&#20294;&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#24494;&#35843;&#20219;&#21153;&#23450;&#20041;&#20026;RL&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#21453;&#39304;&#35757;&#32451;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DPOK&#38598;&#25104;&#20102;KL&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#23545;RL&#24494;&#35843;&#21644;&#30417;&#30563;&#24494;&#35843;&#30340;KL&#27491;&#21017;&#21270;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DPOK&#36890;&#24120;&#20248;&#20110;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#30417;&#30563;&#24494;&#35843;&#21644;&#20197;&#21069;&#30340;RL&#24494;&#35843;&#25216;&#26415;&#12290;DPOK&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;IS&#21644;FID&#24471;&#20998;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human feedback has been shown to improve text-to-image models. These techniques first learn a reward function that captures what humans care about in the task and then improve the models based on the learned reward function. Even though relatively simple approaches (e.g., rejection sampling based on reward scores) have been investigated, fine-tuning text-to-image models with the reward function remains challenging. In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback-trained reward. Our approach, coined DPOK, integrates policy optimization with KL regularization. We conduct an analysis of KL regularization for both RL fine-tuning and supervised fine-tuning. In our experiments, we show that DPOK is generally superior to supervised fine-tuning w
&lt;/p&gt;</description></item><item><title>&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#20854;&#27880;&#24847;&#21147;&#24448;&#24448;&#20250;&#20998;&#25955;&#21040;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#65292;&#36825;&#20250;&#23548;&#33268;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;&#21482;&#21547;&#19968;&#20010;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#22312;&#35299;&#31572;&#20013;&#36873;&#25321;&#27491;&#30830;&#29575;&#24456;&#39640;
&lt;/p&gt;
&lt;p&gt;
Attentiveness to Answer Choices Doesn't Always Entail High QA Accuracy. (arXiv:2305.14596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14596
&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#20854;&#27880;&#24847;&#21147;&#24448;&#24448;&#20250;&#20998;&#25955;&#21040;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#65292;&#36825;&#20250;&#23548;&#33268;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;&#21482;&#21547;&#19968;&#20010;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#38646;&#25110;&#23569;&#26679;&#26412;&#30340;&#37492;&#21035;&#24615;&#20219;&#21153;&#65292;&#20363;&#22914;&#22810;&#39033;&#36873;&#25321;&#39064;&#26102;&#65292;&#23427;&#20204;&#30340;&#27880;&#24847;&#21147;&#65288;&#21363;&#27010;&#29575;&#36136;&#37327;&#65289;&#20250;&#20998;&#25955;&#22312;&#35768;&#22810;&#26080;&#25928;&#30340;&#35789;&#27719;&#31526;&#21495;&#19978;&#12290;&#36825;&#31181;&#22312;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#30340;&#22810;&#20010;&#34920;&#38754;&#24418;&#24335;&#20043;&#38388;&#20998;&#25955;&#23548;&#33268;&#20102;&#27169;&#22411;&#30495;&#23454;&#24615;&#33021;&#30340;&#20302;&#20272;&#65292;&#31216;&#20026;&#8220;&#34920;&#38754;&#24418;&#24335;&#31454;&#20105;&#8221;&#65288;SFC&#65289;&#20551;&#35828;&#12290;&#36825;&#20419;&#20351;&#24341;&#20837;&#21508;&#31181;&#27010;&#29575;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#28982;&#32780;&#20173;&#23384;&#22312;&#35768;&#22810;&#26680;&#24515;&#38382;&#39064;&#26410;&#35299;&#31572;&#12290;&#25105;&#20204;&#22914;&#20309;&#27979;&#37327;SFC&#25110;&#27880;&#24847;&#21147;&#65311;&#26159;&#21542;&#26377;&#30452;&#25509;&#30340;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;&#23545;&#26377;&#25928;&#36873;&#25321;&#30340;&#27880;&#24847;&#21147;&#65311;&#22686;&#21152;&#27880;&#24847;&#21147;&#24635;&#26159;&#33021;&#25552;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#21527;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#27880;&#24847;&#21147;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22686;&#21152;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20165;&#21253;&#21547;&#31572;&#26696;&#36873;&#39033;&#30340;&#19968;&#20010;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
When large language models (LMs) are applied in zero- or few-shot settings to discriminative tasks such as multiple-choice questions, their attentiveness (i.e., probability mass) is spread across many vocabulary tokens that are not valid choices. Such a spread across multiple surface forms with identical meaning is thought to cause an underestimation of a model's true performance, referred to as the "surface form competition" (SFC) hypothesis. This has motivated the introduction of various probability normalization methods. However, many core questions remain unanswered. How do we measure SFC or attentiveness? Are there direct ways of increasing attentiveness on valid choices? Does increasing attentiveness always improve task accuracy? We propose a mathematical formalism for studying this phenomenon, provide a metric for quantifying attentiveness, and identify a simple method for increasing it -namely, in-context learning with even just one example containing answer choices. The form
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20174;&#23427;&#21019;&#24314;&#30340;&#24402;&#22240;&#20250;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14585</link><description>&lt;p&gt;
&#36890;&#36807;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Explanations for Deep Neural Networks via Pseudo Neural Tangent Kernel Surrogate Models. (arXiv:2305.14585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#20174;&#23427;&#21019;&#24314;&#30340;&#24402;&#22240;&#20250;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#25968;&#25454;&#24402;&#23646;&#20219;&#21153;&#65292;&#35299;&#37322;&#22411;AI&#30340;&#36827;&#27493;&#20043;&#19968;&#26159;&#36890;&#36807;&#35299;&#37322;&#31034;&#20363;&#31574;&#30053;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#23558;&#20915;&#31574;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#23578;&#26410;&#30456;&#20114;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#24418;&#25104;&#31070;&#32463;&#32593;&#32476;(NN)&#30340;&#30495;&#27491;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#35777;&#26126;&#20102;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65306;(1)&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35268;&#33539;&#21270;&#30340;&#20266;&#31070;&#32463;&#20999;&#32447;&#26680;(pNTK)&#65292;&#23427;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#20013;&#19982;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#20989;&#25968;&#26356;&#30456;&#20851;&#65292;&#27604;&#22522;&#20110;&#23884;&#20837;&#21644;&#24433;&#21709;&#30340;&#26367;&#20195;&#21697;&#26356;&#20026;&#26377;&#25928;&#65307;(2)&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#35268;&#33539;&#21270;pNTK&#21019;&#24314;&#30340;&#24402;&#22240;&#27604;&#36825;&#20123;&#26367;&#20195;&#21697;&#26356;&#20934;&#30830;&#22320;&#36873;&#25321;&#34987;&#25200;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26680;&#32447;&#24615;&#27169;&#22411;&#26159;&#36328;&#22810;&#20010;&#25968;&#25454;&#39046;&#22495;&#24182;&#26377;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the ways recent progress has been made on explainable AI has been via explain-by-example strategies, specifically, through data attribution tasks. The feature spaces used to attribute decisions to training data, however, have not been compared against one another as to whether they form a truly representative surrogate model of the neural network (NN). Here, we demonstrate the efficacy of surrogate linear feature spaces to neural networks through two means: (1) we establish that a normalized psuedo neural tangent kernel (pNTK) is more correlated to the neural network decision functions than embedding based and influence based alternatives in both computer vision and large language model architectures; (2) we show that the attributions created from the normalized pNTK more accurately select perturbed training data in a data poisoning attribution task than these alternatives. Based on these observations, we conclude that kernel linear models are effective surrogate models across m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#38656;&#21442;&#25968;&#32422;&#26463;&#22320;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.05276</link><description>&lt;p&gt;
&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Subsampled Time Series with Proxy Variables. (arXiv:2305.05276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#38656;&#21442;&#25968;&#32422;&#26463;&#22320;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26159;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#30340;&#26680;&#24515;&#20852;&#36259;&#12290;&#37319;&#26679;&#39057;&#29575;&#36828;&#20302;&#20110;&#22240;&#26524;&#24433;&#21709;&#39057;&#29575;&#26159;&#27492;&#31867;&#25512;&#26029;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#35201;&#20040;&#23616;&#38480;&#20110;&#32447;&#24615;&#24773;&#20917;&#65292;&#35201;&#20040;&#26080;&#27861;&#24314;&#31435;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#21442;&#25968;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#30340;&#24605;&#24819;&#26159;&#65292;&#23376;&#37319;&#26679;&#30340;&#25361;&#25112;&#20027;&#35201;&#26469;&#33258;&#20110;&#8220;&#26410;&#35266;&#23519;&#21040;&#8221;&#30340;&#26102;&#38388;&#27493;&#65292;&#22240;&#27492;&#24212;&#20351;&#29992;&#20026;&#26410;&#35266;&#23519;&#21040;&#21464;&#37327;&#35774;&#35745;&#30340;&#24037;&#20855;&#22788;&#29702;&#27492;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24037;&#20855;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#29305;&#21035;&#36866;&#21512;&#65292;&#22240;&#20026;&#26410;&#35266;&#23519;&#21040;&#21464;&#37327;&#30340;&#20195;&#29702;&#21464;&#37327;&#33258;&#28982;&#26159;&#22312;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#27493;&#19978;&#26412;&#36523;&#12290;&#26681;&#25454;&#36825;&#31181;&#30452;&#35273;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#32467;&#26500;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, i.e., the frequency of measurements is much lower than that of causal influence. To overcome this problem, numerous model-based and model-free methods have been proposed, yet either limited to the linear case or failed to establish identifiability. In this work, we propose a model-free algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. The idea is that the challenge of subsampling arises mainly from \emph{unobserved} time steps and therefore should be handled with tools designed for unobserved variables. Among these tools, we find the proxy variable approach particularly fits, in the sense that the proxy of an unobserved variable is naturally itself at the observed time step. Following this intuition, we establish comprehensive structural identifiabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35780;&#20272;&#21644;&#29702;&#35299;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#20248;&#21155;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#20132;&#20114;&#20013;&#37325;&#29616;&#36825;&#20123;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.13787</link><description>&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Surrogate Assisted Generation of Human-Robot Interaction Scenarios. (arXiv:2304.13787v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35780;&#20272;&#21644;&#29702;&#35299;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#20248;&#21155;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#20132;&#20114;&#20013;&#37325;&#29616;&#36825;&#20123;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#19981;&#21516;&#29615;&#22659;&#21644;&#29992;&#25143;&#19979;&#35780;&#20272;&#21644;&#29702;&#35299;&#36825;&#20123;&#31995;&#32479;&#30340;&#20248;&#32570;&#28857;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#31639;&#27861;&#29983;&#25104;&#20102;&#22810;&#26679;&#30340;&#22330;&#26223;&#65292;&#25581;&#31034;&#20102;&#20849;&#20139;&#25511;&#21046;&#36965;&#25805;&#20316;&#20219;&#21153;&#30340;&#31995;&#32479;&#22833;&#25928;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#20154;&#31574;&#30053;&#21644;&#20154;&#31867;&#34892;&#20026;&#26469;&#30452;&#25509;&#35780;&#20272;&#29983;&#25104;&#30340;&#22330;&#26223;&#12290;&#36825;&#20123;&#35780;&#20272;&#25152;&#38656;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#26367;&#20195;&#27169;&#22411;&#26469;&#39044;&#27979;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#34892;&#20026;&#26469;&#22686;&#24378;&#22330;&#26223;&#29983;&#25104;&#31995;&#32479;&#30340;&#24314;&#35758;&#12290;&#22312;&#20849;&#20139;&#25511;&#21046;&#36965;&#25805;&#20316;&#22495;&#21644;&#26356;&#22797;&#26434;&#30340;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#21327;&#20316;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26367;&#20195;&#27169;&#22411;&#36741;&#21161;&#30340;&#22330;&#26223;&#29983;&#25104;&#21487;&#20197;&#39640;&#25928;&#22320;&#21512;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#25925;&#38556;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20132;&#20114;&#20013;&#26159;&#21487;&#37325;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
As human-robot interaction (HRI) systems advance, so does the difficulty of evaluating and understanding the strengths and limitations of these systems in different environments and with different users. To this end, previous methods have algorithmically generated diverse scenarios that reveal system failures in a shared control teleoperation task. However, these methods require directly evaluating generated scenarios by simulating robot policies and human actions. The computational cost of these evaluations limits their applicability in more complex domains. Thus, we propose augmenting scenario generation systems with surrogate models that predict both human and robot behaviors. In the shared control teleoperation domain and a more complex shared workspace collaboration task, we show that surrogate assisted scenario generation efficiently synthesizes diverse datasets of challenging scenarios. We demonstrate that these failures are reproducible in real-world interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#36229;&#21442;&#25968;&#30830;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#20272;&#35745;&#23616;&#37096;&#33021;&#37327;&#30340;&#20998;&#24067;&#65292;&#23558;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;O(n^3)&#38477;&#20302;&#21040;O(1)&#65292;&#22312;&#35299;&#20915;&#26368;&#22823;&#21106;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.11839</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#36229;&#21442;&#25968;&#30830;&#23450;
&lt;/p&gt;
&lt;p&gt;
Local Energy Distribution Based Hyperparameter Determination for Stochastic Simulated Annealing. (arXiv:2304.11839v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#36229;&#21442;&#25968;&#30830;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#20272;&#35745;&#23616;&#37096;&#33021;&#37327;&#30340;&#20998;&#24067;&#65292;&#23558;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;O(n^3)&#38477;&#20302;&#21040;O(1)&#65292;&#22312;&#35299;&#20915;&#26368;&#22823;&#21106;&#38382;&#39064;&#20013;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#30340;&#38543;&#26426;&#27169;&#25311;&#36864;&#28779;&#65288;SSA&#65289;&#36229;&#21442;&#25968;&#30830;&#23450;&#26041;&#27861;&#12290; SSA&#33021;&#22815;&#27604;&#20856;&#22411;&#30340;&#27169;&#25311;&#36864;&#28779;&#65288;SA&#65289;&#26356;&#24555;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#33258;&#26059;&#65288;&#27010;&#29575;&#27604;&#29305;&#65289;&#30340;&#23616;&#37096;&#33021;&#37327;&#20998;&#24067;&#26469;&#30830;&#23450;&#36229;&#21442;&#25968;&#12290;&#33258;&#26059;&#26159;SSA&#30340;&#22522;&#26412;&#35745;&#31639;&#20803;&#32032;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#19982;&#20854;&#20182;&#33258;&#26059;&#36827;&#34892;&#22270;&#24418;&#36830;&#25509;&#12290;&#23616;&#37096;&#33021;&#37327;&#30340;&#20998;&#24067;&#21487;&#20197;&#22522;&#20110;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65288;CLT&#65289;&#36827;&#34892;&#20272;&#35745;&#12290;&#22522;&#20110;CLT&#30340;&#27491;&#24577;&#20998;&#24067;&#29992;&#20110;&#30830;&#23450;&#36229;&#21442;&#25968;&#65292;&#20854;&#23558;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20256;&#32479;&#26041;&#27861;&#30340;O(n^3)&#38477;&#20302;&#21040;O(1)&#12290;&#20351;&#29992;&#30830;&#23450;&#30340;&#36229;&#21442;&#25968;&#35780;&#20272;&#20102;SSA&#22312;Gset&#21644;K2000&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#65292;&#29992;&#20110;&#26368;&#22823;&#21106;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#24179;&#22343;&#21106;&#20540;&#30340;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a local energy distribution based hyperparameter determination for stochastic simulated annealing (SSA). SSA is capable of solving combinatorial optimization problems faster than typical simulated annealing (SA), but requires a time-consuming hyperparameter search. The proposed method determines hyperparameters based on the local energy distributions of spins (probabilistic bits). The spin is a basic computing element of SSA and is graphically connected to other spins with its weights. The distribution of the local energy can be estimated based on the central limit theorem (CLT). The CLT-based normal distribution is used to determine the hyperparameters, which reduces the time complexity for hyperparameter search from O(n^3) of the conventional method to O(1). The performance of SSA with the determined hyperparameters is evaluated on the Gset and K2000 benchmarks for maximum-cut problems. The results show that the proposed method achieves mean cut values of approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#32447;&#24615;&#20989;&#25968;&#26500;&#24314;&#20132;&#21449;&#30456;&#20851;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Price&#23450;&#29702;&#21644;&#20998;&#27573;&#32447;&#24615;&#20998;&#35299;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#31181;&#20132;&#21449;&#30456;&#20851;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.09242</link><description>&lt;p&gt;
&#20351;&#29992;Price&#23450;&#29702;&#21644;&#20998;&#27573;&#32447;&#24615;&#20998;&#35299;&#20998;&#26512;&#22312;&#32447;&#20132;&#21449;&#30456;&#20851;&#22120;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Framework for Analyzing Online Cross-correlators using Price's Theorem and Piecewise-Linear Decomposition. (arXiv:2304.09242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#32447;&#24615;&#20989;&#25968;&#26500;&#24314;&#20132;&#21449;&#30456;&#20851;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Price&#23450;&#29702;&#21644;&#20998;&#27573;&#32447;&#24615;&#20998;&#35299;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#31181;&#20132;&#21449;&#30456;&#20851;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#20272;&#35745;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20132;&#21449;&#30456;&#20851;&#25110;&#30456;&#20284;&#24230;&#26159;&#20449;&#21495;&#26816;&#27979;&#12289;&#39640;&#32500;&#35745;&#31639;&#12289;&#32852;&#24819;&#35760;&#24518;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26500;&#24314;&#20855;&#26377;&#26356;&#39640;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#30340;&#20132;&#21449;&#30456;&#20851;&#22120;&#30340;&#22823;&#37327;&#31616;&#21333;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Price&#23450;&#29702;&#21644;&#20998;&#27573;&#32447;&#24615;&#20998;&#35299;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#20998;&#26512;&#20351;&#29992;&#28151;&#21512;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#26500;&#24314;&#30340;&#20132;&#21449;&#30456;&#20851;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise estimation of cross-correlation or similarity between two random variables lies at the heart of signal detection, hyperdimensional computing, associative memories, and neural networks. Although a vast literature exists on different methods for estimating cross-correlations, the question what is the best and simplest method to estimate cross-correlations using finite samples ? is still not clear. In this paper, we first argue that the standard empirical approach might not be the optimal method even though the estimator exhibits uniform convergence to the true cross-correlation. Instead, we show that there exists a large class of simple non-linear functions that can be used to construct cross-correlators with a higher signal-to-noise ratio (SNR). To demonstrate this, we first present a general mathematical framework using Price's Theorem that allows us to analyze cross-correlators constructed using a mixture of piece-wise linear functions. Using this framework and high-dimensiona
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#25506;&#35752;&#20102;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#25214;&#21040;&#22240;&#26524;&#39034;&#24207;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#38500;&#20102;&#26041;&#24046;&#25490;&#24207;&#22806;&#65292;&#21464;&#37327;&#30340;&#20915;&#23450;&#31995;&#25968;$R^2$&#25490;&#24207;&#20063;&#21487;&#29992;&#20110;&#21305;&#37197;&#24050;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#19988;&#19981;&#21463;&#25968;&#25454;&#32553;&#25918;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.18211</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#25490;&#24207;&#26631;&#20934;&#26377;&#21161;&#20110;&#22312;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#25214;&#21040;&#22240;&#26524;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simple Sorting Criteria Help Find the Causal Order in Additive Noise Models. (arXiv:2303.18211v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18211
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#25506;&#35752;&#20102;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#20013;&#25214;&#21040;&#22240;&#26524;&#39034;&#24207;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#38500;&#20102;&#26041;&#24046;&#25490;&#24207;&#22806;&#65292;&#21464;&#37327;&#30340;&#20915;&#23450;&#31995;&#25968;$R^2$&#25490;&#24207;&#20063;&#21487;&#29992;&#20110;&#21305;&#37197;&#24050;&#26377;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#19988;&#19981;&#21463;&#25968;&#25454;&#32553;&#25918;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21151;&#33021;&#20551;&#35774;&#65292;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#12290;&#30001;&#20110;&#32570;&#20047;&#31526;&#21512;&#20551;&#35774;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#21512;&#25104;ANM&#25968;&#25454;&#32463;&#24120;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#12290;Reisach&#31561;&#20154;&#65288;2021&#65289;&#34920;&#26126;&#65292;&#23545;&#20110;&#24120;&#35265;&#30340;&#27169;&#25311;&#21442;&#25968;&#65292;&#25353;&#22686;&#22823;&#26041;&#24046;&#30340;&#39034;&#24207;&#21464;&#37327;&#25490;&#21015;&#19982;&#22240;&#26524;&#39034;&#24207;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#24341;&#20837;&#21464;&#24322;&#24615;&#21487;&#25490;&#24207;&#24615;&#26469;&#37327;&#21270;&#36825;&#31181;&#23545;&#40784;&#31243;&#24230;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#38500;&#20102;&#26041;&#24046;&#65292;&#36824;&#26377;&#21464;&#37327;&#30340;&#26041;&#24046;&#34987;&#25152;&#26377;&#20854;&#20182;&#21464;&#37327;&#35299;&#37322;&#30340;&#27604;&#20363;&#65288;&#30001;&#20915;&#23450;&#31995;&#25968;$R^2$&#25429;&#33719;&#65289;&#20542;&#21521;&#20110;&#27839;&#30528;&#22240;&#26524;&#39034;&#24207;&#22686;&#21152;&#12290;&#31616;&#21333;&#30340;&#22522;&#20934;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;$R^2$-sortability&#26469;&#21305;&#37197;&#24050;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;$R^2$&#21487;&#25490;&#24207;&#24615;&#19981;&#21463;&#25968;&#25454;&#32553;&#25918;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#26631;&#20934;&#21270;&#25110;&#37325;&#26032;&#32553;&#25918;&#30340;&#25968;&#25454;&#19978;&#34920;&#29616;&#21516;&#26679;&#20986;&#33394;&#65292;&#35299;&#20915;&#20102;&#21033;&#29992;&#21464;&#24322;&#24615;&#21487;&#25490;&#24207;&#24615;&#30340;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive Noise Models (ANM) encode a popular functional assumption that enables learning causal structure from observational data. Due to a lack of real-world data meeting the assumptions, synthetic ANM data are often used to evaluate causal discovery algorithms. Reisach et al. (2021) show that, for common simulation parameters, a variable ordering by increasing variance is closely aligned with a causal order and introduce var-sortability to quantify the alignment. Here, we show that not only variance, but also the fraction of a variable's variance explained by all others, as captured by the coefficient of determination $R^2$, tends to increase along the causal order. Simple baseline algorithms can use $R^2$-sortability to match the performance of established methods. Since $R^2$-sortability is invariant under data rescaling, these algorithms perform equally well on standardized or rescaled data, addressing a key limitation of algorithms exploiting var-sortability. We characterize and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22823;&#22411;&#32593;&#32476;&#30340;&#30456;&#37051;&#30697;&#38453;&#65292;&#24182;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2303.18059</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#25512;&#26029;&#32593;&#32476;&#32467;&#26500;&#30340;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inferring networks from time series: a neural approach. (arXiv:2303.18059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22823;&#22411;&#32593;&#32476;&#30340;&#30456;&#37051;&#30697;&#38453;&#65292;&#24182;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#32467;&#26500;&#26159;&#35768;&#22810;&#22797;&#26434;&#29616;&#35937;&#30340;&#21160;&#24577;&#22522;&#30784;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#12289;&#39135;&#29289;&#38142;&#12289;&#30005;&#21147;&#32593;&#32476;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#32467;&#26500;&#36890;&#24120;&#26080;&#27861;&#30452;&#25509;&#35266;&#27979;&#21040;&#65292;&#22240;&#27492;&#24517;&#39035;&#20174;&#20854;&#32039;&#24613;&#21160;&#24577;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#23427;&#20204;&#30340;&#30456;&#20114;&#36830;&#25509;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35745;&#31639;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22823;&#22411;&#32593;&#32476;&#30456;&#37051;&#30697;&#38453;&#12290;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#21453;&#26144;&#20102;&#25512;&#26029;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#21644;&#25968;&#25454;&#19978;&#30340;&#22122;&#22768;&#12290;&#36825;&#26159;&#26377;&#29992;&#30340;&#65292;&#22240;&#20026;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#36890;&#24120;&#26159;&#27424;&#23450;&#30340;&#65292;&#24182;&#19988;&#22312;&#32593;&#32476;&#25512;&#26029;&#26041;&#27861;&#20013;&#32570;&#20047;&#36825;&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#35266;&#27979;&#20854;&#21709;&#24212;&#26029;&#30005;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#33521;&#22269;&#30005;&#21147;&#32593;&#32476;&#30340;&#32447;&#36335;&#25925;&#38556;&#20301;&#32622;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network structures underlie the dynamics of many complex phenomena, from gene regulation and foodwebs to power grids and social media. Yet, as they often cannot be observed directly, their connectivities must be inferred from observations of their emergent dynamics. In this work we present a powerful and fast computational method to infer large network adjacency matrices from time series data using a neural network. Using a neural network provides uncertainty quantification on the prediction in a manner that reflects both the non-convexity of the inference problem as well as the noise on the data. This is useful since network inference problems are typically underdetermined, and a feature that has hitherto been lacking from network inference methods. We demonstrate our method's capabilities by inferring line failure locations in the British power grid from observations of its response to a power cut. Since the problem is underdetermined, many classical statistical tools (e.g. regressio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39640;&#32500;Transformer&#30340;&#26550;&#26500;HDformer&#65292;&#24182;&#21033;&#29992;&#38271;&#36317;&#31163;PPG&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;TSA&#65292;&#25104;&#21151;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.11340</link><description>&lt;p&gt;
HDformer: &#19968;&#31181;&#21033;&#29992;&#38271;&#36317;&#31163;&#34880;&#31649;&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#30340;&#39640;&#32500;Transformer
&lt;/p&gt;
&lt;p&gt;
HDformer: A Higher Dimensional Transformer for Diabetes Detection Utilizing Long Range Vascular Signals. (arXiv:2303.11340v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39640;&#32500;Transformer&#30340;&#26550;&#26500;HDformer&#65292;&#24182;&#21033;&#29992;&#38271;&#36317;&#31163;PPG&#20449;&#21495;&#36827;&#34892;&#31958;&#23615;&#30149;&#26816;&#27979;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;TSA&#65292;&#25104;&#21151;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#26159;&#20840;&#29699;&#24615;&#38382;&#39064;&#65292;&#26089;&#26399;&#26816;&#27979;&#26377;&#21161;&#20110;&#39044;&#38450;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#24050;&#20986;&#29616;&#23558;&#24515;&#34880;&#31649;&#20449;&#21495;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24335;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#38480;&#21046;&#20854;&#20020;&#24202;&#24212;&#29992;&#30340;&#26159;&#26377;&#38480;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#21363;Higher Dimensional Transformer&#65288;HDformer&#65289;&#65292;&#23427;&#21033;&#29992;&#38271;&#36317;&#31163;&#20809;&#30005;&#23481;&#31215;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#26469;&#26816;&#27979;&#31958;&#23615;&#30149;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30740;&#31350;&#24120;&#29992;&#30340;&#19981;&#36275;&#19968;&#20998;&#38047;&#30340;PPG&#20449;&#21495;&#65292;&#38271;&#36317;&#31163;PPG&#21253;&#21547;&#26356;&#24191;&#27867;&#12289;&#26356;&#28145;&#20837;&#30340;&#20449;&#21495;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#22686;&#21152;&#22788;&#29702;&#38271;&#36317;&#31163;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;Time Square Attention&#65288;TSA&#65289;&#65292;&#23558;&#26631;&#35760;&#20307;&#31215;&#20943;&#23569;10&#20493;&#20197;&#19978;&#65292;&#21516;&#26102;&#20445;&#30041;&#26412;&#22320;/&#20840;&#23616;&#20381;&#36182;&#20851;&#31995;&#12290;&#23427;&#23558;&#19968;&#32500;&#36755;&#20837; &#36716;&#25442;&#20026;&#20108;&#32500;&#34920;&#31034;&#65292;&#24182;&#23558;&#30456;&#37051;&#28857;&#32452;&#25104;&#19968;&#20010;&#21333;&#29420;&#30340;2D&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetes mellitus is a worldwide concern, and early detection can help to prevent serious complications. Low-cost, non-invasive detection methods, which take cardiovascular signals into deep learning models, have emerged. However, limited accuracy constrains their clinical usage. In this paper, we present a new Transformer-based architecture, Higher Dimensional Transformer (HDformer), which takes long-range photoplethysmography (PPG) signals to detect diabetes. The long-range PPG contains broader and deeper signal contextual information compared to the less-than-one-minute PPG signals commonly utilized in existing research. To increase the capability and efficiency of processing the long range data, we propose a new attention module Time Square Attention (TSA), reducing the volume of the tokens by more than 10x, while retaining the local/global dependencies. It converts the 1-dimensional inputs into 2-dimensional representations and groups adjacent points into a single 2D token, using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06530</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#25209;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.
&lt;/p&gt;
&lt;p&gt;
&#25209;&#26631;&#20934;&#21270;&#65288;BN&#65289;&#36890;&#24120;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#24182;&#21152;&#36895;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#38750;IID&#20998;&#25955;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20351;&#29992;BN&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;BN&#32479;&#35745;&#19981;&#21305;&#37197;&#32780;&#38459;&#30861;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#32676;&#32452;&#24402;&#19968;&#21270;&#65288;GN&#65289;&#26356;&#24120;&#29992;&#20110;FL&#20316;&#20026;BN&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;BN&#21644;GN&#20043;&#38388;&#27809;&#26377;&#19968;&#33268;&#30340;&#20248;&#32988;&#32773;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;FL&#20013;&#24402;&#19968;&#21270;&#23618;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;BN&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;FL&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#25104;&#20026;FL&#26410;&#26469;&#23454;&#38469;&#20351;&#29992;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#38598;&#26102;&#23558;&#25968;&#25454;&#21516;&#26102;&#20256;&#36882;&#32473;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#30005;&#36335;&#65292;&#24182;&#32447;&#24615;&#32452;&#21512;&#20004;&#32773;&#30340;&#36755;&#20986;&#65292;&#20197;&#35299;&#20915;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#25311;&#21512;&#38750;&#35856;&#27874;&#29305;&#24449;&#26102;&#30340;&#22256;&#38590;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03227</link><description>&lt;p&gt;
&#24182;&#34892;&#28151;&#21512;&#32593;&#32476;&#65306;&#37327;&#23376;&#21644;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Parallel Hybrid Networks: an interplay between quantum and classical neural networks. (arXiv:2303.03227v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#38598;&#26102;&#23558;&#25968;&#25454;&#21516;&#26102;&#20256;&#36882;&#32473;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#21644;&#37327;&#23376;&#30005;&#36335;&#65292;&#24182;&#32447;&#24615;&#32452;&#21512;&#20004;&#32773;&#30340;&#36755;&#20986;&#65292;&#20197;&#35299;&#20915;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#25311;&#21512;&#38750;&#35856;&#27874;&#29305;&#24449;&#26102;&#30340;&#22256;&#38590;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26368;&#36817;&#30001;&#20110;&#20854;&#28508;&#22312;&#30340;&#20248;&#21183;&#32780;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#29992;&#25130;&#26029;&#30340;&#20613;&#37324;&#21494;&#32423;&#25968;&#36817;&#20284;&#20854;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#25311;&#21512;&#30340;&#19977;&#35282;&#20989;&#25968;&#29305;&#24615;&#21487;&#33021;&#23548;&#33268;&#35282;&#24230;&#23884;&#20837;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#25311;&#21512;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#38750;&#35856;&#27874;&#29305;&#24449;&#26102;&#20986;&#29616;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#30340;&#28151;&#21512;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#65292;&#23558;&#25968;&#25454;&#38598;&#30340;&#36755;&#20837;&#21516;&#26102;&#20256;&#36882;&#32473;1&#65289;&#32463;&#20856;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;2&#65289;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65292;&#28982;&#21518;&#23558;&#20004;&#32773;&#30340;&#36755;&#20986;&#36827;&#34892;&#32447;&#24615;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#38598;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#24179;&#28369;&#30340;&#27491;&#24358;&#22522;&#30784;&#65292;&#28982;&#21518;&#32463;&#20856;&#24863;&#30693;&#26426;&#22635;&#34917;&#20102;&#35813;&#24179;&#21488;&#19978;&#38750;&#35856;&#27874;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks represent a new machine learning paradigm that has recently attracted much attention due to its potential promise. Under certain conditions, these models approximate the distribution of their dataset with a truncated Fourier series. The trigonometric nature of this fit could result in angle-embedded quantum neural networks struggling to fit the non-harmonic features in a given dataset. Moreover, the interpretability of neural networks remains a challenge. In this work, we introduce a new, interpretable class of hybrid quantum neural networks that pass the inputs of the dataset in parallel to 1) a classical multi-layered perceptron and 2) a variational quantum circuit, and then the outputs of the two are linearly combined. We observe that the quantum neural network creates a smooth sinusoidal foundation base on the training set, and then the classical perceptrons fill the non-harmonic gaps in the landscape. We demonstrate this claim on two synthetic datasets samp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#22797;&#21098;&#20992;&#30707;&#22836;&#24067;&#28216;&#25103;&#30340;&#22810;Agent&#23398;&#20064;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#20960;&#31181;&#23398;&#20064;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#22810;Agent&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2303.03196</link><description>&lt;p&gt;
&#20316;&#20026;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#30340;&#37325;&#22797;&#21098;&#20992;&#30707;&#22836;&#24067;&#30340;&#22522;&#20110;&#20154;&#21475;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Population-based Evaluation in Repeated Rock-Paper-Scissors as a Benchmark for Multiagent Reinforcement Learning. (arXiv:2303.03196v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#22797;&#21098;&#20992;&#30707;&#22836;&#24067;&#28216;&#25103;&#30340;&#22810;Agent&#23398;&#20064;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#20960;&#31181;&#23398;&#20064;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20026;&#22810;Agent&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#23545;&#25239;&#35268;&#21010;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#30410;&#20110;&#22522;&#20934;&#22495;&#65292;&#20174;&#22269;&#38469;&#35937;&#26827;&#21644;&#32463;&#20856;&#30340;UCI&#25968;&#25454;&#38598;&#21040;&#22260;&#26827;&#21644;&#22806;&#20132;&#12290;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#65292;&#23545;Agent&#35780;&#20272;&#20027;&#35201;&#23616;&#38480;&#20110;&#19982;&#19987;&#23478;&#36827;&#34892;&#23569;&#37327;&#20132;&#20114;&#65292;&#26088;&#22312;&#36798;&#21040;&#19968;&#23450;&#30340;&#24615;&#33021;&#27700;&#24179;&#65288;&#22914;&#20987;&#36133;&#20154;&#31867;&#19987;&#19994;&#29609;&#23478;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21098;&#20992;&#30707;&#22836;&#24067;&#30340;&#22810;Agent&#23398;&#20064;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#21313;&#19977;&#20010;&#38182;&#26631;&#36187;&#21442;&#36187;&#20316;&#21697;&#65292;&#20854;&#20013;&#19968;&#20123;&#26159;&#26377;&#24847;&#30340;&#27425;&#20248;&#20316;&#21697;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22522;&#20110;&#24179;&#22343;&#22238;&#25253;&#21644;&#21487;&#24320;&#21457;&#24615;&#30340;&#20195;&#29702;&#36136;&#37327;&#24230;&#37327;&#26631;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;RL&#12289;&#22312;&#32447;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#33391;&#22909;&#30340;&#21453;&#31574;&#30053;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#26368;&#32456;&#20250;&#36755;&#32473;&#34920;&#29616;&#26368;&#20339;&#30340;&#26426;&#22120;&#20154;&#65292;&#20026;&#22810;Agent&#23398;&#20064;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in fields of machine learning and adversarial planning has benefited significantly from benchmark domains, from checkers and the classic UCI data sets to Go and Diplomacy. In sequential decision-making, agent evaluation has largely been restricted to few interactions against experts, with the aim to reach some desired level of performance (e.g. beating a human professional player). We propose a benchmark for multiagent learning based on repeated play of the simple game Rock, Paper, Scissors along with a population of forty-three tournament entries, some of which are intentionally sub-optimal. We describe metrics to measure the quality of agents based both on average returns and exploitability. We then show that several RL, online learning, and language model approaches can learn good counter-strategies and generalize well, but ultimately lose to the top-performing bots, creating an opportunity for research in multiagent learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#29992;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#26367;&#20195;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#24182;&#36866;&#29992;&#20110;&#24066;&#22330;&#35774;&#35745;&#21644;&#26356;&#29616;&#23454;&#22320;&#24314;&#27169;&#24066;&#22330;&#21442;&#19982;&#32773;&#12290;</title><link>http://arxiv.org/abs/2303.01772</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning. (arXiv:2303.01772v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33021;&#28304;&#24066;&#22330;&#28165;&#31639;&#21644;&#20986;&#20215;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#29992;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20197;&#21450;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#26367;&#20195;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#65292;&#26412;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#24182;&#36866;&#29992;&#20110;&#24066;&#22330;&#35774;&#35745;&#21644;&#26356;&#29616;&#23454;&#22320;&#24314;&#27169;&#24066;&#22330;&#21442;&#19982;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#24066;&#22330;&#21487;&#33021;&#20250;&#20026;&#24066;&#22330;&#21442;&#19982;&#32773;&#30340;&#19981;&#33391;&#34892;&#20026;&#25552;&#20379;&#28608;&#21169;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#39044;&#27979;&#33021;&#28304;&#24066;&#22330;&#21442;&#19982;&#32773;&#39044;&#26399;&#34892;&#20026;&#30340;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#35768;&#22810;&#19982;&#31995;&#32479;&#30340;&#20132;&#20114;&#25165;&#33021;&#25910;&#25947;&#65292;&#32780;&#30005;&#21147;&#31995;&#32479;&#29615;&#22659;&#36890;&#24120;&#21253;&#25324;&#24191;&#27867;&#30340;&#35745;&#31639;&#65292;&#20363;&#22914;&#29992;&#20110;&#24066;&#22330;&#28165;&#31639;&#30340;&#26368;&#20248;&#21151;&#29575;&#27969;&#37327;&#65288;OPF&#65289;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#28304;&#24066;&#22330;&#30340;&#27169;&#22411;&#32473;&#22522;&#26412;&#30340;MARL&#31639;&#27861;&#65292;&#36825;&#20010;&#27169;&#22411;&#37319;&#29992;&#20102;&#23398;&#20064;&#30340;OPF&#36817;&#20284;&#20540;&#21644;&#26126;&#30830;&#30340;&#24066;&#22330;&#35268;&#21017;&#12290;&#23398;&#20064;&#30340;OPF&#20195;&#29702;&#27169;&#22411;&#20351;&#24471;OPF&#30340;&#26126;&#30830;&#35299;&#20915;&#21464;&#24471;&#19981;&#24517;&#35201;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#36824;&#23558;&#35757;&#32451;&#26102;&#38388;&#38477;&#20302;&#20102;&#32422;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#20195;&#20215;&#26159;&#30053;&#24494;&#26356;&#24046;&#30340;&#32435;&#20160;&#22343;&#34913;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#22312;&#24212;&#29992;&#26159;&#24066;&#22330;&#35774;&#35745;&#65292;&#26356;&#29616;&#23454;&#22320;&#23545;&#24066;&#22330;&#21442;&#19982;&#32773;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#23545;&#24066;&#22330;&#21160;&#24577;&#30340;&#25913;&#36827;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy markets can provide incentives for undesired behavior of market participants. Multi-agent Reinforcement learning (MARL) is a promising new approach to predicting the expected behavior of energy market participants. However, reinforcement learning requires many interactions with the system to converge, and the power system environment often consists of extensive computations, e.g., optimal power flow (OPF) calculation for market clearing. To tackle this complexity, we provide a model of the energy market to a basic MARL algorithm in the form of a learned OPF approximation and explicit market rules. The learned OPF surrogate model makes an explicit solving of the OPF completely unnecessary. Our experiments demonstrate that the model additionally reduces training time by about one order of magnitude but at the cost of a slightly worse approximation of the Nash equilibrium. Potential applications of our method are market design, more realistic modeling of market participants, and an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#33539;&#25968;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#24809;&#32602;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.01353</link><description>&lt;p&gt;
&#23545;&#27491;&#21017;&#21270;&#20013;&#30340;&#20559;&#24046;&#36827;&#34892;&#24809;&#32602;&#23558;&#20351;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Penalising the biases in norm regularisation enforces sparsity. (arXiv:2303.01353v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#33539;&#25968;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#24809;&#32602;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36890;&#36807;&#25511;&#21046;&#21442;&#25968;&#30340;&#33539;&#25968;&#24448;&#24448;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#33539;&#25968;&#21644;&#25152;&#24471;&#20272;&#35745;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#29702;&#35770;&#19978;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#21333;&#19968;&#38544;&#34255;&#23618;&#21644;&#19968;&#32500;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#20989;&#25968;&#25152;&#38656;&#30340;&#21442;&#25968;&#33539;&#25968;&#30001;&#20854;&#20108;&#38454;&#23548;&#25968;&#30340;&#24635;&#21464;&#24046;&#21152;&#26435;&#24471;&#21040;&#65292;&#20854;&#20013;&#25152;&#21152;&#26435;&#30340;&#22240;&#23376;&#20026;$\sqrt{1+x^2}$&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#19981;&#23545;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#26102;&#65292;&#36825;&#20010;&#21152;&#26435;&#22240;&#23376;&#20250;&#28040;&#22833;&#12290;&#36825;&#20010;&#39069;&#22806;&#30340;&#21152;&#26435;&#22240;&#23376;&#30340;&#23384;&#22312;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#34987;&#35777;&#26126;&#21487;&#20197;&#24378;&#21046;&#23454;&#29616;&#26368;&#23567;&#33539;&#25968;&#20869;&#25554;&#22120;&#30340;&#21807;&#19968;&#24615;&#21644;&#31232;&#30095;&#24615;&#65288;&#22312;&#25296;&#28857;&#25968;&#37327;&#19978;&#65289;&#12290;&#30456;&#21453;&#65292;&#30465;&#30053;&#20559;&#24046;&#30340;&#33539;&#25968;&#21017;&#20250;&#23548;&#33268;&#38750;&#31232;&#30095;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#27491;&#21017;&#21270;&#20013;&#23545;&#20559;&#24046;&#39033;&#36827;&#34892;&#24809;&#32602;&#65292;&#26080;&#35770;&#26159;&#26174;&#24335;&#36824;&#26159;&#38544;&#24335;&#22320;&#65292;&#37117;&#20250;&#23548;&#33268;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling the parameters' norm often yields good generalisation when training neural networks. Beyond simple intuitions, the relation between regularising parameters' norm and obtained estimators remains theoretically misunderstood. For one hidden ReLU layer networks with unidimensional data, this work shows the parameters' norm required to represent a function is given by the total variation of its second derivative, weighted by a $\sqrt{1+x^2}$ factor. Notably, this weighting factor disappears when the norm of bias terms is not regularised. The presence of this additional weighting factor is of utmost significance as it is shown to enforce the uniqueness and sparsity (in the number of kinks) of the minimal norm interpolator. Conversely, omitting the bias' norm allows for non-sparse solutions. Penalising the bias terms in the regularisation, either explicitly or implicitly, thus leads to sparse estimators.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#20998;&#24067;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38024;&#23545;&#24615;&#22320;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#20998;&#24067;&#36716;&#25442;&#23545;&#20110;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13875</link><description>&lt;p&gt;
&#22312;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#26465;&#20214;&#19979;&#35780;&#20272;&#22270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts. (arXiv:2302.13875v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13875
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#20998;&#24067;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#38024;&#23545;&#24615;&#22320;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#20998;&#24067;&#36716;&#25442;&#23545;&#20110;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#38752;&#20915;&#31574;&#31995;&#32479;&#20013;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#20998;&#24067;&#20559;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#25110;&#25552;&#20379;&#20854;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#22270;&#23398;&#20064;&#30340;&#33410;&#28857;&#32423;&#38382;&#39064;&#20013;&#65292;&#20998;&#24067;&#20559;&#31227;&#21487;&#33021;&#23588;&#20026;&#22797;&#26434;&#65292;&#22240;&#20026;&#26679;&#26412;&#26159;&#30456;&#20114;&#20381;&#36182;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#22270;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#37325;&#35201;&#30340;&#26159;&#22312;&#21508;&#31181;&#26377;&#24847;&#20041;&#30340;&#20998;&#24067;&#20559;&#31227;&#19979;&#23545;&#23427;&#20204;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32771;&#34385;&#33410;&#28857;&#32423;&#20998;&#24067;&#20559;&#31227;&#30340;&#22270;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#33410;&#28857;&#29305;&#24449;&#65292;&#32780;&#32467;&#26500;&#23646;&#24615;&#23545;&#22270;&#38382;&#39064;&#20063;&#24456;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#32467;&#26500;&#24341;&#20986;&#22810;&#26679;&#21270;&#20998;&#24067;&#20559;&#31227;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26681;&#25454;&#20960;&#20010;&#33410;&#28857;&#30340;&#32467;&#26500;&#23646;&#24615;&#65306;&#27969;&#34892;&#24230;&#12289;&#23616;&#37096;&#24615;&#21644;&#23494;&#24230;&#26469;&#21019;&#24314;&#25968;&#25454;&#20998;&#21106;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#23545;&#20110;&#29616;&#26377;&#30340;&#22270;&#27169;&#22411;&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36824;&#20462;&#35746;&#20102;&#19968;&#20123;&#20851;&#20110;&#22522;&#20934;&#27979;&#35797;&#22270;&#27169;&#22411;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#32771;&#34385;&#20102;&#32467;&#26500;&#20998;&#24067;&#20559;&#31227;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reliable decision-making systems based on machine learning, models have to be robust to distributional shifts or provide the uncertainty of their predictions. In node-level problems of graph learning, distributional shifts can be especially complex since the samples are interdependent. To evaluate the performance of graph models, it is important to test them on diverse and meaningful distributional shifts. However, most graph benchmarks considering distributional shifts for node-level problems focus mainly on node features, while structural properties are also essential for graph problems. In this work, we propose a general approach for inducing diverse distributional shifts based on graph structure. We use this approach to create data splits according to several structural node properties: popularity, locality, and density. In our experiments, we thoroughly evaluate the proposed distributional shifts and show that they can be quite challenging for existing graph models. We also rev
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#38899;&#20048;&#39046;&#22495;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20986;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20363;&#22914;&#32447;&#24615;&#38899;&#39640;&#21644;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2302.10890</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#23545;&#31216;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Low-dimensional Representation via Physical Symmetry. (arXiv:2302.10890v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10890
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#38899;&#20048;&#39046;&#22495;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20986;&#21487;&#35299;&#37322;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20363;&#22914;&#32447;&#24615;&#38899;&#39640;&#21644;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#23398;&#20064;&#22312;&#21019;&#36896;&#24615;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#38899;&#20048;&#39046;&#22495;&#65292;&#24403;&#21069;&#30340;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#21508;&#31181;&#29305;&#24449;&#65292;&#22914;&#38899;&#39640;&#12289;&#38899;&#33394;&#12289;&#21644;&#24358;&#12289;&#32441;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#38899;&#20048;&#39046;&#22495;&#30693;&#35782;&#12290;&#29616;&#22312;&#36824;&#19981;&#28165;&#26970;&#20160;&#20040;&#26679;&#30340;&#19968;&#33324;&#24615;&#35745;&#31639;&#21407;&#21017;&#20250;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#19982;&#20154;&#31867;&#24863;&#30693;&#20445;&#25345;&#19968;&#33268;&#30340;&#20302;&#32500;&#22240;&#32032;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#29616;&#20195;&#29289;&#29702;&#23398;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#23558;&#29289;&#29702;&#23545;&#31216;&#24615;&#20316;&#20026;&#28508;&#22312;&#31354;&#38388;&#30340;&#33258;&#27965;&#32422;&#26463;&#26465;&#20214;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#35201;&#27714;&#20808;&#39564;&#27169;&#22411;&#23545;&#28508;&#22312;&#29366;&#24577;&#30340;&#21160;&#24577;&#36827;&#34892;&#25551;&#36848;&#65292;&#24182;&#20197;&#26576;&#31181;&#32676;&#21464;&#25442;&#23545;&#20854;&#36827;&#34892;&#31561;&#21464;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29289;&#29702;&#23545;&#31216;&#24615;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20174;&#26410;&#26631;&#35760;&#30340;&#21333;&#22768;&#36947;&#38899;&#20048;&#38899;&#39057;&#20013;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#38899;&#39640;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#23398;&#20064;&#19968;&#20010;&#19977;&#32500;&#31515;&#21345;&#23572;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable representation learning has been playing a key role in creative intelligent systems. In the music domain, current learning algorithms can successfully learn various features such as pitch, timbre, chord, texture, etc. However, most methods rely heavily on music domain knowledge. It remains an open question what general computational principles give rise to interpretable representations, especially low-dim factors that agree with human perception. In this study, we take inspiration from modern physics and use physical symmetry as a self-consistency constraint for the latent space. Specifically, it requires the prior model that characterises the dynamics of the latent states to be equivariant with respect to certain group transformations. We show that physical symmetry leads the model to learn a linear pitch factor from unlabelled monophonic music audio in a self-supervised fashion. In addition, the same methodology can be applied to computer vision, learning a 3D Cartesian
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#33021;&#37327;&#27169;&#22411;&#21644;&#32852;&#24819;&#35760;&#24518;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8212;&#8212;&#33021;&#37327;&#21464;&#25442;&#22120;&#65288;ET&#65289;&#65292;&#23427;&#36890;&#36807;&#29305;&#24847;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#23618;&#20197;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#65292;&#29992;&#20110;&#34920;&#31034;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07253</link><description>&lt;p&gt;
&#33021;&#37327;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Energy Transformer. (arXiv:2302.07253v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#33021;&#37327;&#27169;&#22411;&#21644;&#32852;&#24819;&#35760;&#24518;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#8212;&#8212;&#33021;&#37327;&#21464;&#25442;&#22120;&#65288;ET&#65289;&#65292;&#23427;&#36890;&#36807;&#29305;&#24847;&#35774;&#35745;&#30340;&#27880;&#24847;&#21147;&#23618;&#20197;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#65292;&#29992;&#20110;&#34920;&#31034;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#33021;&#37327;&#27169;&#22411;&#21644;&#32852;&#24819;&#35760;&#24518;&#19977;&#31181;&#28508;&#21147;&#24040;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#32467;&#21512;&#36215;&#26469;&#12290;&#27880;&#24847;&#21147;&#26159;&#25512;&#21160;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#30340;&#21160;&#21147;&#28304;&#65292;&#20294;&#23427;&#32570;&#20047;&#26126;&#30830;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#33021;&#37327;&#27169;&#22411;&#20801;&#35768;&#22312;&#21028;&#21035;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#37319;&#29992;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#20294;&#33021;&#37327;&#20989;&#25968;&#30340;&#35774;&#35745;&#24182;&#19981;&#30452;&#35266;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31264;&#23494;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#25110;&#29616;&#20195;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#19988;&#20801;&#35768;&#33021;&#37327;&#20989;&#25968;&#30340;&#30452;&#35266;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#33021;&#37327;&#21464;&#25442;&#22120;&#65288;&#31616;&#31216;ET&#65289;&#65292;&#23427;&#20351;&#29992;&#19968;&#31995;&#21015;&#32463;&#36807;&#29305;&#24847;&#35774;&#35745;&#20197;&#26368;&#23567;&#21270;&#29305;&#27530;&#35774;&#35745;&#30340;&#33021;&#37327;&#20989;&#25968;&#30340;&#27880;&#24847;&#21147;&#23618;&#65292;&#35813;&#20989;&#25968;&#36127;&#36131;&#34920;&#31034;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ET&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#34917;&#20840;&#25216;&#26415;&#25506;&#32034;&#20102;&#23427;&#30340;&#32463;&#39564;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work combines aspects of three promising paradigms in machine learning, namely, attention mechanism, energy-based models, and associative memory. Attention is the power-house driving modern deep learning successes, but it lacks clear theoretical foundations. Energy-based models allow a principled approach to discriminative and generative tasks, but the design of the energy functional is not straightforward. At the same time, Dense Associative Memory models or Modern Hopfield Networks have a well-established theoretical foundation, and allow an intuitive design of the energy function. We propose a novel architecture, called the Energy Transformer (or ET for short), that uses a sequence of attention layers that are purposely designed to minimize a specifically engineered energy function, which is responsible for representing the relationships between the tokens. In this work, we introduce the theoretical foundations of ET, explore its empirical capabilities using the image completion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#26041;&#27861;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26089;&#20572;&#20934;&#21017;&#26469;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;</title><link>http://arxiv.org/abs/2302.04841</link><description>&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#36890;&#36807;&#36319;&#36394;&#30446;&#26631;&#21160;&#24577;&#26469;&#23454;&#29616;&#26356;&#24555;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics. (arXiv:2302.04841v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#26041;&#27861;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26089;&#20572;&#20934;&#21017;&#26469;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20195;&#34920;&#20102;&#22270;&#20687;&#21512;&#25104;&#30340;&#19979;&#19968;&#20010;&#21457;&#23637;&#38454;&#27573;&#65292;&#20026;&#23454;&#29616;&#28789;&#27963;&#20294;&#31934;&#32454;&#30340;&#25511;&#21046;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#26159;&#23558;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24555;&#36895;&#36866;&#24212;&#21040;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#25110;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39640;&#25928;&#30340;&#36866;&#24212;&#26041;&#27861;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#38477;&#20302;&#20102;&#30740;&#31350;&#23454;&#39564;&#30340;&#36895;&#24230;&#65292;&#24182;&#28040;&#32791;&#20102;&#36807;&#22810;&#30340;GPU&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#26041;&#27861;&#65288;&#22914;&#25991;&#26412;&#20498;&#36716;&#25110;&#26790;&#24187;&#23567;&#23627;&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#26088;&#22312;&#21152;&#36895;&#23427;&#20204;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22810;&#25968;&#27010;&#24565;&#22312;&#26089;&#26399;&#38454;&#27573;&#23601;&#24050;&#32463;&#23398;&#20064;&#21040;&#20102;&#65292;&#24182;&#19988;&#36136;&#37327;&#22312;&#21518;&#26399;&#27809;&#26377;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;&#26159;&#26631;&#20934;&#30340;&#27169;&#22411;&#25910;&#25947;&#25351;&#26631;&#26410;&#33021;&#25351;&#31034;&#36825;&#19968;&#28857;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#26089;&#20572;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#21482;&#38656;&#35201;&#22312;&#25152;&#26377;&#35757;&#32451;&#36845;&#20195;&#20013;&#23545;&#19968;&#32452;&#22266;&#23450;&#36755;&#20837;&#35745;&#31639;&#24120;&#35268;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#23545;...&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation models represent the next step of evolution in image synthesis, offering a natural way to achieve flexible yet fine-grained control over the result. One emerging area of research is the fast adaptation of large text-to-image models to smaller datasets or new visual concepts. However, many efficient methods of adaptation have a long training time, which limits their practical applications, slows down research experiments, and spends excessive GPU resources. In this work, we study the training dynamics of popular text-to-image personalization methods (such as Textual Inversion or DreamBooth), aiming to speed them up. We observe that most concepts are learned at early stages and do not improve in quality later, but standard model convergence metrics fail to indicate that. Instead, we propose a simple drop-in early stopping criterion that only requires computing the regular training objective on a fixed set of inputs for all training iterations. Our experiments on 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#29992;&#25143;&#30340;&#25919;&#31574;&#26469;&#23398;&#20064;&#29992;&#25143;&#23545;&#30446;&#26631;&#30340;&#20559;&#22909;&#65292;&#24182;&#26681;&#25454;&#20559;&#22909;&#35745;&#31639;&#20986;&#36817;&#20284;&#26368;&#20248;&#30340;&#20010;&#24615;&#21270;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2302.03805</link><description>&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#21453;&#39304;&#24341;&#23548;&#20010;&#24615;&#21270;&#22810;&#30446;&#26631;&#20915;&#31574;&#20013;&#30340;&#29992;&#25143;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Eliciting User Preferences for Personalized Multi-Objective Decision Making through Comparative Feedback. (arXiv:2302.03805v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#29992;&#25143;&#30340;&#25919;&#31574;&#26469;&#23398;&#20064;&#29992;&#25143;&#23545;&#30446;&#26631;&#30340;&#20559;&#22909;&#65292;&#24182;&#26681;&#25454;&#20559;&#22909;&#35745;&#31639;&#20986;&#36817;&#20284;&#26368;&#20248;&#30340;&#20010;&#24615;&#21270;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32463;&#20856;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#25919;&#31574;&#26159;&#26681;&#25454;&#26631;&#37327;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#32780;&#25152;&#26377;&#26368;&#20248;&#25919;&#31574;&#22312;&#39044;&#26399;&#22238;&#25253;&#26041;&#38754;&#26159;&#30456;&#21516;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#28041;&#21450;&#21040;&#24179;&#34913;&#22810;&#20010;&#12289;&#26377;&#26102;&#26159;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#30340;&#30456;&#23545;&#20248;&#20808;&#32423;&#20250;&#26681;&#25454;&#27599;&#20010;&#29992;&#25143;&#30340;&#20559;&#22909;&#32780;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#23545;&#19968;&#20010;&#29992;&#25143;&#32780;&#35328;&#26368;&#20248;&#30340;&#25919;&#31574;&#21487;&#33021;&#23545;&#21478;&#19968;&#20010;&#29992;&#25143;&#32780;&#35328;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20915;&#31574;&#26694;&#26550;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#29992;&#25143;&#23545;&#30446;&#26631;&#30340;&#20559;&#22909;&#65292;&#20854;&#20013;&#20559;&#22909;&#26159;&#36890;&#36807;&#25919;&#31574;&#27604;&#36739;&#26469;&#23398;&#20064;&#30340;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19968;&#20010;&#20855;&#26377;&#21521;&#37327;&#20540;&#22870;&#21169;&#20989;&#25968;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#32452;&#25104;&#65292;&#27599;&#20010;&#29992;&#25143;&#37117;&#26377;&#19968;&#20010;&#26410;&#30693;&#30340;&#20559;&#22909;&#21521;&#37327;&#65292;&#34920;&#31034;&#27599;&#20010;&#30446;&#26631;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#30446;&#26631;&#26159;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#32473;&#23450;&#29992;&#25143;&#30340;&#36817;&#20284;&#26368;&#20248;&#25919;&#31574;&#12290;&#25105;&#20204;&#32771;&#34385;&#20004;&#31181;&#29992;&#25143;&#21453;&#39304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classic reinforcement learning (RL) and decision making problems, policies are evaluated with respect to a scalar reward function, and all optimal policies are the same with regards to their expected return. However, many real-world problems involve balancing multiple, sometimes conflicting, objectives whose relative priority will vary according to the preferences of each user. Consequently, a policy that is optimal for one user might be sub-optimal for another. In this work, we propose a multi-objective decision making framework that accommodates different user preferences over objectives, where preferences are learned via policy comparisons. Our model consists of a Markov decision process with a vector-valued reward function, with each user having an unknown preference vector that expresses the relative importance of each objective. The goal is to efficiently compute a near-optimal policy for a given user. We consider two user feedback models. We first address the case where a use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2302.03693</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#30340;&#27010;&#24565;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#26576;&#31181;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#65288;&#25110;&#26041;&#21521;&#65289;&#30340;&#24605;&#24819;&#65292;&#24182;&#24320;&#21457;&#20102;&#36825;&#20010;&#24605;&#24819;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#12290;&#21033;&#29992;&#36825;&#20010;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#19968;&#20010;&#33258;&#28982;&#30340;&#34920;&#31034;&#36873;&#25321;&#20855;&#26377;&#36825;&#31181;&#24615;&#36136;&#65292;&#24182;&#19988;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23545;&#34920;&#31034;&#30340;&#20195;&#25968;&#25805;&#20316;&#26469;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#30340;&#31034;&#20363;&#20013;&#28436;&#31034;&#20102;&#36825;&#20010;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25153;&#24179;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21518;&#39564;&#25512;&#35770;&#20013;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#25153;&#24179;&#21270;&#24615;&#36136;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.02713</link><description>&lt;p&gt;
&#25153;&#24179;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Flat Seeking Bayesian Neural Networks. (arXiv:2302.02713v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25153;&#24179;&#21270;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21518;&#39564;&#25512;&#35770;&#20013;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#25153;&#24179;&#21270;&#24615;&#36136;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36890;&#36807;&#23545;&#27169;&#22411;&#21442;&#25968;&#26045;&#21152;&#20808;&#39564;&#20998;&#24067;&#24182;&#22522;&#20110;&#35266;&#27979;&#25968;&#25454;&#25512;&#26029;&#21518;&#39564;&#20998;&#24067;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#27010;&#29575;&#35299;&#37322;&#12290;&#20174;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#27169;&#22411;&#21487;&#29992;&#20110;&#25552;&#20379;&#38598;&#25104;&#39044;&#27979;&#21644;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#20855;&#26377;&#36739;&#20302;&#23574;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21518;&#39564;&#25512;&#35770;&#23545;&#20110;&#23574;&#24230;/&#25153;&#24179;&#21270;&#24182;&#19981;&#20855;&#22791;&#24847;&#35782;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#20174;&#20854;&#37319;&#26679;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#23574;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#25153;&#24179;&#21270;&#23545;&#21518;&#39564;&#36827;&#34892;&#20102;&#29702;&#35770;&#12289;&#36125;&#21494;&#26031;&#35774;&#23450;&#21644;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20174;&#25153;&#24179;&#21270;&#24847;&#20041;&#19978;&#25512;&#26029;&#30340;&#27169;&#22411;&#20197;&#21450;&#20272;&#35745;&#35813;&#25153;&#24179;&#21270;&#24847;&#20041;&#21518;&#39564;&#30340;&#26368;&#20339;&#36817;&#20284;&#21518;&#39564;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25153;&#24179;&#21270;&#24615;&#36136;&#65292;&#22240;&#27492;&#21487;&#33021;&#20855;&#26377;&#26356;&#39640;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#22806;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks (BNNs) provide a probabilistic interpretation for deep learning models by imposing a prior distribution over model parameters and inferring a posterior distribution based on observed data. The model sampled from the posterior distribution can be used for providing ensemble predictions and quantifying prediction uncertainty. It is well-known that deep learning models with lower sharpness have better generalization ability. However, existing posterior inferences are not aware of sharpness/flatness in terms of formulation, possibly leading to high sharpness for the models sampled from them. In this paper, we develop theories, the Bayesian setting, and the variational inference approach for the sharpness-aware posterior. Specifically, the models sampled from our sharpness-aware posterior, and the optimal approximate posterior estimating this sharpness-aware posterior, have better flatness, hence possibly possessing higher generalization ability. We conduct experime
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35268;&#32422;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22810;&#25209;&#27425;&#31639;&#27861;&#36716;&#21270;&#20026;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#19981;&#20165;&#22312;&#36172;&#21338;&#26426;&#12289;&#34920;&#26684;&#22411;MDPs&#21644;&#34920;&#26684;&#22411;MGs&#26041;&#38754;&#21462;&#24471;&#20102;&#19982;&#29616;&#26377;&#32467;&#26524;&#30456;&#21305;&#37197;&#25110;&#25913;&#36827;&#30340;&#25104;&#26524;&#65292;&#36824;&#39318;&#27425;&#23545;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#19982;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.01477</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#32422;&#30340;&#24310;&#36831;&#21453;&#39304;&#39034;&#24207;&#20915;&#31574;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Reduction-based Framework for Sequential Decision Making with Delayed Feedback. (arXiv:2302.01477v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01477
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35268;&#32422;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#20219;&#20309;&#22810;&#25209;&#27425;&#31639;&#27861;&#36716;&#21270;&#20026;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#25105;&#20204;&#19981;&#20165;&#22312;&#36172;&#21338;&#26426;&#12289;&#34920;&#26684;&#22411;MDPs&#21644;&#34920;&#26684;&#22411;MGs&#26041;&#38754;&#21462;&#24471;&#20102;&#19982;&#29616;&#26377;&#32467;&#26524;&#30456;&#21305;&#37197;&#25110;&#25913;&#36827;&#30340;&#25104;&#26524;&#65292;&#36824;&#39318;&#27425;&#23545;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#19982;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#33324;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#21453;&#39304;&#65292;&#21253;&#25324;&#36172;&#21338;&#26426;&#38382;&#39064;&#12289;&#21333;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#65288;MGs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35268;&#32422;&#30340;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#22810;&#25209;&#27425;&#31639;&#27861;&#36716;&#21270;&#20026;&#33021;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#38543;&#26426;&#24310;&#36831;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#22810;&#25209;&#27425;&#31639;&#27861;&#25554;&#20837;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#31034;&#20363;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#21305;&#37197;&#25110;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#36172;&#21338;&#26426;&#12289;&#34920;&#26684;&#22411;MDPs&#21644;&#34920;&#26684;&#22411;MGs&#30340;&#32467;&#26524;&#65292;&#36824;&#39318;&#27425;&#23545;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#19982;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#20026;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24310;&#36831;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#22871;&#23436;&#25972;&#30340;&#23574;&#38160;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study stochastic delayed feedback in general multi-agent sequential decision making, which includes bandits, single-agent Markov decision processes (MDPs), and Markov games (MGs). We propose a novel reduction-based framework, which turns any multi-batched algorithm for sequential decision making with instantaneous feedback into a sample-efficient algorithm that can handle stochastic delays in sequential decision making. By plugging different multi-batched algorithms into our framework, we provide several examples demonstrating that our framework not only matches or improves existing results for bandits, tabular MDPs, and tabular MGs, but also provides the first line of studies on delays in sequential decision making with function approximation. In summary, we provide a complete set of sharp results for multi-agent sequential decision making with delayed feedback.
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#32047;&#35745;&#30446;&#26631;&#20540;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#32467;&#26524;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.00997</link><description>&lt;p&gt;
&#21463;&#38480;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#65306;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33719;&#24471;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning. (arXiv:2302.00997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00997
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#32047;&#35745;&#30446;&#26631;&#20540;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#32467;&#26524;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20855;&#26377;&#26377;&#38480;&#30340;$T$&#26399;&#32039;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#20808;&#20316;&#20986;&#31532;&#19968;&#38454;&#27573;&#20915;&#31574;&#65292;&#28982;&#21518;&#35266;&#23519;&#27169;&#22411;&#21442;&#25968;&#30340;&#23454;&#29616;&#65292;&#26368;&#21518;&#20174;&#21462;&#20915;&#20110;&#31532;&#19968;&#38454;&#27573;&#20915;&#31574;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#34892;&#38598;&#20013;&#20570;&#20986;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#12290;&#25105;&#20204;&#26088;&#22312;&#26368;&#23567;&#21270;&#32047;&#35745;&#30446;&#26631;&#20540;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#30340;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#25105;&#20204;&#37117;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;&#24403;&#27599;&#20010;&#26102;&#38388;&#27573;&#30340;&#27169;&#22411;&#21442;&#25968;&#37117;&#26159;&#20174;&#30456;&#21516;&#30340;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#26102;&#20505;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;$O&#65288;\sqrt{T}&#65289;$&#36951;&#25022;&#30028;&#65292;&#36825;&#27604;&#20043;&#21069;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#30028;&#26377;&#25152;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#21487;&#20197;&#25269;&#25239;&#27169;&#22411;&#30340;&#25932;&#23545;&#24615;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an online two-stage stochastic optimization with long-term constraints over a finite horizon of $T$ periods. At each period, we take the first-stage action, observe a model parameter realization and then take the second-stage action from a feasible set that depends both on the first-stage decision and the model parameter. We aim to minimize the cumulative objective value while guaranteeing that the long-term average second-stage decision belongs to a set. We develop online algorithms for the online two-stage problem from adversarial learning algorithms. Also, the regret bound of our algorithm cam be reduced to the regret bound of embedded adversarial learning algorithms. Based on our framework, we obtain new results under various settings. When the model parameter at each period is drawn from identical distributions, we derive state-of-art $O(\sqrt{T})$ regret that improves previous bounds under special cases. Our algorithm is also robust to adversarial corruptions of model
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24191;&#20041;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;CFM&#65289;&#30340;&#25216;&#26415;&#65292;&#22312;&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65288;CNFs&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#24341;&#20837;&#20102;&#26368;&#20248;&#20256;&#36755;CFM&#65288;OT-CFM&#65289;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#20197;&#26080;&#27169;&#25311;&#26041;&#24335;&#35745;&#31639;&#21160;&#24577;OT&#65292;&#21152;&#36895;&#20102;&#25512;&#26029;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.00482</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#25209;&#37327;&#20248;&#21270;&#20256;&#36755;&#25913;&#36827;&#21644;&#27867;&#21270;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving and generalizing flow-based generative models with minibatch optimal transport. (arXiv:2302.00482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00482
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24191;&#20041;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;CFM&#65289;&#30340;&#25216;&#26415;&#65292;&#22312;&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65288;CNFs&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#26080;&#38656;&#27169;&#25311;&#35757;&#32451;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#24341;&#20837;&#20102;&#26368;&#20248;&#20256;&#36755;CFM&#65288;OT-CFM&#65289;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#20197;&#26080;&#27169;&#25311;&#26041;&#24335;&#35745;&#31639;&#21160;&#24577;OT&#65292;&#21152;&#36895;&#20102;&#25512;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65288;CNFs&#65289;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#20854;&#22522;&#20110;&#27169;&#25311;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#23384;&#22312;&#23616;&#38480;&#24615;&#32780;&#21463;&#21040;&#32422;&#26463;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#24191;&#20041;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;CFM&#65289;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;CNFs&#30340;&#26080;&#27169;&#25311;&#35757;&#32451;&#30446;&#26631;&#30340;&#38598;&#21512;&#12290;CFM&#20855;&#26377;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#20013;&#29992;&#20110;&#35757;&#32451;&#38543;&#26426;&#27969;&#30340;&#31283;&#23450;&#22238;&#24402;&#30446;&#26631;&#65292;&#20294;&#21516;&#26102;&#20139;&#26377;&#30830;&#23450;&#24615;&#27969;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#26029;&#12290;&#19982;&#25193;&#25955;&#27169;&#22411;&#21644;&#20043;&#21069;&#30340;CNF&#35757;&#32451;&#31639;&#27861;&#30456;&#27604;&#65292;CFM&#19981;&#38656;&#35201;&#28304;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#20063;&#19981;&#38656;&#35201;&#23545;&#20854;&#23494;&#24230;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#30340;&#19968;&#31181;&#21464;&#20307;&#26159;&#26368;&#20248;&#20256;&#36755;CFM&#65288;OT-CFM&#65289;&#65292;&#23427;&#21019;&#24314;&#20102;&#26356;&#31616;&#21333;&#30340;&#27969;&#65292;&#26356;&#23481;&#26131;&#35757;&#32451;&#65292;&#24182;&#19988;&#23548;&#33268;&#26356;&#24555;&#30340;&#25512;&#26029;&#65292;&#22914;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#25152;&#31034;&#12290;&#27492;&#22806;&#65292;OT-CFM&#26159;&#31532;&#19968;&#31181;&#20197;&#26080;&#27169;&#25311;&#26041;&#24335;&#35745;&#31639;&#21160;&#24577;OT&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;CFM&#35757;&#32451;CNFs&#21487;&#20197;&#25913;&#36827;&#21508;&#31181;&#26465;&#20214;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, OT-CFM is the first method to compute dynamic OT in a simulation-free way. Training CNFs with CFM improves results on a variety of conditional and u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#21452;&#25511;&#21046;&#27169;&#22411;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#30340;&#20027;&#21160;&#19981;&#30830;&#23450;&#24615;&#20943;&#23567;&#12290;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#36817;&#20284;&#65292;&#35299;&#20915;&#20102;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00171</link><description>&lt;p&gt;
&#20027;&#21160;&#19981;&#30830;&#23450;&#24615;&#20943;&#23567;&#30340;&#23433;&#20840;&#39640;&#25928;&#20132;&#20114;&#35268;&#21010;&#65306;&#19968;&#31181;&#27880;&#37325;&#20445;&#25252;&#30340;&#21452;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Active Uncertainty Reduction for Safe and Efficient Interaction Planning: A Shielding-Aware Dual Control Approach. (arXiv:2302.00171v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#21452;&#25511;&#21046;&#27169;&#22411;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#30340;&#20027;&#21160;&#19981;&#30830;&#23450;&#24615;&#20943;&#23567;&#12290;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#36817;&#20284;&#65292;&#35299;&#20915;&#20102;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20182;&#20154;&#34892;&#20026;&#23545;&#20110;&#20132;&#20114;&#24335;&#26426;&#22120;&#20154;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#24448;&#24448;&#26080;&#27861;&#33719;&#24471;&#20851;&#38190;&#20449;&#24687;&#65292;&#22914;&#20854;&#20182;&#20195;&#29702;&#30340;&#30446;&#26631;&#12289;&#27880;&#24847;&#21147;&#21644;&#21512;&#20316;&#24847;&#24895;&#12290;&#21452;&#25511;&#21046;&#29702;&#35770;&#36890;&#36807;&#23558;&#39044;&#27979;&#27169;&#22411;&#30340;&#26410;&#30693;&#21442;&#25968;&#35270;&#20026;&#38543;&#26426;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#22312;&#31995;&#32479;&#36816;&#34892;&#36807;&#31243;&#20013;&#20351;&#29992;&#25910;&#38598;&#30340;&#20449;&#24687;&#25512;&#26029;&#20854;&#20540;&#65292;&#20197;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38544;&#24335;&#21452;&#25511;&#21046;&#27169;&#22411;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#30340;&#20027;&#21160;&#19981;&#30830;&#23450;&#24615;&#20943;&#23567;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#37319;&#26679;&#30340;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#30340;&#36817;&#20284;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#21487;&#20197;&#26041;&#20415;&#35299;&#20915;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to accurately predict others' behavior is central to the safety and efficiency of interactive robotics. Unfortunately, robots often lack access to key information on which these predictions may hinge, such as other agents' goals, attention, and willingness to cooperate. Dual control theory addresses this challenge by treating unknown parameters of a predictive model as stochastic hidden states and inferring their values at runtime using information gathered during system operation. While able to optimally and automatically trade off exploration and exploitation, dual control is computationally intractable for general interactive motion planning. In this paper, we present a novel algorithmic approach to enable active uncertainty reduction for interactive motion planning based on the implicit dual control paradigm. Our approach relies on sampling-based approximation of stochastic dynamic programming, leading to a model predictive control problem that can be readily solved by 
&lt;/p&gt;</description></item><item><title>LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.13126</link><description>&lt;p&gt;
LEXTREME&#65306;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13126
&lt;/p&gt;
&lt;p&gt;
LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;transformer&#26550;&#26500;&#30340;&#26174;&#33879;&#36827;&#23637;&#25512;&#21160;&#19979;&#65292;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#22686;&#38271;&#12290;&#20026;&#20102;&#34913;&#37327;&#36827;&#23637;&#65292;&#31934;&#24515;&#31574;&#21010;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20934;&#21482;&#33021;&#22788;&#29702;&#33521;&#25991;&#65292;&#32780;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23578;&#26410;&#26377;&#22810;&#35821;&#35328;&#22522;&#20934;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#24050;&#32463;&#39281;&#21644;&#65292;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26368;&#20339;&#20154;&#31867;&#65292;&#24182;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25991;&#29486;&#65292;&#24182;&#36873;&#25321;&#20102;11&#20010;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;LEXTREME&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32508;&#21512;&#35780;&#20998;&#65292;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#38598;&#65292;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#12290;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#32508;&#21512;&#35780;&#20998;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#34920;&#26126;LEXTREME&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#20026;&#25913;&#36827;&#30041;&#19979;&#20102;&#20805;&#36275;&#31354;&#38388;&#12290;&#20026;&#20102;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20351;&#29992;&#65292;&#25105;&#20204;&#23558;LEXTREME&#19982;&#25152;&#26377;&#25968;&#25454;&#19968;&#36215;&#21457;&#24067;&#22312;huggingface&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#24517;&#35201;&#21644;&#20805;&#20998;&#22240;&#26524;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#19982;&#24863;&#20852;&#36259;&#32467;&#26524;&#30456;&#20851;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2301.12389</link><description>&lt;p&gt;
&#23398;&#20064;&#24517;&#35201;&#21644;&#20805;&#20998;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
On Learning Necessary and Sufficient Causal Graphs. (arXiv:2301.12389v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#24517;&#35201;&#21644;&#20805;&#20998;&#22240;&#26524;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#19982;&#24863;&#20852;&#36259;&#32467;&#26524;&#30456;&#20851;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#38761;&#21629;&#28608;&#21457;&#20102;&#23545;&#21508;&#20010;&#39046;&#22495;&#20013;&#22797;&#26434;&#20851;&#31995;&#30340;&#20852;&#36259;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26088;&#22312;&#22312;&#22797;&#26434;&#30340;&#22823;&#35268;&#27169;&#22270;&#20013;&#21457;&#29616;&#25152;&#26377;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#22270;&#20013;&#20165;&#26377;&#30340;&#19968;&#23567;&#37096;&#20998;&#21464;&#37327;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22823;&#37327;&#38169;&#35823;&#21457;&#29616;&#30340;&#34394;&#20551;&#21464;&#37327;&#65292;&#36825;&#20123;&#34394;&#20551;&#21464;&#37327;&#19982;&#30446;&#26631;&#32467;&#26524;&#39640;&#24230;&#30456;&#20851;&#65292;&#20294;&#23545;&#30446;&#26631;&#32467;&#26524;&#27809;&#26377;&#22240;&#26524;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#24517;&#35201;&#21644;&#20805;&#20998;&#22240;&#26524;&#22270; (NSCG) &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#30001;&#19982;&#24863;&#20852;&#36259;&#32467;&#26524;&#22240;&#26524;&#30456;&#20851;&#30340;&#21464;&#37327;&#32452;&#25104;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#22240;&#26524;&#29305;&#24449;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#22240;&#26524;&#27010;&#29575;&#31995;&#32479;&#22320;&#35780;&#20272;&#22240;&#26524;&#22270;&#20013;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#24110;&#21161;&#25105;&#20204;&#30830;&#23450;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30456;&#20851;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal revolution has stimulated interest in understanding complex relationships in various fields. Most of the existing methods aim to discover causal relationships among all variables within a complex large-scale graph. However, in practice, only a small subset of variables in the graph are relevant to the outcomes of interest. Consequently, causal estimation with the full causal graph -- particularly given limited data -- could lead to numerous falsely discovered, spurious variables that exhibit high correlation with, but exert no causal impact on, the target outcome. In this paper, we propose learning a class of necessary and sufficient causal graphs (NSCG) that exclusively comprises causally relevant variables for an outcome of interest, which we term causal features. The key idea is to employ probabilities of causation to systematically evaluate the importance of features in the causal graph, allowing us to identify a subgraph relevant to the outcome of interest. To learn NSC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Synthesis Model Bank&#65288;SMB&#65289;&#26469;&#22788;&#29702;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20154;&#29289;&#20877;&#35782;&#21035;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;SMB&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#39532;&#27663;&#36317;&#31163;&#30697;&#38453;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#20809;&#29031;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09702</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#20687;&#21512;&#25104;&#36827;&#34892;&#20809;&#29031;&#21464;&#21270;&#26657;&#27491;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20154;&#29289;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2301.09702v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Synthesis Model Bank&#65288;SMB&#65289;&#26469;&#22788;&#29702;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20154;&#29289;&#20877;&#35782;&#21035;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;SMB&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#39532;&#27663;&#36317;&#31163;&#30697;&#38453;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#20809;&#29031;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#20154;&#29289;&#20877;&#35782;&#21035;&#26088;&#22312;&#20174;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#22270;&#20687;&#20013;&#23398;&#20064;&#36523;&#20221;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#22270;&#20687;&#12290;&#35768;&#22810;&#26080;&#30417;&#30563;&#20877;&#35782;&#21035;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#23427;&#20204;&#22312;&#22823;&#30340;&#39046;&#22495;&#21464;&#21270;&#65288;&#22914;&#20809;&#29031;&#12289;&#35270;&#35282;&#21644;&#36974;&#25377;&#65289;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#27169;&#22411;&#24211;&#65288;SMB&#65289;&#26469;&#22788;&#29702;&#26080;&#30417;&#30563;&#20154;&#29289;&#20877;&#35782;&#21035;&#20013;&#30340;&#20809;&#29031;&#21464;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;SMB&#21253;&#25324;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#30340;&#22810;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#29992;&#20110;&#36317;&#31163;&#24230;&#37327;&#30340;&#39532;&#27663;&#36317;&#31163;&#30697;&#38453;&#12290;&#23427;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#20809;&#29031;&#26465;&#20214;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20351;&#24471;SMB&#23545;&#20809;&#29031;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#37327;&#21270;&#20809;&#29031;&#24378;&#24230;&#24182;&#25552;&#39640;&#21512;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;GAN&#30340;&#26032;&#22411;&#19977;&#32500;&#34394;&#25311;&#20154;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#22270;&#20687;&#21512;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to learn identity information from labeled images in source domains and apply it to unlabeled images in a target domain. One major issue with many unsupervised re-identification methods is that they do not perform well relative to large domain variations such as illumination, viewpoint, and occlusions. In this paper, we propose a Synthesis Model Bank (SMB) to deal with illumination variation in unsupervised person re-ID. The proposed SMB consists of several convolutional neural networks (CNN) for feature extraction and Mahalanobis matrices for distance metrics. They are trained using synthetic data with different illumination conditions such that their synergistic effect makes the SMB robust against illumination variation. To better quantify the illumination intensity and improve the quality of synthetic images, we introduce a new 3D virtual-human dataset for GAN-based image synthesis. From our experiments, the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#25509;&#35302;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#27169;&#24335;&#26469;&#26816;&#27979;&#21628;&#21560;&#24322;&#24120;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#25968;&#25454;&#30340;&#21628;&#21560;&#27874;&#24418;&#20002;&#24323;&#24178;&#25200;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#26080;&#21019;&#30340;&#20154;&#20307;&#21628;&#21560;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.03713</link><description>&lt;p&gt;
&#26080;&#25509;&#35302;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#30340;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Non-contact Respiratory Anomaly Detection using Infrared Light-wave Sensing. (arXiv:2301.03713v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#25509;&#35302;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#25216;&#26415;&#65292;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#27169;&#24335;&#26469;&#26816;&#27979;&#21628;&#21560;&#24322;&#24120;&#65292;&#24182;&#19988;&#36890;&#36807;&#39564;&#35777;&#25968;&#25454;&#30340;&#21628;&#21560;&#27874;&#24418;&#20002;&#24323;&#24178;&#25200;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#26080;&#21019;&#30340;&#20154;&#20307;&#21628;&#21560;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#30340;&#21628;&#21560;&#39057;&#29575;&#21644;&#21628;&#21560;&#27169;&#24335;&#20256;&#36798;&#20102;&#20851;&#20110;&#20027;&#20307;&#30340;&#36523;&#20307;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#24322;&#24120;&#21628;&#21560;&#21487;&#33021;&#34920;&#26126;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#20351;&#29992;&#38750;&#30456;&#24178;&#32418;&#22806;&#20809;&#30340;&#26080;&#32447;&#20809;&#27874;&#20256;&#24863;&#65288;LWS&#65289;&#22312;&#19981;&#24341;&#36215;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#31034;&#20102;&#23433;&#20840;&#12289;&#38544;&#34109;&#12289;&#39640;&#25928;&#21644;&#26080;&#21019;&#30340;&#20154;&#20307;&#21628;&#21560;&#30417;&#27979;&#30340;&#28508;&#21147;&#12290;&#21628;&#21560;&#30417;&#27979;&#31995;&#32479;&#38656;&#35201;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#21628;&#21560;&#27169;&#24335;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#35782;&#21035;&#21628;&#21560;&#24322;&#24120;&#12290;&#35813;&#31995;&#32479;&#36824;&#24517;&#39035;&#39564;&#35777;&#25152;&#25910;&#38598;&#30340;&#25968;&#25454;&#26159;&#21542;&#20026;&#21628;&#21560;&#27874;&#24418;&#65292;&#20002;&#24323;&#30001;&#22806;&#37096;&#24178;&#25200;&#12289;&#29992;&#25143;&#31227;&#21160;&#25110;&#31995;&#32479;&#25925;&#38556;&#24341;&#36215;&#30340;&#20219;&#20309;&#38169;&#35823;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#27169;&#25311;&#20154;&#31867;&#21628;&#21560;&#27169;&#24335;&#30340;&#26426;&#22120;&#20154;&#65292;&#27169;&#25311;&#20102;&#27491;&#24120;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#24322;&#24120;&#21628;&#21560;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#32418;&#22806;&#20809;&#27874;&#20256;&#24863;&#25216;&#26415;&#25910;&#38598;&#20102;&#26102;&#38388;&#24207;&#21015;&#21628;&#21560;&#25968;&#25454;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#21628;&#21560;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human respiratory rate and its pattern convey essential information about the physical and psychological states of the subject. Abnormal breathing can indicate fatal health issues leading to further diagnosis and treatment. Wireless light-wave sensing (LWS) using incoherent infrared light shows promise in safe, discreet, efficient, and non-invasive human breathing monitoring without raising privacy concerns. The respiration monitoring system needs to be trained on different types of breathing patterns to identify breathing anomalies.The system must also validate the collected data as a breathing waveform, discarding any faulty data caused by external interruption, user movement, or system malfunction. To address these needs, this study simulated normal and different types of abnormal respiration using a robot that mimics human breathing patterns. Then, time-series respiration data were collected using infrared light-wave sensing technology. Three machine learning algorithms, decision t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38750;&#21442;&#25968;&#23376;&#22270;&#21305;&#37197;&#26694;&#26550;MatchExplainer&#65292;&#21487;&#20197;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#27492;&#26694;&#26550;&#23558;&#30446;&#26631;&#22270;&#19982;&#20854;&#20182;&#23454;&#20363;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#33410;&#28857;&#23545;&#24212;&#30340;&#36317;&#31163;&#26469;&#37492;&#21035;&#26368;&#20851;&#38190;&#30340;&#32852;&#21512;&#23376;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#33539;&#24335;MatchDrop&#26469;&#35299;&#20915;&#35823;&#25253;&#37319;&#26679;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.02780</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#21442;&#25968;&#23376;&#22270;&#21305;&#37197;&#37325;&#26032;&#24605;&#32771;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching. (arXiv:2301.02780v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#38750;&#21442;&#25968;&#23376;&#22270;&#21305;&#37197;&#26694;&#26550;MatchExplainer&#65292;&#21487;&#20197;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#27492;&#26694;&#26550;&#23558;&#30446;&#26631;&#22270;&#19982;&#20854;&#20182;&#23454;&#20363;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#33410;&#28857;&#23545;&#24212;&#30340;&#36317;&#31163;&#26469;&#37492;&#21035;&#26368;&#20851;&#38190;&#30340;&#32852;&#21512;&#23376;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#33539;&#24335;MatchDrop&#26469;&#35299;&#20915;&#35823;&#25253;&#37319;&#26679;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;&#65306;&#8220;&#36755;&#20837;&#22270;&#30340;&#21738;&#19968;&#37096;&#20998;&#23545;&#39044;&#27979;&#26368;&#20026;&#20915;&#23450;&#24615;&#65311;&#8221;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#20854;&#26356;&#24378;&#22823;&#30340;&#35299;&#35835;&#40657;&#31665;&#65288;&#21363;&#30446;&#26631;GNNs&#65289;&#33021;&#21147;&#65292;&#21442;&#25968;&#21270;&#35299;&#37322;&#22120;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#22270;&#36890;&#24120;&#20849;&#20139;&#26576;&#20123;&#24120;&#35265;&#30340;&#27169;&#24335;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#23376;&#22270;&#21305;&#37197;&#26694;&#26550;MatchExplainer&#26469;&#25506;&#32034;&#35299;&#37322;&#24615;&#23376;&#22270;&#12290;&#23427;&#23558;&#30446;&#26631;&#22270;&#19982;&#20854;&#20182;&#30456;&#24212;&#23454;&#20363;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22522;&#20110;&#33410;&#28857;&#23545;&#24212;&#30340;&#36317;&#31163;&#26469;&#35782;&#21035;&#26368;&#20851;&#38190;&#30340;&#32852;&#21512;&#23376;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#29616;&#26377;&#30340;&#22270;&#37319;&#26679;&#25110;&#33410;&#28857;&#21024;&#38500;&#26041;&#27861;&#36890;&#24120;&#20250;&#36935;&#21040;&#35823;&#25253;&#37319;&#26679;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;MatchDrop&#30340;&#26032;&#22686;&#26041;&#26696;&#65292;&#23427;&#21033;&#29992;&#20102;MatchExplainer&#26469;&#20462;&#22797;&#22270;&#30340;&#26368;&#20449;&#24687;&#20016;&#23500;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of graph neural networks (GNNs) provokes the question about explainability: ``Which fraction of the input graph is the most determinant of the prediction?'' Particularly, parametric explainers prevail in existing approaches because of their more robust capability to decipher the black-box (i.e., target GNNs). In this paper, based on the observation that graphs typically share some common motif patterns, we propose a novel non-parametric subgraph matching framework, dubbed MatchExplainer, to explore explanatory subgraphs. It couples the target graph with other counterpart instances and identifies the most crucial joint substructure by minimizing the node corresponding-based distance. Moreover, we note that present graph sampling or node-dropping methods usually suffer from the false positive sampling problem. To alleviate this issue, we designed a new augmentation paradigm named MatchDrop. It takes advantage of MatchExplainer to fix the most informative portion of the graph 
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.06665</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#32508;&#36848;&#65306;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges. (arXiv:2211.06665v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26234;&#33021;&#20195;&#29702;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#20197;&#23454;&#29616;&#38271;&#26399;&#30446;&#26631;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#22797;&#20852;&#25512;&#21160;&#19979;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#24178;&#32467;&#26500;&#34987;&#26222;&#36941;&#35270;&#20026;&#40657;&#30418;&#23376;&#65292;&#38459;&#30861;&#20102;&#20174;&#19994;&#32773;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#20449;&#20219;&#21644;&#20351;&#29992;&#35757;&#32451;&#20195;&#29702;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#37327;&#30340;&#25991;&#29486;&#33268;&#21147;&#20110;&#25581;&#31034;&#26234;&#33021;&#20195;&#29702;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#25110;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#20808;&#21069;&#30340;&#24037;&#20316;&#26126;&#30830;&#22320;&#20998;&#20026;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) is a popular machine learning paradigm where intelligent agents interact with the environment to fulfill a long-term goal. Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great success over a wide spectrum of complex control tasks. Despite the encouraging results achieved, the deep neural network-based backbone is widely deemed as a black box that impedes practitioners to trust and employ trained agents in realistic scenarios where high security and reliability are essential. To alleviate this issue, a large volume of literature devoted to shedding light on the inner workings of the intelligent agents has been proposed, by constructing intrinsic interpretability or post-hoc explainability. In this survey, we provide a comprehensive review of existing works on eXplainable RL (XRL) and introduce a new taxonomy where prior works are clearly categorized into model-explaining, reward-explaining, state-explaining, and task-explaining methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#22810;&#36724;&#39640;&#26031;&#22270;&#24418;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#31232;&#30095;&#22270;&#24418;&#34920;&#31034;&#12290;&#30456;&#27604;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#27599;&#20010;&#36724;&#19978;&#20165;&#20351;&#29992;&#19968;&#27425;&#29305;&#24449;&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2211.02920</link><description>&lt;p&gt;
GmGM: &#19968;&#31181;&#24555;&#36895;&#30340;&#22810;&#36724;&#39640;&#26031;&#22270;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
GmGM: a Fast Multi-Axis Gaussian Graphical Model. (arXiv:2211.02920v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#22810;&#36724;&#39640;&#26031;&#22270;&#24418;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#31232;&#30095;&#22270;&#24418;&#34920;&#31034;&#12290;&#30456;&#27604;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#27599;&#20010;&#36724;&#19978;&#20165;&#20351;&#29992;&#19968;&#27425;&#29305;&#24449;&#20998;&#35299;&#65292;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#26031;&#22810;&#22270;&#24418;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#30697;&#38453;&#21644;&#24352;&#37327;&#21464;&#37327;&#25968;&#25454;&#30340;&#31232;&#30095;&#22270;&#24418;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#20849;&#20139;&#36724;&#30340;&#24352;&#37327;&#19978;&#30340;&#34920;&#31034;&#26469;&#25512;&#24191;&#35813;&#39046;&#22495;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#22914;&#22810;&#32452;&#23398;&#20013;&#36935;&#21040;&#30340;&#25968;&#25454;&#38598;&#65289;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#27599;&#20010;&#36724;&#19978;&#20165;&#20351;&#29992;&#19968;&#27425;&#29305;&#24449;&#20998;&#35299;&#65292;&#30456;&#23545;&#20110;&#38750;&#24191;&#20041;&#24773;&#20917;&#19979;&#30340;&#20808;&#21069;&#24037;&#20316;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#21253;&#25324;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#25968;&#25454;&#22312;&#20869;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#20116;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Gaussian multi-Graphical Model, a model to construct sparse graph representations of matrix- and tensor-variate data. We generalize prior work in this area by simultaneously learning this representation across several tensors that share axes, which is necessary to allow the analysis of multimodal datasets such as those encountered in multi-omics. Our algorithm uses only a single eigendecomposition per axis, achieving an order of magnitude speedup over prior work in the ungeneralized case. This allows the use of our methodology on large multi-modal datasets such as single-cell multi-omics data, which was challenging with previous approaches. We validate our model on synthetic data and five real-world datasets.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#29109;&#26368;&#20248;&#20256;&#36755;(EOT)&#35745;&#21010;&#65292;&#23427;&#20855;&#26377;&#31471;&#21040;&#31471;&#23398;&#20064;&#12289;&#24555;&#36895;&#25512;&#29702;&#21644;&#22788;&#29702;&#23567;&#20540;&#29109;&#27491;&#21017;&#21270;&#31995;&#25968;&#30340;&#20248;&#28857;&#65292;&#21487;&#22312;&#22823;&#35268;&#27169;EOT&#20219;&#21153;&#20013;&#21457;&#25381;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.01156</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#36807;&#31243;&#30340;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Entropic Neural Optimal Transport via Diffusion Processes. (arXiv:2211.01156v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01156
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#29109;&#26368;&#20248;&#20256;&#36755;(EOT)&#35745;&#21010;&#65292;&#23427;&#20855;&#26377;&#31471;&#21040;&#31471;&#23398;&#20064;&#12289;&#24555;&#36895;&#25512;&#29702;&#21644;&#22788;&#29702;&#23567;&#20540;&#29109;&#27491;&#21017;&#21270;&#31995;&#25968;&#30340;&#20248;&#28857;&#65292;&#21487;&#22312;&#22823;&#35268;&#27169;EOT&#20219;&#21153;&#20013;&#21457;&#25381;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#29109;&#26368;&#20248;&#20256;&#36755;(EOT)&#35745;&#21010;&#65292;&#36825;&#20123;&#20998;&#24067;&#21487;&#36890;&#36807;&#26679;&#26412;&#33719;&#24471;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#21160;&#24577;&#29256;&#26412;EOT&#30340;&#38797;&#28857;&#37325;&#26500;&#65292;&#21363;Schr&#246;dinger&#26725;&#38382;&#39064;&#12290;&#19982;&#22823;&#35268;&#27169;EOT&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#31471;&#21040;&#31471;&#30340;&#65292;&#30001;&#21333;&#20010;&#23398;&#20064;&#27493;&#39588;&#32452;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#20801;&#35768;&#22788;&#29702;&#29109;&#27491;&#21017;&#21270;&#31995;&#25968;&#30340;&#23567;&#20540;&#65292;&#36825;&#22312;&#26576;&#20123;&#23454;&#38469;&#24212;&#29992;&#38382;&#39064;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22823;&#35268;&#27169;EOT&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel neural algorithm for the fundamental problem of computing the entropic optimal transport (EOT) plan between continuous probability distributions which are accessible by samples. Our algorithm is based on the saddle point reformulation of the dynamic version of EOT which is known as the Schr\"odinger Bridge problem. In contrast to the prior methods for large-scale EOT, our algorithm is end-to-end and consists of a single learning step, has fast inference procedure, and allows handling small values of the entropy regularization coefficient which is of particular importance in some applied problems. Empirically, we show the performance of the method on several large-scale EOT tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;NAS&#22312;&#28608;&#27963;&#21644;&#36339;&#36291;&#36830;&#25509;&#25628;&#32034;&#19979;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#29702;&#35770;&#30340;&#31639;&#27861;&#26469;&#36873;&#25321;&#24615;&#33021;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2209.07238</link><description>&lt;p&gt;
NAS&#22312;&#28608;&#27963;&#21644;&#36339;&#36291;&#36830;&#25509;&#25628;&#32034;&#19979;&#30340;&#27867;&#21270;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Generalization Properties of NAS under Activation and Skip Connection Search. (arXiv:2209.07238v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;NAS&#22312;&#28608;&#27963;&#21644;&#36339;&#36291;&#36830;&#25509;&#25628;&#32034;&#19979;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#29702;&#35770;&#30340;&#31639;&#27861;&#26469;&#36873;&#25321;&#24615;&#33021;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20419;&#36827;&#20102;&#33258;&#21160;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#23613;&#31649;NAS&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;NAS&#30340;&#29702;&#35770;&#20445;&#35777;&#20851;&#27880;&#29978;&#23569;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;NAS&#22312;&#32479;&#19968;&#26694;&#26550;&#19979;&#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#21253;&#25324;&#20102;&#65288;&#28145;&#23618;&#65289;&#23618;&#32423;&#36339;&#36291;&#36830;&#25509;&#25628;&#32034;&#21644;&#28608;&#27963;&#20989;&#25968;&#25628;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21253;&#25324;&#28151;&#21512;&#28608;&#27963;&#20989;&#25968;&#12289;&#20840;&#36830;&#25509;&#21644;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#29305;&#23450;&#25628;&#32034;&#31354;&#38388;&#65292;&#25512;&#23548;&#20986;&#65288;&#26080;&#65289;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#19979;&#65288;&#19978;&#65289;&#30028;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#23567;&#29305;&#24449;&#20540;&#26469;&#24314;&#31435;NAS&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#22914;&#20309;&#26681;&#25454;&#25105;&#20204;&#25512;&#23548;&#20986;&#30340;&#32467;&#26524;&#24341;&#23548;NAS&#36873;&#25321;&#24615;&#33021;&#26368;&#22909;&#30340;&#26550;&#26500;&#65292;&#21363;&#20351;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has fostered the automatic discovery of state-of-the-art neural architectures. Despite the progress achieved with NAS, so far there is little attention to theoretical guarantees on NAS. In this work, we study the generalization properties of NAS under a unifying framework enabling (deep) layer skip connection search and activation function search. To this end, we derive the lower (and upper) bounds of the minimum eigenvalue of the Neural Tangent Kernel (NTK) under the (in)finite-width regime using a certain search space including mixed activation functions, fully connected, and residual neural networks. We use the minimum eigenvalue to establish generalization error bounds of NAS in the stochastic gradient descent training. Importantly, we theoretically and experimentally show how the derived results can guide NAS to select the top-performing architectures, even in the case without training, leading to a train-free algorithm based on our theory. Accordi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#26041;&#27861;&#26469;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;&#65292;&#36890;&#36807;&#31283;&#23450;&#38477;&#20302;&#26679;&#26412;&#30340;&#35757;&#32451;&#25439;&#22833;&#26469;&#23450;&#20041;&#26679;&#26412;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#38543;&#26426;&#25277;&#26679;&#21644;&#20165;&#26681;&#25454;&#35757;&#32451;&#25439;&#22833;&#36827;&#34892;&#20248;&#20808;&#36873;&#25321;&#30340;&#26041;&#27861;&#26356;&#21152;&#31283;&#20581;&#12290;</title><link>http://arxiv.org/abs/2208.10483</link><description>&lt;p&gt;
&#22312;&#21487;&#32422;&#25439;&#22833;&#20013;&#20026;&#24378;&#21270;&#23398;&#20064;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Samples in Reinforcement Learning with Reducible Loss. (arXiv:2208.10483v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#26041;&#27861;&#26469;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;&#65292;&#36890;&#36807;&#31283;&#23450;&#38477;&#20302;&#26679;&#26412;&#30340;&#35757;&#32451;&#25439;&#22833;&#26469;&#23450;&#20041;&#26679;&#26412;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#27604;&#20110;&#38543;&#26426;&#25277;&#26679;&#21644;&#20165;&#26681;&#25454;&#35757;&#32451;&#25439;&#22833;&#36827;&#34892;&#20248;&#20808;&#36873;&#25321;&#30340;&#26041;&#27861;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#32463;&#39564;&#22238;&#25918;&#32531;&#20914;&#21306;&#21453;&#22797;&#35757;&#32451;&#20195;&#29702;&#24050;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#12290;&#24182;&#38750;&#25152;&#26377;&#26679;&#26412;&#20855;&#26377;&#30456;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#31616;&#21333;&#22320;&#36171;&#20104;&#27599;&#20010;&#26679;&#26412;&#30456;&#31561;&#30340;&#37325;&#35201;&#24615;&#26159;&#19968;&#31181;&#22825;&#30495;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25105;&#20204;&#21487;&#20197;&#20174;&#26679;&#26412;&#20013;&#23398;&#21040;&#22810;&#23569;&#30340;&#26041;&#27861;&#26469;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#26679;&#26412;&#30340;&#21487;&#23398;&#20064;&#24615;&#23450;&#20041;&#20026;&#19982;&#26679;&#26412;&#30456;&#20851;&#30340;&#35757;&#32451;&#25439;&#22833;&#38543;&#26102;&#38388;&#25345;&#32493;&#19979;&#38477;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#36739;&#39640;&#21487;&#23398;&#20064;&#24615;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#23558;&#36739;&#38590;&#23398;&#20064;&#30340;&#26679;&#26412;&#65288;&#36890;&#24120;&#30001;&#22122;&#22768;&#25110;&#38543;&#26426;&#24615;&#24341;&#36215;&#65289;&#36171;&#20104;&#36739;&#20302;&#30340;&#20248;&#20808;&#32423;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#38543;&#26426;&#25277;&#26679;&#26356;&#21152;&#31283;&#20581;&#65292;&#20063;&#20248;&#20110;&#20165;&#26681;&#25454;&#35757;&#32451;&#25439;&#22833;&#65288;&#21363;&#26102;&#38388;&#24046;&#20998;&#25439;&#22833;&#65289;&#36827;&#34892;&#20248;&#20808;&#36873;&#25321;&#65292;&#36825;&#22312;&#20248;&#20808;&#32463;&#39564;&#22238;&#25918;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms take advantage of an experience replay buffer to repeatedly train on samples the agent has observed in the past. Not all samples carry the same amount of significance and simply assigning equal importance to each of the samples is a na\"ive strategy. In this paper, we propose a method to prioritize samples based on how much we can learn from a sample. We define the learn-ability of a sample as the steady decrease of the training loss associated with this sample over time. We develop an algorithm to prioritize samples with high learn-ability, while assigning lower priority to those that are hard-to-learn, typically caused by noise or stochasticity. We empirically show that our method is more robust than random sampling and also better than just prioritizing with respect to the training loss, i.e. the temporal difference loss, which is used in prioritized experience replay.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#20381;&#36182;&#20110;&#22240;&#26524;&#20027;&#24352;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2207.04053</link><description>&lt;p&gt;
&#35770;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Need and Applicability of Causality for Fair Machine Learning. (arXiv:2207.04053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#20381;&#36182;&#20110;&#22240;&#26524;&#20027;&#24352;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#25919;&#27835;&#21644;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#24120;&#35265;&#24212;&#29992;&#26696;&#20363;&#22806;&#65292;&#20107;&#23454;&#35777;&#26126;&#22240;&#26524;&#20851;&#31995;&#22312;&#35780;&#20272;&#33258;&#21160;&#20915;&#31574;&#30340;&#20844;&#27491;&#24615;&#26041;&#38754;&#21313;&#20998;&#37325;&#35201;&#65292;&#26080;&#35770;&#26159;&#22312;&#27861;&#24459;&#19978;&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#20026;&#20309;&#22240;&#26524;&#20851;&#31995;&#23545;&#20844;&#24179;&#24615;&#35780;&#20272;&#23588;&#20026;&#37325;&#35201;&#30340;&#35770;&#28857;&#21644;&#31034;&#20363;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#20197;&#21450;&#20381;&#36182;&#22240;&#26524;&#20027;&#24352;&#30340;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Besides its common use cases in epidemiology, political, and social sciences, causality turns out to be crucial in evaluating the fairness of automated decisions, both in a legal and everyday sense. We provide arguments and examples, of why causality is particularly important for fairness evaluation. In particular, we point out the social impact of non-causal predictions and the legal anti-discrimination process that relies on causal claims. We conclude with a discussion about the challenges and limitations of applying causality in practical scenarios as well as possible solutions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#19977;&#32500;PET&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;Transversal GAN&#65288;TrGAN&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;TrGAN&#30340;&#21028;&#21035;&#22120;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35775;&#38382;&#26435;&#38480;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#21463;&#20445;&#25252;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2206.06448</link><description>&lt;p&gt;
&#29992;&#27178;&#21521;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35780;&#20272;&#21512;&#25104;&#30340;&#19977;&#32500;PET&#25104;&#20687;&#20013;&#30340;&#38544;&#31169;&#27844;&#28431;
&lt;/p&gt;
&lt;p&gt;
Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal GAN. (arXiv:2206.06448v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06448
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#19977;&#32500;PET&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;Transversal GAN&#65288;TrGAN&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;TrGAN&#30340;&#21028;&#21035;&#22120;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35775;&#38382;&#26435;&#38480;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#21463;&#20445;&#25252;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#35757;&#32451;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#30456;&#20851;&#30340;&#31639;&#27861;&#36827;&#34892;&#30142;&#30149;&#35786;&#26029;&#25110;&#22270;&#20687;&#20998;&#21106;&#26159;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#34987;&#24191;&#27867;&#20351;&#29992;&#20197;&#20419;&#36827;&#25968;&#25454;&#20849;&#20139;&#12290;&#28982;&#32780;&#65292;&#19977;&#32500;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#38656;&#35201;&#23545;&#20854;&#38544;&#31169;&#27844;&#28431;&#36827;&#34892;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#19977;&#32500;&#29983;&#25104;&#27169;&#22411;Transversal GAN (TrGAN)&#65292;&#20351;&#29992;&#22836;&#39048;PET&#22270;&#20687;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#20197;&#32959;&#30244;&#25513;&#34109;&#20026;&#26465;&#20214;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#27169;&#22411;&#23450;&#20041;&#20102;&#22270;&#20687;&#20934;&#30830;&#24615;&#12289;&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#30340;&#23450;&#37327;&#24230;&#37327;&#26631;&#20934;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#30830;&#23450;&#29702;&#24819;&#30340;&#20934;&#30830;&#24615;&#12289;&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#25240;&#34935;&#65292;&#24182;&#24314;&#31435;&#36825;&#20123;&#21442;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#26174;&#31034;TrGAN&#30340;&#21028;&#21035;&#22120;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#20960;&#20046;&#21487;&#20197;&#23436;&#20840;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#21738;&#20123;&#26679;&#26412;&#29992;&#20110;&#35757;&#32451;&#65288;AUC = 0.99&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#65292;&#21363;&#20351;&#21482;&#26377;&#23545;&#29983;&#25104;&#27169;&#22411;&#26377;&#38480;&#30340;&#35775;&#38382;&#26435;&#38480;&#30340;&#25915;&#20987;&#32773;&#20063;&#21487;&#20197;&#25512;&#26029;&#20986;&#21463;&#20445;&#25252;&#25968;&#25454;&#30340;&#19968;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training computer-vision related algorithms on medical images for disease diagnosis or image segmentation is difficult in large part due to privacy concerns. For this reason, generative image models are highly sought after to facilitate data sharing. However, 3-D generative models are understudied, and investigation of their privacy leakage is needed. We introduce our 3-D generative model, Transversal GAN (TrGAN), using head &amp; neck PET images which are conditioned on tumour masks as a case study. We define quantitative measures of image fidelity, utility and privacy for our model. These metrics are evaluated in the course of training to identify ideal fidelity, utility and privacy trade-offs and establish the relationships between these parameters. We show that the discriminator of the TrGAN is vulnerable to attack, and that an attacker can identify which samples were used in training with almost perfect accuracy (AUC = 0.99). We also show that an attacker with access to only the gener
&lt;/p&gt;</description></item><item><title>&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2205.14590</link><description>&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14590
&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#22312;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#26377;&#25928;&#65292;&#36890;&#36807;&#26356;&#26032;Q&#20989;&#25968;&#21487;&#20197;&#24341;&#23548;&#31574;&#30053;&#25910;&#25947;&#21040;&#31283;&#23450;&#30340;&#32435;&#20160;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#29609;&#23478;&#19981;&#20102;&#35299;&#28216;&#25103;&#27169;&#22411;&#65292;&#20063;&#19981;&#33021;&#36827;&#34892;&#21327;&#35843;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#29609;&#23478;&#36890;&#36807;&#24322;&#27493;&#26041;&#24335;&#26356;&#26032;&#20182;&#20204;&#30340;&#25171;&#25200;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#20540;&#65292;&#35813;&#20989;&#25968;&#26681;&#25454;&#23454;&#29616;&#30340;&#19968;&#38454;&#27573;&#22870;&#21169;&#35780;&#20272;&#20182;&#20204;&#30340;&#24635;&#20307;&#26465;&#20214;&#20184;&#27454;&#12290;&#28982;&#21518;&#65292;&#29609;&#23478;&#36890;&#36807;&#23558;&#22522;&#20110;&#20272;&#35745;Q&#20989;&#25968;&#30340;&#24179;&#28369;&#26368;&#20248;&#19968;&#38454;&#27573;&#20559;&#24046;&#31574;&#30053;&#32435;&#20837;&#20854;&#31574;&#30053;&#20013;&#26469;&#29420;&#31435;&#22320;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#23398;&#20064;&#21160;&#24577;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;Q&#20989;&#25968;&#20272;&#35745;&#26159;&#20197;&#27604;&#31574;&#30053;&#26356;&#24555;&#30340;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#26356;&#26032;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#21160;&#24577;&#24341;&#23548;&#30340;&#31574;&#30053;&#22312;&#27010;&#29575;1&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#39532;&#23572;&#31185;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#31616;&#21333;&#23398;&#20064;&#21160;&#24577;&#22312;&#36798;&#21040;&#39532;&#23572;&#21487;&#22827;&#28508;&#22312;&#21338;&#24328;&#30340;&#31283;&#23450;&#32435;&#20160;&#24179;&#34913;&#26041;&#38754;&#30340;&#21151;&#25928;&#65292;&#21363;&#20351;&#26159;&#22312;&#29420;&#31435;&#21644;&#21435;&#20013;&#24515;&#21270;&#20195;&#29702;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-agent reinforcement learning dynamics, and analyze its convergence in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not have knowledge of the game model and cannot coordinate. In each stage, players update their estimate of a perturbed Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating a smoothed optimal one-stage deviation strategy based on the estimated Q-function. A key feature of the learning dynamics is that the Q-function estimates are updated at a faster timescale than the policies. We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1. Our results highlight the efficacy of simple learning dynamics in reaching a stationary Nash equilibrium even in environme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BagPipe&#65292;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#23884;&#20837;&#35775;&#38382;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#36890;&#36807;&#32531;&#23384;&#21644;&#39044;&#21462;&#30340;&#26041;&#24335;&#20248;&#21270;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2202.12429</link><description>&lt;p&gt;
BagPipe&#65306;&#21152;&#36895;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BagPipe: Accelerating Deep Recommendation Model Training. (arXiv:2202.12429v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BagPipe&#65292;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#30340;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#23884;&#20837;&#35775;&#38382;&#30340;&#29305;&#23450;&#32467;&#26500;&#65292;&#36890;&#36807;&#32531;&#23384;&#21644;&#39044;&#21462;&#30340;&#26041;&#24335;&#20248;&#21270;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#25512;&#33616;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#27169;&#22411;&#65288;DLRM&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#20960;&#20010;&#20851;&#38190;&#30340;&#21830;&#19994;&#24212;&#29992;&#12290;&#39640;&#25928;&#22320;&#35757;&#32451;&#36825;&#31181;&#25512;&#33616;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#22522;&#20110;&#23884;&#20837;&#30340;&#21442;&#25968;&#65292;&#20174;&#23884;&#20837;&#35775;&#38382;&#20013;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#24320;&#38144;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#30340;DLRM&#35757;&#32451;&#31995;&#32479;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#32422;75&#65285;&#30340;&#36845;&#20195;&#26102;&#38388;&#29992;&#20110;&#23884;&#20837;&#35775;&#38382;&#21644;&#27169;&#22411;&#21516;&#27493;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#23884;&#20837;&#35775;&#38382;&#20855;&#26377;&#29305;&#23450;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#29992;&#20110;&#21152;&#36895;&#35757;&#32451;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#23884;&#20837;&#35775;&#38382;&#20855;&#26377;&#20005;&#37325;&#30340;&#20559;&#26012;&#24615;&#65292;&#32422;1&#65285;&#30340;&#23884;&#20837;&#34920;&#31034;&#20102;&#36229;&#36807;92&#65285;&#30340;&#24635;&#35775;&#38382;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#31163;&#32447;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#27979;&#26410;&#26469;&#25209;&#27425;&#26469;&#30830;&#23450;&#23558;&#22312;&#23558;&#26469;&#30340;&#20309;&#26102;&#36845;&#20195;&#38656;&#35201;&#21738;&#20123;&#23884;&#20837;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Bagpipe&#65292;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#25512;&#33616;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#32531;&#23384;&#21644;&#39044;&#21462;&#26469;&#20248;&#21270;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based recommendation models (DLRM) are widely used in several business critical applications. Training such recommendation models efficiently is challenging because they contain billions of embedding-based parameters, leading to significant overheads from embedding access. By profiling existing systems for DLRM training, we observe that around 75\% of the iteration time is spent on embedding access and model synchronization. Our key insight in this paper is that embedding access has a specific structure which can be used to accelerate training. We observe that embedding accesses are heavily skewed, with around 1\% of embeddings representing more than 92\% of total accesses. Further, we observe that during offline training we can lookahead at future batches to determine exactly which embeddings will be needed at what iteration in the future. Based on these insights, we develop Bagpipe, a system for training deep recommendation models that uses caching and prefetching to ov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SCORE&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#24341;&#20837;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#65292;SCORE&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#65292;&#24182;&#28040;&#38500;&#20102;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.12468</link><description>&lt;p&gt;
SCORE&#65306;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
SCORE: Spurious COrrelation REduction for Offline Reinforcement Learning. (arXiv:2110.12468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SCORE&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#24341;&#20837;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#65292;SCORE&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#65292;&#24182;&#28040;&#38500;&#20102;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35299;&#20915;&#24207;&#36143;&#20915;&#31574;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#35770;&#25991;&#21482;&#35752;&#35770;&#20102;&#23545;&#25239;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#34892;&#20026;&#30340;&#38450;&#24481;&#65292;&#32780;&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#21363;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#19982;&#20915;&#31574;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#36825;&#26159;&#23548;&#33268;&#27425;&#20248;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26377;&#25928;&#19988;&#29702;&#35770;&#19978;&#21487;&#35777;&#26126;&#30340;&#31639;&#27861;&#65306;&#29992;&#20110;&#31163;&#32447;RL&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#38477;&#20302;&#65288;SCORE&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SCORE&#22312;&#26631;&#20934;&#22522;&#20934;&#65288;D4RL&#65289;&#19978;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20197;3.1&#20493;&#21152;&#36895;&#29575;&#23454;&#29616;&#20102;SoTA&#24615;&#33021;&#12290;&#25152;&#25552;&#31639;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#36864;&#28779;&#34892;&#20026;&#20811;&#38534;&#27491;&#21017;&#21270;&#22120;&#26469;&#24110;&#21161;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#23545;&#20110;&#28040;&#38500;&#27425;&#20248;&#24615;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#20854;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#27425;&#32447;&#24615;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) harnesses the power of massive datasets for resolving sequential decision problems. Most existing papers only discuss defending against out-of-distribution (OOD) actions while we investigate a broader issue, the spurious correlations between epistemic uncertainty and decision-making, an essential factor that causes suboptimality. In this paper, we propose Spurious COrrelation REduction (SCORE) for offline RL, a practically effective and theoretically provable algorithm. We empirically show that SCORE achieves the SoTA performance with 3.1x acceleration on various tasks in a standard benchmark (D4RL). The proposed algorithm introduces an annealing behavior cloning regularizer to help produce a high-quality estimation of uncertainty which is critical for eliminating spurious correlations from suboptimality. Theoretically, we justify the rationality of the proposed method and prove its convergence to the optimal policy with a sublinear rate under mild a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26080;&#23548;&#25968;&#20132;&#26367;&#25237;&#24433;&#31639;&#27861;&#65292;&#21253;&#25324;&#20809;&#28369;&#38382;&#39064;&#30340;&#20132;&#26367;&#38543;&#26426;&#26799;&#24230;&#25237;&#24433;&#31639;&#27861;&#65288;ZO-AGP&#65289;&#65292;&#20197;&#21450;&#22359;&#29366;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#20998;&#22359;&#20132;&#26367;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#65288;ZO-BAPG&#65289;&#12290;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#36739;&#23569;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#21644;&#36739;&#39640;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2108.00473</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26080;&#23548;&#25968;&#20132;&#26367;&#25237;&#24433;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Derivative-free Alternating Projection Algorithms for General Nonconvex-Concave Minimax Problems. (arXiv:2108.00473v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.00473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26080;&#23548;&#25968;&#20132;&#26367;&#25237;&#24433;&#31639;&#27861;&#65292;&#21253;&#25324;&#20809;&#28369;&#38382;&#39064;&#30340;&#20132;&#26367;&#38543;&#26426;&#26799;&#24230;&#25237;&#24433;&#31639;&#27861;&#65288;ZO-AGP&#65289;&#65292;&#20197;&#21450;&#22359;&#29366;&#38750;&#20809;&#28369;&#38382;&#39064;&#30340;&#20998;&#22359;&#20132;&#26367;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#65288;ZO-BAPG&#65289;&#12290;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#36739;&#23569;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#21644;&#36739;&#39640;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38646;&#38454;&#31639;&#27861;&#65292;&#36825;&#31867;&#38382;&#39064;&#36817;&#24180;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#38454;&#20132;&#26367;&#38543;&#26426;&#26799;&#24230;&#25237;&#24433;&#65288;ZO-AGP&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#20809;&#28369;&#30340;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}(\varepsilon^{-4})$&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#27425;&#25968;&#20026; $\mathcal{O}(d_{x}+d_{y})$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#38454;&#20998;&#22359;&#20132;&#26367;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#31639;&#27861;&#65288;ZO-BAPG&#65289;&#26469;&#35299;&#20915;&#22359;&#29366;&#38750;&#20809;&#28369;&#30340;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}(\varepsilon^{-4})$&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#20989;&#25968;&#20540;&#20272;&#35745;&#27425;&#25968;&#20026; $\mathcal{O}(K d_{x}+d_{y})$&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#25552;&#20986;&#36825;&#20123;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study zeroth-order algorithms for nonconvex-concave minimax problems, which have attracted widely attention in machine learning, signal processing and many other fields in recent years. We propose a zeroth-order alternating randomized gradient projection (ZO-AGP) algorithm for smooth nonconvex-concave minimax problems, and its iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$, and the number of function value estimation is bounded by $\mathcal{O}(d_{x}+d_{y})$ per iteration. Moreover, we propose a zeroth-order block alternating randomized proximal gradient algorithm (ZO-BAPG) for solving block-wise nonsmooth nonconvex-concave minimax optimization problems, and the iteration complexity to obtain an $\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$ and the number of function value estimation per iteration is bounded by $\mathcal{O}(K d_{x}+d_{y})$. To the best of our knowledge, this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#30340;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65288;LF2I&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#32463;&#20856;&#32479;&#35745;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26500;&#24314;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#30340;&#23454;&#29992;&#31243;&#24207;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#22312;&#21253;&#25324;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#22312;&#20869;&#30340;&#22810;&#20010;&#20363;&#23376;&#20013;&#37117;&#23454;&#29616;&#20102;&#35206;&#30422;&#24615;&#36136;&#24471;&#21040;&#22823;&#24133;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2107.03920</link><description>&lt;p&gt;
&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#22522;&#20110;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65306;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Likelihood-Free Frequentist Inference: Confidence Sets with Correct Conditional Coverage. (arXiv:2107.03920v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#30340;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65288;LF2I&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#32463;&#20856;&#32479;&#35745;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26500;&#24314;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#30340;&#23454;&#29992;&#31243;&#24207;&#21644;&#35786;&#26029;&#26041;&#27861;&#65292;&#22312;&#21253;&#25324;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#22312;&#20869;&#30340;&#22810;&#20010;&#20363;&#23376;&#20013;&#37117;&#23454;&#29616;&#20102;&#35206;&#30422;&#24615;&#36136;&#24471;&#21040;&#22823;&#24133;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#37117;&#24191;&#27867;&#20351;&#29992;&#35745;&#31639;&#26426;&#27169;&#25311;&#22120;&#20197;&#38544;&#21547;&#22797;&#26434;&#31995;&#32479;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#20256;&#32479;&#30340;&#32479;&#35745;&#26041;&#27861;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#20123;&#31216;&#20026;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#25512;&#26029;&#65288;LFI&#65289;&#30340;&#24773;&#20917;&#65292;&#23588;&#20854;&#26159;&#22312;&#28176;&#36817;&#21644;&#20302;&#32500;&#30340;&#26465;&#20214;&#19979;&#12290;&#34429;&#28982;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#24402;&#19968;&#21270;&#27969;&#65292;&#24050;&#32463;&#38761;&#26032;&#20102;LFI&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#23481;&#37327;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#33021;&#20026;&#23567;&#26679;&#26412;&#22823;&#23567;&#20135;&#29983;&#20855;&#26377;&#27491;&#30830;&#26465;&#20214;&#35206;&#30422;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#32463;&#20856;&#32479;&#35745;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#65288;i&#65289;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#21517;&#20041;&#35206;&#30422;&#30340;&#20869;&#26364;&#21306;&#38388;&#24314;&#35774;&#30340;&#23454;&#29992;&#31243;&#24207;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20272;&#35745;&#25972;&#20010;&#21442;&#25968;&#31354;&#38388;&#30340;&#26465;&#20214;&#35206;&#30422;&#30340;&#35786;&#26029;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#31216;&#20026;&#26080;&#20284;&#28982;&#20551;&#35774;&#19979;&#30340;&#39057;&#29575;&#23398;&#27966;&#25512;&#26029;&#65288;LF2I&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20351;&#29992;&#23450;&#20041;&#27979;&#35797;&#32479;&#35745;&#37327;&#30340;&#20219;&#20309;&#26041;&#27861;&#65292;&#22914;&#20284;&#28982;&#27604;&#65292;&#22240;&#27492;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#30340;&#20363;&#23376;&#65292;&#21253;&#25324;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#65292;&#24182;&#35777;&#26126;&#19982;&#29616;&#26377;&#30340;LFI&#26041;&#27861;&#30456;&#27604;&#65292;&#35206;&#30422;&#24615;&#36136;&#24471;&#21040;&#20102;&#22823;&#24133;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many areas of science make extensive use of computer simulators that implicitly encode likelihood functions of complex systems. Classical statistical methods are poorly suited for these so-called likelihood-free inference (LFI) settings, particularly outside asymptotic and low-dimensional regimes. Although new machine learning methods, such as normalizing flows, have revolutionized the sample efficiency and capacity of LFI methods, it remains an open question whether they produce confidence sets with correct conditional coverage for small sample sizes. This paper unifies classical statistics with modern machine learning to present (i) a practical procedure for the Neyman construction of confidence sets with finite-sample guarantees of nominal coverage, and (ii) diagnostics that estimate conditional coverage over the entire parameter space. We refer to our framework as likelihood-free frequentist inference (LF2I). Any method that defines a test statistic, like the likelihood ratio, can 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#22870;&#21169;&#21644;&#36716;&#31227;&#27010;&#29575;&#20004;&#26041;&#38754;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#33104;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#33104;&#36133;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#33104;&#36133;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#26410;&#30693;&#27700;&#24179;&#30340;&#33104;&#36133;&#12290;</title><link>http://arxiv.org/abs/1911.08689</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#33104;&#36133;&#30340;&#40065;&#26834;&#25506;&#32034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Corruption-robust exploration in episodic reinforcement learning. (arXiv:1911.08689v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.08689
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#22312;&#22870;&#21169;&#21644;&#36716;&#31227;&#27010;&#29575;&#20004;&#26041;&#38754;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#33104;&#36133;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35299;&#20915;&#33104;&#36133;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#33104;&#36133;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#26410;&#30693;&#27700;&#24179;&#30340;&#33104;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22810;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#22312;&#22870;&#21169;&#21644;&#36716;&#31227;&#27010;&#29575;&#20004;&#26041;&#38754;&#30340;&#23545;&#25239;&#24615;&#33104;&#36133;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25193;&#23637;&#20102;&#26368;&#36817;&#23545;&#38543;&#26426;&#36172;&#21338;&#26426;&#29305;&#20363;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#8220;&#20048;&#35266;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#8221;&#30340;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25506;&#32034;&#24615;&#25913;&#36827;&#65292;&#24182;&#32467;&#21512;&#8220;&#21160;&#20316;&#28120;&#27760;&#8221;&#21407;&#21017;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#26420;&#32032;&#24212;&#29992;&#21160;&#20316;&#28120;&#27760;&#25152;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;(a)&#22312;&#27809;&#26377;&#33104;&#36133;&#26102;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#21518;&#24724;&#65292;&#24182;&#19988;(b)&#33021;&#22815;&#36866;&#24212;&#26410;&#30693;&#27700;&#24179;&#30340;&#33104;&#36133;&#65292;&#22312;&#24635;&#33104;&#36133;&#24773;&#20917;&#19979;&#21518;&#24724;&#31243;&#24230;&#36880;&#28176;&#38477;&#20302;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#34920;&#26684;&#35774;&#32622;&#19979;&#30340;&#32467;&#26524;&#65288;&#20854;&#20013;&#28041;&#21450;&#29366;&#24577;&#21644;&#34892;&#20026;&#65289;&#20197;&#21450;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#35774;&#32622;&#19979;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate the study of multi-stage episodic reinforcement learning under adversarial corruptions in both the rewards and the transition probabilities of the underlying system extending recent results for the special case of stochastic bandits. We provide a framework which modifies the aggressive exploration enjoyed by existing reinforcement learning approaches based on "optimism in the face of uncertainty", by complementing them with principles from "action elimination". Importantly, our framework circumvents the major challenges posed by naively applying action elimination in the RL setting, as formalized by a lower bound we demonstrate. Our framework yields efficient algorithms which (a) attain near-optimal regret in the absence of corruptions and (b) adapt to unknown levels corruption, enjoying regret guarantees which degrade gracefully in the total corruption encountered. To showcase the generality of our approach, we derive results for both tabular settings (where states and act
&lt;/p&gt;</description></item></channel></rss>