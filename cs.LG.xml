<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;GNN&#20013;&#28040;&#24687;&#20256;&#36882;&#22312;&#38544;&#31169;&#27844;&#38706;&#20013;&#30340;&#26680;&#24515;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#21452;&#38544;&#31169;&#20445;&#25252;GNN&#26694;&#26550;&#65292;&#26377;&#25928;&#20445;&#25252;&#33410;&#28857;&#21644;&#38142;&#36335;&#30340;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2308.13513</link><description>&lt;p&gt;
&#25581;&#31034;GNN&#20013;&#28040;&#24687;&#20256;&#36882;&#22312;&#21452;&#38544;&#31169;&#20445;&#25252;&#19978;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Role of Message Passing in Dual-Privacy Preservation on GNNs. (arXiv:2308.13513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;GNN&#20013;&#28040;&#24687;&#20256;&#36882;&#22312;&#38544;&#31169;&#27844;&#38706;&#20013;&#30340;&#26680;&#24515;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#21452;&#38544;&#31169;&#20445;&#25252;GNN&#26694;&#26550;&#65292;&#26377;&#25928;&#20445;&#25252;&#33410;&#28857;&#21644;&#38142;&#36335;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#23398;&#20064;&#22270;&#24418;&#34920;&#31034;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22914;&#31038;&#20132;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26131;&#21463;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;&#30340;&#24433;&#21709;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20445;&#25252;&#38544;&#31169;&#30340;GNN&#65292;&#24182;&#30528;&#37325;&#20110;&#20445;&#25252;&#33410;&#28857;&#21644;/&#25110;&#38142;&#36335;&#38544;&#31169;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;GNN&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#36129;&#29486;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#27169;&#25311;&#65292;&#30830;&#23450;&#20102;&#32467;&#26500;&#20559;&#24046;&#19979;&#30340;&#28040;&#24687;&#20256;&#36882;&#20316;&#20026;GNN&#8220;&#20256;&#25773;&#8221;&#21644;&#8220;&#25918;&#22823;&#8221;&#38544;&#31169;&#27844;&#38706;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#21452;&#38544;&#31169;&#20445;&#25252;GNN&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#20445;&#25252;&#33410;&#28857;&#21644;&#38142;&#36335;&#38544;&#31169;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27169;&#22359;&#65306;&#19968;&#20010;&#25935;&#24863;&#20449;&#24687;&#28151;&#28102;&#27169;&#22359;&#65292;&#20174;&#33410;&#28857;&#23884;&#20837;&#20013;&#21024;&#38500;&#25935;&#24863;&#20449;&#24687;&#65307;&#19968;&#20010;&#21160;&#24577;&#32467;&#26500;&#21435;&#20559;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are powerful tools for learning representations on graphs, such as social networks. However, their vulnerability to privacy inference attacks restricts their practicality, especially in high-stake domains. To address this issue, privacy-preserving GNNs have been proposed, focusing on preserving node and/or link privacy. This work takes a step back and investigates how GNNs contribute to privacy leakage. Through theoretical analysis and simulations, we identify message passing under structural bias as the core component that allows GNNs to \textit{propagate} and \textit{amplify} privacy leakage. Building upon these findings, we propose a principled privacy-preserving GNN framework that effectively safeguards both node and link privacy, referred to as dual-privacy preservation. The framework comprises three major modules: a Sensitive Information Obfuscation Module that removes sensitive information from node embeddings, a Dynamic Structure Debiasing Module th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#29983;&#25104;&#20195;&#30721;&#20043;&#21069;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#65292;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2308.13507</link><description>&lt;p&gt;
&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#26159;&#21542;&#22686;&#21152;&#20102;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27807;&#36890;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Does Asking Clarifying Questions Increases Confidence in Generated Code? On the Communication Skills of Large Language Models. (arXiv:2308.13507v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13507
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29983;&#25104;&#20195;&#30721;&#20043;&#21069;&#25552;&#38382;&#28548;&#28165;&#38382;&#39064;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#65292;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#25104;&#20026;&#39030;&#32423;&#36719;&#20214;&#24037;&#31243;&#24072;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22522;&#20110;&#35266;&#23519;&#21040;&#39030;&#32423;&#36719;&#20214;&#24037;&#31243;&#24072;&#36890;&#24120;&#20250;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#20943;&#23569;&#38656;&#27714;&#21644;&#32534;&#30721;&#35299;&#20915;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;LLMs&#20063;&#24212;&#35813;&#37319;&#29992;&#21516;&#26679;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#26368;&#32456;&#20195;&#30721;&#20043;&#21069;&#25552;&#20986;&#28145;&#20837;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#20943;&#36731;&#20351;&#29992;LLMs&#36827;&#34892;&#32534;&#31243;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#24847;&#22270;&#35268;&#33539;&#19981;&#26126;&#30830;&#12289;&#35745;&#31639;&#24605;&#32500;&#19981;&#36275;&#21644;&#20195;&#30721;&#36136;&#37327;&#19981;&#29702;&#24819;&#12290;&#36825;&#21453;&#36807;&#26469;&#22686;&#21152;&#20102;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#33258;&#20449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#26356;&#22909;&#30340;&#27807;&#36890;&#25216;&#24039;&#26469;&#22686;&#21152;&#23545;&#29983;&#25104;&#20195;&#30721;&#30340;&#20449;&#24515;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27807;&#36890;&#20026;&#20013;&#24515;&#30340;&#36807;&#31243;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#30340;&#27807;&#36890;&#22120;&#26469;&#35782;&#21035;&#39640;&#24230;&#19981;&#30830;&#23450;&#25110;&#20449;&#24515;&#20302;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, we explore how to leverage better communication skills to achieve greater confidence in generated code. We propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low conf
&lt;/p&gt;</description></item><item><title>A2Q&#26159;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32047;&#21152;&#22120;&#24863;&#30693;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#29420;&#29305;&#30340;&#20844;&#24335;&#65292;&#24182;&#26681;&#25454;&#32047;&#21152;&#22120;&#20301;&#23485;&#36793;&#30028;&#32422;&#26463;&#27169;&#22411;&#26435;&#37325;&#30340;L1&#33539;&#25968;&#65292;&#20197;&#36991;&#20813;&#22312;&#20302;&#31934;&#24230;&#32047;&#21152;&#22120;&#19978;&#21457;&#29983;&#28322;&#20986;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35757;&#32451;QNNs&#24182;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#37096;&#32626;&#22312;&#21487;&#32534;&#31243;&#30828;&#20214;&#22914;FPGAs&#19978;&#12290;</title><link>http://arxiv.org/abs/2308.13504</link><description>&lt;p&gt;
A2Q:&#24102;&#26377;&#20445;&#35777;&#36991;&#20813;&#28322;&#20986;&#21151;&#33021;&#30340;&#32047;&#21152;&#22120;&#24863;&#30693;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance. (arXiv:2308.13504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13504
&lt;/p&gt;
&lt;p&gt;
A2Q&#26159;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32047;&#21152;&#22120;&#24863;&#30693;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#29420;&#29305;&#30340;&#20844;&#24335;&#65292;&#24182;&#26681;&#25454;&#32047;&#21152;&#22120;&#20301;&#23485;&#36793;&#30028;&#32422;&#26463;&#27169;&#22411;&#26435;&#37325;&#30340;L1&#33539;&#25968;&#65292;&#20197;&#36991;&#20813;&#22312;&#20302;&#31934;&#24230;&#32047;&#21152;&#22120;&#19978;&#21457;&#29983;&#28322;&#20986;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35757;&#32451;QNNs&#24182;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#37096;&#32626;&#22312;&#21487;&#32534;&#31243;&#30828;&#20214;&#22914;FPGAs&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32047;&#21152;&#22120;&#24863;&#30693;&#37327;&#21270;&#65288;A2Q&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#65292;&#20197;&#36991;&#20813;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#20302;&#31934;&#24230;&#32047;&#21152;&#22120;&#26102;&#21457;&#29983;&#28322;&#20986;&#12290;A2Q&#24341;&#20837;&#20102;&#19968;&#31181;&#21463;&#26435;&#20540;&#35268;&#33539;&#21270;&#21551;&#21457;&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#26681;&#25454;&#25105;&#20204;&#23548;&#20986;&#30340;&#32047;&#21152;&#22120;&#20301;&#23485;&#36793;&#30028;&#32422;&#26463;&#27169;&#22411;&#26435;&#37325;&#30340;L1&#33539;&#25968;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#20302;&#31934;&#24230;&#32047;&#21152;&#30340;QNNs&#26102;&#65292;A2Q&#36824;&#33258;&#21160;&#20419;&#36827;&#38750;&#32467;&#26500;&#21270;&#26435;&#37325;&#31232;&#30095;&#24615;&#65292;&#20197;&#30830;&#20445;&#36991;&#20813;&#28322;&#20986;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;A2Q&#21487;&#20197;&#35757;&#32451;&#20302;&#31934;&#24230;&#32047;&#21152;&#22120;&#30340;QNNs&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#28014;&#28857;&#25968;&#22522;&#20934;&#30456;&#31454;&#20105;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;A2Q&#23545;&#36890;&#29992;&#24179;&#21488;&#21644;&#21487;&#32534;&#31243;&#30828;&#20214;&#30340;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#20027;&#35201;&#38024;&#23545;&#22312;FPGAs&#19978;&#37096;&#32626;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#34987;&#32534;&#31243;&#20197;&#20805;&#20998;&#21033;&#29992;&#33258;&#23450;&#20041;&#32047;&#21152;&#22120;&#20301;&#23485;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;A2Q&#22312;&#20351;&#29992;&#20302;&#31934;&#24230;&#32047;&#21152;&#22120;&#26102;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;QNNs&#65292;&#24182;&#20445;&#25345;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present accumulator-aware quantization (A2Q), a novel weight quantization method designed to train quantized neural networks (QNNs) to avoid overflow when using low-precision accumulators during inference. A2Q introduces a unique formulation inspired by weight normalization that constrains the L1-norm of model weights according to accumulator bit width bounds that we derive. Thus, in training QNNs for low-precision accumulation, A2Q also inherently promotes unstructured weight sparsity to guarantee overflow avoidance. We apply our method to deep learning-based computer vision tasks to show that A2Q can train QNNs for low-precision accumulators while maintaining model accuracy competitive with a floating-point baseline. In our evaluations, we consider the impact of A2Q on both general-purpose platforms and programmable hardware. However, we primarily target model deployment on FPGAs because they can be programmed to fully exploit custom accumulator bit widths. Our experimentation sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13498</link><description>&lt;p&gt;
&#36867;&#31163;&#26679;&#26412;&#38519;&#38449;&#65306;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Escaping the Sample Trap: Fast and Accurate Epistemic Uncertainty Estimation with Pairwise-Distance Estimators. (arXiv:2308.13498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#24120;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#36895;&#12289;&#26356;&#20934;&#30830;&#22320;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#21644;&#26356;&#39640;&#32500;&#24230;&#19978;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#37197;&#23545;&#36317;&#31163;&#20272;&#35745;&#22120;&#65288;PaiDEs&#65289;&#23545;&#38598;&#25104;&#27169;&#22411;&#36827;&#34892;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#21033;&#29992;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#37197;&#23545;&#36317;&#31163;&#26469;&#24314;&#31435;&#29109;&#30340;&#36793;&#30028;&#65292;&#24182;&#23558;&#36825;&#20123;&#36793;&#30028;&#20316;&#20026;&#22522;&#20110;&#20449;&#24687;&#20934;&#21017;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#26368;&#36817;&#22522;&#20110;&#26679;&#26412;&#30340;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#29992;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;PaiDEs&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#31354;&#38388;&#65288;&#26368;&#22810;100&#20493;&#65289;&#19978;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#65288;&#26368;&#22810;100&#20493;&#65289;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#26356;&#39640;&#32500;&#24230;&#19978;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#35780;&#20272;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#39564;&#65306;&#19968;&#32500;&#27491;&#24358;&#25968;&#25454;&#65292;&#25670;&#21160;&#29289;&#20307;&#65288;Pendulum-v0&#65289;&#65292;&#36339;&#36291;&#26426;&#22120;&#20154;&#65288;Hopper-v2&#65289;&#65292;&#34434;&#34433;&#26426;&#22120;&#20154;&#65288;Ant-v2&#65289;&#21644;&#20154;&#24418;&#26426;&#22120;&#20154;&#65288;Humanoid-v2&#65289;&#12290;&#23545;&#20110;&#27599;&#20010;&#23454;&#39564;&#35774;&#32622;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#26469;&#23637;&#31034;PaiDEs&#22312;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel approach for epistemic uncertainty estimation for ensemble models using pairwise-distance estimators (PaiDEs). These estimators utilize the pairwise-distance between model components to establish bounds on entropy and uses said bounds as estimates for information-based criterion. Unlike recent deep learning methods for epistemic uncertainty estimation, which rely on sample-based Monte Carlo estimators, PaiDEs are able to estimate epistemic uncertainty up to 100$\times$ faster, over a larger space (up to 100$\times$) and perform more accurately in higher dimensions. To validate our approach, we conducted a series of experiments commonly used to evaluate epistemic uncertainty estimation: 1D sinusoidal data, Pendulum-v0, Hopper-v2, Ant-v2 and Humanoid-v2. For each experimental setting, an Active Learning framework was applied to demonstrate the advantages of PaiDEs for epistemic uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Ngambay-French&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#21019;&#36896;&#20102;&#31532;&#19968;&#20010;sba-Fr&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.13497</link><description>&lt;p&gt;
Ngambay-French&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;sba-Fr&#65289;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Ngambay-French Neural Machine Translation (sba-Fr). (arXiv:2308.13497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Ngambay-French&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#21019;&#36896;&#20102;&#31532;&#19968;&#20010;sba-Fr&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#27954;&#21644;&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#20110;&#24320;&#21457;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#65292;&#20197;&#20811;&#26381;&#35821;&#35328;&#38556;&#30861;&#12290;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;NMT&#23588;&#20026;&#24341;&#20154;&#27880;&#30446;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#26469;&#35828;&#65292;&#33719;&#21462;&#19968;&#20010;&#33391;&#22909;&#23545;&#40784;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#21487;&#33021;&#26377;&#25361;&#25112;&#24615;&#12290;&#26597;&#24503;&#22320;&#21306;&#20840;&#29699;&#23569;&#25968;&#35821;&#35328;&#30340;&#25216;&#26415;&#20808;&#36827;&#31243;&#24230;&#19982;NMT&#30740;&#31350;&#30340;&#19981;&#36275;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#23578;&#26410;&#23581;&#35797;&#36807;&#22312;&#20302;&#36164;&#28304;&#26597;&#24503;&#35821;&#35328;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;NMT&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#19982;&#19968;&#20123;&#38750;&#27954;&#35821;&#35328;&#19981;&#21516;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#22312;&#32447;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#21294;&#20047;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#35768;&#22810;&#26597;&#24503;&#35821;&#35328;&#32763;&#35793;&#23545;&#30340;&#21452;&#35821;&#25968;&#25454;&#65292;&#20854;&#20013;&#21478;&#19968;&#31181;&#35821;&#35328;&#26159;&#26377;&#22823;&#37327;&#25968;&#25454;&#30340;&#30693;&#21517;&#35821;&#35328;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;sba-Fr&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#20010;Ngambay&#21040;French&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Africa, and the world at large, there is an increasing focus on developing Neural Machine Translation (NMT) systems to overcome language barriers. NMT for Low-resource language is particularly compelling as it involves learning with limited labelled data. However, obtaining a well-aligned parallel corpus for low-resource languages can be challenging. The disparity between the technological advancement of a few global languages and the lack of research on NMT for local languages in Chad is striking. End-to-end NMT trials on low-resource Chad languages have not been attempted. Additionally, there is a dearth of online and well-structured data gathering for research in Natural Language Processing, unlike some African languages. However, a guided approach for data gathering can produce bitext data for many Chadian language translation pairs with well-known languages that have ample data. In this project, we created the first sba-Fr Dataset, which is a corpus of Ngambay-to-French transla
&lt;/p&gt;</description></item><item><title>TpuGraphs&#26159;&#19968;&#31181;&#20851;&#20110;&#22823;&#22411;&#24352;&#37327;&#35745;&#31639;&#22270;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#20248;&#21270;&#32534;&#35793;&#22120;&#25110;&#33258;&#21160;&#35843;&#20248;&#24037;&#20855;&#30340;&#20915;&#31574;&#65292;&#24182;&#25552;&#20379;&#20102;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.13490</link><description>&lt;p&gt;
TpuGraphs:&#19968;&#31181;&#20851;&#20110;&#22823;&#22411;&#24352;&#37327;&#35745;&#31639;&#22270;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs. (arXiv:2308.13490v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13490
&lt;/p&gt;
&lt;p&gt;
TpuGraphs&#26159;&#19968;&#31181;&#20851;&#20110;&#22823;&#22411;&#24352;&#37327;&#35745;&#31639;&#22270;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#20248;&#21270;&#32534;&#35793;&#22120;&#25110;&#33258;&#21160;&#35843;&#20248;&#24037;&#20855;&#30340;&#20915;&#31574;&#65292;&#24182;&#25552;&#20379;&#20102;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#30828;&#20214;&#24615;&#33021;&#27169;&#22411;&#22312;&#20195;&#30721;&#20248;&#21270;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#32534;&#35793;&#22120;&#20570;&#20986;&#21551;&#21457;&#24615;&#20915;&#31574;&#65292;&#25110;&#32773;&#24110;&#21161;&#33258;&#21160;&#35843;&#20248;&#24037;&#20855;&#25214;&#21040;&#32473;&#23450;&#31243;&#24207;&#30340;&#26368;&#20339;&#37197;&#32622;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TpuGraphs&#65292;&#19968;&#31181;&#22312;Tensor Processing Units&#65288;TPUs&#65289;&#19978;&#36816;&#34892;&#30340;&#20840;&#24352;&#37327;&#31243;&#24207;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#31243;&#24207;&#20197;&#35745;&#31639;&#22270;&#30340;&#24418;&#24335;&#34920;&#31034;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#22270;&#34920;&#31034;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#20027;&#35201;&#35745;&#31639;&#65292;&#20363;&#22914;&#35757;&#32451;&#21608;&#26399;&#25110;&#25512;&#26029;&#27493;&#39588;&#12290;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#21253;&#21547;&#19968;&#20010;&#35745;&#31639;&#22270;&#12289;&#19968;&#20010;&#32534;&#35793;&#37197;&#32622;&#65292;&#20197;&#21450;&#20351;&#29992;&#35813;&#37197;&#32622;&#32534;&#35793;&#26102;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise hardware performance models play a crucial role in code optimizations. They can assist compilers in making heuristic decisions or aid autotuners in identifying the optimal configuration for a given program. For example, the autotuner for XLA, a machine learning compiler, discovered 10-20% speedup on state-of-the-art models serving substantial production traffic at Google. Although there exist a few datasets for program performance prediction, they target small sub-programs such as basic blocks or kernels. This paper introduces TpuGraphs, a performance prediction dataset on full tensor programs, represented as computational graphs, running on Tensor Processing Units (TPUs). Each graph in the dataset represents the main computation of a machine learning workload, e.g., a training epoch or an inference step. Each data sample contains a computational graph, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#26041;&#27861;SAT&#65292;&#36890;&#36807;&#22312;&#32447;&#21160;&#24577;&#23884;&#20837;&#39044;&#27979;&#20943;&#36731;&#20102;&#36807;&#26102;&#24615;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#30340;&#24182;&#21457;&#24615;&#21644;&#36890;&#20449;&#24320;&#38144;&#30340;&#22256;&#25200;&#12290;</title><link>http://arxiv.org/abs/2308.13466</link><description>&lt;p&gt;
&#21487;&#36890;&#36807;&#22312;&#32447;&#21160;&#24577;&#23884;&#20837;&#39044;&#27979;&#20943;&#36731;&#36807;&#26102;&#24615;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Staleness-Alleviated Distributed GNN Training via Online Dynamic-Embedding Prediction. (arXiv:2308.13466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#26041;&#27861;SAT&#65292;&#36890;&#36807;&#22312;&#32447;&#21160;&#24577;&#23884;&#20837;&#39044;&#27979;&#20943;&#36731;&#20102;&#36807;&#26102;&#24615;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#30340;&#24182;&#21457;&#24615;&#21644;&#36890;&#20449;&#24320;&#38144;&#30340;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#37051;&#23621;&#25193;&#25955;&#65292;&#20173;&#28982;&#38590;&#20197;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20998;&#24067;&#24335;&#35745;&#31639;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#20016;&#23500;&#30340;&#35745;&#31639;&#36164;&#28304;&#65288;&#22914;GPU&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#65292;&#30001;&#20110;&#22270;&#25968;&#25454;&#30340;&#33410;&#28857;&#20381;&#36182;&#24615;&#22686;&#21152;&#65292;&#23454;&#29616;&#39640;&#24182;&#21457;&#24615;&#21464;&#24471;&#22256;&#38590;&#65292;&#36825;&#20250;&#23548;&#33268;&#24040;&#22823;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21382;&#21490;&#20540;&#36817;&#20284;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#25216;&#26415;&#31867;&#21035;&#12290;&#23427;&#21033;&#29992;&#31163;&#32447;&#20869;&#23384;&#32531;&#23384;&#21382;&#21490;&#20449;&#24687;&#65288;&#22914;&#33410;&#28857;&#23884;&#20837;&#65289;&#65292;&#20316;&#20026;&#31934;&#30830;&#20540;&#30340;&#21487;&#25215;&#21463;&#30340;&#36817;&#20284;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#24182;&#21457;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22909;&#22788;&#26159;&#20197;&#20351;&#29992;&#36807;&#26102;&#30340;&#35757;&#32451;&#20449;&#24687;&#20026;&#20195;&#20215;&#30340;&#65292;&#23548;&#33268;&#36807;&#26102;&#24615;&#12289;&#19981;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SAT&#65288;&#20943;&#36731;&#36807;&#26102;&#24615;&#35757;&#32451;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success of Graph Neural Networks (GNNs), it remains challenging to train GNNs on large-scale graphs due to neighbor explosions. As a remedy, distributed computing becomes a promising solution by leveraging abundant computing resources (e.g., GPU). However, the node dependency of graph data increases the difficulty of achieving high concurrency in distributed GNN training, which suffers from the massive communication overhead. To address it, Historical value approximation is deemed a promising class of distributed training techniques. It utilizes an offline memory to cache historical information (e.g., node embedding) as an affordable approximation of the exact value and achieves high concurrency. However, such benefits come at the cost of involving dated training information, leading to staleness, imprecision, and convergence issues. To overcome these challenges, this paper proposes SAT (Staleness-Alleviated Training), a novel and scalable distributed GNN training fr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#24178;&#39044;&#25512;&#24191;&#21040;&#19981;&#21516;&#24773;&#22659;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#24178;&#39044;&#26469;&#33258;&#21160;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#26102;&#65292;CB2M&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2308.13453</link><description>&lt;p&gt;
&#23398;&#20064;&#24178;&#39044;&#27010;&#24565;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Learning to Intervene on Concept Bottlenecks. (arXiv:2308.13453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#23558;&#24178;&#39044;&#25512;&#24191;&#21040;&#19981;&#21516;&#24773;&#22659;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#24178;&#39044;&#26469;&#33258;&#21160;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;&#24403;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#26102;&#65292;CB2M&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#32780;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#36890;&#36807;&#20854;&#27010;&#24565;&#34920;&#31034;&#25552;&#20379;&#22266;&#26377;&#30340;&#35299;&#37322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#20204;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#26356;&#26032;&#27010;&#24565;&#20540;&#24182;&#32416;&#27491;&#27169;&#22411;&#30340;&#39044;&#27979;&#36755;&#20986;&#26469;&#36827;&#34892;&#24178;&#39044;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#20013;&#36825;&#20123;&#24178;&#39044;&#20165;&#24212;&#29992;&#20110;&#27169;&#22411;&#19968;&#27425;&#21518;&#21363;&#34987;&#20002;&#24323;&#12290;&#20026;&#20102;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#24565;&#29942;&#39048;&#35760;&#24518;&#27169;&#22411;&#65288;CB2M&#65289;&#65292;&#36825;&#26159;CBM&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CB2M&#36890;&#36807;&#21452;&#25240;&#21472;&#35760;&#24518;&#23398;&#20064;&#23558;&#24178;&#39044;&#30340;&#25512;&#24191;&#21040;&#36866;&#24403;&#30340;&#26032;&#24773;&#22659;&#20013;&#65292;&#20174;&#32780;&#33021;&#22815;&#23398;&#20064;&#26816;&#27979;&#38169;&#35823;&#24182;&#37325;&#26032;&#24212;&#29992;&#20808;&#21069;&#30340;&#24178;&#39044;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CB2M&#33021;&#22815;&#20174;&#26368;&#21021;&#33719;&#24471;&#30340;&#23569;&#37327;&#24178;&#39044;&#20013;&#33258;&#21160;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22914;&#26524;&#27809;&#26377;&#20808;&#21069;&#30340;&#20154;&#31867;&#24178;&#39044;&#20449;&#24687;&#65292;CB2M&#21487;&#20197;&#26816;&#27979;&#21040;CBM&#29942;&#39048;&#30340;&#28508;&#22312;&#38169;&#35823;&#24182;&#35831;&#27714;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
While traditional deep learning models often lack interpretability, concept bottleneck models (CBMs) provide inherent explanations via their concept representations. Specifically, they allow users to perform interventional interactions on these concepts by updating the concept values and thus correcting the predictive output of the model. Traditionally, however, these interventions are applied to the model only once and discarded afterward. To rectify this, we present concept bottleneck memory models (CB2M), an extension to CBMs. Specifically, a CB2M learns to generalize interventions to appropriate novel situations via a two-fold memory with which it can learn to detect mistakes and to reapply previous interventions. In this way, a CB2M learns to automatically improve model performance from a few initially obtained interventions. If no prior human interventions are available, a CB2M can detect potential mistakes of the CBM bottleneck and request targeted interventions. In our experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24809;&#32602;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#25514;&#26045;&#12290;&#22312;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13451</link><description>&lt;p&gt;
&#25235;&#20303;&#23427;&#20204;&#65306;&#22270;&#21305;&#37197;&#21305;&#37197;&#28388;&#27874;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gotta match 'em all: Solution diversification in graph matching matched filters. (arXiv:2308.13451v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24809;&#32602;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#25514;&#26045;&#12290;&#22312;&#29702;&#35770;&#39564;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#24120;&#22823;&#30340;&#32972;&#26223;&#22270;&#20013;&#26597;&#25214;&#22810;&#20010;&#23884;&#20837;&#22312;&#20854;&#20013;&#30340;&#27169;&#26495;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Sussman&#31561;&#20154;&#25552;&#20986;&#30340;&#22270;&#21305;&#37197;&#21305;&#37197;&#28388;&#27874;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#21305;&#37197;&#28388;&#27874;&#31639;&#27861;&#20013;&#36845;&#20195;&#22320;&#24809;&#32602;&#21512;&#36866;&#30340;&#33410;&#28857;&#23545;&#30456;&#20284;&#24230;&#30697;&#38453;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21305;&#37197;&#30340;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31639;&#27861;&#21152;&#36895;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25105;&#20204;&#30340;&#21305;&#37197;&#28388;&#27874;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#22312;&#30456;&#20851;&#30340;Erdos-Renyi&#22270;&#35774;&#32622;&#20013;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#19978;&#30340;&#39564;&#35777;&#65292;&#26174;&#31034;&#20854;&#22312;&#28201;&#21644;&#30340;&#27169;&#22411;&#26465;&#20214;&#19979;&#33021;&#22815;&#39034;&#24207;&#22320;&#21457;&#29616;&#22810;&#20010;&#27169;&#26495;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#27169;&#22411;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#20154;&#33041;&#36830;&#25509;&#32452;&#21644;&#22823;&#22411;&#20132;&#26131;&#30693;&#35782;&#24211;&#65289;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for finding multiple noisily embedded template graphs in a very large background graph. Our method builds upon the graph-matching-matched-filter technique proposed in Sussman et al., with the discovery of multiple diverse matchings being achieved by iteratively penalizing a suitable node-pair similarity matrix in the matched filter algorithm. In addition, we propose algorithmic speed-ups that greatly enhance the scalability of our matched-filter approach. We present theoretical justification of our methodology in the setting of correlated Erdos-Renyi graphs, showing its ability to sequentially discover multiple templates under mild model conditions. We additionally demonstrate our method's utility via extensive experiments both using simulated models and real-world dataset, include human brain connectomes and a large transactional knowledge base.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#32447;&#24615;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#32447;&#24615;&#22238;&#24402;&#12289;&#26680;&#23725;&#22238;&#24402;&#12289;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21644;&#31070;&#32463;&#20999;&#32447;&#27169;&#22411;&#31561;&#22235;&#20010;&#32447;&#24615;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#32447;&#24615;&#29702;&#35770;&#30340;&#23616;&#38480;&#24615;&#21644;&#20854;&#20182;&#26041;&#27861;&#22914;&#20309;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13431</link><description>&lt;p&gt;
&#32447;&#24615;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20845;&#20010;&#35762;&#24231;
&lt;/p&gt;
&lt;p&gt;
Six Lectures on Linearized Neural Networks. (arXiv:2308.13431v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13431
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#32447;&#24615;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#32447;&#24615;&#22238;&#24402;&#12289;&#26680;&#23725;&#22238;&#24402;&#12289;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21644;&#31070;&#32463;&#20999;&#32447;&#27169;&#22411;&#31561;&#22235;&#20010;&#32447;&#24615;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#32447;&#24615;&#29702;&#35770;&#30340;&#23616;&#38480;&#24615;&#21644;&#20854;&#20182;&#26041;&#27861;&#22914;&#20309;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20845;&#20010;&#35762;&#24231;&#20013;&#65292;&#25105;&#20204;&#20174;&#32447;&#24615;&#27169;&#22411;&#30340;&#20998;&#26512;&#20013;&#25506;&#35752;&#20102;&#23545;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#36890;&#36807;&#25152;&#35859;&#30340;&#25042;&#24816;&#27169;&#24335;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#32447;&#24615;&#27169;&#22411;&#30456;&#23545;&#24212;&#30340;&#24773;&#20917;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#32447;&#24615;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22235;&#20010;&#27169;&#22411;&#65306;&#24102;&#26377;&#38598;&#20013;&#29305;&#24449;&#30340;&#32447;&#24615;&#22238;&#24402;&#65292;&#26680;&#23725;&#22238;&#24402;&#65292;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21644;&#31070;&#32463;&#20999;&#32447;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#32447;&#24615;&#29702;&#35770;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20182;&#26041;&#27861;&#22914;&#20309;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In these six lectures, we examine what can be learnt about the behavior of multi-layer neural networks from the analysis of linear models. We first recall the correspondence between neural networks and linear models via the so-called lazy regime. We then review four models for linearized neural networks: linear regression with concentrated features, kernel ridge regression, random feature model and neural tangent model. Finally, we highlight the limitations of the linear theory and discuss how other approaches can overcome them.
&lt;/p&gt;</description></item><item><title>Nougat&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#26415;&#25991;&#26723;&#30340;&#31070;&#32463;&#20809;&#23398;&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#20219;&#21153;&#23558;&#31185;&#23398;&#25991;&#26723;&#36716;&#25442;&#25104;&#26631;&#35760;&#35821;&#35328;&#65292;&#25552;&#39640;&#20102;&#31185;&#23398;&#30693;&#35782;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#21644;&#20195;&#30721;&#20197;&#21152;&#36895;&#26410;&#26469;&#30340;&#31185;&#23398;&#25991;&#26412;&#35782;&#21035;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.13418</link><description>&lt;p&gt;
Nougat: &#29992;&#20110;&#23398;&#26415;&#25991;&#26723;&#30340;&#31070;&#32463;&#20809;&#23398;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Nougat: Neural Optical Understanding for Academic Documents. (arXiv:2308.13418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13418
&lt;/p&gt;
&lt;p&gt;
Nougat&#26159;&#19968;&#31181;&#29992;&#20110;&#23398;&#26415;&#25991;&#26723;&#30340;&#31070;&#32463;&#20809;&#23398;&#29702;&#35299;&#27169;&#22411;&#65292;&#36890;&#36807;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#20219;&#21153;&#23558;&#31185;&#23398;&#25991;&#26723;&#36716;&#25442;&#25104;&#26631;&#35760;&#35821;&#35328;&#65292;&#25552;&#39640;&#20102;&#31185;&#23398;&#30693;&#35782;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#21644;&#20195;&#30721;&#20197;&#21152;&#36895;&#26410;&#26469;&#30340;&#31185;&#23398;&#25991;&#26412;&#35782;&#21035;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#30693;&#35782;&#20027;&#35201;&#23384;&#20648;&#22312;&#22270;&#20070;&#21644;&#31185;&#23398;&#26399;&#21002;&#20013;&#65292;&#36890;&#24120;&#20197;PDF&#26684;&#24335;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;PDF&#26684;&#24335;&#20250;&#23548;&#33268;&#35821;&#20041;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#23588;&#20854;&#26159;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Nougat&#65288;&#29992;&#20110;&#23398;&#26415;&#25991;&#26723;&#30340;&#31070;&#32463;&#20809;&#23398;&#29702;&#35299;&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#36716;&#25442;&#27169;&#22411;&#36827;&#34892;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#20219;&#21153;&#65292;&#23558;&#31185;&#23398;&#25991;&#26723;&#22788;&#29702;&#25104;&#26631;&#35760;&#35821;&#35328;&#65292;&#24182;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#22312;&#25968;&#23383;&#26102;&#20195;&#22686;&#24378;&#31185;&#23398;&#30693;&#35782;&#30340;&#21487;&#35775;&#38382;&#24615;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24357;&#21512;&#20102;&#20154;&#31867;&#21487;&#35835;&#25991;&#26723;&#21644;&#26426;&#22120;&#21487;&#35835;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#27169;&#22411;&#21644;&#20195;&#30721;&#65292;&#20197;&#21152;&#36895;&#26410;&#26469;&#31185;&#23398;&#25991;&#26412;&#35782;&#21035;&#24037;&#20316;&#30340;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#23545;&#24515;&#33039;&#30913;&#20849;&#25391;&#20998;&#21106;&#20013;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20559;&#24046;&#30340;&#20005;&#37325;&#24615;&#21644;&#24615;&#36136;&#19981;&#21516;&#65292;&#24378;&#35843;&#20102;&#27169;&#22411;&#36873;&#25321;&#22312;&#35757;&#32451;&#20844;&#24179;&#30340;&#22522;&#20110;AI&#30340;&#21307;&#23398;&#25104;&#20687;&#20998;&#21106;&#27169;&#22411;&#26102;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13415</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#23545;&#24515;&#33039;&#30913;&#20849;&#25391;&#20998;&#21106;&#20013;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#24046;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An investigation into the impact of deep learning model choice on sex and race bias in cardiac MR segmentation. (arXiv:2308.13415v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13415
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#23545;&#24515;&#33039;&#30913;&#20849;&#25391;&#20998;&#21106;&#20013;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20559;&#24046;&#30340;&#20005;&#37325;&#24615;&#21644;&#24615;&#36136;&#19981;&#21516;&#65292;&#24378;&#35843;&#20102;&#27169;&#22411;&#36873;&#25321;&#22312;&#35757;&#32451;&#20844;&#24179;&#30340;&#22522;&#20110;AI&#30340;&#21307;&#23398;&#25104;&#20687;&#20998;&#21106;&#27169;&#22411;&#26102;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#33258;&#21160;&#21270;&#20363;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#24182;&#21152;&#21095;&#20559;&#24046;&#65292;&#23548;&#33268;&#21463;&#20445;&#25252;&#30340;&#32676;&#20307;&#20043;&#38388;&#30340;&#19981;&#24179;&#31561;&#34920;&#29616;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#36873;&#25321;&#23545;&#22522;&#20110;AI&#30340;&#24515;&#33039;&#30913;&#20849;&#25391;&#22270;&#20687;&#20998;&#21106;&#20013;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20027;&#20307;&#24615;&#21035;&#21644;&#31181;&#26063;&#19981;&#24179;&#34913;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22235;&#20010;&#27169;&#22411;&#20013;&#26377;&#19977;&#20010;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#32780;&#25152;&#26377;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#31181;&#26063;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#20559;&#24046;&#30340;&#20005;&#37325;&#31243;&#24230;&#21644;&#24615;&#36136;&#22240;&#27169;&#22411;&#32780;&#24322;&#65292;&#36825;&#20984;&#26174;&#20102;&#27169;&#22411;&#36873;&#25321;&#22312;&#23581;&#35797;&#35757;&#32451;&#20844;&#24179;&#30340;&#22522;&#20110;AI&#30340;&#21307;&#23398;&#25104;&#20687;&#20998;&#21106;&#27169;&#22411;&#26102;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In medical imaging, artificial intelligence (AI) is increasingly being used to automate routine tasks. However, these algorithms can exhibit and exacerbate biases which lead to disparate performances between protected groups. We investigate the impact of model choice on how imbalances in subject sex and race in training datasets affect AI-based cine cardiac magnetic resonance image segmentation. We evaluate three convolutional neural network-based models and one vision transformer model. We find significant sex bias in three of the four models and racial bias in all of the models. However, the severity and nature of the bias varies between the models, highlighting the importance of model choice when attempting to train fair AI-based segmentation models for medical imaging tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#20171;&#32461;&#20102;&#20351;&#29992;&#35270;&#35273;&#21644;&#36710;&#36742;&#20256;&#24863;&#22120;&#20998;&#26512;&#39550;&#39542;&#34892;&#20026;&#30340;&#25216;&#26415;&#65292;&#25351;&#20986;&#23558;&#35270;&#35273;&#21644;&#36710;&#36742;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#20174;&#32780;&#25913;&#21892;&#39550;&#39542;&#23433;&#20840;&#21644;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#12290;</title><link>http://arxiv.org/abs/2308.13406</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21644;&#36710;&#36742;&#20256;&#24863;&#22120;&#36827;&#34892;&#39550;&#39542;&#34892;&#20026;&#20998;&#26512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Using Visual and Vehicular Sensors for Driver Behavior Analysis: A Survey. (arXiv:2308.13406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13406
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#20171;&#32461;&#20102;&#20351;&#29992;&#35270;&#35273;&#21644;&#36710;&#36742;&#20256;&#24863;&#22120;&#20998;&#26512;&#39550;&#39542;&#34892;&#20026;&#30340;&#25216;&#26415;&#65292;&#25351;&#20986;&#23558;&#35270;&#35273;&#21644;&#36710;&#36742;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#20174;&#32780;&#25913;&#21892;&#39550;&#39542;&#23433;&#20840;&#21644;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32654;&#22269;&#65292;&#39640;&#39118;&#38505;&#39550;&#39542;&#32773;&#21344;&#33268;&#21629;&#20107;&#25925;&#30340;70%&#12290;&#38543;&#30528;&#20256;&#24863;&#22120;&#21644;&#26234;&#33021;&#36710;&#36733;&#31995;&#32479;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23545;&#20110;&#35780;&#20272;&#39550;&#39542;&#34892;&#20026;&#20197;&#25913;&#21892;&#39550;&#39542;&#20307;&#39564;&#21644;&#36947;&#36335;&#23433;&#20840;&#30340;&#30740;&#31350;&#26174;&#33879;&#22686;&#21152;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;&#20351;&#29992;&#35270;&#35273;&#21644;&#36710;&#36742;&#25968;&#25454;&#20998;&#26512;&#39550;&#39542;&#34892;&#20026;&#30340;&#21508;&#31181;&#25216;&#26415;&#65292;&#27010;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#24314;&#35758;&#12290;&#32508;&#36848;&#24471;&#20986;&#32467;&#35770;&#65292;&#23558;&#35270;&#35273;&#21644;&#36710;&#36742;&#20449;&#24687;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39550;&#39542;&#34892;&#20026;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#65292;&#20174;&#32780;&#25913;&#21892;&#23433;&#20840;&#25514;&#26045;&#24182;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#12290;
&lt;/p&gt;
&lt;p&gt;
Risky drivers account for 70% of fatal accidents in the United States. With recent advances in sensors and intelligent vehicular systems, there has been significant research on assessing driver behavior to improve driving experiences and road safety. This paper examines the various techniques used to analyze driver behavior using visual and vehicular data, providing an overview of the latest research in this field. The paper also discusses the challenges and open problems in the field and offers potential recommendations for future research. The survey concludes that integrating vision and vehicular information can significantly enhance the accuracy and effectiveness of driver behavior analysis, leading to improved safety measures and reduced traffic accidents.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#20316;&#20026;&#20851;&#38190;&#35789;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#24120;&#29992;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13399</link><description>&lt;p&gt;
EntropyRank: &#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#21387;&#32553;&#30340;&#21103;&#20449;&#24687;&#20248;&#21270;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression. (arXiv:2308.13399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#20316;&#20026;&#20851;&#38190;&#35789;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#24120;&#29992;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21644;Shannon&#30340;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#21644;&#20851;&#38190;&#35789;&#30701;&#35821;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#22312;LM&#19979;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#12290;&#24471;&#21040;&#30340;&#20851;&#38190;&#35789;&#30701;&#35821;&#38598;&#21512;&#35299;&#20915;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#20449;&#24687;&#35770;&#38382;&#39064;&#65306;&#22914;&#26524;&#20316;&#20026;&#21103;&#20449;&#24687;&#25552;&#20379;&#65292;&#23427;&#20250;&#23548;&#33268;&#20351;&#29992;LM&#21644;&#29109;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#26102;&#30340;&#39044;&#26399;&#26368;&#23567;&#20108;&#36827;&#21046;&#30721;&#38271;&#24230;&#12290;&#21478;&#22806;&#65292;&#24471;&#21040;&#30340;&#38598;&#21512;&#26159;&#36890;&#36807;&#22240;&#26524;LM&#23545;&#22312;&#32473;&#23450;&#26465;&#20214;&#19979;&#26368;&#23567;&#21270;&#25991;&#26412;&#29109;&#30340;&#30701;&#35821;&#38598;&#21512;&#30340;&#36817;&#20284;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20851;&#38190;&#35789;&#25552;&#21462;&#22522;&#20934;&#25361;&#25112;&#20013;&#25552;&#20379;&#20102;&#19982;&#26368;&#24120;&#29992;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised method to extract keywords and keyphrases from texts based on a pre-trained language model (LM) and Shannon's information maximization. Specifically, our method extracts phrases having the highest conditional entropy under the LM. The resulting set of keyphrases turns out to solve a relevant information-theoretic problem: if provided as side information, it leads to the expected minimal binary code length in compressing the text using the LM and an entropy encoder. Alternately, the resulting set is an approximation via a causal LM to the set of phrases that minimize the entropy of the text when conditioned upon it. Empirically, the method provides results comparable to the most commonly used methods in various keyphrase extraction benchmark challenges.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TFDNet&#30340;&#26102;&#39057;&#22686;&#24378;&#20998;&#35299;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#26102;&#39057;&#22495;&#25429;&#33719;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#26412;&#27169;&#24335;&#21644;&#26102;&#38388;&#21608;&#26399;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13386</link><description>&lt;p&gt;
TFDNet&#65306;&#22686;&#24378;&#26102;&#39057;&#20998;&#35299;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TFDNet: Time-Frequency Enhanced Decomposed Network for Long-term Time Series Forecasting. (arXiv:2308.13386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13386
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TFDNet&#30340;&#26102;&#39057;&#22686;&#24378;&#20998;&#35299;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#26102;&#39057;&#22495;&#25429;&#33719;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#26412;&#27169;&#24335;&#21644;&#26102;&#38388;&#21608;&#26399;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#25429;&#33719;&#26469;&#33258;&#21333;&#19968;&#22495;&#65288;&#20363;&#22914;&#26102;&#38388;&#22495;&#25110;&#39057;&#29575;&#22495;&#65289;&#30340;&#22522;&#26412;&#27169;&#24335;&#65292;&#24182;&#19988;&#27809;&#26377;&#20174;&#26102;&#39057;&#22495;&#32508;&#21512;&#22788;&#29702;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#39057;&#22686;&#24378;&#20998;&#35299;&#32593;&#32476;&#65288;TFDNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26102;&#39057;&#22495;&#25429;&#33719;&#38271;&#26399;&#30340;&#22522;&#26412;&#27169;&#24335;&#21644;&#26102;&#38388;&#21608;&#26399;&#24615;&#12290;&#22312;TFDNet&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#26102;&#39057;&#22686;&#24378;&#32534;&#30721;&#22120;&#20027;&#24178;&#65292;&#24182;&#24320;&#21457;&#20004;&#20010;&#20998;&#21035;&#29992;&#20110;&#25429;&#33719;&#22810;&#20998;&#36776;&#29575;&#20013;&#20998;&#35299;&#36235;&#21183;&#21644;&#23395;&#33410;&#20998;&#37327;&#20013;&#30340;&#19981;&#21516;&#27169;&#24335;&#30340;&#36235;&#21183;&#21644;&#23395;&#33410;&#26102;&#39057;&#22359;&#12290;&#36890;&#36807;&#30740;&#31350;&#21644;&#25972;&#21512;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#28508;&#22312;&#19981;&#21516;&#36890;&#36947;&#30456;&#20851;&#27169;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26102;&#39057;&#22359;&#20013;&#26680;&#25805;&#20316;&#30340;&#22810;&#26679;&#21270;&#20869;&#26680;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting is a vital task and has a wide range of real applications. Recent methods focus on capturing the underlying patterns from one single domain (e.g. the time domain or the frequency domain), and have not taken a holistic view to process long-term time series from the time-frequency domains. In this paper, we propose a Time-Frequency Enhanced Decomposed Network (TFDNet) to capture both the long-term underlying patterns and temporal periodicity from the time-frequency domain. In TFDNet, we devise a multi-scale time-frequency enhanced encoder backbone and develop two separate trend and seasonal time-frequency blocks to capture the distinct patterns within the decomposed trend and seasonal components in multi-resolutions. Diverse kernel learning strategies of the kernel operations in time-frequency blocks have been explored, by investigating and incorporating the potential different channel-wise correlation patterns of multivariate time series. Experimental e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21516;&#19968;&#31867;&#21035;&#20013;&#20854;&#20182;&#31995;&#32479;&#30340;&#34892;&#20026;&#26469;&#29702;&#35299;&#21160;&#24577;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13380</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#31995;&#32479;&#36776;&#35782;
&lt;/p&gt;
&lt;p&gt;
In-context learning for model-free system identification. (arXiv:2308.13380v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21516;&#19968;&#31867;&#21035;&#20013;&#20854;&#20182;&#31995;&#32479;&#30340;&#34892;&#20026;&#26469;&#29702;&#35299;&#21160;&#24577;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#31995;&#32479;&#36776;&#35782;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32473;&#23450;&#30340;&#36755;&#20837;/&#36755;&#20986;&#24207;&#21015;&#21644;&#21487;&#29992;&#30340;&#29289;&#29702;&#30693;&#35782;&#26469;&#20272;&#35745;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26159;&#21542;&#36824;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#21516;&#19968;&#31867;&#21035;&#20013;&#20854;&#20182;&#31995;&#32479;&#30340;&#34892;&#20026;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20174;&#23427;&#20204;&#30340;&#36755;&#20837;/&#36755;&#20986;&#27169;&#24335;&#20013;&#29702;&#35299;&#21160;&#24577;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21602;&#65311;&#36825;&#20010;&#26680;&#24515;&#38382;&#39064;&#39537;&#21160;&#30528;&#26412;&#25991;&#30340;&#30740;&#31350;&#12290;&#20316;&#20026;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#36776;&#35782;&#33539;&#24335;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#65306;&#19968;&#27493;&#39044;&#27979;&#21644;&#22810;&#27493;&#27169;&#25311;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#30452;&#25509;&#23545;&#29305;&#23450;&#31995;&#32479;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#65292;&#32780;&#26159;&#39044;&#20808;&#35757;&#32451;&#19968;&#20010;&#20195;&#34920;&#21160;&#24577;&#31995;&#32479;&#31867;&#21035;&#30340;&#20803;&#27169;&#22411;&#12290;&#35813;&#20803;&#27169;&#22411;&#26159;&#36890;&#36807;&#20174;&#26576;&#20010;&#20998;&#24067;&#20013;&#38543;&#26426;&#25277;&#21462;&#30340;&#31995;&#32479;&#29983;&#25104;&#30340;&#28508;&#22312;&#26080;&#38480;&#27969;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22312;&#20854;&#26680;&#24515;&#65292;&#20803;&#27169;&#22411;&#20316;&#20026;&#23545;&#20027;&#35201;&#29305;&#24449;&#30340;&#38544;&#24335;&#34920;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;
In traditional system identification, we estimate a model of an unknown dynamical system based on given input/output sequences and available physical knowledge. Yet, is it also possible to understand the intricacies of dynamical systems not solely from their input/output patterns, but by observing the behavior of other systems within the same class? This central question drives the study presented in this paper.  In response to this query, we introduce a novel paradigm for system identification, addressing two primary tasks: one-step-ahead prediction and multi-step simulation. Unlike conventional methods, we do not directly estimate a model for the specific system. Instead, we pretrain a meta model that represents a class of dynamical systems. This meta model is trained from a potentially infinite stream of synthetic data, generated by systems randomly extracted from a certain distribution. At its core, the meta model serves as an implicit representation of the main characteristics of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21463;&#27745;&#26579;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#20013;&#21435;&#38500;&#30524;&#30005;&#22270;&#20266;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.13371</link><description>&lt;p&gt;
&#36890;&#36807;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20174;&#21333;&#36890;&#36947;&#21644;&#22810;&#36890;&#36947;&#33041;&#30005;&#22270;&#35760;&#24405;&#20013;&#21435;&#38500;&#30524;&#30005;&#22270;&#20266;&#36857;
&lt;/p&gt;
&lt;p&gt;
EOG Artifact Removal from Single and Multi-channel EEG Recordings through the combination of Long Short-Term Memory Networks and Independent Component Analysis. (arXiv:2308.13371v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21463;&#27745;&#26579;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#20013;&#21435;&#38500;&#30524;&#30005;&#22270;&#20266;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;&#65306;&#30001;&#20110;&#20854;&#20016;&#23500;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#33041;&#30005;&#22270;(EEG)&#20449;&#21495;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#27969;&#34892;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20449;&#21495;&#23481;&#26131;&#21463;&#21040;&#26469;&#33258;&#21508;&#31181;&#20266;&#36857;&#28304;&#30340;&#27745;&#26579;&#65292;&#23588;&#20854;&#26159;&#30001;&#30524;&#30555;&#36816;&#21160;&#24341;&#36215;&#30340;&#30524;&#30005;&#22270;(EOG)&#20266;&#36857;&#12290;&#35299;&#20915;EOG&#20266;&#36857;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#26159;&#21516;&#26102;&#35760;&#24405;EOG&#20449;&#21495;&#21644;EEG&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#30450;&#28304;&#20998;&#31163;&#25216;&#26415;&#65292;&#22914;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#20808;&#24405;&#21046;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;EOG&#35760;&#24405;&#30340;&#21487;&#29992;&#24615;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#30446;&#26631;&#65306;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#30340;&#31070;&#32463;&#32593;&#32476;&#19982;ICA&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#20174;&#21463;&#27745;&#26579;&#30340;EEG&#20449;&#21495;&#20013;&#21435;&#38500;EOG&#20266;&#36857;&#30340;&#25361;&#25112;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23454;&#29616;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#65306;1) &#20174;&#21463;&#27745;&#26579;&#30340;EEG&#25968;&#25454;&#20013;&#20272;&#35745;&#27700;&#24179;&#21644;&#22402;&#30452;&#30340;EOG&#20449;&#21495;&#65307;2) &#20351;&#29992;ICA&#23545;&#20272;&#35745;&#30340;EOG&#20449;&#21495;&#36827;&#34892;&#30450;&#28304;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: Electroencephalogram (EEG) signals have gained significant popularity in various applications due to their rich information content. However, these signals are prone to contamination from various sources of artifacts, notably the electrooculogram (EOG) artifacts caused by eye movements. The most effective approach to mitigate EOG artifacts involves recording EOG signals simultaneously with EEG and employing blind source separation techniques, such as independent component analysis (ICA). Nevertheless, the availability of EOG recordings is not always feasible, particularly in pre-recorded datasets. Objective: In this paper, we present a novel methodology that combines a long short-term memory (LSTM)-based neural network with ICA to address the challenge of EOG artifact removal from contaminated EEG signals. Approach: Our approach aims to accomplish two primary objectives: 1) estimate the horizontal and vertical EOG signals from the contaminated EEG data, and 2) employ ICA 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32534;&#30721;&#37096;&#20998;&#31561;&#21464;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#30456;&#24212;&#30340;&#27979;&#37327;&#31354;&#38388;&#21644;P-GENEO&#31354;&#38388;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2308.13357</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#37096;&#20998;&#31561;&#21464;&#24615;&#30340;&#25299;&#25169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A topological model for partial equivariance in deep learning and data analysis. (arXiv:2308.13357v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32534;&#30721;&#37096;&#20998;&#31561;&#21464;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#30456;&#24212;&#30340;&#27979;&#37327;&#31354;&#38388;&#21644;P-GENEO&#31354;&#38388;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32534;&#30721;&#37096;&#20998;&#31561;&#21464;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31867;&#31216;&#20026;P-GENEO&#30340;&#36816;&#31639;&#31526;&#65292;&#20197;&#38750;&#25193;&#24352;&#30340;&#26041;&#24335;&#25913;&#21464;&#36890;&#36807;&#27979;&#37327;&#34920;&#31034;&#30340;&#25968;&#25454;&#65292;&#24182;&#36981;&#24490;&#19968;&#23450;&#38598;&#21512;&#30340;&#21464;&#25442;&#20316;&#29992;&#12290;&#22914;&#26524;&#21464;&#25442;&#20316;&#29992;&#30340;&#38598;&#21512;&#26159;&#19968;&#20010;&#32676;&#65292;&#21017;&#24471;&#21040;&#25152;&#35859;&#30340;GENEO&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21463;&#26576;&#20123;&#33258;&#26144;&#23556;&#20316;&#29992;&#30340;&#27979;&#37327;&#31354;&#38388;&#20197;&#21450;&#36825;&#20123;&#31354;&#38388;&#20043;&#38388;&#30340;P-GENEO&#31354;&#38388;&#12290;&#25105;&#20204;&#22312;&#23427;&#20204;&#19978;&#23450;&#20041;&#20102;&#20266;&#24230;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#32467;&#26524;&#31354;&#38388;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31354;&#38388;&#20855;&#26377;&#20415;&#21033;&#30340;&#36924;&#36817;&#21644;&#20984;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we propose a topological model to encode partial equivariance in neural networks. To this end, we introduce a class of operators, called P-GENEOs, that change data expressed by measurements, respecting the action of certain sets of transformations, in a non-expansive way. If the set of transformations acting is a group, then we obtain the so-called GENEOs. We then study the spaces of measurements, whose domains are subject to the action of certain self-maps, and the space of P-GENEOs between these spaces. We define pseudo-metrics on them and show some properties of the resulting spaces. In particular, we show how such spaces have convenient approximation and convexity properties.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26681;&#25454;&#20351;&#29992;CodeBERT&#27169;&#22411;&#20998;&#26512;&#32534;&#31243;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#22312;&#26631;&#35760;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#24314;&#35758;&#20351;&#29992;&#36825;&#31181;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#26469;&#36873;&#25321;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13354</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#36873;&#25321;&#23545;&#35757;&#32451;&#21644;&#35780;&#20272;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Language Selection for Training and Evaluating Programming Language Models. (arXiv:2308.13354v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13354
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26681;&#25454;&#20351;&#29992;CodeBERT&#27169;&#22411;&#20998;&#26512;&#32534;&#31243;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#22312;&#26631;&#35760;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#24314;&#35758;&#20351;&#29992;&#36825;&#31181;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#26469;&#36873;&#25321;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20986;&#22312;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#19981;&#20165;&#36866;&#29992;&#20110;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#65292;&#32780;&#19988;&#36824;&#25193;&#23637;&#21040;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#22791;&#20174;&#22810;&#31181;&#35821;&#35328;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20294;&#35780;&#20272;&#36890;&#24120;&#21482;&#20851;&#27880;&#21516;&#19968;&#31181;&#35821;&#35328;&#30340;&#29305;&#23450;&#32452;&#21512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;CodeBERT&#27169;&#22411;&#30340;&#32534;&#31243;&#35821;&#35328;&#34920;&#31034;&#20998;&#26512;&#26469;&#35780;&#20272;&#32534;&#31243;&#35821;&#35328;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20687;C++&#12289;Python&#21644;Java&#36825;&#26679;&#30340;&#35821;&#35328;&#20013;&#30340;&#26631;&#35760;&#34920;&#31034;&#20043;&#38388;&#23384;&#22312;&#30456;&#36817;&#24615;&#65292;&#32780;&#20687;Mathematica&#21644;R&#36825;&#26679;&#30340;&#35821;&#35328;&#20013;&#30340;&#30456;&#21516;&#26631;&#35760;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#19981;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25105;&#20204;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#36873;&#25321;&#19968;&#20010;&#21487;&#20197;&#24179;&#34913;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in Transformer-based Language Models have demonstrated significant potential in enhancing the multilingual capabilities of these models. The remarkable progress made in this domain not only applies to natural language tasks but also extends to the domain of programming languages. Despite the ability of these models to learn from multiple languages, evaluations typically focus on particular combinations of the same languages. In this study, we evaluate the similarity of programming languages by analyzing their representations using a CodeBERT-based model. Our experiments reveal that token representation in languages such as C++, Python, and Java exhibit proximity to one another, whereas the same tokens in languages such as Mathematica and R display significant dissimilarity. Our findings suggest that this phenomenon can potentially result in performance challenges when dealing with diverse languages. Thus, we recommend using our similarity measure to select a div
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.13352</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#23436;&#20840;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#27745;&#26579;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A Generic Machine Learning Framework for Fully-Unsupervised Anomaly Detection with Contaminated Data. (arXiv:2308.13352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13352
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21547;&#26377;&#24322;&#24120;&#26679;&#26412;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#35299;&#20915;&#20102;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#20219;&#21153;&#12290;&#36825;&#20123;&#31639;&#27861;&#20013;&#22823;&#22810;&#25968;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#23545;&#19968;&#20010;&#22522;&#20110;&#27531;&#24046;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#26681;&#25454;&#26410;&#35265;&#26679;&#26412;&#19982;&#23398;&#20064;&#21040;&#30340;&#27491;&#24120;&#33539;&#22260;&#30340;&#19981;&#30456;&#20284;&#24615;&#26469;&#20998;&#37197;&#24322;&#24120;&#20998;&#25968;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#21487;&#20197;&#29992;&#26080;&#24322;&#24120;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#20250;&#19982;&#19968;&#23450;&#27604;&#20363;&#30340;&#24322;&#24120;&#26679;&#26412;&#28151;&#21512;&#12290;&#32780;&#21033;&#29992;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24517;&#28982;&#20250;&#23548;&#33268;&#22522;&#20110;&#27531;&#24046;&#30340;&#31639;&#27861;&#30340;AD&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#29992;&#20110;AD&#20219;&#21153;&#30340;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#30340;&#25913;&#36827;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;&#27531;&#24046;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#22810;&#20803;&#26102;&#38388;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) tasks have been solved using machine learning algorithms in various domains and applications. The great majority of these algorithms use normal data to train a residual-based model, and assign anomaly scores to unseen samples based on their dissimilarity with the learned normal regime. The underlying assumption of these approaches is that anomaly-free data is available for training. This is, however, often not the case in real-world operational settings, where the training data may be contaminated with a certain fraction of abnormal samples. Training with contaminated data, in turn, inevitably leads to a deteriorated AD performance of the residual-based algorithms.  In this paper we introduce a framework for a fully unsupervised refinement of contaminated training data for AD tasks. The framework is generic and can be applied to any residual-based machine learning model. We demonstrate the application of the framework to two public datasets of multivariate time s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21387;&#32553;&#26426;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#22312;&#25151;&#39076;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#24515;&#24459;&#38388;&#38548;&#24207;&#21015;&#36827;&#34892;&#21387;&#32553;&#36317;&#31163;&#35745;&#31639;&#21644;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#27169;&#22411;&#20248;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#25509;&#36817;&#20110;&#26368;&#20339;&#30340;&#25151;&#39076;&#26816;&#27979;&#31639;&#27861;&#12290;&#36825;&#34920;&#26126;gzip&#20998;&#31867;&#22120;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#21644;&#36830;&#32493;&#38543;&#26426;&#24207;&#21015;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.13328</link><description>&lt;p&gt;
&#22522;&#20110;&#21387;&#32553;&#26426;&#30340;&#25151;&#39076;&#26816;&#27979;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Compressor-Based Classification for Atrial Fibrillation Detection. (arXiv:2308.13328v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21387;&#32553;&#26426;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#22312;&#25151;&#39076;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#24515;&#24459;&#38388;&#38548;&#24207;&#21015;&#36827;&#34892;&#21387;&#32553;&#36317;&#31163;&#35745;&#31639;&#21644;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#27169;&#22411;&#20248;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#25509;&#36817;&#20110;&#26368;&#20339;&#30340;&#25151;&#39076;&#26816;&#27979;&#31639;&#27861;&#12290;&#36825;&#34920;&#26126;gzip&#20998;&#31867;&#22120;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#21644;&#36830;&#32493;&#38543;&#26426;&#24207;&#21015;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25151;&#39076;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#24459;&#22833;&#24120;&#20043;&#19968;&#65292;&#23545;&#20844;&#20849;&#20581;&#24247;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#33258;&#21160;&#26816;&#27979;&#25151;&#39076;&#21457;&#20316;&#26159;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26412;&#25991;&#23558;&#26368;&#36817;&#24341;&#20837;&#30340;&#22522;&#20110;&#21387;&#32553;&#26426;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#25151;&#39076;&#26816;&#27979;&#20219;&#21153;&#65288;&#24515;&#24459;&#20043;&#38388;&#30340;&#20108;&#20998;&#31867;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#916;RR&#21644;RR&#38388;&#26399;&#24207;&#21015;&#24212;&#29992;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65292;k-&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#37197;&#32622;&#65292;&#20197;&#21450;&#26368;&#20339;&#31383;&#21475;&#38271;&#24230;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#65288;&#24179;&#22343;&#25935;&#24863;&#24230;=97.1%&#65292;&#24179;&#22343;&#29305;&#24322;&#24230;=91.7%&#65292;&#26368;&#20339;&#25935;&#24863;&#24230;&#20026;99.8%&#65292;&#26368;&#20339;&#29305;&#24322;&#24230;&#20026;97.6% 5&#25240;&#20132;&#21449;&#39564;&#35777;&#65289;&#12290;&#25152;&#33719;&#24471;&#30340;&#24615;&#33021;&#25509;&#36817;&#20110;&#26368;&#20339;&#30340;&#19987;&#38376;&#30340;&#25151;&#39076;&#26816;&#27979;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;gzip&#20998;&#31867;&#22120;&#65292;&#26368;&#21021;&#29992;&#20110;&#25991;&#26412;&#65292;&#20063;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#21644;&#36830;&#32493;&#38543;&#26426;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Atrial fibrillation (AF) is one of the most common arrhythmias with challenging public health implications. Automatic detection of AF episodes is therefore one of the most important tasks in biomedical engineering. In this paper, we apply the recently introduced method of compressor-based text classification to the task of AF detection (binary classification between heart rhythms). We investigate the normalised compression distance applied to $\Delta$RR and RR-interval sequences, the configuration of the k-Nearest Neighbour classifier, and an optimal window length. We achieve good classification results (avg. sensitivity = 97.1%, avg. specificity = 91.7%, best sensitivity of 99.8%, best specificity of 97.6% with 5-fold cross-validation). Obtained performance is close to the best specialised AF detection algorithms. Our results suggest that gzip classification, originally proposed for texts, is suitable for biomedical data and continuous stochastic sequences in general.
&lt;/p&gt;</description></item><item><title>&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#20250;&#36951;&#24536;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#35777;&#26126;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2308.13320</link><description>&lt;p&gt;
&#24494;&#35843;&#21487;&#33021;&#21066;&#24369;&#22522;&#30784;&#27169;&#22411;&#65307;&#20445;&#30041;&#29305;&#24449;&#21487;&#33021;&#26159;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13320
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#20250;&#36951;&#24536;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#35777;&#26126;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20027;&#35201;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#23481;&#37327;&#21644;&#23545;&#20174;&#20114;&#32852;&#32593;&#19978;&#29228;&#21462;&#30340;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26292;&#38706;&#65292;&#20139;&#26377;&#23384;&#20648;&#20851;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#27010;&#24565;&#30340;&#30693;&#35782;&#30340;&#20248;&#21183;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20135;&#29983;&#20986;&#33394;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#24494;&#35843;&#27169;&#22411;&#22312;&#19982;&#19979;&#28216;&#20219;&#21153;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#35782;&#21035;&#27010;&#24565;&#30340;&#33021;&#21147;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#26174;&#28982;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#22312;&#39318;&#27425;&#23398;&#20064;&#36825;&#20123;&#27010;&#24565;&#26102;&#65292;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#19981;&#21487;&#21462;&#30340;&#29616;&#35937;&#31216;&#20026;&#8220;&#27010;&#24565;&#36951;&#24536;&#8221;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22823;&#22810;&#25968;&#31471;&#21040;&#31471;&#24494;&#35843;&#26041;&#27861;&#37117;&#20005;&#37325;&#21463;&#21040;&#36825;&#31181;&#21103;&#20316;&#29992;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#24403;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained foundation models, owing primarily to their enormous capacity and exposure to vast amount of training data scraped from the internet, enjoy the advantage of storing knowledge about plenty of real-world concepts. Such models are typically fine-tuned on downstream datasets to produce remarkable state-of-the-art performances. While various fine-tuning methods have been devised and are shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks $\textit{different}$ from the downstream one is reduced significantly compared to its pre-trained counterpart. This is clearly undesirable as a huge amount of time and money went into learning those very concepts in the first place. We call this undesirable phenomenon "concept forgetting" and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we also propose a rather simple fix to this problem by designing a method called LDIFS 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PGI&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#21830;&#19994;&#38382;&#39064;&#20013;&#24212;&#29992;&#20110;GPT&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;GPT&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;PGI&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24110;&#21161;&#35299;&#20915;&#20102;&#20154;&#31867;&#26234;&#33021;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13317</link><description>&lt;p&gt;
&#25913;&#36896;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#30340;&#36755;&#20986;: PGI&#26694;&#26550;&#23545;&#27880;&#24847;&#21147;&#21160;&#24577;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics. (arXiv:2308.13317v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PGI&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#21830;&#19994;&#38382;&#39064;&#20013;&#24212;&#29992;&#20110;GPT&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;GPT&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;PGI&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24110;&#21161;&#35299;&#20915;&#20102;&#20154;&#31867;&#26234;&#33021;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Persona-Grouping-Intelligence (PGI)&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;GPT&#27169;&#22411;&#22312;&#23454;&#38469;&#21830;&#19994;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;PGI&#21033;&#29992;GPT&#27169;&#22411;&#30340;&#20869;&#22312;&#33021;&#21147;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#23454;&#39564;&#22312;&#19968;&#20010;&#21830;&#19994;&#22330;&#26223;&#20013;&#36827;&#34892;&#65292;&#35813;&#22330;&#26223;&#23384;&#22312;&#20154;&#31867;&#26234;&#33021;&#34987;&#20302;&#25928;&#30340;&#21830;&#19994;&#27969;&#31243;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;GPT&#27169;&#22411;&#26469;&#20943;&#36731;&#20154;&#31867;&#22312;&#24191;&#27867;&#12289;&#21333;&#35843;&#21644;&#37325;&#22797;&#30340;&#20219;&#21153;&#20013;&#30340;&#24037;&#20316;&#36127;&#33655;&#65292;&#23558;&#37325;&#28857;&#36716;&#21521;&#20915;&#31574;&#27963;&#21160;&#12290;&#35813;&#23454;&#39564;&#29983;&#25104;&#30340;4,000&#20010;&#22238;&#24212;&#30340;&#39564;&#35777;&#20934;&#30830;&#29575;&#20026;93.81%&#65292;&#31361;&#20986;&#20102;PGI&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20154;&#31867;&#26234;&#33021;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20351;&#20225;&#19994;&#29615;&#22659;&#19982;&#20915;&#31574;&#27963;&#21160;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach named Persona-Grouping-Intelligence (PGI), which has been crafted to tackle the challenges posed by GPT models when applied to real-world business issues. PGI leverages the inherent capabilities of the GPT model to comprehend intricate language structures and generate responses that are contextually relevant. The experiment occurred in a business scenario where human intelligence was being underutilized due to less optimized business processes. The primary objective of this approach is to leverage GPT models to reduce the workload on humans in tasks that are extensive, monotonous, and repetitive. Instead, the focus is redirected toward decision-making activities. Remarkably, the experiment yielded an accuracy rate of 93.81% in validating 4,000 responses generated by the model, underscoring the effectiveness of the PGI strategies. Effectively addressing the issue of underutilized human intelligence, this paradigm shift aligns business environments wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;H&amp;E Otsu thresholding&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#24555;&#36895;&#21435;&#38500;&#26631;&#26412;&#27531;&#30041;&#29289;&#21644;&#20998;&#21106;&#32452;&#32455;&#65292;&#22312;&#34880;&#26579;&#20057;&#37240;&#27915;&#32418;&#26579;&#33394;&#30340;&#27963;&#26816;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13304</link><description>&lt;p&gt;
&#28608;&#20809;&#21644;&#26631;&#26412;&#28040;&#22833;&#20102;&#65281;&#24555;&#36895;&#21435;&#38500;&#26631;&#26412;&#27531;&#30041;&#29289;&#21644;&#32452;&#32455;&#20998;&#21106;&#22312;&#34880;&#26579;&#20057;&#37240;&#27915;&#32418;&#26579;&#33394;&#27963;&#26816;&#20013;
&lt;/p&gt;
&lt;p&gt;
Bang and the Artefacts are Gone! Rapid Artefact Removal and Tissue Segmentation in Haematoxylin and Eosin Stained Biopsies. (arXiv:2308.13304v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;H&amp;E Otsu thresholding&#30340;&#26041;&#26696;&#65292;&#21487;&#20197;&#24555;&#36895;&#21435;&#38500;&#26631;&#26412;&#27531;&#30041;&#29289;&#21644;&#20998;&#21106;&#32452;&#32455;&#65292;&#22312;&#34880;&#26579;&#20057;&#37240;&#27915;&#32418;&#26579;&#33394;&#30340;&#27963;&#26816;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;H&amp;E Otsu thresholding&#26041;&#26696;&#65292;&#29992;&#20110;&#24555;&#36895;&#26816;&#27979;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20013;&#30340;&#32452;&#32455;&#65292;&#21487;&#20197;&#28040;&#38500;&#31508;&#36857;&#21644;&#25195;&#25551;&#27531;&#30041;&#31561;&#21508;&#31181;&#19981;&#33391;&#26631;&#26412;&#27531;&#30041;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21040;&#33719;&#21462;&#20302;&#25918;&#22823;&#20493;&#29575;RGB&#20840;&#26223;&#22270;&#20687;&#30340;&#21452;&#23792;&#34920;&#31034;&#65292;&#20174;&#32780;&#21487;&#20197;&#31616;&#21333;&#22320;&#20351;&#29992;Otsu thresholding&#26041;&#27861;&#23558;&#32452;&#32455;&#19982;&#32972;&#26223;&#21644;&#27531;&#30041;&#29289;&#20998;&#31163;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#21508;&#31181;&#26426;&#26500;&#21644;WSI&#25968;&#23383;&#25195;&#25551;&#20202;&#30340;WSI&#21046;&#22791;&#30340;&#22270;&#20687;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#27599;&#24352;&#22270;&#20687;&#37117;&#21253;&#21547;&#22823;&#37327;&#30340;&#27531;&#30041;&#29289;&#65292;&#20854;&#20182;&#26041;&#27861;&#37117;&#26080;&#27861;&#22788;&#29702;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#32654;&#22937;&#20043;&#22788;&#22312;&#20110;&#20854;&#31616;&#27905;&#24615;&#65306;&#36890;&#36807;&#25805;&#20316;RGB&#39068;&#33394;&#31354;&#38388;&#24182;&#20351;&#29992;Otsu thresholding&#21487;&#20197;&#24555;&#36895;&#21435;&#38500;&#27531;&#30041;&#29289;&#24182;&#20998;&#21106;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present H&amp;E Otsu thresholding, a scheme for rapidly detecting tissue in whole-slide images (WSIs) that eliminates a wide range of undesirable artefacts such as pen marks and scanning artefacts. Our method involves obtaining a bid-modal representation of a low-magnification RGB overview image which enables simple Otsu thresholding to separate tissue from background and artefacts. We demonstrate our method on WSIs prepared from a wide range of institutions and WSI digital scanners, each containing substantial artefacts that cause other methods to fail. The beauty of our approach lies in its simplicity: manipulating RGB colour space and using Otsu thresholding allows for the rapid removal of artefacts and segmentation of tissue.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#36229;&#21442;&#25968;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#26550;&#26500;&#24182;&#26356;&#26377;&#25928;&#22320;&#20849;&#20139;&#36328;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.13300</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#36229;&#21442;&#25968;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Compact Neural Networks with Deep Overparameterised Multitask Learning. (arXiv:2308.13300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#36229;&#21442;&#25968;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23398;&#20064;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#26550;&#26500;&#24182;&#26356;&#26377;&#25928;&#22320;&#20849;&#20139;&#36328;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#29992;&#23567;&#21442;&#25968;&#22823;&#23567;&#21644;&#20302;&#35745;&#31639;&#25104;&#26412;&#26469;&#35757;&#32451;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#12289;&#26356;&#24378;&#22823;&#30340;&#20307;&#31995;&#32467;&#26500;&#30456;&#21516;&#25110;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#23588;&#20854;&#22914;&#27492;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#31454;&#20105;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#26550;&#26500;&#24182;&#26356;&#26377;&#25928;&#22320;&#20849;&#20139;&#36328;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#12290;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#65288;NYUv2&#21644;COCO&#65289;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#21508;&#31181;&#21367;&#31215;&#32593;&#32476;&#21644;&#21442;&#25968;&#22823;&#23567;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compact neural network offers many benefits for real-world applications. However, it is usually challenging to train the compact neural networks with small parameter sizes and low computational costs to achieve the same or better model performance compared to more complex and powerful architecture. This is particularly true for multitask learning, with different tasks competing for resources. We present a simple, efficient and effective multitask learning overparameterisation neural network design by overparameterising the model architecture in training and sharing the overparameterised model parameters more effectively across tasks, for better optimisation and generalisation. Experiments on two challenging multitask datasets (NYUv2 and COCO) demonstrate the effectiveness of the proposed method across various convolutional networks and parameter sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#32447;&#35745;&#31639;&#30340;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#22312;&#22122;&#22768;&#34928;&#33853;&#20449;&#36947;&#19978;&#36827;&#34892;&#30340;&#27169;&#25311;&#20449;&#21495;&#20256;&#36755;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#38477;&#20302;&#32047;&#31215;&#36951;&#25022;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13298</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#32447;&#35745;&#31639;&#23454;&#29616;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Linear Bandit Learning via Over-the-Air Computation. (arXiv:2308.13298v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#32447;&#35745;&#31639;&#30340;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#22312;&#22122;&#22768;&#34928;&#33853;&#20449;&#36947;&#19978;&#36827;&#34892;&#30340;&#27169;&#25311;&#20449;&#21495;&#20256;&#36755;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#38477;&#20302;&#32047;&#31215;&#36951;&#25022;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30001;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#35774;&#22791;&#32452;&#25104;&#30340;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#32852;&#37030;&#32972;&#26223;&#19979;&#30340;&#32447;&#24615;&#36172;&#21338;&#23398;&#20064;&#12290;&#27599;&#20010;&#35774;&#22791;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#22312;&#25509;&#25910;&#21040;&#22870;&#21169;&#21518;&#36873;&#25321;&#19968;&#20010;&#21160;&#20316;&#65292;&#24182;&#23558;&#27169;&#22411;&#26356;&#26032;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#26368;&#23567;&#21270;&#25152;&#26377;&#35774;&#22791;&#30340;&#32047;&#31215;&#36951;&#25022;&#12290;&#20026;&#20102;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#35774;&#22791;&#36890;&#36807;&#26080;&#32447;&#35745;&#31639;&#65288;AirComp&#65289;&#22312;&#22122;&#22768;&#34928;&#33853;&#20449;&#36947;&#19978;&#19982;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#20854;&#20013;&#36890;&#36947;&#22122;&#22768;&#21487;&#33021;&#20250;&#25197;&#26354;&#20449;&#21495;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#26041;&#26696;&#65292;&#20854;&#20013;&#27599;&#20010;&#35774;&#22791;&#20256;&#36755;&#19968;&#20010;&#27169;&#25311;&#20449;&#21495;&#65292;&#26381;&#21153;&#22120;&#25509;&#25910;&#21040;&#30340;&#26159;&#36825;&#20123;&#20449;&#21495;&#30340;&#21472;&#21152;&#65292;&#21463;&#21040;&#20449;&#36947;&#22122;&#22768;&#30340;&#25197;&#26354;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#35813;&#26041;&#26696;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;&#24615;&#33021;&#26041;&#38754;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate federated contextual linear bandit learning within a wireless system that comprises a server and multiple devices. Each device interacts with the environment, selects an action based on the received reward, and sends model updates to the server. The primary objective is to minimize cumulative regret across all devices within a finite time horizon. To reduce the communication overhead, devices communicate with the server via over-the-air computation (AirComp) over noisy fading channels, where the channel noise may distort the signals. In this context, we propose a customized federated linear bandits scheme, where each device transmits an analog signal, and the server receives a superposition of these signals distorted by channel noise. A rigorous mathematical analysis is conducted to determine the regret bound of the proposed scheme. Both theoretical analysis and numerical experiments demonstrate the competitive performance of our proposed scheme in terms o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;REINFORCE&#31639;&#27861;&#30340;&#24402;&#19968;&#21270;&#27969;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#35745;&#31639;&#23494;&#38598;&#22411;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#22312;&#20108;&#32500;Schwinger&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#20272;&#35745;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22681;&#26102;&#38047;&#26102;&#38388;&#19978;&#24555;10&#20493;&#65292;&#19988;&#20869;&#23384;&#20351;&#29992;&#19978;&#33410;&#30465;30%&#12290;</title><link>http://arxiv.org/abs/2308.13294</link><description>&lt;p&gt;
&#35757;&#32451;&#20855;&#26377;&#35745;&#31639;&#23494;&#38598;&#22411;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#30340;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Training normalizing flows with computationally intensive target probability distributions. (arXiv:2308.13294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;REINFORCE&#31639;&#27861;&#30340;&#24402;&#19968;&#21270;&#27969;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#35745;&#31639;&#23494;&#38598;&#22411;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#22312;&#20108;&#32500;Schwinger&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#20272;&#35745;&#22120;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22681;&#26102;&#38047;&#26102;&#38388;&#19978;&#24555;10&#20493;&#65292;&#19988;&#20869;&#23384;&#20351;&#29992;&#19978;&#33410;&#30465;30%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#25152;&#35859;&#30340;&#24402;&#19968;&#21270;&#27969;&#65292;&#22312;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#26684;&#28857;&#22330;&#35770;&#65288;LFT&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#20998;&#24067;&#30001;&#20316;&#29992;&#30340;&#25351;&#25968;&#32473;&#20986;&#12290;&#22522;&#20110;&#8220;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#8221;&#30340;&#24120;&#35265;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#38656;&#35201;&#23545;&#22330;&#30340;&#23548;&#25968;&#36827;&#34892;&#35745;&#31639;&#12290;&#23545;&#20110;&#22797;&#26434;&#30340;&#38750;&#23616;&#37096;&#20316;&#29992;&#65292;&#22914;QCD&#20013;&#30340;&#36153;&#31859;&#23376;&#20316;&#29992;&#65292;&#36825;&#21487;&#33021;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;REINFORCE&#31639;&#27861;&#30340;&#24402;&#19968;&#21270;&#27969;&#20272;&#35745;&#22120;&#65292;&#20197;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#20851;&#38190;&#24615;&#30340;&#20108;&#32500;Schwinger&#27169;&#22411;&#19982;Wilson&#36153;&#31859;&#23376;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30456;&#23545;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#20272;&#35745;&#22120;&#22312;&#22681;&#26102;&#38047;&#26102;&#38388;&#19978;&#24555;10&#20493;&#20197;&#21450;&#22312;&#20869;&#23384;&#20351;&#29992;&#19978;&#33410;&#30465;30%&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques, in particular the so-called normalizing flows, are becoming increasingly popular in the context of Monte Carlo simulations as they can effectively approximate target probability distributions. In the case of lattice field theories (LFT) the target distribution is given by the exponential of the action. The common loss function's gradient estimator based on the "reparametrization trick" requires the calculation of the derivative of the action with respect to the fields. This can present a significant computational cost for complicated, non-local actions like e.g. fermionic action in QCD. In this contribution, we propose an estimator for normalizing flows based on the REINFORCE algorithm that avoids this issue. We apply it to two dimensional Schwinger model with Wilson fermions at criticality and show that it is up to ten times faster in terms of the wall-clock time as well as requiring up to $30\%$ less memory than the reparameterization trick estimator. It 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#27604;&#36739;&#35780;&#21028;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#25945;&#32946;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#31561;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#22914;&#20309;&#36873;&#25321;&#27604;&#36739;&#39033;&#30446;&#30340;&#21487;&#38752;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.13292</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#27604;&#36739;&#35780;&#21028;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Active Learning Approach to Comparative Judgement. (arXiv:2308.13292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13292
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#30340;&#27604;&#36739;&#35780;&#21028;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#25945;&#32946;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#31561;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20102;&#22914;&#20309;&#36873;&#25321;&#27604;&#36739;&#39033;&#30446;&#30340;&#21487;&#38752;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26159;&#25945;&#32946;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#20256;&#32479;&#30340;&#35780;&#20998;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#19968;&#33268;&#24615;&#19981;&#36275;&#65292;&#23384;&#22312;&#26080;&#24847;&#35782;&#30340;&#20559;&#35265;&#65292;&#32473;&#35780;&#20272;&#32773;&#24102;&#26469;&#36739;&#22823;&#30340;&#35748;&#30693;&#36127;&#25285;&#12290;&#27604;&#36739;&#35780;&#21028;&#65288;CJ&#65289;&#26159;&#19968;&#31181;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;CJ&#20013;&#65292;&#35780;&#20272;&#32773;&#20197;&#19968;&#23545;&#39033;&#30446;&#20026;&#21333;&#20301;&#65292;&#36873;&#25321;&#21738;&#20010;&#26356;&#22909;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#27604;&#36739;&#65292;&#21487;&#20197;&#20351;&#29992;&#25490;&#21517;&#27169;&#22411;&#65288;&#20363;&#22914;BTM&#65289;&#25512;&#23548;&#20986;&#19968;&#20010;&#25490;&#21517;&#12290;&#34429;&#28982;CJ&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#21487;&#38752;&#30340;&#35780;&#20998;&#26041;&#27861;&#65292;&#20294;&#20173;&#23384;&#22312;&#36879;&#26126;&#24230;&#21644;&#29983;&#25104;&#21487;&#38752;&#25490;&#21517;&#25152;&#38656;&#30340;&#23545;&#27604;&#27425;&#25968;&#30340;&#29702;&#24819;&#25968;&#37327;&#31561;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24050;&#26377;&#23581;&#35797;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#20197;&#26377;&#25928;&#26041;&#24335;&#36873;&#25321;&#19979;&#19968;&#20010;&#24212;&#27604;&#36739;&#30340;&#39033;&#30446;&#65292;&#20294;&#26576;&#20123;&#29616;&#26377;&#26041;&#27861;&#20250;&#22312;&#32467;&#26524;&#20013;&#20135;&#29983;&#33258;&#24049;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25152;&#20351;&#29992;&#30340;&#21487;&#38752;&#24615;&#24230;&#37327;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessment is a crucial part of education. Traditional marking is a source of inconsistencies and unconscious bias, placing a high cognitive load on the assessors. An approach to address these issues is comparative judgement (CJ). In CJ, the assessor is presented with a pair of items and is asked to select the better one. Following a series of comparisons, a rank is derived using a ranking model, for example, the BTM, based on the results. While CJ is considered a reliable method for marking, there are concerns around transparency, and the ideal number of pairwise comparisons to generate a reliable estimation of the rank order is not known. Additionally, there have been attempts to generate a method of selecting pairs that should be compared next in an informative manner, but some existing methods are known to have created their own bias within results inflating the reliability metric used. As a result, a random selection approach is usually deployed.  We propose a novel Bayesian appro
&lt;/p&gt;</description></item><item><title>JAX-LOB&#26159;&#31532;&#19968;&#20010;GPU&#21152;&#36895;&#30340;LOB&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#24182;&#34892;&#22788;&#29702;&#25968;&#21315;&#20010;&#35746;&#21333;&#31807;&#65292;&#20197;&#36739;&#20302;&#30340;&#22788;&#29702;&#26102;&#38388;&#23454;&#29616;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20132;&#26131;&#65292;&#20026;&#37329;&#34701;&#20132;&#26131;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.13289</link><description>&lt;p&gt;
JAX-LOB&#65306;&#19968;&#31181;&#22522;&#20110;GPU&#21152;&#36895;&#30340;&#38480;&#20215;&#21333;&#31807;&#27169;&#25311;&#22120;&#65292;&#20026;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20132;&#26131;&#35299;&#38145;
&lt;/p&gt;
&lt;p&gt;
JAX-LOB: A GPU-Accelerated limit order book simulator to unlock large scale reinforcement learning for trading. (arXiv:2308.13289v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13289
&lt;/p&gt;
&lt;p&gt;
JAX-LOB&#26159;&#31532;&#19968;&#20010;GPU&#21152;&#36895;&#30340;LOB&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#24182;&#34892;&#22788;&#29702;&#25968;&#21315;&#20010;&#35746;&#21333;&#31807;&#65292;&#20197;&#36739;&#20302;&#30340;&#22788;&#29702;&#26102;&#38388;&#23454;&#29616;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20132;&#26131;&#65292;&#20026;&#37329;&#34701;&#20132;&#26131;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#37329;&#34701;&#20132;&#26131;&#25152;&#20351;&#29992;&#38480;&#20215;&#21333;&#31807;&#65288;LOB&#65289;&#26469;&#22788;&#29702;&#35746;&#21333;&#21644;&#21305;&#37197;&#20132;&#26131;&#12290;&#20026;&#20102;&#30740;&#31350;&#30446;&#30340;&#65292;&#38656;&#35201;&#20855;&#26377;&#22823;&#35268;&#27169;&#39640;&#25928;&#30340;LOB&#21160;&#24577;&#27169;&#25311;&#22120;&#12290;&#20197;&#21069;&#26366;&#22312;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#65288;ABMs&#65289;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20013;&#23454;&#29616;&#20102;LOB&#27169;&#25311;&#22120;&#65292;&#22788;&#29702;&#26469;&#33258;&#21382;&#21490;&#25968;&#25454;&#38598;&#21644;&#25163;&#24037;&#20195;&#29702;&#30340;&#35746;&#21333;&#27969;&#12290;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#65292;&#38656;&#35201;&#22788;&#29702;&#22810;&#20010;&#35746;&#21333;&#31807;&#65292;&#26080;&#35770;&#26159;&#29992;&#20110;ABM&#30340;&#26657;&#20934;&#36824;&#26159;RL&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39318;&#20010;&#33021;&#22815;&#24182;&#34892;&#22788;&#29702;&#25968;&#21315;&#26412;&#35746;&#21333;&#31807;&#19988;&#27599;&#20010;&#28040;&#24687;&#22788;&#29702;&#26102;&#38388;&#26174;&#33879;&#20943;&#23569;&#30340;GPU-enabled LOB&#27169;&#25311;&#22120;-JAX-LOB&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;JAX-LOB&#30340;&#23454;&#29616;&#22522;&#20110;&#35774;&#35745;&#36873;&#25321;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;JAX&#30340;&#21151;&#33021;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#19982;LOB&#30456;&#20851;&#30340;&#26426;&#21046;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#23558;JAX-LOB&#19982;&#20854;&#20182;JAX&#21253;&#38598;&#25104;&#65292;&#20197;&#25552;&#20379;&#22914;&#20309;&#36866;&#29992;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial exchanges across the world use limit order books (LOBs) to process orders and match trades. For research purposes it is important to have large scale efficient simulators of LOB dynamics. LOB simulators have previously been implemented in the context of agent-based models (ABMs), reinforcement learning (RL) environments, and generative models, processing order flows from historical data sets and hand-crafted agents alike. For many applications, there is a requirement for processing multiple books, either for the calibration of ABMs or for the training of RL agents. We showcase the first GPU-enabled LOB simulator designed to process thousands of books in parallel, with a notably reduced per-message processing time. The implementation of our simulator - JAX-LOB - is based on design choices that aim to best exploit the powers of JAX without compromising on the realism of LOB-related mechanisms. We integrate JAX-LOB with other JAX packages, to provide an example of how one may ad
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AtmoRep&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#30830;&#23450;&#22797;&#26434;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13280</link><description>&lt;p&gt;
AtmoRep:&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning. (arXiv:2308.13280v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13280
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AtmoRep&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#30830;&#23450;&#22797;&#26434;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#20174;&#32780;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27668;&#23545;&#20154;&#31867;&#26377;&#22810;&#31181;&#24433;&#21709;&#65292;&#20174;&#22240;&#22825;&#27668;&#19981;&#33391;&#32780;&#20007;&#29983;&#30340;&#25439;&#22833;&#21040;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23545;&#22823;&#27668;&#21160;&#21147;&#23398;&#36827;&#34892;&#35745;&#31639;&#26426;&#27169;&#25311;&#23545;&#25105;&#20204;&#21644;&#26410;&#26469;&#30340;&#19990;&#20195;&#30340;&#31119;&#31049;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AtmoRep&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#38543;&#26426;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#21487;&#20197;&#20026;&#24191;&#27867;&#30340;&#24212;&#29992;&#25552;&#20379;&#25216;&#33021;&#32467;&#26524;&#12290;AtmoRep&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#35268;&#27169;&#34920;&#31034;&#23398;&#20064;&#26469;&#30830;&#23450;&#22823;&#27668;&#39640;&#24230;&#22797;&#26434;&#12289;&#38543;&#26426;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#25551;&#36848;&#65292;&#35813;&#25551;&#36848;&#22522;&#20110;&#21382;&#21490;&#36712;&#36857;&#30340;&#26368;&#20339;&#21487;&#29992;&#20272;&#35745;&#65292;&#36825;&#20123;&#21382;&#21490;&#36712;&#36857;&#21463;&#35266;&#27979;&#32422;&#26463;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#21644;&#19968;&#20010;&#29420;&#29305;&#30340;&#38598;&#21512;&#23454;&#29616;&#30340;&#65292;&#35813;&#38598;&#21512;&#20174;&#38543;&#26426;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#20854;&#21487;&#21464;&#24615;&#21463;&#21382;&#21490;&#35760;&#24405;&#20013;&#30340;&#21487;&#21464;&#24615;&#21551;&#21457;&#12290;AtmoRep&#30340;&#20219;&#21153;&#26080;&#20851;&#24615;&#20351;&#20854;&#33021;&#22815;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#28789;&#27963;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The atmosphere affects humans in a multitude of ways, from loss of life due to adverse weather effects to long-term social and economic impacts on societies. Computer simulations of atmospheric dynamics are, therefore, of great importance for the well-being of our and future generations. Here, we propose AtmoRep, a novel, task-independent stochastic computer model of atmospheric dynamics that can provide skillful results for a wide range of applications. AtmoRep uses large-scale representation learning from artificial intelligence to determine a general description of the highly complex, stochastic dynamics of the atmosphere from the best available estimate of the system's historical trajectory as constrained by observations. This is enabled by a novel self-supervised learning objective and a unique ensemble that samples from the stochastic model with a variability informed by the one in the historical record. The task-independent nature of AtmoRep enables skillful results for a divers
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23558;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13279</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24471;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Random Forests. (arXiv:2308.13279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13279
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23558;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30001;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20998;&#23618;&#32467;&#26500;&#65288;&#26080;&#35770;&#26159;&#38544;&#24335;&#36824;&#26159;&#26174;&#24335;&#65289;&#32780;&#25104;&#20026;&#34920;&#31034;&#25968;&#25454;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#38656;&#35201;&#33021;&#22815;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#35299;&#20915;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#30340;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#26377;&#22810;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#36229;&#24179;&#38754;&#30340;&#20998;&#31867;&#22120;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20998;&#23618;&#25968;&#25454;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#21040;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#30001;&#20110;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#20998;&#21106;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#22823;&#36793;&#30028;&#20998;&#31867;&#22120;&#25214;&#21040;&#20505;&#36873;&#30340;&#27700;&#24179;&#29699;&#12290;&#20026;&#20102;&#20351;&#38750;&#27431;&#20960;&#37324;&#24471;&#38543;&#26426;&#26862;&#26519;&#36866;&#29992;&#20110;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#23427;&#20204;&#30340;&#26368;&#20302;&#20844;&#20849;&#31062;&#20808;&#21644;&#31867;&#24179;&#34913;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic space is becoming a popular choice for representing data due to the hierarchical structure - whether implicit or explicit - of many real-world datasets. Along with it comes a need for algorithms capable of solving fundamental tasks, such as classification, in hyperbolic space. Recently, multiple papers have investigated hyperbolic alternatives to hyperplane-based classifiers, such as logistic regression and SVMs. While effective, these approaches struggle with more complex hierarchical data. We, therefore, propose to generalize the well-known random forests to hyperbolic space. We do this by redefining the notion of a split using horospheres. Since finding the globally optimal split is computationally intractable, we find candidate horospheres through a large-margin classifier. To make hyperbolic random forests work on multi-class data and imbalanced experiments, we furthermore outline a new method for combining classes based on their lowest common ancestor and a class-balan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;Decision Transformers&#38598;&#25104;&#21040;&#35821;&#35328;&#39537;&#21160;&#30340;&#29983;&#25104;&#36136;&#37327;&#22810;&#26679;&#24615;&#38382;&#39064;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#21152;&#20855;&#26377;&#36712;&#36857;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24211;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20381;&#36182;&#20110;&#36825;&#20123;&#25551;&#36848;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13278</link><description>&lt;p&gt;
&#23558;LLMs&#21644;Decision Transformers&#38598;&#25104;&#21040;&#35821;&#35328;&#39537;&#21160;&#30340;&#29983;&#25104;&#36136;&#37327;&#22810;&#26679;&#24615;&#20013;
&lt;/p&gt;
&lt;p&gt;
Integrating LLMs and Decision Transformers for Language Grounded Generative Quality-Diversity. (arXiv:2308.13278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;Decision Transformers&#38598;&#25104;&#21040;&#35821;&#35328;&#39537;&#21160;&#30340;&#29983;&#25104;&#36136;&#37327;&#22810;&#26679;&#24615;&#38382;&#39064;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#21152;&#20855;&#26377;&#36712;&#36857;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24211;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20381;&#36182;&#20110;&#36825;&#20123;&#25551;&#36848;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#39046;&#22495;&#38382;&#39064;&#30340;&#38543;&#26426;&#20248;&#21270;&#20998;&#25903;&#65292;&#20854;&#30446;&#30340;&#26159;&#26500;&#24314;&#34920;&#29616;&#33391;&#22909;&#19988;&#22312;&#34892;&#20026;&#31354;&#38388;&#19978;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#25919;&#31574;/&#25216;&#33021;&#24211;&#12290;&#36825;&#26679;&#30340;&#23384;&#26723;&#36890;&#24120;&#30001;&#26377;&#38480;&#25968;&#37327;&#30340;&#21453;&#24212;&#20195;&#29702;&#32452;&#25104;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#19982;&#21807;&#19968;&#30340;&#34892;&#20026;&#25551;&#36848;&#31526;&#30456;&#20851;&#32852;&#65292;&#32780;&#22312;&#31895;&#30053;&#31163;&#25955;&#21270;&#30340;&#31354;&#38388;&#20043;&#22806;&#23454;&#20363;&#21270;&#34892;&#20026;&#25551;&#36848;&#31526;&#24182;&#19981;&#30452;&#35266;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#29983;&#25104;&#30340;&#36712;&#36857;&#22312;&#30446;&#26631;&#34892;&#20026;&#25551;&#36848;&#31526;&#35268;&#23450;&#20043;&#22806;&#24456;&#38590;&#36827;&#34892;&#23450;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#20855;&#26377;&#38745;&#24577;&#22330;&#26223;&#20803;&#32032;&#30340;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#21152;&#20855;&#26377;&#36712;&#36857;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#24211;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#20381;&#36182;&#20110;&#36825;&#20123;&#25551;&#36848;&#30340;&#31574;&#30053;&#26469;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality-Diversity is a branch of stochastic optimization that is often applied to problems from the Reinforcement Learning and control domains in order to construct repertoires of well-performing policies/skills that exhibit diversity with respect to a behavior space. Such archives are usually composed of a finite number of reactive agents which are each associated to a unique behavior descriptor, and instantiating behavior descriptors outside of that coarsely discretized space is not straight-forward. While a few recent works suggest solutions to that issue, the trajectory that is generated is not easily customizable beyond the specification of a target behavior descriptor. We propose to jointly solve those problems in environments where semantic information about static scene elements is available by leveraging a Large Language Model to augment the repertoire with natural language descriptions of trajectories, and training a policy conditioned on those descriptions. Thus, our method 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HDUS&#30340;&#20998;&#24067;&#24335;&#36951;&#24536;&#26694;&#26550;&#65292;&#20351;&#29992;&#31181;&#23376;&#27169;&#22411;&#33976;&#39311;&#26500;&#24314;&#21487;&#25830;&#38500;&#30340;&#27169;&#22411;&#38598;&#25104;&#65292;&#36866;&#29992;&#20110;&#24322;&#26500;&#30340;&#35774;&#22791;&#31471;&#27169;&#22411;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13269</link><description>&lt;p&gt;
&#24322;&#26500;&#20998;&#24067;&#24335;&#26426;&#22120;&#36951;&#24536;&#21644;&#31181;&#23376;&#27169;&#22411;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Decentralized Machine Unlearning with Seed Model Distillation. (arXiv:2308.13269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HDUS&#30340;&#20998;&#24067;&#24335;&#36951;&#24536;&#26694;&#26550;&#65292;&#20351;&#29992;&#31181;&#23376;&#27169;&#22411;&#33976;&#39311;&#26500;&#24314;&#21487;&#25830;&#38500;&#30340;&#27169;&#22411;&#38598;&#25104;&#65292;&#36866;&#29992;&#20110;&#24322;&#26500;&#30340;&#35774;&#22791;&#31471;&#27169;&#22411;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19968;&#20123;&#26368;&#36817;&#30340;&#20449;&#24687;&#23433;&#20840;&#27861;&#35268;&#36171;&#20104;&#29992;&#25143;&#23545;&#20219;&#20309;&#32463;&#36807;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25317;&#26377;&#34987;&#36951;&#24536;&#30340;&#26080;&#26465;&#20214;&#26435;&#21033;&#65292;&#20010;&#24615;&#21270;&#29289;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#21830;&#24517;&#39035;&#32771;&#34385;&#21040;&#36951;&#24536;&#21151;&#33021;&#12290;&#21462;&#28040;&#23398;&#20064;&#29992;&#25143;&#36129;&#29486;&#30340;&#26368;&#30452;&#25509;&#26041;&#27861;&#26159;&#20174;&#21021;&#22987;&#29366;&#24577;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#22312;&#39057;&#32321;&#30340;&#36951;&#24536;&#35831;&#27714;&#20013;&#65292;&#36825;&#22312;&#39640;&#21534;&#21520;&#37327;&#24212;&#29992;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#19968;&#20123;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#26469;&#21152;&#36895;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#36866;&#24212;&#20998;&#24067;&#24335;&#23398;&#20064;&#22330;&#26223;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;HDUS&#30340;&#20998;&#24067;&#24335;&#36951;&#24536;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#33976;&#39311;&#30340;&#31181;&#23376;&#27169;&#22411;&#20026;&#25152;&#26377;&#23458;&#25143;&#31471;&#26500;&#24314;&#21487;&#25830;&#38500;&#30340;&#38598;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#19982;&#24322;&#26500;&#30340;&#35774;&#22791;&#31471;&#27169;&#22411;&#20860;&#23481;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#21487;&#20280;&#32553;&#24615;&#65292;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;HDUS&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As some recent information security legislation endowed users with unconditional rights to be forgotten by any trained machine learning model, personalized IoT service providers have to put unlearning functionality into their consideration. The most straightforward method to unlearn users' contribution is to retrain the model from the initial state, which is not realistic in high throughput applications with frequent unlearning requests. Though some machine unlearning frameworks have been proposed to speed up the retraining process, they fail to match decentralized learning scenarios. In this paper, we design a decentralized unlearning framework called HDUS, which uses distilled seed models to construct erasable ensembles for all clients. Moreover, the framework is compatible with heterogeneous on-device models, representing stronger scalability in real-world applications. Extensive experiments on three real-world datasets show that our HDUS achieves state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20010;&#24615;&#21270;&#29983;&#25104;&#32593;&#32476;&#23454;&#29616;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#23458;&#25143;&#31471;&#20043;&#38388;&#30693;&#35782;&#20256;&#36882;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13265</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#29983;&#25104;&#32593;&#32476;&#23454;&#29616;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Federated Learning via Personalized Generative Networks. (arXiv:2308.13265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20010;&#24615;&#21270;&#29983;&#25104;&#32593;&#32476;&#23454;&#29616;&#20102;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#23458;&#25143;&#31471;&#20043;&#38388;&#30693;&#35782;&#20256;&#36882;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#26500;&#24314;&#19968;&#20010;&#20849;&#21516;&#30340;&#20840;&#23616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#65292;&#36825;&#38477;&#20302;&#20102;&#24615;&#33021;&#24182;&#20943;&#24930;&#20102;&#21521;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#26126;&#65292;&#26368;&#23567;&#21270;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#26377;&#21161;&#20110;&#27599;&#20010;&#21333;&#29420;&#23458;&#25143;&#31471;&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#25910;&#25947;&#12290;&#36825;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20986;&#29616;&#32463;&#39564;&#27010;&#24565;&#36716;&#21464;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#32771;&#34385;&#21040;&#24050;&#34987;&#30740;&#31350;&#36807;&#30340;&#19981;&#24179;&#34913;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#20256;&#36882;&#26041;&#27861;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#35757;&#32451;&#23458;&#25143;&#31471;&#29305;&#23450;&#30340;&#29983;&#25104;&#22120;&#12290;&#27599;&#20010;&#29983;&#25104;&#22120;&#20026;&#30456;&#24212;&#30340;&#23458;&#25143;&#31471;&#29983;&#25104;&#26679;&#26412;&#65292;&#20197;&#28040;&#38500;&#19982;&#20854;&#20182;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#20914;&#31361;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20197;&#21450;&#29702;&#35770;&#30740;&#31350;&#25903;&#25345;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) allows several clients to construct a common global machine-learning model without having to share their data. FL, however, faces the challenge of statistical heterogeneity between the client's data, which degrades performance and slows down the convergence toward the global model. In this paper, we provide theoretical proof that minimizing heterogeneity between clients facilitates the convergence of a global model for every single client. This becomes particularly important under empirical concept shifts among clients, rather than merely considering imbalanced classes, which have been studied until now. Therefore, we propose a method for knowledge transfer between clients where the server trains client-specific generators. Each generator generates samples for the corresponding client to remove the conflict with other clients' models. Experiments conducted on synthetic and real data, along with a theoretical study, support the effectiveness of our method in cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#21644;&#38750;&#32447;&#24615;&#36924;&#36817;&#26469;&#35299;&#20915;&#22823;&#22411;&#32622;&#25442;&#30697;&#38453;&#30340;&#32500;&#24230;&#28798;&#38590;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;"&#20146;&#21563;&#25968;"&#29702;&#35770;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#34920;&#31034;&#32622;&#25442;&#30697;&#38453;&#25152;&#38656;&#30340;&#26368;&#23567;&#31209;&#65292;&#20174;&#32780;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#34920;&#36798;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2308.13252</link><description>&lt;p&gt;
&#20146;&#21563;&#23547;&#25214;&#21305;&#37197;&#65306;&#39640;&#25928;&#30340;&#20302;&#31209;&#32622;&#25442;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Kissing to Find a Match: Efficient Low-Rank Permutation Representation. (arXiv:2308.13252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#21644;&#38750;&#32447;&#24615;&#36924;&#36817;&#26469;&#35299;&#20915;&#22823;&#22411;&#32622;&#25442;&#30697;&#38453;&#30340;&#32500;&#24230;&#28798;&#38590;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;"&#20146;&#21563;&#25968;"&#29702;&#35770;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#34920;&#31034;&#32622;&#25442;&#30697;&#38453;&#25152;&#38656;&#30340;&#26368;&#23567;&#31209;&#65292;&#20174;&#32780;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#34920;&#36798;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32622;&#25442;&#30697;&#38453;&#22312;&#21305;&#37197;&#21644;&#20998;&#37197;&#38382;&#39064;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#23588;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26174;&#24335;&#34920;&#31034;&#32622;&#25442;&#30697;&#38453;&#30340;&#20869;&#23384;&#38543;&#38382;&#39064;&#35268;&#27169;&#30340;&#22686;&#22823;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#38480;&#21046;&#20102;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#21644;&#38750;&#32447;&#24615;&#36924;&#36817;&#26469;&#35299;&#20915;&#22823;&#22411;&#32622;&#25442;&#30697;&#38453;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20381;&#38752;"&#20146;&#21563;&#25968;"&#29702;&#35770;&#26469;&#25512;&#26029;&#34920;&#31034;&#32473;&#23450;&#22823;&#23567;&#30340;&#32622;&#25442;&#30697;&#38453;&#25152;&#38656;&#30340;&#26368;&#23567;&#31209;&#65292;&#35813;&#31209;&#26126;&#26174;&#23567;&#20110;&#38382;&#39064;&#30340;&#22823;&#23567;&#12290;&#36825;&#23548;&#33268;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#22823;&#24133;&#38477;&#20302;&#65292;&#20363;&#22914;&#65292;&#22312;&#34920;&#31034;$n=20000$&#30340;&#38382;&#39064;&#26102;&#65292;&#20165;&#20351;&#29992;&#20004;&#20010;&#23567;&#30697;&#38453;&#20013;&#30340;$8.4\times10^5$&#20010;&#20803;&#32032;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547;$4\times 10^8$&#20010;&#20803;&#32032;&#30340;&#24040;&#22823;&#30697;&#38453;&#65292;&#20869;&#23384;&#33410;&#30465;&#20102;$3$&#20010;&#25968;&#37327;&#32423;&#12290;&#25152;&#25552;&#20986;&#30340;&#34920;&#31034;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
Permutation matrices play a key role in matching and assignment problems across the fields, especially in computer vision and robotics. However, memory for explicitly representing permutation matrices grows quadratically with the size of the problem, prohibiting large problem instances. In this work, we propose to tackle the curse of dimensionality of large permutation matrices by approximating them using low-rank matrix factorization, followed by a nonlinearity. To this end, we rely on the Kissing number theory to infer the minimal rank required for representing a permutation matrix of a given size, which is significantly smaller than the problem size. This leads to a drastic reduction in computation and memory costs, e.g., up to $3$ orders of magnitude less memory for a problem of size $n=20000$, represented using $8.4\times10^5$ elements in two small matrices instead of using a single huge matrix with $4\times 10^8$ elements. The proposed representation allows for accurate represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24212;&#29992;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#38543;&#26426;&#22870;&#21169;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#38543;&#26426;&#22870;&#21169;&#31283;&#23450;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38543;&#26426;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13246</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;: &#38543;&#26426;&#22870;&#21169;&#31283;&#23450;&#21270;
&lt;/p&gt;
&lt;p&gt;
Model-free Reinforcement Learning with Stochastic Reward Stabilization for Recommender Systems. (arXiv:2308.13246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#24212;&#29992;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#38543;&#26426;&#22870;&#21169;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#38543;&#26426;&#22870;&#21169;&#31283;&#23450;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38543;&#26426;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#22240;&#20854;&#22788;&#29702;&#37096;&#20998;&#21453;&#39304;&#21644;&#38271;&#26399;&#22870;&#21169;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#24573;&#30053;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#65306;&#21516;&#19968;&#29992;&#25143;&#22312;&#19981;&#21516;&#26102;&#38388;&#23545;&#21516;&#19968;&#39033;&#30340;&#21453;&#39304;&#26159;&#38543;&#26426;&#30340;&#12290;&#38543;&#26426;&#22870;&#21169;&#30340;&#29305;&#24615;&#19982;&#20855;&#26377;&#30830;&#23450;&#24615;&#22870;&#21169;&#30340;&#32463;&#20856;RL&#22330;&#26223;&#26412;&#36136;&#19978;&#19981;&#21516;&#65292;&#36825;&#20351;&#24471;&#22522;&#20110;RL&#30340;&#25512;&#33616;&#31995;&#32479;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#20808;&#22312;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#30452;&#25509;&#20351;&#29992;&#38543;&#26426;&#21453;&#39304;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38543;&#26426;&#21453;&#39304;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#38543;&#26426;&#22870;&#21169;&#31283;&#23450;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#29992;&#30417;&#30563;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#26367;&#20195;&#30452;&#25509;&#30340;&#38543;&#26426;&#21453;&#39304;&#12290;&#36825;&#20004;&#20010;&#26694;&#26550;&#37117;&#26159;&#27169;&#22411;&#26080;&#20851;&#30340;&#65292;&#21363;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21508;&#31181;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-free RL-based recommender systems have recently received increasing research attention due to their capability to handle partial feedback and long-term rewards. However, most existing research has ignored a critical feature in recommender systems: one user's feedback on the same item at different times is random. The stochastic rewards property essentially differs from that in classic RL scenarios with deterministic rewards, which makes RL-based recommender systems much more challenging. In this paper, we first demonstrate in a simulator environment where using direct stochastic feedback results in a significant drop in performance. Then to handle the stochastic feedback more efficiently, we design two stochastic reward stabilization frameworks that replace the direct stochastic feedback with that learned by a supervised model. Both frameworks are model-agnostic, i.e., they can effectively utilize various supervised models. We demonstrate the superiority of the proposed framework
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#32676;&#32452;&#20844;&#24179;Plackett-Luce&#25490;&#24207;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#22823;&#21270;&#39044;&#26399;&#30456;&#20851;&#24615;&#24182;&#28385;&#36275;&#34920;&#31034;&#32422;&#26463;&#20197;&#30830;&#20445;&#21518;&#26399;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13242</link><description>&lt;p&gt;
&#20248;&#21270;&#20851;&#20110;&#30456;&#20851;&#24615;&#21644;&#21518;&#26399;&#20844;&#24179;&#24615;&#30340;&#32676;&#32452;&#20844;&#24179;Plackett-Luce&#25490;&#24207;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Optimizing Group-Fair Plackett-Luce Ranking Models for Relevance and Ex-Post Fairness. (arXiv:2308.13242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13242
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#32676;&#32452;&#20844;&#24179;Plackett-Luce&#25490;&#24207;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#22823;&#21270;&#39044;&#26399;&#30456;&#20851;&#24615;&#24182;&#28385;&#36275;&#34920;&#31034;&#32422;&#26463;&#20197;&#30830;&#20445;&#21518;&#26399;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#25490;&#21517;&#20013;&#65292;&#20165;&#20248;&#21270;&#30456;&#20851;&#24615;&#65288;&#25110;&#39044;&#26399;&#25490;&#21517;&#25928;&#29992;&#65289;&#21487;&#33021;&#23545;&#26576;&#20123;&#31867;&#21035;&#30340;&#39033;&#30446;&#36896;&#25104;&#34920;&#29616;&#24615;&#25439;&#23475;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#30456;&#20851;&#24615;&#20998;&#25968;&#20013;&#23384;&#22312;&#38544;&#24615;&#20559;&#35265;&#65292;&#21017;&#23398;&#20064;&#25490;&#21517;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#20248;&#21270;&#30495;&#23454;&#30340;&#30456;&#20851;&#24615;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#35757;&#32451;&#38543;&#26426;&#25490;&#21517;&#27169;&#22411;&#65292;&#20197;&#36798;&#21040;&#32676;&#32452;&#39044;&#26399;&#26292;&#38706;&#30340;&#20844;&#24179;&#24615;&#65288;&#21363;&#26399;&#26395;&#20540;&#65289;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#32676;&#32452;&#21518;&#26399;&#30340;&#34920;&#29616;&#20844;&#24179;&#24615;&#65292;&#21363;&#22312;&#20174;&#38543;&#26426;&#25490;&#24207;&#27169;&#22411;&#20013;&#23454;&#29616;&#25490;&#21517;&#20043;&#21518;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#21518;&#26399;&#22788;&#29702;&#23454;&#29616;&#21518;&#26399;&#20844;&#24179;&#24615;&#65292;&#20294;&#26159;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#35757;&#32451;&#24847;&#35782;&#21040;&#27492;&#21518;&#26399;&#22788;&#29702;&#30340;&#38543;&#26426;&#25490;&#24207;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20165;&#22312;&#28385;&#36275;&#32473;&#23450;&#34920;&#31034;&#32422;&#26463;&#30340;&#25490;&#21517;&#20013;&#26368;&#22823;&#21270;&#39044;&#26399;&#30456;&#20851;&#24615;&#65292;&#20197;&#30830;&#20445;&#21518;&#26399;&#20844;&#24179;&#24615;&#12290;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110;&#21518;&#26399;&#32676;&#32452;&#20844;&#24179;&#25490;&#21517;&#30340;&#26377;&#25928;&#25277;&#26679;&#22120;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20165;&#22312;&#28385;&#36275;&#32473;&#23450;&#34920;&#31034;&#32422;&#26463;&#30340;&#25490;&#21517;&#20013;&#26368;&#22823;&#21270;&#39044;&#26399;&#30456;&#20851;&#24615;&#65292;&#20197;&#30830;&#20445;&#21518;&#26399;&#20844;&#27491;&#24615;&#12290;&#24314;&#31435;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#25277;&#26679;&#22120;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23427;&#26368;&#22823;&#21270;&#22312;&#28385;&#36275;&#32473;&#23450;&#30340;&#34920;&#31034;&#32422;&#26463;&#30340;&#25490;&#21517;&#20013;&#30340;&#39044;&#26399;&#30456;&#20851;&#24615;&#65292;&#20197;&#30830;&#20445;&#21518;&#26399;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In learning-to-rank (LTR), optimizing only the relevance (or the expected ranking utility) can cause representational harm to certain categories of items. Moreover, if there is implicit bias in the relevance scores, LTR models may fail to optimize for true relevance. Previous works have proposed efficient algorithms to train stochastic ranking models that achieve fairness of exposure to the groups ex-ante (or, in expectation), which may not guarantee representation fairness to the groups ex-post, that is, after realizing a ranking from the stochastic ranking model. Typically, ex-post fairness is achieved by post-processing, but previous work does not train stochastic ranking models that are aware of this post-processing.  In this paper, we propose a novel objective that maximizes expected relevance only over those rankings that satisfy given representation constraints to ensure ex-post fairness. Building upon recent work on an efficient sampler for ex-post group-fair rankings, we propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;PINN&#65289;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;&#35777;&#25454;&#26469;&#20248;&#21270;&#27169;&#22411;&#24182;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13222</link><description>&lt;p&gt;
&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Bayesian Reasoning for Physics Informed Neural Networks. (arXiv:2308.13222v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;PINN&#65289;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#31639;&#35777;&#25454;&#26469;&#20248;&#21270;&#27169;&#22411;&#24182;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20844;&#24335;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;MacKay&#22312;Neural Computation&#65288;1992&#24180;&#65289;&#20013;&#25552;&#20986;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#12290;&#36890;&#36807;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#27861;&#65292;&#24471;&#21040;&#21518;&#39564;&#23494;&#24230;&#12290;&#23545;&#20110;&#27599;&#20010;&#27169;&#22411;&#65288;&#25311;&#21512;&#65289;&#65292;&#35745;&#31639;&#25152;&#35859;&#30340;&#35777;&#25454;&#12290;&#23427;&#26159;&#19968;&#31181;&#20998;&#31867;&#20551;&#35774;&#30340;&#24230;&#37327;&#12290;&#26368;&#20248;&#35299;&#20855;&#26377;&#26368;&#22823;&#30340;&#35777;&#25454;&#20540;&#12290;&#36125;&#21494;&#26031;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#36793;&#30028;&#23545;&#24635;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#20107;&#23454;&#19978;&#65292;&#36125;&#21494;&#26031;&#31639;&#27861;&#36890;&#36807;&#24494;&#35843;&#25439;&#22833;&#32452;&#20214;&#30340;&#30456;&#23545;&#26435;&#37325;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#28909;&#21147;&#23398;&#12289;&#27874;&#21160;&#21644;Burger&#26041;&#31243;&#12290;&#25152;&#24471;&#32467;&#26524;&#19982;&#31934;&#30830;&#35299;&#22522;&#26412;&#19968;&#33268;&#12290;&#25152;&#26377;&#35299;&#37117;&#25552;&#20379;&#20102;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#20869;&#35745;&#31639;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics informed neural network (PINN) approach in Bayesian formulation is presented. We adopt the Bayesian neural network framework formulated by MacKay (Neural Computation 4 (3) (1992) 448). The posterior densities are obtained from Laplace approximation. For each model (fit), the so-called evidence is computed. It is a measure that classifies the hypothesis. The most optimal solution has the maximal value of the evidence. The Bayesian framework allows us to control the impact of the boundary contribution to the total loss. Indeed, the relative weights of loss components are fine-tuned by the Bayesian algorithm. We solve heat, wave, and Burger's equations. The obtained results are in good agreement with the exact solutions. All solutions are provided with the uncertainties computed within the Bayesian framework.
&lt;/p&gt;</description></item><item><title>GEMTrans&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#22810;&#32423;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#24515;&#34880;&#31649;&#35786;&#26029;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#22788;&#29702;&#22810;&#20010;&#24515;&#33039;&#35270;&#22270;&#30340;echo&#35270;&#39057;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#24515;&#34880;&#31649;&#27979;&#37327;&#25110;&#35299;&#37322;&#20219;&#21153;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.13217</link><description>&lt;p&gt;
GEMTrans: &#19968;&#31181;&#29992;&#20110;&#24515;&#34880;&#31649;&#35786;&#26029;&#30340;&#36890;&#29992;&#12289;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#22810;&#32423;Transformer&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GEMTrans: A General, Echocardiography-based, Multi-Level Transformer Framework for Cardiovascular Diagnosis. (arXiv:2308.13217v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13217
&lt;/p&gt;
&lt;p&gt;
GEMTrans&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#22810;&#32423;Transformer&#26694;&#26550;&#65292;&#29992;&#20110;&#24515;&#34880;&#31649;&#35786;&#26029;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#22788;&#29702;&#22810;&#20010;&#24515;&#33039;&#35270;&#22270;&#30340;echo&#35270;&#39057;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#24515;&#34880;&#31649;&#27979;&#37327;&#25110;&#35299;&#37322;&#20219;&#21153;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#24515;&#21160;&#22270;&#65288;echo&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#24515;&#34880;&#31649;&#35786;&#26029;&#20219;&#21153;&#30340;&#36229;&#22768;&#25104;&#20687;&#25216;&#26415;&#12290;&#30001;&#20110;&#22522;&#20110;echo&#30340;&#35786;&#26029;&#23384;&#22312;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;echo&#22270;&#20687;&#37319;&#38598;&#30340;&#21464;&#24322;&#24615;&#21644;&#22522;&#20110;&#20020;&#24202;&#32463;&#39564;&#23545;echo&#22270;&#20687;&#30340;&#35299;&#37322;&#23548;&#33268;&#30340;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#20316;&#20026;&#20108;&#27425;&#39564;&#35777;&#23618;&#12290;&#23545;&#20110;&#36825;&#31181;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#20219;&#20309;&#25552;&#20986;&#30340;ML&#26041;&#27861;&#37117;&#24517;&#39035;&#20855;&#22791;&#19968;&#23450;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#26041;&#27861;&#24517;&#39035;&#33021;&#22815;&#22788;&#29702;&#26469;&#33258;&#21508;&#31181;&#24515;&#33039;&#35270;&#22270;&#30340;&#22810;&#20010;echo&#35270;&#39057;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20197;&#36866;&#24403;&#22320;&#23545;&#22810;&#31181;&#24515;&#34880;&#31649;&#27979;&#37327;&#25110;&#35299;&#37322;&#20219;&#21153;&#36827;&#34892;&#39044;&#27979;&#12290;&#29616;&#26377;&#24037;&#20316;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#25110;&#32773;&#22312;&#33539;&#22260;&#19978;&#21463;&#38480;&#20110;&#19987;&#27880;&#20110;&#21333;&#19968;&#24515;&#34880;&#31649;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#22810;&#32423;Transformer&#65288;GEMTrans&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Echocardiography (echo) is an ultrasound imaging modality that is widely used for various cardiovascular diagnosis tasks. Due to inter-observer variability in echo-based diagnosis, which arises from the variability in echo image acquisition and the interpretation of echo images based on clinical experience, vision-based machine learning (ML) methods have gained popularity to act as secondary layers of verification. For such safety-critical applications, it is essential for any proposed ML method to present a level of explainability along with good accuracy. In addition, such methods must be able to process several echo videos obtained from various heart views and the interactions among them to properly produce predictions for a variety of cardiovascular measurements or interpretation tasks. Prior work lacks explainability or is limited in scope by focusing on a single cardiovascular task. To remedy this, we propose a General, Echo-based, Multi-Level Transformer (GEMTrans) framework tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#31639;&#27861;&#65288;PINGO&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13212</link><description>&lt;p&gt;
&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#29992;&#20110;&#38271;&#26399;&#21160;&#21147;&#23398;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Physics-Inspired Neural Graph ODE for Long-term Dynamical Simulation. (arXiv:2308.13212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13212
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE&#31639;&#27861;&#65288;PINGO&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#65292;&#24182;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21644;&#24314;&#27169;&#22810;&#23545;&#35937;&#29289;&#29702;&#31995;&#32479;&#30340;&#38271;&#26399;&#21160;&#24577;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#20855;&#26377;&#31561;&#21464;&#24615;&#36136;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#23545;&#29289;&#29702;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20182;&#20204;&#23558;&#21160;&#21147;&#23398;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#20855;&#26377;&#22266;&#23450;&#26102;&#38388;&#38388;&#38548;&#30340;&#31163;&#25955;&#29366;&#24577;&#65292;&#24182;&#23398;&#20064;&#25152;&#26377;&#30456;&#37051;&#29366;&#24577;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#25509;&#26144;&#23556;&#24573;&#30053;&#20102;&#20004;&#20010;&#29366;&#24577;&#20043;&#38388;&#30340;&#36830;&#32493;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#24050;&#32463;&#39564;&#35777;&#20102;&#22312;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#30452;&#25509;&#26144;&#23556;&#27169;&#22411;&#20013;&#65292;&#20004;&#20010;&#31163;&#25955;&#21160;&#24577;&#29366;&#24577;&#20043;&#38388;&#23384;&#22312;&#26080;&#25968;&#21487;&#33021;&#30340;&#36712;&#36857;&#12290;&#36825;&#20010;&#38382;&#39064;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23548;&#33268;&#38271;&#26399;&#27169;&#25311;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#36890;&#36807;&#31163;&#25955;&#30417;&#30563;&#20449;&#21495;&#24314;&#27169;&#28508;&#22312;&#36712;&#36857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#22270;ODE(PINGO)&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating and modeling the long-term dynamics of multi-object physical systems is an essential and challenging task. Current studies model the physical systems utilizing Graph Neural Networks (GNNs) with equivariant properties. Specifically, they model the dynamics as a sequence of discrete states with a fixed time interval and learn a direct mapping for all the two adjacent states. However, this direct mapping overlooks the continuous nature between the two states. Namely, we have verified that there are countless possible trajectories between two discrete dynamic states in current GNN-based direct mapping models. This issue greatly hinders the model generalization ability, leading to poor performance of the long-term simulation. In this paper, to better model the latent trajectory through discrete supervision signals, we propose a Physics-Inspired Neural Graph ODE (PINGO) algorithm. In PINGO, to ensure the uniqueness of the trajectory, we construct a Physics-Inspired Neural ODE fram
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#38750;&#38190;&#30456;&#20114;&#20316;&#29992;&#31561;&#21464;&#25551;&#36848;&#31526;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#27169;&#25311;&#38271;&#31243;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#38750;&#38190;&#20301;&#21183;&#30340;&#23616;&#37096;&#25551;&#36848;&#31526;&#12290;</title><link>http://arxiv.org/abs/2308.13208</link><description>&lt;p&gt;
&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#38750;&#38190;&#30456;&#20114;&#20316;&#29992;&#31561;&#21464;&#25551;&#36848;&#31526;
&lt;/p&gt;
&lt;p&gt;
Physics-inspired Equivariant Descriptors of Non-bonded Interactions. (arXiv:2308.13208v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13208
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#38750;&#38190;&#30456;&#20114;&#20316;&#29992;&#31561;&#21464;&#25551;&#36848;&#31526;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#27169;&#25311;&#38271;&#31243;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#31867;&#20284;&#38750;&#38190;&#20301;&#21183;&#30340;&#23616;&#37096;&#25551;&#36848;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24212;&#29992;&#20110;&#21407;&#23376;&#23610;&#24230;&#27169;&#25311;&#30340;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#32467;&#26500;&#20960;&#20309;&#30340;&#23616;&#37096;&#25551;&#36848;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#30001;&#38271;&#31243;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#39537;&#21160;&#30340;&#25928;&#24212;&#26041;&#38754;&#22256;&#38590;&#37325;&#37325;&#12290;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#21162;&#21147;&#38598;&#20013;&#20110;&#30452;&#25509;&#23558;&#38745;&#30005;&#24341;&#20837;&#65292;&#36825;&#26159;&#26368;&#31361;&#20986;&#30340;&#25928;&#24212;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#26174;&#24335;&#29289;&#29702;&#27169;&#22411;&#30340;&#21151;&#33021;&#24418;&#24335;&#30456;&#20284;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#21253;&#25324;&#20854;&#20182;&#24418;&#24335;&#30340;&#38750;&#38190;&#30456;&#20114;&#20316;&#29992;&#65292;&#25110;&#32773;&#39044;&#27979;&#38500;&#20102;&#21407;&#23376;&#38388;&#21183;&#33021;&#20043;&#22806;&#30340;&#24615;&#36136;&#65292;&#38656;&#35201;&#36827;&#34892;&#20020;&#26102;&#20462;&#25913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#36828;&#31243;&#31561;&#21464;&#65288;LODE&#65289;&#26694;&#26550;&#25193;&#23637;&#21040;&#29983;&#25104;&#31867;&#20284;&#20219;&#24847;&#28176;&#36817;&#34892;&#20026;&#30340;&#38750;&#38190;&#20301;&#21183;&#30340;&#21407;&#23376;&#29615;&#22659;&#30340;&#23616;&#37096;&#25551;&#36848;&#31526;&#65292;&#20174;&#28857;&#30005;&#33655;&#38745;&#30005;&#21040;&#33394;&#25955;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;LODE&#24418;&#24335;&#20027;&#20041;&#21487;&#36890;&#36807;&#24191;&#20041;&#22810;&#26497;&#23637;&#24320;&#30452;&#35266;&#22320;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing machine-learning schemes applied to atomic-scale simulations rely on a local description of the geometry of a structure, and struggle to model effects that are driven by long-range physical interactions. Efforts to overcome these limitations have focused on the direct incorporation of electrostatics, which is the most prominent effect, often relying on architectures that mirror the functional form of explicit physical models. Including other forms of non-bonded interactions, or predicting properties other than the interatomic potential, requires ad hoc modifications. We propose an alternative approach that extends the long-distance equivariant (LODE) framework to generate local descriptors of an atomic environment that resemble non-bonded potentials with arbitrary asymptotic behaviors, ranging from point-charge electrostatics to dispersion forces. We show that the LODE formalism is amenable to a direct physical interpretation in terms of a generalized multipole exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;SC-GAN&#65292;&#29992;&#20110;&#21512;&#25104;H&amp;E&#22270;&#20687;&#21644;IHC&#26579;&#33394;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36793;&#32536;&#32467;&#26500;&#20449;&#24687;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#22686;&#24378;&#20102;&#29305;&#24449;&#23450;&#20301;&#24182;&#20445;&#30041;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.13182</link><description>&lt;p&gt;
&#32467;&#26500;Cycle GAN&#29992;&#20110;&#22312;&#32467;&#32928;&#20013;&#23545;&#33146;&#20307;&#26631;&#35760;&#36827;&#34892;&#34394;&#25311;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#26579;&#33394;
&lt;/p&gt;
&lt;p&gt;
Structural Cycle GAN for Virtual Immunohistochemistry Staining of Gland Markers in the Colon. (arXiv:2308.13182v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;SC-GAN&#65292;&#29992;&#20110;&#21512;&#25104;H&amp;E&#22270;&#20687;&#21644;IHC&#26579;&#33394;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#36793;&#32536;&#32467;&#26500;&#20449;&#24687;&#21644;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#22686;&#24378;&#20102;&#29305;&#24449;&#23450;&#20301;&#24182;&#20445;&#30041;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#23383;&#25195;&#25551;&#20202;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#35786;&#26029;&#25805;&#20316;&#21487;&#33021;&#20174;&#26174;&#24494;&#38236;&#31227;&#21040;&#26700;&#38754;&#19978;&#12290;&#34880;&#32418;&#32032;&#21644;&#20234;&#32418;&#26579;&#33394;&#65288;H&amp;E&#65289;&#26159;&#26368;&#24120;&#29992;&#20110;&#30142;&#30149;&#20998;&#26512;&#12289;&#35786;&#26029;&#21644;&#20998;&#32423;&#30340;&#26579;&#33394;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#30149;&#29702;&#23398;&#23478;&#30830;&#23454;&#38656;&#35201;&#19981;&#21516;&#30340;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#26579;&#33394;&#26469;&#20998;&#26512;&#29305;&#23450;&#30340;&#32467;&#26500;&#25110;&#32454;&#32990;&#12290;&#22312;&#21333;&#20010;&#26631;&#26412;&#19978;&#33719;&#24471;&#25152;&#26377;&#36825;&#20123;&#26579;&#33394;&#65288;H&amp;E&#21644;&#19981;&#21516;&#30340;IHC&#65289;&#26159;&#19968;&#39033;&#32321;&#29712;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#34394;&#25311;&#26579;&#33394;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#32467;&#26500;Cycle GAN&#65288;SC-GAN&#65289;&#65292;&#29992;&#20110;&#20174;H&amp;E&#22270;&#20687;&#21512;&#25104;IHC&#26579;&#33394;&#21453;&#20043;&#20134;&#28982;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#23558;&#36793;&#32536;&#32467;&#26500;&#20449;&#24687;&#65288;&#38500;&#20102;&#39068;&#33394;&#25968;&#25454;&#65289;&#32435;&#20837;&#21040;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#20013;&#65292;&#24182;&#19988;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#27880;&#24847;&#21147;&#27169;&#22359;&#12290;&#36825;&#31181;&#38598;&#25104;&#22686;&#24378;&#20102;&#29305;&#24449;&#23450;&#20301;&#24182;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20445;&#30041;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of digital scanners and deep learning, diagnostic operations may move from a microscope to a desktop. Hematoxylin and Eosin (H&amp;E) staining is one of the most frequently used stains for disease analysis, diagnosis, and grading, but pathologists do need different immunohistochemical (IHC) stains to analyze specific structures or cells. Obtaining all of these stains (H&amp;E and different IHCs) on a single specimen is a tedious and time-consuming task. Consequently, virtual staining has emerged as an essential research direction. Here, we propose a novel generative model, Structural Cycle-GAN (SC-GAN), for synthesizing IHC stains from H&amp;E images, and vice versa. Our method expressly incorporates structural information in the form of edges (in addition to color data) and employs attention modules exclusively in the decoder of the proposed generator model. This integration enhances feature localization and preserves contextual information during the generation process. In additi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Adamic-Adar&#25351;&#25968;&#31639;&#27861;&#39044;&#27979;&#20102;&#24535;&#24895;&#32773;&#21512;&#20316;&#65292;&#21457;&#29616;AAI&#31639;&#27861;&#22312;&#20998;&#26512;&#22270;&#24418;&#26102;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;JC&#12289;CNC&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13176</link><description>&lt;p&gt;
&#20351;&#29992;Adamic-Adar&#25351;&#25968;&#31639;&#27861;&#39044;&#27979;&#24535;&#24895;&#32773;&#21512;&#20316;&#65306;&#23569;&#21363;&#26159;&#22810;
&lt;/p&gt;
&lt;p&gt;
Using Adamic-Adar Index Algorithm to Predict Volunteer Collaboration: Less is More. (arXiv:2308.13176v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Adamic-Adar&#25351;&#25968;&#31639;&#27861;&#39044;&#27979;&#20102;&#24535;&#24895;&#32773;&#21512;&#20316;&#65292;&#21457;&#29616;AAI&#31639;&#27861;&#22312;&#20998;&#26512;&#22270;&#24418;&#26102;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;JC&#12289;CNC&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#30001;&#20110;&#21442;&#19982;&#32773;&#20043;&#38388;&#28508;&#22312;&#21512;&#20316;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#21576;&#29616;&#20986;&#22797;&#26434;&#30340;&#22270;&#24418;&#32467;&#26500;&#12290;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#20010;&#23454;&#38469;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#32988;&#36807;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22270;&#38142;&#25509;&#39044;&#27979;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#36824;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20351;&#29992;Adamic-Adar&#25351;&#25968;&#31639;&#27861;&#65288;AAI&#65289;&#65292;&#26480;&#21345;&#24503;&#31995;&#25968;&#65288;JC&#65289;&#21644;&#20849;&#21516;&#37051;&#23621;&#20013;&#24515;&#24615;&#65288;CNC&#65289;&#20316;&#20026;&#22270;&#24418;&#29305;&#23450;&#31639;&#27861;&#30340;&#20195;&#34920;&#65292;&#24212;&#29992;&#20110;&#39044;&#27979;&#24535;&#24895;&#32773;&#27963;&#21160;&#26399;&#38388;&#28145;&#22323;&#24066;&#30340;&#28508;&#22312;&#21512;&#20316;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22914;&#38543;&#26426;&#26862;&#26519;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#26799;&#24230;&#25552;&#21319;&#26469;&#20316;&#20026;&#21333;&#19968;&#39044;&#27979;&#22120;&#21644;&#38598;&#25104;&#23398;&#20064;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AAI&#31639;&#27861;&#22312;&#20998;&#26512;&#22270;&#24418;&#26102;&#20248;&#20110;&#20256;&#32479;&#30340;JC&#12289;CNC&#21644;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social networks exhibit a complex graph-like structure due to the uncertainty surrounding potential collaborations among participants. Machine learning algorithms possess generic outstanding performance in multiple real-world prediction tasks. However, whether machine learning algorithms outperform specific algorithms designed for graph link prediction remains unknown to us. To address this issue, the Adamic-Adar Index (AAI), Jaccard Coefficient (JC) and common neighbour centrality (CNC) as representatives of graph-specific algorithms were applied to predict potential collaborations, utilizing data from volunteer activities during the Covid-19 pandemic in Shenzhen city, along with the classical machine learning algorithms such as random forest, support vector machine, and gradient boosting as single predictors and components of ensemble learning. This paper introduces that the AAI algorithm outperformed the traditional JC and CNC, and other machine learning algorithms in analyzing grap
&lt;/p&gt;</description></item><item><title>IOMatch&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24320;&#25918;&#38598;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20869;&#28857;&#21644;&#22806;&#28857;&#38590;&#20197;&#21306;&#20998;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#21033;&#29992;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2308.13168</link><description>&lt;p&gt;
IOMatch:&#31616;&#21270;&#24320;&#25918;&#38598;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21516;&#26102;&#21033;&#29992;&#20869;&#28857;&#21644;&#22806;&#28857;
&lt;/p&gt;
&lt;p&gt;
IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization. (arXiv:2308.13168v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13168
&lt;/p&gt;
&lt;p&gt;
IOMatch&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24320;&#25918;&#38598;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#20869;&#28857;&#21644;&#22806;&#28857;&#38590;&#20197;&#21306;&#20998;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#21033;&#29992;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#22312;&#26631;&#31614;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#25910;&#38598;&#21040;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#21253;&#21547;&#19981;&#23646;&#20110;&#20219;&#20309;&#24050;&#26631;&#35760;&#31867;&#21035;&#30340;&#26410;&#35265;&#31867;&#21035;&#30340;&#24322;&#24120;&#20540;&#12290;&#20026;&#20102;&#24212;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#38598;&#21322;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#20027;&#27969;&#26041;&#27861;&#24448;&#24448;&#39318;&#20808;&#26816;&#27979;&#24322;&#24120;&#20540;&#65292;&#28982;&#21518;&#36807;&#28388;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#20107;&#23454;&#65292;&#24403;&#26631;&#31614;&#26497;&#20026;&#31232;&#32570;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;&#19981;&#21487;&#38752;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#22120;&#21487;&#33021;&#38169;&#35823;&#22320;&#25490;&#38500;&#20102;&#22823;&#37327;&#26377;&#20215;&#20540;&#30340;&#20869;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#38598;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;IOMatch&#65292;&#23427;&#21487;&#20197;&#22312;&#20869;&#28857;&#21644;&#22806;&#28857;&#38590;&#20197;&#21306;&#20998;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#21033;&#29992;&#23427;&#20204;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20108;&#20540;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#26631;&#20934;&#30340;&#38381;&#38598;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#32479;&#19968;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) aims to leverage massive unlabeled data when labels are expensive to obtain. Unfortunately, in many real-world applications, the collected unlabeled data will inevitably contain unseen-class outliers not belonging to any of the labeled classes. To deal with the challenging open-set SSL task, the mainstream methods tend to first detect outliers and then filter them out. However, we observe a surprising fact that such approach could result in more severe performance degradation when labels are extremely scarce, as the unreliable outlier detector may wrongly exclude a considerable portion of valuable inliers. To tackle with this issue, we introduce a novel open-set SSL framework, IOMatch, which can jointly utilize inliers and outliers, even when it is difficult to distinguish exactly between them. Specifically, we propose to employ a multi-binary classifier in combination with the standard closed-set classifier for producing unified open-set classification t
&lt;/p&gt;</description></item><item><title>DAG-ACFL&#26159;&#19968;&#31181;&#22522;&#20110;DAG-DLT&#30340;&#24322;&#27493;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#26469;&#32858;&#21512;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212;&#30340;tip&#36873;&#25321;&#31639;&#27861;&#38477;&#20302;&#36890;&#20449;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.13158</link><description>&lt;p&gt;
DAG-ACFL&#65306;&#22522;&#20110;DAG-DLT&#30340;&#24322;&#27493;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DAG-ACFL: Asynchronous Clustered Federated Learning based on DAG-DLT. (arXiv:2308.13158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13158
&lt;/p&gt;
&lt;p&gt;
DAG-ACFL&#26159;&#19968;&#31181;&#22522;&#20110;DAG-DLT&#30340;&#24322;&#27493;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#27169;&#22411;&#26469;&#32858;&#21512;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212;&#30340;tip&#36873;&#25321;&#31639;&#27861;&#38477;&#20302;&#36890;&#20449;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#22312;&#20445;&#25252;&#23458;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#21327;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#25361;&#25112;&#12290;&#32858;&#31867;&#32852;&#37030;&#23398;&#20064;(CFL)&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;CFL&#26694;&#26550;&#37117;&#37319;&#29992;&#21516;&#27493;&#26694;&#26550;&#32570;&#20047;&#24322;&#27493;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#26080;&#29615;&#22270;&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;(DAG-DLT)&#30340;&#24322;&#27493;CFL&#26694;&#26550;SDAGFL&#65292;&#20294;&#20854;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;&#23548;&#33268;&#20102;&#39640;&#36890;&#20449;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DAG-ACFL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;DAG-DLT&#30340;&#24322;&#27493;&#32858;&#31867;FL&#26694;&#26550;&#12290;&#39318;&#20808;&#35814;&#32454;&#20171;&#32461;&#20102;DAG-ACFL&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;tip&#36873;&#25321;&#31639;&#27861;&#65292;&#20197;&#32858;&#21512;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#12290;&#37319;&#29992;&#33258;&#36866;&#24212;tip&#36873;&#25321;&#31639;&#27861;&#65292;&#21033;&#29992;&#21464;&#28857;&#26816;&#27979;&#21160;&#24577;&#30830;&#23450;&#25152;&#36873;tip&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;DAG-ACFL&#30340;&#24615;&#33021;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) aims to collaboratively train a global model while ensuring client data privacy. However, FL faces challenges from the non-IID data distribution among clients. Clustered FL (CFL) has emerged as a promising solution, but most existing CFL frameworks adopt synchronous frameworks lacking asynchrony. An asynchronous CFL framework called SDAGFL based on directed acyclic graph distributed ledger techniques (DAG-DLT) was proposed, but its complete decentralization leads to high communication and storage costs. We propose DAG-ACFL, an asynchronous clustered FL framework based on directed acyclic graph distributed ledger techniques (DAG-DLT). We first detail the components of DAG-ACFL. A tip selection algorithm based on the cosine similarity of model parameters is then designed to aggregate models from clients with similar distributions. An adaptive tip selection algorithm leveraging change-point detection dynamically determines the number of selected tips. We evaluate t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#23454;&#26045;&#32852;&#37030;&#23398;&#20064;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#37325;&#28857;&#20851;&#27880;&#20102;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#36164;&#28304;&#12289;&#24322;&#26500;&#23458;&#25143;&#31471;&#25968;&#25454;&#23384;&#22312;&#12289;&#26381;&#21153;&#22120;&#23481;&#37327;&#21644;&#39640;&#36890;&#20449;&#25104;&#26412;&#31561;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#20010;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13157</link><description>&lt;p&gt;
&#29289;&#32852;&#32593;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#20174;&#36164;&#28304;&#38480;&#21046;&#30340;&#35282;&#24230;&#36827;&#34892;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in IoT: a Survey from a Resource-Constrained Perspective. (arXiv:2308.13157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#35843;&#30740;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#23454;&#26045;&#32852;&#37030;&#23398;&#20064;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#37325;&#28857;&#20851;&#27880;&#20102;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#36164;&#28304;&#12289;&#24322;&#26500;&#23458;&#25143;&#31471;&#25968;&#25454;&#23384;&#22312;&#12289;&#26381;&#21153;&#22120;&#23481;&#37327;&#21644;&#39640;&#36890;&#20449;&#25104;&#26412;&#31561;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#20010;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#26234;&#33021;&#20915;&#31574;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#25955;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24191;&#27867;&#29992;&#20110;&#20174;&#21508;&#31181;&#20998;&#24067;&#24335;&#25968;&#25454;&#28304;&#20013;&#25910;&#38598;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#29289;&#32852;&#32593;&#21644;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#24182;&#20849;&#21516;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#29289;&#32852;&#32593;&#35774;&#22791;&#36164;&#28304;&#26377;&#38480;&#30340;&#29305;&#24615;&#38480;&#21046;&#20102;FL&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#20174;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20004;&#20010;&#23618;&#38754;&#65292;&#20840;&#38754;&#35843;&#30740;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#23454;&#26045;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20851;&#27880;&#26377;&#20851;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#36164;&#28304;&#12289;&#24322;&#26500;&#23458;&#25143;&#31471;&#25968;&#25454;&#23384;&#22312;&#12289;&#26381;&#21153;&#22120;&#23481;&#37327;&#21644;&#39640;&#36890;&#20449;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26681;&#25454;&#24212;&#29992;&#20301;&#32622;&#65288;&#21363;&#29289;&#32852;&#32593;&#23458;&#25143;&#31471;&#21644;FL&#26381;&#21153;&#22120;&#65289;&#23545;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The IoT ecosystem is able to leverage vast amounts of data for intelligent decision-making. Federated Learning (FL), a decentralized machine learning technique, is widely used to collect and train machine learning models from a variety of distributed data sources. Both IoT and FL systems can be complementary and used together. However, the resource-constrained nature of IoT devices prevents the widescale deployment FL in the real world. This research paper presents a comprehensive survey of the challenges and solutions associated with implementing Federated Learning (FL) in resource-constrained Internet of Things (IoT) environments, viewed from 2 levels, client and server. We focus on solutions regarding limited client resources, presence of heterogeneous client data, server capacity, and high communication costs, and assess their effectiveness in various scenarios. Furthermore, we categorize the solutions based on the location of their application, i.e., the IoT client, and the FL ser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12289;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#21644;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#65292;&#22312;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.13150</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#30340;&#36801;&#31227;ResNet&#22686;&#24378;&#20083;&#33146;&#30284;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Breast Cancer Classification Using Transfer ResNet with Lightweight Attention Mechanism. (arXiv:2308.13150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;ResNet&#27169;&#22411;&#21644;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#12289;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#21644;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#65292;&#22312;&#20083;&#33146;&#30284;&#20998;&#31867;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#28508;&#22312;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#29305;&#24449;&#23618;&#27425;&#32467;&#26500;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#22270;&#20687;&#20998;&#31867;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;ResNet&#27169;&#22411;&#30340;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#36731;&#37327;&#32423;&#27880;&#24847;&#26426;&#21046;&#26694;&#26550;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#20248;&#21270;&#29305;&#24449;&#34920;&#31034;&#65292;&#22686;&#24378;&#20998;&#31867;&#33021;&#21147;&#65292;&#25913;&#21892;&#29305;&#24449;&#21487;&#36776;&#21035;&#24615;&#12290;&#25105;&#20204;&#22312;Breakhis&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#35768;&#22810;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20256;&#32479;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#65292;&#22312;&#24403;&#20195;&#35270;&#35273;&#21464;&#25442;&#22120;&#31561;&#26368;&#26032;&#26041;&#27861;&#19978;&#20063;&#26174;&#31034;&#20986;&#20248;&#21183;&#12290;&#22312;&#35832;&#22914;&#31934;&#24230;&#12289;&#20934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;G-means&#31561;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#21516;&#26102;&#22312;&#25910;&#25947;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#36825;&#20123;&#32467;&#26524;&#22686;&#24378;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24041;&#22266;&#20102;&#20854;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have revolutionized image classification by learning complex feature hierarchies in raw pixel data. This paper introduces an image classification method based on the ResNet model, and introduces a lightweight attention mechanism framework to improve performance. The framework optimizes feature representation, enhances classification capabilities, and improves feature discriminativeness. We verified the effectiveness of the algorithm on the Breakhis dataset, showing its superior performance in many aspects. Not only in terms of conventional models, our method also shows advantages on state-of-the-art methods such as contemporary visual transformers. Significant improvements have been achieved in metrics such as precision, accuracy, recall, F1-score, and G-means, while also performing well in terms of convergence time. These results strengthen the performance of the algorithm and solidify its application prospects in practical image classification tasks. Keywords: Re
&lt;/p&gt;</description></item><item><title>MatchXML&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;label2vec&#26041;&#27861;&#29983;&#25104;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#23884;&#20837;&#26500;&#24314;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65292;MatchXML&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#21462;&#20986;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#38745;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.13139</link><description>&lt;p&gt;
MatchXML: &#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification. (arXiv:2308.13139v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13139
&lt;/p&gt;
&lt;p&gt;
MatchXML&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;label2vec&#26041;&#27861;&#29983;&#25104;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#23884;&#20837;&#26500;&#24314;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65292;MatchXML&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#21462;&#20986;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#38745;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;XMC&#65289;&#26159;&#25351;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#20174;&#19968;&#20010;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#26631;&#31614;&#38598;&#20013;&#65288;&#20363;&#22914;&#25968;&#30334;&#19975;&#20010;&#26631;&#31614;&#65289;&#20026;&#25991;&#26412;&#26679;&#26412;&#20998;&#37197;&#30456;&#20851;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MatchXML&#65292;&#19968;&#31181;&#29992;&#20110;XMC&#30340;&#39640;&#25928;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30001;&#31232;&#30095;&#30340;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#29305;&#24449;&#29983;&#25104;&#30340;&#26631;&#31614;&#23884;&#20837;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;label2vec&#65292;&#36890;&#36807;Skip-gram&#27169;&#22411;&#26469;&#26377;&#25928;&#35757;&#32451;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36825;&#20123;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#26469;&#26500;&#24314;&#19968;&#20010;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;Transformer&#26102;&#65292;&#25105;&#20204;&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#22312;&#20108;&#20998;&#22270;&#20013;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20174;&#24494;&#35843;&#21518;&#30340;Transformer&#20013;&#25552;&#21462;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#38500;&#20102;&#24494;&#35843;&#21518;&#30340;&#23494;&#38598;&#25991;&#26412;&#23884;&#20837;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#39044;&#35757;&#32451;&#30340;Sentence Transformer&#20013;&#25552;&#21462;&#38745;&#24577;&#30340;&#23494;&#38598;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The eXtreme Multi-label text Classification(XMC) refers to training a classifier that assigns a text sample with relevant labels from an extremely large-scale label set (e.g., millions of labels). We propose MatchXML, an efficient text-label matching framework for XMC. We observe that the label embeddings generated from the sparse Term Frequency-Inverse Document Frequency(TF-IDF) features have several limitations. We thus propose label2vec to effectively train the semantic dense label embeddings by the Skip-gram model. The dense label embeddings are then used to build a Hierarchical Label Tree by clustering. In fine-tuning the pre-trained encoder Transformer, we formulate the multi-label text classification as a text-label matching problem in a bipartite graph. We then extract the dense text representations from the fine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also extract the static dense sentence embeddings from a pre-trained Sentence Transformer. Finally,
&lt;/p&gt;</description></item><item><title>OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13137</link><description>&lt;p&gt;
OmniQuant&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13137
&lt;/p&gt;
&lt;p&gt;
OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#20102;&#20854;&#24222;&#22823;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25552;&#39640;LLM&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#25163;&#24037;&#21046;&#23450;&#37327;&#21270;&#21442;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#20302;&#24182;&#19988;&#19981;&#33021;&#22788;&#29702;&#26497;&#20302;&#20301;&#37327;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#65288;OmniQuant&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;LLMs&#65292;&#23427;&#22312;&#22810;&#31181;&#37327;&#21270;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#26469;&#20445;&#25345;PTQ&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;OmniQuant&#21253;&#21547;&#20004;&#20010;&#21019;&#26032;&#32452;&#20214;&#65292;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#21098;&#35009;&#65288;LWC&#65289;&#21644;&#21487;&#23398;&#20064;&#30340;&#31561;&#25928;&#21464;&#25442;&#65288;LET&#65289;&#12290;LWC&#36890;&#36807;&#20248;&#21270;&#21098;&#35009;&#38408;&#20540;&#26469;&#35843;&#33410;&#26435;&#37325;&#30340;&#26497;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LET&#22788;&#29702;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21487;&#21152;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#21487;&#35299;&#37322;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#32447;&#24615;&#20551;&#35774;&#38480;&#21046;&#65292;&#21516;&#26102;&#25552;&#20379;&#36739;&#24378;&#30340;&#20915;&#31574;&#24314;&#35758;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13135</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#21487;&#21152;&#20540;&#20989;&#25968;&#65306;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21450;&#20854;&#22312;&#22806;&#31185;&#25163;&#26415;&#24674;&#22797;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Additive Value Functions: Interpretable Reinforcement Learning with an Application to Surgical Recovery. (arXiv:2308.13135v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21487;&#21152;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#21487;&#35299;&#37322;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#27169;&#22411;&#30340;&#32447;&#24615;&#20551;&#35774;&#38480;&#21046;&#65292;&#21516;&#26102;&#25552;&#20379;&#36739;&#24378;&#30340;&#20915;&#31574;&#24314;&#35758;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21487;&#21152;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20272;&#35745;&#21487;&#35299;&#37322;&#30340;&#20540;&#20989;&#25968;&#12290;&#23398;&#20064;&#20381;&#38752;&#25968;&#23383;&#34920;&#22411;&#29305;&#24449;&#30340;&#26377;&#25928;&#33258;&#36866;&#24212;&#20020;&#24202;&#24178;&#39044;&#26159;&#21307;&#21153;&#20154;&#21592;&#37325;&#35270;&#30340;&#38382;&#39064;&#12290;&#22312;&#33034;&#26609;&#25163;&#26415;&#26041;&#38754;&#65292;&#20851;&#20110;&#24739;&#32773;&#36816;&#21160;&#33021;&#21147;&#24674;&#22797;&#30340;&#19981;&#21516;&#26415;&#21518;&#24674;&#22797;&#24314;&#35758;&#21487;&#33021;&#20250;&#23548;&#33268;&#24739;&#32773;&#24674;&#22797;&#31243;&#24230;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#28216;&#25103;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#30340;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#40657;&#30418;&#26041;&#27861;&#65292;&#22914;&#31070;&#32463;&#32593;&#32476;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#38459;&#30861;&#20102;&#32771;&#23519;&#27599;&#20010;&#29305;&#24449;&#23545;&#20110;&#20135;&#29983;&#26368;&#32456;&#24314;&#35758;&#20915;&#31574;&#30340;&#36129;&#29486;&#12290;&#34429;&#28982;&#22312;&#32463;&#20856;&#31639;&#27861;&#65288;&#22914;&#26368;&#23567;&#20108;&#20056;&#31574;&#30053;&#36845;&#20195;&#65289;&#20013;&#21487;&#20197;&#36731;&#26494;&#25552;&#20379;&#36825;&#26679;&#30340;&#35299;&#37322;&#65292;&#20294;&#22522;&#26412;&#30340;&#32447;&#24615;&#20551;&#35774;&#38459;&#27490;&#20102;&#23398;&#20064;&#29305;&#24449;&#20043;&#38388;&#30340;&#39640;&#38454;&#28789;&#27963;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#25216;&#26415;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#24471;&#21040;&#35299;&#37322;&#24615;&#24378;&#30340;&#20915;&#31574;&#24314;&#35758;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a nonparametric additive model for estimating interpretable value functions in reinforcement learning. Learning effective adaptive clinical interventions that rely on digital phenotyping features is a major for concern medical practitioners. With respect to spine surgery, different post-operative recovery recommendations concerning patient mobilization can lead to significant variation in patient recovery. While reinforcement learning has achieved widespread success in domains such as games, recent methods heavily rely on black-box methods, such neural networks. Unfortunately, these methods hinder the ability of examining the contribution each feature makes in producing the final suggested decision. While such interpretations are easily provided in classical algorithms such as Least Squares Policy Iteration, basic linearity assumptions prevent learning higher-order flexible interactions between features. In this paper, we present a novel method that offers a flexible techniq
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24211;&#23384;&#31649;&#29702;&#35774;&#32622;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#20248;&#21270;&#39044;&#27979;&#25351;&#26631;&#65292;&#30456;&#36739;&#20110;&#20248;&#21270;&#20256;&#32479;&#26631;&#20934;&#30340;&#19982;&#19994;&#21153;&#26080;&#20851;&#30340;&#25351;&#26631;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#19994;&#21153;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.13118</link><description>&lt;p&gt;
&#38754;&#21521;&#24211;&#23384;&#31649;&#29702;&#30340;&#19994;&#21153;&#25351;&#26631;&#24863;&#30693;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Business Metric-Aware Forecasting for Inventory Management. (arXiv:2308.13118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24211;&#23384;&#31649;&#29702;&#35774;&#32622;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#20248;&#21270;&#39044;&#27979;&#25351;&#26631;&#65292;&#30456;&#36739;&#20110;&#20248;&#21270;&#20256;&#32479;&#26631;&#20934;&#30340;&#19982;&#19994;&#21153;&#26080;&#20851;&#30340;&#25351;&#26631;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#21319;&#19994;&#21153;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#19994;&#21153;&#35268;&#21010;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#32773;&#36890;&#24120;&#20248;&#21270;&#19982;&#19994;&#21153;&#30446;&#26631;&#26080;&#20851;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#21487;&#33021;&#20135;&#29983;&#19982;&#19994;&#21153;&#20559;&#22909;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20256;&#32479;&#39044;&#27979;&#25351;&#26631;&#30340;&#20248;&#21270;&#36890;&#24120;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#19994;&#21153;&#32489;&#25928;&#12290;&#38024;&#23545;&#24211;&#23384;&#31649;&#29702;&#30340;&#24773;&#22659;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#19979;&#25512;&#23548;&#20986;&#20102;&#35745;&#31639;&#21644;&#20248;&#21270;&#24120;&#35265;&#19994;&#21153;&#25351;&#26631;&#20195;&#29702;&#30340;&#39640;&#25928;&#36807;&#31243;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#21487;&#33021;&#30340;&#25104;&#26412;&#26435;&#34913;&#24773;&#22659;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#31471;&#21040;&#31471;&#20248;&#21270;&#36890;&#24120;&#20248;&#20110;&#20248;&#21270;&#26631;&#20934;&#30340;&#19982;&#19994;&#21153;&#26080;&#20851;&#30340;&#39044;&#27979;&#25351;&#26631;&#65288;&#23545;&#20110;&#31616;&#21333;&#30340;&#32553;&#25918;&#27169;&#22411;&#65292;&#25552;&#21319;&#39640;&#36798;45.7%&#65292;&#23545;&#20110;LSTM&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25552;&#21319;&#39640;&#36798;54.0%&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#22914;&#20309;&#22312;&#20854;&#20182;&#19994;&#21153;&#29615;&#22659;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasts play a critical role in business planning. However, forecasters typically optimize objectives that are agnostic to downstream business goals and thus can produce forecasts misaligned with business preferences. In this work, we demonstrate that optimization of conventional forecasting metrics can often lead to sub-optimal downstream business performance. Focusing on the inventory management setting, we derive an efficient procedure for computing and optimizing proxies of common downstream business metrics in an end-to-end differentiable manner. We explore a wide range of plausible cost trade-off scenarios, and empirically demonstrate that end-to-end optimization often outperforms optimization of standard business-agnostic forecasting metrics (by up to 45.7% for a simple scaling model, and up to 54.0% for an LSTM encoder-decoder model). Finally, we discuss how our findings could benefit other business contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Laplace-LoRA&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#22686;&#24378;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13111</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20302;&#31209;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bayesian low-rank adaptation for large language models. (arXiv:2308.13111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Laplace-LoRA&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#22686;&#24378;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#39640;&#25928;&#24494;&#35843;&#30340;&#26032;&#33539;&#24335;&#65292;&#20854;&#20013;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#24448;&#24448;&#21464;&#24471;&#36807;&#20110;&#33258;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#20855;&#26377;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#21487;&#20316;&#20026;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#24182;&#22686;&#24378;&#26657;&#20934;&#33021;&#21147;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Laplace-LoRA&#65292;&#19968;&#31181;&#30452;&#35266;&#32780;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23427;&#23558;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#24212;&#29992;&#20110;LoRA&#21442;&#25968;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially on when fine-tuned on smaller datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#29983;&#23384;&#20998;&#26512;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#25130;&#26029;&#21644;&#35266;&#23519;&#25968;&#25454;&#30340;&#29983;&#23384;&#26102;&#38271;&#26469;&#23450;&#20041;&#26102;&#24207;&#21306;&#21035;&#24230;&#65292;&#26500;&#24314;&#36127;&#26679;&#26412;&#23545;&#12290;</title><link>http://arxiv.org/abs/2308.13104</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#29983;&#23384;&#20998;&#26512;&#30340;&#23545;&#27604;&#23398;&#20064;&#65306;&#26500;&#24314;&#26102;&#24207;&#21306;&#21035;&#24230;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning of Temporal Distinctiveness for Survival Analysis in Electronic Health Records. (arXiv:2308.13104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#29983;&#23384;&#20998;&#26512;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#25130;&#26029;&#21644;&#35266;&#23519;&#25968;&#25454;&#30340;&#29983;&#23384;&#26102;&#38271;&#26469;&#23450;&#20041;&#26102;&#24207;&#21306;&#21035;&#24230;&#65292;&#26500;&#24314;&#36127;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#22312;&#35768;&#22810;&#21307;&#30103;&#20915;&#31574;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21487;&#20197;&#25903;&#25345;&#23545;&#24739;&#32773;&#21307;&#30103;&#36807;&#31243;&#20013;&#24863;&#20852;&#36259;&#20107;&#20214;&#30340;&#39118;&#38505;&#39044;&#27979;&#12290;&#37492;&#20110;&#25968;&#25454;&#25130;&#26029;&#30340;&#23384;&#22312;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#26159;&#24378;&#21046;&#20445;&#25345;&#25130;&#26029;&#21644;&#35266;&#23519;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#24207;&#19968;&#33268;&#24615;&#65292;&#26088;&#22312;&#21033;&#29992;&#25130;&#26029;&#21069;&#30340;&#26102;&#38388;&#38388;&#38548;&#20316;&#20026;&#37096;&#20998;&#35266;&#27979;&#30340;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#26631;&#31614;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#37319;&#29992;&#25490;&#24207;&#26041;&#27861;&#26469;&#36861;&#27714;&#25490;&#24207;&#30446;&#26631;&#65292;&#20294;&#23578;&#26410;&#23545;&#23545;&#27604;&#26041;&#27861;&#36827;&#34892;&#28145;&#20837;&#25506;&#32034;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#20043;&#38388;&#30340;&#23545;&#27604;&#26469;&#23398;&#20064;&#26377;&#21306;&#21035;&#24615;&#30340;&#23884;&#20837;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26412;&#20307;&#24863;&#30693;&#30340;&#26102;&#24207;&#23545;&#27604;&#29983;&#23384;&#20998;&#26512;&#65288;OTCSurv&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#25130;&#26029;&#21644;&#35266;&#23519;&#25968;&#25454;&#30340;&#29983;&#23384;&#26102;&#38271;&#23450;&#20041;&#26102;&#24207;&#21306;&#21035;&#24230;&#65292;&#24182;&#26500;&#24314;&#36127;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis plays a crucial role in many healthcare decisions, where the risk prediction for the events of interest can support an informative outlook for a patient's medical journey. Given the existence of data censoring, an effective way of survival analysis is to enforce the pairwise temporal concordance between censored and observed data, aiming to utilize the time interval before censoring as partially observed time-to-event labels for supervised learning. Although existing studies mostly employed ranking methods to pursue an ordering objective, contrastive methods which learn a discriminative embedding by having data contrast against each other, have not been explored thoroughly for survival analysis. Therefore, in this paper, we propose a novel Ontology-aware Temporality-based Contrastive Survival (OTCSurv) analysis framework that utilizes survival durations from both censored and observed data to define temporal distinctiveness and construct negative sample pairs with adj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#33258;&#20027;Formula SAE&#36187;&#36710;&#30340;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#23398;&#20064;&#36187;&#36710;&#39550;&#39542;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#30495;&#23454;&#36187;&#36947;&#19978;&#30340;&#29289;&#29702;&#24179;&#21488;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#23545;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.13088</link><description>&lt;p&gt;
&#23545;&#19968;&#36742;&#33258;&#20027;Formula SAE&#36187;&#36710;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#36827;&#34892;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
Racing Towards Reinforcement Learning based control of an Autonomous Formula SAE Car. (arXiv:2308.13088v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#33258;&#20027;Formula SAE&#36187;&#36710;&#30340;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#25104;&#21151;&#23398;&#20064;&#36187;&#36710;&#39550;&#39542;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#30495;&#23454;&#36187;&#36947;&#19978;&#30340;&#29289;&#29702;&#24179;&#21488;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#23545;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#20027;&#23548;&#33322;&#30740;&#31350;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;Formula Student&#65288;FS&#65289;&#36187;&#20107;&#22312;&#20854;&#27604;&#36187;&#21015;&#34920;&#20013;&#24341;&#20837;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;DV&#65289;&#31867;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#36825;&#20123;&#27604;&#36187;&#20013;&#30340;&#33258;&#20027;FS&#36187;&#36710;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#25511;&#21046;&#30340;&#21021;&#27493;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#31867;&#20284;&#20110;&#23454;&#38469;&#35774;&#35745;&#30340;&#36187;&#36947;&#19978;&#65292;&#36890;&#36807;&#22312;Turtlebot2&#24179;&#21488;&#19978;&#36827;&#34892;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23398;&#20064;&#36187;&#36710;&#39550;&#39542;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#36187;&#36947;&#19978;&#30340;&#29289;&#29702;&#24179;&#21488;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#20840;&#23610;&#24230;&#33258;&#20027;FS&#36187;&#36710;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising popularity of autonomous navigation research, Formula Student (FS) events are introducing a Driverless Vehicle (DV) category to their event list. This paper presents the initial investigation into utilising Deep Reinforcement Learning (RL) for end-to-end control of an autonomous FS race car for these competitions. We train two state-of-the-art RL algorithms in simulation on tracks analogous to the full-scale design on a Turtlebot2 platform. The results demonstrate that our approach can successfully learn to race in simulation and then transfer to a real-world racetrack on the physical platform. Finally, we provide insights into the limitations of the presented approach and guidance into the future directions for applying RL toward full-scale autonomous FS racing.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHIELD&#30340;&#28151;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#20013;&#24515;&#31649;&#29702;&#65292;&#26088;&#22312;&#20248;&#21270;&#30899;&#25490;&#25918;&#12289;&#27700;&#36275;&#36857;&#21644;&#33021;&#28304;&#25104;&#26412;&#12290;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#30340;&#23616;&#37096;&#25628;&#32034;&#21644;&#20998;&#35299;&#24335;&#36827;&#21270;&#31639;&#27861;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26234;&#33021;&#22320;&#31649;&#29702;&#24037;&#20316;&#36127;&#36733;&#20998;&#37197;&#65292;&#23454;&#29616;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.13086</link><description>&lt;p&gt;
SHIELD: &#21487;&#25345;&#32493;&#28151;&#21512;&#36827;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#30899;&#65292;&#24223;&#27700;&#21644;&#33021;&#28304;&#24863;&#30693;&#30340;&#25968;&#25454;&#20013;&#24515;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
SHIELD: Sustainable Hybrid Evolutionary Learning Framework for Carbon, Wastewater, and Energy-Aware Data Center Management. (arXiv:2308.13086v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13086
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHIELD&#30340;&#28151;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#20013;&#24515;&#31649;&#29702;&#65292;&#26088;&#22312;&#20248;&#21270;&#30899;&#25490;&#25918;&#12289;&#27700;&#36275;&#36857;&#21644;&#33021;&#28304;&#25104;&#26412;&#12290;&#36890;&#36807;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#30340;&#23616;&#37096;&#25628;&#32034;&#21644;&#20998;&#35299;&#24335;&#36827;&#21270;&#31639;&#27861;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26234;&#33021;&#22320;&#31649;&#29702;&#24037;&#20316;&#36127;&#36733;&#20998;&#37197;&#65292;&#23454;&#29616;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#20113;&#25968;&#25454;&#20013;&#24515;&#36890;&#24120;&#20998;&#24067;&#22312;&#22320;&#29702;&#19978;&#65292;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#25968;&#25454;&#26381;&#21153;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#22320;&#29702;&#20998;&#24067;&#30340;&#25968;&#25454;&#20013;&#24515;&#65288;GDDC&#65289;&#30001;&#20110;&#19981;&#26029;&#22686;&#21152;&#30340;&#30899;&#25490;&#25918;&#21644;&#27700;&#20351;&#29992;&#37327;&#32780;&#23545;&#29615;&#22659;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#38656;&#35201;&#21152;&#20197;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#25805;&#20316;&#36825;&#20123;&#25968;&#25454;&#20013;&#24515;&#30340;&#33021;&#28304;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31216;&#20026;SHIELD&#30340;&#28151;&#21512;&#24037;&#20316;&#36127;&#36733;&#31649;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#30340;&#23616;&#37096;&#25628;&#32034;&#19982;&#22522;&#20110;&#20998;&#35299;&#30340;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26469;&#21327;&#21516;&#20248;&#21270;GDDC&#30340;&#30899;&#25490;&#25918;&#37327;&#65292;&#27700;&#36275;&#36857;&#21644;&#33021;&#28304;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32771;&#34385;&#20102;&#22320;&#29702;&#22240;&#32032;&#21644;&#26102;&#38388;&#24046;&#24322;&#23545;&#21151;&#29575;&#29983;&#25104;/&#20351;&#29992;&#25104;&#26412;&#21644;&#29615;&#22659;&#24433;&#21709;&#30340;&#24433;&#21709;&#65292;&#20197;&#26234;&#33021;&#22320;&#31649;&#29702;GDDC&#21644;&#25968;&#25454;&#20013;&#24515;&#30340;&#24037;&#20316;&#36127;&#36733;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SHIELD&#21487;&#20197;&#23454;&#29616;34.4&#20493;&#30340;&#21152;&#36895;&#21644;2.1&#20493;&#30340;&#24085;&#32047;&#25176;&#36229;&#20307;&#31215;&#25913;&#36827;&#65292;&#22312;&#20943;&#23569;&#30899;&#36275;&#36857;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#36895;&#24230;&#21644;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today's cloud data centers are often distributed geographically to provide robust data services. But these geo-distributed data centers (GDDCs) have a significant associated environmental impact due to their increasing carbon emissions and water usage, which needs to be curtailed. Moreover, the energy costs of operating these data centers continue to rise. This paper proposes a novel framework to co-optimize carbon emissions, water footprint, and energy costs of GDDCs, using a hybrid workload management framework called SHIELD that integrates machine learning guided local search with a decomposition-based evolutionary algorithm. Our framework considers geographical factors and time-based differences in power generation/use, costs, and environmental impacts to intelligently manage workload distribution across GDDCs and data center operation. Experimental results show that SHIELD can realize 34.4x speedup and 2.1x improvement in Pareto Hypervolume while reducing the carbon footprint by u
&lt;/p&gt;</description></item><item><title>&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#32452;&#32455;&#19981;&#22815;&#26377;&#24207;&#21644;&#35780;&#20272;&#21327;&#35758;&#26377;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#35780;&#20272;&#20102;&#35768;&#22810;&#26368;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#20102;&#38024;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#30340;&#38382;&#39064;&#21450;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13068</link><description>&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;: &#28843;&#37239;&#31639;&#27861;&#21644;&#26377;&#32570;&#38519;&#30340;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series Anomaly Detection: Fancy Algorithms and Flawed Evaluation Methodology. (arXiv:2308.13068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13068
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#32452;&#32455;&#19981;&#22815;&#26377;&#24207;&#21644;&#35780;&#20272;&#21327;&#35758;&#26377;&#32570;&#38519;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#35780;&#20272;&#20102;&#35768;&#22810;&#26368;&#36817;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25351;&#20986;&#20102;&#38024;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#35780;&#20272;&#21327;&#35758;&#23384;&#22312;&#30340;&#38382;&#39064;&#21450;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MVTS&#65289;&#30340;&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#22823;&#37327;&#30740;&#31350;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#25991;&#29486;&#30340;&#20180;&#32454;&#30740;&#31350;&#35753;&#25105;&#20204;&#24847;&#35782;&#21040;&#65306;1&#65289;&#35813;&#39046;&#22495;&#30340;&#31038;&#21306;&#27963;&#36291;&#65292;&#20294;&#24182;&#19981;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#37027;&#26679;&#32452;&#32455;&#26377;&#24207;&#65307;2&#65289;&#22823;&#22810;&#25968;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#19981;&#21512;&#36866;&#25110;&#23384;&#22312;&#26126;&#26174;&#32570;&#38519;&#30340;&#35780;&#20272;&#21327;&#35758;&#36827;&#34892;&#35780;&#20272;&#65292;&#32570;&#20047;&#31185;&#23398;&#22522;&#30784;&#12290;&#20854;&#20013;&#19968;&#20010;&#38750;&#24120;&#27969;&#34892;&#30340;&#21327;&#35758;&#65292;&#21363;&#25152;&#35859;&#30340; \pa &#21327;&#35758;&#65292;&#26159;&#22914;&#27492;&#26377;&#32570;&#38519;&#65292;&#20197;&#33267;&#20110;&#38543;&#26426;&#29468;&#27979;&#21487;&#20197;&#26174;&#31034;&#31995;&#32479;&#22320;&#20248;&#20110;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#30340;\emph{&#25152;&#26377;}&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26356;&#20581;&#22766;&#30340;&#21327;&#35758;&#23545;&#35768;&#22810;&#26368;&#36817;&#30340;&#31639;&#27861;&#36827;&#34892;&#22238;&#39038;&#21644;&#35780;&#20272;&#65292;&#24182;&#35752;&#35770;&#22312;MVTS&#24322;&#24120;&#26816;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#19968;&#20010;&#26412;&#26469;&#24456;&#22909;&#30340;&#21327;&#35758;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#20197;&#21450;&#22914;&#20309;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#34920;&#36798;&#20102;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time Series (MVTS) anomaly detection is a long-standing and challenging research topic that has attracted tremendous research effort from both industry and academia recently. However, a careful study of the literature makes us realize that 1) the community is active but not as organized as other sibling machine learning communities such as Computer Vision (CV) and Natural Language Processing (NLP), and 2) most proposed solutions are evaluated using either inappropriate or highly flawed protocols, with an apparent lack of scientific foundation. So flawed is one very popular protocol, the so-called \pa protocol, that a random guess can be shown to systematically outperform \emph{all} algorithms developed so far. In this paper, we review and evaluate many recent algorithms using more robust protocols and discuss how a normally good protocol may have weaknesses in the context of MVTS anomaly detection and how to mitigate them. We also share our concerns about benchmark dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;VAE&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#20013;&#24674;&#22797;&#20302;&#32500;&#27969;&#24418;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#20998;&#23376;&#30340;&#23646;&#24615;&#32479;&#35745;&#65292;&#32780;&#26080;&#38656;&#38598;&#25104;&#23646;&#24615;&#39044;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.13066</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#38454;&#27573;VAE&#23545;&#20998;&#23376;&#23646;&#24615;&#36827;&#34892;&#23458;&#35266;&#26080;&#20851;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Objective-Agnostic Enhancement of Molecule Properties via Multi-Stage VAE. (arXiv:2308.13066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;VAE&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#20013;&#24674;&#22797;&#20302;&#32500;&#27969;&#24418;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#20998;&#23376;&#30340;&#23646;&#24615;&#32479;&#35745;&#65292;&#32780;&#26080;&#38656;&#38598;&#25104;&#23646;&#24615;&#39044;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#33647;&#29289;&#21457;&#29616;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26550;&#26500;&#21644;&#27969;&#31243;&#26469;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;VAE&#26041;&#27861;&#22312;&#25968;&#25454;&#20301;&#20110;&#39640;&#32500;&#29615;&#22659;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#20302;&#32500;&#27969;&#24418;&#19978;&#26102;&#65292;&#24456;&#38590;&#24674;&#22797;&#27969;&#24418;&#12290;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#65292;&#36825;&#19968;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#27969;&#24418;&#24674;&#22797;&#30340;&#22810;&#38454;&#27573;VAE&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#21040;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#12290;&#25105;&#20204;&#20351;&#29992;ChEMBL&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#22810;&#38454;&#27573;VAE&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#23558;&#23646;&#24615;&#39044;&#27979;&#22120;&#32435;&#20837;&#35757;&#32451;&#27969;&#31243;&#30340;&#21069;&#25552;&#19979;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#25152;&#29983;&#25104;&#20998;&#23376;&#30340;&#23646;&#24615;&#32479;&#35745;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20004;&#20010;&#31934;&#24515;&#31574;&#21010;&#19988;&#36739;&#23567;&#30340;&#20998;&#23376;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#38024;&#23545;&#19981;&#21516;&#30340;&#34507;&#30333;&#36136;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#23646;&#24615;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoder (VAE) is a popular method for drug discovery and various architectures and pipelines have been proposed to improve its performance. However, VAE approaches are known to suffer from poor manifold recovery when the data lie on a low-dimensional manifold embedded in a higher dimensional ambient space [Dai and Wipf, 2019]. The consequences of it in drug discovery are somewhat under-explored. In this paper, we explore applying a multi-stage VAE approach, that can improve manifold recovery on a synthetic dataset, to the field of drug discovery. We experimentally evaluate our multi-stage VAE approach using the ChEMBL dataset and demonstrate its ability to improve the property statistics of generated molecules substantially from pre-existing methods without incorporating property predictors into the training pipeline. We further fine-tune our models on two curated and much smaller molecule datasets that target different proteins. Our experiments show an increase in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#36719;&#20214;&#20013;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;&#20855;&#26377;&#20391;&#20449;&#36947;&#27844;&#28431;&#30340;&#33030;&#24369;&#20195;&#30721;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;&#36890;&#36807;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#21644;&#21160;&#24577;&#20998;&#26512;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#20855;&#26377;&#27844;&#28431;&#40065;&#26834;&#24615;&#30340;&#20505;&#36873;&#26367;&#25442;&#12290;</title><link>http://arxiv.org/abs/2308.13062</link><description>&lt;p&gt;
ZeroLeak: &#20351;&#29992;LLMs&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#25104;&#26412;&#26377;&#25928;&#30340;&#20391;&#20449;&#36947;&#34917;&#19969;
&lt;/p&gt;
&lt;p&gt;
ZeroLeak: Using LLMs for Scalable and Cost Effective Side-Channel Patching. (arXiv:2308.13062v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#36719;&#20214;&#20013;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;&#20855;&#26377;&#20391;&#20449;&#36947;&#27844;&#28431;&#30340;&#33030;&#24369;&#20195;&#30721;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;&#36890;&#36807;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#21644;&#21160;&#24577;&#20998;&#26512;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#20855;&#26377;&#27844;&#28431;&#40065;&#26834;&#24615;&#30340;&#20505;&#36873;&#26367;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#30340;&#36719;&#20214;&#65292;&#22914;OpenSSL&#65292;&#30001;&#20110;&#32570;&#20047;&#36164;&#28304;&#25110;&#19987;&#23478;&#65292;&#23384;&#22312;&#35768;&#22810;&#26410;&#20462;&#34917;&#30340;&#20391;&#20449;&#36947;&#27844;&#28431;&#12290;&#38543;&#30528;&#20195;&#30721;&#24320;&#21457;&#36895;&#24230;&#30340;&#21152;&#24555;&#65292;&#24320;&#21457;&#20154;&#21592;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33258;&#21160;&#29983;&#25104;&#20195;&#30721;&#65292;&#24773;&#20917;&#21482;&#20250;&#21464;&#24471;&#26356;&#31967;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;&#20855;&#26377;&#24494;&#20307;&#31995;&#32467;&#26500;&#20391;&#20449;&#36947;&#27844;&#28431;&#30340;&#33030;&#24369;&#20195;&#30721;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#25506;&#32034;&#24378;&#22823;&#30340;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#37319;&#29992;&#20102;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#26377;&#29983;&#25104;&#30340;&#20195;&#30721;&#37117;&#36890;&#36807;&#27844;&#28431;&#26816;&#27979;&#24037;&#20855;&#36827;&#34892;&#21160;&#24577;&#20998;&#26512;&#65292;&#36825;&#20123;&#24037;&#20855;&#33021;&#22815;&#20934;&#30830;&#25351;&#31034;&#20174;&#31192;&#23494;&#30456;&#20851;&#35775;&#38382;&#25110;&#20998;&#25903;&#25110;&#33030;&#24369;&#30340;Spectre gadget&#27844;&#28431;&#30340;&#25351;&#20196;&#32423;&#30340;&#20449;&#24687;&#27844;&#28431;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#34987;&#29992;&#26469;&#29983;&#25104;&#33030;&#24369;&#20195;&#30721;&#30340;&#20505;&#36873;&#26367;&#25442;&#65292;&#28982;&#21518;&#23545;&#20854;&#36827;&#34892;&#27491;&#30830;&#24615;&#21644;&#27844;&#28431;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security critical software, e.g., OpenSSL, comes with numerous side-channel leakages left unpatched due to a lack of resources or experts. The situation will only worsen as the pace of code development accelerates, with developers relying on Large Language Models (LLMs) to automatically generate code. In this work, we explore the use of LLMs in generating patches for vulnerable code with microarchitectural side-channel leakages. For this, we investigate the generative abilities of powerful LLMs by carefully crafting prompts following a zero-shot learning approach. All generated code is dynamically analyzed by leakage detection tools, which are capable of pinpointing information leakage at the instruction level leaked either from secret dependent accesses or branches or vulnerable Spectre gadgets, respectively. Carefully crafted prompts are used to generate candidate replacements for vulnerable code, which are then analyzed for correctness and for leakage resilience. From a cost/perform
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.13049</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Exploration Networks. (arXiv:2308.13049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20026;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#20248;&#38597;&#30340;&#26041;&#27861;&#12290;&#26368;&#26174;&#33879;&#30340;&#26159;&#65292;&#36125;&#21494;&#26031;&#20195;&#29702;&#19981;&#20250;&#38754;&#20020;&#39057;&#29575;&#26041;&#27861;&#30340;&#25506;&#32034;/&#24320;&#21457;&#22256;&#22659;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#36825;&#22312;&#29609;&#20855;&#39046;&#22495;&#20013;&#26159;&#21487;&#35745;&#31639;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#32780;&#19981;&#26159;&#22312;&#39640;&#32500;&#29366;&#24577;&#36716;&#31227;&#20998;&#24067;&#20013;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#35201;&#20040;&#19981;&#36890;&#36807;MDP&#20256;&#25773;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#22312;&#19968;&#32452;&#35821;&#22659;&#31574;&#30053;&#20013;&#20248;&#21270;&#32780;&#19981;&#26159;&#25152;&#26377;&#21382;&#21490;&#26465;&#20214;&#31574;&#30053;&#12290;&#36825;&#20004;&#20010;&#36817;&#20284;&#24471;&#21040;&#30340;&#31574;&#30053;&#21487;&#33021;&#26159;&#20219;&#24847;&#36125;&#21494;&#26031;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#65288;Bayesian exploration network&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. A key challenge for Bayesian RL is the computational complexity of learning Bayes-optimal policies, which is only tractable in toy domains. In this paper we propose a novel model-free approach to address this challenge. Rather than modelling uncertainty in high-dimensional state transition distributions as model-based approaches do, we model uncertainty in a one-dimensional Bellman operator. Our theoretical analysis reveals that existing model-free approaches either do not propagate epistemic uncertainty through the MDP or optimise over a set of contextual policies instead of all history-conditioned policies. Both approximations yield policies that can be arbitrarily Bayes-suboptimal. To overcome these issues, we introduce the Bayesian explo
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24182;&#35299;&#20915;&#22240;&#20026;&#32570;&#22833;&#20540;&#24341;&#20837;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13047</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#35266;&#27979;&#25968;&#25454;&#30340;&#32852;&#37030;&#22240;&#26524;&#25928;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Causal Effects from Incomplete Observational Data. (arXiv:2308.13047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13047
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24182;&#35299;&#20915;&#22240;&#20026;&#32570;&#22833;&#20540;&#24341;&#20837;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#23545;&#22240;&#26524;&#25512;&#26029;&#25552;&#20986;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#36825;&#20123;&#25968;&#25454;&#28304;&#26080;&#27861;&#21512;&#24182;&#20026;&#19968;&#20010;&#23454;&#20307;&#65292;&#32780;&#20854;&#20013;&#30340;&#32570;&#22833;&#20540;&#21487;&#33021;&#20250;&#24341;&#20837;&#20559;&#24046;&#21040;&#22240;&#26524;&#20272;&#35745;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#20998;&#24067;&#24335;&#21644;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#20013;&#36827;&#34892;&#32852;&#37030;&#22240;&#26524;&#25512;&#26029;&#65292;&#20174;&#32780;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25439;&#22833;&#20989;&#25968;&#25286;&#20998;&#20026;&#22810;&#20010;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#29305;&#23450;&#25968;&#25454;&#28304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32570;&#22833;&#38543;&#26426;&#20551;&#35774;&#19979;&#32771;&#34385;&#20102;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#20272;&#35745;&#20102;&#22240;&#26524;&#20272;&#35745;&#30340;&#39640;&#38454;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#20998;&#25955;&#30340;&#25968;&#25454;&#28304;&#20013;&#24674;&#22797;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20272;&#35745;&#20102;&#24322;&#36136;&#30340;&#26465;&#20214;&#20998;&#24067;&#20197;&#24212;&#23545;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized and incomplete data sources are prevalent in real-world applications, posing a formidable challenge for causal inference. These sources cannot be consolidated into a single entity owing to privacy constraints, and the presence of missing values within them can potentially introduce bias to the causal estimands. We introduce a new approach for federated causal inference from incomplete data, enabling the estimation of causal effects from multiple decentralized and incomplete data sources. Our approach disentangles the loss function into multiple components, each corresponding to a specific data source with missing values. Our approach accounts for the missing data under the missing at random assumption, while also estimating higher-order statistics of the causal estimands. Our method recovers the conditional distribution of missing confounders given the observed confounders from the decentralized data sources to identify causal effects. Our framework estimates heterogeneou
&lt;/p&gt;</description></item><item><title>&#35270;&#39057;&#33014;&#22218;&#20869;&#38236;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#21449;&#39046;&#22495;&#38754;&#20020;&#30528;&#25216;&#26415;&#36127;&#25285;&#12289;&#35299;&#37322;&#25104;&#26412;&#39640;&#26114;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#12290;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;VCE&#25968;&#25454;&#20998;&#31867;&#20197;&#21450;&#21019;&#24314;&#36741;&#21161;&#19987;&#23478;&#30340;&#27880;&#37322;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.13035</link><description>&lt;p&gt;
&#35270;&#39057;&#33014;&#22218;&#20869;&#38236;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#21449;&#39046;&#22495;&#65306;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#29420;&#29305;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The intersection of video capsule endoscopy and artificial intelligence: addressing unique challenges using machine learning. (arXiv:2308.13035v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13035
&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#33014;&#22218;&#20869;&#38236;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#21449;&#39046;&#22495;&#38754;&#20020;&#30528;&#25216;&#26415;&#36127;&#25285;&#12289;&#35299;&#37322;&#25104;&#26412;&#39640;&#26114;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#12290;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;VCE&#25968;&#25454;&#20998;&#31867;&#20197;&#21450;&#21019;&#24314;&#36741;&#21161;&#19987;&#23478;&#30340;&#27880;&#37322;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#35328;&#65306;&#35270;&#39057;&#33014;&#22218;&#20869;&#38236;&#65288;VCE&#65289;&#30340;&#25216;&#26415;&#36127;&#25285;&#21644;&#32791;&#26102;&#30340;&#23457;&#26597;&#36807;&#31243;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26377;&#26395;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;AI&#19982;VCE&#30340;&#20132;&#21449;&#39046;&#22495;&#25581;&#31034;&#20102;&#24517;&#39035;&#20808;&#20811;&#26381;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#35201;&#35299;&#20915;&#30340;&#20116;&#20010;&#25361;&#25112;&#12290;&#25361;&#25112;1&#65306;VCE&#25968;&#25454;&#26159;&#38543;&#26426;&#30340;&#19988;&#21253;&#21547;&#26174;&#33879;&#20266;&#36857;&#12290;&#25361;&#25112;2&#65306;VCE&#35299;&#37322;&#25104;&#26412;&#39640;&#26114;&#12290;&#25361;&#25112;3&#65306;VCE&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#19981;&#24179;&#34913;&#30340;&#12290;&#25361;&#25112;4&#65306;&#29616;&#26377;&#30340;VCE AIMLT&#35745;&#31639;&#19978;&#32321;&#29712;&#12290;&#25361;&#25112;5&#65306;&#20020;&#24202;&#21307;&#29983;&#25239;&#25298;&#19981;&#33021;&#35299;&#37322;&#36807;&#31243;&#30340;AIMLT&#12290;&#26041;&#27861;&#65306;&#37319;&#29992;&#35299;&#21078;&#22320;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#27979;&#35797;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#23545;VCE&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#31181;&#36741;&#21161;&#19987;&#23478;&#23545;VCE&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#30340;&#24037;&#20855;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22810;&#24103;&#26041;&#27861;&#12289;&#22522;&#20110;&#22270;&#30340;CNN&#27169;&#22411;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction: Technical burdens and time-intensive review processes limit the practical utility of video capsule endoscopy (VCE). Artificial intelligence (AI) is poised to address these limitations, but the intersection of AI and VCE reveals challenges that must first be overcome. We identified five challenges to address. Challenge #1: VCE data are stochastic and contains significant artifact. Challenge #2: VCE interpretation is cost-intensive. Challenge #3: VCE data are inherently imbalanced. Challenge #4: Existing VCE AIMLT are computationally cumbersome. Challenge #5: Clinicians are hesitant to accept AIMLT that cannot explain their process.  Methods: An anatomic landmark detection model was used to test the application of convolutional neural networks (CNNs) to the task of classifying VCE data. We also created a tool that assists in expert annotation of VCE data. We then created more elaborate models using different approaches including a multi-frame approach, a CNN based on graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.13032</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 GPT&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 Large Language Model (LLM) &#23545;&#37329;&#34701;&#26032;&#38395;&#36827;&#34892;&#22810;&#20219;&#21153;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;PEFT/LoRA&#26041;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#20027;&#35201;&#21253;&#25324;&#20174;&#37329;&#34701;&#24066;&#22330;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#12289;&#31361;&#20986;&#25991;&#26412;&#30340;&#20027;&#35201;&#35266;&#28857;&#12289;&#23545;&#25991;&#26412;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20855;&#26377;&#36866;&#24403;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#31561;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#22810;&#20219;&#21153;&#30340;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;&#65292;&#20854;&#21709;&#24212;&#30340;&#32467;&#26500;&#21487;&#20197;&#37096;&#20998;&#20026;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#21478;&#19968;&#37096;&#20998;&#25968;&#25454;&#21487;&#20197;&#37319;&#29992;JSON&#26684;&#24335;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#23450;&#37327;&#30446;&#26631;&#21464;&#37327;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32477;&#28909;&#28436;&#21270;&#21407;&#29702;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#20026;&#32463;&#20856;&#35757;&#32451;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13028</link><description>&lt;p&gt;
&#20351;&#29992;&#36890;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks with Universal Adiabatic Quantum Computing. (arXiv:2308.13028v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32477;&#28909;&#28436;&#21270;&#21407;&#29702;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#20026;&#32463;&#20856;&#35757;&#32451;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#35757;&#32451;&#26159;&#19968;&#20010;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#28909;&#37327;&#23376;&#35745;&#31639;&#65288;AQC&#65289;&#26469;&#35757;&#32451;NN&#30340;&#26032;&#26041;&#27861;&#65292;AQC &#26159;&#21033;&#29992;&#32477;&#28909;&#28436;&#21270;&#21407;&#29702;&#26469;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#33539;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#38376;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#23454;&#29616;&#30340;&#36890;&#29992; AQC &#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#21704;&#23494;&#39039;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#20108;&#36827;&#21046;&#26435;&#37325;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;AQC&#21487;&#20197;&#38750;&#24120;&#39640;&#25928;&#22320;&#25214;&#21040;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#20026;&#32463;&#20856;&#35757;&#32451;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of neural networks (NNs) is a computationally intensive task requiring significant time and resources. This paper presents a novel approach to NN training using Adiabatic Quantum Computing (AQC), a paradigm that leverages the principles of adiabatic evolution to solve optimisation problems. We propose a universal AQC method that can be implemented on gate quantum computers, allowing for a broad range of Hamiltonians and thus enabling the training of expressive neural networks. We apply this approach to various neural networks with continuous, discrete, and binary weights. Our results indicate that AQC can very efficiently find the global minimum of the loss function, offering a promising alternative to classical training methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26497;&#20540;&#29702;&#35770;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32531;&#35299;&#26497;&#31471;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#20998;&#24067;&#39044;&#27979;&#30340;&#26497;&#20540;&#26469;&#22686;&#24378;RL&#26234;&#33021;&#20307;&#22312;&#38754;&#23545;&#38750;&#24120;&#32597;&#35265;&#21644;&#21361;&#38505;&#20107;&#20214;&#26102;&#30340;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13011</link><description>&lt;p&gt;
&#20351;&#29992;&#26497;&#20540;&#29702;&#35770;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26497;&#31471;&#39118;&#38505;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Extreme Risk Mitigation in Reinforcement Learning using Extreme Value Theory. (arXiv:2308.13011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26497;&#20540;&#29702;&#35770;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#32531;&#35299;&#26497;&#31471;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#20998;&#24067;&#39044;&#27979;&#30340;&#26497;&#20540;&#26469;&#22686;&#24378;RL&#26234;&#33021;&#20307;&#22312;&#38754;&#23545;&#38750;&#24120;&#32597;&#35265;&#21644;&#21361;&#38505;&#20107;&#20214;&#26102;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064; (RL) &#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#39118;&#38505;&#24847;&#35782;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#23545;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#30340;&#39640;&#24230;&#32597;&#35265;&#30340;&#39118;&#38505;&#20107;&#20214; (&#22870;&#21169;) &#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#20123;&#32597;&#35265;&#20107;&#20214;&#23545;&#20110;&#26088;&#22312;&#20934;&#30830;&#25429;&#25417;&#27492;&#31867;&#39118;&#38505;&#20107;&#20214;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#23384;&#22312;&#39118;&#38505;&#24863;&#30693;&#30340; RL &#25216;&#26415;&#65292;&#24403;&#24314;&#27169;&#36825;&#20123;&#32597;&#35265;&#20107;&#20214;&#26102;&#65292;&#23427;&#20204;&#30340;&#39118;&#38505;&#35268;&#36991;&#27700;&#24179;&#20005;&#37325;&#20381;&#36182;&#20110;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#20272;&#35745;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#36890;&#36807;&#19987;&#27880;&#20110;&#25913;&#36827;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#20998;&#24067;&#39044;&#27979;&#30340;&#26497;&#20540;&#26469;&#22686;&#24378; RL &#26234;&#33021;&#20307;&#22312;&#38754;&#23545;&#38750;&#24120;&#32597;&#35265;&#21644;&#21361;&#38505;&#20107;&#20214;&#26102;&#30340;&#38887;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#20998;&#24067;&#30340;&#26497;&#20540;&#34920;&#31034;&#20026;&#21442;&#25968;&#21270;&#20998;&#24067;&#65292;&#24182;&#20174;&#20013;&#33719;&#24471;&#28789;&#24863;
&lt;/p&gt;
&lt;p&gt;
Risk-sensitive reinforcement learning (RL) has garnered significant attention in recent years due to the growing interest in deploying RL agents in real-world scenarios. A critical aspect of risk awareness involves modeling highly rare risk events (rewards) that could potentially lead to catastrophic outcomes. These infrequent occurrences present a formidable challenge for data-driven methods aiming to capture such risky events accurately. While risk-aware RL techniques do exist, their level of risk aversion heavily relies on the precision of the state-action value function estimation when modeling these rare occurrences. Our work proposes to enhance the resilience of RL agents when faced with very rare and risky events by focusing on refining the predictions of the extreme values predicted by the state-action value function distribution. To achieve this, we formulate the extreme values of the state-action value function distribution as parameterized distributions, drawing inspiration 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#27604;&#20102;&#20256;&#32479;&#35774;&#35745;&#20248;&#21270;&#26041;&#27861;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36870;&#21521;&#35774;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#24773;&#26223;&#30340;&#22522;&#20934;&#38382;&#39064;&#26469;&#36827;&#34892;&#27604;&#36739;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;...</title><link>http://arxiv.org/abs/2308.13000</link><description>&lt;p&gt;
&#35774;&#35745;&#20248;&#21270;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36870;&#21521;&#35774;&#35745;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Performance Comparison of Design Optimization and Deep Learning-based Inverse Design. (arXiv:2308.13000v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;&#20256;&#32479;&#35774;&#35745;&#20248;&#21270;&#26041;&#27861;&#19982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36870;&#21521;&#35774;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#24773;&#26223;&#30340;&#22522;&#20934;&#38382;&#39064;&#26469;&#36827;&#34892;&#27604;&#36739;&#12290;&#26681;&#25454;&#30740;&#31350;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#20248;&#21270;&#26041;&#27861;&#22312;&#24037;&#31243;&#35774;&#35745;&#39046;&#22495;&#36234;&#26469;&#36234;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#28041;&#21450;&#20351;&#29992;&#20223;&#30495;&#25110;&#23454;&#38469;&#23454;&#39564;&#25968;&#25454;&#21019;&#24314;&#22522;&#20110;&#30446;&#26631;&#20989;&#25968;&#25110;&#32422;&#26463;&#26465;&#20214;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#25968;&#20540;&#20248;&#21270;&#26041;&#27861;&#20174;&#27169;&#22411;&#20013;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36870;&#21521;&#35774;&#35745;&#26041;&#27861;&#30340;&#36827;&#23637;&#20351;&#24471;&#23454;&#26102;&#29983;&#25104;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#20180;&#32454;&#27604;&#36739;&#20102;&#36825;&#31181;&#26032;&#26041;&#27861;&#19982;&#20256;&#32479;&#35774;&#35745;&#20248;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#20855;&#20307;&#20248;&#32570;&#28857;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#24773;&#26223;&#30340;&#22522;&#20934;&#38382;&#39064;&#26469;&#27604;&#36739;&#20256;&#32479;&#35774;&#35745;&#20248;&#21270;&#26041;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36870;&#21521;&#35774;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Surrogate model-based optimization has been increasingly used in the field of engineering design. It involves creating a surrogate model with objective functions or constraints based on the data obtained from simulations or real-world experiments, and then finding the optimal solution from the model using numerical optimization methods. Recent advancements in deep learning-based inverse design methods have made it possible to generate real-time optimal solutions for engineering design problems, eliminating the requirement for iterative optimization processes. Nevertheless, no comprehensive study has yet closely examined the specific advantages and disadvantages of this novel approach compared to the traditional design optimization method. The objective of this paper is to compare the performance of traditional design optimization methods with deep learning-based inverse design methods by employing benchmark problems across various scenarios. Based on the findings of this study, we prov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#26080;&#20154;&#26426;&#39030;&#37096;&#26426;&#26800;&#33218;&#25191;&#34892;&#22120;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#38388;&#21040;&#30896;&#25758;&#30340;&#36816;&#21160;&#35268;&#21010;&#27169;&#22411;&#20197;&#32469;&#36807;&#38556;&#30861;&#29289;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;Q-learning&#27169;&#22411;&#29420;&#31435;&#36861;&#36394;&#21644;&#25511;&#21046;&#26426;&#26800;&#33218;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#26399;&#26395;&#36712;&#36857;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19968;&#31995;&#21015;&#22312;&#39640;&#38590;&#24230;&#21644;&#21361;&#38505;&#29615;&#22659;&#20013;&#30340;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.12843</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#39030;&#37096;&#26426;&#26800;&#33218;&#25191;&#34892;&#22120;&#36712;&#36857;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Actuator Trajectory Planning for UAVs with Overhead Manipulator using Reinforcement Learning. (arXiv:2308.12843v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#26080;&#20154;&#26426;&#39030;&#37096;&#26426;&#26800;&#33218;&#25191;&#34892;&#22120;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26102;&#38388;&#21040;&#30896;&#25758;&#30340;&#36816;&#21160;&#35268;&#21010;&#27169;&#22411;&#20197;&#32469;&#36807;&#38556;&#30861;&#29289;&#12290;&#21516;&#26102;&#65292;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;Q-learning&#27169;&#22411;&#29420;&#31435;&#36861;&#36394;&#21644;&#25511;&#21046;&#26426;&#26800;&#33218;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#26399;&#26395;&#36712;&#36857;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#19968;&#31995;&#21015;&#22312;&#39640;&#38590;&#24230;&#21644;&#21361;&#38505;&#29615;&#22659;&#20013;&#30340;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31354;&#20013;&#26426;&#26800;&#33218;&#31995;&#32479;&#65292;&#21363;&#35013;&#22791;&#26377;&#21487;&#25511;&#21046;&#30340;&#20108;&#33258;&#30001;&#24230;&#33218;&#30340;&#26080;&#20154;&#26426;&#65292;&#20197;&#23454;&#29616;&#21363;&#26102;&#25191;&#34892;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#20351;&#29992;Q-learning&#26041;&#27861;&#26469;&#25511;&#21046;&#33218;&#23574;&#31471;&#65288;&#21363;&#26411;&#31471;&#25191;&#34892;&#22120;&#65289;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#21040;&#30896;&#25758;&#65288;TTC&#65289;&#30340;&#36816;&#21160;&#35268;&#21010;&#27169;&#22411;&#65292;&#20351;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#33021;&#22815;&#22312;&#20445;&#35777;&#26426;&#26800;&#33218;&#21487;&#36798;&#24615;&#30340;&#21516;&#26102;&#32469;&#36807;&#38556;&#30861;&#29289;&#33322;&#34892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;Q-learning&#27169;&#22411;&#29420;&#31435;&#36861;&#36394;&#21644;&#25511;&#21046;&#26426;&#26800;&#33218;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#26399;&#26395;&#36712;&#36857;&#65292;&#32473;&#23450;&#26080;&#20154;&#26426;&#24179;&#21488;&#30340;&#20219;&#24847;&#22522;&#20934;&#36712;&#36857;&#12290;&#36825;&#31181;&#32452;&#21512;&#20351;&#24471;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#25191;&#34892;&#20219;&#21153;&#65292;&#22914;&#39640;&#31354;&#28938;&#25509;&#12289;&#32467;&#26500;&#30417;&#27979;&#21644;&#20462;&#22797;&#12289;&#30005;&#27744;&#26356;&#25442;&#12289;&#25490;&#27700;&#27807;&#28165;&#29702;&#12289;&#25705;&#22825;&#22823;&#27004;&#28165;&#27905;&#21644;&#30005;&#21147;&#32447;&#36335;&#32500;&#25252;&#22312;&#38590;&#20197;&#21040;&#36798;&#21644;&#21361;&#38505;&#30340;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the operation of an aerial manipulator system, namely an Unmanned Aerial Vehicle (UAV) equipped with a controllable arm with two degrees of freedom to carry out actuation tasks on the fly. Our solution is based on employing a Q-learning method to control the trajectory of the tip of the arm, also called \textit{end-effector}. More specifically, we develop a motion planning model based on Time To Collision (TTC), which enables a quadrotor UAV to navigate around obstacles while ensuring the manipulator's reachability. Additionally, we utilize a model-based Q-learning model to independently track and control the desired trajectory of the manipulator's end-effector, given an arbitrary baseline trajectory for the UAV platform. Such a combination enables a variety of actuation tasks such as high-altitude welding, structural monitoring and repair, battery replacement, gutter cleaning, sky scrapper cleaning, and power line maintenance in hard-to-reach and risky en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Match-And-Deform&#65288;MAD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#21644;&#26102;&#38388;&#23545;&#40784;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#20013;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#24182;&#20801;&#35768;&#26102;&#38388;&#22833;&#30495;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MAD&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#39046;&#22495;&#24182;&#26368;&#22823;&#21270;&#32593;&#32476;&#21028;&#21035;&#33021;&#21147;&#30340;&#26032;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.12686</link><description>&lt;p&gt;
Match-And-Deform: &#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#21644;&#26102;&#38388;&#23545;&#40784;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Match-And-Deform: Time Series Domain Adaptation through Optimal Transport and Temporal Alignment. (arXiv:2308.12686v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Match-And-Deform&#65288;MAD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#21644;&#26102;&#38388;&#23545;&#40784;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#20013;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#24182;&#20801;&#35768;&#26102;&#38388;&#22833;&#30495;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MAD&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#39046;&#22495;&#24182;&#26368;&#22823;&#21270;&#32593;&#32476;&#21028;&#21035;&#33021;&#21147;&#30340;&#26032;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#20294;&#30456;&#20851;&#32852;&#30340;&#26631;&#31614;&#24448;&#24448;&#24456;&#23569;&#12290;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#26088;&#22312;&#21033;&#29992;&#26469;&#33258;&#28304;&#39046;&#22495;&#30340;&#26631;&#31614;&#26469;&#23545;&#26469;&#33258;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#24403;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#38500;&#20102;&#26631;&#20934;&#29305;&#24449;&#20998;&#24067;&#20559;&#31227;&#20043;&#22806;&#65292;&#36824;&#20250;&#20986;&#29616;&#26102;&#38388;&#20559;&#31227;&#30340;&#26032;&#38590;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Match-And-Deform&#65288;MAD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22312;&#20801;&#35768;&#26102;&#38388;&#22833;&#30495;&#30340;&#21516;&#26102;&#22312;&#28304;&#26102;&#38388;&#24207;&#21015;&#21644;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#25214;&#21040;&#23545;&#24212;&#20851;&#31995;&#12290;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25439;&#22833;&#21644;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#21516;&#26102;&#23545;&#40784;&#20102;&#31995;&#21015;&#12290;&#24403;&#23884;&#20837;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26102;&#65292;MAD&#26377;&#21161;&#20110;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#34920;&#31034;&#65292;&#26082;&#21487;&#20197;&#23545;&#40784;&#39046;&#22495;&#21448;&#21487;&#20197;&#26368;&#22823;&#21270;&#32593;&#32476;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;MAD&#20855;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large volumes of unlabeled data are usually available, associated labels are often scarce. The unsupervised domain adaptation problem aims at exploiting labels from a source domain to classify data from a related, yet different, target domain. When time series are at stake, new difficulties arise as temporal shifts may appear in addition to the standard feature distribution shift. In this paper, we introduce the Match-And-Deform (MAD) approach that aims at finding correspondences between the source and target time series while allowing temporal distortions. The associated optimization problem simultaneously aligns the series thanks to an optimal transport loss and the time stamps through dynamic time warping. When embedded into a deep neural network, MAD helps learning new representations of time series that both align the domains and maximize the discriminative power of the network. Empirical studies on benchmark datasets and remote sensing data demonstrate that MAD makes meanin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36974;&#32617;&#29305;&#24449;&#24314;&#27169;&#26041;&#27861;(MFM)&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#22359;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#20998;&#35789;&#22120;&#26469;&#37325;&#26500;&#35270;&#39057;&#20013;&#23545;&#35937;&#30340;&#36974;&#32617;&#29305;&#24449;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;GAT&#22359;&#34701;&#20837;&#21040;&#35270;&#39057;&#20107;&#20214;&#35782;&#21035;&#26550;&#26500;ViGAT&#20013;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12673</link><description>&lt;p&gt;
&#36974;&#32617;&#29305;&#24449;&#24314;&#27169;&#65306;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#22359;&#30340;&#29305;&#24449;&#36974;&#32617;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Masked Feature Modelling: Feature Masking for the Unsupervised Pre-training of a Graph Attention Network Block for Bottom-up Video Event Recognition. (arXiv:2308.12673v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36974;&#32617;&#29305;&#24449;&#24314;&#27169;&#26041;&#27861;(MFM)&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#22359;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#20998;&#35789;&#22120;&#26469;&#37325;&#26500;&#35270;&#39057;&#20013;&#23545;&#35937;&#30340;&#36974;&#32617;&#29305;&#24449;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;GAT&#22359;&#34701;&#20837;&#21040;&#35270;&#39057;&#20107;&#20214;&#35782;&#21035;&#26550;&#26500;ViGAT&#20013;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36974;&#32617;&#29305;&#24449;&#24314;&#27169;(MFM)&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;(GAT)&#22359;&#12290;MFM&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#20998;&#35789;&#22120;&#26469;&#37325;&#26500;&#35270;&#39057;&#20013;&#23545;&#35937;&#30340;&#36974;&#32617;&#29305;&#24449;&#65292;&#21033;&#29992;MiniKinetics&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#23558;&#39044;&#35757;&#32451;&#30340;GAT&#22359;&#34701;&#20837;&#21040;&#26368;&#20808;&#36827;&#30340;&#33258;&#24213;&#21521;&#19978;&#26377;&#30417;&#30563;&#35270;&#39057;&#20107;&#20214;&#35782;&#21035;&#26550;&#26500;ViGAT&#20013;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#36215;&#28857;&#21644;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#22312;YLI-MED&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;MFM&#22312;&#25552;&#39640;&#20107;&#20214;&#35782;&#21035;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Masked Feature Modelling (MFM), a novel approach for the unsupervised pre-training of a Graph Attention Network (GAT) block. MFM utilizes a pretrained Visual Tokenizer to reconstruct masked features of objects within a video, leveraging the MiniKinetics dataset. We then incorporate the pre-trained GAT block into a state-of-the-art bottom-up supervised video-event recognition architecture, ViGAT, to improve the model's starting point and overall accuracy. Experimental evaluations on the YLI-MED dataset demonstrate the effectiveness of MFM in improving event recognition performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12219</link><description>&lt;p&gt;
&#25193;&#23637;&#24615;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#22810;&#31181;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#24471;&#30410;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#26041;&#38754;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#20808;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#20877;&#36890;&#36807;&#25193;&#25955;&#36866;&#24212;&#23558;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#21644;&#25351;&#23548;&#35843;&#20248;&#26469;&#21457;&#25496;&#20854;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11521</link><description>&lt;p&gt;
&#33258;&#25105;&#27450;&#39575;&#65306;&#36870;&#21521;&#30772;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#38450;&#28779;&#22681;
&lt;/p&gt;
&lt;p&gt;
Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#20855;&#26377;&#25509;&#36817;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#34429;&#28982;&#20026;&#21508;&#31181;&#31038;&#20250;&#38656;&#27714;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;LLM&#20063;&#38477;&#20302;&#20102;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;LLM&#24320;&#21457;&#20154;&#21592;&#24050;&#32463;&#37096;&#32626;&#20102;&#35821;&#20041;&#32423;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25298;&#32477;&#21487;&#33021;&#23548;&#33268;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#25552;&#31034;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#38450;&#24481;&#26426;&#21046;&#24182;&#19981;&#23436;&#20840;&#21487;&#38752;&#65292;&#19968;&#20123;&#25915;&#20987;&#32773;&#24050;&#32463;&#35774;&#35745;&#20986;&#20102;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#65292;&#20020;&#26102;&#20351;LLM&#24536;&#35760;&#20869;&#23481;&#38450;&#24481;&#35268;&#21017;&#24182;&#22238;&#31572;&#20219;&#20309;&#19981;&#36866;&#24403;&#30340;&#38382;&#39064;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#23578;&#26080;&#20851;&#20110;&#36825;&#20123;&#35821;&#20041;&#32423;&#25915;&#20987;&#21644;&#38450;&#24481;&#21407;&#21017;&#30340;&#26126;&#30830;&#35299;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates trad
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#21542;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.11490</link><description>&lt;p&gt;
&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#22815;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#21542;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#65292;&#33258;&#21160;&#23558;&#20316;&#32773;&#30340;&#39118;&#26684;&#20174;&#20854;&#20889;&#20316;&#20869;&#23481;&#20013;&#20998;&#31163;&#20986;&#26469;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#19988;&#21487;&#33021;&#19981;&#21487;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#26377;&#22823;&#37327;&#24102;&#26377;&#20316;&#32773;&#26631;&#31614;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#21487;&#29992;&#65292;&#20351;&#24471;&#20197;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23398;&#20064;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#25104;&#20026;&#21487;&#33021;&#65292;&#29992;&#20110;&#20316;&#32773;&#24402;&#23646;&#36825;&#19968;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26174;&#28982;&#26356;&#22810;&#22320;&#20381;&#36182;&#20110;&#32534;&#30721;&#20889;&#20316;&#39118;&#26684;&#32780;&#19981;&#26159;&#32534;&#30721;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#19968;&#26367;&#20195;&#20219;&#21153;&#30340;&#25104;&#21151;&#24182;&#19981;&#33021;&#30830;&#20445;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#65292;&#22240;&#20026;&#20316;&#32773;&#36523;&#20221;&#20063;&#21487;&#33021;&#19982;&#20854;&#20182;&#28508;&#22312;&#21464;&#37327;&#65288;&#22914;&#20027;&#39064;&#65289;&#30456;&#20851;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#34920;&#24449;&#25152;&#20256;&#36882;&#30340;&#20449;&#24687;&#30340;&#26412;&#36136;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#39564;&#35777;&#20854;&#20027;&#35201;&#32534;&#30721;&#30340;&#26159;&#20889;&#20316;&#39118;&#26684;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#39564;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#36825;&#20123;&#34920;&#24449;&#12290;&#36825;&#20123;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20316;&#32773;&#34920;&#31034;&#23398;&#20064;&#30340;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#26469;&#25913;&#21892;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#39033;&#24335;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#22312;&#26041;&#31243;&#24674;&#22797;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#20854;&#20013;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26159;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#31526;&#21495;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.10892</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#39033;&#24335;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Bayesian polynomial neural networks and polynomial neural ordinary differential equations. (arXiv:2308.10892v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#26469;&#25913;&#21892;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#39033;&#24335;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#22312;&#26041;&#31243;&#24674;&#22797;&#38382;&#39064;&#20013;&#30340;&#34920;&#29616;&#65292;&#20854;&#20013;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26159;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#31526;&#21495;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#39033;&#24335;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26159;&#26368;&#36817;&#29992;&#20110;&#31185;&#23398;&#21644;&#24037;&#31243;&#38382;&#39064;&#26041;&#31243;&#24674;&#22797;&#30340;&#20004;&#31181;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#33021;&#25552;&#20379;&#27169;&#22411;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#65292;&#24182;&#19988;&#30446;&#21069;&#19981;&#33021;&#36866;&#24212;&#22122;&#22768;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#21644;&#39564;&#35777;&#20197;&#19979;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;: &#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#12289;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;(MCMC)&#37319;&#26679;&#26041;&#27861;&#21644;&#21464;&#20998;&#25512;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26159;&#36825;&#31867;&#38382;&#39064;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#25152;&#23646;&#30340;&#26356;&#24191;&#27867;&#30340;&#31526;&#21495;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression with polynomial neural networks and polynomial neural ordinary differential equations (ODEs) are two recent and powerful approaches for equation recovery of many science and engineering problems. However, these methods provide point estimates for the model parameters and are currently unable to accommodate noisy data. We address this challenge by developing and validating the following Bayesian inference methods: the Laplace approximation, Markov Chain Monte Carlo (MCMC) sampling methods, and variational inference. We have found the Laplace approximation to be the best method for this class of problems. Our work can be easily extended to the broader class of symbolic neural networks to which the polynomial neural network belongs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33021;&#28304;&#31649;&#29702;&#26041;&#27861;&#26469;&#20248;&#21270;&#25805;&#20316;&#20154;&#24037;&#19978;&#28044;&#31995;&#32479;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#32467;&#21512;&#20998;&#20301;&#32593;&#32476;&#21644;&#28145;&#24230;&#31454;&#20105;&#32593;&#32476;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.10199</link><description>&lt;p&gt;
&#20154;&#24037;&#19978;&#28044;&#33021;&#28304;&#31649;&#29702;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Artificial Upwelling Energy Management. (arXiv:2308.10199v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33021;&#28304;&#31649;&#29702;&#26041;&#27861;&#26469;&#20248;&#21270;&#25805;&#20316;&#20154;&#24037;&#19978;&#28044;&#31995;&#32479;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#32467;&#21512;&#20998;&#20301;&#32593;&#32476;&#21644;&#28145;&#24230;&#31454;&#20105;&#32593;&#32476;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#19978;&#28044;&#65288;AU&#65289;&#20316;&#20026;&#19968;&#31181;&#23558;&#23500;&#21547;&#33829;&#20859;&#30340;&#24213;&#23618;&#27700;&#25552;&#21319;&#21040;&#28023;&#38754;&#12289;&#21050;&#28608;&#28023;&#34299;&#29983;&#38271;&#24182;&#22686;&#21152;&#28023;&#27915;&#30899;&#23553;&#23384;&#30340;&#26041;&#27861;&#65292;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36825;&#23548;&#33268;&#22312;&#20013;&#22269;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#22826;&#38451;&#33021;&#20379;&#30005;&#21644;&#31354;&#27668;&#22686;&#21387;&#30340;AU&#31995;&#32479;&#65288;AUS&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#28023;&#27915;&#29615;&#22659;&#20013;&#39640;&#25928;&#35843;&#24230;&#27668;&#20307;&#21943;&#23556;&#31995;&#32479;&#20173;&#28982;&#26159;&#25805;&#20316;AUS&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#24320;&#21457;AUS&#36816;&#34892;&#30340;&#39640;&#25928;&#31574;&#30053;&#30340;&#26032;&#33021;&#28304;&#31649;&#29702;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26368;&#22823;&#21270;AUS&#33021;&#28304;&#25928;&#29575;&#30340;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;QR-DQN&#65289;&#20013;&#30340;&#20998;&#20301;&#32593;&#32476;&#19982;&#28145;&#24230;&#31454;&#20105;&#32593;&#32476;&#30456;&#32467;&#21512;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#20223;&#30495;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;AUS&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of artificial upwelling (AU) as a means of lifting nutrient-rich bottom water to the surface, stimulating seaweed growth, and consequently enhancing ocean carbon sequestration, has been gaining increasing attention in recent years. This has led to the development of the first solar-powered and air-lifted AU system (AUS) in China. However, efficient scheduling of air injection systems in complex marine environments remains a crucial challenge in operating AUS, as it holds the potential to significantly improve energy efficiency. To tackle this challenge, we propose a novel energy management approach that utilizes deep reinforcement learning (DRL) algorithm to develop efficient strategies for operating AUS. Specifically, we formulate the problem of maximizing the energy efficiency of AUS as a Markov decision process and integrate the quantile network in distributional reinforcement learning (QR-DQN) with the deep dueling network to solve it. Through extensive simulations, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RANL&#30340;&#26032;&#39062;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;Hessian&#21021;&#22987;&#21270;&#21644;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#21306;&#22495;&#20998;&#37197;&#65292;&#20811;&#26381;&#20102;&#29275;&#39039;&#27861;&#22312;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.10154</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#36164;&#28304;&#33258;&#36866;&#24212;&#29275;&#39039;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resource-Adaptive Newton's Method for Distributed Learning. (arXiv:2308.10154v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RANL&#30340;&#26032;&#39062;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;Hessian&#21021;&#22987;&#21270;&#21644;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#21306;&#22495;&#20998;&#37197;&#65292;&#20811;&#26381;&#20102;&#29275;&#39039;&#27861;&#22312;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#26354;&#29575;&#20449;&#24687;&#25552;&#20379;&#20102;&#27604;&#19968;&#38454;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#29275;&#39039;&#26041;&#27861;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#35832;&#22810;&#25361;&#25112;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#19982;Hessian&#30697;&#38453;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12289;&#23376;&#27169;&#22411;&#22810;&#26679;&#24615;&#12289;&#35757;&#32451;&#30340;&#36807;&#26102;&#24615;&#21644;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RANL&#30340;&#26032;&#39062;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#31616;&#21333;&#30340;Hessian&#21021;&#22987;&#21270;&#21644;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#21306;&#22495;&#20998;&#37197;&#26469;&#20811;&#26381;&#29275;&#39039;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;&#35813;&#31639;&#27861;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#22312;&#38543;&#26426;&#20248;&#21270;&#30340;&#26631;&#20934;&#20551;&#35774;&#19979;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;RANL&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#21487;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed stochastic optimization methods based on Newton's method offer significant advantages over first-order methods by leveraging curvature information for improved performance. However, the practical applicability of Newton's method is hindered in large-scale and heterogeneous learning environments due to challenges such as high computation and communication costs associated with the Hessian matrix, sub-model diversity, staleness in training, and data heterogeneity. To address these challenges, this paper introduces a novel and efficient algorithm called RANL, which overcomes the limitations of Newton's method by employing a simple Hessian initialization and adaptive assignments of training regions. The algorithm demonstrates impressive convergence properties, which are rigorously analyzed under standard assumptions in stochastic optimization. The theoretical analysis establishes that RANL achieves a linear convergence rate while effectively adapting to available resources and 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#29420;&#29305;&#30340;&#32467;&#26500;&#26469;&#32531;&#35299;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.08915</link><description>&lt;p&gt;
&#36229;&#36234;&#20849;&#20139;&#65306;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beyond Sharing: Conflict-Aware Multivariate Time Series Anomaly Detection. (arXiv:2308.08915v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08915
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#29420;&#29305;&#30340;&#32467;&#26500;&#26469;&#32531;&#35299;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;(KPI)&#20197;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;(MTS)&#30340;&#24418;&#24335;&#36827;&#34892;&#30417;&#27979;&#65292;&#20197;&#30830;&#20445;&#36719;&#20214;&#24212;&#29992;&#31243;&#24207;&#21644;&#26381;&#21153;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#20934;&#30830;&#26816;&#27979;MTS&#30340;&#24322;&#24120;&#23545;&#20110;&#21518;&#32493;&#30340;&#25925;&#38556;&#25490;&#38500;&#38750;&#24120;&#20851;&#38190;&#12290;&#24322;&#24120;&#30340;&#31232;&#32570;&#24615;&#21644;&#25163;&#21160;&#26631;&#35760;&#23548;&#33268;&#20102;&#21508;&#31181;&#33258;&#30417;&#30563;&#30340;MTS&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#26041;&#27861;&#20248;&#21270;&#20102;&#19968;&#20010;&#28085;&#30422;&#25152;&#26377;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;/&#25439;&#22833;&#30340;&#25972;&#20307;&#30446;&#26631;/&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#25351;&#26631;&#22238;&#24402;&#30446;&#26631;&#20043;&#38388;&#20914;&#31361;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23548;&#33268;MTS&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#25439;&#22833;&#20013;&#25379;&#25166;&#12290;&#36825;&#19968;&#20851;&#38190;&#26041;&#38754;&#26174;&#33879;&#24433;&#21709;&#26816;&#27979;&#24615;&#33021;&#65292;&#20294;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#20223;&#22810;&#38376;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;(MMoE)&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CAD&#65292;&#19968;&#31181;&#20914;&#31361;&#24863;&#30693;&#30340;&#22810;&#21464;&#37327;KPI&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;CAD&#20026;&#27599;&#20010;&#25351;&#26631;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#32467;&#26500;&#65292;&#20197;&#32531;&#35299;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive key performance indicators (KPIs) are monitored as multivariate time series data (MTS) to ensure the reliability of the software applications and service system. Accurately detecting the abnormality of MTS is very critical for subsequent fault elimination. The scarcity of anomalies and manual labeling has led to the development of various self-supervised MTS anomaly detection (AD) methods, which optimize an overall objective/loss encompassing all metrics' regression objectives/losses. However, our empirical study uncovers the prevalence of conflicts among metrics' regression objectives, causing MTS models to grapple with different losses. This critical aspect significantly impacts detection performance but has been overlooked in existing approaches. To address this problem, by mimicking the design of multi-gate mixture-of-experts (MMoE), we introduce CAD, a Conflict-aware multivariate KPI Anomaly Detection algorithm. CAD offers an exclusive structure for each metric to mitigate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#26680;&#21270;&#30340;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#27492;&#31639;&#27861;&#29983;&#25104;&#30340;&#32593;&#32476;&#30340;&#29305;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26680;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#31639;&#27861;&#21487;&#20197;&#23558;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#26080;&#21521;&#22270;&#65292;&#24182;&#25552;&#21462;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#20316;&#20026;&#22270;&#24418;&#12290;&#20116;&#31181;&#26680;&#20989;&#25968;&#34987;&#29992;&#20110;&#27492;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.08163</link><description>&lt;p&gt;
&#30001;&#26680;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#29983;&#25104;&#30340;&#32593;&#32476;&#30340;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Characteristics of networks generated by kernel growing neural gas. (arXiv:2308.08163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#26680;&#21270;&#30340;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#30001;&#27492;&#31639;&#27861;&#29983;&#25104;&#30340;&#32593;&#32476;&#30340;&#29305;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26680;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#31639;&#27861;&#21487;&#20197;&#23558;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#26080;&#21521;&#22270;&#65292;&#24182;&#25552;&#21462;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#20316;&#20026;&#22270;&#24418;&#12290;&#20116;&#31181;&#26680;&#20989;&#25968;&#34987;&#29992;&#20110;&#27492;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#26680;&#21270;&#30340;&#29983;&#38271;&#31070;&#32463;&#27668;&#20307;&#65288;GNG&#65289;&#31639;&#27861;&#30340;&#26680;GNG&#65292;&#24182;&#30740;&#31350;&#30001;&#26680;GNG&#29983;&#25104;&#30340;&#32593;&#32476;&#30340;&#29305;&#24449;&#12290;GNG&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#23558;&#25968;&#25454;&#38598;&#36716;&#21270;&#20026;&#26080;&#21521;&#22270;&#65292;&#20174;&#32780;&#25552;&#21462;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#20316;&#20026;&#22270;&#24418;&#12290;GNG&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21521;&#37327;&#37327;&#21270;&#12289;&#32858;&#31867;&#21644;&#19977;&#32500;&#22270;&#24418;&#20013;&#12290;&#26680;&#26041;&#27861;&#24120;&#29992;&#20110;&#23558;&#25968;&#25454;&#38598;&#26144;&#23556;&#21040;&#29305;&#24449;&#31354;&#38388;&#65292;&#20854;&#20013;&#25903;&#25345;&#21521;&#37327;&#26426;&#26159;&#26368;&#37325;&#35201;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26680;GNG&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#30001;&#26680;GNG&#29983;&#25104;&#30340;&#32593;&#32476;&#30340;&#29305;&#24615;&#12290;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#20116;&#31181;&#26680;&#20989;&#25968;&#65292;&#21253;&#25324;&#39640;&#26031;&#26680;&#12289;&#25289;&#26222;&#25289;&#26031;&#26680;&#12289;&#26607;&#35199;&#26680;&#12289;&#21453;&#22810;&#39033;&#24335;&#26680;&#21644;&#23545;&#25968;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to develop kernel GNG, a kernelized version of the growing neural gas (GNG) algorithm, and to investigate the features of the networks generated by the kernel GNG. The GNG is an unsupervised artificial neural network that can transform a dataset into an undirected graph, thereby extracting the features of the dataset as a graph. The GNG is widely used in vector quantization, clustering, and 3D graphics. Kernel methods are often used to map a dataset to feature space, with support vector machines being the most prominent application. This paper introduces the kernel GNG approach and explores the characteristics of the networks generated by kernel GNG. Five kernels, including Gaussian, Laplacian, Cauchy, inverse multiquadric, and log kernels, are used in this study.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#31995;&#32479;&#21270;&#32534;&#30721;&#21644;&#21452;&#37325;&#36974;&#34109;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08128</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#36827;&#34892;&#36974;&#34109;&#65306;&#31995;&#32479;&#21270;&#19982;&#21452;&#37325;&#36974;&#34109;
&lt;/p&gt;
&lt;p&gt;
How to Mask in Error Correction Code Transformer: Systematic and Double Masking. (arXiv:2308.08128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08128
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#31995;&#32479;&#21270;&#32534;&#30721;&#21644;&#21452;&#37325;&#36974;&#34109;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#20449;&#21644;&#23384;&#20648;&#31995;&#32479;&#20013;&#65292;&#32416;&#38169;&#30721;&#65288;ECC&#65289;&#23545;&#20110;&#30830;&#20445;&#25968;&#25454;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#24191;&#27867;&#25193;&#23637;&#65292;&#31070;&#32463;&#32593;&#32476;&#35299;&#30721;&#22120;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#65292;&#36229;&#36234;&#20256;&#32479;&#35299;&#30721;&#31639;&#27861;&#12290;&#22312;&#36825;&#20123;&#31070;&#32463;&#35299;&#30721;&#22120;&#20013;&#65292;&#32416;&#38169;&#30721;&#21464;&#21387;&#22120;&#65288;ECCT&#65289;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;ECCT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;ECC&#30340;&#31995;&#32479;&#32534;&#30721;&#25216;&#26415;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36974;&#34109;&#30697;&#38453;&#26469;&#25913;&#21892;ECCT&#30340;&#24615;&#33021;&#24182;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ECCT&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#31216;&#20026;&#21452;&#37325;&#36974;&#34109;&#30340;ECCT&#12290;&#35813;&#26550;&#26500;&#20197;&#24182;&#34892;&#26041;&#24335;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#36974;&#34109;&#30697;&#38453;&#65292;&#20197;&#23398;&#20064;&#36974;&#34109;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#32534;&#30721;&#23383;&#20301;&#20043;&#38388;&#26356;&#22810;&#26679;&#30340;&#29305;&#24449;&#20851;&#31995;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In communication and storage systems, error correction codes (ECCs) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer (ECCT) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of ECCT, we propose two novel methods. First, leveraging the systematic encoding technique of ECCs, we introduce a new masking matrix for ECCT, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of ECCT called a double-masked ECCT. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive si
&lt;/p&gt;</description></item><item><title>AudioFormer&#26159;&#19968;&#31181;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#34920;&#31034;&#26469;&#25429;&#25417;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07221</link><description>&lt;p&gt;
AudioFormer: &#36890;&#36807;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#38899;&#39057;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
AudioFormer: Audio Transformer learns audio feature representations from discrete acoustic codes. (arXiv:2308.07221v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07221
&lt;/p&gt;
&lt;p&gt;
AudioFormer&#26159;&#19968;&#31181;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32852;&#21512;&#34920;&#31034;&#26469;&#25429;&#25417;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioFormer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#26469;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#38543;&#21518;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#30340;&#24418;&#24335;&#65292;&#20511;&#21161;&#29616;&#26377;&#30340;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#35757;&#32451;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411; (MLM)&#65292;&#20174;&#32780;&#33719;&#24471;&#38899;&#39057;&#29305;&#24449;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#21019;&#20102;&#19968;&#31181;&#22810;&#27491;&#26679;&#26412;&#23545;&#27604; (MPC) &#23398;&#20064;&#26041;&#27861;&#30340;&#25972;&#21512;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21516;&#19968;&#38899;&#39057;&#36755;&#20837;&#20013;&#22810;&#20010;&#31163;&#25955;&#22768;&#23398;&#20195;&#30721;&#38388;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#31163;&#25955;&#30340;&#22768;&#23398;&#20195;&#30721;&#35270;&#20026;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#31867;&#20284;&#22635;&#31354;&#39064;&#30340;&#26041;&#27861;&#35757;&#32451;&#19968;&#20010;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#26368;&#32456;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MPC&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#21040;&#38899;&#39057;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method named AudioFormer,which learns audio feature representations through the acquisition of discrete acoustic codes and subsequently fine-tunes them for audio classification tasks. Initially,we introduce a novel perspective by considering the audio classification task as a form of natural language understanding (NLU). Leveraging an existing neural audio codec model,we generate discrete acoustic codes and utilize them to train a masked language model (MLM),thereby obtaining audio feature representations. Furthermore,we pioneer the integration of a Multi-Positive sample Contrastive (MPC) learning approach. This method enables the learning of joint representations among multiple discrete acoustic codes within the same audio input. In our experiments,we treat discrete acoustic codes as textual data and train a masked language model using a cloze-like methodology,ultimately deriving high-quality audio representations. Notably,the MPC learning technique effectively captures c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#19987;&#23478;&#26435;&#37325;&#24179;&#22343;&#21270;&#23454;&#29616;&#20102;&#23545;ViTs&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#25512;&#26029;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.06093</link><description>&lt;p&gt;
&#19987;&#23478;&#26435;&#37325;&#24179;&#22343;&#21270;: &#19968;&#31181;&#35270;&#35273;Transformer&#30340;&#26032;&#36890;&#29992;&#35757;&#32451;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Experts Weights Averaging: A New General Training Scheme for Vision Transformers. (arXiv:2308.06093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#19987;&#23478;&#26435;&#37325;&#24179;&#22343;&#21270;&#23454;&#29616;&#20102;&#23545;ViTs&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#25512;&#26029;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#37325;&#21442;&#25968;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#36890;&#29992;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#25512;&#26029;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#12290;&#38543;&#30528;&#35270;&#35273;Transformer (ViTs)&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#36880;&#28176;&#36229;&#36234;CNNs&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;: &#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;ViTs&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#25512;&#26029;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#24615;&#33021;&#65311;&#26368;&#36817;&#65292;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#26377;&#25928;&#22320;&#25193;&#23637;Transformer&#30340;&#23481;&#37327;&#65292;&#32780;&#25104;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#32771;&#34385;&#21040;MoE&#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#22810;&#25903;&#31995;&#32467;&#26500;&#65292;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;MoE&#26469;&#23454;&#29616;&#31867;&#20284;&#32467;&#26500;&#37325;&#21442;&#25968;&#21270;&#30340;ViT&#35757;&#32451;&#26041;&#26696;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;ViTs&#36890;&#29992;&#35757;&#32451;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20998;&#31163;&#20102;ViTs&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#26367;&#25442;&#20102;&#19968;&#20123;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;
&lt;/p&gt;
&lt;p&gt;
Structural re-parameterization is a general training scheme for Convolutional Neural Networks (CNNs), which achieves performance improvement without increasing inference cost. As Vision Transformers (ViTs) are gradually surpassing CNNs in various visual tasks, one may question: if a training scheme specifically for ViTs exists that can also achieve performance improvement without increasing inference cost? Recently, Mixture-of-Experts (MoE) has attracted increasing attention, as it can efficiently scale up the capacity of Transformers at a fixed cost through sparsely activated experts. Considering that MoE can also be viewed as a multi-branch structure, can we utilize MoE to implement a ViT training scheme similar to structural re-parameterization? In this paper, we affirmatively answer these questions, with a new general training strategy for ViTs. Specifically, we decouple the training and inference phases of ViTs. During training, we replace some Feed-Forward Networks (FFNs) of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20195;&#30721;&#20013;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#23618;&#26469;&#25552;&#39640;&#22312;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.03312</link><description>&lt;p&gt;
&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#27861;&#29992;&#20110;&#23398;&#20064;&#20195;&#30721;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20195;&#30721;&#20013;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#23618;&#26469;&#25552;&#39640;&#22312;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#21160;&#31243;&#24207;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36825;&#26159;&#35768;&#22810;&#23433;&#20840;&#20219;&#21153;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#20195;&#30721;&#30340;LLM&#26550;&#26500;&#36890;&#24120;&#20174;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#20511;&#29992;&#65292;&#24341;&#21457;&#23545;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#26410;&#30693;&#20195;&#30721;&#30340;&#20581;&#22766;&#24615;&#30340;&#25285;&#24551;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#27867;&#21270;&#25361;&#25112;&#26159;&#23558;&#20195;&#30721;&#35821;&#20041;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#32435;&#20837;LLM&#26550;&#26500;&#20013;&#12290;&#21463;&#21040;&#21033;&#29992;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#21367;&#31215;&#23618;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20195;&#30721;&#23545;&#31216;&#24615;&#22914;&#20309;&#22686;&#24378;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#30340;LLM&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#32676;&#35770;&#26694;&#26550;&#65292;&#24418;&#24335;&#21270;&#22320;&#23450;&#20041;&#20102;&#20195;&#30721;&#23545;&#31216;&#24615;&#20316;&#20026;&#20445;&#25345;&#35821;&#20041;&#30340;&#21464;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;LLM&#26550;&#26500;&#20013;&#31934;&#30830;&#25512;&#29702;&#23545;&#31216;&#24615;&#20445;&#25345;&#30340;&#25216;&#26415;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20445;&#25345;&#31243;&#24207;&#23545;&#31216;&#24615;&#30340;&#26032;&#22411;&#33258;&#27880;&#24847;&#21147;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.  Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiv
&lt;/p&gt;</description></item><item><title>ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01423</link><description>&lt;p&gt;
ChatMOF: &#19968;&#31181;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;
&lt;/p&gt;
&lt;p&gt;
ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01423
&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#20010;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#65288;MOFs&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;gpt-3.5-turbo&#65289;&#65292;ChatMOF&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#21018;&#24615;&#32467;&#26500;&#21270;&#26597;&#35810;&#30340;&#38656;&#27714;&#12290;&#35813;&#31995;&#32479;&#30001;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#65288;&#21363;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#65289;&#32452;&#25104;&#65292;&#24418;&#25104;&#19968;&#20010;&#24378;&#22823;&#30340;&#27969;&#27700;&#32447;&#65292;&#31649;&#29702;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#12290;&#35813;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
&lt;/p&gt;</description></item><item><title>BubbleML&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29289;&#29702;&#39537;&#21160;&#27169;&#25311;&#33719;&#24471;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#24182;&#22312;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.14623</link><description>&lt;p&gt;
BubbleML: &#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning. (arXiv:2307.14623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14623
&lt;/p&gt;
&lt;p&gt;
BubbleML&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#29289;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#29289;&#29702;&#39537;&#21160;&#27169;&#25311;&#33719;&#24471;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#24182;&#22312;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#20854;&#21487;&#38752;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30456;&#21464;&#29616;&#35937;&#39046;&#22495;&#65292;&#32570;&#20047;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#30340;&#21487;&#35775;&#38382;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23454;&#39564;&#25968;&#25454;&#38598;&#36890;&#24120;&#21463;&#38480;&#65292;&#21487;&#29992;&#24615;&#26377;&#38480;&#19988;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#31232;&#32570;&#65292;&#38459;&#30861;&#20102;&#25105;&#20204;&#23545;&#36825;&#31181;&#22797;&#26434;&#22810;&#29289;&#29702;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BubbleML&#25968;&#25454;&#38598;&#65288;https://github.com/HPCForge/BubbleML&#65289;&#65292;&#23427;&#21033;&#29992;&#29289;&#29702;&#39537;&#21160;&#30340;&#27169;&#25311;&#20026;&#21508;&#31181;&#27832;&#33150;&#22330;&#26223;&#25552;&#20379;&#20934;&#30830;&#30340;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#65292;&#21253;&#25324;&#26680;&#27873;&#27744;&#27832;&#33150;&#12289;&#27969;&#21160;&#27832;&#33150;&#21644;&#20122;&#20919;&#27832;&#33150;&#12290;&#36825;&#20010;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#21442;&#25968;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#37325;&#21147;&#26465;&#20214;&#12289;&#27969;&#37327;&#12289;&#20122;&#20919;&#27700;&#24179;&#21644;&#22721;&#38754;&#36807;&#28909;&#65292;&#24635;&#20849;&#26377;51&#20010;&#27169;&#25311;&#12290;BubbleML&#24050;&#32463;&#36890;&#36807;&#23454;&#39564;&#35266;&#23519;&#21644;&#36235;&#21183;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#34987;&#30830;&#35748;&#20026;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20419;&#36827;&#22810;&#26679;&#21270;&#38477;&#20302;&#28201;&#24230;&#27832;&#33150;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth data, impeding our understanding of this complex multi-physics phenomena. To bridge this gap, we present the BubbleML Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 51 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate exploration of diverse dow
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.07873</link><description>&lt;p&gt;
&#25506;&#32034;&#20174;&#26367;&#20195;&#35757;&#32451;&#20013;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;DNNs&#30340;&#23545;&#25239;&#26679;&#26412;(AEs)&#24050;&#32463;&#34920;&#26126;&#26159;&#21487;&#36716;&#31227;&#30340;&#65306;&#25104;&#21151;&#27450;&#39575;&#30333;&#30418;&#23376;&#26367;&#20195;&#27169;&#22411;&#30340;AEs&#20063;&#21487;&#20197;&#27450;&#39575;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#12290;&#34429;&#28982;&#35768;&#22810;&#32463;&#39564;&#30740;&#31350;&#25552;&#20379;&#20102;&#29983;&#25104;&#39640;&#24230;&#21487;&#36716;&#31227;AE&#30340;&#25351;&#23548;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#32570;&#20047;&#35299;&#37322;&#29978;&#33267;&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;&#26412;&#25991;&#22312;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#26041;&#38754;&#12290;&#20174;&#30528;&#21517;&#30340;&#23567;&#20581;&#22766;&#24615;&#29616;&#35937;&#24320;&#22987;&#65292;&#36890;&#36807;&#20197;&#36731;&#24494;&#25200;&#21160;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#23545;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#24402;&#22240;&#20110;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23427;&#20204;&#30340;&#20849;&#21516;&#25928;&#26524;&#19978;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#19982;&#21487;&#36716;&#31227;&#24615;&#30340;&#21333;&#29420;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#25512;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20102;min-max&#20248;&#21270;&#22312;&#24310;&#36831;&#19979;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#31616;&#21333;&#23454;&#20363;&#65292;&#21363;&#20351;&#26159;&#23567;&#30340;&#24310;&#36831;&#20063;&#21487;&#33021;&#23548;&#33268;Extra-gradient&#31639;&#27861;&#21457;&#25955;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#24310;&#36831;&#29256;&#26412;&#30340;min-max&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#25216;&#26415;&#20551;&#35774;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#22312;&#24310;&#36831;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.06886</link><description>&lt;p&gt;
Min-Max&#20248;&#21270;&#22312;&#24310;&#36831;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Min-Max Optimization under Delays. (arXiv:2307.06886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06886
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20102;min-max&#20248;&#21270;&#22312;&#24310;&#36831;&#19979;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#31616;&#21333;&#23454;&#20363;&#65292;&#21363;&#20351;&#26159;&#23567;&#30340;&#24310;&#36831;&#20063;&#21487;&#33021;&#23548;&#33268;Extra-gradient&#31639;&#27861;&#21457;&#25955;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#24310;&#36831;&#29256;&#26412;&#30340;min-max&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#30340;&#25216;&#26415;&#20551;&#35774;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#31639;&#27861;&#22312;&#24310;&#36831;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#20449;&#36215;&#37325;&#35201;&#20316;&#29992;&#30340;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#24310;&#36831;&#21644;&#24322;&#27493;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#30740;&#31350;&#22242;&#38431;&#24191;&#27867;&#20998;&#26512;&#20102;&#20855;&#26377;&#24310;&#36831;&#26799;&#24230;&#30340;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26080;&#31867;&#20284;&#30340;&#29702;&#35770;&#21487;&#29992;&#20110;min-max&#20248;&#21270;&#65292;&#36825;&#20010;&#35805;&#39064;&#30001;&#20110;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#21338;&#24328;&#35770;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#32780;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;&#24102;&#26377;&#24310;&#36831;&#26799;&#24230;&#26356;&#26032;&#30340;&#26631;&#20934;min-max&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#65288;&#32463;&#39564;&#24615;&#22320;&#65289;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#23567;&#30340;&#24310;&#36831;&#20063;&#21487;&#33021;&#23548;&#33268;&#20687;Extra-gradient (EG) &#36825;&#26679;&#30340;&#26480;&#20986;&#31639;&#27861;&#22312;&#31616;&#21333;&#23454;&#20363;&#19978;&#21457;&#25955;&#65292;&#32780;&#22312;&#27809;&#26377;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;EG&#21487;&#20197;&#20445;&#35777;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#26377;&#24517;&#35201;&#23545;&#24310;&#36831;&#29256;&#26412;&#30340;min-max&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20180;&#32454;&#20998;&#26512;&#12290;&#30456;&#24212;&#22320;&#65292;&#22312;&#36866;&#24403;&#30340;&#25216;&#26415;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477; - &#19978;&#21319; (GDA)&#31639;&#27861;&#22312;&#24310;&#36831;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delays and asynchrony are inevitable in large-scale machine-learning problems where communication plays a key role. As such, several works have extensively analyzed stochastic optimization with delayed gradients. However, as far as we are aware, no analogous theory is available for min-max optimization, a topic that has gained recent popularity due to applications in adversarial robustness, game theory, and reinforcement learning. Motivated by this gap, we examine the performance of standard min-max optimization algorithms with delayed gradient updates. First, we show (empirically) that even small delays can cause prominent algorithms like Extra-gradient (\texttt{EG}) to diverge on simple instances for which \texttt{EG} guarantees convergence in the absence of delays. Our empirical study thus suggests the need for a careful analysis of delayed versions of min-max optimization algorithms. Accordingly, under suitable technical assumptions, we prove that Gradient Descent-Ascent (\texttt{G
&lt;/p&gt;</description></item><item><title>IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06698</link><description>&lt;p&gt;
IntelliGraphs: &#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06698
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#36830;&#32493;&#34920;&#31034;&#12290;&#25991;&#29486;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#26159;&#39044;&#27979;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#38142;&#25509;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#19981;&#20165;&#20165;&#26159;&#38142;&#25509;&#30340;&#38598;&#21512;&#65292;&#36824;&#20855;&#26377;&#20854;&#32467;&#26500;&#20013;&#30340;&#35821;&#20041;&#12290;&#35821;&#20041;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#26597;&#35810;&#22238;&#31572;&#25110;&#25512;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23376;&#22270;&#25512;&#26029;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#24517;&#39035;&#29983;&#25104;&#21487;&#33021;&#30340;&#24182;&#19988;&#35821;&#20041;&#19978;&#26377;&#25928;&#30340;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IntelliGraphs&#65292;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#30340;&#38598;&#21512;&#12290;IntelliGraphs&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#20256;&#32479;KGE&#30340;&#19977;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#35821;&#20041;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#19968;&#22522;&#20934;&#23558;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;YOLOv5&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#26410;&#35265;&#36807;&#30446;&#26631;&#30340;&#36793;&#30028;&#26694;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.17829</link><description>&lt;p&gt;
&#32852;&#21512;&#38598;&#25104;YOLOv5 - &#19968;&#31181;&#26356;&#22909;&#30340;&#24191;&#20041;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Ensemble YOLOv5 - A Better Generalized Object Detection Algorithm. (arXiv:2306.17829v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#35757;&#32451;&#30340;YOLOv5&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#26410;&#35265;&#36807;&#30446;&#26631;&#30340;&#36793;&#30028;&#26694;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#31639;&#27861;&#24050;&#32463;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#65292;&#20294;&#26159;&#32852;&#21512;&#23398;&#20064;&#31639;&#27861;&#22914;&#32852;&#21512;&#24179;&#22343;&#65288;FED Avg&#65289;&#25110;&#32852;&#21512;SGD&#65288;FED SGD&#65289;&#19982;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#30340;&#30456;&#20284;&#24615;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;FL&#22312;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#26041;&#27861;&#30340;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;FL&#35757;&#32451;&#30340;YOLOv5&#27169;&#22411;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#37319;&#29992;&#26080;&#26367;&#25442;&#30340;&#38543;&#26426;&#25277;&#26679;&#31574;&#30053;&#65292;&#20351;&#27599;&#20010;&#23458;&#25143;&#31471;&#25345;&#26377;&#19968;&#37096;&#20998;&#29992;&#20110;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#30456;&#21516;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;FL&#30446;&#26631;&#26816;&#27979;&#22120;&#30340;&#20840;&#23616;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;&#26410;&#35265;&#36807;&#30446;&#26631;&#30340;&#36793;&#30028;&#26694;&#26041;&#38754;&#30340;&#21331;&#36234;&#25928;&#29575;&#65292;&#27979;&#35797;&#38598;&#26159;&#26469;&#33258;&#20004;&#20010;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#23545;&#35937;&#30340;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has gained significant traction as a privacy-preserving algorithm, but the underlying resembles of federated learning algorithm like Federated averaging (FED Avg) or Federated SGD (FED SGD) to ensemble learning algorithms has not been fully explored. The purpose of this paper is to examine the application of FL to object detection as a method to enhance generalizability, and to compare its performance against a centralized training approach for an object detection algorithm. Specifically, we investigate the performance of a YOLOv5 model trained using FL across multiple clients and employ a random sampling strategy without replacement, so each client holds a portion of the same dataset used for centralized training. Our experimental results showcase the superior efficiency of the FL object detector's global model in generating accurate bounding boxes for unseen objects, with the test set being a mixture of objects from two distinct clients not represented in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#36136;&#37327;&#26816;&#39564;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;YOLOv5&#20316;&#20026;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#21644;Federated Averaging&#20316;&#20026;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#25972;&#20307;&#23458;&#25143;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#30456;&#23545;&#20110;&#20351;&#29992;&#26412;&#22320;&#23458;&#25143;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#30340;&#36793;&#30028;&#26694;&#12290;</title><link>http://arxiv.org/abs/2306.17645</link><description>&lt;p&gt;
&#20849;&#20139;&#29983;&#20135;&#20013;&#30340;&#32852;&#37030;&#30446;&#26631;&#26816;&#27979;&#29992;&#20110;&#36136;&#37327;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Federated Object Detection for Quality Inspection in Shared Production. (arXiv:2306.17645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#36136;&#37327;&#26816;&#39564;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;YOLOv5&#20316;&#20026;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#21644;Federated Averaging&#20316;&#20026;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#25972;&#20307;&#23458;&#25143;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#29983;&#25104;&#30456;&#23545;&#20110;&#20351;&#29992;&#26412;&#22320;&#23458;&#25143;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#30340;&#36793;&#30028;&#26694;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#22312;&#20998;&#25955;&#25968;&#25454;&#30340;&#26465;&#20214;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#65292;&#32780;&#19981;&#20250;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;YOLOv5&#20316;&#20026;&#30446;&#26631;&#26816;&#27979;&#31639;&#27861;&#21644;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#20316;&#20026;FL&#31639;&#27861;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#36136;&#37327;&#26816;&#39564;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#21046;&#36896;&#19994;&#20013;&#30340;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#65292;&#22810;&#20010;&#24037;&#21378;/&#23458;&#25143;&#20849;&#20139;&#25968;&#25454;&#20197;&#35757;&#32451;&#20840;&#23616;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#38750;IID&#25968;&#25454;&#38598;&#19978;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#25972;&#20307;&#23458;&#25143;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#36793;&#30028;&#26694;&#30456;&#23545;&#20110;&#20351;&#29992;&#26412;&#22320;&#23458;&#25143;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#21046;&#36896;&#19994;&#36136;&#37327;&#26816;&#39564;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#21033;&#29992;YOLOv5&#21644;FedAvg&#36827;&#34892;&#32852;&#37030;&#30446;&#26631;&#26816;&#27979;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a promising approach for training machine learning models on decentralized data without compromising data privacy. In this paper, we propose a FL algorithm for object detection in quality inspection tasks using YOLOv5 as the object detection algorithm and Federated Averaging (FedAvg) as the FL algorithm. We apply this approach to a manufacturing use-case where multiple factories/clients contribute data for training a global object detection model while preserving data privacy on a non-IID dataset. Our experiments demonstrate that our FL approach achieves better generalization performance on the overall clients' test dataset and generates improved bounding boxes around the objects compared to models trained using local clients' datasets. This work showcases the potential of FL for quality inspection tasks in the manufacturing industry and provides valuable insights into the performance and feasibility of utilizing YOLOv5 and FedAvg for federated ob
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.15749</link><description>&lt;p&gt;
&#20309;&#21435;&#20309;&#20174;&#65306;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#25968;&#23383;&#30828;&#20214;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
To Spike or Not To Spike: A Digital Hardware Perspective on Deep Learning Acceleration. (arXiv:2306.15749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15749
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#26088;&#22312;&#36890;&#36807;&#20223;&#30495;&#33041;&#37096;&#25805;&#20316;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20294;&#26159;&#22312;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#19978;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#22312;&#28085;&#30422;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#31454;&#20105;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#25928;&#29575;&#20026;&#20195;&#20215;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#33021;&#21147;&#12290;&#29983;&#29289;&#33041;&#30340;&#21151;&#32791;&#25928;&#29575;&#36229;&#36807;&#20219;&#20309;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65307;&#22240;&#27492;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#35797;&#22270;&#27169;&#20223;&#33041;&#37096;&#25805;&#20316;&#65292;&#20363;&#22914;&#22522;&#20110;&#33033;&#20914;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#20197;&#25552;&#39640;DL&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#33041;&#37096;&#26377;&#35832;&#22914;&#39640;&#25928;&#30340;&#20449;&#24687;&#20256;&#36755;&#12289;&#23494;&#38598;&#30340;&#31070;&#32463;&#20803;&#36830;&#25509;&#21644;&#35745;&#31639;&#19982;&#23384;&#20648;&#30340;&#20849;&#21516;&#20301;&#32622;&#31561;&#20248;&#21183;&#65292;&#20294;&#21487;&#29992;&#30340;&#29983;&#29289;&#22522;&#24213;&#20005;&#37325;&#38480;&#21046;&#20102;&#29983;&#29289;&#22823;&#33041;&#30340;&#36827;&#21270;&#12290;&#30005;&#23376;&#30828;&#20214;&#27809;&#26377;&#30456;&#21516;&#30340;&#32422;&#26463;&#65307;&#22240;&#27492;&#65292;&#34429;&#28982;&#24314;&#27169;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21487;&#33021;&#25581;&#31034;&#20102;&#19968;&#20010;&#35868;&#39064;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;SNNs&#30340;&#39640;&#25928;&#30828;&#20214;&#21518;&#31471;&#35774;&#35745;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning models scale, they become increasingly competitive from domains spanning computer vision to natural language processing; however, this happens at the expense of efficiency since they require increasingly more memory and computing power. The power efficiency of the biological brain outperforms the one of any large-scale deep learning (DL) model; thus, neuromorphic computing tries to mimic the brain operations, such as spike-based information processing, to improve the efficiency of DL models. Despite the benefits of the brain, such as efficient information transmission, dense neuronal interconnects, and the co-location of computation and memory, the available biological substrate has severely constrained the evolution of biological brains. Electronic hardware does not have the same constraints; therefore, while modeling spiking neural networks (SNNs) might uncover one piece of the puzzle, the design of efficient hardware backends for SNNs needs further investigation, po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#24490;&#29615;&#24037;&#20316;&#27969;&#65292;&#29992;&#20110;&#33258;&#28982;&#28798;&#23475;&#21518;&#24555;&#36895;&#35757;&#32451;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22312;2023&#24180;&#23494;&#35199;&#35199;&#27604;&#24030;&#28378;&#21160;&#21449;&#21475;&#40857;&#21367;&#39118;&#20107;&#20214;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.12589</link><description>&lt;p&gt;
&#24555;&#36895;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#24037;&#20316;&#27969;&#65306;&#38024;&#23545;2023&#24180;&#23494;&#35199;&#35199;&#27604;&#24030;&#28378;&#21160;&#21449;&#21475;&#40857;&#21367;&#39118;&#20107;&#20214;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Rapid building damage assessment workflow: An implementation for the 2023 Rolling Fork, Mississippi tornado event. (arXiv:2306.12589v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#24490;&#29615;&#24037;&#20316;&#27969;&#65292;&#29992;&#20110;&#33258;&#28982;&#28798;&#23475;&#21518;&#24555;&#36895;&#35757;&#32451;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#22312;2023&#24180;&#23494;&#35199;&#35199;&#27604;&#24030;&#28378;&#21160;&#21449;&#21475;&#40857;&#21367;&#39118;&#20107;&#20214;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#28798;&#23475;&#21518;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#23545;&#20110;&#25351;&#23548;&#21644;&#20248;&#21270;&#31532;&#19968;&#24212;&#31572;&#32773;&#30340;&#21162;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#28798;&#23475;&#25439;&#20260;&#12289;&#21355;&#26143;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#32570;&#20047;&#24191;&#27867;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#31561;&#38382;&#39064;&#65292;&#20197;&#33258;&#21160;&#21270;&#26041;&#24335;&#25191;&#34892;&#27492;&#31867;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#24182;&#19981;&#23481;&#26131;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#22312;&#24490;&#29615;&#24037;&#20316;&#27969;&#65292;&#29992;&#20110;&#22312;&#33258;&#28982;&#28798;&#23475;&#21518;&#24555;&#36895;&#35757;&#32451;&#24314;&#31569;&#25439;&#20260;&#35780;&#20272;&#27169;&#22411;&#12290;&#35813;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#27492;&#24037;&#20316;&#27969;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#24037;&#20316;&#27969;&#26159;&#19982;&#32654;&#22269;&#32418;&#21313;&#23383;&#20250;&#21512;&#20316;&#25191;&#34892;&#30340;&#65292;&#38024;&#23545;2023&#24180;3&#26376;&#23494;&#35199;&#35199;&#27604;&#24030;&#28378;&#21160;&#21449;&#21475;&#40857;&#21367;&#39118;&#20107;&#20214;&#12290;&#26681;&#25454;&#21518;&#28798;&#24773;&#25910;&#38598;&#30340;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#20154;&#22312;&#24490;&#29615;&#27169;&#22411;&#36807;&#31243;&#30340;&#36755;&#20986;&#22312;&#21463;&#25439;&#24314;&#31569;&#26041;&#38754;&#23454;&#29616;&#20102;0.86&#30340;&#31934;&#24230;&#21644;0.80&#30340;&#21484;&#22238;&#29575;&#12290;&#36825;&#20010;&#24037;&#20316;&#27969;&#30340;&#31471;&#21040;&#31471;&#23454;&#29616;&#26102;&#38388;&#19981;&#21040;2&#20010;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid and accurate building damage assessments from high-resolution satellite imagery following a natural disaster is essential to inform and optimize first responder efforts. However, performing such building damage assessments in an automated manner is non-trivial due to the challenges posed by variations in disaster-specific damage, diversity in satellite imagery, and the dearth of extensive, labeled datasets. To circumvent these issues, this paper introduces a human-in-the-loop workflow for rapidly training building damage assessment models after a natural disaster. This article details a case study using this workflow, executed in partnership with the American Red Cross during a tornado event in Rolling Fork, Mississippi in March, 2023. The output from our human-in-the-loop modeling process achieved a precision of 0.86 and recall of 0.80 for damaged buildings when compared to ground truth data collected post-disaster. This workflow was implemented end-to-end in under 2 hours per s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;AutoML&#25216;&#26415;&#26469;&#20943;&#23569;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#36827;&#20248;&#21270;&#20989;&#25968;&#21644;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#32467;&#21512;&#20844;&#24179;&#30446;&#26631;&#65292;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#22522;&#20110;ML&#30340;&#36719;&#20214;&#20013;&#30340;&#20559;&#35265;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;AutoML&#30340;&#20844;&#24179;&#24863;&#30693;&#25628;&#32034;&#31354;&#38388;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20462;&#22797;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.09297</link><description>&lt;p&gt;
&#20462;&#22797;&#20844;&#24179;&#24615;&#65292;&#32780;&#19981;&#26159;&#30772;&#22351;&#20934;&#30830;&#24615;&#65306;&#20351;&#29992;AutoML&#30340;&#24615;&#33021;&#24863;&#30693;&#20844;&#24179;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Fix Fairness, Don't Ruin Accuracy: Performance Aware Fairness Repair using AutoML. (arXiv:2306.09297v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;AutoML&#25216;&#26415;&#26469;&#20943;&#23569;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#36827;&#20248;&#21270;&#20989;&#25968;&#21644;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#32467;&#21512;&#20844;&#24179;&#30446;&#26631;&#65292;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#22522;&#20110;ML&#30340;&#36719;&#20214;&#20013;&#30340;&#20559;&#35265;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;AutoML&#30340;&#20844;&#24179;&#24863;&#30693;&#25628;&#32034;&#31354;&#38388;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20462;&#22797;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#20851;&#38190;&#20915;&#31574;&#36719;&#20214;&#20013;&#65292;&#20294;&#20107;&#25925;&#24341;&#21457;&#20102;&#20851;&#20110;ML&#39044;&#27979;&#20844;&#24179;&#24615;&#30340;&#36136;&#30097;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38656;&#35201;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#26469;&#20943;&#23569;&#22522;&#20110;ML&#30340;&#36719;&#20214;&#20013;&#30340;&#20559;&#35265;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20559;&#35265;&#32531;&#35299;&#31639;&#27861;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#21482;&#33021;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#25216;&#26415;&#26469;&#20943;&#23569;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#20989;&#25968;&#21644;&#19968;&#20010;&#20844;&#24179;&#24863;&#30693;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#36890;&#36807;&#25913;&#36827;AutoML&#30340;&#40664;&#35748;&#20248;&#21270;&#20989;&#25968;&#24182;&#32467;&#21512;&#20844;&#24179;&#30446;&#26631;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;AutoML&#30340;&#20844;&#24179;&#24863;&#30693;&#25628;&#32034;&#31354;&#38388;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20462;&#22797;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#26368;&#20808;&#36827;&#30340;Auto-Sklearn&#24037;&#20855;&#19978;&#65292;&#26088;&#22312;&#20943;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) is increasingly being used in critical decision-making software, but incidents have raised questions about the fairness of ML predictions. To address this issue, new tools and methods are needed to mitigate bias in ML-based software. Previous studies have proposed bias mitigation algorithms that only work in specific situations and often result in a loss of accuracy. Our proposed solution is a novel approach that utilizes automated machine learning (AutoML) techniques to mitigate bias. Our approach includes two key innovations: a novel optimization function and a fairness-aware search space. By improving the default optimization function of AutoML and incorporating fairness objectives, we are able to mitigate bias with little to no loss of accuracy. Additionally, we propose a fairness-aware search space pruning method for AutoML to reduce computational cost and repair time. Our approach, built on the state-of-the-art Auto-Sklearn tool, is designed to reduce bias i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;&#27169;&#22411;&#25913;&#21892;&#20154;&#26426;&#22270;&#20687;&#20998;&#26512;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20811;&#26381;&#38024;&#23545;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.05952</link><description>&lt;p&gt;
&#20811;&#26381;&#38024;&#23545;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Overcoming Adversarial Attacks for Human-in-the-Loop Applications. (arXiv:2306.05952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05952
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;&#27169;&#22411;&#25913;&#21892;&#20154;&#26426;&#22270;&#20687;&#20998;&#26512;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20811;&#26381;&#38024;&#23545;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21253;&#21547;&#20154;&#31867;&#20998;&#26512;&#21487;&#33021;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#38887;&#24615;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30456;&#23545;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#31070;&#32463;&#32593;&#32476;&#35270;&#35273;&#35299;&#37322;&#22270;&#32463;&#24120;&#23481;&#26131;&#36973;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#20026;&#20102;&#35753;&#22270;&#20687;&#20998;&#26512;&#32773;&#35780;&#20272;&#32473;&#23450;&#27169;&#22411;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#36873;&#25321;&#31283;&#20581;&#30340;&#35299;&#37322;&#21487;&#35270;&#21270;&#12290;&#36825;&#20123;&#22240;&#32032;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#20154;&#26426;&#20132;&#20114;&#65288;HITL&#65289;&#35780;&#20272;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#23545;&#25239;&#24615;&#22270;&#20687;&#65292;&#21253;&#25324;&#35299;&#37322;&#22270;&#21644;&#40065;&#26834;&#24615;&#27979;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;&#27169;&#22411;&#21487;&#20197;&#25913;&#21892;&#20154;&#26426;&#22270;&#20687;&#20998;&#26512;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#65292;&#22312;&#36825;&#31181;&#23545;&#25239;&#24615;&#29615;&#22659;&#19979;&#65292;&#22914;&#20309;&#20351;HITL&#35780;&#20272;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Including human analysis has the potential to positively affect the robustness of Deep Neural Networks and is relatively unexplored in the Adversarial Machine Learning literature. Neural network visual explanation maps have been shown to be prone to adversarial attacks. Further research is needed in order to select robust visualizations of explanations for the image analyst to evaluate a given model. These factors greatly impact Human-In-The-Loop (HITL) evaluation tools due to their reliance on adversarial images, including explanation maps and measurements of robustness. We believe models of human visual attention may improve interpretability and robustness of human-machine imagery analysis systems. Our challenge remains, how can HITL evaluation be robust in this adversarial landscape?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02207</link><description>&lt;p&gt;
SpeechGen: &#21033;&#29992;&#25552;&#31034;&#35299;&#38145;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;ChatGPT&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#36830;&#32493;&#35821;&#38899;&#30452;&#25509;&#36866;&#24212;&#20110;&#22788;&#29702;&#31163;&#25955;&#26631;&#35760;&#30340;LLM&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#36825;&#22952;&#30861;&#20102;LLM&#22312;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#39640;&#32423;&#35821;&#38899;LM&#20204;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#35821;&#38899;&#20449;&#21495;&#25152;&#21253;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#21253;&#25324;&#35828;&#35805;&#32773;&#21644;&#24773;&#24863;&#31561;&#65292;&#36825;&#20123;&#20449;&#24687;&#20165;&#36890;&#36807;&#25991;&#26412;&#25968;&#25454;&#26080;&#27861;&#33719;&#21462;&#12290;&#22312;&#19968;&#20123;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#31454;&#20105;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20294;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#22320;&#28608;&#21457;&#35821;&#38899;LM&#30340;&#29983;&#25104;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20808;&#39537;&#24615;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#22312;&#31216;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#20013;&#20351;&#29992;&#25552;&#31034;&#35843;&#33410;&#26469;&#21050;&#28608;&#35821;&#38899;LM&#36827;&#34892;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20855;&#26377;&#32422;10M&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#26680;&#28151;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#34920;&#31034;&#36830;&#32493;&#21644;&#31163;&#25955;&#38543;&#26426;&#21464;&#37327;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#26500;&#24314;&#21487;&#24494;&#20998;&#30340;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#23494;&#24230;&#20272;&#35745;&#12289;&#25512;&#29702;&#21644;&#37319;&#26679;&#65292;&#20197;&#21450;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#21028;&#21035;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.18204</link><description>&lt;p&gt;
&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#30340;&#37327;&#23376;&#26680;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantum Kernel Mixtures for Probabilistic Deep Learning. (arXiv:2305.18204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#26680;&#28151;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#34920;&#31034;&#36830;&#32493;&#21644;&#31163;&#25955;&#38543;&#26426;&#21464;&#37327;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#26500;&#24314;&#21487;&#24494;&#20998;&#30340;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#23494;&#24230;&#20272;&#35745;&#12289;&#25512;&#29702;&#21644;&#37319;&#26679;&#65292;&#20197;&#21450;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#21028;&#21035;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#37327;&#23376;&#26680;&#28151;&#21512;&#65292;&#23427;&#26159;&#20174;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#30340;&#25968;&#23398;&#24418;&#24335;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#34920;&#31034;&#36830;&#32493;&#21644;&#31163;&#25955;&#38543;&#26426;&#21464;&#37327;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#26500;&#24314;&#21487;&#24494;&#20998;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23494;&#24230;&#20272;&#35745;&#12289;&#25512;&#29702;&#21644;&#37319;&#26679;&#65292;&#20174;&#32780;&#33021;&#22815;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#36793;&#38469;&#21644;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#34920;&#31034;&#65292;&#21487;&#20197;&#24320;&#21457;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#12289;&#32452;&#21512;&#30340;&#21644;&#21487;&#36870;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#23494;&#24230;&#20272;&#35745;&#12289;&#21028;&#21035;&#23398;&#20064;&#21644;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#31034;&#20363;&#26469;&#35828;&#26126;&#35813;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65306;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#33258;&#28982;&#22320;&#36716;&#21270;&#20026;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#24471;&#30410;&#20110;&#37327;&#23376;&#26680;&#28151;&#21512;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to probabilistic deep learning (PDL), quantum kernel mixtures, derived from the mathematical formalism of quantum density matrices, which provides a simpler yet effective mechanism for representing joint probability distributions of both continuous and discrete random variables. The framework allows for the construction of differentiable models for density estimation, inference, and sampling, enabling integration into end-to-end deep neural models. In doing so, we provide a versatile representation of marginal and joint probability distributions that allows us to develop a differentiable, compositional, and reversible inference procedure that covers a wide range of machine learning tasks, including density estimation, discriminative learning, and generative modeling. We illustrate the broad applicability of the framework with two examples: an image classification model, which can be naturally transformed into a conditional generative model thanks to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#30830;&#23450;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#35782;&#21035;&#20998;&#24067;&#22806;&#30340;&#27979;&#35797;&#26679;&#26412;&#24182;&#25351;&#23548;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.07618</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation for Deep Learning Image Reconstruction using a Local Lipschitz Metric. (arXiv:2305.07618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#30830;&#23450;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#35782;&#21035;&#20998;&#24067;&#22806;&#30340;&#27979;&#35797;&#26679;&#26412;&#24182;&#25351;&#23548;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#19982;&#25104;&#20687;&#30456;&#20851;&#30340;&#21453;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22270;&#20687;&#37325;&#24314;&#12290;&#22312;&#27169;&#22411;&#37096;&#32626;&#26102;&#65292;&#21487;&#33021;&#20250;&#36935;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#24046;&#24322;&#36739;&#22823;&#30340;&#36755;&#20837;&#20998;&#24067;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#25968;&#25454;&#20559;&#24046;&#25110;&#28418;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20174;&#21333;&#20010;&#35757;&#32451;&#27169;&#22411;&#20013;&#30830;&#23450;&#30340;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#22270;&#20687;&#37325;&#24314;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;Lipschitz&#20540;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#21333;&#35843;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#27492;&#26041;&#27861;&#25552;&#20379;&#30830;&#23450;&#26159;&#21542;&#36866;&#21512;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#30340;&#38408;&#20540;&#12290;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21487;&#29992;&#20110;&#35782;&#21035;&#20998;&#24067;&#22806;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#20851;&#32852;&#20851;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#20449;&#24687;&#65292;&#24182;&#25351;&#23548;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#20013;&#29305;&#21035;&#37325;&#35201;&#30340;&#26159;&#65292;&#37327;&#21270;&#23398;&#20064;&#37325;&#26500;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#20026;&#35786;&#26029;&#21644;&#27835;&#30103;&#21487;&#33021;&#20250;&#21463;&#21040;&#37325;&#24314;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of deep learning approaches for image reconstruction is of contemporary interest in radiology, especially for approaches that solve inverse problems associated with imaging. In deployment, these models may be exposed to input distributions that are widely shifted from training data, due in part to data biases or drifts. We propose a metric based on local Lipschitz determined from a single trained model that can be used to estimate the model uncertainty for image reconstructions. We demonstrate a monotonic relationship between the local Lipschitz value and Mean Absolute Error and show that this method can be used to provide a threshold that determines whether a given DL reconstruction approach was well suited to the task. Our uncertainty estimation method can be used to identify out-of-distribution test samples, relate information regarding epistemic uncertainties, and guide proper data augmentation. Quantifying uncertainty of learned reconstruction approaches is especially pert
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PubMed&#25968;&#25454;&#24211;&#30340;PGB&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#24322;&#26500;&#22270;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#21644;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;21&#20010;&#31995;&#32479;&#24615;&#35780;&#20215;&#20027;&#39064;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.02691</link><description>&lt;p&gt;
PGB&#65306;&#29992;&#20110;&#24322;&#26500;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#30340;PubMed&#22270;&#25968;&#25454;&#38598;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PGB: A PubMed Graph Benchmark for Heterogeneous Network Representation Learning. (arXiv:2305.02691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02691
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PubMed&#25968;&#25454;&#24211;&#30340;PGB&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#24322;&#26500;&#22270;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#21644;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;21&#20010;&#31995;&#32479;&#24615;&#35780;&#20215;&#20027;&#39064;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#30340;&#25968;&#37327;&#24555;&#36895;&#22686;&#38271;&#65292;&#20294;&#26159;&#25429;&#25417;&#36825;&#20123;&#25991;&#31456;&#30340;&#25991;&#29486;&#20449;&#24687;&#30340;&#24322;&#36136;&#24615;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#23613;&#31649;&#36890;&#36807;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#25366;&#25496;&#30740;&#31350;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#25429;&#25417;&#21040;&#20102;PubMed&#25968;&#25454;&#24211;&#30340;&#24322;&#36136;&#24615;&#20173;&#19981;&#28165;&#26970;&#65292;&#32780;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;3300&#19975;&#31687;&#25991;&#31456;&#30340;&#24222;&#22823;&#25968;&#23383;&#36164;&#26009;&#24211;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PubMed Graph Benchmark&#65288;PGB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#24322;&#26500;&#22270;&#23884;&#20837;&#30340;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;PGB&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#24322;&#26500;&#32593;&#32476;&#20043;&#19968;&#65292;&#21253;&#21547;3000&#19975;&#31687;&#33521;&#25991;&#25991;&#31456;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#21253;&#21547;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#20316;&#32773;&#12289;&#24341;&#29992;&#12289;MeSH&#26415;&#35821;&#12289;MeSH&#23618;&#27425;&#32467;&#26500;&#21644;&#20854;&#20182;&#19968;&#20123;&#20449;&#24687;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;3&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;21&#20010;&#31995;&#32479;&#24615;&#35780;&#20215;&#20027;&#39064;&#30340;&#35780;&#20272;&#20219;&#21153;&#12290;&#22312;PGB&#20013;&#65292;&#25105;&#20204;&#23558;&#19982;PubMed&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#30456;&#20851;&#30340;&#20803;&#25968;&#25454;&#32858;&#21512;&#25104;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a rapid growth in biomedical literature, yet capturing the heterogeneity of the bibliographic information of these articles remains relatively understudied. Although graph mining research via heterogeneous graph neural networks has taken center stage, it remains unclear whether these approaches capture the heterogeneity of the PubMed database, a vast digital repository containing over 33 million articles. We introduce PubMed Graph Benchmark (PGB), a new benchmark dataset for evaluating heterogeneous graph embeddings for biomedical literature. PGB is one of the largest heterogeneous networks to date and consists of 30 million English articles. The benchmark contains rich metadata including abstract, authors, citations, MeSH terms, MeSH hierarchy, and some other information. The benchmark contains an evaluation task of 21 systematic reviews topics from 3 different datasets. In PGB, we aggregate the metadata associated with the biomedical articles from PubMed into a unified
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21560;&#24341;&#23376;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#25552;&#21319;&#21644;&#37325;&#26500;&#26426;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#21560;&#24341;&#22495;&#20043;&#38388;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;&#21482;&#38656;&#19977;&#20010;&#33258;&#30001;&#24230;&#30340;&#32447;&#24615;&#37325;&#26500;&#23601;&#21487;&#20197;&#20840;&#23616;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.11860</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#20010;&#21560;&#24341;&#23376;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#25552;&#21319;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
On the lifting and reconstruction of nonlinear systems with multiple attractors. (arXiv:2304.11860v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21560;&#24341;&#23376;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#25552;&#21319;&#21644;&#37325;&#26500;&#26426;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#21560;&#24341;&#22495;&#20043;&#38388;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;&#21482;&#38656;&#19977;&#20010;&#33258;&#30001;&#24230;&#30340;&#32447;&#24615;&#37325;&#26500;&#23601;&#21487;&#20197;&#20840;&#23616;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#36890;&#36807;&#20851;&#27880;&#19981;&#21464;&#23376;&#31354;&#38388;&#20013;&#30340;&#35266;&#27979;&#37327;&#30340;&#28436;&#21270;&#65292;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#32447;&#24615;&#35270;&#35282;&#12290;&#24863;&#20852;&#36259;&#30340;&#35266;&#27979;&#37327;&#36890;&#24120;&#26159;&#20174;Koopman&#29305;&#24449;&#20989;&#25968;&#32447;&#24615;&#37325;&#26500;&#20986;&#26469;&#30340;&#12290;&#23613;&#31649;Koopman&#31639;&#23376;&#22312;&#36807;&#21435;&#20960;&#24180;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#20010;&#31283;&#23450;&#28857;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#20851;&#20110;Koopman&#31639;&#23376;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#19968;&#20123;&#35823;&#35299;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#20855;&#26377;&#22810;&#20010;&#21560;&#24341;&#23376;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#25552;&#21319;&#26426;&#21046;&#12290;&#36890;&#36807;&#32771;&#34385;Duffing&#25391;&#33633;&#22120;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#21033;&#29992;&#21560;&#24341;&#22495;&#20043;&#38388;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;Koopman&#21487;&#35266;&#27979;&#31354;&#38388;&#20013;&#20855;&#26377;&#19977;&#20010;&#33258;&#30001;&#24230;&#30340;&#32447;&#24615;&#37325;&#26500;&#23601;&#36275;&#20197;&#20840;&#23616;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Koopman operator provides a linear perspective on non-linear dynamics by focusing on the evolution of observables in an invariant subspace. Observables of interest are typically linearly reconstructed from the Koopman eigenfunctions. Despite the broad use of Koopman operators over the past few years, there exist some misconceptions about the applicability of Koopman operators to dynamical systems with more than one fixed point. In this work, an explanation is provided for the mechanism of lifting for the Koopman operator of nonlinear systems with multiple attractors. Considering the example of the Duffing oscillator, we show that by exploiting the inherent symmetry between the basins of attraction, a linear reconstruction with three degrees of freedom in the Koopman observable space is sufficient to globally linearize the system.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#20165;&#20381;&#36182;&#31895;&#31961;&#30340;&#31181;&#26063;&#31867;&#21035;&#21487;&#33021;&#25513;&#30422;&#20102;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#20013;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#31181;&#26063;&#25968;&#25454;&#37319;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.09270</link><description>&lt;p&gt;
&#31895;&#31961;&#30340;&#31181;&#26063;&#25968;&#25454;&#25513;&#30422;&#20102;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Coarse race data conceals disparities in clinical risk score performance. (arXiv:2304.09270v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09270
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#20165;&#20381;&#36182;&#31895;&#31961;&#30340;&#31181;&#26063;&#31867;&#21035;&#21487;&#33021;&#25513;&#30422;&#20102;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#20013;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#31181;&#26063;&#25968;&#25454;&#37319;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#36890;&#24120;&#21482;&#35760;&#24405;&#30149;&#20154;&#30340;&#31895;&#30053;&#31181;&#26063;&#32452;&#65306;&#20363;&#22914;&#65292;&#21360;&#24230;&#21644;&#20013;&#22269;&#30149;&#20154;&#36890;&#24120;&#37117;&#34987;&#32534;&#30721;&#20026;&#8220;&#20122;&#27954;&#20154;&#8221;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#31181;&#31895;&#30053;&#32534;&#30721;&#26159;&#21542;&#25513;&#30422;&#20102;&#31934;&#32454;&#31181;&#26063;&#32452;&#20043;&#38388;&#30340;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#26412;&#25991;&#21033;&#29992;418K&#32039;&#24613;&#31185;&#23460;&#23601;&#35786;&#30340;&#25968;&#25454;&#65292;&#35780;&#20272;&#20102;&#19977;&#31181;&#32467;&#23616;&#12289;&#20116;&#31181;&#39118;&#38505;&#35780;&#20998;&#21644;&#22235;&#31181;&#34920;&#29616;&#25351;&#26631;&#30340;&#31934;&#32454;&#31181;&#26063;&#32452;&#20043;&#38388;&#30340;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#24046;&#24322;&#12290;&#22312;&#21508;&#31181;&#32467;&#23616;&#21644;&#25351;&#26631;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#31895;&#30053;&#31181;&#26063;&#31867;&#21035;&#20869;&#23384;&#22312;&#37325;&#35201;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#31895;&#30053;&#31867;&#21035;&#20869;&#65292;&#24615;&#33021;&#25351;&#26631;&#30340;&#21464;&#24322;&#24120;&#24120;&#36229;&#36807;&#31895;&#30053;&#31867;&#21035;&#20043;&#38388;&#30340;&#21464;&#24322;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#24046;&#24322;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#32467;&#23616;&#29575;&#12289;&#29305;&#24449;&#20998;&#24067;&#20197;&#21450;&#29305;&#24449;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#19981;&#21516;&#31934;&#32454;&#31181;&#26063;&#31867;&#21035;&#20043;&#38388;&#37117;&#26377;&#26174;&#30528;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20381;&#36182;&#31895;&#31961;&#30340;&#31181;&#26063;&#31867;&#21035;&#21487;&#33021;&#25513;&#30422;&#20102;&#20020;&#24202;&#39118;&#38505;&#35780;&#20998;&#34920;&#29616;&#20013;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#38656;&#35201;&#25910;&#38598;&#26356;&#31934;&#32454;&#30340;&#31181;&#26063;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare data in the United States often records only a patient's coarse race group: for example, both Indian and Chinese patients are typically coded as ``Asian.'' It is unknown, however, whether this coarse coding conceals meaningful disparities in the performance of clinical risk scores across granular race groups. Here we show that it does. Using data from 418K emergency department visits, we assess clinical risk score performance disparities across granular race groups for three outcomes, five risk scores, and four performance metrics. Across outcomes and metrics, we show that there are significant granular disparities in performance within coarse race categories. In fact, variation in performance metrics within coarse groups often exceeds the variation between coarse groups. We explore why these disparities arise, finding that outcome rates, feature distributions, and the relationships between features and outcomes all vary significantly across granular race categories. Our res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedLSM&#26694;&#26550;&#20197;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#26631;&#31614;&#38598;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#20197;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25110;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20998;&#31867;&#23618;&#37319;&#29992;&#36880;&#31867;&#21035;&#33258;&#36866;&#24212;&#32858;&#21512;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26631;&#31614;&#38598;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.06931</link><description>&lt;p&gt;
&#38754;&#21521;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#26631;&#31614;&#38598;&#19981;&#21305;&#37197;&#30340;&#35268;&#27169;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scale Federated Learning for Label Set Mismatch in Medical Image Classification. (arXiv:2304.06931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedLSM&#26694;&#26550;&#20197;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#26631;&#31614;&#38598;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#20197;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25110;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20998;&#31867;&#23618;&#37319;&#29992;&#36880;&#31867;&#21035;&#33258;&#36866;&#24212;&#32858;&#21512;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26631;&#31614;&#38598;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#20551;&#23450;&#27599;&#20010;&#23458;&#25143;&#31471;&#25317;&#26377;&#30456;&#21516;&#30340;&#26631;&#31614;&#38598;&#12290;&#20107;&#23454;&#19978;&#65292;&#21307;&#23398;&#19987;&#23478;&#24448;&#24448;&#21482;&#27880;&#37322;&#20854;&#30693;&#35782;&#39046;&#22495;&#25110;&#20852;&#36259;&#33539;&#22260;&#20869;&#30340;&#30142;&#30149;&#12290;&#36825;&#24847;&#21619;&#30528;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#38598;&#21487;&#33021;&#26159;&#19981;&#21516;&#29978;&#33267;&#26159;&#19981;&#30456;&#20132;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedLSM&#26694;&#26550;&#20197;&#35299;&#20915;&#26631;&#31614;&#38598;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;FedLSM&#37319;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#22788;&#29702;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#31243;&#24230;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25110;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20998;&#31867;&#23618;&#37319;&#29992;&#36880;&#31867;&#21035;&#33258;&#36866;&#24212;&#32858;&#21512;&#20197;&#36991;&#20813;&#23458;&#25143;&#31471;&#32570;&#22833;&#26631;&#31614;&#26102;&#30340;&#19981;&#20934;&#30830;&#32858;&#21512;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#23454;&#38469;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;FedLSM&#65292;&#21253;&#25324;&#25317;&#26377;112,120&#24352;&#33016;&#36879;&#22270;&#20687;&#30340;&#33016;&#37096;X&#23556;&#32447;&#35786;&#26029;&#21644;&#30382;&#32932;&#30149;&#21464;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has been introduced to the healthcare domain as a decentralized learning paradigm that allows multiple parties to train a model collaboratively without privacy leakage. However, most previous studies have assumed that every client holds an identical label set. In reality, medical specialists tend to annotate only diseases within their knowledge domain or interest. This implies that label sets in each client can be different and even disjoint. In this paper, we propose the framework FedLSM to solve the problem Label Set Mismatch. FedLSM adopts different training strategies on data with different uncertainty levels to efficiently utilize unlabeled or partially labeled data as well as class-wise adaptive aggregation in the classification layer to avoid inaccurate aggregation when clients have missing labels. We evaluate FedLSM on two public real-world medical image datasets, including chest x-ray (CXR) diagnosis with 112,120 CXR images and skin lesion diagnosis wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#65292;&#23427;&#29992;&#19968;&#31181;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.05527</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#30830;&#23450;&#24615;&#30446;&#26631;&#30340;&#40657;&#21283;&#23376;&#21464;&#20998;&#25512;&#26029;&#65306;&#26356;&#24555;&#65292;&#26356;&#31934;&#30830;&#65292;&#26356;&#40657;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#65292;&#23427;&#29992;&#19968;&#31181;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#21464;&#20998;&#25512;&#26029;&#65288;ADVI&#65289;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#20195;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#20013;&#24555;&#36895;&#26131;&#29992;&#30340;&#21518;&#39564;&#36817;&#20284;&#26041;&#27861;&#12290;&#28982;&#32780;&#23427;&#30340;&#38543;&#26426;&#20248;&#21270;&#22120;&#32570;&#20047;&#26126;&#30830;&#30340;&#25910;&#25947;&#26631;&#20934;&#65292;&#24182;&#19988;&#38656;&#35201;&#35843;&#25972;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;ADVI&#32487;&#25215;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;MFVB&#65289;&#30340;&#36739;&#24046;&#21518;&#39564;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#30830;&#23450;&#24615;ADVI&#8221;&#65288;DADVI&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;DADVI&#29992;&#22266;&#23450;&#30340;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26367;&#25442;&#20102;MFVB&#30340;&#19981;&#21487;&#35299;&#30446;&#26631;&#65292;&#36825;&#19968;&#25216;&#26415;&#22312;&#38543;&#26426;&#20248;&#21270;&#25991;&#29486;&#20013;&#34987;&#31216;&#20026;&#8220;&#26679;&#26412;&#24179;&#22343;&#36817;&#20284;&#8221;&#65288;SAA&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#36817;&#20284;&#20294;&#30830;&#23450;&#30340;&#30446;&#26631;&#65292;DADVI&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#20108;&#38454;&#20248;&#21270;&#65292;&#32780;&#19988;&#19982;&#26631;&#20934;&#22343;&#20540;&#22330;ADVI&#19981;&#21516;&#30340;&#26159;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#26356;&#20934;&#30830;&#30340;&#21518;&#39564;&#32447;&#24615;&#21709;&#24212;&#65288;LR&#65289;&#21327;&#26041;&#24046;&#20272;&#35745;&#12290;&#19982;&#29616;&#26377;&#30340;&#26368;&#22351;&#24773;&#20917;&#29702;&#35770;&#30456;&#21453;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24120;&#35265;&#30340;&#32479;&#35745;&#38382;&#39064;&#31867;&#21035;&#19978;&#65292;DADVI&#21644;SAA&#21487;&#20197;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic differentiation variational inference (ADVI) offers fast and easy-to-use posterior approximation in multiple modern probabilistic programming languages. However, its stochastic optimizer lacks clear convergence criteria and requires tuning parameters. Moreover, ADVI inherits the poor posterior uncertainty estimates of mean-field variational Bayes (MFVB). We introduce ``deterministic ADVI'' (DADVI) to address these issues. DADVI replaces the intractable MFVB objective with a fixed Monte Carlo approximation, a technique known in the stochastic optimization literature as the ``sample average approximation'' (SAA). By optimizing an approximate but deterministic objective, DADVI can use off-the-shelf second-order optimization, and, unlike standard mean-field ADVI, is amenable to more accurate posterior linear response (LR) covariance estimates. In contrast to existing worst-case theory, we show that, on certain classes of common statistical problems, DADVI and the SAA can perform 
&lt;/p&gt;</description></item><item><title>StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.03853</link><description>&lt;p&gt;
StepMix: &#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables. (arXiv:2304.03853v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03853
&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#22806;&#37096;&#21464;&#37327;&#24191;&#20041;&#28151;&#21512;&#27169;&#22411;&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#21333;&#27493;&#21644;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#36827;&#34892;&#27169;&#22411;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
StepMix&#26159;&#19968;&#20010;&#29992;&#20110;&#24191;&#20041;&#26377;&#38480;&#28151;&#21512;&#27169;&#22411;(&#28508;&#22312;&#21078;&#38754;&#21644;&#28508;&#22312;&#31867;&#20998;&#26512;)&#19982;&#22806;&#37096;&#21464;&#37327;(&#21327;&#21464;&#37327;&#21644;&#36828;&#31243;&#32467;&#26524;)&#30340;&#20266;&#20284;&#28982;&#20272;&#35745;(&#21333;&#27493;&#12289;&#20004;&#27493;&#21644;&#19977;&#27493;&#26041;&#27861;)&#30340;&#24320;&#28304;&#36719;&#20214;&#21253;&#12290;&#22312;&#35768;&#22810;&#31038;&#20250;&#31185;&#23398;&#30340;&#24212;&#29992;&#20013;&#65292;&#20027;&#35201;&#30446;&#26631;&#19981;&#20165;&#26159;&#23558;&#20010;&#20307;&#32858;&#31867;&#25104;&#28508;&#22312;&#31867;&#21035;&#65292;&#36824;&#21253;&#25324;&#20351;&#29992;&#36825;&#20123;&#31867;&#21035;&#26469;&#24320;&#21457;&#26356;&#22797;&#26434;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20998;&#20026;&#19968;&#20010;&#23558;&#28508;&#22312;&#31867;&#21035;&#19982;&#35266;&#23519;&#25351;&#26631;&#30456;&#20851;&#32852;&#30340;&#27979;&#37327;&#27169;&#22411;&#21644;&#19968;&#20010;&#23558;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#21464;&#37327;&#19982;&#28508;&#22312;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#27169;&#22411;&#12290;&#27979;&#37327;&#21644;&#32467;&#26500;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#25152;&#35859;&#30340;&#19968;&#27493;&#27861;&#20849;&#21516;&#20272;&#35745;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#36880;&#27493;&#26041;&#27861;&#36880;&#27493;&#20272;&#35745;&#65292;&#23545;&#20110;&#20174;&#19994;&#20154;&#21592;&#26469;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20272;&#35745;&#28508;&#22312;&#31867;&#21035;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#38500;&#20102;&#19968;&#27493;&#27861;&#65292;StepMix&#36824;&#23454;&#29616;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#26368;&#37325;&#35201;&#30340;&#36880;&#27493;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#26041;&#20415;&#27169;&#22411;&#30340;&#20272;&#35745;&#12289;&#36873;&#25321;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
StepMix is an open-source software package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;ECG&#20449;&#21495;&#36827;&#34892;5&#31867;&#21644;17&#31867;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#30340;&#36229;&#36731;&#37327;&#32423;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;(BNN)&#65292;&#22312;&#23384;&#20648;&#20351;&#29992;&#29575;&#26368;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;96.90%&#21644;97.50%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.01568</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#36731;&#37327;&#32423;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Arrhythmia Classifier Based on Ultra-Lightweight Binary Neural Network. (arXiv:2304.01568v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;ECG&#20449;&#21495;&#36827;&#34892;5&#31867;&#21644;17&#31867;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#30340;&#36229;&#36731;&#37327;&#32423;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;(BNN)&#65292;&#22312;&#23384;&#20648;&#20351;&#29992;&#29575;&#26368;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#36798;&#21040;&#20102;96.90%&#21644;97.50%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#29702;&#26377;&#25928;&#22320;&#36890;&#36807;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#30417;&#27979;&#24515;&#24459;&#22833;&#24120;&#23545;&#20154;&#31867;&#20581;&#24247;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ECG&#20998;&#31867;&#31639;&#27861;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#22312;&#39640;&#31934;&#24230;&#21644;&#22797;&#26434;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#23548;&#33268;&#23384;&#20648;&#20351;&#29992;&#29575;&#21644;&#21151;&#32791;&#24456;&#39640;&#12290;&#36825;&#20063;&#19981;&#21487;&#36991;&#20813;&#22320;&#22686;&#21152;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#21487;&#31359;&#25140;&#20154;&#24037;&#26234;&#33021;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#23454;&#29616;&#30340;&#38590;&#24230;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#36229;&#36731;&#37327;&#32423;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;(BNN)&#65292;&#33021;&#22815;&#36890;&#36807;ECG&#20449;&#21495;&#36827;&#34892;5&#31867;&#21644;17&#31867;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;BNN&#22312;5&#31867;&#21644;17&#31867;&#20998;&#31867;&#20013;&#20998;&#21035;&#36798;&#21040;&#20102;96.90% (&#23436;&#20840;&#31934;&#24230;97.09%)&#21644;97.50% (&#23436;&#20840;&#31934;&#24230;98.00%)&#30340;&#20934;&#30830;&#29575;&#65292;&#23384;&#20648;&#20351;&#29992;&#29575;&#26368;&#20302;(3.76 KB&#21644;4.45 KB)&#12290;&#19982;&#20854;&#20182;&#20108;&#20540;&#21270;&#20316;&#21697;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25903;&#25345;&#20004;&#20010;&#22810;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasonably and effectively monitoring arrhythmias through ECG signals has significant implications for human health. With the development of deep learning, numerous ECG classification algorithms based on deep learning have emerged. However, most existing algorithms trade off high accuracy for complex models, resulting in high storage usage and power consumption. This also inevitably increases the difficulty of implementation on wearable Artificial Intelligence-of-Things (AIoT) devices with limited resources. In this study, we proposed a universally applicable ultra-lightweight binary neural network(BNN) that is capable of 5-class and 17-class arrhythmia classification based on ECG signals. Our BNN achieves 96.90% (full precision 97.09%) and 97.50% (full precision 98.00%) accuracy for 5-class and 17-class classification, respectively, with state-of-the-art storage usage (3.76 KB and 4.45 KB). Compared to other binarization works, our approach excels in supporting two multi-classificatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#27861;&#30340;&#28145;&#24230;&#31070;&#32463;&#27491;&#21017;&#21270;&#22120;&#23478;&#26063;&#65292;&#20445;&#35777;&#21487;&#20197;&#36866;&#37197;&#25968;&#25454;&#24182;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;&#22312;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#23567;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#31561;&#38382;&#39064;&#19978;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2304.00015</link><description>&lt;p&gt;
DRIP: &#36870;&#38382;&#39064;&#30340;&#28145;&#24230;&#27491;&#21017;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
DRIP: Deep Regularizers for Inverse Problems. (arXiv:2304.00015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#27861;&#30340;&#28145;&#24230;&#31070;&#32463;&#27491;&#21017;&#21270;&#22120;&#23478;&#26063;&#65292;&#20445;&#35777;&#21487;&#20197;&#36866;&#37197;&#25968;&#25454;&#24182;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;&#22312;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#23567;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#31561;&#38382;&#39064;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#38382;&#39064;&#22312;&#25968;&#23398;&#19978;&#26159;&#19981;&#33391;&#23450;&#20041;&#30340;&#65292;&#22240;&#27492;&#23545;&#20110;&#19968;&#20123;&#65288;&#24102;&#26377;&#22122;&#22768;&#30340;&#65289;&#25968;&#25454;&#65292;&#21487;&#33021;&#20250;&#26377;&#19981;&#27490;&#19968;&#20010;&#19982;&#25968;&#25454;&#21305;&#37197;&#30340;&#35299;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20123;&#33021;&#22815;&#25214;&#21040;&#26368;&#21512;&#36866;&#35299;&#20915;&#26041;&#26696;&#30340;&#28145;&#24230;&#31070;&#32463;&#25216;&#26415;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65306;&#22823;&#22810;&#25968;&#25216;&#26415;&#26080;&#27861;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#22312;&#25512;&#29702;&#26102;&#21305;&#37197;&#25968;&#25454;&#65307;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#30340;&#25512;&#23548;&#26159;&#22522;&#20110;&#19968;&#20010;&#26377;&#25928;&#30340;&#26631;&#37327;&#27491;&#21017;&#21270;&#20989;&#25968;&#23384;&#22312;&#30340;&#22522;&#30784;&#20043;&#19978;&#65292;&#20294;&#22312;&#23454;&#38469;&#36816;&#29992;&#20013;&#36825;&#20123;&#25216;&#26415;&#24182;&#27809;&#26377;&#20381;&#36182;&#20110;&#36825;&#26679;&#19968;&#20010;&#20989;&#25968;&#65292;&#22240;&#27492;&#19982;&#20256;&#32479;&#30340;&#21464;&#20998;&#25216;&#26415;&#26377;&#25152;&#20559;&#31163;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#27491;&#21017;&#21270;&#22120;&#23478;&#26063;&#26469;&#35299;&#20915;&#36870;&#38382;&#39064;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#22120;&#22522;&#20110;&#21464;&#20998;&#24418;&#24335;&#65292;&#24182;&#20445;&#35777;&#36866;&#37197;&#25968;&#25454;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#23427;&#20204;&#22312;&#19968;&#20123;&#39640;&#24230;ill-posed&#38382;&#39064;&#19978;&#30340;&#20351;&#29992;&#65292;&#21253;&#25324;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#23567;&#35282;&#24230;&#23618;&#26512;&#25104;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems are mathematically ill-posed. Thus, given some (noisy) data, there is more than one solution that fits the data. In recent years, deep neural techniques that find the most appropriate solution, in the sense that it contains a-priori information, were developed. However, they suffer from several shortcomings. First, most techniques cannot guarantee that the solution fits the data at inference. Second, while the derivation of the techniques is inspired by the existence of a valid scalar regularization function, such techniques do not in practice rely on such a function, and therefore veer away from classical variational techniques. In this work we introduce a new family of neural regularizers for the solution of inverse problems. These regularizers are based on a variational formulation and are guaranteed to fit the data. We demonstrate their use on a number of highly ill-posed problems, from image deblurring to limited angle tomography.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#20165;&#32771;&#34385;&#30452;&#25509;&#24433;&#21709;&#65292;&#36824;&#33021;&#32771;&#34385;&#38388;&#25509;&#24433;&#21709;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#37327;&#21270;&#22240;&#26524;&#24402;&#22240;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#22240;&#26524;&#24402;&#22240;&#12290;</title><link>http://arxiv.org/abs/2303.13850</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22240;&#26524;&#24402;&#22240;&#23398;&#20064;&#65306;&#36229;&#36234;&#30452;&#25509;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#20165;&#32771;&#34385;&#30452;&#25509;&#24433;&#21709;&#65292;&#36824;&#33021;&#32771;&#34385;&#38388;&#25509;&#24433;&#21709;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#37327;&#21270;&#22240;&#26524;&#24402;&#22240;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#22240;&#26524;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25429;&#25417;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#22240;&#26524;&#26041;&#27861;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20551;&#35774;&#36755;&#20837;&#21464;&#37327;&#29420;&#31435;&#65288;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65289;&#65292;&#22240;&#27492;&#20165;&#30740;&#31350;&#30452;&#25509;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#22312;&#36755;&#20837;&#29305;&#24449;&#20013;&#24341;&#20837;&#36793;&#32536;&#20197;&#25429;&#25417;&#21644;&#32500;&#25252;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#26377;&#25928;&#30340;&#36817;&#20284;&#31574;&#30053;&#26469;&#37327;&#21270;&#39640;&#32500;&#25968;&#25454;&#30340;&#22240;&#26524;&#24402;&#22240;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#25509;&#36817;&#22522;&#26412;&#20107;&#23454;&#25928;&#26524;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#35813;&#31639;&#27861;&#22312;&#32593;&#32476;&#22270;&#20026;&#36830;&#36890;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.11789</link><description>&lt;p&gt;
&#22270;&#19978;&#38543;&#26426;&#36870;&#38382;&#39064;&#65306;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random Inverse Problems Over Graphs: Decentralized Online Learning. (arXiv:2303.11789v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#35813;&#31639;&#27861;&#22312;&#32593;&#32476;&#22270;&#20026;&#36830;&#36890;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#38543;&#26426;&#36870;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#35813;&#38382;&#39064;&#20855;&#26377;&#23454;&#26102;&#30340;&#22270;&#19978;&#35266;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22312;&#32447;&#25968;&#25454;&#27969;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#21442;&#25968;&#20272;&#35745;&#21644;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#22343;&#26041;&#38382;&#39064;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25910;&#25947;&#24615;&#36716;&#21270;&#20026;&#24102;&#26377;L2&#26377;&#30028;&#38789;&#24046;&#20998;&#39033;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#38543;&#26426;&#26102;&#21464;&#24046;&#20998;&#26041;&#31243;&#30340;&#28176;&#36817;&#31283;&#23450;&#24615;&#65292;&#24182;&#21457;&#23637;&#20102;L2-&#28176;&#36817;&#31283;&#23450;&#24615;&#29702;&#35770;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#32593;&#32476;&#22270;&#26159;&#36830;&#36890;&#30340;&#65292;&#24182;&#19988;&#27491;&#21521;&#31639;&#23376;&#24207;&#21015;&#28385;&#36275;&#26080;&#38480;&#32500;&#24230;&#26102;&#31354;&#21169;&#30913;&#26465;&#20214;&#65292;&#21017;&#25152;&#26377;&#33410;&#28857;&#30340;&#20272;&#35745;&#22343;&#20026;&#22343;&#26041;&#21644;&#20960;&#20046;&#24517;&#28982;&#30340;&#24378;&#19968;&#33268;&#30340;&#12290;&#36890;&#36807;&#23558;RKHS&#20013;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#38382;&#39064;&#31561;&#25928;&#22320;&#36716;&#21270;&#20026;&#22270;&#19978;&#38543;&#26426;&#36870;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#20013;&#24515;&#33410;&#28857;&#30340;RKHS&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into the asymptotic stability of randomly time-varying difference equations in Hilbert space with L2-bounded martingale difference terms and develop the L2 -asymptotic stability theory. It is shown that if the network graph is connected and the sequence of forward operators satisfies the infinitedimensional spatio-temporal persistence of excitation condition, then the estimates of all nodes are mean square and almost surely strongly consistent. By equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;GAN&#21644;VAE&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05699</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;GAN&#21644;VAE&#30340;&#29305;&#24449;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Feature Unlearning for Pre-trained GANs and VAEs. (arXiv:2303.05699v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;GAN&#21644;VAE&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;GAN&#21644;VAE&#65289;&#20013;&#28040;&#38500;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;&#19982;&#24120;&#35265;&#30340;&#28040;&#38500;&#20219;&#21153;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20174;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#20013;&#28040;&#38500;&#29305;&#23450;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#38754;&#37096;&#22270;&#20687;&#20013;&#30340;&#21457;&#22411;&#12290;&#30001;&#20110;&#30446;&#26631;&#29305;&#24449;&#20165;&#20986;&#29616;&#22312;&#22270;&#20687;&#30340;&#23616;&#37096;&#21306;&#22495;&#20013;&#65292;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#28040;&#38500;&#25972;&#20010;&#22270;&#20687;&#21487;&#33021;&#23548;&#33268;&#22833;&#21435;&#22270;&#20687;&#21097;&#20313;&#21306;&#22495;&#20013;&#30340;&#20854;&#20182;&#32454;&#33410;&#12290;&#20026;&#20102;&#25351;&#23450;&#35201;&#28040;&#38500;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#25910;&#38598;&#21253;&#21547;&#30446;&#26631;&#29305;&#24449;&#30340;&#38543;&#26426;&#29983;&#25104;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35782;&#21035;&#19982;&#30446;&#26631;&#29305;&#24449;&#23545;&#24212;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#34920;&#31034;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MNIST&#21644;CelebA&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25104;&#21151;&#21024;&#38500;&#30446;&#26631;&#29305;&#24449;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#36827;&#19968;&#27493;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#23454;&#39564;&#35777;&#26126;&#20102;&#28040;&#38500;&#21518;&#30340;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of feature unlearning from a pre-trained image generative model: GANs and VAEs. Unlike a common unlearning task where an unlearning target is a subset of the training set, we aim to unlearn a specific feature, such as hairstyle from facial images, from the pre-trained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pre-trained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we collect randomly generated images that contain the target features. We then identify a latent representation corresponding to the target feature and then use the representation to fine-tune the pre-trained model. Through experiments on MNIST and CelebA datasets, we show that target features are successfully removed while keeping the fidelity of the original models. Further experiments with an adversarial attack show that the unlearned model is mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;PDSketch&#35821;&#35328;&#21644;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#22312;&#32447;&#35268;&#21010;&#65292;&#21152;&#36895;&#20102;&#26426;&#22120;&#20154;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05501</link><description>&lt;p&gt;
PDSketch: &#38598;&#25104;&#35268;&#21010;&#39046;&#22495;&#32534;&#31243;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PDSketch: Integrated Planning Domain Programming and Learning. (arXiv:2303.05501v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;PDSketch&#35821;&#35328;&#21644;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#22312;&#32447;&#35268;&#21010;&#65292;&#21152;&#36895;&#20102;&#26426;&#22120;&#20154;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#27169;&#22411;&#23398;&#20064;&#21644;&#22312;&#32447;&#35268;&#21010;&#26041;&#27861;&#65292;&#20197;&#26500;&#24314;&#28789;&#27963;&#21644;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#24213;&#23618;&#29615;&#22659;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#23616;&#37096;&#24615;&#21644;&#31232;&#30095;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#36816;&#34892;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#23450;&#20041;&#35821;&#35328;&#65292;&#21517;&#20026;PDSketch&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#28789;&#27963;&#22320;&#23450;&#20041;&#36716;&#25442;&#27169;&#22411;&#20013;&#30340;&#39640;&#32423;&#32467;&#26500;&#65292;&#20363;&#22914;&#23545;&#35937;&#21644;&#29305;&#24449;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#31867;&#20284;&#20110;&#31243;&#24207;&#21592;&#20351;&#29992;TensorFlow&#25110;PyTorch&#25351;&#23450;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#22823;&#23567;&#21644;&#38544;&#34255;&#32500;&#24230;&#30340;&#26041;&#24335;&#12290;&#36716;&#25442;&#27169;&#22411;&#30340;&#32454;&#33410;&#23558;&#30001;&#21487;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22635;&#20805;&#12290;&#22522;&#20110;&#23450;&#20041;&#30340;&#32467;&#26500;&#21644;&#23398;&#20064;&#21442;&#25968;&#65292;PDSketch&#33258;&#21160;&#29983;&#25104;&#19982;&#22495;&#26080;&#20851;&#30340;&#35268;&#21010;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#34893;&#29983;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#21152;&#36895;&#20102;&#23545;&#26032;&#30446;&#26631;&#30340;&#35268;&#21010;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a model learning and online planning approach towards building flexible and general robots. Specifically, we investigate how to exploit the locality and sparsity structures in the underlying environmental transition model to improve model generalization, data-efficiency, and runtime-efficiency. We present a new domain definition language, named PDSketch. It allows users to flexibly define high-level structures in the transition models, such as object and feature dependencies, in a way similar to how programmers use TensorFlow or PyTorch to specify kernel sizes and hidden dimensions of a convolutional neural network. The details of the transition model will be filled in by trainable neural networks. Based on the defined structures and learned parameters, PDSketch automatically generates domain-independent planning heuristics without additional training. The derived heuristics accelerate the performance-time planning for novel goals.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36719;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#26576;&#20123;&#26426;&#22120;&#20154;&#37197;&#32622;&#19979;&#23384;&#22312;&#32570;&#22833;&#29366;&#24577;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#36801;&#31227;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#37197;&#32622;&#19979;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01693</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#23454;&#29616;&#36719;&#26426;&#22120;&#20154;&#30340;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Transfer Learning and State Inference for Soft Robots via a Semi-supervised Sequential Variational Bayes Framework. (arXiv:2303.01693v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36719;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#26576;&#20123;&#26426;&#22120;&#20154;&#37197;&#32622;&#19979;&#23384;&#22312;&#32570;&#22833;&#29366;&#24577;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#36801;&#31227;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#37197;&#32622;&#19979;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#36719;&#26426;&#22120;&#20154;&#24314;&#27169;&#21644;&#29366;&#24577;&#25512;&#26029;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#25165;&#33021;&#26377;&#25928;&#22320;&#36816;&#34892;&#65292;&#36825;&#38656;&#35201;&#36827;&#34892;&#35814;&#23613;&#21644;&#36136;&#37327;&#33391;&#22909;&#30340;&#25968;&#25454;&#37319;&#38598;&#65292;&#23588;&#20854;&#26159;&#29366;&#24577;&#26631;&#31614;&#30340;&#37319;&#38598;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#36719;&#26426;&#22120;&#20154;&#30340;&#20256;&#24863;&#22120;&#21270;&#22256;&#38590;&#21644;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#25454;&#30340;&#19981;&#20415;&#31561;&#21407;&#22240;&#65292;&#33719;&#21462;&#26631;&#27880;&#30340;&#36719;&#26426;&#22120;&#20154;&#31995;&#32479;&#29366;&#24577;&#25968;&#25454;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;DSVB&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#26576;&#20123;&#26426;&#22120;&#20154;&#37197;&#32622;&#20013;&#23384;&#22312;&#32570;&#22833;&#29366;&#24577;&#26631;&#31614;&#30340;&#36719;&#26426;&#22120;&#20154;&#30340;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;&#12290;&#32771;&#34385;&#21040;&#36719;&#26426;&#22120;&#20154;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#37197;&#32622;&#19979;&#21487;&#33021;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#36801;&#31227;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#22312;&#22810;&#20010;&#37197;&#32622;&#19979;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, data-driven models such as deep neural networks have shown to be promising tools for modelling and state inference in soft robots. However, voluminous amounts of data are necessary for deep models to perform effectively, which requires exhaustive and quality data collection, particularly of state labels. Consequently, obtaining labelled state data for soft robotic systems is challenged for various reasons, including difficulty in the sensorization of soft robots and the inconvenience of collecting data in unstructured environments. To address this challenge, in this paper, we propose a semi-supervised sequential variational Bayes (DSVB) framework for transfer learning and state inference in soft robots with missing state labels on certain robot configurations. Considering that soft robots may exhibit distinct dynamics under different robot configurations, a feature space transfer strategy is also incorporated to promote the adaptation of latent features across multiple config
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#25511;&#21046;&#33258;&#20027;&#26426;&#32676;&#65292;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#26377;&#25928;&#25511;&#21046;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#20986;&#34892;&#31995;&#32479;&#25968;&#25454;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#31163;&#32447;&#23398;&#20064;&#24674;&#22797;AMoD&#25511;&#21046;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.14833</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#25511;&#21046;&#33258;&#20027;&#26426;&#32676;
&lt;/p&gt;
&lt;p&gt;
Learning to Control Autonomous Fleets from Observation via Offline Reinforcement Learning. (arXiv:2302.14833v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#25511;&#21046;&#33258;&#20027;&#26426;&#32676;&#65292;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#26377;&#25928;&#25511;&#21046;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#20986;&#34892;&#31995;&#32479;&#25968;&#25454;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#31163;&#32447;&#23398;&#20064;&#24674;&#22797;AMoD&#25511;&#21046;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#65288;AMoD&#65289;&#31995;&#32479;&#26159;&#19968;&#31181;&#19981;&#26029;&#21457;&#23637;&#30340;&#20132;&#36890;&#26041;&#24335;&#65292;&#20854;&#20013;&#30001;&#20013;&#22830;&#21327;&#35843;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#32452;&#25104;&#30340;&#36710;&#38431;&#21160;&#24577;&#22320;&#25552;&#20379;&#20986;&#34892;&#26381;&#21153;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#25511;&#21046;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22823;&#35268;&#27169;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#38598;&#20013;&#24335;RL&#26041;&#27861;&#20851;&#27880;&#22312;&#32447;&#23398;&#20064;&#25968;&#25454;&#65292;&#24573;&#35270;&#20102;&#23454;&#38469;&#20132;&#36890;&#31995;&#32479;&#20013;&#27599;&#20010;&#26679;&#26412;&#20132;&#20114;&#30340;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35282;&#26469;&#24418;&#24335;&#21270;AMoD&#31995;&#32479;&#30340;&#25511;&#21046;&#65292;&#24182;&#20165;&#20351;&#29992;&#21487;&#29992;&#20110;&#24403;&#21069;&#20986;&#34892;&#36816;&#33829;&#21830;&#30340;&#31163;&#32447;&#25968;&#25454;&#23398;&#20064;&#26377;&#25928;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#22522;&#20110;&#30495;&#23454;&#20986;&#34892;&#31995;&#32479;&#30340;&#25968;&#25454;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#65292;&#23637;&#31034;&#20102;&#31163;&#32447;&#23398;&#20064;&#22914;&#20309;&#24674;&#22797;AMoD&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous Mobility-on-Demand (AMoD) systems are an evolving mode of transportation in which a centrally coordinated fleet of self-driving vehicles dynamically serves travel requests. The control of these systems is typically formulated as a large network optimization problem, and reinforcement learning (RL) has recently emerged as a promising approach to solve the open challenges in this space. Recent centralized RL approaches focus on learning from online data, ignoring the per-sample-cost of interactions within real-world transportation systems. To address these limitations, we propose to formalize the control of AMoD systems through the lens of offline reinforcement learning and learn effective control strategies using solely offline data, which is readily available to current mobility operators. We further investigate design decisions and provide empirical evidence based on data from real-world mobility systems showing how offline learning allows to recover AMoD control policies t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#19982;&#27969;&#34892;&#30340;TreeSHAP&#31639;&#27861;&#30456;&#27604;&#65292;&#36793;&#38469;Shapley&#20540;&#22312;&#30456;&#21516;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#26641;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#35745;&#31639;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#12290;</title><link>http://arxiv.org/abs/2302.08434</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On marginal feature attributions of tree-based models. (arXiv:2302.08434v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#19982;&#27969;&#34892;&#30340;TreeSHAP&#31639;&#27861;&#30456;&#27604;&#65292;&#36793;&#38469;Shapley&#20540;&#22312;&#30456;&#21516;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#19988;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#26641;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#35745;&#31639;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24378;&#22823;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#29305;&#28857;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#26641;&#38598;&#25104;&#31561;&#22522;&#20110;&#26641;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#36793;&#38469;&#26399;&#26395;&#30340;&#23616;&#37096;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#20363;&#22914;&#36793;&#38469;&#65288;&#24178;&#39044;&#65289;Shapley&#12289;Owen&#25110;Banzhaf&#20540;&#12290;&#36825;&#20123;&#26041;&#27861;&#23545;&#27169;&#22411;&#30495;&#23454;&#19988;&#23454;&#29616;&#19981;&#21464;&#65292;&#21363;&#20165;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#36755;&#20837;&#36755;&#20986;&#20989;&#25968;&#12290;&#36890;&#36807;&#25552;&#20379;&#20004;&#20010;&#65288;&#20855;&#26377;&#30456;&#20284;&#32479;&#35745;&#24615;&#36136;&#30340;&#65289;&#20915;&#31574;&#26641;&#26469;&#23545;&#27604;&#36825;&#19968;&#28857;&#65292;&#36825;&#20004;&#20010;&#20915;&#31574;&#26641;&#35745;&#31639;&#23436;&#20840;&#30456;&#21516;&#30340;&#20989;&#25968;&#65292;&#20294;&#8220;&#36335;&#24452;&#30456;&#20851;&#8221;&#30340;TreeSHAP&#26041;&#27861;&#32473;&#20986;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#25490;&#24207;&#65292;&#32780;&#36793;&#38469;Shapley&#20540;&#37325;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#20869;&#37096;&#32467;&#26500;&#26469;&#24110;&#21161;&#35745;&#31639;&#23427;&#20204;&#30340;&#36793;&#38469;&#29305;&#24449;&#24402;&#22240;&#65292;&#20197;&#24471;&#21040;&#32447;&#24615;&#21338;&#24328;&#20540;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#35266;&#23519;&#26159;&#65292;&#36825;&#20123;&#20989;&#25968;&#22312;&#26576;&#20010;&#24120;&#25968;&#21306;&#38388;&#20869;&#26159;&#31616;&#21333;&#30340;&#65288;&#20998;&#27573;&#24120;&#25968;&#65289;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their power and ease of use, tree-based machine learning models, such as random forests and gradient-boosted tree ensembles, have become very popular. To interpret them, local feature attributions based on marginal expectations, e.g. marginal (interventional) Shapley, Owen or Banzhaf values, may be employed. Such methods are true to the model and implementation invariant, i.e. dependent only on the input-output function of the model. We contrast this with the popular TreeSHAP algorithm by presenting two (statistically similar) decision trees that compute the exact same function for which the "path-dependent" TreeSHAP yields different rankings of features, whereas the marginal Shapley values coincide. Furthermore, we discuss how the internal structure of tree-based models may be leveraged to help with computing their marginal feature attributions according to a linear game value. One important observation is that these are simple (piecewise-constant) functions with respect to a c
&lt;/p&gt;</description></item><item><title>LExecutor&#26159;&#19968;&#20010;&#23398;&#20064;&#24341;&#23548;&#30340;&#25191;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31070;&#32463;&#27169;&#22411;&#39044;&#27979;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#21040;&#25191;&#34892;&#20013;&#65292;&#21487;&#20197;&#20197;&#33258;&#30001;&#32422;&#26463;&#30340;&#26041;&#24335;&#25191;&#34892;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#12290;&#35813;&#26041;&#27861;&#22312;Python&#20195;&#30721;&#21644;&#20174;Stack Overflow&#25552;&#21462;&#30340;&#20195;&#30721;&#29255;&#27573;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.02343</link><description>&lt;p&gt;
LExecutor: &#23398;&#20064;&#24341;&#23548;&#30340;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
LExecutor: Learning-Guided Execution. (arXiv:2302.02343v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02343
&lt;/p&gt;
&lt;p&gt;
LExecutor&#26159;&#19968;&#20010;&#23398;&#20064;&#24341;&#23548;&#30340;&#25191;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#35753;&#31070;&#32463;&#27169;&#22411;&#39044;&#27979;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#21040;&#25191;&#34892;&#20013;&#65292;&#21487;&#20197;&#20197;&#33258;&#30001;&#32422;&#26463;&#30340;&#26041;&#24335;&#25191;&#34892;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#12290;&#35813;&#26041;&#27861;&#22312;Python&#20195;&#30721;&#21644;&#20174;Stack Overflow&#25552;&#21462;&#30340;&#20195;&#30721;&#29255;&#27573;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25191;&#34892;&#20195;&#30721;&#23545;&#20110;&#21508;&#31181;&#31243;&#24207;&#20998;&#26512;&#20219;&#21153;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20363;&#22914;&#36890;&#36807;&#24322;&#24120;&#26816;&#27979;&#38169;&#35823;&#25110;&#33719;&#21462;&#25191;&#34892;&#36319;&#36394;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#21160;&#24577;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#25191;&#34892;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#36890;&#24120;&#24456;&#22256;&#38590;&#65292;&#20363;&#22914;&#30001;&#20110;&#32570;&#23569;&#21464;&#37327;&#23450;&#20041;&#12289;&#32570;&#23569;&#29992;&#25143;&#36755;&#20837;&#21644;&#32570;&#23569;&#31532;&#19977;&#26041;&#20381;&#36182;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LExecutor&#65292;&#19968;&#31181;&#23398;&#20064;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#33258;&#30001;&#32422;&#26463;&#30340;&#26041;&#24335;&#25191;&#34892;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35753;&#31070;&#32463;&#27169;&#22411;&#39044;&#27979;&#21542;&#21017;&#20250;&#23548;&#33268;&#31243;&#24207;&#20572;&#28382;&#30340;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#36825;&#20123;&#20540;&#27880;&#20837;&#21040;&#25191;&#34892;&#20013;&#12290;&#20363;&#22914;&#65292;LExecutor&#20026;&#26410;&#23450;&#20041;&#30340;&#21464;&#37327;&#27880;&#20837;&#21487;&#33021;&#30340;&#20540;&#65292;&#24182;&#20026;&#32570;&#22833;&#30340;&#20989;&#25968;&#35843;&#29992;&#36820;&#22238;&#39044;&#27979;&#21487;&#33021;&#30340;&#36820;&#22238;&#20540;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#27969;&#34892;&#24320;&#28304;&#39033;&#30446;&#30340;Python&#20195;&#30721;&#21644;&#20174;Stack Overflow&#20013;&#25552;&#21462;&#30340;&#20195;&#30721;&#29255;&#27573;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#12290;&#31070;&#32463;&#27169;&#22411;&#20197;79.5%&#21040;98.2%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#30495;&#23454;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Executing code is essential for various program analysis tasks, e.g., to detect bugs that manifest through exceptions or to obtain execution traces for further dynamic analysis. However, executing an arbitrary piece of code is often difficult in practice, e.g., because of missing variable definitions, missing user inputs, and missing third-party dependencies. This paper presents LExecutor, a learning-guided approach for executing arbitrary code snippets in an underconstrained way. The key idea is to let a neural model predict missing values that otherwise would cause the program to get stuck, and to inject these values into the execution. For example, LExecutor injects likely values for otherwise undefined variables and likely return values of calls to otherwise missing functions. We evaluate the approach on Python code from popular open-source projects and on code snippets extracted from Stack Overflow. The neural model predicts realistic values with an accuracy between 79.5% and 98.2
&lt;/p&gt;</description></item><item><title>NeuRI&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#21270;&#29983;&#25104;&#30001;&#25968;&#30334;&#31181;&#25805;&#20316;&#31526;&#32452;&#25104;&#30340;&#26377;&#25928;&#19988;&#22810;&#26679;&#21270;&#30340;DL&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24402;&#32435;&#24335;&#31243;&#24207;&#21512;&#25104;&#25512;&#26029;&#25805;&#20316;&#31526;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#37319;&#29992;&#31526;&#21495;&#21644;&#20855;&#20307;&#25805;&#20316;&#30340;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.02261</link><description>&lt;p&gt;
NeuRI&#65306;&#36890;&#36807;&#24402;&#32435;&#35268;&#21017;&#25512;&#26029;&#23454;&#29616;DNN&#29983;&#25104;&#30340;&#22810;&#26679;&#21270;
&lt;/p&gt;
&lt;p&gt;
NeuRI: Diversifying DNN Generation via Inductive Rule Inference. (arXiv:2302.02261v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02261
&lt;/p&gt;
&lt;p&gt;
NeuRI&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#21270;&#29983;&#25104;&#30001;&#25968;&#30334;&#31181;&#25805;&#20316;&#31526;&#32452;&#25104;&#30340;&#26377;&#25928;&#19988;&#22810;&#26679;&#21270;&#30340;DL&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24402;&#32435;&#24335;&#31243;&#24207;&#21512;&#25104;&#25512;&#26029;&#25805;&#20316;&#31526;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#37319;&#29992;&#31526;&#21495;&#21644;&#20855;&#20307;&#25805;&#20316;&#30340;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;(DL)&#22312;&#21508;&#20010;&#34892;&#19994;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25913;&#21892;&#20915;&#31574;&#21644;&#33258;&#21160;&#21270;&#27969;&#31243;&#65292;&#20854;&#25512;&#21160;&#21147;&#26469;&#33258;&#19981;&#26029;&#21457;&#23637;&#30340;DL&#24211;&#21644;&#32534;&#35793;&#22120;&#12290;DL&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#23545;&#20110;&#20449;&#20219;DL&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#28010;&#28526;&#19968;&#30452;&#22312;&#30740;&#31350;&#29992;&#20110;&#27169;&#31946;DL&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#29992;&#20363;&#21512;&#25104;&#65288;&#21363;DNN&#27169;&#22411;&#21644;&#20854;&#36755;&#20837;&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#29983;&#25104;&#22120;&#21482;&#28085;&#30422;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#25805;&#20316;&#31526;&#65292;&#32570;&#20047;&#24191;&#27867;&#24314;&#27169;&#25805;&#20316;&#31526;&#32422;&#26463;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuRI&#65292;&#19968;&#31181;&#20840;&#33258;&#21160;&#29983;&#25104;&#30001;&#25968;&#30334;&#31181;&#25805;&#20316;&#31526;&#32452;&#25104;&#30340;&#26377;&#25928;&#19988;&#22810;&#26679;&#21270;&#30340;DL&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;NeuRI&#37319;&#29992;&#20102;&#19977;&#27493;&#36807;&#31243;&#65306;(i)&#20174;&#21508;&#31181;&#26469;&#28304;&#25910;&#38598;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;API&#36861;&#36394;&#65307;(ii)&#22312;&#36861;&#36394;&#25968;&#25454;&#19978;&#24212;&#29992;&#24402;&#32435;&#24335;&#31243;&#24207;&#21512;&#25104;&#65292;&#25512;&#26029;&#26500;&#24314;&#26377;&#25928;&#27169;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#65307;(iii)&#36890;&#36807;&#32467;&#21512;&#31526;&#21495;&#21644;&#20855;&#20307;&#25805;&#20316;&#25191;&#34892;&#28151;&#21512;&#27169;&#22411;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications. As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints. To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process: (i) collecting valid and invalid API traces from various sources; (ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and (iii) performing hybrid model generation by incorporating both symbolic and concrete ope
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#21270;&#30340;&#22330;&#26223;&#25551;&#36848;&#21644;&#36816;&#21160;&#39044;&#27979;&#30340;&#22330;&#26223;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#24182;&#19981;&#21516;&#22330;&#26223;&#30340;&#25968;&#25454;&#24182;&#21033;&#29992;&#21521;&#37327;&#21270;&#22330;&#26223;&#25551;&#36848;&#20013;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#32454;&#24494;&#24046;&#21035;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#33258;&#21160;&#21270;&#36710;&#36742;&#22312;&#26410;&#35265;&#36807;&#22330;&#26223;&#19979;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2302.01161</link><description>&lt;p&gt;
&#22522;&#20110;&#21521;&#37327;&#21270;&#30340;&#22330;&#26223;&#25551;&#36848;&#21644;&#36816;&#21160;&#39044;&#27979;&#30340;&#22330;&#26223;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Vectorized Scenario Description and Motion Prediction for Scenario-Based Testing. (arXiv:2302.01161v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#21270;&#30340;&#22330;&#26223;&#25551;&#36848;&#21644;&#36816;&#21160;&#39044;&#27979;&#30340;&#22330;&#26223;&#27979;&#35797;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#24182;&#19981;&#21516;&#22330;&#26223;&#30340;&#25968;&#25454;&#24182;&#21033;&#29992;&#21521;&#37327;&#21270;&#22330;&#26223;&#25551;&#36848;&#20013;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#32454;&#24494;&#24046;&#21035;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#33258;&#21160;&#21270;&#36710;&#36742;&#22312;&#26410;&#35265;&#36807;&#22330;&#26223;&#19979;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#36710;&#36742;(AVs)&#36890;&#24120;&#22312;&#22810;&#26679;&#21270;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#27979;&#35797;&#65292;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#30001;&#36895;&#24230;&#12289;&#36317;&#31163;&#25110;&#26354;&#32447;&#21322;&#24452;&#31561;&#21442;&#25968;&#26469;&#30830;&#23450;&#12290;&#20026;&#20102;&#33021;&#22815;&#32479;&#19968;&#25551;&#36848;&#36825;&#20123;&#22330;&#26223;&#65292;&#29420;&#31435;&#20110;&#36825;&#20123;&#21442;&#25968;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36947;&#36335;&#20960;&#20309;&#21644;&#36710;&#36742;&#36712;&#36857;&#30340;&#21521;&#37327;&#21270;&#22330;&#26223;&#25551;&#36848;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#31181;&#24418;&#24335;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19977;&#20010;&#22330;&#26223;&#30340;&#25968;&#25454;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#36215;&#26469;&#29992;&#20110;&#35757;&#32451;&#36816;&#21160;&#39044;&#27979;&#27169;&#22411;VectorNet&#65292;&#20174;&#32780;&#21487;&#20197;&#39044;&#27979;&#26410;&#35265;&#36807;&#22330;&#26223;&#19979;AV&#30340;&#36712;&#36857;&#12290;&#36890;&#36807;&#39044;&#27979;&#22330;&#26223;&#35780;&#20272;&#25351;&#26631;&#65292;VectorNet&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#27604;&#21333;&#29420;&#22788;&#29702;&#19977;&#20010;&#22330;&#26223;&#25968;&#25454;&#30340;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20840;&#38754;&#30340;&#27867;&#21270;&#65292;&#24517;&#39035;&#30830;&#20445;&#35757;&#32451;&#25968;&#25454;&#20013;&#20855;&#26377;&#36275;&#22815;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#21512;&#24182;&#19981;&#21516;&#22330;&#26223;&#30340;&#25968;&#25454;&#24182;&#21033;&#29992;&#21521;&#37327;&#21270;&#22330;&#26223;&#25551;&#36848;&#20013;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#32454;&#24494;&#24046;&#21035;&#12290;&#36825;&#26679;&#19968;&#26469;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#25351;&#23450;&#30340;&#27979;&#35797;&#22330;&#26223;&#21644;&#30495;&#23454;&#22330;&#26223;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated vehicles (AVs) are tested in diverse scenarios, typically specified by parameters such as velocities, distances, or curve radii. To describe scenarios uniformly independent of such parameters, this paper proposes a vectorized scenario description defined by the road geometry and vehicles' trajectories. Data of this form are generated for three scenarios, merged, and used to train the motion prediction model VectorNet, allowing to predict an AV's trajectory for unseen scenarios. Predicting scenario evaluation metrics, VectorNet partially achieves lower errors than regression models that separately process the three scenarios' data. However, for comprehensive generalization, sufficient variance in the training data must be ensured. Thus, contrary to existing methods, our proposed method can merge diverse scenarios' data and exploit spatial and temporal nuances in the vectorized scenario description. As a result, data from specified test scenarios and real-world scenarios can be
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#31574;&#30053;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19982;&#20043;&#30456;&#20851;&#19988;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#19981;&#31283;&#23450;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.00270</link><description>&lt;p&gt;
&#20869;&#37096;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Internally Rewarded Reinforcement Learning. (arXiv:2302.00270v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00270
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#31574;&#30053;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19982;&#20043;&#30456;&#20851;&#19988;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#19981;&#31283;&#23450;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#29992;&#20110;&#31574;&#30053;&#23398;&#20064;&#30340;&#22870;&#21169;&#20449;&#21495;&#30001;&#19968;&#20010;&#19982;&#31574;&#30053;&#30456;&#20851;&#19988;&#19982;&#31574;&#30053;&#21516;&#26102;&#20248;&#21270;&#30340;&#21028;&#21035;&#22120;&#29983;&#25104;&#12290;&#31574;&#30053;&#21644;&#21028;&#21035;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#23548;&#33268;&#20102;&#19981;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#22240;&#20026;&#26469;&#33258;&#19981;&#25104;&#29087;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20449;&#21495;&#26159;&#22024;&#26434;&#30340;&#65292;&#38459;&#30861;&#20102;&#31574;&#30053;&#30340;&#23398;&#20064;&#65307;&#21453;&#36807;&#26469;&#65292;&#26410;&#32463;&#20248;&#21270;&#30340;&#31574;&#30053;&#20063;&#20250;&#38459;&#30861;&#21028;&#21035;&#22120;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23398;&#20064;&#35774;&#32622;&#31216;&#20026;&#8220;&#20869;&#37096;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#8221;&#65288;IRRL&#65289;&#65292;&#22240;&#20026;&#22870;&#21169;&#19981;&#26159;&#30452;&#25509;&#26469;&#33258;&#29615;&#22659;&#65292;&#32780;&#26159;&#30001;&#21028;&#21035;&#22120;&#8220;&#20869;&#37096;&#8221;&#25552;&#20379;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#22320;&#34920;&#36848;&#20102;IRRL&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31867;&#23646;&#20110;IRRL&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#24182;&#32463;&#39564;&#24615;&#22320;&#20998;&#26512;&#20102;IRRL&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#25552;&#20986;&#20102;&#20462;&#21098;&#32447;&#24615;&#22870;&#21169;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#25345;&#32493;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a class of reinforcement learning problems where the reward signals for policy learning are generated by a discriminator that is dependent on and jointly optimized with the policy. This interdependence between the policy and the discriminator leads to an unstable learning process because reward signals from an immature discriminator are noisy and impede policy learning, and conversely, an under-optimized policy impedes discriminator learning. We call this learning setting \textit{Internally Rewarded Reinforcement Learning} (IRRL) as the reward is not provided directly by the environment but \textit{internally} by the discriminator. In this paper, we formally formulate IRRL and present a class of problems that belong to IRRL. We theoretically derive and empirically analyze the effect of the reward function in IRRL and based on these analyses propose the clipped linear reward function. Experimental results show that the proposed reward function can consistently stabilize the tra
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#38472;&#36848;&#36873;&#25321;&#35843;&#26597;&#65292;&#35843;&#26597;&#20102;&#30005;&#23376;&#21830;&#21153;&#29992;&#25143;&#23545;&#20132;&#20184;&#36873;&#25321;&#30340;&#20559;&#22909;&#65292;&#21457;&#29616;&#20132;&#20184;&#26041;&#24335;&#30340;&#36153;&#29992;&#12289;&#26102;&#38388;&#21644;&#26102;&#38388;&#27573;&#22823;&#23567;&#26159;&#29992;&#25143;&#36873;&#25321;&#30340;&#37325;&#35201;&#20915;&#23450;&#22240;&#32032;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#29992;&#25143;&#30340;&#20559;&#22909;&#19982;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#36828;&#31243;&#21150;&#20844;&#39057;&#29575;&#21644;&#26159;&#21542;&#26377;&#24555;&#36882;&#26588;&#31561;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2301.00666</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#29992;&#25143;&#23545;&#20132;&#20184;&#36873;&#25321;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
E-commerce users' preferences for delivery options. (arXiv:2301.00666v2 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00666
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#38472;&#36848;&#36873;&#25321;&#35843;&#26597;&#65292;&#35843;&#26597;&#20102;&#30005;&#23376;&#21830;&#21153;&#29992;&#25143;&#23545;&#20132;&#20184;&#36873;&#25321;&#30340;&#20559;&#22909;&#65292;&#21457;&#29616;&#20132;&#20184;&#26041;&#24335;&#30340;&#36153;&#29992;&#12289;&#26102;&#38388;&#21644;&#26102;&#38388;&#27573;&#22823;&#23567;&#26159;&#29992;&#25143;&#36873;&#25321;&#30340;&#37325;&#35201;&#20915;&#23450;&#22240;&#32032;&#65292;&#21516;&#26102;&#20063;&#21457;&#29616;&#29992;&#25143;&#30340;&#20559;&#22909;&#19982;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#36828;&#31243;&#21150;&#20844;&#39057;&#29575;&#21644;&#26159;&#21542;&#26377;&#24555;&#36882;&#26588;&#31561;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24066;&#22330;&#20026;&#20102;&#28385;&#36275;&#29992;&#25143;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#65292;&#25552;&#20379;&#20813;&#36153;&#30340;&#24555;&#36895;&#20132;&#20184;&#36873;&#39033;&#65292;&#36825;&#32473;&#22478;&#24066;&#29289;&#27969;&#24102;&#26469;&#20102;&#36807;&#37325;&#30340;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#30005;&#23376;&#21830;&#21153;&#29992;&#25143;&#23545;&#20132;&#20184;&#36873;&#25321;&#30340;&#20559;&#22909;&#26159;&#35774;&#35745;&#29289;&#27969;&#25919;&#31574;&#30340;&#20851;&#38190;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#39033;&#38472;&#36848;&#36873;&#25321;&#35843;&#26597;&#65292;&#21463;&#35775;&#32773;&#38754;&#20020;&#19981;&#21516;&#20132;&#20184;&#36873;&#25321;&#21644;&#26102;&#38388;&#27573;&#20043;&#38388;&#30340;&#36873;&#25321;&#20219;&#21153;&#65292;&#22312;&#26085;&#26412;&#19977;&#20010;&#20027;&#35201;&#37117;&#24066;&#22320;&#21306;&#30340;4062&#21517;&#29992;&#25143;&#23436;&#25104;&#20102;&#35843;&#26597;&#12290;&#20026;&#20102;&#20998;&#26512;&#25968;&#25454;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#25429;&#25417;&#21697;&#21619;&#24322;&#36136;&#24615;&#20197;&#21450;&#28789;&#27963;&#26367;&#20195;&#27169;&#24335;&#30340;&#28151;&#21512;&#36923;&#36753;&#27169;&#22411;&#12290;&#27169;&#22411;&#20272;&#35745;&#32467;&#26524;&#34920;&#26126;&#65292;&#21253;&#25324;&#36153;&#29992;&#12289;&#26102;&#38388;&#21644;&#26102;&#38388;&#27573;&#22823;&#23567;&#22312;&#20869;&#30340;&#20132;&#20184;&#23646;&#24615;&#26159;&#20132;&#20184;&#36873;&#25321;&#30340;&#37325;&#35201;&#20915;&#23450;&#22240;&#32032;&#12290;&#36824;&#25552;&#20986;&#20102;&#29992;&#25143;&#20559;&#22909;&#19982;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#36828;&#31243;&#21150;&#20844;&#39057;&#29575;&#21644;&#26159;&#21542;&#26377;&#24555;&#36882;&#26588;&#31561;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many e-commerce marketplaces offer their users fast delivery options for free to meet the increasing needs of users, imposing an excessive burden on city logistics. Therefore, understanding e-commerce users' preference for delivery options is a key to designing logistics policies. To this end, this study designs a stated choice survey in which respondents are faced with choice tasks among different delivery options and time slots, which was completed by 4,062 users from the three major metropolitan areas in Japan. To analyze the data, mixed logit models capturing taste heterogeneity as well as flexible substitution patterns have been estimated. The model estimation results indicate that delivery attributes including fee, time, and time slot size are significant determinants of the delivery option choices. Associations between users' preferences and socio-demographic characteristics, such as age, gender, teleworking frequency and the presence of a delivery box, were also suggested. More
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#26426;&#22120;&#20154;&#20013;&#35270;&#22495;&#22806;&#25968;&#25454;&#30340;&#31995;&#32479;&#23618;&#38754;&#35266;&#28857;&#65292;&#24378;&#35843;&#32771;&#34385;&#26426;&#22120;&#20154;&#22312;OOD&#26465;&#20214;&#19979;&#30340;&#25972;&#20307;&#31995;&#32479;&#23618;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20026;&#26410;&#26469;&#23433;&#20840;&#21487;&#38752;&#30340;&#23398;&#20064;&#21551;&#29992;&#33258;&#20027;&#24615;&#30740;&#31350;&#25552;&#20986;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.14020</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20013;&#35270;&#22495;&#22806;&#25968;&#25454;&#30340;&#31995;&#32479;&#23618;&#38754;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
A System-Level View on Out-of-Distribution Data in Robotics. (arXiv:2212.14020v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#26426;&#22120;&#20154;&#20013;&#35270;&#22495;&#22806;&#25968;&#25454;&#30340;&#31995;&#32479;&#23618;&#38754;&#35266;&#28857;&#65292;&#24378;&#35843;&#32771;&#34385;&#26426;&#22120;&#20154;&#22312;OOD&#26465;&#20214;&#19979;&#30340;&#25972;&#20307;&#31995;&#32479;&#23618;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#20026;&#26410;&#26469;&#23433;&#20840;&#21487;&#38752;&#30340;&#23398;&#20064;&#21551;&#29992;&#33258;&#20027;&#24615;&#30740;&#31350;&#25552;&#20986;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#27979;&#35797;&#26465;&#20214;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#19968;&#33268;&#26102;&#65292;&#25152;&#35859;&#30340;&#35270;&#22495;&#22806;&#65288;OOD&#65289;&#36755;&#20837;&#21487;&#33021;&#24433;&#21709;&#29616;&#20195;&#26426;&#22120;&#20154;&#33258;&#20027;&#22534;&#26632;&#20013;&#30340;&#23398;&#20064;&#32452;&#20214;&#30340;&#21487;&#38752;&#24615;&#12290;&#22240;&#27492;&#65292;&#24212;&#23545;OOD&#25968;&#25454;&#26159;&#20449;&#20219;&#23398;&#20064;&#21551;&#29992;&#30340;&#24320;&#25918;&#19990;&#30028;&#33258;&#20027;&#24615;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#31034;&#19982;&#25968;&#25454;&#39537;&#21160;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;OOD&#25968;&#25454;&#21450;&#20854;&#30456;&#20851;&#25361;&#25112;&#26377;&#20851;&#30340;&#20027;&#39064;&#65292;&#24182;&#23558;&#20854;&#19982;ML&#31038;&#21306;&#20013;&#20851;&#20110;OOD&#25968;&#25454;&#23545;&#23396;&#31435;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#30740;&#31350;&#30340;&#26032;&#20852;&#33539;&#24335;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20316;&#20026;&#26426;&#22120;&#20154;&#23398;&#23478;&#65292;&#25105;&#20204;&#24212;&#35813;&#32771;&#34385;&#26426;&#22120;&#20154;&#22312;OOD&#26465;&#20214;&#19979;&#30340;&#25972;&#20307;&#31995;&#32479;&#23618;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24378;&#35843;&#22260;&#32469;OOD&#38382;&#39064;&#30340;&#31995;&#32479;&#23618;&#38754;&#35266;&#28857;&#30340;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#23433;&#20840;&#21487;&#38752;&#30340;&#23398;&#20064;&#21551;&#29992;&#33258;&#20027;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
When testing conditions differ from those represented in training data, so-called out-of-distribution (OOD) inputs can mar the reliability of learned components in the modern robot autonomy stack. Therefore, coping with OOD data is an important challenge on the path towards trustworthy learning-enabled open-world autonomy. In this paper, we aim to demystify the topic of OOD data and its associated challenges in the context of data-driven robotic systems, drawing connections to emerging paradigms in the ML community that study the effect of OOD data on learned models in isolation. We argue that as roboticists, we should reason about the overall \textit{system-level} competence of a robot as it operates in OOD conditions. We highlight key research questions around this system-level view of OOD problems to guide future research toward safe and reliable learning-enabled autonomy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#35780;&#20272;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#25968;&#25454;&#20013;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#27979;&#24615;&#33021;&#20272;&#35745;&#37327;&#30340;&#36793;&#30028;&#36827;&#34892;&#21435;&#20559;&#20506;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#39044;&#27979;&#31639;&#27861;&#22312;&#36873;&#25321;&#24615;&#35266;&#23519;&#24773;&#22659;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.09844</link><description>&lt;p&gt;
&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#19979;&#65292;&#39044;&#27979;&#31639;&#27861;&#30340;&#40065;&#26834;&#35774;&#35745;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robust Design and Evaluation of Predictive Algorithms under Unobserved Confounding. (arXiv:2212.09844v4 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#35780;&#20272;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#25968;&#25454;&#20013;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#27979;&#24615;&#33021;&#20272;&#35745;&#37327;&#30340;&#36793;&#30028;&#36827;&#34892;&#21435;&#20559;&#20506;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#39044;&#27979;&#31639;&#27861;&#22312;&#36873;&#25321;&#24615;&#35266;&#23519;&#24773;&#22659;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#31639;&#27861;&#22312;&#20154;&#31867;&#20915;&#31574;&#32773;&#20316;&#20986;&#36873;&#25321;&#21518;&#36873;&#25321;&#24615;&#22320;&#35266;&#23519;&#21040;&#32467;&#26524;&#30340;&#24773;&#22659;&#20013;&#36827;&#34892;&#37325;&#35201;&#20915;&#31574;&#12290;&#36890;&#24120;&#23384;&#22312;&#30528;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#20915;&#31574;&#32773;&#30340;&#36873;&#25321;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#25968;&#25454;&#19979;&#23545;&#39044;&#27979;&#31639;&#27861;&#36827;&#34892;&#40065;&#26834;&#35774;&#35745;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#39044;&#27979;&#31639;&#27861;&#30340;&#26465;&#20214;&#24179;&#22343;&#32467;&#26524;&#22312;&#24050;&#35266;&#23519;&#21040;&#30340;&#21327;&#21464;&#37327;&#21644;&#24050;&#35782;&#21035;&#30340;&#24178;&#25200;&#21442;&#25968;&#19978;&#21487;&#33021;&#30340;&#21464;&#21270;&#31243;&#24230;&#25552;&#20986;&#20102;&#19968;&#33324;&#24615;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#24418;&#24335;&#21270;&#20102;&#29992;&#20110;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#30340;&#24120;&#29992;&#23454;&#35777;&#31574;&#30053;&#65292;&#22914;&#20195;&#29702;&#32467;&#26524;&#21644;&#24037;&#20855;&#21464;&#37327;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#21435;&#20559;&#20506;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#23545;&#22823;&#31867;&#39044;&#27979;&#24615;&#33021;&#20272;&#35745;&#37327;&#30340;&#36793;&#30028;&#65292;&#20363;&#22914;&#32467;&#26524;&#30340;&#26465;&#20214;&#20284;&#28982;&#12289;&#39044;&#27979;&#31639;&#27861;&#30340;&#22343;&#26041;&#35823;&#24046;&#12289;&#30495;/&#20551;&#38451;&#24615;&#29575;&#31561;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive algorithms inform consequential decisions in settings where the outcome is selectively observed given some choices made by human decision makers. There often exists unobserved confounders that affected the decision maker's choice and the outcome. We propose a unified methodology for the robust design and evaluation of predictive algorithms in selectively observed data under such unobserved confounding. Our approach imposes general assumptions on how much the outcome may vary on average between unselected and selected units conditional on observed covariates and identified nuisance parameters, formalizing popular empirical strategies for imputing missing data such as proxy outcomes and instrumental variables. We develop debiased machine learning estimators for the bounds on a large class of predictive performance estimands, such as the conditional likelihood of the outcome, a predictive algorithm's mean square error, true/false positive rate, and many others, under these assu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#23558;&#39044;&#35757;&#32451;&#32593;&#32476;&#25972;&#21512;&#21040;&#30446;&#26631;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#65292;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#31574;&#30053;&#25552;&#21462;&#26377;&#29992;&#30340;&#30446;&#26631;&#39046;&#22495;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#28304;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#31574;&#30053;&#33021;&#22815;&#25104;&#21151;&#22320;&#25552;&#39640;&#33258;&#36866;&#24212;&#24615;&#33021;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2212.07585</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39044;&#35757;&#32451;&#32593;&#32476;&#22312;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Role of Pre-Trained Networks in Source-Free Domain Adaptation. (arXiv:2212.07585v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#23558;&#39044;&#35757;&#32451;&#32593;&#32476;&#25972;&#21512;&#21040;&#30446;&#26631;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#65292;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#31574;&#30053;&#25552;&#21462;&#26377;&#29992;&#30340;&#30446;&#26631;&#39046;&#22495;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#28304;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#31574;&#30053;&#33021;&#22815;&#25104;&#21151;&#22320;&#25552;&#39640;&#33258;&#36866;&#24212;&#24615;&#33021;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#26088;&#22312;&#23558;&#22312;&#23436;&#20840;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#36866;&#24212;&#21040;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#22312;&#28304;&#35757;&#32451;&#26399;&#38388;&#65292;&#20351;&#29992;&#22823;&#25968;&#25454;&#39044;&#35757;&#32451;&#32593;&#32476;&#26469;&#21021;&#22987;&#21270;&#28304;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#20854;&#20002;&#24323;&#12290;&#28982;&#32780;&#65292;&#28304;&#35757;&#32451;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#28304;&#25968;&#25454;&#20998;&#24067;&#19978;&#36807;&#25311;&#21512;&#65292;&#24182;&#20007;&#22833;&#36866;&#29992;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#35758;&#23558;&#39044;&#35757;&#32451;&#32593;&#32476;&#25972;&#21512;&#21040;&#30446;&#26631;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#37325;&#35201;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#22810;&#26679;&#21270;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#19982;&#28304;&#27169;&#22411;&#19981;&#21516;&#30340;&#29305;&#24449;&#21644;&#20998;&#31867;&#20915;&#31574;&#30340;&#21478;&#19968;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#31574;&#30053;&#25552;&#28860;&#26377;&#29992;&#30340;&#30446;&#26631;&#39046;&#22495;&#20449;&#24687;&#65292;&#20197;&#25913;&#21892;&#24494;&#35843;&#28304;&#27169;&#22411;&#30340;&#30446;&#26631;&#20266;&#26631;&#31614;&#36136;&#37327;&#12290;&#23545;4&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31574;&#30053;&#25913;&#21892;&#20102;&#33258;&#36866;&#24212;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#25104;&#21151;&#22320;&#19982;&#29616;&#26377;&#30340;SFDA&#26041;&#27861;&#38598;&#25104;&#12290;&#21033;&#29992;&#29616;&#20195;&#39044;&#35757;&#32451;&#32593;&#32476;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#26469;&#23454;&#29616;&#39046;&#22495;&#33258;&#36866;&#24212;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Source-free domain adaptation (SFDA) aims to adapt a source model trained on a fully-labeled source domain to an unlabeled target domain. Large-data pre-trained networks are used to initialize source models during source training, and subsequently discarded. However, source training can cause the model to overfit to source data distribution and lose applicable target domain knowledge. We propose to integrate the pre-trained network into the target adaptation process as it has diversified features important for generalization and provides an alternate view of features and classification decisions different from the source model. We propose to distil useful target domain information through a co-learning strategy to improve target pseudolabel quality for finetuning the source model. Evaluation on 4 benchmark datasets show that our proposed strategy improves adaptation performance and can be successfully integrated with existing SFDA methods. Leveraging modern pre-trained networks that ha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20984;&#35268;&#21017;&#21270;&#22120;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#30001;&#20984;&#33034;&#20989;&#25968;&#32452;&#25104;&#30340;&#35268;&#21017;&#21270;&#22120;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#21333;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#26799;&#24230;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21435;&#22122;&#12289;CT&#21644;MRI&#37325;&#24314;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#31867;&#20284;&#21487;&#38752;&#24615;&#20445;&#35777;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.12461</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20984;&#35268;&#21017;&#21270;&#22120;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
A Neural-Network-Based Convex Regularizer for Image Reconstruction. (arXiv:2211.12461v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20984;&#35268;&#21017;&#21270;&#22120;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#30001;&#20984;&#33034;&#20989;&#25968;&#32452;&#25104;&#30340;&#35268;&#21017;&#21270;&#22120;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#21333;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#26799;&#24230;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21435;&#22122;&#12289;CT&#21644;MRI&#37325;&#24314;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#20379;&#31867;&#20284;&#21487;&#38752;&#24615;&#20445;&#35777;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#19978;&#30340;&#20986;&#29616;&#65292;&#20351;&#24471;&#37325;&#24314;&#36136;&#37327;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#27492;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#22914;&#20309;&#22312;&#20445;&#25345;&#24615;&#33021;&#25552;&#21319;&#30340;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#30001;&#20984;&#33034;&#20989;&#25968;&#32452;&#25104;&#30340;&#35268;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#35268;&#21017;&#21270;&#22120;&#30340;&#26799;&#24230;&#30001;&#19968;&#20010;&#20855;&#26377;&#21333;&#20010;&#38544;&#34255;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#20854;&#20013;&#21253;&#21547;&#36880;&#28176;&#22686;&#21152;&#21644;&#21487;&#23398;&#20064;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#35813;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22810;&#27493;&#39640;&#26031;&#21435;&#22122;&#22120;&#22312;&#20960;&#20998;&#38047;&#20869;&#36827;&#34892;&#35757;&#32451;&#12290;&#21435;&#22122;&#12289;CT&#21644;MRI&#37325;&#24314;&#30340;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#25552;&#20379;&#31867;&#20284;&#21487;&#38752;&#24615;&#20445;&#35777;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#20855;&#26377;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of deep-learning-based methods to solve image-reconstruction problems has enabled a significant increase in reconstruction quality. Unfortunately, these new methods often lack reliability and explainability, and there is a growing interest to address these shortcomings while retaining the boost in performance. In this work, we tackle this issue by revisiting regularizers that are the sum of convex-ridge functions. The gradient of such regularizers is parameterized by a neural network that has a single hidden layer with increasing and learnable activation functions. This neural network is trained within a few minutes as a multistep Gaussian denoiser. The numerical experiments for denoising, CT, and MRI reconstruction show improvements over methods that offer similar reliability guarantees.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#20248;&#21270;&#29305;&#24449;&#36890;&#20449;&#12289;&#27169;&#22411;&#31934;&#24230;&#21644;&#20998;&#24067;&#24335;&#21516;&#27493;&#12290;</title><link>http://arxiv.org/abs/2211.00216</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Distributed Graph Neural Network Training: A Survey. (arXiv:2211.00216v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00216
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#26469;&#20248;&#21270;&#29305;&#24449;&#36890;&#20449;&#12289;&#27169;&#22411;&#31934;&#24230;&#21644;&#20998;&#24067;&#24335;&#21516;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31181;&#22312;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;&#23613;&#31649;GNNs&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#26159;&#23558;&#20854;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#22270;&#20381;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20998;&#24067;&#24335;&#35745;&#31639;&#25104;&#20026;&#35757;&#32451;&#22823;&#35268;&#27169;GNNs&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#33021;&#25552;&#20379;&#20016;&#23500;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#22270;&#32467;&#26500;&#30340;&#20381;&#36182;&#24615;&#20351;&#24471;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#36890;&#20449;&#21644;&#36127;&#36733;&#19981;&#24179;&#34913;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#36827;&#34892;&#20102;&#35768;&#22810;&#21162;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#35757;&#32451;&#31639;&#27861;&#21644;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#24067;&#24335;&#25191;&#34892;GNN&#35757;&#32451;&#30340;&#20248;&#21270;&#25216;&#26415;&#26041;&#38754;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#12290;&#26412;&#35843;&#26597;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#30340;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#22823;&#35268;&#27169;&#29305;&#24449;&#36890;&#20449;&#12289;&#27169;&#22411;&#31934;&#24230;&#25439;&#22833;&#21644;&#20998;&#24067;&#24335;&#21516;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are a type of deep learning models that are trained on graphs and have been successfully applied in various domains. Despite the effectiveness of GNNs, it is still challenging for GNNs to efficiently scale to large graphs. As a remedy, distributed computing becomes a promising solution of training large-scale GNNs, since it is able to provide abundant computing resources. However, the dependency of graph structure increases the difficulty of achieving high-efficiency distributed GNN training, which suffers from the massive communication and workload imbalance. In recent years, many efforts have been made on distributed GNN training, and an array of training algorithms and systems have been proposed. Yet, there is a lack of systematic review on the optimization techniques for the distributed execution of GNN training. In this survey, we analyze three major challenges in distributed GNN training that are massive feature communication, the loss of model accura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#25193;&#25955;&#27169;&#22411;(DPDMs)&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#38544;&#31169;&#30340;&#20445;&#25252;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#33021;&#22815;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;DP-SGD&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2210.09929</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Diffusion Models. (arXiv:2210.09929v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#25193;&#25955;&#27169;&#22411;(DPDMs)&#65292;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#38544;&#31169;&#30340;&#20445;&#25252;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#33021;&#22815;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;DP-SGD&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#28982;&#32780;&#22312;&#28041;&#21450;&#38544;&#31169;&#30340;&#39046;&#22495;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#26377;&#38480;&#30340;&#12290;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#32469;&#36807;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20379;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#35775;&#38382;&#12290;&#26412;&#25991;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#25193;&#25955;&#27169;&#22411;(DPDMs)&#65292;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(DP-SGD)&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;DPDM&#20013;&#30340;&#21442;&#25968;&#21270;&#21644;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22122;&#22768;&#22810;&#26679;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;DM&#35757;&#32451;&#30340;&#24378;&#22823;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26032;&#39062;DPDMs&#65292;&#24182;&#22312;&#25152;&#26377;&#23454;&#39564;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;DPDM&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#34920;&#29616;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;DP-SGD&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#30456;&#24403;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#27809;&#26377;&#34987;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
While modern machine learning models rely on increasingly large training datasets, data is often limited in privacy-sensitive domains. Generative models trained with differential privacy (DP) on sensitive data can sidestep this challenge, providing access to synthetic data instead. We build on the recent success of diffusion models (DMs) and introduce Differentially Private Diffusion Models (DPDMs), which enforce privacy using differentially private stochastic gradient descent (DP-SGD). We investigate the DM parameterization and the sampling algorithm, which turn out to be crucial ingredients in DPDMs, and propose noise multiplicity, a powerful modification of DP-SGD tailored to the training of DMs. We validate our novel DPDMs on image generation benchmarks and achieve state-of-the-art performance in all experiments. Moreover, on standard benchmarks, classifiers trained on DPDM-generated synthetic data perform on par with task-specific DP-SGD-trained classifiers, which has not been dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30149;&#29702;&#23398;&#24341;&#23548;&#30340;&#20998;&#23618;&#32593;&#32476;&#65288;PSSN&#65289;&#65292;&#36890;&#36807;&#21453;&#24212;&#25193;&#25955;&#27169;&#22411;&#23558;&#24050;&#24314;&#31435;&#30340;AD&#30149;&#29702;&#23398;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#30340;&#38271;&#26399;&#36712;&#36857;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#24403;&#21069;&#32570;&#22833;&#30340;&#31070;&#32463;&#30149;&#29702;&#23398;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2210.05880</link><description>&lt;p&gt;
Alzheimer&#30149;&#20013;&#30340;&#30149;&#29702;&#23398;&#24341;&#23548;&#20998;&#23618;&#32593;&#32476;&#29992;&#20110;&#20122;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Pathology Steered Stratification Network for Subtype Identification in Alzheimer's Disease. (arXiv:2210.05880v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30149;&#29702;&#23398;&#24341;&#23548;&#30340;&#20998;&#23618;&#32593;&#32476;&#65288;PSSN&#65289;&#65292;&#36890;&#36807;&#21453;&#24212;&#25193;&#25955;&#27169;&#22411;&#23558;&#24050;&#24314;&#31435;&#30340;AD&#30149;&#29702;&#23398;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#20102;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#30340;&#38271;&#26399;&#36712;&#36857;&#12290;&#36825;&#19968;&#26041;&#27861;&#22635;&#34917;&#20102;&#24403;&#21069;&#32570;&#22833;&#30340;&#31070;&#32463;&#30149;&#29702;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Alzheimer&#30149;(AD)&#26159;&#19968;&#31181;&#24322;&#36136;&#24615;&#30340;&#12289;&#22810;&#22240;&#32032;&#24341;&#36215;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20197;&#946;-&#28096;&#31881;&#26679;&#34507;&#30333;&#12289;&#30149;&#29702;&#24615;tau&#34507;&#30333;&#21644;&#31070;&#32463;&#36864;&#21270;&#20026;&#29305;&#24449;&#12290;&#30446;&#21069;&#23545;&#20110;&#26202;&#26399;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#27809;&#26377;&#26377;&#25928;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#36843;&#20999;&#38656;&#35201;&#26089;&#26399;&#24178;&#39044;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AD&#20122;&#22411;&#35782;&#21035;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#24573;&#35270;&#20102;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#30149;&#24773;&#19981;&#26126;&#30830;&#19988;&#19982;&#22522;&#26412;&#31070;&#32463;&#23398;&#21407;&#21017;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#30149;&#29702;&#23398;&#24341;&#23548;&#30340;&#20998;&#23618;&#32593;&#32476;(PSSN)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#24212;&#25193;&#25955;&#27169;&#22411;&#23558;&#24050;&#24314;&#31435;&#30340;AD&#30149;&#29702;&#23398;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#32771;&#34385;&#20027;&#35201;&#29983;&#29289;&#26631;&#24535;&#29289;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#27839;&#33041;&#32467;&#26500;&#32593;&#32476;&#30340;&#25193;&#25955;&#12290;&#36890;&#36807;&#23545;&#32437;&#21521;&#22810;&#27169;&#24577;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#29983;&#29289;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#25429;&#25417;&#21040;&#20010;&#20307;&#21457;&#23637;&#27169;&#24335;&#30340;&#38271;&#26399;&#36712;&#36857;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#32570;&#22833;&#30340;&#31070;&#32463;&#30149;&#29702;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD) is a heterogeneous, multifactorial neurodegenerative disorder characterized by beta-amyloid, pathologic tau, and neurodegeneration. There are no effective treatments for Alzheimer's disease at a late stage, urging for early intervention. However, existing statistical inference approaches of AD subtype identification ignore the pathological domain knowledge, which could lead to ill-posed results that are sometimes inconsistent with the essential neurological principles. Integrating systems biology modeling with machine learning, we propose a novel pathology steered stratification network (PSSN) that incorporates established domain knowledge in AD pathology through a reaction-diffusion model, where we consider non-linear interactions between major biomarkers and diffusion along brain structural network. Trained on longitudinal multimodal neuroimaging data, the biological model predicts long-term trajectories that capture individual progression pattern, filling in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#36827;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#37319;&#29992;&#22810;&#38454;&#27573;Transformer&#32534;&#30721;&#22120;&#24314;&#27169;&#20132;&#20114;&#24847;&#35782;&#21644;&#24847;&#22270;&#24847;&#35782;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#28508;&#22312;Transformer&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21152;&#36895;&#35757;&#32451;&#21644;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2208.12263</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Augmenting Reinforcement Learning with Transformer-based Scene Representation Learning for Decision-making of Autonomous Driving. (arXiv:2208.12263v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#25913;&#36827;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#37319;&#29992;&#22810;&#38454;&#27573;Transformer&#32534;&#30721;&#22120;&#24314;&#27169;&#20132;&#20114;&#24847;&#35782;&#21644;&#24847;&#22270;&#24847;&#35782;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#28508;&#22312;Transformer&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#21152;&#36895;&#35757;&#32451;&#21644;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#30340;&#20915;&#31574;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#30001;&#20110;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#38543;&#26426;&#24615;&#21644;&#36947;&#36335;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20915;&#31574;&#26041;&#26696;&#22312;&#22788;&#29702;&#22478;&#24066;&#39550;&#39542;&#22330;&#26223;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#23427;&#30340;&#37319;&#26679;&#25928;&#29575;&#20302;&#19988;&#36866;&#24212;&#24615;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Scene-Rep Transformer&#26469;&#25913;&#21892;RL&#20915;&#31574;&#33021;&#21147;&#65292;&#36890;&#36807;&#26356;&#22909;&#30340;&#22330;&#26223;&#34920;&#31034;&#32534;&#30721;&#21644;&#39034;&#24207;&#39044;&#27979;&#28508;&#22312;&#33976;&#39311;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;Transformer&#65288;MST&#65289;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#24314;&#27169;&#33258;&#36710;&#19982;&#20854;&#37051;&#23621;&#20043;&#38388;&#30340;&#20132;&#20114;&#24847;&#35782;&#20197;&#21450;&#20195;&#29702;&#32773;&#19982;&#20505;&#36873;&#36335;&#24452;&#20043;&#38388;&#30340;&#24847;&#22270;&#24847;&#35782;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#30340;&#39034;&#24207;&#28508;&#22312;Transformer&#65288;SLT&#65289;&#65292;&#23558;&#26410;&#26469;&#30340;&#39044;&#27979;&#20449;&#24687;&#33976;&#39311;&#21040;&#28508;&#22312;&#30340;&#22330;&#26223;&#34920;&#31034;&#20013;&#65292;&#20197;&#20943;&#23569;&#25506;&#32034;&#31354;&#38388;&#24182;&#21152;&#24555;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making for urban autonomous driving is challenging due to the stochastic nature of interactive traffic participants and the complexity of road structures. Although reinforcement learning (RL)-based decision-making scheme is promising to handle urban driving scenarios, it suffers from low sample efficiency and poor adaptability. In this paper, we propose Scene-Rep Transformer to improve the RL decision-making capabilities with better scene representation encoding and sequential predictive latent distillation. Specifically, a multi-stage Transformer (MST) encoder is constructed to model not only the interaction awareness between the ego vehicle and its neighbors but also intention awareness between the agents and their candidate routes. A sequential latent Transformer (SLT) with self-supervised learning objectives is employed to distill the future predictive information into the latent scene representation, in order to reduce the exploration space and speed up training. The fina
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;&#65288;G2L2&#65289;&#26159;&#19968;&#31181;&#20174;&#22522;&#30784;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#21547;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21333;&#35789;&#26144;&#23556;&#21040;&#35821;&#27861;&#31867;&#22411;&#21644;&#31070;&#32463;&#31526;&#21495;&#35821;&#20041;&#31243;&#24207;&#65292;&#21033;&#29992;&#22522;&#20110;&#35821;&#27861;&#30340;&#32452;&#21512;&#25512;&#23548;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#26368;&#32456;&#21487;&#20197;&#22312;&#22522;&#30784;&#36755;&#20837;&#19978;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2202.08806</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Grammar-Based Grounded Lexicon Learning. (arXiv:2202.08806v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08806
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;&#65288;G2L2&#65289;&#26159;&#19968;&#31181;&#20174;&#22522;&#30784;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#21547;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21333;&#35789;&#26144;&#23556;&#21040;&#35821;&#27861;&#31867;&#22411;&#21644;&#31070;&#32463;&#31526;&#21495;&#35821;&#20041;&#31243;&#24207;&#65292;&#21033;&#29992;&#22522;&#20110;&#35821;&#27861;&#30340;&#32452;&#21512;&#25512;&#23548;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#26368;&#32456;&#21487;&#20197;&#22312;&#22522;&#30784;&#36755;&#20837;&#19978;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;&#26041;&#27861;&#65288;G2L2&#65289;&#65292;&#29992;&#20110;&#20174;&#22522;&#30784;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#37197;&#23545;&#65289;&#20013;&#23398;&#20064;&#35821;&#35328;&#30340;&#32452;&#21512;&#21644;&#22522;&#20110;&#22522;&#30784;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;G2L2&#30340;&#26680;&#24515;&#26159;&#19968;&#32452;&#35789;&#27719;&#26465;&#30446;&#65292;&#23558;&#27599;&#20010;&#21333;&#35789;&#26144;&#23556;&#21040;&#19968;&#20010;&#35821;&#27861;&#31867;&#22411;&#21644;&#31070;&#32463;&#31526;&#21495;&#35821;&#20041;&#31243;&#24207;&#30340;&#20803;&#32452;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#21477;&#23376;&#65292;G2L2&#39318;&#20808;&#26597;&#25214;&#19982;&#27599;&#20010;&#26631;&#35760;&#30456;&#20851;&#32852;&#30340;&#35789;&#27719;&#26465;&#30446;&#12290;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#35821;&#27861;&#30340;&#32452;&#21512;&#35789;&#27719;&#21547;&#20041;&#26469;&#25512;&#23548;&#21477;&#23376;&#30340;&#21547;&#20041;&#20316;&#20026;&#21487;&#25191;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#12290;&#24674;&#22797;&#30340;&#21547;&#20041;&#31243;&#24207;&#21487;&#20197;&#22312;&#22522;&#30784;&#36755;&#20837;&#19978;&#25191;&#34892;&#12290;&#20026;&#20102;&#22312;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#20419;&#36827;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22238;&#24402;&#30340;channel pruning&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach toward learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of G2L2 is a collection of lexicon entries, which map each word to a tuple of a syntactic type and a neuro-symbolic semantic program. For example, the word shiny has a syntactic type of adjective; its neuro-symbolic semantic program has the symbolic form {\lambda}x. filter(x, SHINY), where the concept SHINY is associated with a neural network embedding, which will be used to classify shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries associated with each token. It then derives the meaning of the sentence as an executable neuro-symbolic program by composing lexical meanings based on syntax. The recovered meaning programs can be executed on grounded inputs. To facilitate learning in an exponentially-growing compositional space, we introd
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#20307;&#20869;&#22270;&#20687;&#19978;&#35782;&#21035;&#32958;&#32467;&#30707;&#30340;&#24615;&#33021;&#65292;&#20026;&#32958;&#32467;&#30707;&#35786;&#26029;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2201.08865</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#20307;&#20869;&#35782;&#21035;&#32958;&#32467;&#30707;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the in vivo recognition of kidney stones using machine learning. (arXiv:2201.08865v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#20307;&#20869;&#22270;&#20687;&#19978;&#35782;&#21035;&#32958;&#32467;&#30707;&#30340;&#24615;&#33021;&#65292;&#20026;&#32958;&#32467;&#30707;&#35786;&#26029;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#32958;&#32467;&#30707;&#30340;&#31867;&#22411;&#21487;&#20197;&#24110;&#21161;&#27852;&#23615;&#31185;&#21307;&#29983;&#39044;&#38450;&#32958;&#32467;&#30707;&#30340;&#22797;&#21457;&#12290;&#33258;&#21160;&#21270;&#30340;&#20307;&#20869;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#31867;&#26041;&#27861;&#23558;&#26159;&#35786;&#26029;&#30340;&#31532;&#19968;&#38454;&#27573;&#20013;&#31435;&#21363;&#35782;&#21035;&#25152;&#38656;&#32958;&#32467;&#30707;&#31867;&#22411;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#20307;&#22806;&#30340;&#25968;&#25454;&#19978;&#65292;&#33258;&#21160;&#21270;&#30340;&#32958;&#32467;&#30707;&#20998;&#31867;&#30830;&#23454;&#26159;&#21487;&#34892;&#30340;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20845;&#31181;&#27973;&#23618;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#20307;&#20869;&#22270;&#20687;&#19978;&#36827;&#34892;&#32958;&#32467;&#30707;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#20869;&#31397;&#38236;&#22312;&#26631;&#20934;&#36755;&#23615;&#31649;&#38236;&#26816;&#26415;&#26399;&#38388;&#33719;&#24471;&#30340;&#22235;&#31181;&#26368;&#24120;&#35265;&#23615;&#36335;&#32467;&#30707;&#31867;&#22411;&#30340;&#22270;&#20687;&#36827;&#34892;&#27979;&#35797;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25968;&#25454;&#24211;&#26500;&#24314;&#21644;&#27979;&#35797;&#30340;&#32958;&#32467;&#30707;&#20998;&#31867;&#22120;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining the type of kidney stones allows urologists to prescribe a treatment to avoid recurrence of renal lithiasis. An automated in-vivo image-based classification method would be an important step towards an immediate identification of the kidney stone type required as a first phase of the diagnosis. In the literature it was shown on ex-vivo data (i.e., in very controlled scene and image acquisition conditions) that an automated kidney stone classification is indeed feasible. This pilot study compares the kidney stone recognition performances of six shallow machine learning methods and three deep-learning architectures which were tested with in-vivo images of the four most frequent urinary calculi types acquired with an endoscope during standard ureteroscopies. This contribution details the database construction and the design of the tested kidney stones classifiers. Even if the best results were obtained by the Inception v3 architecture (weighted precision, recall and F1-score o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#20572;&#31574;&#30053;&#26469;&#35299;&#20915;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#21644;DIP&#21464;&#20307;&#20013;&#25345;&#32493;&#26816;&#27979;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;DIP&#23454;&#29992;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2112.06074</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#30340;&#26089;&#20572;&#27861;
&lt;/p&gt;
&lt;p&gt;
Early Stopping for Deep Image Prior. (arXiv:2112.06074v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.06074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26089;&#20572;&#31574;&#30053;&#26469;&#35299;&#20915;&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#21644;DIP&#21464;&#20307;&#20013;&#25345;&#32493;&#26816;&#27979;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#65292;&#31361;&#30772;&#20102;DIP&#23454;&#29992;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#20687;&#20808;&#39564;(DIP)&#21450;&#20854;&#21464;&#20307;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#35299;&#20915;&#36870;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#38469;&#30340;DIP&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#25311;&#21512;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#21040;&#22823;&#37096;&#20998;&#26399;&#26395;&#30340;&#35270;&#35273;&#20869;&#23481;&#65292;&#28982;&#21518;&#36880;&#28176;&#25429;&#25417;&#21040;&#28508;&#22312;&#30340;&#24314;&#27169;&#21644;&#35266;&#27979;&#22122;&#22768;&#65292;&#21363;&#36807;&#25311;&#21512;&#12290;&#22240;&#27492;&#65292;DIP&#30340;&#23454;&#29992;&#24615;&#24448;&#24448;&#20851;&#38190;&#21462;&#20915;&#20110;&#33391;&#22909;&#30340;&#26089;&#20572;&#31574;&#30053;&#65292;&#20197;&#25429;&#25417;&#36807;&#28193;&#26399;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#30340;DIP&#24037;&#20316;&#21482;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#25253;&#21578;&#20102;&#19982;&#30495;&#23454;&#32467;&#26524;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#22914;&#20309;&#22312;&#27809;&#26377;&#30495;&#23454;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#25805;&#20316;&#24615;&#22320;&#33719;&#24471;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#24182;&#27809;&#26377;&#32473;&#20986;&#32447;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#31361;&#30772;DIP&#30340;&#23454;&#29992;&#24615;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26089;&#20572;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#21644;DIP&#21464;&#20307;&#20013;&#25345;&#32493;&#22320;&#26816;&#27979;&#21040;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep image prior (DIP) and its variants have showed remarkable potential for solving inverse problems in computer vision, without any extra training data. Practical DIP models are often substantially overparameterized. During the fitting process, these models learn mostly the desired visual content first, and then pick up the potential modeling and observational noise, i.e., overfitting. Thus, the practicality of DIP often depends critically on good early stopping (ES) that captures the transition period. In this regard, the majority of DIP works for vision tasks only demonstrates the potential of the models -- reporting the peak performance against the ground truth, but provides no clue about how to operationally obtain near-peak performance without access to the groundtruth. In this paper, we set to break this practicality barrier of DIP, and propose an efficient ES strategy, which consistently detects near-peak performance across several vision tasks and DIP variants. Based on a sim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23616;&#37096;&#24377;&#24615;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;$S_{\rm rel}$&#23450;&#20041;&#30340;&#20840;&#38754;&#30740;&#31350;&#24182;&#25552;&#20986;&#26032;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#21457;&#29616;&#26032;&#30340;&#23450;&#20041;&#33021;&#26356;&#25935;&#38160;&#22320;&#26816;&#27979;&#20986;&#26435;&#37325;&#26356;&#26032;&#26356;&#20559;&#21521;&#20110;&#22312;&#19982;&#26679;&#26412;&#25968;&#25454;&#21516;&#19968;&#31867;&#21035;&#20869;&#36827;&#34892;&#39044;&#27979;&#21464;&#21270;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.01166</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#23616;&#37096;&#24377;&#24615;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Local Elasticity During Training of Neural Nets. (arXiv:2111.01166v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23616;&#37096;&#24377;&#24615;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;$S_{\rm rel}$&#23450;&#20041;&#30340;&#20840;&#38754;&#30740;&#31350;&#24182;&#25552;&#20986;&#26032;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#21457;&#29616;&#26032;&#30340;&#23450;&#20041;&#33021;&#26356;&#25935;&#38160;&#22320;&#26816;&#27979;&#20986;&#26435;&#37325;&#26356;&#26032;&#26356;&#20559;&#21521;&#20110;&#22312;&#19982;&#26679;&#26412;&#25968;&#25454;&#21516;&#19968;&#31867;&#21035;&#20869;&#36827;&#34892;&#39044;&#27979;&#21464;&#21270;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36712;&#36857;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#23646;&#24615;&#65292;&#21363;&#8220;&#23616;&#37096;&#24377;&#24615;&#8221;&#65288;&#34920;&#31034;&#20026;$S_{\rm rel}$&#65289;&#12290;&#23616;&#37096;&#24377;&#24615;&#35797;&#22270;&#37327;&#21270;&#26679;&#26412;&#25968;&#25454;&#28857;&#23545;&#39044;&#27979;&#32467;&#26524;&#22312;&#20854;&#20182;&#25968;&#25454;&#28857;&#19978;&#30340;&#24433;&#21709;&#20256;&#25773;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#24050;&#26377;&#30340;$S_{\rm rel}$&#23450;&#20041;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23450;&#20041;&#65292;&#35299;&#20915;&#20102;&#21407;&#22987;&#23450;&#20041;&#22312;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#22312;SVHN&#65292;CIFAR-10&#21644;CIFAR-100&#19978;&#36827;&#34892;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26032;&#30340;$S_{\rm rel}$&#23450;&#20041;&#30456;&#27604;&#20110;&#21407;&#22987;&#23450;&#20041;&#26356;&#21152;&#25935;&#38160;&#22320;&#26816;&#27979;&#20986;&#26435;&#37325;&#26356;&#26032;&#26356;&#20559;&#21521;&#20110;&#22312;&#19982;&#26679;&#26412;&#25968;&#25454;&#21516;&#19968;&#31867;&#21035;&#20869;&#36827;&#34892;&#39044;&#27979;&#21464;&#21270;&#30340;&#29305;&#24615;&#12290;&#22312;&#31070;&#32463;&#22238;&#24402;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21407;&#22987;$S_{\rm rel}$&#26174;&#31034;&#20986;&#19968;&#20010;2&#38454;&#27573;&#34892;&#20026;--&#36890;&#36807;&#21021;&#22987;&#24377;&#24615;&#38454;&#27573;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the recent past, a property of neural training trajectories in weight-space had been isolated, that of "local elasticity" (denoted as $S_{\rm rel}$). Local elasticity attempts to quantify the propagation of the influence of a sampled data point on the prediction at another data. In this work, we embark on a comprehensive study of the existing notion of $S_{\rm rel}$ and also propose a new definition that addresses the limitations that we point out for the original definition in the classification setting. On various state-of-the-art neural network training on SVHN, CIFAR-10 and CIFAR-100 we demonstrate how our new proposal of $S_{\rm rel}$, as opposed to the original definition, much more sharply detects the property of the weight updates preferring to make prediction changes within the same class as the sampled data.  In neural regression experiments we demonstrate that the original $S_{\rm rel}$ reveals a $2-$phase behavior -- that the training proceeds via an initial elastic phas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#32463;&#27982;&#22330;&#26223;&#29983;&#25104;&#22120;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25972;&#20010;&#20869;&#37096;&#24066;&#22330;&#39118;&#38505;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#27431;&#27954;&#30417;&#31649;&#25209;&#20934;&#30340;&#20869;&#37096;&#27169;&#22411;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2109.10072</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24066;&#22330;&#39118;&#38505;&#27169;&#22411;&#30340;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scenario generation for market risk models using generative neural networks. (arXiv:2109.10072v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.10072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#32463;&#27982;&#22330;&#26223;&#29983;&#25104;&#22120;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25972;&#20010;&#20869;&#37096;&#24066;&#22330;&#39118;&#38505;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#27431;&#27954;&#30417;&#31649;&#25209;&#20934;&#30340;&#20869;&#37096;&#27169;&#22411;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#25193;&#23637;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20316;&#20026;&#32463;&#27982;&#22330;&#26223;&#29983;&#25104;&#22120;&#65288;ESG&#65289;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25972;&#20010;&#20869;&#37096;&#24066;&#22330;&#39118;&#38505;&#27169;&#22411;&#65292;&#20197;&#27169;&#25311;&#20445;&#38505;&#20844;&#21496;&#30340;&#20840;&#26041;&#20301;&#25237;&#36164;&#39118;&#38505;&#22240;&#32032;&#65292;&#24182;&#31526;&#21512; Solvency 2 &#25152;&#35201;&#27714;&#30340;&#19968;&#24180;&#26102;&#38388;&#33539;&#22260;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110; GAN &#30340;&#20869;&#37096;&#27169;&#22411;&#30340;&#32467;&#26524;&#19982;&#27431;&#27954;&#30417;&#31649;&#25209;&#20934;&#30340;&#20869;&#37096;&#27169;&#22411;&#31867;&#20284;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#23558;&#22522;&#20110; GAN &#30340;&#27169;&#22411;&#35270;&#20026;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#24066;&#22330;&#39118;&#38505;&#24314;&#27169;&#30340;&#26367;&#20195;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we show how to expand existing approaches of using generative adversarial networks (GANs) as economic scenario generators (ESG) to a whole internal market risk model - with enough risk factors to model the full band-width of investments for an insurance company and for a one year time horizon as required in Solvency 2. We demonstrate that the results of a GAN-based internal model are similar to regulatory approved internal models in Europe. Therefore, GAN-based models can be seen as a data-driven alternative way of market risk modeling.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#21487;&#20998;&#25968;&#25454;&#20351;&#29992;Bregman proximal point&#31639;&#27861;&#21644;Mirror Descent&#36827;&#34892;&#23398;&#20064;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;, &#25105;&#20204;&#21457;&#29616;BPPA&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#31639;&#27861;&#27491;&#21017;&#21270;&#24615;&#36136;, &#24182;&#19988;&#35777;&#26126;&#20102;&#36793;&#30028;&#19982;Bregman&#36317;&#31163;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;, &#36825;&#25581;&#31034;&#20102;BPPA&#23545;&#20110;&#23398;&#20064;&#20998;&#31867;&#22120;&#36136;&#37327;&#30340;&#24433;&#21709;&#21644;&#37325;&#35201;&#24615;</title><link>http://arxiv.org/abs/2108.06808</link><description>&lt;p&gt;
Bregman Proximal Point&#31639;&#27861;&#21644;Mirror Descent&#22312;&#21487;&#20998;&#25968;&#25454;&#19978;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit Regularization of Bregman Proximal Point Algorithm and Mirror Descent on Separable Data. (arXiv:2108.06808v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06808
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#21487;&#20998;&#25968;&#25454;&#20351;&#29992;Bregman proximal point&#31639;&#27861;&#21644;Mirror Descent&#36827;&#34892;&#23398;&#20064;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;, &#25105;&#20204;&#21457;&#29616;BPPA&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#31639;&#27861;&#27491;&#21017;&#21270;&#24615;&#36136;, &#24182;&#19988;&#35777;&#26126;&#20102;&#36793;&#30028;&#19982;Bregman&#36317;&#31163;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;, &#36825;&#25581;&#31034;&#20102;BPPA&#23545;&#20110;&#23398;&#20064;&#20998;&#31867;&#22120;&#36136;&#37327;&#30340;&#24433;&#21709;&#21644;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bregman proximal point&#31639;&#27861;&#65288;BPPA&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#29702;&#35770;&#29702;&#35299;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#21487;&#20998;&#25968;&#25454;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#30740;&#31350;&#20102;BPPA&#30340;&#35745;&#31639;&#24615;&#36136;&#65292;&#24182;&#35777;&#26126;&#20102;BPPA&#30340;&#21487;&#39564;&#35777;&#31639;&#27861;&#27491;&#21017;&#21270;&#12290;&#23545;&#20110;&#20219;&#20309;&#20351;&#29992;&#22266;&#23450;Bregman&#36317;&#31163;&#23454;&#20363;&#21270;&#30340;BPPA&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;BPPA&#25152;&#33719;&#24471;&#30340;&#36793;&#30028;&#30340;&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#19982;&#20219;&#24847;&#36873;&#25321;&#30340;&#33539;&#25968;&#30456;&#20851;&#12290;&#25152;&#24471;&#21040;&#30340;&#36793;&#30028;&#19979;&#30028;&#19982;&#26368;&#22823;&#36793;&#30028;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20056;&#27861;&#22240;&#23376;&#30340;&#24046;&#24322;&#65292;&#35813;&#20056;&#27861;&#22240;&#23376;&#19982;&#20197;&#23545;&#20598;&#33539;&#25968;&#24230;&#37327;&#30340;&#36317;&#31163;&#29983;&#25104;&#20989;&#25968;&#30340;&#26465;&#20214;&#25968;&#25104;&#21453;&#27604;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#26465;&#20214;&#25968;&#30340;&#20381;&#36182;&#26159;&#32039;&#33268;&#30340;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#24046;&#24322;&#23545;&#20110;&#24433;&#21709;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#25512;&#24191;&#21040;&#20102;mirror descent&#65292;&#23545;&#20110;&#35813;&#31639;&#27861;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36793;&#30028;&#21644;Bregman&#36317;&#31163;&#20043;&#38388;&#30340;&#31867;&#20284;&#32852;&#31995;&#65292;&#20197;&#21450;&#19968;&#20010;&#38750;-a
&lt;/p&gt;
&lt;p&gt;
Bregman proximal point algorithm (BPPA) has witnessed emerging machine learning applications, yet its theoretical understanding has been largely unexplored. We study the computational properties of BPPA through learning linear classifiers with separable data, and demonstrate provable algorithmic regularization of BPPA. For any BPPA instantiated with a fixed Bregman divergence, we provide a lower bound of the margin obtained by BPPA with respect to an arbitrarily chosen norm. The obtained margin lower bound differs from the maximal margin by a multiplicative factor, which inversely depends on the condition number of the distance-generating function measured in the dual norm. We show that the dependence on the condition number is tight, thus demonstrating the importance of divergence in affecting the quality of the learned classifiers. We then extend our findings to mirror descent, for which we establish similar connections between the margin and Bregman divergence, together with a non-a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#39640;&#32500;&#24230;&#30340;&#35823;&#24046;&#21464;&#37327;&#22266;&#23450;&#35774;&#35745;&#29615;&#22659;&#20013;&#20998;&#26512;&#20102;&#20027;&#25104;&#20998;&#22238;&#24402;&#27169;&#22411;&#35782;&#21035;&#21644;&#26679;&#26412;&#22806;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#20110;&#24050;&#30693;&#26368;&#20339;&#36895;&#29575;&#30340;&#38750;&#28176;&#36827;&#39044;&#27979;&#20445;&#35777;&#12290;&#22312;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#32447;&#24615;&#20195;&#25968;&#26465;&#20214;&#26469;&#36991;&#20813;&#26679;&#26412;&#22806;&#39044;&#27979;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#26469;&#26816;&#26597;&#35813;&#26465;&#20214;&#22312;&#23454;&#36341;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#30340;&#32467;&#26524;&#20063;&#23545;&#21512;&#25104;&#23545;&#29031;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2010.14449</link><description>&lt;p&gt;
&#20851;&#20110;&#20027;&#25104;&#20998;&#22238;&#24402;&#27169;&#22411;&#35782;&#21035;&#21644;&#26679;&#26412;&#22806;&#39044;&#27979;&#30340;&#30740;&#31350;: &#24212;&#29992;&#20110;&#21512;&#25104;&#23545;&#29031;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Model Identification and Out-of-Sample Prediction of Principal Component Regression: Applications to Synthetic Controls. (arXiv:2010.14449v5 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.14449
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#39640;&#32500;&#24230;&#30340;&#35823;&#24046;&#21464;&#37327;&#22266;&#23450;&#35774;&#35745;&#29615;&#22659;&#20013;&#20998;&#26512;&#20102;&#20027;&#25104;&#20998;&#22238;&#24402;&#27169;&#22411;&#35782;&#21035;&#21644;&#26679;&#26412;&#22806;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#20110;&#24050;&#30693;&#26368;&#20339;&#36895;&#29575;&#30340;&#38750;&#28176;&#36827;&#39044;&#27979;&#20445;&#35777;&#12290;&#22312;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#32447;&#24615;&#20195;&#25968;&#26465;&#20214;&#26469;&#36991;&#20813;&#26679;&#26412;&#22806;&#39044;&#27979;&#30340;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#26469;&#26816;&#26597;&#35813;&#26465;&#20214;&#22312;&#23454;&#36341;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#30340;&#32467;&#26524;&#20063;&#23545;&#21512;&#25104;&#23545;&#29031;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#39640;&#32500;&#24230;&#30340;&#35823;&#24046;&#21464;&#37327;&#22266;&#23450;&#35774;&#35745;&#29615;&#22659;&#20013;&#20998;&#26512;&#20102;&#20027;&#25104;&#20998;&#22238;&#24402;(PCR)&#12290;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PCR&#33021;&#22815;&#19968;&#33268;&#22320;&#35782;&#21035;&#20986;&#20855;&#26377;&#26368;&#23567;L2&#33539;&#25968;&#30340;&#21807;&#19968;&#27169;&#22411;&#12290;&#36825;&#20123;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#24314;&#31435;&#36215;&#20248;&#20110;&#24050;&#30693;&#26368;&#20339;&#36895;&#29575;&#30340;&#38750;&#28176;&#36827;&#30340;&#26679;&#26412;&#22806;&#39044;&#27979;&#20445;&#35777;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#32447;&#24615;&#20195;&#25968;&#26465;&#20214;&#65292;&#32852;&#31995;&#20102;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#22806;&#30340;&#21327;&#21464;&#37327;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#26679;&#26412;&#22806;&#39044;&#27979;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#27492;&#26465;&#20214;&#22312;&#27867;&#21270;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#21363;&#20351;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20551;&#35774;&#26816;&#39564;&#26469;&#26816;&#26597;&#36825;&#20010;&#26465;&#20214;&#22312;&#23454;&#36341;&#20013;&#26159;&#21542;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#20026;&#21512;&#25104;&#23545;&#29031;&#25991;&#29486;&#25552;&#20379;&#20102;&#26032;&#30340;&#21457;&#29616;&#65292;&#21512;&#25104;&#23545;&#29031;&#26159;&#19968;&#31181;&#20027;&#35201;&#30340;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#22312;&#22266;&#23450;&#35774;&#35745;&#29615;&#22659;&#20013;&#30340;&#39044;&#27979;&#20445;&#35777;&#23578;&#26410;&#34987;&#30740;&#31350;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze principal component regression (PCR) in a high-dimensional error-in-variables setting with fixed design. Under suitable conditions, we show that PCR consistently identifies the unique model with minimum $\ell_2$-norm. These results enable us to establish non-asymptotic out-of-sample prediction guarantees that improve upon the best known rates. In the course of our analysis, we introduce a natural linear algebraic condition between the in- and out-of-sample covariates, which allows us to avoid distributional assumptions for out-of-sample predictions. Our simulations illustrate the importance of this condition for generalization, even under covariate shifts. Accordingly, we construct a hypothesis test to check when this conditions holds in practice. As a byproduct, our results also lead to novel results for the synthetic controls literature, a leading approach for policy evaluation. To the best of our knowledge, our prediction guarantees for the fixed design setting have been 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31574;&#30053;&#20998;&#35299;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#31526;&#21495;&#20851;&#31995;&#38382;&#39064;&#30340;&#21487;&#21464;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2009.12462</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31574;&#30053;&#20998;&#35299;&#30340;&#31526;&#21495;&#20851;&#31995;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symbolic Relational Deep Reinforcement Learning based on Graph Neural Networks and Autoregressive Policy Decomposition. (arXiv:2009.12462v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.12462
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31574;&#30053;&#20998;&#35299;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#31526;&#21495;&#20851;&#31995;&#38382;&#39064;&#30340;&#21487;&#21464;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20102;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#20110;&#20197;&#23545;&#35937;&#12289;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#21160;&#20316;&#26469;&#33258;&#28982;&#23450;&#20041;&#30340;&#31526;&#21495;&#20851;&#31995;&#38382;&#39064;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20855;&#26377;&#21487;&#21464;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#32780;&#35328;&#65292;&#25214;&#21040;&#19968;&#20010;&#22266;&#23450;&#38271;&#24230;&#30340;&#34920;&#31034;&#26159;&#22256;&#38590;&#30340;&#65292;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33258;&#22238;&#24402;&#31574;&#30053;&#20998;&#35299;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#24212;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#23436;&#20840;&#26159;&#39046;&#22495;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#38382;&#39064;&#22823;&#23567;&#19978;&#24341;&#20154;&#27880;&#30446;&#30340;&#38646;-shot&#27867;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on reinforcement learning (RL) in relational problems that are naturally defined in terms of objects, their relations, and object-centric actions. These problems are characterized by variable state and action spaces, and finding a fixed-length representation, required by most existing RL methods, is difficult, if not impossible. We present a deep RL framework based on graph neural networks and auto-regressive policy decomposition that naturally works with these problems and is completely domain-independent. We demonstrate the framework's broad applicability in three distinct domains and show impressive zero-shot generalization over different problem sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;&#38543;&#26426;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#65288;RLSVI&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#34892;&#21160;&#20540;&#20989;&#25968;&#30340;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#36827;&#34892;&#25200;&#21160;&#65292;&#35825;&#23548;&#20986;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#20302;&#31209;&#36716;&#31227;&#21160;&#24577;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLSVI&#30340;&#39057;&#29575;&#21518;&#24724;&#19978;&#30028;&#20026;$\widetilde O(d^2 H^2 \sqrt{T})$&#12290;&#36825;&#26159;&#23545;&#20110;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#38543;&#26426;&#25506;&#32034;&#30340;&#39318;&#20010;&#39057;&#29575;&#21518;&#24724;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/1911.00567</link><description>&lt;p&gt;
&#38543;&#26426;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Frequentist Regret Bounds for Randomized Least-Squares Value Iteration. (arXiv:1911.00567v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.00567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#29256;&#26412;&#30340;&#38543;&#26426;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#65288;RLSVI&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#34892;&#21160;&#20540;&#20989;&#25968;&#30340;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#36827;&#34892;&#25200;&#21160;&#65292;&#35825;&#23548;&#20986;&#20102;&#25506;&#32034;&#36807;&#31243;&#12290;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#20302;&#31209;&#36716;&#31227;&#21160;&#24577;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLSVI&#30340;&#39057;&#29575;&#21518;&#24724;&#19978;&#30028;&#20026;$\widetilde O(d^2 H^2 \sqrt{T})$&#12290;&#36825;&#26159;&#23545;&#20110;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#38543;&#26426;&#25506;&#32034;&#30340;&#39318;&#20010;&#39057;&#29575;&#21518;&#24724;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26377;&#38480;&#26102;&#38388;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;-&#21033;&#29992;&#22256;&#22659;&#12290;&#24403;&#29366;&#24577;&#31354;&#38388;&#24456;&#22823;&#25110;&#36830;&#32493;&#26102;&#65292;&#20256;&#32479;&#30340;&#34920;&#26684;&#26041;&#27861;&#19981;&#21487;&#34892;&#65292;&#24517;&#39035;&#37319;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;&#24418;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20048;&#35266;&#21021;&#22987;&#21270;&#30340;&#25913;&#36827;&#29256;&#26412;&#30340;&#38543;&#26426;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#65288;RLSVI&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#19968;&#31181;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#20854;&#20013;&#25506;&#32034;&#26159;&#36890;&#36807;&#25200;&#21160;&#34892;&#21160;&#20540;&#20989;&#25968;&#30340;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#26469;&#35825;&#23548;&#30340;&#12290;&#22312;&#20551;&#35774;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20855;&#26377;&#20302;&#31209;&#36716;&#31227;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLSVI&#30340;&#39057;&#29575;&#21518;&#24724;&#23558;&#19978;&#30028;&#20026;$\widetilde O(d^2 H^2 \sqrt{T})$&#65292;&#20854;&#20013;$ d $&#26159;&#29305;&#24449;&#32500;&#24230;&#65292;$ H $&#26159;&#26102;&#38388;&#38480;&#21046;&#65292;$ T $&#26159;&#24635;&#27493;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;&#20110;&#24102;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#38543;&#26426;&#25506;&#32034;&#30340;&#31532;&#19968;&#20010;&#39057;&#29575;&#21518;&#24724;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the exploration-exploitation dilemma in finite-horizon reinforcement learning (RL). When the state space is large or continuous, traditional tabular approaches are unfeasible and some form of function approximation is mandatory. In this paper, we introduce an optimistically-initialized variant of the popular randomized least-squares value iteration (RLSVI), a model-free algorithm where exploration is induced by perturbing the least-squares approximation of the action-value function. Under the assumption that the Markov decision process has low-rank transition dynamics, we prove that the frequentist regret of RLSVI is upper-bounded by $\widetilde O(d^2 H^2 \sqrt{T})$ where $ d $ are the feature dimension, $ H $ is the horizon, and $ T $ is the total number of steps. To the best of our knowledge, this is the first frequentist regret analysis for randomized exploration with function approximation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#26694;&#26550;&#30340;&#22312;&#32447;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#23454;&#20363;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20284;&#24615;&#12290;&#19981;&#21516;&#20110;&#20256;&#32479;&#26041;&#27861;&#21482;&#33021;&#23398;&#20064;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22810;&#20010;&#23618;&#27425;&#30340;&#24230;&#37327;&#31354;&#38388;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/1805.05510</link><description>&lt;p&gt;
&#22312;&#32447;&#24230;&#37327;&#23398;&#20064;&#30340;&#22810;&#23618;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multilayer Framework for Online Metric Learning. (arXiv:1805.05510v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1805.05510
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#26694;&#26550;&#30340;&#22312;&#32447;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#23454;&#20363;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20284;&#24615;&#12290;&#19981;&#21516;&#20110;&#20256;&#32479;&#26041;&#27861;&#21482;&#33021;&#23398;&#20064;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22810;&#20010;&#23618;&#27425;&#30340;&#24230;&#37327;&#31354;&#38388;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24230;&#37327;&#23398;&#20064;&#22312;&#20998;&#31867;&#21644;&#26816;&#32034;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#23558;&#30456;&#20284;&#23454;&#20363;&#19982;&#19981;&#30456;&#20284;&#23454;&#20363;&#20998;&#24320;&#19968;&#23450;&#36793;&#36317;&#26469;&#33258;&#21160;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36866;&#21512;&#30340;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22312;&#32447;&#24230;&#37327;&#23398;&#20064;&#31639;&#27861;&#22312;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#23454;&#38469;&#20998;&#31867;&#20013;&#24615;&#33021;&#26377;&#38480;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#22312;&#32447;&#24230;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#23454;&#20363;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20284;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#22312;&#32447;&#24230;&#37327;&#23398;&#20064;&#19981;&#21516;&#65292;&#35813;&#25552;&#20986;&#30340;&#22810;&#23618;&#22312;&#32447;&#24230;&#37327;&#23398;&#20064;&#65288;MLOML&#65289;&#23558;&#22312;&#32447;&#24230;&#37327;&#23398;&#20064;&#31639;&#27861;&#20316;&#20026;&#24230;&#37327;&#23618;&#65292;&#24182;&#23398;&#20064;&#22810;&#20010;&#20998;&#23618;&#24230;&#37327;&#31354;&#38388;&#65292;&#20854;&#20013;&#27599;&#20010;&#24230;&#37327;&#23618;&#22343;&#36981;&#24490;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#27491;&#21521;&#20256;&#25773;&#65288;FP&#65289;&#31574;&#30053;&#21644;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#31574;&#30053;&#26469;&#35757;&#32451;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online metric learning has been widely applied in classification and retrieval. It can automatically learn a suitable metric from data by restricting similar instances to be separated from dissimilar instances with a given margin. However, the existing online metric learning algorithms have limited performance in real-world classifications, especially when data distributions are complex. To this end, this paper proposes a multilayer framework for online metric learning to capture the nonlinear similarities among instances. Different from the traditional online metric learning, which can only learn one metric space, the proposed Multi-Layer Online Metric Learning (MLOML) takes an online metric learning algorithm as a metric layer and learns multiple hierarchical metric spaces, where each metric layer follows a nonlinear layers for the complicated data distribution. Moreover, the forward propagation (FP) strategy and backward propagation (BP) strategy are employed to train the hierarchic
&lt;/p&gt;</description></item></channel></rss>