<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;NLP&#25216;&#26415;&#21644;&#33258;&#21160;&#29983;&#25104;&#36816;&#34892;&#26102;&#20998;&#26512;&#25991;&#20214;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;5G&#36719;&#20214;&#26632;&#20013;&#30340;&#28431;&#27934;&#12289;&#24847;&#22806;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#65292;&#35797;&#39564;&#34920;&#26126;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08226</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#29983;&#25104;&#36816;&#34892;&#26102;&#20998;&#26512;&#25991;&#20214;&#30340;NLP&#25216;&#26415;&#30340;&#36328;&#23618;5G&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NLP-based Cross-Layer 5G Vulnerabilities Detection via Fuzzing Generated Run-Time Profiling. (arXiv:2305.08226v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;NLP&#25216;&#26415;&#21644;&#33258;&#21160;&#29983;&#25104;&#36816;&#34892;&#26102;&#20998;&#26512;&#25991;&#20214;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;5G&#36719;&#20214;&#26632;&#20013;&#30340;&#28431;&#27934;&#12289;&#24847;&#22806;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#65292;&#35797;&#39564;&#34920;&#26126;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#36719;&#20214;&#26632;&#28431;&#27934;&#21644;&#24847;&#22806;&#34892;&#20026;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#23545;&#20110;5G&#30340;&#20445;&#38556;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23427;&#22312;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#20013;&#30340;&#24212;&#29992;&#12290;&#27979;&#35797;&#26041;&#27861;&#21644;&#32593;&#32476;&#23433;&#20840;&#30740;&#31350;&#20013;&#30340;&#21487;&#20280;&#32553;&#24615;&#21644;&#33258;&#21160;&#21270;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20195;&#30721;&#24211;&#20013;&#30340;fuzz&#27979;&#35797;&#30456;&#23545;&#24212;&#30340;&#36816;&#34892;&#26102;&#20998;&#26512;&#25991;&#20214;&#33258;&#21160;&#26816;&#27979;5G&#36719;&#20214;&#26632;&#20013;&#30340;&#28431;&#27934;&#12289;&#24847;&#22806;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#65292;&#22312;srsRAN&#19978;&#36827;&#34892;&#20102;&#35797;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;fuzz&#27979;&#35797;&#29983;&#25104;&#30340;Logging Information (LogInfo)&#23558;&#36816;&#34892;&#26102;&#20998;&#26512;&#26144;&#23556;&#21040;&#39640;&#32500;&#24230;&#24230;&#37327;&#31354;&#38388;&#65292;&#28982;&#21518;&#22522;&#20110;&#23427;&#20204;&#30340;&#26102;&#38388;&#25139;&#20449;&#24687;&#26500;&#24314;&#29305;&#24449;&#31354;&#38388;&#65292;&#26368;&#21518;&#36827;&#19968;&#27493;&#21033;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#31639;&#27861;&#65292;&#21253;&#25324;Logistic&#22238;&#24402;&#65292;K-&#36817;&#37051;&#21644;&#38543;&#26426;&#26862;&#26519;&#23545;&#20854;&#23545;&#24615;&#33021;&#21644;&#23433;&#20840;&#23646;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness and efficiency of 5G software stack vulnerability and unintended behavior detection are essential for 5G assurance, especially for its applications in critical infrastructures. Scalability and automation are the main challenges in testing approaches and cybersecurity research. In this paper, we propose an innovative approach for automatically detecting vulnerabilities, unintended emergent behaviors, and performance degradation in 5G stacks via run-time profiling documents corresponding to fuzz testing in code repositories. Piloting on srsRAN, we map the run-time profiling via Logging Information (LogInfo) generated by fuzzing test to a high dimensional metric space first and then construct feature spaces based on their timestamp information. Lastly, we further leverage machine learning-based classification algorithms, including Logistic Regression, K-Nearest Neighbors, and Random Forest to categorize the impacts on performance and security attributes. The performance 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25968;&#25454;&#38598;&#34701;&#21512;&#8221;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#21512;&#25104;&#31639;&#27861;&#65292;&#21487;&#23558;&#26469;&#33258;&#22810;&#20010;&#21516;&#36136;&#25968;&#25454;&#38598;&#30340;&#21608;&#26399;&#20449;&#21495;&#34701;&#21512;&#20026;&#21333;&#20010;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#20004;&#20010;&#21516;&#36136;&#24863;&#24212;&#30005;&#21160;&#26426;&#65288;IM&#65289;&#25925;&#38556;&#25968;&#25454;&#38598;&#30340;3&#30456;&#30005;&#27969;&#25968;&#25454;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#26377;&#28508;&#21147;&#22312;&#36328;&#22810;&#20010;&#26469;&#28304;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.08197</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21516;&#36136;&#21608;&#26399;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#34701;&#21512;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Dataset Fusion Algorithm for Generalised Anomaly Detection in Homogeneous Periodic Time Series Datasets. (arXiv:2305.08197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#25968;&#25454;&#38598;&#34701;&#21512;&#8221;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#21512;&#25104;&#31639;&#27861;&#65292;&#21487;&#23558;&#26469;&#33258;&#22810;&#20010;&#21516;&#36136;&#25968;&#25454;&#38598;&#30340;&#21608;&#26399;&#20449;&#21495;&#34701;&#21512;&#20026;&#21333;&#20010;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#22312;&#20004;&#20010;&#21516;&#36136;&#24863;&#24212;&#30005;&#21160;&#26426;&#65288;IM&#65289;&#25925;&#38556;&#25968;&#25454;&#38598;&#30340;3&#30456;&#30005;&#27969;&#25968;&#25454;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#21183;&#65292;&#26377;&#28508;&#21147;&#22312;&#36328;&#22810;&#20010;&#26469;&#28304;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25512;&#24191;&#24448;&#24448;&#34987;&#25991;&#29486;&#24573;&#30053;&#65292;&#22240;&#20026;NN&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#25968;&#25454;&#28304;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#25968;&#25454;&#38598;&#27169;&#22411;&#20013;&#65292;&#30001;&#20110;&#26469;&#33258;&#19981;&#21516;&#20256;&#24863;&#22120;&#21644;&#37319;&#38598;&#35268;&#33539;&#30340;&#36830;&#32493;&#25968;&#25454;&#30340;&#34701;&#21512;&#22256;&#38590;&#65292;&#36825;&#21464;&#24471;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#65292;&#36890;&#29992;&#24615;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#22312;AI&#27169;&#22411;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#20043;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#25968;&#25454;&#38598;&#34701;&#21512;&#8221;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#26469;&#33258;&#22810;&#20010;&#21516;&#36136;&#25968;&#25454;&#38598;&#30340;&#21608;&#26399;&#20449;&#21495;&#34701;&#21512;&#20026;&#21333;&#20010;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#21512;&#25104;&#31639;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#36890;&#29992;&#24322;&#24120;&#26816;&#27979;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#32463;&#36807;&#23454;&#39564;&#65292;&#22312;&#20351;&#29992;&#26080;&#30417;&#30563;LSTMCaps NN&#23545;&#26469;&#33258;2&#20010;&#19981;&#21516;&#21516;&#36136;&#24863;&#24212;&#30005;&#21160;&#26426;&#65288;IM&#65289;&#25925;&#38556;&#25968;&#25454;&#38598;&#30340;3&#30456;&#30005;&#27969;&#25968;&#25454;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#24120;&#35268;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#31034;&#20986;&#22312;&#36328;&#22810;&#20010;&#26469;&#28304;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalisation of Neural Networks (NN) to multiple datasets is often overlooked in literature due to NNs typically being optimised for specific data sources. This becomes especially challenging in time-series-based multi-dataset models due to difficulties in fusing sequential data from different sensors and collection specifications. In a commercial environment, however, generalisation can effectively utilise available data and computational power, which is essential in the context of Green AI, the sustainable development of AI models. This paper introduces "Dataset Fusion," a novel dataset composition algorithm for fusing periodic signals from multiple homogeneous datasets into a single dataset while retaining unique features for generalised anomaly detection. The proposed approach, tested on a case study of 3-phase current data from 2 different homogeneous Induction Motor (IM) fault datasets using an unsupervised LSTMCaps NN, significantly outperforms conventional training approa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#20840;&#38754;&#27880;&#37322;&#30340;&#20581;&#36523;&#27963;&#21160;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#22522;&#20110;&#20687;&#32032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#21487;&#20197;&#19982;&#22522;&#20110;&#23039;&#21183;&#20272;&#35745;&#30340;&#26368;&#20808;&#36827;&#30340;&#21160;&#20316;&#35782;&#21035;&#27969;&#31243;&#30456;&#31454;&#20105;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#25903;&#25345;&#26102;&#38388;&#19978;&#30340;&#32454;&#31890;&#24230;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.08191</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#26159;&#21542;&#36275;&#20197;&#25903;&#25345;&#20581;&#36523;&#27963;&#21160;&#35782;&#21035;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is end-to-end learning enough for fitness activity recognition?. (arXiv:2305.08191v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08191
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#20840;&#38754;&#27880;&#37322;&#30340;&#20581;&#36523;&#27963;&#21160;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#22522;&#20110;&#20687;&#32032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#21487;&#20197;&#19982;&#22522;&#20110;&#23039;&#21183;&#20272;&#35745;&#30340;&#26368;&#20808;&#36827;&#30340;&#21160;&#20316;&#35782;&#21035;&#27969;&#31243;&#30456;&#31454;&#20105;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#25903;&#25345;&#26102;&#38388;&#19978;&#30340;&#32454;&#31890;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#23398;&#20064;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#19982;&#38745;&#27490;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#20248;&#21270;&#20135;&#29983;&#20102;&#38750;&#24120;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#34892;&#20026;&#35782;&#21035;&#20173;&#28982;&#20027;&#35201;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#27969;&#31243;&#25511;&#21046;&#30528;&#65292;&#21482;&#26377;&#20010;&#21035;&#32452;&#20214;&#34987;&#31070;&#32463;&#32593;&#32476;&#26367;&#25442;&#65292;&#36825;&#20123;&#32593;&#32476;&#36890;&#24120;&#21482;&#22312;&#21333;&#20010;&#24103;&#19978;&#36816;&#34892;&#12290;&#20316;&#20026;&#23545;&#36825;&#20123;&#27969;&#31243;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#30740;&#31350;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#20840;&#38754;&#27880;&#37322;&#30340;&#20581;&#36523;&#27963;&#21160;&#35270;&#39057;&#25968;&#25454;&#38598;&#12290;&#20219;&#20309;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#35782;&#21035;&#33021;&#21147;&#20960;&#20046;&#37117;&#26159;&#30001;&#20154;&#20307;&#23039;&#21183;&#21450;&#20854;&#26102;&#38388;&#21160;&#24577;&#30340;&#20989;&#25968;&#65292;&#22240;&#27492;&#22522;&#20110;&#23039;&#21183;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#20250;&#34920;&#29616;&#24471;&#24456;&#22909;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#36825;&#20123;&#26631;&#35760;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#65292;&#22522;&#20110;&#21407;&#22987;&#20687;&#32032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#21487;&#20197;&#19982;&#22522;&#20110;&#23039;&#21183;&#20272;&#35745;&#30340;&#26368;&#20808;&#36827;&#30340;&#21160;&#20316;&#35782;&#21035;&#27969;&#31243;&#30456;&#31454;&#20105;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#65292;&#31471;&#21040;&#31471;&#23398;&#20064;&#21487;&#20197;&#25903;&#25345;&#23454;&#26102;&#30340;&#37325;&#22797;&#35745;&#25968;&#31561;&#26102;&#38388;&#19978;&#30340;&#32454;&#31890;&#24230;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end learning has taken hold of many computer vision tasks, in particular, related to still images, with task-specific optimization yielding very strong performance. Nevertheless, human-centric action recognition is still largely dominated by hand-crafted pipelines, and only individual components are replaced by neural networks that typically operate on individual frames. As a testbed to study the relevance of such pipelines, we present a new fully annotated video dataset of fitness activities. Any recognition capabilities in this domain are almost exclusively a function of human poses and their temporal dynamics, so pose-based solutions should perform well. We show that, with this labelled data, end-to-end learning on raw pixels can compete with state-of-the-art action recognition pipelines based on pose estimation. We also show that end-to-end learning can support temporally fine-grained tasks such as real-time repetition counting.
&lt;/p&gt;</description></item><item><title>ResidualPlanner&#26159;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36793;&#32536;&#30340;&#30697;&#38453;&#26426;&#21046;&#65292;&#26082;&#20248;&#21270;&#21448;&#21487;&#25193;&#23637;&#65292;&#21487;&#20197;&#20248;&#21270;&#35768;&#22810;&#21487;&#20197;&#20889;&#25104;&#36793;&#38469;&#26041;&#24046;&#30340;&#20984;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.08175</link><description>&lt;p&gt;
&#19968;&#31181;&#20248;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#30697;&#38453;&#26426;&#21046;&#29992;&#20110;&#25200;&#21160;&#36793;&#32536;&#25968;&#25454;&#19979;&#20984;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Optimal and Scalable Matrix Mechanism for Noisy Marginals under Convex Loss Functions. (arXiv:2305.08175v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08175
&lt;/p&gt;
&lt;p&gt;
ResidualPlanner&#26159;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36793;&#32536;&#30340;&#30697;&#38453;&#26426;&#21046;&#65292;&#26082;&#20248;&#21270;&#21448;&#21487;&#25193;&#23637;&#65292;&#21487;&#20197;&#20248;&#21270;&#35768;&#22810;&#21487;&#20197;&#20889;&#25104;&#36793;&#38469;&#26041;&#24046;&#30340;&#20984;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25200;&#21160;&#30340;&#36793;&#32536;&#25968;&#25454;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24418;&#24335;&#65292;&#21487;&#29992;&#20110;&#35832;&#22914;&#21015;&#32852;&#34920;&#20998;&#26512;&#12289;&#36125;&#21494;&#26031;&#32593;&#32476;&#26500;&#24314;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ResidualPlanner&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#36793;&#32536;&#30340;&#30697;&#38453;&#26426;&#21046;&#65292;&#26082;&#20248;&#21270;&#21448;&#21487;&#25193;&#23637;&#12290;ResidualPlanner&#21487;&#20197;&#20248;&#21270;&#35768;&#22810;&#21487;&#20197;&#20889;&#25104;&#36793;&#38469;&#26041;&#24046;&#30340;&#20984;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;ResidualPlanner&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#20248;&#21270;&#22823;&#35268;&#27169;&#35774;&#32622;&#20013;&#30340;&#36793;&#32536;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65288;HDMM&#65289;&#20063;&#20250;&#21344;&#29992;&#36807;&#22810;&#30340;&#20869;&#23384;&#12290;&#29978;&#33267;&#22312;&#20855;&#26377;100&#20010;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#20063;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;ResidualPlanner&#36824;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#27599;&#20010;&#36793;&#32536;&#30340;&#26041;&#24046;/&#21327;&#26041;&#24046;&#20540;&#65288;&#20043;&#21069;&#30340;&#26041;&#27861;&#20250;&#24456;&#24555;&#22833;&#36133;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noisy marginals are a common form of confidentiality-protecting data release and are useful for many downstream tasks such as contingency table analysis, construction of Bayesian networks, and even synthetic data generation. Privacy mechanisms that provide unbiased noisy answers to linear queries (such as marginals) are known as matrix mechanisms.  We propose ResidualPlanner, a matrix mechanism for marginals with Gaussian noise that is both optimal and scalable. ResidualPlanner can optimize for many loss functions that can be written as a convex function of marginal variances (prior work was restricted to just one predefined objective function). ResidualPlanner can optimize the accuracy of marginals in large scale settings in seconds, even when the previous state of the art (HDMM) runs out of memory. It even runs on datasets with 100 attributes in a couple of minutes. Furthermore ResidualPlanner can efficiently compute variance/covariance values for each marginal (prior methods quickly
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#35745;&#31639;&#24310;&#36831;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#26032;&#22411;&#20107;&#20214;&#35302;&#21457;&#22120;, &#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2305.08169</link><description>&lt;p&gt;
&#23398;&#20064;&#33021;&#21542;&#38477;&#20302;&#25511;&#21046;&#65311;&#20998;&#26512;&#39640;&#26031;&#36807;&#31243;&#20107;&#20214;&#35302;&#21457;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Can Learning Deteriorate Control? Analyzing Computational Delays in Gaussian Process-Based Event-Triggered Online Learning. (arXiv:2305.08169v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#35745;&#31639;&#24310;&#36831;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#26032;&#22411;&#20107;&#20214;&#35302;&#21457;&#22120;, &#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#35745;&#31639;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31995;&#32479;&#30340;&#21160;&#24577;&#26410;&#30693;&#26102;&#65292;&#36890;&#24120;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#25512;&#26029;&#27169;&#22411;&#12290;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26159;&#19968;&#31181;&#29305;&#21035;&#27969;&#34892;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#22240;&#20026;&#23384;&#22312;&#39044;&#27979;&#35823;&#24046;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;GP&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#39640;&#25928;&#30340;&#22312;&#32447;&#26356;&#26032;&#65292;&#20174;&#32780;&#21487;&#20197;&#37319;&#29992;&#20107;&#20214;&#35302;&#21457;&#22312;&#32447;&#23398;&#20064;&#31574;&#30053;&#20197;&#30830;&#20445;&#29305;&#23450;&#30340;&#36319;&#36394;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#24120;&#35268;&#35745;&#31639;&#26102;&#38388;&#30340;&#23384;&#22312;&#65292;&#24517;&#39035;&#33021;&#22815;&#22312;&#20219;&#24847;&#26102;&#38388;&#35780;&#20272;&#29616;&#26377;&#30340;&#35302;&#21457;&#26465;&#20214;&#65292;&#32780;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#26080;&#27861;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#19968;&#20010;&#24310;&#36831;&#24863;&#30693;&#30340;&#36319;&#36394;&#35823;&#24046;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#31934;&#24230;-&#26102;&#24310;&#30340;&#25240;&#34935;&#12290;&#22522;&#20110;&#27492;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#35745;&#31639;&#24310;&#36831;&#30340;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#26032;&#22411;&#20107;&#20214;&#35302;&#21457;&#22120;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#35302;&#21457;&#22120;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;&#35745;&#31639;&#26102;&#38388;&#32780;&#35328;&#20855;&#26377;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20107;&#20214;&#35302;&#21457;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the dynamics of systems are unknown, supervised machine learning techniques are commonly employed to infer models from data. Gaussian process (GP) regression is a particularly popular learning method for this purpose due to the existence of prediction error bounds. Moreover, GP models can be efficiently updated online, such that event-triggered online learning strategies can be pursued to ensure specified tracking accuracies. However, existing trigger conditions must be able to be evaluated at arbitrary times, which cannot be achieved in practice due to non-negligible computation times. Therefore, we first derive a delay-aware tracking error bound, which reveals an accuracy-delay trade-off. Based on this result, we propose a novel event trigger for GP-based online learning with computational delays, which we show to offer advantages over offline trained GP models for sufficiently small computation times. Finally, we demonstrate the effectiveness of the proposed event trigger for o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MuLTI&#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#21644;&#25490;&#21015;&#26426;&#21046;&#26469;&#20174;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35782;&#21035;&#28508;&#22312;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.08164</link><description>&lt;p&gt;
&#20174;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#28508;&#22312;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Latent Processes Identification From Multi-View Time Series. (arXiv:2305.08164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MuLTI&#30340;&#26032;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#21644;&#25490;&#21015;&#26426;&#21046;&#26469;&#20174;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35782;&#21035;&#28508;&#22312;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21160;&#24577;&#36890;&#24120;&#38656;&#35201;&#35782;&#21035;&#25968;&#25454;&#29983;&#25104;&#30340;&#21807;&#19968;&#28508;&#22312;&#22240;&#32032;&#65292;&#21363;&#28508;&#22312;&#36807;&#31243;&#35782;&#21035;&#12290;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#65288;i&#65289;&#22797;&#26434;&#30340;&#25968;&#25454;&#32467;&#26500;&#65288;&#22914;&#26102;&#38388;&#20381;&#36182;&#24615;&#65289;&#21487;&#33021;&#23548;&#33268;&#29420;&#31435;&#20551;&#35774;&#30340;&#36829;&#21453;&#65292;&#65288;ii&#65289;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#22240;&#32032;&#36890;&#24120;&#26159;&#37325;&#21472;&#30340;&#19988;&#38590;&#20197;&#32858;&#21512;&#20026;&#23436;&#25972;&#30340;&#38598;&#21512;&#65292;&#22240;&#27492;&#65292;&#23558;&#29616;&#26377;&#24037;&#20316;&#25193;&#23637;&#21040;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;MuLTI&#65292;&#23427;&#37319;&#29992;&#20102;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#26469;&#21453;&#28436;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20197;&#22686;&#24378;&#21487;&#35782;&#21035;&#24615;&#12290;&#27492;&#22806;&#65292;MuLTI&#38598;&#25104;&#20102;&#19968;&#20010;&#25490;&#21015;&#26426;&#21046;&#65292;&#36890;&#36807;&#24314;&#31435;&#26368;&#20248;&#20256;&#36755;&#20844;&#24335;&#26469;&#21512;&#24182;&#30456;&#24212;&#30340;&#37325;&#21472;&#21464;&#37327;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MuLTI&#22312;&#20174;&#22810;&#35270;&#35282;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35782;&#21035;&#28508;&#22312;&#36807;&#31243;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the dynamics of time series data typically requires identifying the unique latent factors for data generation, \textit{a.k.a.}, latent processes identification. Driven by the independent assumption, existing works have made great progress in handling single-view data. However, it is a non-trivial problem that extends them to multi-view time series data because of two main challenges: (i) the complex data structure, such as temporal dependency, can result in violation of the independent assumption; (ii) the factors from different views are generally overlapped and are hard to be aggregated to a complete set. In this work, we propose a novel framework MuLTI that employs the contrastive learning technique to invert the data generative process for enhanced identifiability. Additionally, MuLTI integrates a permutation mechanism that merges corresponding overlapped variables by the establishment of an optimal transport formula. Extensive experimental results on synthetic and re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32422;&#26463;&#24674;&#22797;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#38024;&#23545;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#12290;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#25351;&#25968;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;</title><link>http://arxiv.org/abs/2305.08130</link><description>&lt;p&gt;
&#24102;&#26377;&#32422;&#26463;&#24674;&#22797;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning With Constraint Recovery. (arXiv:2305.08130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32422;&#26463;&#24674;&#22797;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#38024;&#23545;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#12290;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#25351;&#25968;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#30340;&#26032;&#22411;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#31639;&#27861;&#12290;&#22312;&#26631;&#20934;&#30340;IRL&#38382;&#39064;&#20013;&#65292;&#36870;&#23398;&#20064;&#32773;&#25110;&#20195;&#29702;&#20154;&#23547;&#27714;&#24674;&#22797;MDP&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#32473;&#23450;&#26368;&#20248;&#31574;&#30053;&#30340;&#36712;&#36857;&#28436;&#31034;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19981;&#20165;&#23547;&#27714;&#25512;&#26029;CMDP&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36824;&#35201;&#25512;&#26029;&#32422;&#26463;&#12290;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#25105;&#20204;&#34920;&#26126;IRL&#19982;&#32422;&#26463;&#24674;&#22797;&#65288;IRL-CR&#65289;&#38382;&#39064;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#20854;&#20943;&#23569;&#20026;&#20132;&#26367;&#21463;&#38480;&#21046;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#23376;&#38382;&#39064;&#26159;&#20984;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel inverse reinforcement learning (IRL) algorithm for constrained Markov decision process (CMDP) problems. In standard IRL problems, the inverse learner or agent seeks to recover the reward function of the MDP, given a set of trajectory demonstrations for the optimal policy. In this work, we seek to infer not only the reward functions of the CMDP, but also the constraints. Using the principle of maximum entropy, we show that the IRL with constraint recovery (IRL-CR) problem can be cast as a constrained non-convex optimization problem. We reduce it to an alternating constrained optimization problem whose sub-problems are convex. We use exponentiated gradient descent algorithm to solve it. Finally, we demonstrate the efficacy of our algorithm for the grid world environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20803;&#25968;&#25454;&#21644;&#37319;&#29992;&#22810;&#27169;&#22411;&#38598;&#25104;&#25216;&#26415;&#26469;&#35299;&#20915;OTT&#23186;&#20307;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#22810;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08120</link><description>&lt;p&gt;
&#38024;&#23545;OTT&#23186;&#20307;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#20919;&#21551;&#21160;&#38590;&#39064;&#65306;&#20803;&#27934;&#23519;&#21644;&#22810;&#27169;&#24577;&#38598;&#25104;&#25484;&#25569;&#30340;&#25581;&#31034;
&lt;/p&gt;
&lt;p&gt;
Unraveling Cold Start Enigmas in Predictive Analytics for OTT Media: Synergistic Meta-Insights and Multimodal Ensemble Mastery. (arXiv:2305.08120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20803;&#25968;&#25454;&#21644;&#37319;&#29992;&#22810;&#27169;&#22411;&#38598;&#25104;&#25216;&#26415;&#26469;&#35299;&#20915;OTT&#23186;&#20307;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#22810;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20919;&#21551;&#21160;&#38382;&#39064;&#26159;&#21508;&#20010;&#39046;&#22495;&#24120;&#35265;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#23186;&#20307;&#24212;&#29992;&#26696;&#20363;&#65292;&#27604;&#22914;&#39044;&#27979;&#22312;Over-The-Top&#65288;OTT&#65289;&#24179;&#21488;&#19978;&#26032;&#25512;&#20986;&#33410;&#30446;&#30340;&#25910;&#35270;&#29575;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20803;&#25968;&#25454;&#21644;&#37319;&#29992;&#22810;&#27169;&#22411;&#38598;&#25104;&#25216;&#26415;&#26469;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#29305;&#24449;&#24037;&#31243;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#22522;&#20110;&#39044;&#27979;&#21152;&#26435;&#24179;&#22343;&#20540;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#24615;&#33021;&#24230;&#37327;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21333;&#20010;&#27169;&#22411;&#30456;&#27604;&#65292;&#22810;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cold start problem is a common challenge in various domains, including media use cases such as predicting viewership for newly launched shows on Over-The-Top (OTT) platforms. In this study, we propose a generic approach to tackle cold start problems by leveraging metadata and employing multi-model ensemble techniques. Our methodology includes feature engineering, model selection, and an ensemble approach based on a weighted average of predictions. The performance of our proposed method is evaluated using various performance metrics. Our results indicate that the multi-model ensemble approach significantly improves prediction accuracy compared to individual models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26368;&#21487;&#33021;&#20986;&#38169;&#30340;&#35266;&#23519;&#32467;&#26524;&#20998;&#31163;&#25104;&#8220;&#27880;&#24847;&#38598;&#8221;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35786;&#26029;&#21644;&#25913;&#36827;&#12290;&#37319;&#29992;&#22522;&#20110;&#29305;&#24449;&#20999;&#29255;&#30340;&#31574;&#30053;&#65292;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12289;&#27169;&#22411;&#26080;&#20851;&#24615;&#21644;&#36739;&#23569;&#30340;&#36741;&#21161;&#36755;&#20837;&#25110;&#30693;&#35782;&#65292;&#32780;&#19988;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.08115</link><description>&lt;p&gt;
&#33258;&#21160;&#20135;&#29983;&#27880;&#24847;&#35268;&#21017;&#20197;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Automatic Generation of Attention Rules For Containment of Machine Learning Model Errors. (arXiv:2305.08115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26368;&#21487;&#33021;&#20986;&#38169;&#30340;&#35266;&#23519;&#32467;&#26524;&#20998;&#31163;&#25104;&#8220;&#27880;&#24847;&#38598;&#8221;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35786;&#26029;&#21644;&#25913;&#36827;&#12290;&#37319;&#29992;&#22522;&#20110;&#29305;&#24449;&#20999;&#29255;&#30340;&#31574;&#30053;&#65292;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12289;&#27169;&#22411;&#26080;&#20851;&#24615;&#21644;&#36739;&#23569;&#30340;&#36741;&#21161;&#36755;&#20837;&#25110;&#30693;&#35782;&#65292;&#32780;&#19988;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#21830;&#19994;&#32423;&#21035;&#19978;&#20351;&#29992;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#23558;&#27169;&#22411;&#30340;&#35823;&#24046;&#29575;&#32500;&#25345;&#22312;&#21487;&#25509;&#21463;&#30340;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#23558;&#26368;&#26377;&#21487;&#33021;&#20986;&#29616;&#38169;&#35823;&#39044;&#27979;&#30340;&#35266;&#23519;&#32467;&#26524;&#20998;&#31163;&#20026;&#8220;&#27880;&#24847;&#38598;&#8221;&#65292;&#36825;&#20123;&#38598;&#21512;&#21487;&#20197;&#30452;&#25509;&#24110;&#21161;&#27169;&#22411;&#35786;&#26029;&#21644;&#25913;&#36827;&#65292;&#24182;&#29992;&#20110;&#20915;&#23450;&#36825;&#20123;&#38382;&#39064;&#35266;&#23519;&#32467;&#26524;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20960;&#31181;&#29992;&#20110;&#30830;&#23450;&#26368;&#20339;&#35268;&#21017;&#20197;&#20998;&#31163;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#30340;&#31639;&#27861;&#65288;&#8220;&#31574;&#30053;&#8221;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#37319;&#29992;&#22522;&#20110;&#29305;&#24449;&#20999;&#29255;&#30340;&#31574;&#30053;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12289;&#27169;&#22411;&#26080;&#20851;&#24615;&#65292;&#24182;&#38656;&#35201;&#26368;&#23569;&#30340;&#36741;&#21161;&#36755;&#20837;&#25110;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) solutions are prevalent in many applications. However, many challenges exist in making these solutions business-grade. For instance, maintaining the error rate of the underlying ML models at an acceptably low level. Typically, the true relationship between feature inputs and the target feature to be predicted is uncertain, and hence statistical in nature. The approach we propose is to separate the observations that are the most likely to be predicted incorrectly into 'attention sets'. These can directly aid model diagnosis and improvement, and be used to decide on alternative courses of action for these problematic observations. We present several algorithms (`strategies') for determining optimal rules to separate these observations. In particular, we prefer strategies that use feature-based slicing because they are human-interpretable, model-agnostic, and require minimal supplementary inputs or knowledge. In addition, we show that these strategies outperform seve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#36827;&#34892;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#20445;&#25345;&#25968;&#25454;&#31169;&#23494;&#21644;&#23433;&#20840;&#12290;&#25991;&#31456;&#23545;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#22411;&#27867;&#21270;&#31561;&#25216;&#26415;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#32456;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.08107</link><description>&lt;p&gt;
&#24179;&#34913;&#20986;&#31199;&#36710;&#26102;&#31354;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#29992;&#20110;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Balancing Privacy and Utility of Spatio-Temporal Data for Taxi-Demand Prediction. (arXiv:2305.08107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#36827;&#34892;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#20445;&#25345;&#25968;&#25454;&#31169;&#23494;&#21644;&#23433;&#20840;&#12290;&#25991;&#31456;&#23545;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#22411;&#27867;&#21270;&#31561;&#25216;&#26415;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#32456;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#23427;&#21487;&#20197;&#20351;&#20986;&#31199;&#36710;&#25552;&#20379;&#35774;&#26045;&#20248;&#21270;&#20854;&#36816;&#33829;&#65292;&#22478;&#24066;&#35268;&#21010;&#32773;&#25913;&#21892;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#21644;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20013;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#24341;&#21457;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#23398;&#20064;&#36827;&#34892;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#65292;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#20445;&#25345;&#25968;&#25454;&#31169;&#23494;&#21644;&#23433;&#20840;&#30340;&#21516;&#26102;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#20351;&#32452;&#32455;&#22312;&#19981;&#24471;&#20197;&#33719;&#21462;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#27169;&#22411;&#12290;&#23613;&#31649;&#32852;&#21512;&#23398;&#20064;&#20026;&#20986;&#31199;&#36710;&#38656;&#27714;&#39044;&#27979;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#30410;&#22788;&#65292;&#20294;&#23427;&#20063;&#38754;&#20020;&#30528;&#19968;&#20123;&#25216;&#26415;&#25361;&#25112;&#65292;&#20363;&#22914;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#26576;&#20123;&#21442;&#19982;&#26041;&#30340;&#25968;&#25454;&#31232;&#32570;&#20197;&#21450;&#38656;&#35201;&#30830;&#20445;&#27169;&#22411;&#27867;&#21270;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#35774;&#26045;&#21644;&#22320;&#29702;&#21306;&#22495;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#21306;&#22495;&#26080;&#20851;&#30340;&#32534;&#30721;&#36827;&#34892;&#22320;&#29702;&#29305;&#24449;&#22788;&#29702;&#65292;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#26469;&#24179;&#34913;&#25968;&#25454;&#31232;&#32570;&#65292;&#20197;&#21450;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;&#26102;&#31354;&#20986;&#31199;&#36710;&#38656;&#27714;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#36798;&#21040;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxi-demand prediction is an important application of machine learning that enables taxi-providing facilities to optimize their operations and city planners to improve transportation infrastructure and services. However, the use of sensitive data in these systems raises concerns about privacy and security. In this paper, we propose the use of federated learning for taxi-demand prediction that allows multiple parties to train a machine learning model on their own data while keeping the data private and secure. This can enable organizations to build models on data they otherwise would not be able to access. Despite its potential benefits, federated learning for taxi-demand prediction poses several technical challenges, such as class imbalance, data scarcity among some parties, and the need to ensure model generalization to accommodate diverse facilities and geographic regions. To effectively address these challenges, we propose a system that utilizes region-independent encoding for geogr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21306;&#22359;&#38142;&#20132;&#26131;&#36153;&#29992;&#39044;&#27979;&#65292;&#27604;&#36739;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#23567;&#27874;&#38408;&#20540;&#21435;&#22122;&#21644;&#30697;&#38453;&#21078;&#38754;&#25968;&#25454;&#22788;&#29702;&#65292;&#39044;&#27979;&#22810;&#20010;&#26102;&#38388;&#36328;&#24230;&#19978;&#30340;5&#20998;&#38047;&#20869;&#26368;&#20302;&#29028;&#27668;&#20215;&#26684;&#65292;&#20026;&#20132;&#26131;&#36153;&#29992;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08105</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#20132;&#26131;&#36153;&#29992;&#39044;&#27979;&#65306;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Blockchain Transaction Fee Forecasting: A Comparison of Machine Learning Methods. (arXiv:2305.08105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21306;&#22359;&#38142;&#20132;&#26131;&#36153;&#29992;&#39044;&#27979;&#65292;&#27604;&#36739;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#23567;&#27874;&#38408;&#20540;&#21435;&#22122;&#21644;&#30697;&#38453;&#21078;&#38754;&#25968;&#25454;&#22788;&#29702;&#65292;&#39044;&#27979;&#22810;&#20010;&#26102;&#38388;&#36328;&#24230;&#19978;&#30340;5&#20998;&#38047;&#20869;&#26368;&#20302;&#29028;&#27668;&#20215;&#26684;&#65292;&#20026;&#20132;&#26131;&#36153;&#29992;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29028;&#27668;&#26159;&#20197;&#22826;&#22346;&#32593;&#32476;&#30340;&#20132;&#26131;&#36153;&#35745;&#37327;&#31995;&#32479;&#12290;&#32593;&#32476;&#30340;&#29992;&#25143;&#38656;&#35201;&#36873;&#25321;&#19968;&#20010;&#29028;&#27668;&#20215;&#26684;&#26469;&#25552;&#20132;&#20182;&#20204;&#30340;&#20132;&#26131;&#65292;&#36825;&#20250;&#22686;&#21152;&#36807;&#39640;&#25903;&#20184;&#30340;&#39118;&#38505;&#65292;&#25110;&#32773;&#23548;&#33268;&#36873;&#25321;&#24310;&#36831;/&#26410;&#22788;&#29702;&#30340;&#20132;&#26131;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20262;&#25958;&#30828;&#20998;&#21449;&#21518;&#30340;&#25968;&#25454;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#35813;&#20998;&#21449;&#21518;&#32593;&#32476;&#30340;&#20132;&#26131;&#21160;&#24577;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;2019&#24180;&#20043;&#21069;&#20851;&#20110;EthUSD BitUSD&#21644;&#29028;&#27668;&#20215;&#26684;&#20043;&#38388;&#38142;&#25509;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#26356;&#26032;&#12290;&#20026;&#39044;&#27979;&#29028;&#27668;&#20215;&#26684;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#20123;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#30452;&#25509;&#36882;&#24402;&#28151;&#21512;LSTM&#12289;CNNLSTM&#21644;&#27880;&#24847;&#21147;LSTM&#31561;&#65292;&#32467;&#21512;&#23567;&#27874;&#38408;&#20540;&#21435;&#22122;&#21644;&#30697;&#38453;&#21078;&#38754;&#25968;&#25454;&#22788;&#29702;&#65292;&#39044;&#27979;&#22810;&#20010;&#26102;&#38388;&#36328;&#24230;&#19978;&#30340;5&#20998;&#38047;&#20869;&#26368;&#20302;&#29028;&#27668;&#20215;&#26684;&#12290;&#20316;&#20026;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#27425;&#23558;&#30697;&#38453;&#21078;&#38754;&#24212;&#29992;&#20110;&#29028;&#27668;&#20215;&#26684;&#25968;&#25454;&#21644;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#30697;&#38453;&#37197;&#32622;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#20132;&#26131;&#36153;&#29992;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gas is the transaction-fee metering system of the Ethereum network. Users of the network are required to select a gas price for submission with their transaction, creating a risk of overpaying or delayed/unprocessed transactions in this selection. In this work, we investigate data in the aftermath of the London Hard Fork and shed insight into the transaction dynamics of the net-work after this major fork. As such, this paper provides an update on work previous to 2019 on the link between EthUSD BitUSD and gas price. For forecasting, we compare a novel combination of machine learning methods such as Direct Recursive Hybrid LSTM, CNNLSTM, and Attention LSTM. These are combined with wavelet threshold denoising and matrix profile data processing toward the forecasting of block minimum gas price, on a 5-min timescale, over multiple lookaheads. As the first application of the matrix profile being applied to gas price data and forecasting we are aware of, this study demonstrates that matrix p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861; QFedTD&#65292;&#22312;&#26377;&#38480;&#36895;&#25273;&#36890;&#36947;&#19979;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#20197;&#36798;&#21040;&#32447;&#24615;&#21152;&#36895;&#30340;&#25928;&#26524;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.08104</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#36895;&#25273;&#36890;&#36947;&#30340;&#32852;&#37030; TD &#23398;&#20064;&#65306;&#39532;&#23572;&#21487;&#22827;&#37319;&#26679;&#19979;&#30340;&#32447;&#24615;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Federated TD Learning over Finite-Rate Erasure Channels: Linear Speedup under Markovian Sampling. (arXiv:2305.08104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#31639;&#27861; QFedTD&#65292;&#22312;&#26377;&#38480;&#36895;&#25273;&#36890;&#36947;&#19979;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#20197;&#36798;&#21040;&#32447;&#24615;&#21152;&#36895;&#30340;&#25928;&#26524;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#22312;&#36890;&#20449;&#21644;&#38544;&#31169;&#32422;&#26463;&#19979;&#21152;&#36895;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064; (FL)&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#21152;&#36895;&#65292;&#22312;&#29702;&#35770;&#19978;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#38024;&#23545;&#36825;&#20010;&#26041;&#21521;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#37030;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#22312;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20013;&#22830;&#32858;&#21512;&#22120;&#36827;&#34892;&#36890;&#20449;&#65292;&#20197;&#21152;&#24555;&#20849;&#21516;&#31574;&#30053;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#25429;&#25417; FL &#20013;&#30340;&#20856;&#22411;&#36890;&#20449;&#32422;&#26463;&#65292;&#25105;&#20204;&#32771;&#34385;&#21487;&#20197;&#26681;&#25454;&#20271;&#21162;&#21033;&#25830;&#25325;&#27169;&#22411;&#20002;&#24323;&#25968;&#25454;&#21253;&#30340;&#26377;&#38480;&#23481;&#37327;&#19978;&#34892;&#38142;&#36335;&#36890;&#36947;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102; QFedTD-&#19968;&#31181;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#37327;&#21270;&#32852;&#37030;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#25552;&#20379; QFedTD &#30340;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#65292;&#35813;&#20998;&#26512;&#31361;&#20986;&#20102;&#37327;&#21270;&#21644;&#25273;&#38500;&#23545;&#25910;&#25947;&#36895;&#29575;&#30340;&#24433;&#21709;&#65307;&#24182;&#24314;&#31435;&#20102;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#30340;&#21152;&#36895;&#24230;&#12290;(&#32763;&#35793;&#20165;&#20379;&#21442;&#32771;&#65292;&#19981;&#20195;&#34920;&#36798;&#24847;&#23436;&#20840;&#27491;&#30830;)
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has recently gained much attention due to its effectiveness in speeding up supervised learning tasks under communication and privacy constraints. However, whether similar speedups can be established for reinforcement learning remains much less understood theoretically. Towards this direction, we study a federated policy evaluation problem where agents communicate via a central aggregator to expedite the evaluation of a common policy. To capture typical communication constraints in FL, we consider finite capacity up-link channels that can drop packets based on a Bernoulli erasure model. Given this setting, we propose and analyze QFedTD - a quantized federated temporal difference learning algorithm with linear function approximation. Our main technical contribution is to provide a finite-sample analysis of QFedTD that (i) highlights the effect of quantization and erasures on the convergence rate; and (ii) establishes a linear speedup w.r.t. the number of agents un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26412;&#26500;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#30340;&#25429;&#25417;&#19982;&#36895;&#29575;&#30456;&#20851;&#30340;&#24212;&#21147;-&#24212;&#21464;&#20851;&#31995;&#21644;&#19968;&#33268;&#30340;&#20999;&#32447;&#27169;&#37327;&#65292;&#20197;&#30740;&#31350;&#21547;&#28287;&#24230;&#32435;&#31859;&#39063;&#31890;/&#29615;&#27687;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#30340;&#24490;&#29615;&#31896;&#24377;-&#31896;&#22609;&#24615;-&#30772;&#22351;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.08102</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21547;&#28287;&#24230;&#29615;&#27687;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#31896;&#24377;-&#31896;&#22609;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A machine learning-based viscoelastic-viscoplastic model for epoxy nanocomposites with moisture content. (arXiv:2305.08102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26412;&#26500;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#30340;&#25429;&#25417;&#19982;&#36895;&#29575;&#30456;&#20851;&#30340;&#24212;&#21147;-&#24212;&#21464;&#20851;&#31995;&#21644;&#19968;&#33268;&#30340;&#20999;&#32447;&#27169;&#37327;&#65292;&#20197;&#30740;&#31350;&#21547;&#28287;&#24230;&#32435;&#31859;&#39063;&#31890;/&#29615;&#27687;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#30340;&#24490;&#29615;&#31896;&#24377;-&#31896;&#22609;&#24615;-&#30772;&#22351;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26412;&#26500;&#27169;&#22411;&#65292;&#29992;&#20110;&#30740;&#31350;&#21547;&#28287;&#24230;&#32435;&#31859;&#39063;&#31890;/&#29615;&#27687;&#32435;&#31859;&#22797;&#21512;&#26448;&#26009;&#30340;&#24490;&#29615;&#31896;&#24377;-&#31896;&#22609;&#24615;-&#30772;&#22351;&#34892;&#20026;&#12290;&#37319;&#29992;&#19968;&#31181;&#37319;&#26679;&#25216;&#26415;&#21644;&#25200;&#21160;&#26041;&#27861;&#30340;&#32452;&#21512;&#26694;&#26550;&#35757;&#32451;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65292;&#23558;&#23454;&#39564;&#39564;&#35777;&#30340;&#31896;&#24377;-&#31896;&#22609;&#24615;&#27169;&#22411;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#21040;&#19982;&#21152;&#36733;&#36895;&#29575;&#30456;&#20851;&#30340;&#24212;&#21147;-&#24212;&#21464;&#20851;&#31995;&#21644;&#19968;&#33268;&#30340;&#20999;&#32447;&#27169;&#37327;&#12290;&#27492;&#22806;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26412;&#26500;&#27169;&#22411;&#23454;&#29616;&#21040;&#26377;&#38480;&#20803;&#20998;&#26512;&#20013;&#12290;&#36890;&#36807;&#26377;&#38480;&#20803;&#27169;&#25311;&#30740;&#31350;&#20102;&#21152;&#36733;&#36895;&#29575;&#21644;&#28287;&#24230;&#21547;&#37327;&#23545;&#32435;&#31859;&#39063;&#31890;/&#29615;&#27687;&#26679;&#21697;&#30340;&#21147;-&#20301;&#31227;&#21709;&#24212;&#30340;&#24433;&#21709;&#12290;&#25968;&#20540;&#23454;&#20363;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#21462;&#20915;&#20110;&#21152;&#36733;&#26465;&#20214;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26412;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a deep learning (DL)-based constitutive model for investigating the cyclic viscoelastic-viscoplastic-damage behavior of nanoparticle/epoxy nanocomposites with moisture content. For this, a long short-term memory network is trained using a combined framework of a sampling technique and a perturbation method. The training framework, along with the training data generated by an experimentally validated viscoelastic-viscoplastic model, enables the DL model to accurately capture the rate-dependent stress-strain relationship and consistent tangent moduli. In addition, the DL-based constitutive model is implemented into finite element analysis. Finite element simulations are performed to study the effect of load rate and moisture content on the force-displacement response of nanoparticle/ epoxy samples. Numerical examples show that the computational efficiency of the DL model depends on the loading condition and is significantly higher than the conventional constituti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#24179;&#22343;&#23884;&#20837;&#38382;&#39064;&#65292;&#26500;&#36896;&#20102;&#38750;&#32447;&#24615;&#25968;&#25454;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#20351;&#29992;&#27491;&#23450;&#26680;&#30340;&#20984;&#38598;&#65292;&#24471;&#21040;&#22810;&#31181;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21644;&#29305;&#24449;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.08100</link><description>&lt;p&gt;
&#26465;&#20214;&#24179;&#22343;&#23884;&#20837;&#21644;&#36890;&#36807;&#27491;&#23450;&#26680;&#30340;&#26368;&#20248;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Conditional mean embeddings and optimal feature selection via positive definite kernels. (arXiv:2305.08100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#24179;&#22343;&#23884;&#20837;&#38382;&#39064;&#65292;&#26500;&#36896;&#20102;&#38750;&#32447;&#24615;&#25968;&#25454;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#20351;&#29992;&#27491;&#23450;&#26680;&#30340;&#20984;&#38598;&#65292;&#24471;&#21040;&#22810;&#31181;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21644;&#29305;&#24449;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#24179;&#22343;&#23884;&#20837;&#38382;&#39064;&#12290;&#22522;&#20110;&#35889;&#20998;&#26512;&#20248;&#21270;&#31639;&#27861;&#21644;&#26680;&#12289;&#38543;&#26426;&#36807;&#31243;&#20197;&#21450;&#24314;&#35774;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#32452;&#21512;&#32467;&#26524;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#23545;&#20110;&#38750;&#32447;&#24615;&#25968;&#25454;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#20351;&#29992;&#27491;&#23450;&#26680;&#30340;&#20984;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#36873;&#25321;&#26368;&#20248;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20174;&#32780;&#24471;&#21040;&#22810;&#31181;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21644;&#29305;&#24449;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#26032;&#24819;&#27861;&#26159;&#65292;&#25105;&#20204;&#20801;&#35768;&#20174;&#27491;&#23450;&#26680;&#30340;&#20984;&#38598;&#20013;&#36873;&#25321;&#19968;&#32452;&#26680;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#36827;&#34892;&#20108;&#27425;&#20248;&#21270;&#65292;&#20197;&#33719;&#24471;&#8220;&#26368;&#20339;&#8221;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by applications, we consider here new operator theoretic approaches to Conditional mean embeddings (CME). Our present results combine a spectral analysis-based optimization scheme with the use of kernels, stochastic processes, and constructive learning algorithms. For initially given non-linear data, we consider optimization-based feature selections. This entails the use of convex sets of positive definite (p.d.) kernels in a construction of optimal feature selection via regression algorithms from learning models. Thus, with initial inputs of training data (for a suitable learning algorithm,) each choice of p.d. kernel $K$ in turn yields a variety of Hilbert spaces and realizations of features. A novel idea here is that we shall allow an optimization over selected sets of kernels $K$ from a convex set $C$ of positive definite kernels $K$. Hence our \textquotedblleft optimal\textquotedblright{} choices of feature representations will depend on a secondary optimization over p.d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2305.08099</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#35299;&#32806;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#24050;&#32463;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22312;&#20302;&#26631;&#27880;&#36164;&#28304;&#24773;&#20917;&#19979;&#35777;&#26126;&#38750;&#24120;&#26377;&#29992;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#25216;&#26415;&#22312;&#35828;&#35805;&#20154;&#12289;&#24773;&#24863;&#21644;&#35821;&#35328;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have demonstrated state-of-the-art performance on automatic speech recognition (ASR) and proved to be extremely useful in low label-resource settings. However, the success of SSL models has yet to transfer to utterance-level tasks such as speaker, emotion, and language recognition, which still require supervised fine-tuning of the SSL models to obtain good performance. We argue that the problem is caused by the lack of disentangled representations and an utterance-level learning objective for these tasks. Inspired by how HuBERT uses clustering to discover hidden acoustic units, we formulate a factor analysis (FA) model that uses the discovered hidden acoustic units to align the SSL features. The underlying utterance-level representations are disentangled from the content of speech using probabilistic inference on the aligned features. Furthermore, the variational lower bound derived from the FA model provides an ut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#22788;&#29702;&#27169;&#22359;Meta-DM&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#20013;&#37117;&#33021;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.08092</link><description>&lt;p&gt;
Meta-DM&#65306;&#25193;&#25955;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Meta-DM: Applications of Diffusion Models on Few-Shot Learning. (arXiv:2305.08092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#22788;&#29702;&#27169;&#22359;Meta-DM&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#20013;&#37117;&#33021;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#65292;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#25913;&#36827;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#22788;&#29702;&#27169;&#22359;&#30340;&#20316;&#29992;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#36890;&#29992;&#25968;&#25454;&#22788;&#29702;&#27169;&#22359;Meta-DM&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#12290;Meta-DM&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#20013;&#37117;&#33021;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#23545;Meta-DM&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#22312;&#20960;&#31181;&#31639;&#27861;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;Meta-DM&#19982;&#26576;&#20123;&#26041;&#27861;&#30456;&#32467;&#21512;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of few-shot learning (FSL), extensive research has focused on improving network structures and training strategies. However, the role of data processing modules has not been fully explored. Therefore, in this paper, we propose Meta-DM, a generalized data processing module for FSL problems based on diffusion models. Meta-DM is a simple yet effective module that can be easily integrated with existing FSL methods, leading to significant performance improvements in both supervised and unsupervised settings. We provide a theoretical analysis of Meta-DM and evaluate its performance on several algorithms. Our experiments show that combining Meta-DM with certain methods achieves state-of-the-art results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#12289;&#20934;&#30830;&#19988;&#32463;&#27982;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#19968;&#20010;&#22810;&#30446;&#26631;&#20223;&#30495;&#27169;&#22411;&#65292;&#29992;&#20110;&#26234;&#33021;&#20303;&#23429;&#20013;&#30340;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;&#65288;DRP&#65289;&#65292;&#20197;&#25552;&#39640;&#23621;&#27665;&#30340;&#32463;&#27982;&#25928;&#30410;&#21644;&#28909;&#33298;&#36866;&#12290;</title><link>http://arxiv.org/abs/2305.08077</link><description>&lt;p&gt;
&#32771;&#34385;&#20303;&#25143;&#28909;&#33298;&#36866;&#21644;&#38544;&#31169;&#30340;&#20303;&#23429;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;&#25104;&#26412;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization of Residential Demand Response Program Cost with Consideration for Occupants Thermal Comfort and Privacy. (arXiv:2305.08077v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#12289;&#20934;&#30830;&#19988;&#32463;&#27982;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#19968;&#20010;&#22810;&#30446;&#26631;&#20223;&#30495;&#27169;&#22411;&#65292;&#29992;&#20110;&#26234;&#33021;&#20303;&#23429;&#20013;&#30340;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;&#65288;DRP&#65289;&#65292;&#20197;&#25552;&#39640;&#23621;&#27665;&#30340;&#32463;&#27982;&#25928;&#30410;&#21644;&#28909;&#33298;&#36866;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23621;&#27665;&#20351;&#29992;&#23478;&#24237;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#65288;HEMS&#65289;&#26102;&#65292;&#21487;&#20197;&#21033;&#29992;&#38656;&#27714;&#21709;&#24212;&#35745;&#21010;&#65288;DRP&#65289;&#26469;&#38477;&#20302;&#33021;&#28304;&#25104;&#26412;&#65292;&#36890;&#36807;&#33258;&#21160;&#35843;&#25972;&#31354;&#35843;&#35774;&#23450;&#28201;&#24230;&#21644;&#23558;&#26576;&#20123;&#30005;&#22120;&#36716;&#31227;&#21040;&#20302;&#23792;&#26102;&#27573;&#12290;&#22914;&#26524;HEMS&#20102;&#35299;&#23621;&#20303;&#29366;&#24577;&#65292;&#23621;&#27665;&#21487;&#20197;&#33719;&#24471;&#26356;&#22810;&#32463;&#27982;&#25928;&#30410;&#21644;&#28909;&#33298;&#36866;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24314;&#31569;&#29289;&#30340;&#23621;&#20303;&#29366;&#24577;&#65292;&#30452;&#25509;&#20256;&#24863;&#25104;&#26412;&#39640;&#12289;&#20934;&#30830;&#24615;&#20302;&#19988;&#23545;&#23621;&#27665;&#20855;&#26377;&#20405;&#20837;&#24615;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#31639;&#27861;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#12289;&#20934;&#30830;&#19988;&#32463;&#27982;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#19968;&#20010;&#22810;&#30446;&#26631;&#20223;&#30495;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#26234;&#33021;&#20303;&#23429;&#20013;&#30340;DRP&#65292;&#20854;&#20013;&#21253;&#25324;&#65288;a&#65289;&#30005;&#21147;&#36127;&#33655;&#38656;&#27714;&#20943;&#23569;&#65292;&#65288;b&#65289;&#35843;&#25972;&#28909;&#33298;&#36866;&#65288;&#31354;&#35843;&#65289;&#28201;&#24230;&#35774;&#23450;&#28857;&#65292;&#20197;&#21450;&#65288;c&#65289;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20445;&#23432;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residential consumers can use the demand response program (DRP) if they can utilize the home energy management system (HEMS), which reduces consumer costs by automatically adjusting air conditioning (AC) setpoints and shifting some appliances to off-peak hours. If HEMS knows occupancy status, consumers can gain more economic benefits and thermal comfort. However, for the building occupancy status, direct sensing is costly, inaccurate, and intrusive for residents. So, forecasting algorithms could serve as an effective alternative. The goal of this study is to present a non-intrusive, accurate, and cost-effective approach, to develop a multi-objective simulation model for the application of DRPs in a smart residential house, where (a) electrical load demand reduction, (b) adjustment in thermal comfort (AC) temperature setpoints, and (c) , worst cases scenario approach is very conservative. Because that is unlikely all uncertain parameters take their worst values at all times. So, the fle
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38416;&#36848;&#20102;&#23545;&#25239;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23041;&#32961;&#65292;&#20171;&#32461;&#20102;&#38450;&#24481;&#24615;&#33976;&#39311;&#30340;&#26041;&#27861;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#25945;&#24072;&#21161;&#25163;&#26469;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08076</link><description>&lt;p&gt;
&#21033;&#29992;&#25945;&#24072;&#21161;&#25163;&#25552;&#39640;&#38450;&#24481;&#24615;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Improving Defensive Distillation using Teacher Assistant. (arXiv:2305.08076v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08076
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38416;&#36848;&#20102;&#23545;&#25239;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23041;&#32961;&#65292;&#20171;&#32461;&#20102;&#38450;&#24481;&#24615;&#33976;&#39311;&#30340;&#26041;&#27861;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#25945;&#24072;&#21161;&#25163;&#26469;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#23545;&#29616;&#20195;&#24212;&#29992;&#20013;&#24212;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#22312;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#65292;&#19987;&#23478;&#21487;&#20197;&#21033;&#29992;&#27169;&#22411;&#32467;&#26500;&#30340;&#30693;&#35782;&#26469;&#21019;&#24314;&#20154;&#31867;&#35270;&#35273;&#26080;&#27861;&#23519;&#35273;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#36825;&#20123;&#25915;&#20987;&#21487;&#33021;&#20250;&#23548;&#33268;&#27969;&#34892;&#30340;&#24212;&#29992;&#31243;&#24207;&#65288;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12289;&#20154;&#33080;&#35782;&#21035;&#31561;&#65289;&#23384;&#22312;&#23433;&#20840;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#23545;&#27492;&#31867;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#32593;&#32476;&#26159;&#38750;&#24120;&#24517;&#35201;&#21644;&#37325;&#35201;&#30340;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#38450;&#24481;&#24615;&#33976;&#39311;&#26159;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#33021;&#22815;&#21019;&#24314;&#23545;&#19968;&#20123;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25915;&#20987;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#26292;&#38706;&#20102;&#38450;&#24481;&#24615;&#33976;&#39311;&#30340;&#24369;&#28857;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#20174;&#25945;&#24072;&#21161;&#25163;&#30693;&#35782;&#33976;&#39311;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#24182;&#25552;&#20986;&#24341;&#20837;&#36741;&#21161;&#32593;&#32476;&#21487;&#20197;&#25552;&#39640;&#33976;&#39311;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks pose a significant threat to the security and safety of deep neural networks being applied to modern applications. More specifically, in computer vision-based tasks, experts can use the knowledge of model architecture to create adversarial samples imperceptible to the human eye. These attacks can lead to security problems in popular applications such as self-driving cars, face recognition, etc. Hence, building networks which are robust to such attacks is highly desirable and essential. Among the various methods present in literature, defensive distillation has shown promise in recent years. Using knowledge distillation, researchers have been able to create models robust against some of those attacks. However, more attacks have been developed exposing weakness in defensive distillation. In this project, we derive inspiration from teacher assistant knowledge distillation and propose that introducing an assistant network can improve the robustness of the distilled mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#32929;&#31080;&#32452;&#20869;&#21644;&#32452;&#38388;&#32467;&#26500;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#23618;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#36827;&#34892;&#35774;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.08073</link><description>&lt;p&gt;
HiPerformer&#65306;&#23618;&#32423;&#25490;&#21015;&#31561;&#21464;&#25442;&#22120;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HiPerformer: Hierarchically Permutation-Equivariant Transformer for Time Series Forecasting. (arXiv:2305.08073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#32929;&#31080;&#32452;&#20869;&#21644;&#32452;&#38388;&#32467;&#26500;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#23618;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#36827;&#34892;&#35774;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#38656;&#35201;&#36827;&#34892;&#21306;&#20998;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#32929;&#31080;&#20215;&#26684;&#65292;&#24448;&#24448;&#23558;&#32929;&#31080;&#20998;&#20026;&#20855;&#26377;&#30456;&#21516;&#29305;&#24449;&#30340;&#32452;&#65292;&#20855;&#26377;&#19982;&#35813;&#32452;&#32467;&#26500;&#19968;&#33268;&#30340;&#20851;&#31995;&#30340;&#27169;&#22411;&#24212;&#35813;&#26159;&#26377;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23618;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;&#27010;&#24565;&#65292;&#19987;&#27880;&#20110;&#32452;&#20869;&#21644;&#32452;&#38388;&#25104;&#20998;&#30340;&#25351;&#25968;&#20132;&#25442;&#65292;&#20197;&#35774;&#35745;&#32771;&#34385;&#36825;&#31181;&#32452;&#32467;&#26500;&#30340;&#27169;&#22411;&#12290;&#24403;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#20998;&#23618;&#25490;&#21015;&#31561;&#21464;&#24615;&#26102;&#65292;&#39044;&#27979;&#32467;&#26524;&#31526;&#21512;&#25104;&#20998;&#30340;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#32423;&#25490;&#21015;&#31561;&#21464;&#24615;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#21516;&#19968;&#32452;&#20013;&#25104;&#20998;&#20043;&#38388;&#21644;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#24320;&#23637;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is imperative to discern the relationships between multiple time series for accurate forecasting. In particular, for stock prices, components are often divided into groups with the same characteristics, and a model that extracts relationships consistent with this group structure should be effective. Thus, we propose the concept of hierarchical permutation-equivariance, focusing on index swapping of components within and among groups, to design a model that considers this group structure. When the prediction model has hierarchical permutation-equivariance, the prediction is consistent with the group relationships of the components. Therefore, we propose a hierarchically permutation-equivariant model that considers both the relationship among components in the same group and the relationship among groups. The experiments conducted on real-world data demonstrate that the proposed method outperforms existing state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#23545;&#29616;&#26377;&#32852;&#37030;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#32508;&#36848;&#65292;&#38416;&#36848;&#20102;&#32852;&#37030;&#35780;&#20272;&#22312;&#23458;&#25143;&#31471;&#36873;&#25321;&#12289;&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;&#12289;&#24694;&#24847;&#25915;&#20987;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#32852;&#37030;&#35780;&#20272;&#22312;&#22686;&#24378;FL&#24615;&#33021;&#26041;&#38754;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.08070</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32852;&#37030;&#35780;&#20272;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Federated Evaluation in Federated Learning. (arXiv:2305.08070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#23545;&#29616;&#26377;&#32852;&#37030;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#32508;&#36848;&#65292;&#38416;&#36848;&#20102;&#32852;&#37030;&#35780;&#20272;&#22312;&#23458;&#25143;&#31471;&#36873;&#25321;&#12289;&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;&#12289;&#24694;&#24847;&#25915;&#20987;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#32852;&#37030;&#35780;&#20272;&#22312;&#22686;&#24378;FL&#24615;&#33021;&#26041;&#38754;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;&#25152;&#26377;&#25968;&#25454;&#26679;&#26412;&#37117;&#30001;&#26381;&#21153;&#22120;&#36827;&#34892;&#38598;&#20013;&#31649;&#29702;&#65292;&#22240;&#27492;&#36827;&#34892;&#27169;&#22411;&#35780;&#20272;&#38750;&#24120;&#31616;&#21333;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#27169;&#22411;&#35780;&#20272;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#34987;&#31216;&#20026;&#26412;&#25991;&#20013;&#30340;&#32852;&#37030;&#35780;&#20272;&#12290;&#36825;&#26159;&#22240;&#20026;&#23458;&#25143;&#31471;&#19981;&#20250;&#20844;&#24320;&#20854;&#21407;&#22987;&#25968;&#25454;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#32852;&#37030;&#35780;&#20272;&#22312;&#23458;&#25143;&#31471;&#36873;&#25321;&#12289;&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;&#12289;&#24694;&#24847;&#25915;&#20987;&#26816;&#27979;&#31561;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#32852;&#37030;&#35780;&#20272;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32852;&#37030;&#35780;&#20272;&#22312;&#22686;&#24378;FL&#24615;&#33021;&#26041;&#38754;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#26368;&#32456;&#36890;&#36807;&#23637;&#26395;&#19968;&#20123;&#25361;&#25112;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional machine learning, it is trivial to conduct model evaluation since all data samples are managed centrally by a server. However, model evaluation becomes a challenging problem in federated learning (FL), which is called federated evaluation in this work. This is because clients do not expose their original data to preserve data privacy. Federated evaluation plays a vital role in client selection, incentive mechanism design, malicious attack detection, etc. In this paper, we provide the first comprehensive survey of existing federated evaluation methods. Moreover, we explore various applications of federated evaluation for enhancing FL performance and finally present future research directions by envisioning some challenges.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#33021;&#35299;&#20915;&#38271;&#23614;&#30446;&#26631;&#26816;&#27979;&#20013;&#26679;&#26412;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;IRFS&#65292;&#23558;&#23454;&#20363;&#21644;&#22270;&#20687;&#35745;&#25968;&#32467;&#21512;&#24212;&#29992;&#21040;&#37325;&#26032;&#37319;&#26679;&#36807;&#31243;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;IRFS&#20248;&#20110;&#29616;&#26377;&#37319;&#26679;&#26041;&#27861;&#24182;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08069</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#20363;&#24863;&#30693;&#37325;&#22797;&#22240;&#23376;&#37319;&#26679;&#30340;&#38271;&#23614;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection. (arXiv:2305.08069v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08069
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#33021;&#35299;&#20915;&#38271;&#23614;&#30446;&#26631;&#26816;&#27979;&#20013;&#26679;&#26412;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;IRFS&#65292;&#23558;&#23454;&#20363;&#21644;&#22270;&#20687;&#35745;&#25968;&#32467;&#21512;&#24212;&#29992;&#21040;&#37325;&#26032;&#37319;&#26679;&#36807;&#31243;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;IRFS&#20248;&#20110;&#29616;&#26377;&#37319;&#26679;&#26041;&#27861;&#24182;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#8212;&#8212;&#23454;&#20363;&#24863;&#30693;&#37325;&#22797;&#22240;&#23376;&#37319;&#26679;&#65288;IRFS&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38271;&#23614;&#30446;&#26631;&#26816;&#27979;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30446;&#26631;&#26816;&#27979;&#20013;&#65292;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#36890;&#24120;&#20250;&#21463;&#21040;&#27599;&#20010;&#31867;&#21035;&#23454;&#20363;&#25968;&#37327;&#30340;&#24040;&#22823;&#24046;&#24322;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#22312;&#32597;&#35265;&#31867;&#21035;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#25968;&#25454;&#37319;&#26679;&#25216;&#26415;&#12290;&#30001;&#20110;&#20854;&#31616;&#27905;&#21644;&#26377;&#25928;&#24615;&#65292;&#37325;&#22797;&#22240;&#23376;&#37319;&#26679;&#65288;RFS&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#23613;&#31649;RFS&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#23427;&#23436;&#20840;&#24573;&#30053;&#20102;&#23454;&#20363;&#25968;&#37327;&#65292;&#20165;&#20381;&#36182;&#20110;&#22270;&#20687;&#35745;&#25968;&#26469;&#36827;&#34892;&#37325;&#26032;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#21516;&#31867;&#21035;&#30340;&#23454;&#20363;&#35745;&#25968;&#21487;&#33021;&#24040;&#22823;&#19981;&#21516;&#65292;&#34429;&#28982;&#20854;&#22270;&#20687;&#35745;&#25968;&#30456;&#20284;&#12290;&#36825;&#31181;&#24046;&#24322;&#24378;&#35843;&#20102;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#22270;&#20687;&#21644;&#23454;&#20363;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IRFS&#65292;&#23427;&#23558;&#23454;&#20363;&#21644;&#22270;&#20687;&#35745;&#25968;&#32479;&#19968;&#20026;&#37325;&#26032;&#37319;&#26679;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#20197;&#20415;&#30693;&#36947;&#19981;&#24179;&#34913;&#24615;&#38382;&#39064;&#30340;&#19981;&#21516;&#35270;&#35282;&#12290;&#25105;&#20204;&#22312;&#33509;&#24178;&#20010;&#38271;&#23614;&#26816;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;IRFS&#20248;&#20110;&#29616;&#26377;&#37319;&#26679;&#26041;&#27861;&#24182;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an embarrassingly simple method -- instance-aware repeat factor sampling (IRFS) to address the problem of imbalanced data in long-tailed object detection. Imbalanced datasets in real-world object detection often suffer from a large disparity in the number of instances for each class. To improve the generalization performance of object detection models on rare classes, various data sampling techniques have been proposed. Repeat factor sampling (RFS) has shown promise due to its simplicity and effectiveness. Despite its efficiency, RFS completely neglects the instance counts and solely relies on the image count during re-sampling process. However, instance count may immensely vary for different classes with similar image counts. Such variation highlights the importance of both image and instance for addressing the long-tail distributions. Thus, we propose IRFS which unifies instance and image counts for the re-sampling process to be aware of different perspectives of the imbal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38901;&#24459;&#20851;&#27880;&#21644;&#38901;&#24459;&#33976;&#39311;&#26041;&#27861;&#26469;&#21033;&#29992;&#38901;&#24459;&#29305;&#24449;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#20041;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#38901;&#24459;&#33976;&#39311;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#22312;SLURP&#21644;STOP&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;8&#65285;&#21644;2&#65285;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08067</link><description>&lt;p&gt;
&#21033;&#29992;&#38901;&#24459;&#20851;&#27880;&#21644;&#33976;&#39311;&#26469;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#20041;&#29702;&#35299;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving End-to-End SLU performance with Prosodic Attention and Distillation. (arXiv:2305.08067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38901;&#24459;&#20851;&#27880;&#21644;&#38901;&#24459;&#33976;&#39311;&#26041;&#27861;&#26469;&#21033;&#29992;&#38901;&#24459;&#29305;&#24449;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#20041;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#38901;&#24459;&#33976;&#39311;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#22312;SLURP&#21644;STOP&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;8&#65285;&#21644;2&#65285;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#31471;&#21040;&#31471;&#35821;&#20041;&#29702;&#35299;&#65288;SLU&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25110;&#35821;&#35328;&#27169;&#22411;&#21151;&#33021;&#26469;&#36827;&#34892;&#24847;&#22270;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#20013;&#30340;&#20854;&#20182;&#37325;&#35201;&#20449;&#24687;&#65292;&#22914;&#38901;&#24459;&#65292;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#38901;&#24459;&#20449;&#24687;&#21512;&#24182;&#21040;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20013;&#21487;&#20197;&#33719;&#24471;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38901;&#24459;&#20851;&#27880;&#65292;&#23427;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#24335;&#21033;&#29992;&#38901;&#24459;&#29305;&#24449;&#26469;&#29983;&#25104;&#36328;&#26102;&#38388;&#24103;&#30340;&#27880;&#24847;&#21147;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38901;&#24459;&#33976;&#39311;&#65292;&#26469;&#26126;&#30830;&#22320;&#23398;&#20064;&#22768;&#23398;&#32534;&#30721;&#22120;&#20013;&#30340;&#38901;&#24459;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#36830;&#25509;&#38544;&#21547;&#30340;&#38901;&#24459;&#29305;&#24449;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#25552;&#39640;&#20102;&#22522;&#32447;&#32467;&#26524;&#65292;&#20854;&#20013;&#38901;&#24459;&#33976;&#39311;&#26041;&#27861;&#22312;SLURP&#21644;STOP&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#38901;&#24459;&#22522;&#32447;&#25552;&#39640;&#20102;8&#65285;&#21644;2&#65285;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most End-to-End SLU methods depend on the pretrained ASR or language model features for intent prediction. However, other essential information in speech, such as prosody, is often ignored. Recent research has shown improved results in classifying dialogue acts by incorporating prosodic information. The margins of improvement in these methods are minimal as the neural models ignore prosodic features. In this work, we propose prosody-attention, which uses the prosodic features differently to generate attention maps across time frames of the utterance. Then we propose prosody-distillation to explicitly learn the prosodic information in the acoustic encoder rather than concatenating the implicit prosodic features. Both the proposed methods improve the baseline results, and the prosody-distillation method gives an intent classification accuracy improvement of 8\% and 2\% on SLURP and STOP datasets over the prosody baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;OffCEM&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#23545;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12290;&#35813;&#20272;&#35745;&#22120;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#20445;&#25345;&#26080;&#20559;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08062</link><description>&lt;p&gt;
&#22522;&#20110;&#36830;&#35789;&#25928;&#24212;&#24314;&#27169;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling. (arXiv:2305.08062v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;OffCEM&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#23545;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#36827;&#34892;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#12290;&#35813;&#20272;&#35745;&#22120;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#20445;&#25345;&#26080;&#20559;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23545;&#20110;&#20256;&#32479;&#37325;&#35201;&#24615;&#21152;&#26435;&#26041;&#27861;&#26041;&#24040;&#30340;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#19978;&#19979;&#25991;&#21305;&#37197;&#31574;&#30053;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#26041;&#24040;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;OffCEM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36830;&#35789;&#25928;&#24212;&#27169;&#22411;&#65288;CEM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#25928;&#24212;&#20998;&#20026;&#32676;&#38598;&#25928;&#24212;&#21644;&#27531;&#24046;&#25928;&#24212;&#12290;OffCEM&#20165;&#23545;&#34892;&#21160;&#32676;&#38598;&#24212;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#22870;&#21169;&#20272;&#35745;&#26469;&#22788;&#29702;&#27531;&#20313;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26032;&#30340;&#26412;&#22320;&#27491;&#30830;&#24615;&#26465;&#20214;&#19979;&#65292;&#35813;&#20272;&#35745;&#22120;&#26159;&#26080;&#20559;&#30340;&#65292;&#35813;&#26465;&#20214;&#20165;&#35201;&#27714;&#27531;&#24046;&#25928;&#24212;&#27169;&#22411;&#20445;&#30041;&#27599;&#20010;&#32676;&#38598;&#20013;&#34892;&#21160;&#30340;&#30456;&#23545;&#26399;&#26395;&#22870;&#21169;&#24046;&#24322;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;CEM&#21644;&#26412;&#22320;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#36807;&#31243;&#65292;&#29992;&#20110;&#25191;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#20272;&#35745;&#65292;&#31532;&#19968;&#27493;&#26368;&#23567;&#21270;&#20559;&#24046;&#65292;&#31532;&#20108;&#27493;&#26368;&#23567;&#21270;&#26041;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#24471;&#21040;&#30340;OPE&#20272;&#35745;&#22120;OffCEM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#22823;&#21160;&#20316;&#31354;&#38388;&#25968;&#25454;&#38598;&#19978;&#37117;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study off-policy evaluation (OPE) of contextual bandit policies for large discrete action spaces where conventional importance-weighting approaches suffer from excessive variance. To circumvent this variance issue, we propose a new estimator, called OffCEM, that is based on the conjunct effect model (CEM), a novel decomposition of the causal effect into a cluster effect and a residual effect. OffCEM applies importance weighting only to action clusters and addresses the residual causal effect through model-based reward estimation. We show that the proposed estimator is unbiased under a new condition, called local correctness, which only requires that the residual-effect model preserves the relative expected reward differences of the actions within each cluster. To best leverage the CEM and local correctness, we also propose a new two-step procedure for performing model-based estimation that minimizes bias in the first step and variance in the second step. We find that the resulting O
&lt;/p&gt;</description></item><item><title>CREMP&#26159;&#19968;&#20010;&#36164;&#28304;&#65292;&#21253;&#21547;&#36229;&#36807;3&#21315;&#19975;&#20010;&#22823;&#29615;&#32957;&#26500;&#35937;&#24418;&#29366;&#65292;&#26088;&#22312;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#27169;&#25311;&#22823;&#29615;&#32957;&#30340;&#26500;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.08057</link><description>&lt;p&gt;
CREMP: &#22823;&#29615;&#32957;&#30340;&#26500;&#35937;&#26059;&#36716;&#32452;&#21512;&#38598;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CREMP: Conformer-Rotamer Ensembles of Macrocyclic Peptides for Machine Learning. (arXiv:2305.08057v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08057
&lt;/p&gt;
&lt;p&gt;
CREMP&#26159;&#19968;&#20010;&#36164;&#28304;&#65292;&#21253;&#21547;&#36229;&#36807;3&#21315;&#19975;&#20010;&#22823;&#29615;&#32957;&#26500;&#35937;&#24418;&#29366;&#65292;&#26088;&#22312;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#27169;&#25311;&#22823;&#29615;&#32957;&#30340;&#26500;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#27169;&#25311;&#22823;&#29615;&#32957;&#30340;&#26500;&#35937;&#20855;&#26377;&#28508;&#22312;&#30340;&#21551;&#31034;&#24615;&#20316;&#29992;&#65292;&#33021;&#22815;&#23454;&#29616;&#21512;&#29702;&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#20280;&#32553;&#30340;&#24314;&#27169;&#22823;&#29615;&#32957;&#20960;&#20309;&#24418;&#29366;&#30340;&#26041;&#27861;&#20173;&#28982;&#24456;&#38590;&#24471;&#21040;&#12290;&#36817;&#26399;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26174;&#33879;&#21152;&#36895;&#20102;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#21644;&#23567;&#20998;&#23376;&#26500;&#35937;&#38598;&#21512;&#30340;&#29983;&#25104;&#65292;&#20294;&#30001;&#20110;&#22823;&#29615;&#32957;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#22312;&#22823;&#29615;&#32957;&#30340;&#24314;&#27169;&#26041;&#38754;&#20173;&#26410;&#21462;&#24471;&#31867;&#20284;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CREMP&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#26469;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#22823;&#29615;&#32957;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;CREMP&#21253;&#21547;36,198&#20010;&#29420;&#29305;&#30340;&#22823;&#29615;&#32957;&#21450;&#20854;&#21033;&#29992;&#26500;&#35937;-&#26059;&#36716;&#32452;&#21512;&#37319;&#26679;&#24037;&#20855;&#65288;CREST&#65289;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26500;&#35937;&#38598;&#21512;&#12290;&#24635;&#20043;&#65292;&#36825;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#36817;3130&#19975;&#20010;&#29420;&#29305;&#30340;&#22823;&#29615;&#32957;&#20960;&#20309;&#24418;&#29366;&#65292;&#27599;&#20010;&#24418;&#29366;&#37117;&#29992;&#21322;&#32463;&#39564;&#25193;&#23637;&#32039;&#26463;&#32538;&#65288;xTB&#65289;DF&#23548;&#20986;&#30340;&#33021;&#37327;&#36827;&#34892;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational and machine learning approaches to model the conformational landscape of macrocyclic peptides have the potential to enable rational design and optimization. However, accurate, fast, and scalable methods for modeling macrocycle geometries remain elusive. Recent deep learning approaches have significantly accelerated protein structure prediction and the generation of small-molecule conformational ensembles, yet similar progress has not been made for macrocyclic peptides due to their unique properties. Here, we introduce CREMP, a resource generated for the rapid development and evaluation of machine learning models for macrocyclic peptides. CREMP contains 36,198 unique macrocyclic peptides and their high-quality structural ensembles generated using the Conformer-Rotamer Ensemble Sampling Tool (CREST). Altogether, this new dataset contains nearly 31.3 million unique macrocycle geometries, each annotated with energies derived from semi-empirical extended tight-binding (xTB) DF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#20102;&#24433;&#21709;&#27867;&#21270;&#38388;&#38553;&#30340;&#20307;&#31995;&#32467;&#26500;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.08048</link><description>&lt;p&gt;
&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Generalization of Graph Neural Networks. (arXiv:2305.08048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#21457;&#29616;&#20102;&#24433;&#21709;&#27867;&#21270;&#38388;&#38553;&#30340;&#20307;&#31995;&#32467;&#26500;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#21644;&#34920;&#31034;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#20961;&#30340;&#25104;&#21151;&#65292;&#20294;&#20174;&#29702;&#35770;&#19978;&#29702;&#35299;&#23427;&#20204;&#30340;&#24037;&#20316;&#26426;&#21046;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#26412;&#25991;&#20174;&#27867;&#21270;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#39318;&#20808;&#32771;&#34385;&#20102;&#38543;&#26426;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#20256;&#36882;&#23398;&#20064;&#30340;&#27867;&#21270;&#38388;&#38553;&#21644;&#26799;&#24230;&#30340;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#20026;&#27969;&#34892;&#30340;GNN&#25552;&#20379;&#20102;&#27867;&#21270;&#38388;&#38553;&#30340;&#39640;&#27010;&#29575;&#19978;&#38480;&#12290;&#29702;&#35770;&#32467;&#26524;&#25581;&#31034;&#20102;&#24433;&#21709;&#27867;&#21270;&#38388;&#38553;&#30340;&#20307;&#31995;&#32467;&#26500;&#29305;&#23450;&#22240;&#32032;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#29702;&#35770;&#32467;&#26524;&#21644;&#32463;&#39564;&#35777;&#25454;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#29702;&#35299;GNN&#27867;&#21270;&#30340;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are the most widely adopted model in graph-structured data oriented learning and representation. Despite their extraordinary success in real-world applications, understanding their working mechanism by theory is still on primary stage. In this paper, we move towards this goal from the perspective of generalization. To be specific, we first establish high probability bounds of generalization gap and gradients in transductive learning with consideration of stochastic optimization. After that, we provide high probability bounds of generalization gap for popular GNNs. The theoretical results reveal the architecture specific factors affecting the generalization gap. Experimental results on benchmark datasets show the consistency between theoretical results and empirical evidence. Our results provide new insights in understanding the generalization of GNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;EEG&#20449;&#21495;&#20316;&#20026;&#31070;&#32463;&#24037;&#25928;&#23398;&#30340;&#29983;&#29702;&#27979;&#37327;&#21592;&#35780;&#20272;&#20102;&#21442;&#19982;&#32773;&#22312;&#20856;&#22411;&#21150;&#20844;&#20219;&#21153;&#26399;&#38388;&#30340;&#35760;&#24518;&#36127;&#33655;&#12290;&#32467;&#26524;&#34920;&#26126;&#21333;&#26174;&#31034;&#22120;&#21644;&#21452;&#26174;&#31034;&#22120;&#35774;&#32622;&#19979;&#30340;&#35760;&#24518;&#24037;&#20316;&#36127;&#33655;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#39640;&#36127;&#33655;&#21644;&#20302;&#36127;&#33655;&#30340;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2305.08044</link><description>&lt;p&gt;
&#20351;&#29992;EEG&#20449;&#21495;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#35780;&#20272;&#35760;&#24518;&#26816;&#32034;&#24037;&#20316;&#36127;&#33655;
&lt;/p&gt;
&lt;p&gt;
Using EEG Signals to Assess Workload during Memory Retrieval in a Real-world Scenario. (arXiv:2305.08044v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;EEG&#20449;&#21495;&#20316;&#20026;&#31070;&#32463;&#24037;&#25928;&#23398;&#30340;&#29983;&#29702;&#27979;&#37327;&#21592;&#35780;&#20272;&#20102;&#21442;&#19982;&#32773;&#22312;&#20856;&#22411;&#21150;&#20844;&#20219;&#21153;&#26399;&#38388;&#30340;&#35760;&#24518;&#36127;&#33655;&#12290;&#32467;&#26524;&#34920;&#26126;&#21333;&#26174;&#31034;&#22120;&#21644;&#21452;&#26174;&#31034;&#22120;&#35774;&#32622;&#19979;&#30340;&#35760;&#24518;&#24037;&#20316;&#36127;&#33655;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#39640;&#36127;&#33655;&#21644;&#20302;&#36127;&#33655;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#33041;&#30005;&#22270;(electroencephalogram, EEG)&#20316;&#20026;&#31070;&#32463;&#24037;&#25928;&#23398;&#30340;&#19968;&#31181;&#29983;&#29702;&#27979;&#37327;&#65292;&#22240;&#20026;&#20855;&#26377;&#23458;&#35266;&#24615;&#12289;&#19981;&#26131;&#21463;&#21040;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#33021;&#22815;&#35780;&#20272;&#35748;&#30693;&#29366;&#24577;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#22312;&#20154;&#31867;&#22240;&#32032;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#35760;&#24518;&#36127;&#33655;&#19982;EEG&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#22312;&#21442;&#19982;&#32773;&#22312;&#21333;&#26174;&#31034;&#22120;&#21644;&#21452;&#26174;&#31034;&#22120;&#35774;&#32622;&#19979;&#36827;&#34892;&#20856;&#22411;&#21150;&#20844;&#20219;&#21153;&#26399;&#38388;&#65292;&#25105;&#20204;&#39044;&#35745;&#21333;&#26174;&#31034;&#22120;&#30340;&#24773;&#20917;&#19979;&#23558;&#20855;&#26377;&#26356;&#39640;&#30340;&#35760;&#24518;&#36127;&#33655;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#39564;&#65292;&#27169;&#25311;&#20102;&#34987;&#35797;&#25191;&#34892;&#19968;&#20123;&#21150;&#20844;&#24037;&#20316;&#30340;&#22330;&#26223;&#65292;&#24182;&#26816;&#26597;&#20102;&#34987;&#35797;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#21150;&#20844;&#26700;&#38754;&#35774;&#32622;&#65288;&#21333;&#26174;&#31034;&#22120;&#35774;&#32622;&#21644;&#21452;&#26174;&#31034;&#22120;&#35774;&#32622;&#65289;&#19979;&#26159;&#21542;&#32463;&#21382;&#20102;&#19981;&#21516;&#27700;&#24179;&#30340;&#35760;&#24518;&#24037;&#20316;&#36127;&#33655;&#12290;&#25105;&#20204;&#20351;&#29992;EEG&#24102;&#21151;&#29575;&#12289;&#20114;&#20449;&#24687;&#21644;&#30456;&#24178;&#24615;&#20316;&#20026;&#29305;&#24449;&#65292;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#20998;&#31867;&#39640;&#36127;&#33655;&#21644;&#20302;&#36127;&#33655;&#29366;&#24577;&#12290;&#20027;&#35201;&#32467;&#26524;&#65306;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#29305;&#24449;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24046;&#24322;...
&lt;/p&gt;
&lt;p&gt;
Objective: The Electroencephalogram (EEG) is gaining popularity as a physiological measure for neuroergonomics in human factor studies because it is objective, less prone to bias, and capable of assessing the dynamics of cognitive states. This study investigated the associations between memory workload and EEG during participants' typical office tasks on a single-monitor and dual-monitor arrangement. We expect a higher memory workload for the single-monitor arrangement. Approach: We designed an experiment that mimics the scenario of a subject performing some office work and examined whether the subjects experienced various levels of memory workload in two different office setups: 1) a single-monitor setup and 2) a dual-monitor setup. We used EEG band power, mutual information, and coherence as features to train machine learning models to classify high versus low memory workload states. Main results: The study results showed that these characteristics exhibited significant differences t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08040</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#27744;&#21270;&#30340;&#21487;&#35777;&#26126;&#22810;&#23454;&#20363;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provable Multi-instance Deep AUC Maximization with Stochastic Pooling. (arXiv:2305.08040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#20351;&#29992;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#21253;&#21547;&#22823;&#37327;&#23454;&#20363;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#24471;&#21482;&#38656;&#23545;&#27599;&#20010;&#21253;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#21363;&#21487;&#35745;&#31639;MIDAM&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;AUC&#26368;&#22823;&#21270;&#65288;DAM&#65289;&#30340;&#26032;&#22411;&#24212;&#29992;&#65292;&#29992;&#20110;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#65292;&#20854;&#20013;&#23558;&#21333;&#20010;&#31867;&#26631;&#31614;&#20998;&#37197;&#32473;&#19968;&#32452;&#23454;&#20363;&#65288;&#20363;&#22914;&#65292;&#24739;&#32773;&#30340;&#22810;&#20010;CT&#25195;&#25551;&#30340;&#22810;&#20010;2D&#20999;&#29255;&#65289;&#12290;&#25105;&#20204;&#22312;DAM&#30340;&#32972;&#26223;&#19979;&#35299;&#20915;&#20102;MIL&#20013;&#34987;&#24573;&#30053;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21363;&#21253;&#22823;&#23567;&#36807;&#22823;&#65292;&#26080;&#27861;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#21152;&#36733;&#21040;GPU&#20869;&#23384;&#20013;&#65292;&#36825;&#26159;MIL&#26631;&#20934;&#27744;&#21270;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20851;&#20110;&#27719;&#32858;&#39044;&#27979;&#30340;&#25439;&#22833;&#20989;&#25968;&#26500;&#36896;&#20026;&#22810;&#32423;&#32452;&#21512;&#20989;&#25968;&#12290;&#36890;&#36807;&#32508;&#21512;&#38543;&#26426;&#32452;&#21512;&#20248;&#21270;&#21644;&#38750;&#20984;&#26497;&#23567;&#26368;&#22823;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#19988;&#21487;&#35777;&#26126;&#30340;&#22810;&#23454;&#20363;DAM&#65288;MIDAM&#65289;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#26368;&#22823;&#27744;&#21270;&#25110;&#38543;&#26426;&#27880;&#24847;&#21147;&#27744;&#21270;&#65292;&#20165;&#23545;&#27599;&#20010;&#21253;&#23545;&#24212;&#30340;&#23454;&#20363;&#36827;&#34892;&#23569;&#37327;&#37319;&#26679;&#26469;&#35745;&#31639; sto&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a novel application of deep AUC maximization (DAM) for multi-instance learning (MIL), in which a single class label is assigned to a bag of instances (e.g., multiple 2D slices of a CT scan for a patient). We address a neglected yet non-negligible computational challenge of MIL in the context of DAM, i.e., bag size is too large to be loaded into {GPU} memory for backpropagation, which is required by the standard pooling methods of MIL. To tackle this challenge, we propose variance-reduced stochastic pooling methods in the spirit of stochastic optimization by formulating the loss function over the pooled prediction as a multi-level compositional function. By synthesizing techniques from stochastic compositional optimization and non-convex min-max optimization, we propose a unified and provable muli-instance DAM (MIDAM) algorithm with stochastic smoothed-max pooling or stochastic attention-based pooling, which only samples a few instances for each bag to compute a sto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#23545;&#28151;&#27788;&#21160;&#21147;&#23398;&#36827;&#34892;&#23567;&#25968;&#25454;&#38477;&#38454;&#24314;&#27169;&#65292;&#36890;&#36807;&#22312;&#38477;&#38454;&#31354;&#38388;&#20013;&#26045;&#21152;&#21512;&#25104;&#32422;&#26463;&#26469;&#20445;&#25345;&#23436;&#20840;&#38750;&#32447;&#24615;&#21644;&#39640;&#24230;&#19981;&#31283;&#23450;&#30340;&#33258;&#30001;&#24230;&#30340;&#21516;&#26102;&#65292;&#38450;&#27490;&#20102;&#21457;&#25955;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#21363;&#20415;&#26159;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#20135;&#29983;&#20302;&#35823;&#24046;&#30340;&#20013;&#38271;&#26399;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.08036</link><description>&lt;p&gt;
&#36890;&#36807;SyCo-AE: Synthetically Constrained Autoencoders&#23545;&#28151;&#27788;&#21160;&#21147;&#23398;&#36827;&#34892;&#23567;&#25968;&#25454;&#38477;&#38454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Small-data Reduced Order Modeling of Chaotic Dynamics through SyCo-AE: Synthetically Constrained Autoencoders. (arXiv:2305.08036v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#23545;&#28151;&#27788;&#21160;&#21147;&#23398;&#36827;&#34892;&#23567;&#25968;&#25454;&#38477;&#38454;&#24314;&#27169;&#65292;&#36890;&#36807;&#22312;&#38477;&#38454;&#31354;&#38388;&#20013;&#26045;&#21152;&#21512;&#25104;&#32422;&#26463;&#26469;&#20445;&#25345;&#23436;&#20840;&#38750;&#32447;&#24615;&#21644;&#39640;&#24230;&#19981;&#31283;&#23450;&#30340;&#33258;&#30001;&#24230;&#30340;&#21516;&#26102;&#65292;&#38450;&#27490;&#20102;&#21457;&#25955;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#21363;&#20415;&#26159;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#20135;&#29983;&#20302;&#35823;&#24046;&#30340;&#20013;&#38271;&#26399;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#28151;&#27788;&#21160;&#21147;&#23398;&#38477;&#38454;&#24314;&#27169;&#21487;&#33021;&#20250;&#23548;&#33268;&#31995;&#32479;&#21457;&#25955;&#25110;&#28781;&#32477;&#12290;&#26412;&#25991;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#36816;&#31639;&#25512;&#26029;&#33258;&#30001;&#65292;&#36890;&#36807;&#22312;&#38477;&#38454;&#31354;&#38388;&#20013;&#26045;&#21152;&#21512;&#25104;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21512;&#25104;&#32422;&#26463;&#20351;&#25105;&#20204;&#30340;&#38477;&#38454;&#27169;&#22411;&#22312;&#20445;&#25345;&#23436;&#20840;&#38750;&#32447;&#24615;&#21644;&#39640;&#24230;&#19981;&#31283;&#23450;&#30340;&#33258;&#30001;&#24230;&#30340;&#21516;&#26102;&#65292;&#38450;&#27490;&#20102;&#21457;&#25955;&#12290;&#25105;&#20204;&#21033;&#29992;&#32463;&#20856;&#30340;40&#21464;&#37327;Lorenz '96&#26041;&#31243;&#35828;&#26126;&#20102;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#21487;&#20197;&#20135;&#29983;&#20302;&#35823;&#24046;&#30340;&#20013;&#38271;&#26399;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven reduced order modeling of chaotic dynamics can result in systems that either dissipate or diverge catastrophically. Leveraging non-linear dimensionality reduction of autoencoders and the freedom of non-linear operator inference with neural-networks, we aim to solve this problem by imposing a synthetic constraint in the reduced order space. The synthetic constraint allows our reduced order model both the freedom to remain fully non-linear and highly unstable while preventing divergence. We illustrate the methodology with the classical 40-variable Lorenz '96 equations, showing that our methodology is capable of producing medium-to-long range forecasts with lower error using less data.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#22788;&#29702;&#26497;&#31471;&#27668;&#21160;&#23398;&#38382;&#39064;&#30340;&#22522;&#26412;&#29289;&#29702;&#26426;&#21046;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08024</link><description>&lt;p&gt;
&#38477;&#32500;&#22788;&#29702;&#26497;&#31471;&#27668;&#21160;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Grasping Extreme Aerodynamics on a Low-Dimensional Manifold. (arXiv:2305.08024v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08024
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#22788;&#29702;&#26497;&#31471;&#27668;&#21160;&#23398;&#38382;&#39064;&#30340;&#22522;&#26412;&#29289;&#29702;&#26426;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33322;&#31354;&#22120;&#25191;&#34892;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#36816;&#36755;&#12289;&#22269;&#38450;&#12289;&#30417;&#35270;&#21644;&#25937;&#25588;&#12290;&#36825;&#20123;&#39134;&#26426;&#21487;&#20197;&#22312;&#24179;&#38745;&#30340;&#26465;&#20214;&#19979;&#39134;&#34892;&#65292;&#20294;&#36991;&#20813;&#22312;&#31364;&#35895;&#65292;&#23665;&#21306;&#21644;&#33337;&#33334;&#23614;&#36857;&#31561;&#22810;&#39118;&#21306;&#25805;&#20316;&#12290;&#20854;&#20013;&#65292;&#23567;&#22411;&#39134;&#26426;&#23588;&#20854;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#27668;&#27969;&#24178;&#25200;&#12290;&#38543;&#30528;&#20840;&#29699;&#21464;&#26262;&#20351;&#26497;&#31471;&#22825;&#27668;&#36234;&#26469;&#36234;&#39057;&#32321;&#65292;&#39044;&#35745;&#39134;&#26426;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#36739;&#23567;&#30340;&#39134;&#26426;&#65292;&#20250;&#36935;&#21040;&#22823;&#35268;&#27169;&#30340;&#22823;&#27668;&#24178;&#25200;&#65292;&#20294;&#20173;&#24212;&#25191;&#34892;&#31283;&#23450;&#30340;&#39134;&#34892;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20960;&#20046;&#27809;&#26377;&#29702;&#35770;&#26469;&#25551;&#36848;&#24378;&#28872;&#30340;&#27668;&#27969;&#23545;&#39134;&#34892;&#22120;&#30340;&#24433;&#21709;&#12290;&#26356;&#20026;&#22797;&#26434;&#30340;&#26159;&#65292;&#32764;&#38754;&#36935;&#21040;&#27668;&#27969;&#30340;&#21442;&#25968;&#31354;&#38388;&#38750;&#24120;&#22823;&#65292;&#32780;&#19981;&#21516;&#21442;&#25968;&#32452;&#21512;&#20043;&#38388;&#30340;&#28065;&#26059;&#27668;&#27969;&#21644;&#32764;&#30340;&#30456;&#20114;&#20316;&#29992;&#20284;&#20046;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#28857;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#30340;&#22522;&#26412;&#29289;&#29702;&#26426;&#21046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern air vehicles perform a wide range of operations, including transportation, defense, surveillance, and rescue. These aircraft can fly in calm conditions but avoid operations in gusty environments, which are seen in urban canyons, over mountainous terrains, and in ship wakes. Smaller aircraft are especially prone to such gust disturbances. With extreme weather becoming ever more frequent due to global warming, it is anticipated that aircraft, especially those that are smaller in size, encounter large-scale atmospheric disturbances and still be expected to manage stable flight. However, there exists virtually no foundation to describe the influence of extreme vortical gusts on flying bodies. To compound on this difficult problem, there is an enormous parameter space for gusty conditions wings encounter. While the interaction between the vortical gusts and wings is seemingly complex and different for each combination of gust parameters, we show in this study that the fundamental phy
&lt;/p&gt;</description></item><item><title>TIPS&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;AnytimeNNs&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#36129;&#29486;&#26368;&#22823;&#30340;&#36335;&#24452;&#26469;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;2%-6.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#20934;&#30830;&#29575;-FLOPs&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.08021</link><description>&lt;p&gt;
TIPS&#65306;&#20219;&#20309;&#26102;&#20505;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#37325;&#35201;&#36335;&#24452;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
TIPS: Topologically Important Path Sampling for Anytime Neural Networks. (arXiv:2305.08021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08021
&lt;/p&gt;
&lt;p&gt;
TIPS&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;AnytimeNNs&#26694;&#26550;&#65292;&#36890;&#36807;&#35782;&#21035;&#36129;&#29486;&#26368;&#22823;&#30340;&#36335;&#24452;&#26469;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#21644;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;2%-6.6%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#20934;&#30830;&#29575;-FLOPs&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#26102;&#20505;&#31070;&#32463;&#32593;&#32476;(AnytimeNNs) &#26159;&#19968;&#31181;&#33021;&#22815;&#22312;&#21508;&#31181;&#30828;&#20214;&#36164;&#28304;&#32422;&#26463;&#19979;&#36866;&#24212;&#24615;&#35843;&#25972;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#30340; AnytimeNNs &#24448;&#24448;&#20250;&#21463;&#21040;&#35774;&#35745;&#24072;&#20808;&#21069;&#32463;&#39564;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#25552;&#20379;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#25163;&#24037;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558; AnytimeNNs &#30340;&#35757;&#32451;&#36807;&#31243;&#24314;&#27169;&#20026;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#38142;(DTMC)&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#35782;&#21035;&#23545; AnytimeNNs &#35757;&#32451;&#36129;&#29486;&#26368;&#22823;&#30340;&#36335;&#24452;&#12290;&#22522;&#20110;&#36825;&#31181;&#26032;&#30340; DTMC &#22522;&#30784;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102; TIPS (Topologically Important Path Sampling) &#26694;&#26550;&#65292;&#20197;&#33258;&#21160;&#35774;&#35745;&#36866;&#24212;&#21508;&#31181;&#30828;&#20214;&#32422;&#26463;&#30340; AnytimeNNs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TIPS &#33021;&#22815;&#25552;&#39640; AnytimeNNs &#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#19982;&#29616;&#26377; AnytimeNNs &#26041;&#27861;&#30456;&#27604;&#65292;TIPS &#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23558;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102; 2%-6.6%&#65292;&#24182;&#23454;&#29616;&#20102; SOTA &#30340;&#20934;&#30830;&#29575;-FLOPs &#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anytime neural networks (AnytimeNNs) are a promising solution to adaptively adjust the model complexity at runtime under various hardware resource constraints. However, the manually-designed AnytimeNNs are biased by designers' prior experience and thus provide sub-optimal solutions. To address the limitations of existing hand-crafted approaches, we first model the training process of AnytimeNNs as a discrete-time Markov chain (DTMC) and use it to identify the paths that contribute the most to the training of AnytimeNNs. Based on this new DTMC-based analysis, we further propose TIPS, a framework to automatically design AnytimeNNs under various hardware constraints. Our experimental results show that TIPS can improve the convergence rate and test accuracy of AnytimeNNs. Compared to the existing AnytimeNNs approaches, TIPS improves the accuracy by 2%-6.6% on multiple datasets and achieves SOTA accuracy-FLOPs tradeoffs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#32467;&#26500;&#30340;&#26694;&#26550;&#65292;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#21160;&#24577;&#37325;&#36830;&#26469;&#30830;&#20445;&#36880;&#28176;&#23494;&#38598;&#21270;&#30340;&#22270;&#24418;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#20801;&#35768;&#36328;&#23618;&#33410;&#28857;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2305.08018</link><description>&lt;p&gt;
DRew&#65306;&#24102;&#24310;&#36831;&#30340;&#21160;&#24577;&#37325;&#36830;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
DRew: Dynamically Rewired Message Passing with Delay. (arXiv:2305.08018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#32467;&#26500;&#30340;&#26694;&#26550;&#65292;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#21160;&#24577;&#37325;&#36830;&#26469;&#30830;&#20445;&#36880;&#28176;&#23494;&#38598;&#21270;&#30340;&#22270;&#24418;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#20801;&#35768;&#36328;&#23618;&#33410;&#28857;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#23384;&#22312;&#36807;&#24230;&#21387;&#32553;&#29616;&#35937;&#65292;&#23548;&#33268;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#21482;&#22312;&#33410;&#28857;&#30340;&#30456;&#37051;&#23621;&#20043;&#38388;&#36827;&#34892;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#12290;&#35797;&#22270;&#20351;&#22270;&#24418;&#8220;&#26356;&#36830;&#36890;&#8221;&#24182;&#19988;&#26356;&#36866;&#21512;&#38271;&#31243;&#20219;&#21153;&#30340;&#37325;&#36830;&#26041;&#27861;&#36890;&#24120;&#20250;&#22833;&#21435;&#22522;&#20110;&#22270;&#24418;&#36317;&#31163;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20351;&#36828;&#31243;&#33410;&#28857;&#22312;&#27599;&#19968;&#23618;&#20013;&#31435;&#21363;&#36890;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#26550;&#26500;&#65292;&#20197;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#37325;&#36830;&#65292;&#20197;&#30830;&#20445;&#36880;&#28176;&#21152;&#23494;&#22270;&#24418;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#23618;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#36317;&#31163;&#22312;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#36339;&#36291;&#36830;&#25509;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#38271;&#31243;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#20854;&#20248;&#20110;&#22270;&#24418;&#21464;&#25442;&#22120;&#21644;&#22810;&#36339;MPNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing neural networks (MPNNs) have been shown to suffer from the phenomenon of over-squashing that causes poor performance for tasks relying on long-range interactions. This can be largely attributed to message passing only occurring locally, over a node's immediate neighbours. Rewiring approaches attempting to make graphs `more connected', and supposedly better suited to long-range tasks, often lose the inductive bias provided by distance on the graph since they make distant nodes communicate instantly at every layer. In this paper we propose a framework, applicable to any MPNN architecture, that performs a layer-dependent rewiring to ensure gradual densification of the graph. We also propose a delay mechanism that permits skip connections between nodes depending on the layer and their mutual distance. We validate our approach on several long-range tasks and show that it outperforms graph Transformers and multi-hop MPNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08014</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#30636;&#26102;&#39640;&#28165;&#32908;&#30005;&#22270;&#20687;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#21487;&#20197;&#24320;&#36767;&#21457;&#23637;&#26356;&#27969;&#30021;&#12289;&#26356;&#33258;&#28982;&#30340;&#32908;&#32905;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#26032;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36328;&#22330;&#26223;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23384;&#22312;&#26497;&#22823;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#38750;&#24120;&#22823;&#19988;&#22797;&#26434;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;&#22522;&#20110;2SRNN&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#26469;&#36924;&#36817;&#30001;&#36825;&#20123;&#36328;&#22330;&#26223;&#25968;&#25454;&#21464;&#24322;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38656;&#35201;&#22312;&#39044;&#35757;&#32451;&#21644;&#36866;&#24212;&#38454;&#27573;&#20013;&#22312;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#21442;&#25968;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#36827;&#34892;&#39640;&#31471;&#36164;&#28304;&#32422;&#26463;&#21644;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;(TL)&#26469;&#22686;&#24378;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#19968;&#33324;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#29942;&#39048;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25311;&#21512;&#21644;&#21387;&#32553;&#38454;&#27573;&#12290;&#36890;&#36807;&#35813;&#20998;&#26512;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.08013</link><description>&lt;p&gt;
&#36890;&#36807;&#25439;&#22833;&#21387;&#32553;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20449;&#24687;&#29942;&#39048;(arXiv:2305.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression. (arXiv:2305.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#19968;&#33324;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20449;&#24687;&#29942;&#39048;&#20998;&#26512;&#65292;&#20197;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25311;&#21512;&#21644;&#21387;&#32553;&#38454;&#27573;&#12290;&#36890;&#36807;&#35813;&#20998;&#26512;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#20854;&#26680;&#24515;&#22312;&#20110;&#36319;&#36394;&#38544;&#34255;&#23618;&#19982;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20540;&#21644;&#38544;&#34255;&#23618;&#19982;DNN&#36755;&#20837;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20540;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#25454;Shwartz-Ziv&#21644;Tishby(2017)&#25552;&#20986;&#30340;&#20551;&#35828;&#65292;&#35757;&#32451;&#36807;&#31243;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#32452;&#25104;:&#25311;&#21512;&#21644;&#21387;&#32553;&#12290;&#21518;&#32773;&#34987;&#35748;&#20026;&#26159;DNNs&#34920;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#21407;&#22240;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#36827;&#34892;&#23545;&#19968;&#33324;NNs&#30340;IB&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Goldfeld&#31561;&#20154;(2019)&#25552;&#20986;&#30340;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#31181;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs). Its essence lies in tracking the dynamics of two mutual information (MI) values: one between the hidden layer and the class label, and the other between the hidden layer and the DNN input. According to the hypothesis put forth by Shwartz-Ziv and Tishby (2017), the training process consists of two distinct phases: fitting and compression. The latter phase is believed to account for the good generalization performance exhibited by DNNs. Due to the challenging nature of estimating MI between high-dimensional random vectors, this hypothesis has only been verified for toy NNs or specific types of NNs, such as quantized NNs and dropout NNs. In this paper, we introduce a comprehensive framework for conducting IB analysis of general NNs. Our approach leverages the stochastic NN method proposed by Goldfeld et al. (2019) and incorporates a compres
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#20855;&#26377; Kronecker &#32467;&#26500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24322;&#27493;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#32500;&#24230;&#30340;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#23436;&#25104;&#27599;&#27425;&#36845;&#20195;&#12290;</title><link>http://arxiv.org/abs/2305.08001</link><description>&lt;p&gt;
&#20855;&#26377;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#39640;&#25928;&#24322;&#27493;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Asynchronize Stochastic Gradient Algorithm with Structured Data. (arXiv:2305.08001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#20855;&#26377; Kronecker &#32467;&#26500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24322;&#27493;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#32500;&#24230;&#30340;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#23436;&#25104;&#27599;&#27425;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22240;&#20854;&#33391;&#22909;&#30340;&#27867;&#21270;&#32780;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#24555;&#36895;&#35757;&#32451;&#20855;&#26377;&#22823;&#37327;&#23618;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#21033;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#25216;&#26415;&#25110;&#26576;&#20123;&#25968;&#25454;&#32467;&#26500;&#30340;&#31354;&#38388;&#21010;&#20998;&#26469;&#20943;&#36731;&#27599;&#27425;&#36845;&#20195;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#36755;&#20837;&#25968;&#25454;&#28857;&#30340;&#35282;&#24230;&#21152;&#36895;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#35745;&#31639;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#38024;&#23545;&#19968;&#20010;&#20004;&#23618;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#19968;&#20123;&#29305;&#27530;&#23646;&#24615;&#65292;&#20363;&#22914; Kronecker &#32467;&#26500;&#26102;&#65292;&#27599;&#27425;&#36845;&#20195;&#21487;&#20197;&#22312;&#25968;&#25454;&#32500;&#24230;&#30340;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has achieved impressive success in a variety of fields because of its good generalization. However, it has been a challenging problem to quickly train a neural network with a large number of layers. The existing works utilize the locality-sensitive hashing technique or some data structures on space partitioning to alleviate the training cost in each iteration. In this work, we try accelerating the computations in each iteration from the perspective of input data points. Specifically, for a two-layer fully connected neural network, when the training data have some special properties, e.g., Kronecker structure, each iteration can be completed in sublinear time in the data dimension.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#20223;&#23556;&#23398;&#20064;&#65288;SAL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#12290; &#35813;&#27169;&#22411;&#36890;&#36807;&#35299;&#20915;&#28041;&#21450;&#28608;&#27963;&#20989;&#25968;&#30340;&#20108;&#27425;/&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#23398;&#20064;&#20223;&#23556;&#26144;&#23556;&#65292;&#20294;&#26159;&#22312;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#24046;&#20043;&#21518;&#12290;</title><link>http://arxiv.org/abs/2305.07996</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#20223;&#23556;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Successive Affine Learning for Deep Neural Networks. (arXiv:2305.07996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#20223;&#23556;&#23398;&#20064;&#65288;SAL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#12290; &#35813;&#27169;&#22411;&#36890;&#36807;&#35299;&#20915;&#28041;&#21450;&#28608;&#27963;&#20989;&#25968;&#30340;&#20108;&#27425;/&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#23398;&#20064;&#20223;&#23556;&#26144;&#23556;&#65292;&#20294;&#26159;&#22312;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#24046;&#20043;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#36830;&#32493;&#20223;&#23556;&#23398;&#20064;(SAL)&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;DNN&#26159;&#36890;&#36807;&#35299;&#20915;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#26500;&#24314;&#30340;&#12290;&#30001;&#20110;&#20854;&#38750;&#20984;&#24615;&#21644;&#23618;&#25968;&#20247;&#22810;&#65292;&#36890;&#24120;&#38590;&#20197;&#22312;&#25968;&#20540;&#19978;&#35299;&#20915;&#36825;&#31181;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20316;&#32773;&#21551;&#21457;&#20110;&#20154;&#31867;&#25945;&#32946;&#31995;&#32479;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#22810;&#32423;&#28145;&#24230;&#23398;&#20064;(MGDL)&#27169;&#22411;&#12290;MGDL&#27169;&#22411;&#20197;&#22810;&#20010;&#24180;&#32423;&#30340;&#24418;&#24335;&#23398;&#20064;DNN&#65292;&#22312;&#27599;&#20010;&#24180;&#32423;&#20013;&#26500;&#24314;&#30001;&#23569;&#37327;&#23618;&#25968;&#32452;&#25104;&#30340;&#27973;&#23618;DNN&#12290;MGDL&#27169;&#22411;&#20173;&#38656;&#35201;&#35299;&#20915;&#20960;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;SAL&#27169;&#22411;&#26159;&#22312;MGDL&#27169;&#22411;&#22522;&#30784;&#19978;&#28436;&#21464;&#32780;&#26469;&#12290;&#21457;&#29616;DNN&#30340;&#27599;&#23618;&#37117;&#30001;&#20223;&#23556;&#26144;&#23556;&#21644;&#28608;&#27963;&#20989;&#25968;&#32452;&#25104;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35299;&#20915;&#28041;&#21450;&#28608;&#27963;&#20989;&#25968;&#30340;&#20108;&#27425;/&#20984;&#20248;&#21270;&#38382;&#39064;&#26469;&#23398;&#20064;&#20223;&#23556;&#26144;&#23556;&#65292;&#20294;&#26159;&#22312;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#24046;&#20043;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a successive affine learning (SAL) model for constructing deep neural networks (DNNs). Traditionally, a DNN is built by solving a non-convex optimization problem. It is often challenging to solve such a problem numerically due to its non-convexity and having a large number of layers. To address this challenge, inspired by the human education system, the multi-grade deep learning (MGDL) model was recently initiated by the author of this paper. The MGDL model learns a DNN in several grades, in each of which one constructs a shallow DNN consisting of a small number of layers. The MGDL model still requires solving several non-convex optimization problems. The proposed SAL model mutates from the MGDL model. Noting that each layer of a DNN consists of an affine map followed by an activation function, we propose to learn the affine map by solving a quadratic/convex optimization problem which involves the activation function only {\it after} the weight matrix and the bias
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20598;&#24418;&#24335;&#30340;&#38750;&#36127;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24352;&#37327;&#34917;&#20840;&#20013;&#24573;&#35270;&#25968;&#25454;&#38750;&#36127;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07976</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#20598;&#24418;&#24335;&#23398;&#20064;&#38750;&#36127;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#21450;&#20854;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#20462;&#22797;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Nonnegative Low-Rank Tensor Completion via Dual Formulation with Applications to Image and Video Completion. (arXiv:2305.07976v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20598;&#24418;&#24335;&#30340;&#38750;&#36127;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24352;&#37327;&#34917;&#20840;&#20013;&#24573;&#35270;&#25968;&#25454;&#38750;&#36127;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24352;&#37327;&#34917;&#20840;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#25968;&#25454;&#30340;&#38750;&#36127;&#32467;&#26500;&#12290;&#26412;&#25991;&#32771;&#34385;&#23398;&#20064;&#19968;&#20010;&#38750;&#36127;&#20302;&#31209;&#24352;&#37327;&#65292;&#24182;&#21033;&#29992;&#23545;&#20598;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#12290;&#36825;&#31181;&#20998;&#35299;&#23558;&#38750;&#36127;&#32422;&#26463;&#20174;&#20302;&#31209;&#32422;&#26463;&#20013;&#20998;&#31163;&#20986;&#26469;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#38382;&#39064;&#26159;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#31181;&#30340;&#40654;&#26364;&#20849;&#36717;&#26799;&#24230;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#39068;&#33394;&#22270;&#20687;&#20462;&#34917;&#12289;&#35270;&#39057;&#34917;&#20840;&#21644;&#39640;&#20809;&#35889;&#22270;&#20687;&#34917;&#20840;&#31561;&#22810;&#20010;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches to the tensor completion problem have often overlooked the nonnegative structure of the data. We consider the problem of learning a nonnegative low-rank tensor, and using duality theory, we propose a novel factorization of such tensors. The factorization decouples the nonnegative constraints from the low-rank constraints. The resulting problem is an optimization problem on manifolds, and we propose a variant of Riemannian conjugate gradients to solve it. We test the proposed algorithm across various tasks such as colour image inpainting, video completion, and hyperspectral image completion. Experimental results show that the proposed method outperforms many state-of-the-art tensor completion algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#38271;&#26399;&#25345;&#32493;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26159;&#21542;&#33021;&#25552;&#39640;&#33021;&#37327;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#39044;&#31639;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07973</link><description>&lt;p&gt;
&#35770;&#38543;&#26426;&#23433;&#20840;&#24615;&#30340;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
On the Computational Cost of Stochastic Security. (arXiv:2305.07973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#38271;&#26399;&#25345;&#32493;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26159;&#21542;&#33021;&#25552;&#39640;&#33021;&#37327;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#39044;&#31639;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30340;&#38271;&#26399;&#25345;&#32493;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26159;&#21542;&#20250;&#25552;&#39640;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#25152;&#36798;&#21040;&#30340;&#34920;&#24449;&#36136;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#26041;&#26696;&#65292;&#20854;&#20013;&#20351;&#29992;&#35757;&#32451;&#36807;&#30340;EBM&#30340;&#25193;&#25955;&#36807;&#31243;&#30340;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#29992;&#20110;&#25552;&#39640;&#29420;&#31435;&#20998;&#31867;&#22120;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25345;&#32493;&#23545;&#27604;&#25955;&#24230;&#30340;&#35745;&#31639;&#39044;&#31639;&#22686;&#21152;&#21513;&#24067;&#26031;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#28548;&#28165;&#20102;&#23454;&#29616;&#26377;&#25928;&#20174;&#36830;&#32493;&#33021;&#37327;&#21183;&#20013;&#36827;&#34892;&#21513;&#24067;&#26031;&#37319;&#26679;&#30340;&#26032;&#37327;&#23376;&#21644;&#32463;&#20856;&#30828;&#20214;&#21644;&#36719;&#20214;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate whether long-run persistent chain Monte Carlo simulation of Langevin dynamics improves the quality of the representations achieved by energy-based models (EBM). We consider a scheme wherein Monte Carlo simulation of a diffusion process using a trained EBM is used to improve the adversarial robustness and the calibration score of an independent classifier network. Our results show that increasing the computational budget of Gibbs sampling in persistent contrastive divergence improves the calibration and adversarial robustness of the model, elucidating the practical merit of realizing new quantum and classical hardware and software for efficient Gibbs sampling from continuous energy potentials.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32039;&#33268;&#24555;&#36895;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#38480;&#65292;&#21487;&#20197;&#20445;&#35777;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#22270;&#23884;&#20837;&#22312;&#23454;&#38469;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#19979;&#30340;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2305.07971</link><description>&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#20013;&#22270;&#23884;&#20837;&#30340;&#32039;&#33268;&#24555;&#36895;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Tight and fast generalization error bound of graph embedding in metric space. (arXiv:2305.07971v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32039;&#33268;&#24555;&#36895;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#38480;&#65292;&#21487;&#20197;&#20445;&#35777;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#22270;&#23884;&#20837;&#22312;&#23454;&#38469;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#19979;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#23454;&#29616;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#22270;&#23884;&#20837;&#65292;&#26088;&#22312;&#33719;&#24471;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#21453;&#26144;&#22270;&#32467;&#26500;&#30340;&#39030;&#28857;&#34920;&#31034;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#30340;&#22270;&#23884;&#20837;&#22312;&#23884;&#20837;&#20855;&#26377;&#20998;&#23618;&#26641;&#32467;&#26500;&#30340;&#22270;&#19978;&#65292;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#30693;&#35782;&#24211;&#20013;&#30340;&#25968;&#25454;&#65292;&#24050;&#32463;&#33719;&#24471;&#20102;&#23454;&#39564;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;&#65292;&#38750;&#27431;&#20960;&#37324;&#24471;&#22270;&#23884;&#20837;&#30340;&#27867;&#21270;&#35823;&#24046;&#27604;&#27431;&#20960;&#37324;&#24471;&#22270;&#23884;&#20837;&#30340;&#35823;&#24046;&#19978;&#38480;&#35201;&#39640;&#24471;&#22810;&#65292;&#39640;&#27867;&#21270;&#35823;&#24046;&#34920;&#26126;&#25968;&#25454;&#20013;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#22122;&#22768;&#21487;&#33021;&#20250;&#26174;&#33879;&#25439;&#22351;&#23398;&#20064;&#24615;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#29616;&#26377;&#30340;&#30028;&#38480;&#19981;&#33021;&#20445;&#35777;&#23454;&#38469;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#19979;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#22270;&#23884;&#20837;&#25104;&#21151;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#27490;&#38750;&#27431;&#20960;&#37324;&#24471;&#22270;&#23884;&#20837;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#24230;&#37327;&#31354;&#38388;&#20013;&#22270;&#23884;&#20837;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#32039;&#20945;&#19988;&#24555;&#36895;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#23454;&#38469;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#19979;&#38750;&#27431;&#20960;&#37324;&#24471;&#22270;&#23884;&#20837;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have experimentally shown that we can achieve in non-Euclidean metric space effective and efficient graph embedding, which aims to obtain the vertices' representations reflecting the graph's structure in the metric space. Specifically, graph embedding in hyperbolic space has experimentally succeeded in embedding graphs with hierarchical-tree structure, e.g., data in natural languages, social networks, and knowledge bases. However, recent theoretical analyses have shown a much higher upper bound on non-Euclidean graph embedding's generalization error than Euclidean one's, where a high generalization error indicates that the incompleteness and noise in the data can significantly damage learning performance. It implies that the existing bound cannot guarantee the success of graph embedding in non-Euclidean metric space in a practical training data size, which can prevent non-Euclidean graph embedding's application in real problems. This paper provides a novel upper bound of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#20302;&#31209;&#24352;&#37327;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#22312;&#27969;&#24418;&#19978;&#27714;&#35299;&#65292;&#35299;&#20915;&#20102;&#37096;&#20998;&#35266;&#27979;&#21644;&#32467;&#26500;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07967</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#20302;&#31209;&#24352;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structured Low-Rank Tensor Learning. (arXiv:2305.07967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#20302;&#31209;&#24352;&#37327;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#22312;&#27969;&#24418;&#19978;&#27714;&#35299;&#65292;&#35299;&#20915;&#20102;&#37096;&#20998;&#35266;&#27979;&#21644;&#32467;&#26500;&#32422;&#26463;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#20855;&#26377;&#32467;&#26500;&#32422;&#26463;&#30340;&#37096;&#20998;&#35266;&#23519;&#24773;&#20917;&#19979;&#23398;&#20064;&#20302;&#31209;&#24352;&#37327;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#38382;&#39064;&#26159;&#22312;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#38454;&#21644;&#20108;&#38454;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#24050;&#25512;&#23548;&#20986;&#25152;&#24471;&#38382;&#39064;&#30340;&#23545;&#20598;&#38388;&#38553;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#38750;&#36127;&#32422;&#26463;&#21644;Hankel&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning low-rank tensors from partial observations with structural constraints, and propose a novel factorization of such tensors, which leads to a simpler optimization problem. The resulting problem is an optimization problem on manifolds. We develop first-order and second-order Riemannian optimization algorithms to solve it. The duality gap for the resulting problem is derived, and we experimentally verify the correctness of the proposed algorithm. We demonstrate the algorithm on nonnegative constraints and Hankel constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#65292;&#35299;&#20915;&#22312;&#35813;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.07961</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#65292;&#35299;&#20915;&#22312;&#35813;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#21551;&#29992;&#23454;&#26102;&#30340;&#22810;&#36718;&#23545;&#35805;&#20351;&#29992;&#25143;&#26356;&#21152;&#36879;&#26126;&#21644;&#25484;&#25511;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#23545;&#35805;&#33258;&#28982;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#19990;&#30028;&#30693;&#35782;&#21644;&#24120;&#35782;&#25512;&#29702;&#34701;&#20837;&#21040;&#35821;&#35328;&#29702;&#35299;&#20013;&#65292;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#36825;&#19968;&#33539;&#24335;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#21253;&#25324;&#36866;&#24403;&#22320;&#29702;&#35299;&#21644;&#25511;&#21046;&#22797;&#26434;&#30340;&#23545;&#35805;&#21644;&#20174;&#22806;&#37096;&#20449;&#24687;&#28304;&#26816;&#32034;&#12290;&#30001;&#20110;&#22823;&#32780;&#19981;&#26029;&#22686;&#38271;&#30340;&#39033;&#30446;&#35821;&#26009;&#24211;&#21644;&#32570;&#20047;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#38382;&#39064;&#21152;&#21095;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#25143;&#20559;&#22909;&#29702;&#35299;&#12289;&#28789;&#27963;&#30340;&#23545;&#35805;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#20316;&#20026;&#25972;&#20010;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#30340;&#26032;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28436;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35825;&#23548;&#20998;&#31867;&#26641;&#65292;&#21033;&#29992;&#35760;&#24518;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#32467;&#26500;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#24403;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.07959</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#26641;&#20248;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#35760;&#24518;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A Novel Memetic Strategy for Optimized Learning of Classification Trees. (arXiv:2305.07959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28436;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35825;&#23548;&#20998;&#31867;&#26641;&#65292;&#21033;&#29992;&#35760;&#24518;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#32467;&#26500;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#24403;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20998;&#31867;&#26641;&#22240;&#20854;&#29627;&#29827;&#30418;&#32467;&#26500;&#20877;&#27425;&#24341;&#36215;&#20102;&#31185;&#23398;&#30028;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#36138;&#24515;&#31639;&#27861;&#26500;&#24314;&#65292;&#36890;&#36807;&#35299;&#20915;&#38382;&#39064;&#26469;&#25214;&#21040;&#26368;&#23567;&#21270;&#26576;&#20123;&#19981;&#32431;&#24230;&#24230;&#37327;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20999;&#21106;&#28857;&#12290;&#19982;&#36825;&#31181;&#26631;&#20934;&#30340;&#36138;&#24515;&#26041;&#27861;&#21644;&#26368;&#36817;&#36890;&#36807;&#22522;&#20110;MILP&#30340;&#31934;&#30830;&#20844;&#24335;&#26469;&#23450;&#20041;&#23398;&#20064;&#38382;&#39064;&#30340;&#36827;&#23637;&#30456;&#21453;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28436;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35825;&#23548;&#20998;&#31867;&#26641;&#65292;&#21033;&#29992;&#19968;&#31181;&#35760;&#24518;&#26041;&#27861;&#22788;&#29702;&#25968;&#20197;&#21315;&#35745;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#21487;&#34892;&#35299;&#31354;&#38388;&#30340;&#25506;&#32034;&#19982;&#23616;&#37096;&#25628;&#32034;&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the increasing interest in interpretable machine learning, classification trees have again attracted the attention of the scientific community because of their glass-box structure. These models are usually built using greedy procedures, solving subproblems to find cuts in the feature space that minimize some impurity measures. In contrast to this standard greedy approach and to the recent advances in the definition of the learning problem through MILP-based exact formulations, in this paper we propose a novel evolutionary algorithm for the induction of classification trees that exploits a memetic approach that is able to handle datasets with thousands of points. Our procedure combines the exploration of the feasible space of solutions with local searches to obtain structures with generalization capabilities that are competitive with the state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#20445;&#35777;&#23433;&#20840;&#31574;&#30053;&#20248;&#21270;(SPI)&#30340;&#24615;&#33021;&#65292;&#24182;&#38477;&#20302;&#20102;SPIBB&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.07958</link><description>&lt;p&gt;
&#26356;&#23569;&#30340;&#25968;&#25454;&#26356;&#24378;&#30340;&#23433;&#20840;&#31574;&#30053;&#20248;&#21270;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
More for Less: Safe Policy Improvement With Stronger Performance Guarantees. (arXiv:2305.07958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#20445;&#35777;&#23433;&#20840;&#31574;&#30053;&#20248;&#21270;(SPI)&#30340;&#24615;&#33021;&#65292;&#24182;&#38477;&#20302;&#20102;SPIBB&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#23433;&#20840;&#31574;&#30053;&#20248;&#21270;(SPI)&#38382;&#39064;&#26088;&#22312;&#26681;&#25454;&#29983;&#25104;&#26679;&#26412;&#25968;&#25454;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;SPI&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#36739;&#39640;&#25968;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#25552;&#20379;&#23545;&#25913;&#36827;&#31574;&#30053;&#24615;&#33021;&#30340;&#23454;&#38469;&#27010;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;SPI&#38382;&#39064;&#65292;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#21363;&#21487;&#33719;&#24471;&#36825;&#26679;&#30340;&#20445;&#35777;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#35777;&#26126;&#36825;&#20123;&#20445;&#35777;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#21644;&#22522;&#30784;&#29615;&#22659;&#27169;&#22411;&#19978;&#30340;&#38544;&#24335;&#36716;&#25442;&#65292;&#36825;&#20123;&#36716;&#25442;&#20316;&#20026;&#25512;&#23548;&#26356;&#32039;&#23494;&#30340;SPI&#25913;&#36827;&#30028;&#38480;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#20351;&#29992;&#19987;&#19994;&#30340;SPI&#19982;&#22522;&#32447;&#24341;&#23548;(SPIBB)&#31639;&#27861;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#23454;&#26174;&#33879;&#38477;&#20302;&#20102;SPIBB&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an offline reinforcement learning setting, the safe policy improvement (SPI) problem aims to improve the performance of a behavior policy according to which sample data has been generated. State-of-the-art approaches to SPI require a high number of samples to provide practical probabilistic guarantees on the improved policy's performance. We present a novel approach to the SPI problem that provides the means to require less data for such guarantees. Specifically, to prove the correctness of these guarantees, we devise implicit transformations on the data set and the underlying environment model that serve as theoretical foundations to derive tighter improvement bounds for SPI. Our empirical evaluation, using the well-established SPI with baseline bootstrapping (SPIBB) algorithm, on standard benchmarks shows that our method indeed significantly reduces the sample complexity of the SPIBB algorithm.
&lt;/p&gt;</description></item><item><title>CodeT5+&#26159;&#19968;&#32452;&#28789;&#27963;&#32452;&#21512;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#28151;&#21512;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#27604;&#29616;&#26377;&#20195;&#30721;-specific LLMs&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07922</link><description>&lt;p&gt;
CodeT5+: &#29992;&#20110;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#24320;&#25918;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeT5+: Open Code Large Language Models for Code Understanding and Generation. (arXiv:2305.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07922
&lt;/p&gt;
&lt;p&gt;
CodeT5+&#26159;&#19968;&#32452;&#28789;&#27963;&#32452;&#21512;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#28151;&#21512;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#27604;&#29616;&#26377;&#20195;&#30721;-specific LLMs&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#37327;&#28304;&#20195;&#30721;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#30721;LLM&#22312;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#26041;&#38754;&#26377;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#29305;&#23450;&#30340;&#26550;&#26500;(&#20165;&#32534;&#30721;&#22120;&#25110;&#20165;&#35299;&#30721;&#22120;)&#25110;&#20381;&#36182;&#20110;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12290;&#21069;&#19968;&#31181;&#33539;&#24335;&#21463;&#21040;&#24212;&#29992;&#28789;&#27963;&#24615;&#30340;&#38480;&#21046;&#65292;&#32780;&#22312;&#21518;&#19968;&#31181;&#33539;&#24335;&#20013;&#65292;&#27169;&#22411;&#34987;&#35270;&#20026;&#25152;&#26377;&#20219;&#21153;&#30340;&#21333;&#19968;&#31995;&#32479;&#65292;&#23548;&#33268;&#22312;&#26576;&#20123;&#20219;&#21153;&#30340;&#23376;&#38598;&#19978;&#24615;&#33021;&#19981;&#20248;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#21487;&#33021;&#19982;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#19981;&#30456;&#20851;&#65292;&#22240;&#27492;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;CodeT5+&#8221;&#65292;&#36825;&#26159;&#19968;&#32452;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#20854;&#20013;&#32452;&#20214;&#27169;&#22359;&#21487;&#20197;&#28789;&#27963;&#32452;&#21512;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#26159;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#28151;&#21512;&#39044;&#35757;&#32451;&#30446;&#26631;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;CodeT5+&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#20195;&#30721;&#29305;&#23450;LLM&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretrai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24310;&#36831;&#36172;&#21338;&#21453;&#39304;&#30340;&#23545;&#25239;MDP&#20013;&#30340;PO&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;DAPO&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;tabular MDP&#21644;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#21518;&#24724;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.07911</link><description>&lt;p&gt;
&#20855;&#26377;&#24310;&#36831;&#36172;&#21338;&#21453;&#39304;&#30340;&#23545;&#25239;MDP&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;&#31574;&#30053;&#20248;&#21270;&#21644;&#25913;&#36827;&#21518;&#24724;
&lt;/p&gt;
&lt;p&gt;
Delay-Adapted Policy Optimization and Improved Regret for Adversarial MDP with Delayed Bandit Feedback. (arXiv:2305.07911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#24310;&#36831;&#36172;&#21338;&#21453;&#39304;&#30340;&#23545;&#25239;MDP&#20013;&#30340;PO&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;DAPO&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;tabular MDP&#21644;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#21518;&#24724;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#20248;&#21270;&#65288;PO&#65289;&#26159;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;PO&#31639;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#24050;&#32463;&#25104;&#20026;RL&#30028;&#23588;&#20026;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20960;&#20046;&#25152;&#26377;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#23384;&#22312;&#30340;&#25361;&#25112;&#30340;&#23545;&#25239;MDP&#20013;&#30340;PO - \textit{&#24310;&#36831;&#36172;&#21338;&#21453;&#39304;} &#12290;&#25105;&#20204;&#20026;tabular MDP&#20013;&#30340;PO&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#21518;&#24724;&#19978;&#30028;&#65292;&#29978;&#33267;&#21487;&#33021;&#36229;&#36807;&#20102;&#29366;&#24577;-of-the-art&#65292;&#32780;&#20854;&#20351;&#29992;&#30340;&#26159;&#19981;&#22826;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;Delay-Adapted PO&#65288;DAPO&#65289;&#26131;&#20110;&#23454;&#29616;&#21644;&#25512;&#24191;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#25193;&#23637;&#21040;&#65306;&#65288;i&#65289;&#22312;&#32447;&#24615;$Q$&#20989;&#25968;&#30340;&#20551;&#35774;&#19979;&#20855;&#26377;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#24310;&#36831;&#21453;&#39304;&#30340;&#31532;&#19968;&#20010;&#21518;&#24724;&#30028;&#65307;&#65288;ii&#65289;&#28145;&#24230;RL&#65292;&#22312;MuJoCo&#39046;&#22495;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization (PO) is one of the most popular methods in Reinforcement Learning (RL). Thus, theoretical guarantees for PO algorithms have become especially important to the RL community. In this paper, we study PO in adversarial MDPs with a challenge that arises in almost every real-world application -- \textit{delayed bandit feedback}. We give the first near-optimal regret bounds for PO in tabular MDPs, and may even surpass state-of-the-art (which uses less efficient methods). Our novel Delay-Adapted PO (DAPO) is easy to implement and to generalize, allowing us to extend our algorithm to: (i) infinite state space under the assumption of linear $Q$-function, proving the first regret bounds for delayed feedback with function approximation. (ii) deep RL, demonstrating its effectiveness in experiments on MuJoCo domains.
&lt;/p&gt;</description></item><item><title>&#32473;&#20986;&#20102;&#22312;&#38543;&#26426;&#27700;&#24211;&#27169;&#22411;&#19978;&#20351;&#29992;&#22352;&#26631;&#19979;&#38477;&#27861;&#36827;&#34892;&#20248;&#21270;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#23610;&#24230;&#23450;&#24459;&#65292;&#20026;&#30828;&#20214;&#32593;&#32476;&#20248;&#21270;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2305.07908</link><description>&lt;p&gt;
&#24067;&#23572;&#26435;&#37325;&#20248;&#21270;&#30340;&#25910;&#25947;&#24615;&#21644;&#23610;&#24230;&#22312;&#30828;&#20214;&#27700;&#24211;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convergence and scaling of Boolean-weight optimization for hardware reservoirs. (arXiv:2305.07908v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07908
&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#22312;&#38543;&#26426;&#27700;&#24211;&#27169;&#22411;&#19978;&#20351;&#29992;&#22352;&#26631;&#19979;&#38477;&#27861;&#36827;&#34892;&#20248;&#21270;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#23610;&#24230;&#23450;&#24459;&#65292;&#20026;&#30828;&#20214;&#32593;&#32476;&#20248;&#21270;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#30828;&#20214;&#21270;&#26159;&#23454;&#29616;&#19979;&#19968;&#20195;&#39640;&#25928;&#21644;&#24378;&#22823;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#38500;&#20102;&#23454;&#29616;&#24182;&#34892;&#12289;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#30828;&#20214;&#26550;&#26500;&#22806;&#65292;&#29992;&#25277;&#26679;&#26377;&#25928;&#30340;&#26041;&#27861;&#20248;&#21270;&#31995;&#32479;&#26497;&#22823;&#30340;&#21442;&#25968;&#31354;&#38388;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#22320;&#23548;&#20986;&#20102;&#39640;&#25928;&#22352;&#26631;&#19979;&#38477;&#27861;&#22312;&#20248;&#21270;&#38543;&#26426;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;--&#27700;&#24211;&#30340;&#35835;&#20986;&#23618;&#25152;&#38656;&#30340;&#23610;&#24230;&#23450;&#24459;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25910;&#25947;&#26159;&#25351;&#25968;&#32423;&#30340;&#65292;&#24182;&#19988;&#38543;&#30528;&#32593;&#32476;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#32447;&#24615;&#32553;&#25918;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23436;&#20840;&#22797;&#29616;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20809;&#23376;&#27700;&#24211;&#23454;&#39564;&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#23610;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#30828;&#20214;&#32593;&#32476;&#20248;&#21270;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#65292;&#24182;&#30830;&#23450;&#20102;&#26377;&#21069;&#36884;&#30340;&#20248;&#21270;&#25910;&#26463;&#36895;&#24230;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hardware implementation of neural network are an essential step to implement next generation efficient and powerful artificial intelligence solutions.  Besides the realization of a parallel, efficient and scalable hardware architecture, the optimization of the system's extremely large parameter space with sampling-efficient approaches is essential.  Here, we analytically derive the scaling laws for highly efficient Coordinate Descent applied to optimizing the readout layer of a random recurrently connection neural network, a reservoir.  We demonstrate that the convergence is exponential and scales linear with the network's number of neurons.  Our results perfectly reproduce the convergence and scaling of a large-scale photonic reservoir implemented in a proof-of-concept experiment.  Our work therefore provides a solid foundation for such optimization in hardware networks, and identifies future directions that are promising for optimizing convergence speed during learning leveraging mea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21704;&#33945;&#33707;&#29305; Hessian &#19968;&#33268;&#24615;&#30340;&#20840;&#20998;&#24067;&#24335;&#29275;&#39039;&#22411;&#20248;&#21270;&#31639;&#27861; Network-GIANT&#65292;&#23558;&#26799;&#24230;&#36319;&#36394;&#21644;&#29275;&#39039;&#22411;&#36845;&#20195;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#32463;&#35777;&#26126;&#23545;&#20005;&#26684;&#20984;&#21644;&#20809;&#28369;&#25439;&#22833;&#20989;&#25968;&#26377;&#21322;&#20840;&#23616;&#21644;&#25351;&#25968;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#30340;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126; Network-GIANT &#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914; Network-DANE &#21644; Newton-Raphson Consensus&#65289;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07898</link><description>&lt;p&gt;
Network-GIANT: &#22522;&#20110;&#21704;&#33945;&#33707;&#29305; Hessian &#19968;&#33268;&#24615;&#30340;&#20840;&#20998;&#24067;&#24335;&#29275;&#39039;&#22411;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Network-GIANT: Fully distributed Newton-type optimization via harmonic Hessian consensus. (arXiv:2305.07898v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21704;&#33945;&#33707;&#29305; Hessian &#19968;&#33268;&#24615;&#30340;&#20840;&#20998;&#24067;&#24335;&#29275;&#39039;&#22411;&#20248;&#21270;&#31639;&#27861; Network-GIANT&#65292;&#23558;&#26799;&#24230;&#36319;&#36394;&#21644;&#29275;&#39039;&#22411;&#36845;&#20195;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#32463;&#35777;&#26126;&#23545;&#20005;&#26684;&#20984;&#21644;&#20809;&#28369;&#25439;&#22833;&#20989;&#25968;&#26377;&#21322;&#20840;&#23616;&#21644;&#25351;&#25968;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#30340;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126; Network-GIANT &#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914; Network-DANE &#21644; Newton-Raphson Consensus&#65289;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20998;&#24067;&#24335;&#22810;&#20195;&#29702;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20840;&#23616;&#30446;&#26631;&#26159;&#36890;&#36807;&#26412;&#22320;&#20248;&#21270;&#21644;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#26469;&#26368;&#23567;&#21270;&#26412;&#22320;&#30446;&#26631;&#65288;&#32463;&#39564;&#25439;&#22833;&#65289;&#20989;&#25968;&#30340;&#24635;&#21644;&#12290; &#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29275;&#39039;&#22411;&#23436;&#20840;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;Network-GIANT&#65292;&#23427;&#22522;&#20110; GIANT&#65292;&#36825;&#26159;&#19968;&#31181;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290; Network-GIANT &#31639;&#27861;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#20351;&#29992;&#26799;&#24230;&#36319;&#36394;&#21644;&#29275;&#39039;&#22411;&#36845;&#20195;&#31639;&#27861;&#30340;&#32452;&#21512;&#20197;&#21450;&#26412;&#22320;&#26799;&#24230;&#21644;&#29275;&#39039;&#26356;&#26032;&#30340;&#20849;&#35782;&#24179;&#22343;&#26469;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#20102;&#23545;&#32593;&#32476;&#19978;&#30340;&#20005;&#26684;&#20984;&#21644;&#20809;&#28369;&#25439;&#22833;&#20989;&#25968;&#30340;&#21322;&#20840;&#23616;&#21644;&#25351;&#25968;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102; Network-GIANT &#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914; Network-DANE &#21644; Newton-Raphson Consensus&#65289;&#30340;&#25910;&#25947;&#24615;&#33021;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of distributed multi-agent learning, where the global aim is to minimize a sum of local objective (empirical loss) functions through local optimization and information exchange between neighbouring nodes. We introduce a Newton-type fully distributed optimization algorithm, Network-GIANT, which is based on GIANT, a Federated learning algorithm that relies on a centralized parameter server. The Network-GIANT algorithm is designed via a combination of gradient-tracking and a Newton-type iterative algorithm at each node with consensus based averaging of local gradient and Newton updates. We prove that our algorithm guarantees semi-global and exponential convergence to the exact solution over the network assuming strongly convex and smooth loss functions. We provide empirical evidence of the superior convergence performance of Network-GIANT over other state-of-art distributed learning algorithms such as Network-DANE and Newton-Raphson Consensus.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#19977;&#32500;&#26080;&#30417;&#30563;&#21644;(&#28145;&#24230;)&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22686;&#26448;&#21046;&#36896;&#20214;&#23380;&#38553;&#29575;&#26816;&#27979;&#30340;&#20307;&#32032;&#32423;&#20998;&#31867;&#65292;&#24471;&#20986;&#20351;&#29992;&#20307;&#32032;&#32423;&#20998;&#31867;&#30340;&#19977;&#32500;DL&#27169;&#22411;&#22312;AM&#38646;&#20214;&#30340;&#23380;&#38553;&#29575;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2305.07894</link><description>&lt;p&gt;
&#21033;&#29992;&#19977;&#32500;&#26080;&#30417;&#30563;&#21644;(&#28145;&#24230;)&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22686;&#26448;&#21046;&#36896;&#20214;&#23380;&#38553;&#29575;&#26816;&#27979;&#30340;&#20307;&#32032;&#32423;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Voxel-wise classification for porosity investigation of additive manufactured parts with 3D unsupervised and (deeply) supervised neural networks. (arXiv:2305.07894v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#19977;&#32500;&#26080;&#30417;&#30563;&#21644;(&#28145;&#24230;)&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22686;&#26448;&#21046;&#36896;&#20214;&#23380;&#38553;&#29575;&#26816;&#27979;&#30340;&#20307;&#32032;&#32423;&#20998;&#31867;&#65292;&#24471;&#20986;&#20351;&#29992;&#20307;&#32032;&#32423;&#20998;&#31867;&#30340;&#19977;&#32500;DL&#27169;&#22411;&#22312;AM&#38646;&#20214;&#30340;&#23380;&#38553;&#29575;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#21046;&#36896;&#24050;&#25104;&#20026;&#19968;&#31181;&#29983;&#20135;&#36827;&#31243;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#23383;&#27169;&#22411;&#20013;&#29983;&#20135;&#26679;&#26412;&#12290;&#20026;&#20102;&#30830;&#20445;&#22312;&#25972;&#20010;&#21046;&#36896;&#25209;&#27425;&#20013;&#30340;&#25152;&#26377;&#20135;&#21697;&#26679;&#26412;&#37117;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#65292;&#36890;&#24120;&#20351;&#29992;X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;X-CT&#65289;&#19982;&#33258;&#21160;&#21270;&#24322;&#24120;&#26816;&#27979;&#30456;&#32467;&#21512;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#36817;&#30340;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;X-CT&#22270;&#20687;&#30340;AM&#26679;&#21697;&#20013;&#30340;&#23380;&#38553;&#20998;&#26512;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20197;&#25509;&#21463;3D&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#20307;&#32032;&#32423;&#20998;&#31867;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#35201;&#27714;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20307;&#32032;&#32423;&#20998;&#31867;&#30340;&#19977;&#32500;DL&#27169;&#22411;&#22312;AM&#38646;&#20214;&#30340;&#23380;&#38553;&#29575;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive Manufacturing (AM) has emerged as a manufacturing process that allows the direct production of samples from digital models. To ensure that quality standards are met in all manufactured samples of a batch, X-ray computed tomography (X-CT) is often used combined with automated anomaly detection. For the latter, deep learning (DL) anomaly detection techniques are increasingly, as they can be trained to be robust to the material being analysed and resilient towards poor image quality. Unfortunately, most recent and popular DL models have been developed for 2D image processing, thereby disregarding valuable volumetric information.  This study revisits recent supervised (UNet, UNet++, UNet 3+, MSS-UNet) and unsupervised (VAE, ceVAE, gmVAE, vqVAE) DL models for porosity analysis of AM samples from X-CT images and extends them to accept 3D input data with a 3D-patch pipeline for lower computational requirements, improved efficiency and generalisability. The supervised models were trai
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#25552;&#39640;&#20803;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#30693;&#35782;&#20449;&#24687;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36866;&#24403;&#30340;MR&#30446;&#26631;&#23558;&#20803;&#30693;&#35782;&#21021;&#27493;&#38598;&#25104;&#21040;&#20803;&#30446;&#26631;&#20013;&#65292;&#26469;&#35268;&#33539;&#20803;&#27169;&#22411;&#20989;&#25968;&#31867;&#30340;&#23481;&#37327;&#22797;&#26434;&#24230;&#65292;&#26377;&#21161;&#20110;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07892</link><description>&lt;p&gt;
DAC-MR: &#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#19968;&#33268;&#24615;&#30340;&#20803;&#23398;&#20064;&#20803;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning. (arXiv:2305.07892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07892
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#20803;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20803;&#30693;&#35782;&#20449;&#24687;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36866;&#24403;&#30340;MR&#30446;&#26631;&#23558;&#20803;&#30693;&#35782;&#21021;&#27493;&#38598;&#25104;&#21040;&#20803;&#30446;&#26631;&#20013;&#65292;&#26469;&#35268;&#33539;&#20803;&#27169;&#22411;&#20989;&#25968;&#31867;&#30340;&#23481;&#37327;&#22797;&#26434;&#24230;&#65292;&#26377;&#21161;&#20110;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20803;&#23398;&#20064;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20869;&#22791;&#21463;&#20851;&#27880;&#24182;&#25512;&#21160;&#20102;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;&#34920;&#29616;&#33391;&#22909;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#20855;&#26377;&#39640;&#36136;&#37327;&#20803;&#25968;&#25454;&#30340;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#34920;&#31034;&#24213;&#23618;&#20219;&#21153;&#27867;&#21270;&#30446;&#26631;&#65292;&#26377;&#26102;&#23545;&#20110;&#30495;&#23454;&#24212;&#29992;&#32780;&#35328;&#38590;&#20197;&#33719;&#24471;&#12290;&#24403;&#21069;&#22522;&#20110;&#20803;&#25968;&#25454;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#20351;&#29992;&#19981;&#23436;&#32654;&#30340;&#35757;&#32451;&#20219;&#21153;&#35757;&#32451;&#20196;&#20154;&#28385;&#24847;&#30340;&#20803;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#30693;&#35782;&#20449;&#24687;&#22686;&#24378;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65288;MKIML&#65289;&#65292;&#36890;&#36807;&#23558;&#34917;&#20607;&#30340;&#20803;&#30693;&#35782;&#38598;&#25104;&#21040;&#20803;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#20840;&#38754;&#25552;&#39640;&#20803;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36866;&#24403;&#30340;&#20803;&#27491;&#21017;&#21270;&#65288;MR&#65289;&#30446;&#26631;&#23558;&#20803;&#30693;&#35782;&#21021;&#27493;&#38598;&#25104;&#21040;&#20803;&#30446;&#26631;&#20013;&#65292;&#26469;&#35268;&#33539;&#20803;&#27169;&#22411;&#20989;&#25968;&#31867;&#30340;&#23481;&#37327;&#22797;&#26434;&#24230;&#65292;&#26377;&#21161;&#20110;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20316;&#20026;&#19968;&#31181;&#23454;&#29992;&#21270;&#23454;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta learning recently has been heavily researched and helped advance the contemporary machine learning. However, achieving well-performing meta-learning model requires a large amount of training tasks with high-quality meta-data representing the underlying task generalization goal, which is sometimes difficult and expensive to obtain for real applications. Current meta-data-driven meta-learning approaches, however, are fairly hard to train satisfactory meta-models with imperfect training tasks. To address this issue, we suggest a meta-knowledge informed meta-learning (MKIML) framework to improve meta-learning by additionally integrating compensated meta-knowledge into meta-learning process. We preliminarily integrate meta-knowledge into meta-objective via using an appropriate meta-regularization (MR) objective to regularize capacity complexity of the meta-model function class to facilitate better generalization on unseen tasks. As a practical implementation, we introduce data augmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;VINO&#65292;&#36890;&#36807;&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#21069;&#21521;&#39044;&#27979;&#21644;&#21453;&#21521;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#26041;&#38754;&#21487;&#20197;&#27604;&#20256;&#32479;&#26377;&#38480;&#20803;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#21028;&#26029;&#12290;</title><link>http://arxiv.org/abs/2305.07889</link><description>&lt;p&gt;
&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural operator for structural simulation and bridge health monitoring. (arXiv:2305.07889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32467;&#26500;&#27169;&#25311;&#21644;&#26725;&#26753;&#20581;&#24247;&#30417;&#27979;&#30340;&#31070;&#32463;&#36816;&#31639;&#22120;VINO&#65292;&#36890;&#36807;&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#21069;&#21521;&#39044;&#27979;&#21644;&#21453;&#21521;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#26041;&#38754;&#21487;&#20197;&#27604;&#20256;&#32479;&#26377;&#38480;&#20803;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#21644;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#32467;&#26500;&#24037;&#31243;&#30456;&#32467;&#21512;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#29992;&#20110;&#21069;&#21521;&#38382;&#39064;&#65288;&#32467;&#26500;&#27169;&#25311;&#65289;&#21644;&#21453;&#21521;&#38382;&#39064;&#65288;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65289;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#36816;&#31639;&#22120;&#65292;&#25552;&#20986;&#20102;VINO&#65288;&#36710;&#36742;-&#26725;&#26753;&#30456;&#20114;&#20316;&#29992;&#31070;&#32463;&#36816;&#31639;&#22120;&#65289;&#65292;&#20316;&#20026;&#26725;&#26753;&#32467;&#26500;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;VINO&#23398;&#20064;&#32467;&#26500;&#21709;&#24212;&#22330;&#21644;&#25439;&#20260;&#22330;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36816;&#34892;&#21442;&#25968;&#26377;&#38480;&#20803;&#65288;FE&#65289;&#27169;&#25311;&#65292;&#32771;&#34385;&#32467;&#26500;&#21021;&#22987;&#25439;&#20260;&#22330;&#30340;&#38543;&#26426;&#20998;&#24067;&#65292;&#24314;&#31435;&#20102;VBI-FE&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#22312;&#22235;&#31181;&#25439;&#20260;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#20135;&#29983;&#20102;VBI-EXP&#25968;&#25454;&#38598;&#12290;&#22312;VINO&#36890;&#36807;VBI-FE&#39044;&#35757;&#32451;&#24182;&#22312;&#20581;&#24247;&#29366;&#24577;&#19979;&#36890;&#36807;VBI-EXP&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#23454;&#29616;&#20102;&#20197;&#19979;&#20004;&#20010;&#25913;&#36827;&#12290;&#39318;&#20808;&#65292;&#21069;&#21521;&#30340;VINO&#27604;FE&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#20174;&#25439;&#20260;&#22330;&#36755;&#20837;&#39044;&#27979;&#32467;&#26500;&#21709;&#24212;&#12290;&#20854;&#27425;&#65292;&#21453;&#21521;&#30340;VINO&#21487;&#20197;&#30830;&#23450;&#25439;&#20260;&#21306;&#22495;&#21644;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infusing deep learning with structural engineering has received widespread attention for both forward problems (structural simulation) and inverse problems (structural health monitoring). Based on Fourier Neural Operator, this study proposes VINO (Vehicle-bridge Interaction Neural Operator) to serve as the digital twin of bridge structures. VINO learns mappings between structural response fields and damage fields. In this study, VBI-FE dataset was established by running parametric finite element (FE) simulations considering a random distribution of structural initial damage field. Subsequently, VBI-EXP dataset was produced by conducting an experimental study under four damage scenarios. After VINO was pre-trained by VBI-FE and fine-tuned by VBI-EXP from the bridge at the healthy state, the model achieved the following two improvements. First, forward VINO can predict structural responses from damage field inputs more accurately than the FE model. Second, inverse VINO can determine, loc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#39046;&#22495;&#27867;&#21270;&#65288;CDG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#28872;&#23545;&#27604;&#30340;&#25968;&#25454;&#23545;&#25152;&#23637;&#31034;&#30340;&#35821;&#20041;&#19981;&#21464;&#24615;&#36827;&#34892;&#21033;&#29992;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#8212;&#8212;Logit Attribution Matching (LAM)&#65292;&#20197;&#23454;&#29616;CDG&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAM&#20165;&#20351;&#29992;&#23569;&#37327;&#37197;&#23545;&#25968;&#25454;&#23601;&#33021;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;DG&#26041;&#27861;&#65292;&#19988;&#26377;&#21161;&#20110;&#27169;&#22411;&#26356;&#22909;&#22320;&#20851;&#27880;&#23545;&#39046;&#22495;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.07888</link><description>&lt;p&gt;
&#36890;&#36807;Logit Attribution&#21305;&#37197;&#23454;&#29616;&#23545;&#27604;&#24230;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Contrastive Domain Generalization via Logit Attribution Matching. (arXiv:2305.07888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#39046;&#22495;&#27867;&#21270;&#65288;CDG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#28872;&#23545;&#27604;&#30340;&#25968;&#25454;&#23545;&#25152;&#23637;&#31034;&#30340;&#35821;&#20041;&#19981;&#21464;&#24615;&#36827;&#34892;&#21033;&#29992;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#8212;&#8212;Logit Attribution Matching (LAM)&#65292;&#20197;&#23454;&#29616;CDG&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAM&#20165;&#20351;&#29992;&#23569;&#37327;&#37197;&#23545;&#25968;&#25454;&#23601;&#33021;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;DG&#26041;&#27861;&#65292;&#19988;&#26377;&#21161;&#20110;&#27169;&#22411;&#26356;&#22909;&#22320;&#20851;&#27880;&#23545;&#39046;&#22495;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#28145;&#24230;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#29978;&#33267;&#24494;&#23567;&#31243;&#24230;&#30340;&#39046;&#22495;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20005;&#37325;&#25439;&#23475;&#20854;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#35757;&#32451;&#39046;&#22495;&#19978;&#21152;&#24378;&#21508;&#31181;&#19981;&#21464;&#37327;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#20026;&#26032;&#30340;&#27979;&#35797;&#39046;&#22495;&#25552;&#20379;&#24456;&#22909;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#23545;&#27604;&#39046;&#22495;&#27867;&#21270;&#65288;CDG&#65289;&#65292;&#23427;&#21033;&#29992;&#24378;&#28872;&#23545;&#27604;&#30340;&#25968;&#25454;&#23545;&#25152;&#23637;&#31034;&#30340;&#35821;&#20041;&#19981;&#21464;&#24615;&#65292;&#32780;&#19981;&#26159;&#22810;&#20010;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#39046;&#22495;&#27867;&#21270;&#29702;&#35770;&#65292;&#23637;&#31034;&#20102;CDG&#30340;&#28508;&#22312;&#33021;&#21147;&#65307;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#8212;&#8212;Logit Attribution Matching (LAM)&#65292;&#20197;&#23454;&#29616;CDG&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#65292;LAM&#20165;&#20351;&#29992;&#23569;&#37327;&#37197;&#23545;&#25968;&#25454;&#23601;&#33021;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;DG&#26041;&#27861;&#65292;&#32780;&#19988;LAM&#26377;&#21161;&#20110;&#27169;&#22411;&#26356;&#22909;&#22320;&#20851;&#27880;&#23545;&#39046;&#22495;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization (DG) is an important open problem in machine learning. Deep models are susceptible to domain shifts of even minute degrees, which severely compromises their reliability in real applications. To alleviate the issue, most existing methods enforce various invariant constraints across multiple training domains. However,such an approach provides little performance guarantee for novel test domains in general. In this paper, we investigate a different approach named Contrastive Domain Generalization (CDG), which exploits semantic invariance exhibited by strongly contrastive data pairs in lieu of multiple domains. We present a causal DG theory that shows the potential capability of CDG; together with a regularization technique, Logit Attribution Matching (LAM), for realizing CDG. We empirically show that LAM outperforms state-of-the-art DG methods with only a small portion of paired data and that LAM helps models better focus on semantic features which are crucial to DG.
&lt;/p&gt;</description></item><item><title>&#23457;&#31295;&#20154;&#20998;&#37197;&#38382;&#39064;&#26159;&#19968;&#20010;30&#24180;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#33258;&#21160;&#23558;&#35770;&#25991;&#19982;&#26368;&#21305;&#37197;&#30340;&#23457;&#31295;&#20154;&#20851;&#32852;&#24050;&#25104;&#20026;&#32531;&#35299;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#32508;&#36848;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#35813;&#39046;&#22495;&#30340;&#27010;&#36848;&#21644;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07887</link><description>&lt;p&gt;
&#23457;&#31295;&#20154;&#20998;&#37197;&#38382;&#39064;&#65306;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reviewer assignment problem: A scoping review. (arXiv:2305.07887v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07887
&lt;/p&gt;
&lt;p&gt;
&#23457;&#31295;&#20154;&#20998;&#37197;&#38382;&#39064;&#26159;&#19968;&#20010;30&#24180;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#33258;&#21160;&#23558;&#35770;&#25991;&#19982;&#26368;&#21305;&#37197;&#30340;&#23457;&#31295;&#20154;&#20851;&#32852;&#24050;&#25104;&#20026;&#32531;&#35299;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#32508;&#36848;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#35813;&#39046;&#22495;&#30340;&#27010;&#36848;&#21644;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#34892;&#35780;&#23457;&#26159;&#31185;&#23398;&#30740;&#31350;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#21516;&#34892;&#35780;&#23457;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#21457;&#34920;&#30340;&#30740;&#31350;&#36136;&#37327;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#33021;&#21542;&#25307;&#21215;&#21040;&#36866;&#24403;&#30340;&#23457;&#31295;&#20154;&#26469;&#35780;&#23457;&#25552;&#20132;&#30340;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31185;&#23398;&#35770;&#25991;&#30340;&#25345;&#32493;&#22686;&#21152;&#20197;&#21450;&#23398;&#32773;&#30340;&#24037;&#20316;&#36127;&#25285;&#19981;&#26029;&#22686;&#21152;&#31561;&#22810;&#31181;&#22240;&#32032;&#65292;&#25214;&#21040;&#36825;&#26679;&#30340;&#23457;&#31295;&#20154;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#35299;&#20915;&#33258;&#21160;&#23558;&#35770;&#25991;&#19982;&#8220;&#26368;&#21305;&#37197;&#8221;&#30340;&#23457;&#31295;&#20154;&#20851;&#32852;&#30340;&#38382;&#39064;&#65288;&#36890;&#24120;&#31216;&#20026;&#23457;&#31295;&#20154;&#20998;&#37197;&#38382;&#39064;RAP&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#30340;&#20027;&#39064;&#19977;&#21313;&#24180;&#20102;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#32570;&#23569;&#26368;&#36817;&#30340;RAP&#30456;&#20851;&#25991;&#29486;&#30340;&#31995;&#32479;&#32508;&#21512;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#24182;&#25903;&#25345;&#36827;&#19968;&#27493;&#30340;RAP&#30456;&#20851;&#30740;&#31350;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35299;&#20915;RAP&#30340;&#35745;&#31639;&#26041;&#27861;&#30340;&#32508;&#36848;&#30740;&#31350;&#12290;&#26681;&#25454;&#26368;&#26032;&#30340;&#32508;&#36848;&#26041;&#27861;&#35770;&#25351;&#21335;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#30456;&#20851;&#39046;&#22495;&#30340;&#25991;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;RAP&#39046;&#22495;&#21457;&#29616;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer review is an integral component of scientific research. The quality of peer review, and consequently the published research, depends to a large extent on the ability to recruit adequate reviewers for submitted papers. However, finding such reviewers is an increasingly difficult task due to several factors, such as the continuous increase both in the production of scientific papers and the workload of scholars. To mitigate these challenges, solutions for automated association of papers with "well matching" reviewers - the task often referred to as reviewer assignment problem (RAP) - have been the subject of research for thirty years now. Even though numerous solutions have been suggested, to our knowledge, a recent systematic synthesis of the RAP-related literature is missing. To fill this gap and support further RAP-related research, in this paper, we present a scoping review of computational approaches for addressing RAP. Following the latest methodological guidance for scoping r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#34880;&#28082;&#26816;&#26597;&#25968;&#20540;&#30340;&#30149;&#27602;&#19982;&#32454;&#33740;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#24863;&#26579;&#31867;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;CRP&#27700;&#24179;10-40 mg/L&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21306;&#20998;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#30340;&#20934;&#30830;&#24615;&#65292;&#35777;&#26126;&#20102;&#22810;&#31181;&#34880;&#28082;&#21442;&#25968;&#23545;&#20110;&#35786;&#26029;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07877</link><description>&lt;p&gt;
&#22522;&#20110;&#20363;&#34892;&#34880;&#28082;&#26816;&#26597;&#25968;&#20540;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37492;&#21035;&#30149;&#27602;&#21644;&#32454;&#33740;&#24863;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiating Viral and Bacterial Infections: A Machine Learning Model Based on Routine Blood Test Values. (arXiv:2305.07877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#34880;&#28082;&#26816;&#26597;&#25968;&#20540;&#30340;&#30149;&#27602;&#19982;&#32454;&#33740;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#24863;&#26579;&#31867;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;CRP&#27700;&#24179;10-40 mg/L&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21306;&#20998;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#30340;&#20934;&#30830;&#24615;&#65292;&#35777;&#26126;&#20102;&#22810;&#31181;&#34880;&#28082;&#21442;&#25968;&#23545;&#20110;&#35786;&#26029;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25239;&#29983;&#32032;&#32784;&#33647;&#24615;&#26085;&#30410;&#23041;&#32961;&#65292;&#27491;&#30830;&#21306;&#20998;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#20197;&#36827;&#34892;&#27491;&#30830;&#30340;&#25239;&#29983;&#32032;&#20351;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;16&#20010;&#20363;&#34892;&#34880;&#28082;&#26816;&#26597;&#32467;&#26524;&#12289;C-&#21453;&#24212;&#34507;&#30333;&#27700;&#24179;&#12289;&#29983;&#29289;&#24615;&#21035;&#21644;&#24180;&#40836;&#30340;&#30149;&#27602;&#19982;&#32454;&#33740;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#36825;&#20123;&#24863;&#26579;&#31867;&#22411;&#12290;&#20351;&#29992;&#21333;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;44,120&#20010;&#26696;&#20363;&#25968;&#25454;&#38598;&#65292;"&#30149;&#27602; vs. &#32454;&#33740;"&#27169;&#22411;&#34920;&#29616;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;82.2%&#30340;&#20934;&#30830;&#29575;&#65292;0.129&#30340;Brier&#24471;&#20998;&#21644;0.91&#30340;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;CRP&#20915;&#31574;&#35268;&#21017;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#22312;CRP&#33539;&#22260;&#20026;10-40 mg/L&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#20934;&#30830;&#24615;&#65292;&#36825;&#20010;&#33539;&#22260;&#20869;&#20165;&#38752;CRP&#26080;&#27861;&#20026;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#36827;&#34892;&#21306;&#20998;&#30340;&#35786;&#26029;&#20215;&#20540;&#26377;&#38480;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#35786;&#26029;&#20915;&#31574;&#20013;&#32771;&#34385;&#22810;&#31181;&#34880;&#28082;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24314;&#35758;&#30149;&#27602; vs. &#32454;&#33740;&#27169;&#22411;&#20351;&#24471;&#24212;&#29992;&#20110;&#20020;&#24202;&#20915;&#31574;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing threat of antibiotic resistance necessitates accurate differentiation between bacterial and viral infections for proper antibiotic administration. In this study, a Virus vs. Bacteria machine learning model was developed to discern between these infection types using 16 routine blood test results, C-reactive protein levels, biological sex, and age. With a dataset of 44,120 cases from a single medical center, the Virus vs. Bacteria model demonstrated remarkable accuracy of 82.2%, a Brier score of 0.129, and an area under the ROC curve of 0.91, surpassing the performance of traditional CRP decision rule models. The model demonstrates substantially improved accuracy within the CRP range of 10 40 mg/L, an interval in which CRP alone offers limited diagnostic value for distinguishing between bacterial and viral infections. These findings underscore the importance of considering multiple blood parameters for diagnostic decision-making and suggest that the Virus vs. Bacteria model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPP-CNN&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#32593;&#32476;&#40065;&#26834;&#24615;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#21367;&#31215;&#21644;&#20840;&#36830;&#25509;&#23618;&#20043;&#38388;&#28155;&#21152;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#23618;&#65292;&#20811;&#26381;&#20102;CNN&#39044;&#27979;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#32508;&#21512;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07872</link><description>&lt;p&gt;
SPP-CNN&#65306;&#19968;&#31181;&#32593;&#32476;&#40065;&#26834;&#24615;&#39044;&#27979;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SPP-CNN: An Efficient Framework for Network Robustness Prediction. (arXiv:2305.07872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPP-CNN&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#32593;&#32476;&#40065;&#26834;&#24615;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#21367;&#31215;&#21644;&#20840;&#36830;&#25509;&#23618;&#20043;&#38388;&#28155;&#21152;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#23618;&#65292;&#20811;&#26381;&#20102;CNN&#39044;&#27979;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#22312;&#32508;&#21512;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#32593;&#32476;&#22312;&#36973;&#21463;&#24694;&#24847;&#25915;&#20987;&#26102;&#32500;&#25345;&#36830;&#25509;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;&#26032;&#25552;&#20986;&#30340;&#26694;&#26550;SPP-CNN&#22312;&#21367;&#31215;&#21644;&#20840;&#36830;&#25509;&#23618;&#20043;&#38388;&#35013;&#36733;&#31354;&#38388;&#37329;&#23383;&#22612;&#27744;&#21270;&#23618;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;CNN&#30340;&#39044;&#27979;&#26041;&#27861;&#20013;&#24120;&#35265;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the robustness of a network to sustain its connectivity and controllability against malicious attacks. This kind of network robustness is typically measured by the time-consuming attack simulation, which returns a sequence of values that record the remaining connectivity and controllability after a sequence of node- or edge-removal attacks. For improvement, this paper develops an efficient framework for network robustness prediction, the spatial pyramid pooling convolutional neural network (SPP-CNN). The new framework installs a spatial pyramid pooling layer between the convolutional and fully-connected layers, overcoming the common mismatch issue in the CNN-based prediction approaches and extending its generalizability. Extensive experiments are carried out by comparing SPP-CNN with three state-of-the-art robustness predictors, namely a CNN-based and two graph neural networks-based frameworks. Synthetic and real-world networks, both directed and undirected, are in
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#26412;&#21644;&#31185;&#23398;&#38382;&#39064;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20248;&#31168;&#30340;&#25945;&#32946;&#38382;&#39064;&#33258;&#21160;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.07871</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#25945;&#23398;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scalable Educational Question Generation with Pre-trained Language Models. (arXiv:2305.07871v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07871
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#26412;&#21644;&#31185;&#23398;&#38382;&#39064;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20248;&#31168;&#30340;&#25945;&#32946;&#38382;&#39064;&#33258;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#20154;&#21475;&#22312;&#25506;&#32034;&#20010;&#24615;&#21270;&#23398;&#20064;&#20043;&#26053;&#26102;&#65292;&#25945;&#32946;&#38382;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23558;&#22312;&#22312;&#32447;&#25945;&#32946;&#30340;&#25193;&#23637;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#65292;&#23454;&#29616;&#22823;&#35268;&#27169;&#30340;&#33258;&#25105;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;EduQG&#65292;&#36890;&#36807;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26500;&#24314;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;EduQG&#33021;&#22815;&#36890;&#36807;&#22312;&#31185;&#23398;&#25991;&#26412;&#21644;&#31185;&#23398;&#38382;&#39064;&#25968;&#25454;&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#20986;&#26356;&#20248;&#31168;&#30340;&#25945;&#32946;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic generation of educational questions will play a key role in scaling online education, enabling self-assessment at scale when a global population is manoeuvring their personalised learning journeys. We develop \textit{EduQG}, a novel educational question generation model built by adapting a large language model. Our extensive experiments demonstrate that \textit{EduQG} can produce superior educational questions by further pre-training and fine-tuning a pre-trained language model on the scientific text and science question data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30452;&#25509;&#27169;&#25311;&#32597;&#35265;&#20107;&#20214;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32467;&#21512;&#37325;&#35201;&#24615;&#37319;&#26679;&#33719;&#24471;&#39640;&#31934;&#24230;&#30340;&#22797;&#26434;&#31215;&#20998;&#21644;&#26399;&#26395;&#20272;&#35745;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#24182;&#20026;&#32597;&#35265;&#20107;&#20214;&#20998;&#24067;&#25552;&#20379;&#33268;&#21629;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.07863</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32597;&#35265;&#20107;&#20214;&#27169;&#25311;&#30340;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Flow-Based Generative Model for Rare-Event Simulation. (arXiv:2305.07863v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30452;&#25509;&#27169;&#25311;&#32597;&#35265;&#20107;&#20214;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#32467;&#21512;&#37325;&#35201;&#24615;&#37319;&#26679;&#33719;&#24471;&#39640;&#31934;&#24230;&#30340;&#22797;&#26434;&#31215;&#20998;&#21644;&#26399;&#26395;&#20272;&#35745;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#24182;&#20026;&#32597;&#35265;&#20107;&#20214;&#20998;&#24067;&#25552;&#20379;&#33268;&#21629;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#20272;&#35745;&#20915;&#31574;&#30340;&#26399;&#26395;&#32467;&#26524;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#37319;&#26679;&#21487;&#33021;&#20250;&#24573;&#30053;&#32597;&#35265;&#20294;&#37325;&#35201;&#30340;&#20107;&#20214;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#35757;&#32451;&#20102;&#19968;&#20010;&#24402;&#19968;&#21270;&#27969;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#30452;&#25509;&#20174;&#26465;&#20214;&#20998;&#24067;&#20013;&#27169;&#25311;&#26679;&#26412;&#65292;&#21069;&#25552;&#26159;&#21457;&#29983;&#32597;&#35265;&#20107;&#20214;&#12290;&#36890;&#36807;&#21033;&#29992;&#32806;&#21512;&#27969;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#21407;&#21017;&#19978;&#20219;&#24847;&#22909;&#22320;&#36924;&#36817;&#20219;&#20309;&#37319;&#26679;&#20998;&#24067;&#12290;&#36890;&#36807;&#23558;&#36924;&#36817;&#26041;&#27861;&#19982;&#37325;&#35201;&#24615;&#37319;&#26679;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#31934;&#24230;&#30340;&#22797;&#26434;&#31215;&#20998;&#21644;&#26399;&#26395;&#20272;&#35745;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#20960;&#20010;&#31034;&#20363;&#26469;&#28436;&#31034;&#22914;&#20309;&#22312;&#39640;&#32500;&#21644;&#32597;&#35265;&#20107;&#20214;&#35774;&#32622;&#20013;&#20351;&#29992;&#35813;&#26041;&#27861;&#36827;&#34892;&#26377;&#25928;&#37319;&#26679;&#21644;&#20272;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#30452;&#25509;&#20174;&#32597;&#35265;&#20107;&#20214;&#20998;&#24067;&#20013;&#27169;&#25311;&#21487;&#20197;&#33719;&#24471;&#20851;&#20110;&#32597;&#35265;&#20107;&#20214;&#21457;&#29983;&#26041;&#24335;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving decision problems in complex, stochastic environments is often achieved by estimating the expected outcome of decisions via Monte Carlo sampling. However, sampling may overlook rare, but important events, which can severely impact the decision making process. We present a method in which a Normalizing Flow generative model is trained to simulate samples directly from a conditional distribution given that a rare event occurs. By utilizing Coupling Flows, our model can, in principle, approximate any sampling distribution arbitrarily well. By combining the approximation method with Importance Sampling, highly accurate estimates of complicated integrals and expectations can be obtained. We include several examples to demonstrate how the method can be used for efficient sampling and estimation, even in high-dimensional and rare-event settings. We illustrate that by simulating directly from a rare-event distribution significant insight can be gained into the way rare events happen.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512; AI &#36741;&#21161;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65288;HAiVA&#65289;&#65292;&#21487;&#20197;&#29992;&#20110;&#25506;&#32034;&#20113;&#23646;&#24615;&#21644;&#27668;&#20505;&#27169;&#24335;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#35774;&#35745;&#21644;&#27979;&#35797;&#28023;&#27915;&#20113;&#22686;&#30333;&#65288;MCB&#65289;&#30340;&#24178;&#39044;&#26041;&#26696;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#27668;&#20505;&#27169;&#24335;&#30340;&#39044;&#26399;&#21644;&#24847;&#22806;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.07859</link><description>&lt;p&gt;
HAiVA&#65306;&#28151;&#21512; AI &#36741;&#21161;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#30740;&#31350;&#20113;&#23646;&#24615;&#23545;&#27668;&#20505;&#27169;&#24335;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
HAiVA: Hybrid AI-assisted Visual Analysis Framework to Study the Effects of Cloud Properties on Climate Patterns. (arXiv:2305.07859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512; AI &#36741;&#21161;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65288;HAiVA&#65289;&#65292;&#21487;&#20197;&#29992;&#20110;&#25506;&#32034;&#20113;&#23646;&#24615;&#21644;&#27668;&#20505;&#27169;&#24335;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#35774;&#35745;&#21644;&#27979;&#35797;&#28023;&#27915;&#20113;&#22686;&#30333;&#65288;MCB&#65289;&#30340;&#24178;&#39044;&#26041;&#26696;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#27668;&#20505;&#27169;&#24335;&#30340;&#39044;&#26399;&#21644;&#24847;&#22806;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#23545;&#22320;&#29699;&#27668;&#20505;&#31995;&#32479;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#36890;&#36807;&#28023;&#27915;&#20113;&#22686;&#30333;&#65288;Marine Cloud Brightening&#65292;MCB&#65289;&#31561;&#27668;&#20505;&#24178;&#39044;&#25216;&#26415;&#26469;&#35843;&#33410;&#22320;&#29699;&#36752;&#23556;&#24179;&#34913;&#21644;&#39537;&#21160;&#21306;&#22495;&#24615;&#28201;&#24230;&#21644;&#38477;&#27700;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36991;&#20813; MCB &#30340;&#24847;&#22806;&#24433;&#21709;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#30340;&#20113;&#21040;&#27668;&#20505;&#21709;&#24212;&#20989;&#25968;&#12290;&#20351;&#29992;&#20256;&#32479;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#26469;&#35774;&#35745;&#21644;&#27979;&#35797;&#36825;&#26679;&#30340;&#24178;&#39044;&#26041;&#26696;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512; AI &#36741;&#21161;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65288;HAiVA&#65289;&#26469;&#39537;&#21160;&#36825;&#20123;&#31185;&#23398;&#30740;&#31350;&#65292;&#24182;&#20419;&#36827;&#19981;&#21516; MCB &#24178;&#39044;&#26041;&#26696;&#30340;&#20132;&#20114;&#24335; what-if &#25506;&#32034;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#27668;&#20505;&#27169;&#24335;&#30340;&#39044;&#26399;&#21644;&#24847;&#22806;&#24433;&#21709;&#12290;&#25105;&#20204;&#19982;&#27668;&#20505;&#31185;&#23398;&#23478;&#22242;&#38431;&#21512;&#20316;&#65292;&#24320;&#21457;&#20102;&#19968;&#22871;&#28151;&#21512; AI &#27169;&#22411;&#26469;&#27169;&#25311;&#22522;&#20110;&#21382;&#21490;&#35266;&#27979;&#21644;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#30340;&#20113;-&#27668;&#20505;&#21709;&#24212;&#20989;&#25968;&#12290;HAiVA &#23558;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#20132;&#20114;&#24335;&#25968;&#25454;&#21487;&#35270;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#25506;&#32034;&#20113;&#23646;&#24615;&#21644;&#27668;&#20505;&#27169;&#24335;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#35774;&#35745;&#21644;&#27979;&#35797; MCB &#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clouds have a significant impact on the Earth's climate system. They play a vital role in modulating Earth's radiation budget and driving regional changes in temperature and precipitation. This makes clouds ideal for climate intervention techniques like Marine Cloud Brightening (MCB) which refers to modification in cloud reflectivity, thereby cooling the surrounding region. However, to avoid unintended effects of MCB, we need a better understanding of the complex cloud to climate response function. Designing and testing such interventions scenarios with conventional Earth System Models is computationally expensive. Therefore, we propose a hybrid AI-assisted visual analysis framework to drive such scientific studies and facilitate interactive what-if investigation of different MCB intervention scenarios to assess their intended and unintended impacts on climate patterns. We work with a team of climate scientists to develop a suite of hybrid AI models emulating cloud-climate response fun
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#31639;&#27861;&#26469;&#21306;&#20998;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07854</link><description>&lt;p&gt;
&#22522;&#20110;&#21305;&#37197;&#29305;&#24449;&#25552;&#21462;&#30340;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction. (arXiv:2305.07854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#31639;&#27861;&#26469;&#21306;&#20998;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#38656;&#35201;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#24320;&#21457;&#20934;&#30830;&#21487;&#38752;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20005;&#26684;&#30340;&#25968;&#25454;&#38544;&#31169;&#27861;&#24459;&#21644;&#20016;&#23500;&#30340;&#36793;&#32536;&#24037;&#19994;&#25968;&#25454;&#38656;&#35201;&#20998;&#25955;&#24335;&#25968;&#25454;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#20010;&#20998;&#25955;&#24335;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#38750;&#24120;&#36866;&#29992;&#20110;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#30001;&#20110;&#19981;&#21516;&#30340;&#36864;&#21270;&#26426;&#21046;&#21644;&#19981;&#24179;&#31561;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#25152;&#23548;&#33268;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#24320;&#21457;&#31934;&#24230;&#39640;&#30340;&#35757;&#32451;&#27169;&#22411;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#32479;&#35745;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;FL&#22312;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#21442;&#25968;&#32858;&#21512;&#31639;&#27861;&#30340; FL &#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven industrial health prognostics require rich training data to develop accurate and reliable predictive models. However, stringent data privacy laws and the abundance of edge industrial data necessitate decentralized data utilization. Thus, the industrial health prognostics field is well suited to significantly benefit from federated learning (FL), a decentralized and privacy-preserving learning technique. However, FL-based health prognostics tasks have hardly been investigated due to the complexities of meaningfully aggregating model parameters trained from heterogeneous data to form a high performing federated model. Specifically, data heterogeneity among edge devices, stemming from dissimilar degradation mechanisms and unequal dataset sizes, poses a critical statistical challenge for developing accurate federated models. We propose a pioneering FL-based health prognostic model with a feature similarity-matched parameter aggregation algorithm to discriminatingly learn from h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;Meta-Polyp&#65292;&#23558;Meta-Former&#19982;UNet&#34701;&#21512;&#24182;&#24341;&#20837;&#22810;&#23610;&#24230;&#19978;&#37319;&#26679;&#22359;&#21644;Convformer&#22359;&#65292;&#35299;&#20915;&#20102;CNN&#21644;Vision Transformer&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#38598;&#12289;&#32570;&#22833;&#36793;&#30028;&#21644;&#23567;&#24687;&#32905;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#20102;&#24687;&#32905;&#20998;&#21106;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.07848</link><description>&lt;p&gt;
Meta-Polyp&#65306;&#39640;&#25928;&#24687;&#32905;&#20998;&#21106;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-Polyp: a baseline for efficient Polyp segmentation. (arXiv:2305.07848v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;Meta-Polyp&#65292;&#23558;Meta-Former&#19982;UNet&#34701;&#21512;&#24182;&#24341;&#20837;&#22810;&#23610;&#24230;&#19978;&#37319;&#26679;&#22359;&#21644;Convformer&#22359;&#65292;&#35299;&#20915;&#20102;CNN&#21644;Vision Transformer&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#38598;&#12289;&#32570;&#22833;&#36793;&#30028;&#21644;&#23567;&#24687;&#32905;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#20102;&#24687;&#32905;&#20998;&#21106;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24687;&#32905;&#20998;&#21106;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#24182;&#19988;&#35768;&#22810;&#26041;&#27861;&#21033;&#29992;CNN&#12289;Vision Transformer&#21644;Transformer&#25216;&#26415;&#24320;&#21457;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#22806;&#25968;&#25454;&#38598;&#12289;&#32570;&#22833;&#36793;&#30028;&#21644;&#23567;&#24687;&#32905;&#26102;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;2022&#24180;&#65292;Meta-Former&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#22522;&#20934;&#32447;&#34987;&#24341;&#20837;&#65292;&#23427;&#19981;&#20165;&#25552;&#39640;&#20102;&#22810;&#20219;&#21153;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#35299;&#20915;&#20102;Vision Transformer&#21644;CNN&#23478;&#26063;&#39592;&#26550;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#20998;&#21106;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Meta-Former&#19982;UNet&#30340;&#34701;&#21512;&#65292;&#21516;&#26102;&#22312;&#35299;&#30721;&#22120;&#38454;&#27573;&#24341;&#20837;&#20102;&#22810;&#23610;&#24230;&#19978;&#37319;&#26679;&#22359;&#19982;&#32423;&#32852;&#32452;&#21512;&#65292;&#20197;&#22686;&#24378;&#32441;&#29702;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Convformer&#22359;&#65292;&#22522;&#20110;Meta-Former&#30340;&#24605;&#24819;&#65292;&#20197;&#21152;&#24378;&#23616;&#37096;&#29305;&#24449;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#36825;&#20123;&#22359;&#23558;&#20840;&#23616;&#20449;&#24687;&#65288;&#20363;&#22914;&#24687;&#32905;&#30340;&#25972;&#20307;&#24418;&#29366;&#65289;&#19982;&#23616;&#37096;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#36827;&#34892;&#24687;&#32905;&#20998;&#21106;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, polyp segmentation has gained significant importance, and many methods have been developed using CNN, Vision Transformer, and Transformer techniques to achieve competitive results. However, these methods often face difficulties when dealing with out-of-distribution datasets, missing boundaries, and small polyps. In 2022, Meta-Former was introduced as a new baseline for vision, which not only improved the performance of multi-task computer vision but also addressed the limitations of the Vision Transformer and CNN family backbones. To further enhance segmentation, we propose a fusion of Meta-Former with UNet, along with the introduction of a Multi-scale Upsampling block with a level-up combination in the decoder stage to enhance the texture, also we propose the Convformer block base on the idea of the Meta-former to enhance the crucial information of the local feature. These blocks enable the combination of global information, such as the overall shape of the polyp, wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.07845</link><description>&lt;p&gt;
&#29702;&#35299;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#24179;&#22343;&#25216;&#26415;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21457;&#29616;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#24179;&#22343;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#20250;&#32858;&#38598;&#35757;&#32451;&#20110;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#27169;&#22411;&#20197;&#33719;&#24471;&#34920;&#29616;&#33391;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#29702;&#23578;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#21487;&#35270;&#21270;&#25439;&#22833;/&#38169;&#35823;&#26223;&#35266;&#26469;&#30740;&#31350;&#27169;&#22411;&#24179;&#22343;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#23458;&#25143;&#31471;&#27169;&#22411;&#29615;&#32469;&#20840;&#23616;&#27169;&#22411;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#30406;&#22320;&#20869;&#65292;&#24182;&#19988;&#21363;&#20351;&#20840;&#23616;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20063;&#21487;&#33021;&#20559;&#31163;&#30406;&#22320;&#24213;&#37096;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20840;&#23616;&#27169;&#22411;&#22312;&#26089;&#26399;&#35757;&#32451;&#21518;&#30340;&#35823;&#24046;&#20027;&#35201;&#26469;&#33258;&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#21644;&#20840;&#23616;&#25968;&#25454;&#38598;&#20043;&#38388;&#38750;&#37325;&#21472;&#30340;&#25968;&#25454;&#21450;&#20840;&#23616;&#27169;&#22411;&#19982;&#23458;&#25143;&#31471;&#27169;&#22411;&#20043;&#38388;&#30340;&#26368;&#22823;&#36317;&#31163;&#20004;&#20010;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#24102;&#26377;&#26410;&#30693;&#21442;&#25968;&#21644;&#26080;&#20449;&#24687;&#21160;&#20316;&#30340;&#21442;&#25968;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;PMDP&#30340;&#20551;&#35774;&#65292;&#24182;&#20351;&#29992;&#27748;&#26222;&#26862;&#25277;&#26679;&#20445;&#35777;&#20102;&#20854;&#28176;&#36817;&#26368;&#20248;&#30340;&#26399;&#26395;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.07844</link><description>&lt;p&gt;
&#24102;&#26377;&#26080;&#20449;&#24687;&#21160;&#20316;&#30340;&#21442;&#25968;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Thompson Sampling for Parameterized Markov Decision Processes with Uninformative Actions. (arXiv:2305.07844v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07844
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#24102;&#26377;&#26410;&#30693;&#21442;&#25968;&#21644;&#26080;&#20449;&#24687;&#21160;&#20316;&#30340;&#21442;&#25968;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;PMDP&#30340;&#20551;&#35774;&#65292;&#24182;&#20351;&#29992;&#27748;&#26222;&#26862;&#25277;&#26679;&#20445;&#35777;&#20102;&#20854;&#28176;&#36817;&#26368;&#20248;&#30340;&#26399;&#26395;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#26410;&#30693;&#20851;&#38190;&#21442;&#25968;&#30340;&#21442;&#25968;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;PMDP&#65289;&#65292;&#24517;&#39035;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#23450;&#20041;&#29305;&#24449;&#26159;&#20855;&#26377;&#8220;&#26080;&#20449;&#24687;&#8221;&#21160;&#20316;&#65292;&#36825;&#20123;&#21160;&#20316;&#19981;&#25552;&#20379;&#20851;&#20110;&#26410;&#30693;&#21442;&#25968;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#20851;&#20110;PMDP&#30340;&#20551;&#35774;&#65292;&#26681;&#25454;&#36825;&#20123;&#20551;&#35774;&#65292;&#27748;&#26222;&#26862;&#25277;&#26679;&#21487;&#20445;&#35777;&#28176;&#36817;&#26368;&#20248;&#30340;&#26399;&#26395;&#36951;&#25022;&#30028;&#20026;$O(T^{-1})$&#65292;&#36825;&#20123;&#20551;&#35774;&#26131;&#20110;&#39564;&#35777;&#65292;&#36866;&#29992;&#20110;&#35768;&#22810;&#38382;&#39064;&#31867;&#21035;&#65292;&#20363;&#22914;&#25490;&#38431;&#65292;&#24211;&#23384;&#25511;&#21046;&#21644;&#21160;&#24577;&#23450;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study parameterized MDPs (PMDPs) in which the key parameters of interest are unknown and must be learned using Bayesian inference. One key defining feature of such models is the presence of "uninformative" actions that provide no information about the unknown parameters. We contribute a set of assumptions for PMDPs under which Thompson sampling guarantees an asymptotically optimal expected regret bound of $O(T^{-1})$, which are easily verified for many classes of problems such as queuing, inventory control, and dynamic pricing.
&lt;/p&gt;</description></item><item><title>COPP-Net&#26159;&#19968;&#31181;&#21033;&#29992;&#21152;&#26435;&#34917;&#19969;&#36136;&#37327;&#39044;&#27979;&#36827;&#34892;&#23616;&#37096;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#26080;&#21442;&#32771;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;NR-PCQA&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07829</link><description>&lt;p&gt;
&#22522;&#20110;&#21152;&#26435;&#34917;&#19969;&#36136;&#37327;&#39044;&#27979;&#30340;&#26080;&#21442;&#32771;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
No-Reference Point Cloud Quality Assessment via Weighted Patch Quality Prediction. (arXiv:2305.07829v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07829
&lt;/p&gt;
&lt;p&gt;
COPP-Net&#26159;&#19968;&#31181;&#21033;&#29992;&#21152;&#26435;&#34917;&#19969;&#36136;&#37327;&#39044;&#27979;&#36827;&#34892;&#23616;&#37096;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#26080;&#21442;&#32771;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;NR-PCQA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#28857;&#20113;&#30340;&#19977;&#32500;&#35270;&#35273;&#24212;&#29992;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#23616;&#37096;&#21306;&#22495;&#30456;&#20851;&#24615;&#20998;&#26512;&#33021;&#21147;&#30340;&#26080;&#21442;&#32771;&#28857;&#20113;&#36136;&#37327;&#35780;&#20272;&#65288;NR-PCQA&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;COPP-Net&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#28857;&#20113;&#20998;&#20026;&#34917;&#19969;&#65292;&#20026;&#27599;&#20010;&#34917;&#19969;&#29983;&#25104;&#32441;&#29702;&#21644;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#34701;&#21512;&#25104;&#34917;&#19969;&#29305;&#24449;&#26469;&#39044;&#27979;&#34917;&#19969;&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25910;&#38598;&#19968;&#20010;&#28857;&#20113;&#20013;&#25152;&#26377;&#34917;&#19969;&#30340;&#29305;&#24449;&#36827;&#34892;&#30456;&#20851;&#24615;&#20998;&#26512;&#65292;&#20197;&#33719;&#24471;&#30456;&#20851;&#24615;&#26435;&#37325;&#12290;&#26368;&#21518;&#65292;&#39044;&#27979;&#30340;&#36136;&#37327;&#21644;&#25152;&#26377;&#34917;&#19969;&#30340;&#30456;&#20851;&#26435;&#37325;&#29992;&#20110;&#25512;&#23548;&#26368;&#32456;&#36136;&#37327;&#24471;&#20998;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;NR-PCQA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of 3D vision applications based on point clouds, point cloud quality assessment(PCQA) is becoming an important research topic. However, the prior PCQA methods ignore the effect of local quality variance across different areas of the point cloud. To take an advantage of the quality distribution imbalance, we propose a no-reference point cloud quality assessment (NR-PCQA) method with local area correlation analysis capability, denoted as COPP-Net. More specifically, we split a point cloud into patches, generate texture and structure features for each patch, and fuse them into patch features to predict patch quality. Then, we gather the features of all the patches of a point cloud for correlation analysis, to obtain the correlation weights. Finally, the predicted qualities and correlation weights for all the patches are used to derive the final quality score. Experimental results show that our method outperforms the state-of-the-art benchmark NR-PCQA methods. Th
&lt;/p&gt;</description></item><item><title>DCASE 2023 &#25361;&#25112;&#20219;&#21153;2&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#20013;&#65292;&#37096;&#32626;&#26032;&#22411;&#26426;&#22120;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#65292;&#20165;&#20351;&#29992;&#26497;&#23569;&#37327;&#30340;&#27491;&#24120;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#19988;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2305.07828</link><description>&lt;p&gt;
DCASE 2023 &#25361;&#25112;&#20219;&#21153;2&#65306;&#38754;&#21521;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#30340;&#39318;&#27425;&#26080;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#30340;&#25551;&#36848;&#19982;&#35752;&#35770;(arXiv:2305.07828v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
Description and Discussion on DCASE 2023 Challenge Task 2: First-Shot Unsupervised Anomalous Sound Detection for Machine Condition Monitoring. (arXiv:2305.07828v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07828
&lt;/p&gt;
&lt;p&gt;
DCASE 2023 &#25361;&#25112;&#20219;&#21153;2&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#20013;&#65292;&#37096;&#32626;&#26032;&#22411;&#26426;&#22120;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#65292;&#20165;&#20351;&#29992;&#26497;&#23569;&#37327;&#30340;&#27491;&#24120;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#19988;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 &#25361;&#25112;&#20219;&#21153; 2:&#8220;&#38754;&#21521;&#26426;&#22120;&#29366;&#24577;&#30417;&#27979;&#30340;&#39318;&#27425;&#26080;&#30417;&#30563;&#24322;&#24120;&#22768;&#38899;&#26816;&#27979;&#8221;&#12290;&#35813;&#20219;&#21153;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20165;&#20351;&#29992;&#23569;&#25968;&#27491;&#24120;&#26679;&#26412;&#23601;&#33021;&#24555;&#36895;&#37096;&#32626;&#38024;&#23545;&#26032;&#22411;&#26426;&#22120;&#30340; ASD &#31995;&#32479;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#22312;&#36807;&#21435;&#30340; ASD &#20219;&#21153;&#20013;&#65292;&#21457;&#23637;&#30340;&#26041;&#27861;&#20026;&#27599;&#31181;&#26426;&#22120;&#31867;&#22411;&#35843;&#25972;&#36229;&#21442;&#25968;&#65292;&#22240;&#20026;&#21457;&#23637;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#20855;&#26377;&#30456;&#21516;&#30340;&#26426;&#22120;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#25910;&#38598;&#27491;&#24120;&#21644;&#24322;&#24120;&#25968;&#25454;&#38598;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312; 2023 Task 2 &#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#35299;&#20915;&#39318;&#27425;&#38382;&#39064;&#65292;&#21363;&#22312;&#23436;&#20840;&#26032;&#22411;&#30340;&#26426;&#22120;&#31867;&#22411;&#30340;&#23569;&#25968;&#26426;&#22120;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#65288;i&#65289;&#27599;&#31181;&#26426;&#22120;&#31867;&#22411;&#20165;&#26377;&#19968;&#20010;&#37096;&#20998;&#65292;&#65288;ii&#65289;&#21457;&#23637;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#30340;&#26426;&#22120;&#31867;&#22411;&#23436;&#20840;&#19981;&#21516;&#12290;&#25105;&#20204;&#23558;&#28155;&#21152;&#23376;&#20219;&#21153;&#30340;&#25361;&#25112;&#32467;&#26524;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge Task 2: "First-shot unsupervised anomalous sound detection (ASD) for machine condition monitoring". The main goal is to enable rapid deployment of ASD systems for new kinds of machines using only a few normal samples, without the need for hyperparameter tuning. In the past ASD tasks, developed methods tuned hyperparameters for each machine type, as the development and evaluation datasets had the same machine types. However, collecting normal and anomalous data as the development dataset can be infeasible in practice. In 2023 Task 2, we focus on solving first-shot problem, which is the challenge of training a model on a few machines of a completely novel machine type. Specifically, (i) each machine type has only one section, and (ii) machine types in the development and evaluation datasets are completely different. We will add challenge results and analysis of the sub
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#37197;&#30005;&#31995;&#32479;&#25215;&#36733;&#33021;&#21147;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#20998;&#24067;&#24335;&#33021;&#28304;&#38598;&#25104;&#22330;&#26223;&#26469;&#26500;&#24314;HC&#26367;&#20195;&#27169;&#22411;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#20102;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07818</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#37197;&#30005;&#31995;&#32479;&#25215;&#36733;&#33021;&#21147;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Active Learning-based Approach for Hosting Capacity Analysis in Distribution Systems. (arXiv:2305.07818v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#37197;&#30005;&#31995;&#32479;&#25215;&#36733;&#33021;&#21147;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#20998;&#24067;&#24335;&#33021;&#28304;&#38598;&#25104;&#22330;&#26223;&#26469;&#26500;&#24314;HC&#26367;&#20195;&#27169;&#22411;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#20102;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20998;&#24067;&#24335;&#33021;&#28304;&#30340;&#19981;&#26029;&#38598;&#25104;&#65292;&#27169;&#25311;&#21644;&#20998;&#26512;&#26410;&#26469;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#30340;&#25215;&#36733;&#33021;&#21147;(HC)&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;(AL)&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#22320;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;DER&#38598;&#25104;&#22330;&#26223;&#65292;&#24182;&#23558;&#20854;&#19982;&#20197;&#21069;&#27169;&#25311;&#30340;&#22330;&#26223;&#30456;&#32467;&#21512;&#26500;&#24314;HC&#26367;&#20195;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35813;&#26367;&#20195;&#27169;&#22411;&#26469;&#35782;&#21035;&#23454;&#38469;&#27169;&#25311;&#20013;&#26368;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#22330;&#26223;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;HCA&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;HC&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#21487;&#22788;&#29702;DER&#38598;&#25104;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing amount of distributed energy resources (DERs) integration, there is a significant need to model and analyze hosting capacity (HC) for future electric distribution grids. Hosting capacity analysis (HCA) examines the amount of DERs that can be safely integrated into the grid and is a challenging task in full generality because there are many possible integration of DERs in foresight. That is, there are numerous extreme points between feasible and infeasible sets. Moreover, HC depends on multiple factors such as (a) adoption patterns of DERs that depend on socio-economic behaviors and (b) how DERs are controlled and managed. These two factors are intrinsic to the problem space because not all integration of DERs may be centrally planned, and could largely change our understanding about HC. This paper addresses the research gap by capturing the two factors (a) and (b) in HCA and by identifying a few most insightful HC scenarios at the cost of domain knowledge. We propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23485;&#24230;&#20026; $n$&#65292;&#28145;&#24230;&#20026; $L$ &#30340;&#38543;&#26426;&#20840;&#36830;&#25509; ReLU &#32593;&#32476;&#20013; $\mu$P &#23398;&#20064;&#29575;&#23545; $n$ &#21644; $L$ &#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#38500;&#31532;&#19968;&#23618;&#21644;&#26368;&#21518;&#19968;&#23618;&#20197;&#22806;&#65292;&#26368;&#22823;&#23398;&#20064;&#29575;&#19982; $n$ &#26080;&#20851;&#65292;&#20294;&#19982; $L$ &#25353; $L^{-3/2}$ &#32553;&#25918;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.07810</link><description>&lt;p&gt;
ReLU MLPs &#20013; $\mu$P &#23398;&#20064;&#36895;&#29575;&#30340;&#28145;&#24230;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth Dependence of $\mu$P Learning Rates in ReLU MLPs. (arXiv:2305.07810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23485;&#24230;&#20026; $n$&#65292;&#28145;&#24230;&#20026; $L$ &#30340;&#38543;&#26426;&#20840;&#36830;&#25509; ReLU &#32593;&#32476;&#20013; $\mu$P &#23398;&#20064;&#29575;&#23545; $n$ &#21644; $L$ &#30340;&#20381;&#36182;&#24615;&#65292;&#21457;&#29616;&#38500;&#31532;&#19968;&#23618;&#21644;&#26368;&#21518;&#19968;&#23618;&#20197;&#22806;&#65292;&#26368;&#22823;&#23398;&#20064;&#29575;&#19982; $n$ &#26080;&#20851;&#65292;&#20294;&#19982; $L$ &#25353; $L^{-3/2}$ &#32553;&#25918;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23485;&#24230;&#20026; $n$&#65292;&#28145;&#24230;&#20026; $L$ &#30340;&#38543;&#26426;&#20840;&#36830;&#25509; ReLU &#32593;&#32476;&#65292;&#24182;&#37197;&#22791;&#20102;&#24179;&#22343;&#22330;&#26435;&#37325;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#30740;&#31350; $\mu$P &#23398;&#20064;&#29575;&#23545; $n$ &#21644; $L$ &#30340;&#20381;&#36182;&#24615;&#8212;&#8212;&#22312; $n,L$ &#24456;&#22823;&#26102; &#65292;&#32463;&#36807;&#26799;&#24230;&#19979;&#38477;&#19968;&#27493;&#21518;&#30340;&#39044;&#28608;&#27963;&#22343;&#26041;&#24046;&#21464;&#21270;&#20173;&#20445;&#25345;&#22343;&#21248;&#26377;&#30028;&#30340;&#26368;&#22823;&#23398;&#20064;&#29575;&#12290;&#19982; Yang &#31561;&#20154;&#20851;&#20110; $\mu$P &#30340;&#20808;&#21069;&#24037;&#20316;&#19968;&#26679;&#65292;&#25105;&#20204;&#21457;&#29616;&#38500;&#31532;&#19968;&#23618;&#21644;&#26368;&#21518;&#19968;&#23618;&#30340;&#26435;&#37325;&#22806;&#65292;&#36825;&#20010;&#26368;&#22823;&#26356;&#26032;&#23398;&#20064;&#29575;&#19982; $n$ &#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#23545; $L$ &#26377;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20381;&#36182;&#24615;&#65292;&#25353; $L^{-3/2}$ &#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short note we consider random fully connected ReLU networks of width $n$ and depth $L$ equipped with a mean-field weight initialization. Our purpose is to study the dependence on $n$ and $L$ of the maximal update ($\mu$P) learning rate, the largest learning rate for which the mean squared change in pre-activations after one step of gradient descent remains uniformly bounded at large $n,L$. As in prior work on $\mu$P of Yang et. al., we find that this maximal update learning rate is independent of $n$ for all but the first and last layer weights. However, we find that it has a non-trivial dependence of $L$, scaling like $L^{-3/2}.$
&lt;/p&gt;</description></item><item><title>Mesh2SSM&#26159;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#25490;&#21015;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27169;&#26495;&#28857;&#20113;&#21464;&#24418;&#20026;&#29305;&#23450;&#20027;&#20307;&#30340;&#32593;&#26684;&#65292;&#24418;&#25104;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#35299;&#21078;&#23398;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07805</link><description>&lt;p&gt;
Mesh2SSM: &#20174;&#34920;&#38754;&#32593;&#26684;&#21040;&#35299;&#21078;&#23398;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mesh2SSM: From Surface Meshes to Statistical Shape Models of Anatomy. (arXiv:2305.07805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07805
&lt;/p&gt;
&lt;p&gt;
Mesh2SSM&#26159;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#25490;&#21015;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#27169;&#26495;&#28857;&#20113;&#21464;&#24418;&#20026;&#29305;&#23450;&#20027;&#20307;&#30340;&#32593;&#26684;&#65292;&#24418;&#25104;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#35299;&#21078;&#23398;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#24418;&#24577;&#24314;&#27169;&#26159;&#20174;&#21307;&#23398;&#22270;&#20687;&#65288;&#22914;MRI&#21644;CT&#25195;&#25551;&#65289;&#20013;&#25429;&#33719;&#30340;&#20998;&#21106;&#35299;&#21078;&#32467;&#26500;&#20013;&#21457;&#29616;&#26174;&#33879;&#24418;&#24577;&#21442;&#25968;&#30340;&#35745;&#31639;&#36807;&#31243;&#65292;&#21487;&#20840;&#38754;&#25551;&#36848;&#31181;&#32676;&#20013;&#29305;&#23450;&#20027;&#20307;&#30340;&#35299;&#21078;&#23398;&#12290;&#30001;&#20110;&#20154;&#20307;&#35299;&#21078;&#32467;&#26500;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#38750;&#32447;&#24615;&#21464;&#24322;&#65292;&#20256;&#32479;&#30340;&#24418;&#24577;&#24314;&#27169;&#36807;&#31243;&#24120;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#24418;&#24577;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#26356;&#24544;&#23454;&#20110;&#22522;&#30784;&#31181;&#32676;&#21464;&#24322;&#30340;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#38656;&#35201;&#24050;&#24314;&#31435;/&#20248;&#21270;&#30340;&#24418;&#24577;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Mesh2SSM&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#30417;&#30563;&#12289;&#25490;&#21015;&#19981;&#21464;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#20272;&#35745;&#22914;&#20309;&#23558;&#27169;&#26495;&#28857;&#20113;&#21464;&#24418;&#20026;&#29305;&#23450;&#20027;&#20307;&#30340;&#32593;&#26684;&#65292;&#24418;&#25104;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#24418;&#24577;&#27169;&#22411;&#12290;Mesh2SSM&#36824;&#21487;&#20197;&#23398;&#20064;&#31181;&#32676;&#29305;&#23450;&#30340;&#27169;&#26495;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#25968;&#25454;&#23545;&#40784;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical shape modeling is the computational process of discovering significant shape parameters from segmented anatomies captured by medical images (such as MRI and CT scans), which can fully describe subject-specific anatomy in the context of a population. The presence of substantial non-linear variability in human anatomy often makes the traditional shape modeling process challenging. Deep learning techniques can learn complex non-linear representations of shapes and generate statistical shape models that are more faithful to the underlying population-level variability. However, existing deep learning models still have limitations and require established/optimized shape models for training. We propose Mesh2SSM, a new approach that leverages unsupervised, permutation-invariant representation learning to estimate how to deform a template point cloud to subject-specific meshes, forming a correspondence-based shape model. Mesh2SSM can also learn a population-specific template, reduci
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.07804</link><description>&lt;p&gt;
Dr. LLaMA&#65306;&#36890;&#36807;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;QA&#20013;&#30340;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#38543;&#30528;&#20854;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20063;&#38754;&#20020;&#30528;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#23481;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#32858;&#28966;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#21644;PubMedQA&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLM&#26377;&#25928;&#22320;&#32454;&#21270;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#20351;&#24471;&#23567;&#22411;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26368;&#32456;&#26088;&#22312;&#20026;&#19987;&#19994;&#24212;&#29992;&#21019;&#24314;&#26356;&#39640;&#25928;&#21644;&#33021;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Deepfake&#25216;&#26415;&#20135;&#29983;&#27809;&#26377;&#37325;&#38899;&#30340;&#35821;&#38899;&#26469;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#37325;&#38899;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#35821;&#38899;&#21644;&#21475;&#36848;&#35821;&#38899;&#65292;&#33021;&#22815;&#20998;&#31163;&#20986;&#30456;&#23545;&#23481;&#26131;&#26816;&#27979;&#21040;&#30340;&#37325;&#38899;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.07791</link><description>&lt;p&gt;
&#20351;&#29992;Deepfake&#25216;&#26415;&#23454;&#29616;&#35821;&#38899;&#37325;&#38899;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Using Deepfake Technologies for Word Emphasis Detection. (arXiv:2305.07791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Deepfake&#25216;&#26415;&#20135;&#29983;&#27809;&#26377;&#37325;&#38899;&#30340;&#35821;&#38899;&#26469;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#37325;&#38899;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#35821;&#38899;&#21644;&#21475;&#36848;&#35821;&#38899;&#65292;&#33021;&#22815;&#20998;&#31163;&#20986;&#30456;&#23545;&#23481;&#26131;&#26816;&#27979;&#21040;&#30340;&#37325;&#38899;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#33258;&#21160;&#35821;&#38899;&#37325;&#38899;&#26816;&#27979;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#37325;&#38899;&#21463;&#21040;&#35762;&#35805;&#32773;&#35821;&#38899;&#29305;&#28857;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#21475;&#38899;&#12289;&#26041;&#35328;&#25110;&#22768;&#38899;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;Deepfake&#25216;&#26415;&#20026;&#36825;&#20010;&#35828;&#35805;&#32773;&#20135;&#29983;&#19968;&#20010;&#27809;&#26377;&#37325;&#38899;&#30340;&#35821;&#38899;&#12290;&#36825;&#38656;&#35201;&#25552;&#21462;&#21475;&#36848;&#35821;&#38899;&#30340;&#25991;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#26469;&#33258;&#21516;&#19968;&#28436;&#35762;&#32773;&#30340;&#35821;&#38899;&#26679;&#26412;&#26469;&#20026;&#36825;&#20010;&#20219;&#21153;&#20135;&#29983;&#27809;&#26377;&#37325;&#38899;&#30340;&#35821;&#38899;&#12290;&#36890;&#36807;&#27604;&#36739;&#29983;&#25104;&#30340;&#35821;&#38899;&#21644;&#21475;&#36848;&#35821;&#38899;&#65292;&#25105;&#20204;&#33021;&#22815;&#20998;&#31163;&#20986;&#30456;&#23545;&#23481;&#26131;&#26816;&#27979;&#21040;&#30340;&#37325;&#38899;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider the task of automated emphasis detection for spoken language. This problem is challenging in that emphasis is affected by the particularities of speech of the subject, for example the subject accent, dialect or voice. To address this task, we propose to utilize deep fake technology to produce an emphasis devoid speech for this speaker. This requires extracting the text of the spoken voice, and then using a voice sample from the same speaker to produce emphasis devoid speech for this task. By comparing the generated speech with the spoken voice, we are able to isolate patterns of emphasis which are relatively easy to detect.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26399;&#26395;&#27425;&#27169;&#24615;&#30340;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#36138;&#24515;&#31639;&#27861;&#22312;&#26399;&#26395;&#24847;&#20041;&#19979;&#20445;&#35777;&#20102;&#36873;&#21462;&#36817;&#20284;&#26368;&#20248;&#21521;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#32531;&#35299;&#20102;&#24120;&#29992;&#21305;&#37197;&#36861;&#36394;&#65288;MP&#65289;&#31639;&#27861;&#20013;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2305.07782</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21305;&#37197;&#36861;&#36394;&#65306;&#36229;&#36234;&#36817;&#20284;&#27425;&#27169;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Matching Pursuit: Beyond Approximate Submodularity. (arXiv:2305.07782v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26399;&#26395;&#27425;&#27169;&#24615;&#30340;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#36138;&#24515;&#31639;&#27861;&#22312;&#26399;&#26395;&#24847;&#20041;&#19979;&#20445;&#35777;&#20102;&#36873;&#21462;&#36817;&#20284;&#26368;&#20248;&#21521;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#32531;&#35299;&#20102;&#24120;&#29992;&#21305;&#37197;&#36861;&#36394;&#65288;MP&#65289;&#31639;&#27861;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#22823;&#37327;&#21521;&#37327;&#20013;&#36873;&#25321;&#23376;&#38598;&#65292;&#20197;&#20415;&#22312;&#19968;&#32452;&#20989;&#25968;&#19978;&#33719;&#24471;&#26368;&#20339;&#20449;&#21495;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#36138;&#24515;&#26041;&#27861;&#24050;&#24191;&#27867;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#19988;&#35768;&#22810;&#36825;&#20123;&#31639;&#27861;&#24050;&#22312;&#65288;&#24494;&#24369;&#30340;&#65289;&#27425;&#27169;&#24615;&#30340;&#35270;&#35282;&#20043;&#19979;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#20013;&#27809;&#26377;&#19968;&#20010;&#26126;&#30830;&#20351;&#29992;&#36825;&#31181;&#20989;&#25968;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#37325;&#26032;&#23457;&#35270;&#21521;&#37327;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#19968;&#20010;&#34987;&#35777;&#26126;&#20855;&#26377;&#26399;&#26395;&#27425;&#27169;&#24615;&#30340;&#20989;&#25968;&#12290;&#36825;&#20010;&#20989;&#25968;&#19981;&#20165;&#36890;&#36807;&#36138;&#24515;&#31639;&#27861;&#22312;&#26399;&#26395;&#24847;&#20041;&#19979;&#20445;&#35777;&#20102;&#36817;&#20284;&#26368;&#20248;&#24615;&#65292;&#32780;&#19988;&#36824;&#32531;&#35299;&#20102;&#24120;&#29992;&#21305;&#37197;&#36861;&#36394;&#65288;MP&#65289;&#31639;&#27861;&#20013;&#30340;&#29616;&#26377;&#32570;&#38519;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#36138;&#24515;&#31639;&#27861;&#30340;&#21333;&#28857;&#20272;&#35745;&#29256;&#26412;&#19982;MP&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#35282;&#24230;&#21040;&#36798;&#20272;&#35745;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#30340;&#25903;&#25345;&#65292;&#36825;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#20449;&#21495;&#34920;&#31034;&#20219;&#21153;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of selecting a subset of vectors from a large set, to obtain the best signal representation over a family of functions. Although greedy methods have been widely used for tackling this problem and many of those have been analyzed under the lens of (weak) submodularity, none of these algorithms are explicitly devised using such a functional property. Here, we revisit the vector-selection problem and introduce a function which is shown to be submodular in expectation. This function does not only guarantee near-optimality through a greedy algorithm in expectation, but also alleviates the existing deficiencies in commonly used matching pursuit (MP) algorithms. We further show the relation between the single-point-estimate version of the proposed greedy algorithm and MP variants. Our theoretical results are supported by numerical experiments for the angle of arrival estimation problem, a typical signal representation task; the experiments demonstrate the benefits of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21152;&#36895;&#22120;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#24212;&#29992;&#20110;RNN-T&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;NNA&#25805;&#20316;&#65292;&#20351;&#29992;&#25143;&#24863;&#30693;&#24310;&#36831;&#65288;UPL&#65289;&#20943;&#23569;&#65292;&#21516;&#26102;&#22312;&#24341;&#25806;&#24310;&#36831;&#26041;&#38754;&#26377;5-7&#65285;&#30340;&#25913;&#36827;&#65292;&#33410;&#30465;&#39640;&#36798;10&#65285;&#30340;WER&#30456;&#23545;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2305.07778</link><description>&lt;p&gt;
&#22522;&#20110;&#21152;&#36895;&#22120;&#30340;&#35757;&#32451;&#65306;&#29992;&#20110;&#36716;&#25442;&#22120;&#35821;&#38899;&#35782;&#21035;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerator-Aware Training for Transducer-Based Speech Recognition. (arXiv:2305.07778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21152;&#36895;&#22120;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#24212;&#29992;&#20110;RNN-T&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;NNA&#25805;&#20316;&#65292;&#20351;&#29992;&#25143;&#24863;&#30693;&#24310;&#36831;&#65288;UPL&#65289;&#20943;&#23569;&#65292;&#21516;&#26102;&#22312;&#24341;&#25806;&#24310;&#36831;&#26041;&#38754;&#26377;5-7&#65285;&#30340;&#25913;&#36827;&#65292;&#33410;&#30465;&#39640;&#36798;10&#65285;&#30340;WER&#30456;&#23545;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#29366;&#24577;&#20197;&#20840;&#31934;&#24230;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#33455;&#29255;&#19978;&#36827;&#34892;&#37096;&#32626;&#26102;&#65292;&#36825;&#20250;&#23548;&#33268;&#36816;&#34892;&#26102;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#38454;&#27573;&#22797;&#21046;&#20102;NNA&#36816;&#31639;&#31526;&#26469;&#32771;&#34385;&#20302;&#31934;&#24230;&#25512;&#29702;&#24102;&#26469;&#30340;&#21518;&#21521;&#20256;&#25773;&#20013;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;NNA&#25805;&#20316;&#65292;&#36991;&#20813;&#20102;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#65288;CPU&#65289;&#20013;&#30340;&#37327;&#21270;&#38169;&#35823;&#65292;&#20174;&#32780;&#26368;&#32456;&#20943;&#23569;&#20102;&#29992;&#25143;&#24863;&#30693;&#24310;&#36831;&#65288;UPL&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#36716;&#24405;&#22120;&#65288;RNN-T&#65289;&#20013;&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#35774;&#22791;&#19978;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#20248;&#31168;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;270K&#23567;&#26102;&#30340;&#33521;&#35821;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#24341;&#25806;&#24310;&#36831;&#26041;&#38754;&#26377;5-7&#65285;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#33410;&#30465;&#39640;&#36798;10&#65285;&#30340;WER&#30456;&#23545;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning model weights and activations are represented in full-precision during training. This leads to performance degradation in runtime when deployed on neural network accelerator (NNA) chips, which leverage highly parallelized fixed-point arithmetic to improve runtime memory and latency. In this work, we replicate the NNA operators during the training phase, accounting for the degradation due to low-precision inference on the NNA in back-propagation. Our proposed method efficiently emulates NNA operations, thus foregoing the need to transfer quantization error-prone data to the Central Processing Unit (CPU), ultimately reducing the user perceived latency (UPL). We apply our approach to Recurrent Neural Network-Transducer (RNN-T), an attractive architecture for on-device streaming speech recognition tasks. We train and evaluate models on 270K hours of English data and show a 5-7% improvement in engine latency while saving up to 10% relative degradation in WER.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Nazr&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36830;&#32493;&#30417;&#27979;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.07772</link><description>&lt;p&gt;
&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30417;&#27979;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Monitoring and Adapting ML Models on Mobile Devices. (arXiv:2305.07772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Nazr&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36830;&#32493;&#30417;&#27979;&#21644;&#35843;&#25972;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20302;&#24310;&#36831;&#25512;&#29702;&#21644;&#31163;&#32447;&#25805;&#20316;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#21040;&#31227;&#21160;&#35774;&#22791;&#19978;&#12290;&#28982;&#32780;&#65292;&#19968;&#26086;&#37096;&#32626;&#20102;&#27169;&#22411;&#65292;&#36816;&#33829;&#32773;&#38590;&#20197;&#36861;&#36394;&#20854;&#31934;&#30830;&#24230;&#65292;&#21487;&#33021;&#20250;&#22240;&#20026;&#25968;&#25454;&#28418;&#31227;&#31561;&#38382;&#39064;&#32780;&#19981;&#21487;&#39044;&#27979;&#22320;&#38477;&#20302;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;Nazr&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36830;&#32493;&#30417;&#27979;&#21644;&#35843;&#25972;&#27169;&#22411;&#65292;&#26080;&#38656;&#29992;&#25143;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#27169;&#22411;&#36864;&#21270;&#36890;&#24120;&#26159;&#30001;&#29305;&#23450;&#30340;&#26681;&#26412;&#21407;&#22240;&#36896;&#25104;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#22823;&#37327;&#35774;&#22791;&#12290;&#22240;&#27492;&#65292;&#19968;&#26086;Nazr&#26816;&#27979;&#21040;&#22823;&#37327;&#35774;&#22791;&#19978;&#30340;&#19968;&#33268;&#24615;&#36864;&#21270;&#65292;&#23427;&#23601;&#20250;&#37319;&#29992;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#26469;&#30830;&#23450;&#38382;&#39064;&#30340;&#36215;&#28304;&#65292;&#24182;&#24212;&#29992;&#29305;&#23450;&#20110;&#21407;&#22240;&#30340;&#36866;&#24212;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Nazr&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#25552;&#39640;&#20934;&#30830;&#24615;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#20174;&#39550;&#39542;&#27773;&#36710;&#20013;&#25910;&#38598;&#30340;&#29031;&#29255;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;Nazr&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML models are increasingly being pushed to mobile devices, for low-latency inference and offline operation. However, once the models are deployed, it is hard for ML operators to track their accuracy, which can degrade unpredictably (e.g., due to data drift). We design Nazar, the first end-to-end system for continuously monitoring and adapting models on mobile devices without requiring feedback from users. Our key observation is that often model degradation is due to a specific root cause, which may affect a large group of devices. Therefore, once Nazar detects a consistent degradation across a large number of devices, it employs a root cause analysis to determine the origin of the problem and applies a cause-specific adaptation. We evaluate Nazar on two computer vision datasets, and show it consistently boosts accuracy compared to existing approaches. On a dataset containing photos collected from driving cars, Nazar improves the accuracy on average by 15%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2305.07759</link><description>&lt;p&gt;
TinyStories: &#35821;&#35328;&#27169;&#22411;&#33021;&#31616;&#23567;&#21040;&#20160;&#20040;&#31243;&#24230;&#21364;&#20381;&#28982;&#33021;&#22815;&#35762;&#36848;&#36830;&#36143;&#30340;&#33521;&#25991;&#25925;&#20107;&#65311;
&lt;/p&gt;
&lt;p&gt;
TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#23567;&#22411;&#21270;&#26102;&#32463;&#24120;&#38590;&#20197;&#20135;&#29983;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026; TinyStories &#30340;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#35268;&#27169;&#23567;&#12289;&#22797;&#26434;&#24230;&#20302;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#30701;&#25925;&#20107;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).  In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet stil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#29109;&#20272;&#35745;&#30340;&#31169;&#26377;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23545;&#20110;&#22810;&#21464;&#37327;&#32852;&#21512;&#20998;&#24067;&#65292;&#20854;&#26679;&#26412;&#25968;&#19982;&#21464;&#37327;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65307;&#31639;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.07751</link><description>&lt;p&gt;
&#29992;&#20110;&#29109;&#20272;&#35745;&#30340;&#31169;&#26377;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Private and Communication-Efficient Algorithms for Entropy Estimation. (arXiv:2305.07751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#29109;&#20272;&#35745;&#30340;&#31169;&#26377;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23545;&#20110;&#22810;&#21464;&#37327;&#32852;&#21512;&#20998;&#24067;&#65292;&#20854;&#26679;&#26412;&#25968;&#19982;&#21464;&#37327;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65307;&#31639;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#32479;&#35745;&#20272;&#35745;&#36890;&#24120;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#36827;&#34892;&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#21333;&#20010;&#29992;&#25143;&#65292;&#35813;&#29992;&#25143;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20849;&#20139;&#20854;&#25968;&#25454;&#12290;&#29992;&#25143;&#36890;&#24120;&#20851;&#24515;&#20445;&#25252;&#20854;&#26679;&#26412;&#30340;&#38544;&#31169;&#65292;&#24182;&#23613;&#37327;&#20943;&#23569;&#20854;&#24517;&#39035;&#21521;&#26381;&#21153;&#22120;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#31169;&#26377;&#21644;&#36890;&#20449;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20998;&#24067;&#29109;&#30340;&#20960;&#20010;&#27969;&#34892;&#24230;&#37327;&#12290;&#25105;&#20204;&#25152;&#26377;&#30340;&#31639;&#27861;&#36890;&#20449;&#25104;&#26412;&#22266;&#23450;&#19988;&#31526;&#21512;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#12290;&#23545;&#20110;&#30001;&#26641;&#32473;&#20986;&#26465;&#20214;&#29420;&#31435;&#24615;&#30340;&#22810;&#21464;&#37327;&#32852;&#21512;&#20998;&#24067;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20272;&#35745;Shannon&#29109;&#30340;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#25968;&#19982;&#21464;&#37327;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#32780;&#20808;&#21069;&#24037;&#20316;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;&#20108;&#27425;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;Gini&#29109;&#30340;&#31639;&#27861;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#27809;&#26377;&#20381;&#36182;&#20110;&#20998;&#24067;&#25903;&#25345;&#22823;&#23567;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;noteq
&lt;/p&gt;
&lt;p&gt;
Modern statistical estimation is often performed in a distributed setting where each sample belongs to a single user who shares their data with a central server. Users are typically concerned with preserving the privacy of their samples, and also with minimizing the amount of data they must transmit to the server. We give improved private and communication-efficient algorithms for estimating several popular measures of the entropy of a distribution. All of our algorithms have constant communication cost and satisfy local differential privacy. For a joint distribution over many variables whose conditional independence is given by a tree, we describe algorithms for estimating Shannon entropy that require a number of samples that is linear in the number of variables, compared to the quadratic sample complexity of prior work. We also describe an algorithm for estimating Gini entropy whose sample complexity has no dependence on the support size of the distribution and can be implemented usi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#32852;&#21512;&#20272;&#35745;&#65288;WDJE&#65289;&#30340;&#20998;&#26512;&#26041;&#27861;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#36716;&#31227;&#33021;&#21147;&#35780;&#20272;&#21644;&#20915;&#31574;&#65292;&#35206;&#30422;&#20102;&#39046;&#22495;&#21644;&#20219;&#21153;&#24046;&#24322;&#65292;&#20197;&#21450;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#12290;&#20174;&#27604;&#36739;&#30446;&#26631;&#39118;&#38505;&#30340;&#35282;&#24230;&#20419;&#36827;&#20915;&#31574;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#35745;&#31639;&#30340;&#30446;&#26631;&#39118;&#38505;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.07741</link><description>&lt;p&gt;
&#36716;&#31227;&#36824;&#26159;&#19981;&#36716;&#31227;&#65306;&#32479;&#19968;&#30340;&#21487;&#36716;&#31227;&#24230;&#37327;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
To transfer or not transfer: Unified transferability metric and analysis. (arXiv:2305.07741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#32852;&#21512;&#20272;&#35745;&#65288;WDJE&#65289;&#30340;&#20998;&#26512;&#26041;&#27861;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#36716;&#31227;&#33021;&#21147;&#35780;&#20272;&#21644;&#20915;&#31574;&#65292;&#35206;&#30422;&#20102;&#39046;&#22495;&#21644;&#20219;&#21153;&#24046;&#24322;&#65292;&#20197;&#21450;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#12290;&#20174;&#27604;&#36739;&#30446;&#26631;&#39118;&#38505;&#30340;&#35282;&#24230;&#20419;&#36827;&#20915;&#31574;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#35745;&#31639;&#30340;&#30446;&#26631;&#39118;&#38505;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36716;&#31227;&#23398;&#20064;&#20013;&#65292;&#21487;&#36716;&#31227;&#24615;&#26159;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#65292;&#26088;&#22312;&#35780;&#20272;&#20219;&#24847;&#36716;&#31227;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;&#29616;&#26377;&#30740;&#31350;&#20851;&#27880;&#20998;&#31867;&#20219;&#21153;&#65292;&#24573;&#35270;&#20102;&#39046;&#22495;&#25110;&#20219;&#21153;&#24046;&#24322;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#32570;&#20047;&#30830;&#23450;&#26159;&#21542;&#36716;&#31227;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#21644;&#24230;&#37327;&#26631;&#20934;-&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#32852;&#21512;&#20272;&#35745;&#65288;WDJE&#65289;&#65292;&#29992;&#20110;&#36716;&#31227;&#33021;&#21147;&#35780;&#20272;&#21644;&#20915;&#31574;&#65292;&#24182;&#22312;&#32479;&#19968;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#38382;&#39064;&#20013;&#32771;&#34385;&#20102;&#39046;&#22495;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;WDJE&#36890;&#36807;&#27604;&#36739;&#26377;&#26080;&#36716;&#31227;&#30340;&#30446;&#26631;&#39118;&#38505;&#26469;&#20419;&#36827;&#20915;&#31574;&#21046;&#23450;&#12290;&#20026;&#20102;&#20351;&#27604;&#36739;&#25104;&#31435;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#23545;&#31216;&#65292;&#26131;&#20110;&#29702;&#35299;&#21644;&#26131;&#20110;&#35745;&#31639;&#30340;&#30446;&#26631;&#39118;&#38505;&#36793;&#30028;&#65292;&#20197;&#36817;&#20284;&#30446;&#26631;&#36716;&#31227;&#39118;&#38505;&#65292;&#21363;&#20351;&#26377;&#38480;&#30340;&#30446;&#26631;&#26631;&#31614;&#20063;&#21487;&#20197;&#32988;&#20219;&#12290;&#25152;&#25552;&#20986;&#30340;&#36793;&#30028;&#23558;&#30446;&#26631;&#39118;&#38505;&#19982;&#28304;&#27169;&#22411;&#24615;&#33021;&#65292;&#39046;&#22495;&#21644;&#20849;&#20139;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In transfer learning, transferability is one of the most fundamental problems, which aims to evaluate the effectiveness of arbitrary transfer tasks. Existing research focuses on classification tasks and neglects domain or task differences. More importantly, there is a lack of research to determine whether to transfer or not. To address these, we propose a new analytical approach and metric, Wasserstein Distance based Joint Estimation (WDJE), for transferability estimation and determination in a unified setting: classification and regression problems with domain and task differences. The WDJE facilitates decision-making on whether to transfer or not by comparing the target risk with and without transfer. To enable the comparison, we approximate the target transfer risk by proposing a non-symmetric, easy-to-understand and easy-to-calculate target risk bound that is workable even with limited target labels. The proposed bound relates the target risk to source model performance, domain and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#26681;&#26893;&#20110;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#24778;&#22855;&#35745;&#31639;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22797;&#26434;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24778;&#22855;&#34892;&#20026;&#26816;&#27979;&#65292;&#24182;&#21487;&#29992;&#20110;&#20132;&#36890;&#23433;&#20840;&#20013;&#30340;&#20914;&#31361;&#35782;&#21035;&#21644;&#39550;&#39542;&#34892;&#20026;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.07733</link><description>&lt;p&gt;
&#37326;&#22806;&#24778;&#22855;&#24230;&#30340;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Surprise in the Wild. (arXiv:2305.07733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#26681;&#26893;&#20110;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#24778;&#22855;&#35745;&#31639;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#22797;&#26434;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24778;&#22855;&#34892;&#20026;&#26816;&#27979;&#65292;&#24182;&#21487;&#29992;&#20110;&#20132;&#36890;&#23433;&#20840;&#20013;&#30340;&#20914;&#31361;&#35782;&#21035;&#21644;&#39550;&#39542;&#34892;&#20026;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#24778;&#22855;&#26159;&#22914;&#20309;&#34987;&#20307;&#39564;&#30340;&#20197;&#21450;&#20309;&#26102;&#34987;&#20307;&#39564;&#30340;&#23450;&#37327;&#27979;&#37327;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20165;&#38480;&#20110;&#23454;&#39564;&#23460;&#30740;&#31350;&#65292;&#24182;&#19988;&#25193;&#23637;&#21040;&#33258;&#28982;&#29615;&#22659;&#19979;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#23637;&#31034;&#20102;&#26681;&#26893;&#20110;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#24778;&#22855;&#35745;&#31639;&#27169;&#22411;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#26816;&#27979;&#22797;&#26434;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24778;&#22855;&#20154;&#31867;&#34892;&#20026;&#65292;&#22914;&#36947;&#36335;&#20132;&#36890;&#12290;&#22312;&#20132;&#36890;&#23433;&#20840;&#26041;&#38754;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#25903;&#25345;&#20132;&#36890;&#20914;&#31361;&#30340;&#35782;&#21035;&#65292;&#36947;&#36335;&#29992;&#25143;&#21709;&#24212;&#26102;&#38388;&#30340;&#24314;&#27169;&#20197;&#21450;&#20154;&#31867;&#21644;&#33258;&#21160;&#39550;&#39542;&#21592;&#34892;&#20026;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#37327;&#21270;&#24778;&#22855;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#33258;&#28982;&#39550;&#39542;&#22330;&#26223;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24778;&#22855;&#24230;&#30340;&#20248;&#21183;&#12290;&#20351;&#29992;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#26469;&#24314;&#27169;&#24778;&#22855;&#34892;&#20026;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#65292;&#21487;&#25512;&#24191;&#21040;&#20219;&#20309;&#21160;&#24577;&#29616;&#23454;&#19990;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantitative measurement of how and when we experience surprise has mostly remained limited to laboratory studies, and its extension to naturalistic settings has been challenging. Here we demonstrate, for the first time, how computational models of surprise rooted in cognitive science and neuroscience combined with state-of-the-art machine learned generative models can be used to detect surprising human behavior in complex, dynamic environments like road traffic. In traffic safety, such models can support the identification of traffic conflicts, modeling of road user response time, and driving behavior evaluation for both human and autonomous drivers. We also present novel approaches to quantify surprise and use naturalistic driving scenarios to demonstrate a number of advantages over existing surprise measures from the literature. Modeling surprising behavior using learned generative models is a novel concept that can be generalized beyond traffic safety to any dynamic real-world 
&lt;/p&gt;</description></item><item><title>ATMGNN&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23427;&#33021;&#22815;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#39044;&#27979;&#26410;&#26469;&#30123;&#24773;&#36235;&#21183;&#12290;&#36890;&#36807;&#23398;&#20064;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22810;&#23610;&#24230;&#30340;&#31354;&#38388;&#22270;&#20013;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#21495;&#65292;&#24182;&#24314;&#27169;&#38271;&#31243;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ATMGNN&#22312;&#26032;&#35199;&#20848;&#30340;COVID-19&#30123;&#24773;&#39044;&#27979;&#20013;&#34920;&#29616;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07731</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#35199;&#20848;COVID-19&#30123;&#24773;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Predicting COVID-19 pandemic by spatio-temporal graph neural networks: A New Zealand's study. (arXiv:2305.07731v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07731
&lt;/p&gt;
&lt;p&gt;
ATMGNN&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23427;&#33021;&#22815;&#32467;&#21512;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#39044;&#27979;&#26410;&#26469;&#30123;&#24773;&#36235;&#21183;&#12290;&#36890;&#36807;&#23398;&#20064;&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22810;&#23610;&#24230;&#30340;&#31354;&#38388;&#22270;&#20013;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#21495;&#65292;&#24182;&#24314;&#27169;&#38271;&#31243;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ATMGNN&#22312;&#26032;&#35199;&#20848;&#30340;COVID-19&#30123;&#24773;&#39044;&#27979;&#20013;&#34920;&#29616;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30123;&#24773;&#21160;&#24577;&#24314;&#27169;&#21644;&#27169;&#25311;&#22312;&#29702;&#35299;&#21644;&#24212;&#23545;COVID-19&#31561;&#39640;&#20256;&#26579;&#24615;&#30142;&#30149;&#20256;&#25773;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#20998;&#36776;&#29575;&#22270;&#31070;&#32463;&#32593;&#32476;&#8221;(ATMGNN)&#30340;&#26032;&#39062;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#23398;&#20064;&#23558;&#31354;&#38388;&#22270;&#20449;&#24687;(&#21363;&#22320;&#29702;&#25968;&#25454;)&#19982;&#26102;&#38388;&#20449;&#24687;(&#21363;COVID-19&#30149;&#20363;&#25968;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;)&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#30123;&#24773;&#26410;&#26469;&#21160;&#24577;&#12290;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#32858;&#31867;&#31639;&#27861;&#25429;&#33719;&#31354;&#38388;&#22270;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26550;&#26500;&#21487;&#20197;&#23398;&#20064;&#25429;&#25417;&#30123;&#24773;&#30340;&#23616;&#37096;&#25110;&#20840;&#23616;&#20449;&#21495;&#65292;&#24182;&#24314;&#27169;&#38271;&#31243;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25910;&#38598;&#21644;&#32452;&#35013;&#20102;&#26032;&#35199;&#20848;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#32479;&#35745;&#26041;&#27861;&#12289;&#26102;&#38388;&#26550;&#26500;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#22312;&#39044;&#27979;&#26032;&#35199;&#20848;COVID-19&#30123;&#24773;&#36235;&#21183;&#26041;&#38754;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;ATMGNN&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and simulations of pandemic dynamics play an essential role in understanding and addressing the spreading of highly infectious diseases such as COVID-19. In this work, we propose a novel deep learning architecture named Attention-based Multiresolution Graph Neural Networks (ATMGNN) that learns to combine the spatial graph information, i.e. geographical data, with the temporal information, i.e. timeseries data of number of COVID-19 cases, to predict the future dynamics of the pandemic. The key innovation is that our method can capture the multiscale structures of the spatial graph via a learning to cluster algorithm in a data-driven manner. This allows our architecture to learn to pick up either local or global signals of a pandemic, and model both the long-range spatial and temporal dependencies. Importantly, we collected and assembled a new dataset for New Zealand. We established a comprehensive benchmark of statistical methods, temporal architectures, graph neural networks a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;BOED&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#25105;&#20204;&#33021;&#22815;&#20174;&#20013;&#27169;&#25311;&#25968;&#25454;&#30340;&#20219;&#20309;&#31867;&#22411;&#30340;&#27169;&#22411;&#25214;&#21040;&#26368;&#20248;&#23454;&#39564;&#65292;&#20197;&#33719;&#24471;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#12290;</title><link>http://arxiv.org/abs/2305.07721</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#26368;&#20248;&#34892;&#20026;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Designing Optimal Behavioral Experiments Using Machine Learning. (arXiv:2305.07721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;BOED&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#25105;&#20204;&#33021;&#22815;&#20174;&#20013;&#27169;&#25311;&#25968;&#25454;&#30340;&#20219;&#20309;&#31867;&#22411;&#30340;&#27169;&#22411;&#25214;&#21040;&#26368;&#20248;&#23454;&#39564;&#65292;&#20197;&#33719;&#24471;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#27169;&#22411;&#26159;&#29702;&#35299;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23427;&#20204;&#33021;&#22815;&#28165;&#26224;&#12289;&#31934;&#30830;&#22320;&#34920;&#36798;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#32454;&#24494;&#32780;&#24120;&#24120;&#20986;&#20154;&#24847;&#26009;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20016;&#23500;&#24615;&#21644;&#24778;&#21916;&#30340;&#33021;&#21147;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#31185;&#23398;&#30452;&#35273;&#21644;&#20256;&#32479;&#24037;&#20855;&#19981;&#36866;&#21512;&#35774;&#35745;&#23454;&#39564;&#26469;&#27979;&#35797;&#21644;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#38519;&#38449;&#24182;&#23454;&#29616;&#35745;&#31639;&#24314;&#27169;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#23454;&#39564;&#30340;&#24037;&#20855;&#65292;&#20197;&#28165;&#26224;&#22320;&#22238;&#31572;&#27169;&#22411;&#35299;&#37322;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#24335;&#20197;&#21450;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#20570;&#20986;&#30340;&#36741;&#21161;&#20551;&#35774;&#12290;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65288;BOED&#65289;&#36890;&#36807;&#30830;&#23450;&#39044;&#26399;&#20135;&#29983;&#20449;&#24687;&#24615;&#25968;&#25454;&#30340;&#23454;&#39564;&#26469;&#27491;&#24335;&#21270;&#25628;&#32034;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;BOED&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#25105;&#20204;&#33021;&#22815;&#20174;&#20013;&#27169;&#25311;&#25968;&#25454;&#30340;&#20219;&#20309;&#31867;&#22411;&#30340;&#27169;&#22411;&#25214;&#21040;&#26368;&#20248;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational models are powerful tools for understanding human cognition and behavior. They let us express our theories clearly and precisely, and offer predictions that can be subtle and often counter-intuitive. However, this same richness and ability to surprise means our scientific intuitions and traditional tools are ill-suited to designing experiments to test and compare these models. To avoid these pitfalls and realize the full potential of computational modeling, we require tools to design experiments that provide clear answers about what models explain human behavior and the auxiliary assumptions those models must make. Bayesian optimal experimental design (BOED) formalizes the search for optimal experimental designs by identifying experiments that are expected to yield informative data. In this work, we provide a tutorial on leveraging recent advances in BOED and machine learning to find optimal experiments for any kind of model that we can simulate data from, and show how by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;14&#20010;&#26837;&#30702;&#26143;&#27169;&#22411;&#32593;&#26684;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21457;&#29616;&#26837;&#30702;&#26143;&#30340;&#26377;&#25928;&#28201;&#24230;&#21487;&#20197;&#34987;&#39044;&#27979;&#65292;&#20294;&#25512;&#26029;&#34920;&#38754;&#37325;&#21147;&#21152;&#36895;&#24230;&#21644;&#37329;&#23646;&#20016;&#24230;&#19982;&#27169;&#22411;&#32593;&#26684;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.07719</link><description>&lt;p&gt;
&#26837;&#30702;&#26143;&#27169;&#22411;&#32593;&#26684;&#19982;&#26426;&#22120;&#23398;&#20064;&#22823;&#27668;&#21453;&#28436;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Intercomparison of Brown Dwarf Model Grids and Atmospheric Retrieval Using Machine Learning. (arXiv:2305.07719v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;14&#20010;&#26837;&#30702;&#26143;&#27169;&#22411;&#32593;&#26684;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21457;&#29616;&#26837;&#30702;&#26143;&#30340;&#26377;&#25928;&#28201;&#24230;&#21487;&#20197;&#34987;&#39044;&#27979;&#65292;&#20294;&#25512;&#26029;&#34920;&#38754;&#37325;&#21147;&#21152;&#36895;&#24230;&#21644;&#37329;&#23646;&#20016;&#24230;&#19982;&#27169;&#22411;&#32593;&#26684;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#27425;&#24658;&#26143;&#20809;&#35889;&#25968;&#25454;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#23588;&#20854;&#23545;&#20110;&#33258;&#27965;&#27169;&#22411;&#32593;&#26684;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#37319;&#29992;&#38543;&#26426;&#26862;&#26519;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;14&#20010;&#20197;&#21069;&#21457;&#34920;&#30340;&#26837;&#30702;&#26143;&#27169;&#22411;&#32593;&#26684;&#65288;&#20174;1997&#21040;2021&#24180;&#65289;&#12290;&#38543;&#26426;&#26862;&#26519;&#26041;&#27861;&#35753;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#32593;&#26684;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#22312;&#36817;&#20284;&#36125;&#21494;&#26031;&#35745;&#31639;&#26694;&#26550;&#19979;&#35299;&#37322;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;3&#20010;&#22522;&#20934;&#26837;&#30702;&#26143;&#65288;Gl 570D&#65292;&#949; Indi Ba&#21644;Bb&#65289;&#20197;&#21450;19&#20010;L&#22411;&#21644;T&#22411;&#30702;&#26143;&#30340;&#26679;&#26412;&#65307;&#36825;&#20010;&#26679;&#26412;&#20043;&#21069;&#26366;&#20351;&#29992;&#20256;&#32479;&#36125;&#21494;&#26031;&#26041;&#27861;&#65288;&#23884;&#22871;&#21462;&#26679;&#65289;&#22312;Lueber&#31561;&#20154;&#65288;2022&#65289;&#20013;&#36827;&#34892;&#36807;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#20197;&#29420;&#31435;&#20110;&#25152;&#36873;&#25321;&#30340;&#27169;&#22411;&#32593;&#26684;&#65292;&#24378;&#26377;&#21147;&#22320;&#39044;&#27979;&#26837;&#30702;&#26143;&#30340;&#26377;&#25928;&#28201;&#24230;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#26837;&#30702;&#26143;&#34920;&#38754;&#37325;&#21147;&#21152;&#36895;&#24230;&#21644;&#37329;&#23646;&#20016;&#24230;&#22240;&#25152;&#36873;&#25321;&#30340;&#27169;&#22411;&#32593;&#26684;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding differences between sub-stellar spectral data and models has proven to be a major challenge, especially for self-consistent model grids that are necessary for a thorough investigation of brown dwarf atmospheres. Using the supervised machine learning method of the random forest, we study the information content of 14 previously published model grids of brown dwarfs (from 1997 to 2021). The random forest method allows us to analyze the predictive power of these model grids, as well as interpret data within the framework of Approximate Bayesian Computation (ABC). Our curated dataset includes 3 benchmark brown dwarfs (Gl 570D, {\epsilon} Indi Ba and Bb) as well as a sample of 19 L and T dwarfs; this sample was previously analyzed in Lueber et al. (2022) using traditional Bayesian methods (nested sampling). We find that the effective temperature of a brown dwarf can be robustly predicted independent of the model grid chosen for the interpretation. However, inference of the sur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;ResNets&#23548;&#20986;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#25351;&#20986;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#26159;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2305.07715</link><description>&lt;p&gt;
&#36890;&#36807;&#27531;&#24046;&#32553;&#25918;&#23454;&#29616;ResNets&#30340;&#20449;&#21495;&#26368;&#20248;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Optimal signal propagation in ResNets through residual scaling. (arXiv:2305.07715v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;ResNets&#23548;&#20986;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#25351;&#20986;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#26159;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Residual&#32593;&#32476;&#65288;ResNets&#65289;&#22312;&#22823;&#28145;&#24230;&#19978;&#27604;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#24341;&#20837;&#36339;&#36807;&#36830;&#25509;&#21487;&#20197;&#20419;&#36827;&#20449;&#21495;&#21521;&#26356;&#28145;&#23618;&#30340;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#20026;&#27531;&#24046;&#20998;&#25903;&#28155;&#21152;&#32553;&#25918;&#21442;&#25968;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#23613;&#31649;&#20182;&#20204;&#32463;&#39564;&#24615;&#22320;&#30830;&#23450;&#20102;&#36825;&#31181;&#32553;&#25918;&#21442;&#25968;&#29305;&#21035;&#26377;&#21033;&#30340;&#21462;&#20540;&#33539;&#22260;&#65292;&#20294;&#20854;&#30456;&#20851;&#30340;&#24615;&#33021;&#25552;&#21319;&#21450;&#20854;&#22312;&#32593;&#32476;&#36229;&#21442;&#25968;&#19978;&#30340;&#26222;&#36866;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;&#23545;&#20110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFNets&#65289;&#65292;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#22312;&#20449;&#21495;&#20256;&#25773;&#21644;&#36229;&#21442;&#25968;&#35843;&#33410;&#26041;&#38754;&#33719;&#24471;&#20102;&#37325;&#35201;&#27934;&#35265;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20026;ResNets&#23548;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#20197;&#30740;&#31350;&#20449;&#21495;&#20256;&#25773;&#21450;&#20854;&#23545;&#27531;&#24046;&#20998;&#25903;&#32553;&#25918;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#23548;&#20986;&#21709;&#24212;&#20989;&#25968;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#36825;&#26159;&#34913;&#37327;&#32593;&#32476;&#23545;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#19968;&#31181;&#25351;&#26631;&#65292;&#24182;&#34920;&#26126;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#22312;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residual networks (ResNets) have significantly better trainability and thus performance than feed-forward networks at large depth. Introducing skip connections facilitates signal propagation to deeper layers. In addition, previous works found that adding a scaling parameter for the residual branch further improves generalization performance. While they empirically identified a particularly beneficial range of values for this scaling parameter, the associated performance improvement and its universality across network hyperparameters yet need to be understood. For feed-forward networks (FFNets), finite-size theories have led to important insights with regard to signal propagation and hyperparameter tuning. We here derive a systematic finite-size theory for ResNets to study signal propagation and its dependence on the scaling for the residual branch. We derive analytical expressions for the response function, a measure for the network's sensitivity to inputs, and show that for deep netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#35782;&#21035;&#21361;&#38505;&#23398;&#29983;&#22238;&#22797;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07709</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#21361;&#38505;&#30340;&#23398;&#29983;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Using Language Models to Detect Alarming Student Responses. (arXiv:2305.07709v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#35782;&#21035;&#21361;&#38505;&#23398;&#29983;&#22238;&#22797;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#35782;&#21035;&#21361;&#38505;&#23398;&#29983;&#22238;&#22797;&#30340;&#31995;&#32479;&#30340;&#36827;&#23637;&#12290;&#35813;&#31995;&#32479;&#38598;&#25104;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#24179;&#21488;&#20013;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#29983;&#30340;&#22238;&#22797;&#26159;&#21542;&#34920;&#26126;&#20182;&#20204;&#23545;&#33258;&#24049;&#25110;&#20182;&#20154;&#26500;&#25104;&#23041;&#32961;&#12290;&#36825;&#20123;&#22238;&#22797;&#21487;&#33021;&#21253;&#25324;&#20851;&#20110;&#26292;&#21147;&#23041;&#32961;&#12289;&#20005;&#37325;&#25233;&#37057;&#12289;&#33258;&#26432;&#39118;&#38505;&#21644;&#34384;&#24453;&#25551;&#36848;&#30340;&#32454;&#33410;&#12290;&#26368;&#26032;&#27169;&#22411;&#26159;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;&#30001;&#23398;&#29983;&#22238;&#22797;&#21644;&#34917;&#20805;&#25991;&#26412;&#26500;&#25104;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#32780;&#25104;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#27604;&#27492;&#21069;&#29256;&#26412;&#30340;&#31995;&#32479;&#33021;&#22815;&#22823;&#24133;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article details the advances made to a system that uses artificial intelligence to identify alarming student responses. This system is built into our assessment platform to assess whether a student's response indicates they are a threat to themselves or others. Such responses may include details concerning threats of violence, severe depression, suicide risks, and descriptions of abuse. Driven by advances in natural language processing, the latest model is a fine-tuned language model trained on a large corpus consisting of student responses and supplementary texts. We demonstrate that the use of a language model delivers a substantial improvement in accuracy over the previous iterations of this system.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20351;&#29992;&#21333;&#20154;&#28216;&#25103;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32593;&#32476;&#25915;&#20987;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#35757;&#32451;&#30340;&#20195;&#29702;&#20154;&#21644;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#23450;&#20041;&#65292;&#21457;&#29616;&#20248;&#21270;&#25915;&#20987;&#25110;&#38450;&#24481;&#32593;&#32476;&#23545;&#29305;&#23450;&#30446;&#26631;&#38750;&#24120;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.07687</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#25484;&#25569;&#31867;&#28183;&#36879;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Mastering Percolation-like Games with Deep Learning. (arXiv:2305.07687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07687
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20351;&#29992;&#21333;&#20154;&#28216;&#25103;&#21644;&#28145;&#24230;&#23398;&#20064;&#22312;&#32593;&#32476;&#25915;&#20987;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#35757;&#32451;&#30340;&#20195;&#29702;&#20154;&#21644;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#23450;&#20041;&#65292;&#21457;&#29616;&#20248;&#21270;&#25915;&#20987;&#25110;&#38450;&#24481;&#32593;&#32476;&#23545;&#29305;&#23450;&#30446;&#26631;&#38750;&#24120;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32593;&#32476;&#23545;&#38543;&#26426;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#26234;&#33021;&#20195;&#29702;&#30340;&#25925;&#24847;&#30772;&#22351;&#24182;&#19981;&#36866;&#29992;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#26230;&#26684;&#19978;&#30340;&#21333;&#20154;&#28216;&#25103;&#65292;&#27169;&#25311;&#25915;&#20987;&#32773;&#35797;&#22270;&#25703;&#27585;&#32593;&#32476;&#30340;&#36923;&#36753;&#12290;&#28216;&#25103;&#30340;&#30446;&#26631;&#26159;&#22312;&#26368;&#23569;&#30340;&#27493;&#39588;&#20013;&#31105;&#29992;&#25152;&#26377;&#33410;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;Q&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#23398;&#20064;&#29609;&#36825;&#20010;&#28216;&#25103;&#65292;&#24182;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#26368;&#20248;&#22320;&#25915;&#20987;&#32593;&#32476;&#12290;&#30001;&#20110;&#23398;&#20064;&#31639;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#25105;&#20204;&#35757;&#32451;&#20195;&#29702;&#20154;&#22312;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#23450;&#20041;&#19978;&#24182;&#27604;&#36739;&#23398;&#20064;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34920;&#38754;&#19978;&#30456;&#20284;&#30340;&#40065;&#26834;&#24615;&#23450;&#20041;&#24341;&#23548;&#35757;&#32451;&#20195;&#29702;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#26263;&#31034;&#30528;&#20248;&#21270;&#25915;&#20987;&#25110;&#38450;&#24481;&#32593;&#32476;&#23545;&#29305;&#23450;&#30446;&#26631;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#29702;&#35299;&#32593;&#32476;&#31283;&#20581;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#31163;&#25955;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though robustness of networks to random attacks has been widely studied, intentional destruction by an intelligent agent is not tractable with previous methods. Here we devise a single-player game on a lattice that mimics the logic of an attacker attempting to destroy a network. The objective of the game is to disable all nodes in the fewest number of steps. We develop a reinforcement learning approach using deep Q-learning that is capable of learning to play this game successfully, and in so doing, to optimally attack a network. Because the learning algorithm is universal, we train agents on different definitions of robustness and compare the learned strategies. We find that superficially similar definitions of robustness induce different strategies in the trained agent, implying that optimally attacking or defending a network is sensitive the particular objective. Our method provides a new approach to understand network robustness, with potential applications to other discrete proces
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20808;&#36827;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#33829;&#20859;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#28145;&#20837;&#30340;&#36136;&#37327;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#32454;&#33268;&#20998;&#26512;&#21512;&#25104;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#21644;&#20445;&#25252;&#26426;&#23494;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#30830;&#20445;&#21512;&#25104;&#25968;&#25454;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07685</link><description>&lt;p&gt;
&#19968;&#39033;&#32437;&#21521;&#38431;&#21015;&#30740;&#31350;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;-- &#23545;&#24050;&#21457;&#24067;&#25968;&#25454;&#20998;&#26512;&#32467;&#26524;&#30340;&#39564;&#35777;&#12289;&#26041;&#27861;&#25193;&#23637;&#21644;&#20877;&#29616;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation for a longitudinal cohort study -- Evaluation, method extension and reproduction of published data analysis results. (arXiv:2305.07685v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20808;&#36827;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#33829;&#20859;&#39046;&#22495;&#20013;&#30340;&#25968;&#25454;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#28145;&#20837;&#30340;&#36136;&#37327;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#32454;&#33268;&#20998;&#26512;&#21512;&#25104;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#21644;&#20445;&#25252;&#26426;&#23494;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#30830;&#20445;&#21512;&#25104;&#25968;&#25454;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35775;&#38382;&#20010;&#20154;&#20581;&#24247;&#25968;&#25454;&#23545;&#20110;&#33719;&#24471;&#26032;&#30340;&#35265;&#35299;&#21644;&#25512;&#21160;&#31185;&#23398;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#29616;&#20195;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#35775;&#38382;&#20010;&#20154;&#32423;&#21035;&#30340;&#25968;&#25454;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#26159;&#29983;&#25104;&#23436;&#20840;&#32508;&#21512;&#25968;&#25454;&#65292;&#21363;&#36890;&#36807;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#20855;&#26377;&#19982;&#21407;&#22987;&#25968;&#25454;&#30456;&#20284;&#32479;&#35745;&#29305;&#24615;&#30340;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#19982;&#21407;&#22987;&#20010;&#20154;&#32423;&#21035;&#35760;&#24405;&#27809;&#26377;&#19968;&#23545;&#19968;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#23545;&#22312;&#33829;&#20859;&#39046;&#22495;&#20013;&#29305;&#23450;&#29992;&#20363;&#30340;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#28145;&#20837;&#30340;&#36136;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#65292;&#36825;&#36229;&#20986;&#20102;&#25551;&#32472;&#24615;&#32479;&#35745;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20171;&#32461;&#22914;&#20309;&#23454;&#29616;&#32508;&#21512;&#25968;&#25454;&#38598;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#36890;&#36807;&#26041;&#27861;&#30340;&#25193;&#23637;&#21644;&#20445;&#25252;&#26426;&#23494;&#24615;&#65292;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#25104;&#20026;&#21487;&#37325;&#22797;&#30740;&#31350;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Access to individual-level health data is essential for gaining new insights and advancing science. In particular, modern methods based on artificial intelligence rely on the availability of and access to large datasets. In the health sector, access to individual-level data is often challenging due to privacy concerns. A promising alternative is the generation of fully synthetic data, i.e. data generated through a randomised process that have similar statistical properties as the original data, but do not have a one-to-one correspondence with the original individual-level records. In this study, we use a state-of-the-art synthetic data generation method and perform in-depth quality analyses of the generated data for a specific use case in the field of nutrition. We demonstrate the need for careful analyses of synthetic data that go beyond descriptive statistics and provide valuable insights into how to realise the full potential of synthetic datasets. By extending the methods, but also
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22522;&#20110;&#20449;&#24687;&#25216;&#26415;&#30340;&#25945;&#23398;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#35299;&#20915;&#32452;&#32455;&#22914;&#20309;&#26356;&#22909;&#22320;&#20256;&#25773;&#21363;&#23558;&#36864;&#20241;&#19987;&#23478;&#30340;&#30693;&#35782;&#24182;&#20256;&#25480;&#32473;&#26032;&#25163;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07681</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25945;&#23398;&#31995;&#32479;&#65306;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ML-Based Teaching Systems: A Conceptual Framework. (arXiv:2305.07681v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22522;&#20110;&#20449;&#24687;&#25216;&#26415;&#30340;&#25945;&#23398;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#35299;&#20915;&#32452;&#32455;&#22914;&#20309;&#26356;&#22909;&#22320;&#20256;&#25773;&#21363;&#23558;&#36864;&#20241;&#19987;&#23478;&#30340;&#30693;&#35782;&#24182;&#20256;&#25480;&#32473;&#26032;&#25163;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#21475;&#32467;&#26500;&#21464;&#21270;&#25152;&#24102;&#26469;&#30340;&#25216;&#33021;&#30701;&#32570;&#38382;&#39064;&#19981;&#26029;&#24694;&#21270;&#65292;&#32452;&#32455;&#26426;&#26500;&#22914;&#20309;&#20445;&#30041;&#21363;&#23558;&#36864;&#20241;&#19987;&#23478;&#30340;&#30693;&#35782;&#24182;&#20256;&#25480;&#32473;&#26032;&#25163;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#24615;&#25361;&#25112;&#12290;&#20256;&#32479;&#19978;&#36825;&#31181;&#30693;&#35782;&#20256;&#25773;&#26159;&#36890;&#36807;&#20010;&#20154;&#20114;&#21160;&#26469;&#23436;&#25104;&#30340;&#65292;&#20294;&#36825;&#31181;&#26041;&#24335;&#32570;&#23569;&#21487;&#20280;&#32553;&#24615;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#19982;&#26102;&#38388;&#12290;&#22522;&#20110;&#20449;&#24687;&#25216;&#26415;&#30340;&#25945;&#23398;&#31995;&#32479;&#35299;&#20915;&#20102;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#30340;&#24320;&#21457;&#20173;&#28982;&#32791;&#36153;&#26102;&#38388;&#19988;&#32321;&#29712;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#32452;&#32455;&#32972;&#26223;&#19979;&#20419;&#36827;&#30693;&#35782;&#20256;&#25773;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;&#22522;&#20110;&#20449;&#24687;&#25216;&#26415;&#30340;&#25945;&#23398;&#31995;&#32479;&#12290;&#36890;&#36807;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#26680;&#24515;&#27010;&#24565;&#12289;&#20027;&#39064;&#21644;&#32500;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35774;&#35745;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25945;&#23398;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22522;&#20110;&#20449;&#24687;&#25216;&#26415;&#30340;&#25945;&#23398;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#24402;&#32435;&#22320;&#20998;&#26512;&#20102;&#30456;&#20851;&#39046;&#22495;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the shortage of skilled workers continues to be a pressing issue, exacerbated by demographic change, it is becoming a critical challenge for organizations to preserve the knowledge of retiring experts and to pass it on to novices. While this knowledge transfer has traditionally taken place through personal interaction, it lacks scalability and requires significant resources and time. IT-based teaching systems have addressed this scalability issue, but their development is still tedious and time-consuming. In this work, we investigate the potential of machine learning (ML) models to facilitate knowledge transfer in an organizational context, leading to more cost-effective IT-based teaching systems. Through a systematic literature review, we examine key concepts, themes, and dimensions to better understand and design ML-based teaching systems. To do so, we capture and consolidate the capabilities of ML models in IT-based teaching systems, inductively analyze relevant concepts in this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#20013;&#30340;&#36895;&#29575;&#22833;&#30495;&#22797;&#26434;&#24230;(RDC)&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RDC&#24863;&#30693;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;RDCAuto&#65292;&#20854;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#21387;&#32553;&#37197;&#32622;&#20197;&#23454;&#29616;&#30446;&#26631;&#36895;&#29575;&#12289;&#22833;&#30495;&#21644;&#32534;&#30721;&#22797;&#26434;&#24230;&#65292;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.07678</link><description>&lt;p&gt;
&#25506;&#32034;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#20013;&#30340;&#36895;&#29575;-&#22833;&#30495;-&#22797;&#26434;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Exploring the Rate-Distortion-Complexity Optimization in Neural Image Compression. (arXiv:2305.07678v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#20013;&#30340;&#36895;&#29575;&#22833;&#30495;&#22797;&#26434;&#24230;(RDC)&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RDC&#24863;&#30693;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;RDCAuto&#65292;&#20854;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#21387;&#32553;&#37197;&#32622;&#20197;&#23454;&#29616;&#30446;&#26631;&#36895;&#29575;&#12289;&#22833;&#30495;&#21644;&#32534;&#30721;&#22797;&#26434;&#24230;&#65292;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#30528;&#30701;&#26242;&#30340;&#21382;&#21490;&#65292;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#22312;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20013;&#30340;&#22823;&#22810;&#25968;&#37117;&#36973;&#21463;&#30528;&#26174;&#33879;&#36739;&#38271;&#30340;&#35299;&#30721;&#26102;&#38388;&#30340;&#22256;&#25200;&#65292;&#36825;&#38459;&#30861;&#20102;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#26377;&#25928;&#20294;&#32791;&#26102;&#30340;&#33258;&#22238;&#24402;&#19978;&#19979;&#25991;&#27169;&#22411;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#26356;&#21152;&#31361;&#20986;&#20102;&#65292;&#22240;&#20026;&#23427;&#20250;&#23558;&#29109;&#35299;&#30721;&#26102;&#38388;&#22686;&#21152;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#20013;&#30340;&#36895;&#29575;&#22833;&#30495;&#22797;&#26434;&#24230;(RDC)&#20248;&#21270;&#65292;&#36890;&#36807;&#23558;&#35299;&#30721;&#22797;&#26434;&#24230;&#37327;&#21270;&#20026;&#20248;&#21270;&#30446;&#26631;&#30340;&#22240;&#32032;&#65292;&#25105;&#20204;&#29616;&#22312;&#33021;&#22815;&#31934;&#30830;&#22320;&#25511;&#21046;RDC&#26435;&#34913;&#65292;&#28982;&#21518;&#35777;&#26126;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#30340;&#36895;&#29575;&#22833;&#30495;&#24615;&#33021;&#22914;&#20309;&#36866;&#24212;&#19981;&#21516;&#30340;&#22797;&#26434;&#24230;&#35201;&#27714;&#12290;&#38500;&#20102;&#30740;&#31350;RDC&#20248;&#21270;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RDC&#24863;&#30693;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;RDCAuto&#65292;&#23427;&#21160;&#24577;&#35843;&#25972;&#20854;&#21387;&#32553;&#37197;&#32622;&#26469;&#23454;&#29616;&#30446;&#26631;&#36895;&#29575;&#12289;&#22833;&#30495;&#21644;&#32534;&#30721;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;RDCAuto&#24182;&#35777;&#26126;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;&#31070;&#32463;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#65292;&#22312;Diverse Image Dataset (DID)&#22522;&#20934;&#27979;&#35797;&#21644;Kodak Lossless True Color Image Suite (Kodak)&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#21387;&#32553;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite a short history, neural image codecs have been shown to surpass classical image codecs in terms of rate-distortion performance. However, most of them suffer from significantly longer decoding times, which hinders the practical applications of neural image codecs. This issue is especially pronounced when employing an effective yet time-consuming autoregressive context model since it would increase entropy decoding time by orders of magnitude. In this paper, unlike most previous works that pursue optimal RD performance while temporally overlooking the coding complexity, we make a systematical investigation on the rate-distortion-complexity (RDC) optimization in neural image compression. By quantifying the decoding complexity as a factor in the optimization goal, we are now able to precisely control the RDC trade-off and then demonstrate how the rate-distortion performance of neural image codecs could adapt to various complexity demands. Going beyond the investigation of RDC optim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Masked Audio Text Encoders&#65288;MATE&#65289;&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#20351;&#29992;MATE&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36827;&#34892;&#22810;&#27169;&#24577;&#25171;&#20998;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#19979;&#23601;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.07677</link><description>&lt;p&gt;
Masked Audio Text Encoders &#22312;&#22810;&#27169;&#24577;&#37325;&#25171;&#20998;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Audio Text Encoders are Effective Multi-Modal Rescorers. (arXiv:2305.07677v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Masked Audio Text Encoders&#65288;MATE&#65289;&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#20351;&#29992;MATE&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36827;&#34892;&#22810;&#27169;&#24577;&#25171;&#20998;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#19979;&#23601;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#20108;&#27425;&#25171;&#20998;&#38750;&#24120;&#26377;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Masked Audio Text Encoder&#65288;MATE&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#26469;&#26377;&#25928;&#22320;&#23545;&#40784;&#21508;&#31181;&#27169;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#37325;&#26032;&#25171;&#20998;&#22120;&#23545;ASR&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#24456;&#26377;&#22909;&#22788;&#12290;&#19982;&#20165;&#25991;&#26412;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#22495;&#20869;&#25968;&#25454;&#32452;&#19978;&#65292;MATE &#21487;&#20197;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;4&#65285;-16&#65285;&#65292;&#22312;&#22495;&#22806;&#25968;&#25454;&#32452;&#19978;&#21487;&#23558;WER&#38477;&#20302;3&#65285;-7&#65285;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;0.8&#23567;&#26102;&#65289;&#65292;MATE&#23601;&#21487;&#20197;&#23558;WER&#27604;&#19968;&#27425;&#25171;&#20998;&#30340;&#22522;&#32447;&#38477;&#20302;8&#65285;-23&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and 3%-7% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours), MATE achieves a WER reduction of 8%-23% over the first-pass baseline.
&lt;/p&gt;</description></item><item><title>LatentPINNs&#26159;&#19968;&#20010;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#30340;&#29289;&#29702;&#23398;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#21387;&#32553;&#34920;&#31034;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#12289;&#26356;&#26377;&#25928;&#22320;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#12290;</title><link>http://arxiv.org/abs/2305.07671</link><description>&lt;p&gt;
LatentPINNs&#65306;&#36890;&#36807;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#30340;&#29289;&#29702;&#23398;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LatentPINNs: Generative physics-informed neural networks via a latent representation learning. (arXiv:2305.07671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07671
&lt;/p&gt;
&lt;p&gt;
LatentPINNs&#26159;&#19968;&#20010;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#30340;&#29289;&#29702;&#23398;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#21387;&#32553;&#34920;&#31034;&#65292;&#21487;&#20197;&#26356;&#24555;&#36895;&#12289;&#26356;&#26377;&#25928;&#22320;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;(PINNs)&#36890;&#36807;&#23545;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#27714;&#35299;&#36827;&#34892;&#29289;&#29702;&#23398;&#32422;&#26463;&#65292;&#25552;&#20379;&#26356;&#21152;&#31934;&#30830;&#21644;&#28789;&#27963;&#30340;PDE&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;PINNs&#30340;&#35757;&#32451;&#36895;&#24230;&#30456;&#23545;&#36739;&#24930;&#19988;&#38656;&#35201;&#23545;&#19981;&#21516;&#30340;PDE&#21442;&#25968;&#36827;&#34892;&#39069;&#22806;&#30340;&#12289;&#21487;&#33021;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;latentPINN&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;PDE&#21442;&#25968;&#28508;&#22312;&#34920;&#31034;&#20316;&#20026;PINN&#38468;&#21152;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#24182;&#20801;&#35768;&#22312;&#36825;&#20123;&#21442;&#25968;&#30340;&#20998;&#24067;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;PDE&#21442;&#25968;&#20998;&#24067;&#30340;&#21387;&#32553;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#21463;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#33021;&#22815;&#31934;&#30830;&#27714;&#35299;PDE&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) are promising to replace conventional partial differential equation (PDE) solvers by offering more accurate and flexible PDE solutions. However, they are hampered by the relatively slow convergence and the need to perform additional, potentially expensive, training for different PDE parameters. To solve this limitation, we introduce latentPINN, a framework that utilizes latent representations of the PDE parameters as additional (to the coordinates) inputs into PINNs and allows for training over the distribution of these parameters. Motivated by the recent progress on generative models, we promote the use of latent diffusion models to learn compressed latent representations of the PDE parameters distribution and act as input parameters to NN functional solutions. We use a two-stage training scheme in which the first stage, we learn the latent representations for the distribution of PDE parameters. In the second stage, we train a physics-informed 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#19981;&#21516;&#32925;&#33039;&#30142;&#30149;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20998;&#26512;&#24615;&#33021;&#24182;&#36890;&#36807;&#20248;&#21270;&#25216;&#26415;&#25214;&#20986;&#26368;&#20339;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07670</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32925;&#33039;&#24863;&#26579;&#39044;&#27979;&#20998;&#26512;&#65306;&#36890;&#36807;&#20248;&#21270;&#25216;&#26415;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#26512;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Liver Infection Prediction Analysis using Machine Learning to Evaluate Analytical Performance in Neural Networks by Optimization Techniques. (arXiv:2305.07670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#19981;&#21516;&#32925;&#33039;&#30142;&#30149;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20998;&#26512;&#24615;&#33021;&#24182;&#36890;&#36807;&#20248;&#21270;&#25216;&#26415;&#25214;&#20986;&#26368;&#20339;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32925;&#33039;&#24863;&#26579;&#26159;&#19968;&#31181;&#24120;&#35265;&#30142;&#30149;&#65292;&#23545;&#20154;&#31867;&#20581;&#24247;&#26500;&#25104;&#20102;&#24040;&#22823;&#23041;&#32961;&#65292;&#20294;&#30446;&#21069;&#36824;&#33021;&#22815;&#35782;&#21035;&#19968;&#31181;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#31579;&#26597;&#30340;&#26368;&#20339;&#25216;&#26415;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#39044;&#27979;&#20998;&#26512;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#22240;&#27492;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#29992;&#20110;&#19981;&#21516;&#30142;&#30149;&#65292;&#20197;&#25972;&#21512;&#21487;&#35270;&#21270;&#27169;&#24335;&#12290;&#26412;&#25991;&#20351;&#29992;&#19981;&#21516;&#30340;&#32925;&#33039;&#30142;&#30149;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#21442;&#25968;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#20998;&#26512;&#24615;&#33021;&#12290;&#36873;&#25321;&#30340;&#20998;&#31867;&#31639;&#27861;&#20998;&#26512;&#32467;&#26524;&#30340;&#24046;&#24322;&#65292;&#24182;&#25214;&#20986;&#26368;&#20248;&#30340;&#32925;&#30149;&#20998;&#31867;&#27169;&#22411;&#12290;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#26159;&#36890;&#36807;&#20462;&#25913;&#36229;&#21442;&#25968;&#26469;&#37319;&#29992;&#20248;&#21270;&#26041;&#27861;&#20043;&#19968;&#26469;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968;&#30340;&#36807;&#31243;&#12290;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#21253;&#25324;&#30967;&#37240;&#37238;&#65292;&#30452;&#25509;&#32966;&#32418;&#32032;&#65292;&#34507;&#30333;&#36136;&#65292;&#30333;&#34507;&#30333;&#21644;&#29699;&#34507;&#30333;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Liver infection is a common disease, which poses a great threat to human health, but there is still able to identify an optimal technique that can be used on large-level screening. This paper deals with ML algorithms using different data sets and predictive analyses. Therefore, machine ML can be utilized in different diseases for integrating a piece of pattern for visualization. This paper deals with various machine learning algorithms on different liver illness datasets to evaluate the analytical performance using different types of parameters and optimization techniques. The selected classification algorithms analyze the difference in results and find out the most excellent categorization models for liver disease. Machine learning optimization is the procedure of modifying hyperparameters in arrange to employ one of the optimization approaches to minimise the cost function. To set the hyperparameter, include a number of Phosphotase,Direct Billirubin, Protiens, Albumin and Albumin Glo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.07663</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23450;&#37327;&#35821;&#20041;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Quantified Semantic Comparison of Convolutional Neural Networks. (arXiv:2305.07663v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24212;&#29992;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#24037;&#20316;&#21407;&#29702;&#21364;&#24456;&#38590;&#38416;&#26126;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36825;&#31867;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#27169;&#22411;&#36873;&#25321;&#36824;&#24212;&#32771;&#34385;&#20505;&#36873;&#27169;&#22411;&#22312;&#27169;&#22411;&#36879;&#26126;&#24615;&#26041;&#38754;&#22914;&#20309;&#34920;&#31034;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;CNN&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#33879;&#21517;&#25216;&#26415;&#20316;&#20026;&#22522;&#30784;&#65292;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#33719;&#24471;&#27599;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#27010;&#24565;&#30340;&#20840;&#23616;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#22312;&#27979;&#35797;&#36755;&#20837;&#19978;&#30340;&#28608;&#27963;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#24037;&#20316;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#20004;&#20010;&#19981;&#21516;&#33539;&#22260;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state-of-the-art in convolutional neural networks (CNNs) for computer vision excels in performance, while remaining opaque. But due to safety regulations for safety-critical applications, like perception for automated driving, the choice of model should also take into account how candidate models represent semantic information for model transparency reasons. To tackle this yet unsolved problem, our work proposes two methods for quantifying the similarity between semantic information in CNN latent spaces. These allow insights into both the flow and similarity of semantic information within CNN layers, and into the degree of their similitude between different networks. As a basis, we use renown techniques from the field of explainable artificial intelligence (XAI), which are used to obtain global vector representations of semantic concepts in each latent space. These are compared with respect to their activation on test inputs. When applied to three diverse object detectors and two d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#20449;&#24687;&#22495;&#30340;&#31070;&#32463;CSI&#21387;&#32553;&#19982;&#29305;&#24449;&#32806;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#20449;&#24687;&#20316;&#20026;CSI&#30340;&#34920;&#24449;&#65292;&#22312;&#26032;&#23450;&#20041;&#30340;&#33258;&#20449;&#24687;&#22495;&#20013;&#25552;&#21462;&#26102;&#38388;&#21644;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#26377;&#25928;&#21387;&#32553;&#65292;&#24182;&#22312;&#21387;&#32553;CSI&#21453;&#39304;&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;3.22dB&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.07662</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#20449;&#24687;&#22495;&#30340;&#31070;&#32463;CSI&#21387;&#32553;&#19982;&#29305;&#24449;&#32806;&#21512;
&lt;/p&gt;
&lt;p&gt;
Self-information Domain-based Neural CSI Compression with Feature Coupling. (arXiv:2305.07662v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#20449;&#24687;&#22495;&#30340;&#31070;&#32463;CSI&#21387;&#32553;&#19982;&#29305;&#24449;&#32806;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#20449;&#24687;&#20316;&#20026;CSI&#30340;&#34920;&#24449;&#65292;&#22312;&#26032;&#23450;&#20041;&#30340;&#33258;&#20449;&#24687;&#22495;&#20013;&#25552;&#21462;&#26102;&#38388;&#21644;&#31354;&#38388;&#29305;&#24449;&#36827;&#34892;&#26377;&#25928;&#21387;&#32553;&#65292;&#24182;&#22312;&#21387;&#32553;CSI&#21453;&#39304;&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;3.22dB&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#21453;&#39304;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#24310;&#36831;&#21644;&#35282;&#24230;&#29305;&#24449;&#30452;&#25509;&#21387;&#32553;CSI&#30697;&#38453;&#65292;&#20294;&#24456;&#23569;&#32771;&#34385;CSI&#30697;&#38453;&#20013;&#20449;&#24687;&#30340;&#37327;&#24230;&#12290;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#35282;&#24230;&#24341;&#20837;&#33258;&#20449;&#24687;&#20316;&#20026;CSI&#30340;&#34920;&#24449;&#65292;&#23427;&#20197;&#26126;&#30830;&#30340;&#26041;&#24335;&#21453;&#26144;&#20102;&#21407;&#22987;CSI&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#37327;&#12290;&#28982;&#21518;&#65292;&#22312;&#33258;&#20449;&#24687;&#22495;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DL-based&#32593;&#32476;SD-CsiNet&#65292;&#29992;&#20110;&#20020;&#26102;CSI&#21387;&#32553;&#12290;SD-CsiNet&#23558;&#21407;&#22987;CSI&#25237;&#24433;&#21040;&#33258;&#20449;&#24687;&#30697;&#38453;&#19978;&#65292;&#22312;&#26032;&#23450;&#20041;&#30340;&#33258;&#20449;&#24687;&#22495;&#20013;&#25552;&#21462;&#33258;&#20449;&#24687;&#30697;&#38453;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#29305;&#24449;&#65292;&#28982;&#21518;&#32806;&#21512;&#36825;&#20004;&#31181;&#29305;&#24449;&#36827;&#34892;&#26377;&#25928;&#21387;&#32553;&#12290;&#23454;&#39564;&#32467;&#26524;&#36890;&#36807;&#21033;&#29992;CSI&#30340;&#33258;&#20449;&#24687;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#21387;&#32553;CSI&#21453;&#39304;&#65292;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#21487;&#20197;&#36798;&#21040;&#39640;&#36798;3.22dB&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL)-based channel state information (CSI) feedback methods compressed the CSI matrix by exploiting its delay and angle features straightforwardly, while the measure in terms of information contained in the CSI matrix has rarely been considered. Based on this observation, we introduce self-information as an informative CSI representation from the perspective of information theory, which reflects the amount of information of the original CSI matrix in an explicit way. Then, a novel DL-based network is proposed for temporal CSI compression in the self-information domain, namely SD-CsiNet. The proposed SD-CsiNet projects the raw CSI onto a self-information matrix in the newly-defined self-information domain, extracts both temporal and spatial features of the self-information matrix, and then couples these two features for effective compression. Experimental results verify the effectiveness of the proposed SD-CsiNet by exploiting the self-information of CSI. Particularly for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07598</link><description>&lt;p&gt;
RHINO&#65306;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#30340;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#30340;&#26059;&#36716;DETR
&lt;/p&gt;
&lt;p&gt;
RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection. (arXiv:2305.07598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;DINO&#30340;&#21457;&#24067;&#65292;&#19968;&#31181;DETR&#30340;&#21464;&#20307;&#65292;&#26816;&#27979;&#21464;&#21387;&#22120;&#27491;&#22312;&#36890;&#36807;&#20854;&#31471;&#21040;&#31471;&#35774;&#35745;&#21644;&#21487;&#25193;&#23637;&#24615;&#22312;&#30446;&#26631;&#26816;&#27979;&#22522;&#20934;&#20013;&#21047;&#26032;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#39044;&#35745;&#20174;&#20854;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#33719;&#24471;&#26356;&#22810;&#30340;&#22909;&#22788;&#65292;&#22914;&#28040;&#38500;NMS&#21644;&#19982;&#38170;&#30456;&#20851;&#30340;&#25104;&#26412;&#65292;&#20294;&#23578;&#26410;&#24443;&#24213;&#30740;&#31350;DETR&#22312;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#30340;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20351;&#29992;DETR&#36827;&#34892;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#24182;&#19981;&#33021;&#20445;&#35777;&#19981;&#37325;&#22797;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25104;&#26412;&#26469;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#21256;&#29273;&#21033;&#21305;&#37197;&#26469;&#36807;&#28388;&#20887;&#20313;&#30340;&#24102;&#22122;&#22768;&#30340;&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#26597;&#35810;&#23545;&#40784;&#26469;&#20445;&#25345;Transformer&#35299;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#26059;&#36716;DETR&#21644;&#20854;&#20182;&#23545;&#25163;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a novel denoising strategy that uses Hungarian matching to filter redundant noised queries and query alignment to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X&#65292;&#24182;&#35777;&#26126;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#23548;&#33268;&#20102;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2305.07437</link><description>&lt;p&gt;
&#24102;&#26377;&#38750;&#23545;&#35282;&#20449;&#24687;&#30340;&#35270;&#35273;-&#35821;&#35328;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Vision-Language Representaion Learning with Off-Diagonal Information. (arXiv:2305.07437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X&#65292;&#24182;&#35777;&#26126;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#23548;&#33268;&#20102;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36890;&#36807;&#27969;&#25968;&#25454;&#25345;&#32493;&#35757;&#32451;CLIP&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#36861;&#36394;&#36830;&#32493;&#26356;&#26032;&#30340;CLIP&#27169;&#22411;&#20013;&#34920;&#31034;&#21521;&#37327;&#30340;&#26041;&#21521;&#21464;&#21270;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#24635;&#32467;&#20102;&#36825;&#20123;&#31354;&#38388;&#21464;&#21270;&#65292;&#31216;&#20026;&#31354;&#38388;&#28151;&#20081;&#65288;SD&#65289;&#65292;&#21487;&#20197;&#20998;&#20026;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20869;&#37096;&#26059;&#36716;&#21644;&#36328;&#27169;&#24577;&#20559;&#24046;&#22914;&#20309;&#23548;&#33268;CLIP&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#31354;&#38388;&#28151;&#20081;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;Mod-X: &#32500;&#25252;&#38750;&#23545;&#35282;&#20449;&#24687;&#30697;&#38453;&#12290;&#22312;&#21508;&#31181;&#19981;&#21516;&#35268;&#27169;&#21644;&#33539;&#22260;&#30340;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the feasibility of continuously training the CLIP model through streaming data. Then, by tracking the directional changes of the representation vectors in the continuously updated CLIP model, we explore and summarize these spatial variations as Spatial Disorder (SD), which can be divided into Intra-modal Rotation and Inter-modal Deviation. Moreover, we demonstrate how intra-modal rotation and inter-modal deviation lead to a performance decline for CLIP on cross-modal retrieval tasks in both empirically and theoretically. To alleviate the spatial disorder, we propose a simple yet effective continual learning framework Mod-X: Maintain off-diagonal information-matriX. The experiments (in Section \ref{method}, \ref{experiments} and Appendix \ref{Appendix_to_experiments}) on commonly used datasets with different scales and scopes have illustrated the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20132;&#21449;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#21644;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#19982;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06969</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20132;&#21449;&#20844;&#24179;&#24615;&#35843;&#26597;&#65306;&#27010;&#24565;&#12289;&#32531;&#35299;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Intersectional Fairness in Machine Learning: Notions, Mitigation, and Challenges. (arXiv:2305.06969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20132;&#21449;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#21644;&#32531;&#35299;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#19982;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#20026;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#22914;&#21009;&#20107;&#21028;&#20915;&#21644;&#38134;&#34892;&#36151;&#27454;&#65292;&#24341;&#21457;&#20102;&#23545;&#20844;&#24179;&#24615;&#30340;&#26356;&#22810;&#20851;&#27880;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#31639;&#27861;&#21644;&#25351;&#26631;&#26469;&#32531;&#35299;&#21644;&#34913;&#37327;&#36825;&#20123;&#27495;&#35270;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20559;&#35265;&#24418;&#24335;&#65292;&#31216;&#20026;&#20132;&#21449;&#20559;&#35265;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#65292;&#20363;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#20132;&#21449;&#20844;&#24179;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#21449;&#20844;&#24179;&#24615;&#21644;&#32531;&#35299;&#30340;&#20998;&#31867;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25351;&#23548;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of Machine Learning systems, especially in more decision-critical applications such as criminal sentencing and bank loans, has led to increased concerns about fairness implications. Algorithms and metrics have been developed to mitigate and measure these discriminations. More recently, works have identified a more challenging form of bias called intersectional bias, which encompasses multiple sensitive attributes, such as race and gender, together. In this survey, we review the state-of-the-art in intersectional fairness. We present a taxonomy for intersectional notions of fairness and mitigation. Finally, we identify the key challenges and provide researchers with guidelines for future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#25919;&#31574;&#32454;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23450;&#29702;&#39564;&#35777;&#20854;&#25910;&#25947;&#24615;&#12289;&#40065;&#26834;&#24615;&#30028;&#38480;&#12289;&#27867;&#21270;&#35823;&#24046;&#21644;&#23545;&#27169;&#22411;&#38169;&#35823;&#30340;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06796</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#25919;&#31574;&#32454;&#21270;&#30340;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Theoretical Understanding of Data-Driven Policy Refinement. (arXiv:2305.06796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#25919;&#31574;&#32454;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23450;&#29702;&#39564;&#35777;&#20854;&#25910;&#25947;&#24615;&#12289;&#40065;&#26834;&#24615;&#30028;&#38480;&#12289;&#27867;&#21270;&#35823;&#24046;&#21644;&#23545;&#27169;&#22411;&#38169;&#35823;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#25919;&#31574;&#32454;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#26469;&#22686;&#24378;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#21644;&#20248;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#36825;&#20010;&#25968;&#25454;&#39537;&#21160;&#25919;&#31574;&#32454;&#21270;&#27010;&#24565;&#30340;&#25968;&#23398;&#34920;&#36848;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25968;&#25454;&#39537;&#21160;&#39564;&#35777;&#20013;&#28014;&#29616;&#30340;&#21453;&#20363;&#23398;&#20064;&#65292;&#31995;&#32479;&#22320;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23450;&#29702;&#65292;&#38416;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#29702;&#35770;&#24615;&#36136;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#12289;&#40065;&#26834;&#24615;&#30028;&#38480;&#12289;&#27867;&#21270;&#35823;&#24046;&#21644;&#23545;&#27169;&#22411;&#38169;&#35823;&#30340;&#24377;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#19981;&#20165;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19988;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#36825;&#20010;&#26041;&#27861;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach for data-driven policy refinement in reinforcement learning, specifically designed for safety-critical applications. Our methodology leverages the strengths of data-driven optimization and reinforcement learning to enhance policy safety and optimality through iterative refinement. Our principal contribution lies in the mathematical formulation of this data-driven policy refinement concept. This framework systematically improves reinforcement learning policies by learning from counterexamples surfaced during data-driven verification. Furthermore, we present a series of theorems elucidating key theoretical properties of our approach, including convergence, robustness bounds, generalization error, and resilience to model mismatch. These results not only validate the effectiveness of our methodology but also contribute to a deeper understanding of its behavior in different environments and scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.06784</link><description>&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#28040;&#36153;&#32773;&#30340;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning. (arXiv:2305.06784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25293;&#21334;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;AFL&#65289;&#22240;&#36890;&#36807;&#32463;&#27982;&#25163;&#27573;&#28608;&#21169;&#25968;&#25454;&#25317;&#26377;&#32773;&#21152;&#20837;FL&#32780;&#21463;&#21040;&#24191;&#27867;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#29616;&#26377;&#24037;&#20316;&#20551;&#35774;&#22312;AFL&#24066;&#22330;&#19978;&#20165;&#23384;&#22312;&#19968;&#20010;&#25968;&#25454;&#28040;&#36153;&#32773;&#21644;&#22810;&#20010;&#25968;&#25454;&#25317;&#26377;&#32773;&#65288;&#21363;&#22404;&#26029;&#24066;&#22330;&#65289;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#25317;&#26377;&#32773;&#31454;&#26631;&#21152;&#20837;&#25968;&#25454;&#28040;&#36153;&#32773;&#36827;&#34892;FL&#12290;&#20294;&#26159;&#65292;&#22312;&#23454;&#38469;&#30340;AFL&#24066;&#22330;&#20013;&#65292;&#22810;&#20010;&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#33021;&#20250;&#31454;&#20105;&#20197;&#21560;&#24341;&#25968;&#25454;&#25317;&#26377;&#32773;&#21152;&#20837;&#20182;&#20204;&#21508;&#33258;&#30340;FL&#20219;&#21153;&#65292;&#36825;&#31181;&#20551;&#35774;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#31454;&#26631;&#31574;&#30053;&#65288;Fed-Bidder&#65289;&#65292;&#20351;&#22810;&#20010;FL&#25968;&#25454;&#28040;&#36153;&#32773;&#21487;&#20197;&#36890;&#36807;AFL&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#31454;&#20105;&#25968;&#25454;&#25317;&#26377;&#32773;&#65292;&#24182;&#25552;&#20379;&#20102;&#33021;&#22815;&#23481;&#32435;&#19981;&#21516;&#24066;&#22330;&#21160;&#24577;&#30340;&#21508;&#31181;&#33719;&#32988;&#20989;&#25968;&#30340;&#25928;&#29992;&#20272;&#35745;&#33021;&#21147;&#12290;&#22522;&#20110;&#20845;&#20010;&#24120;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20102;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auction-based Federated Learning (AFL) has attracted extensive research interest due to its ability to motivate data owners to join FL through economic means. Existing works assume that only one data consumer and multiple data owners exist in an AFL marketplace (i.e., a monopoly market). Therefore, data owners bid to join the data consumer for FL. However, this assumption is not realistic in practical AFL marketplaces in which multiple data consumers can compete to attract data owners to join their respective FL tasks. In this paper, we bridge this gap by proposing a first-of-its-kind utility-maximizing bidding strategy for data consumers in federated learning (Fed-Bidder). It enables multiple FL data consumers to compete for data owners via AFL effectively and efficiently by providing with utility estimation capabilities which can accommodate diverse forms of winning functions, each reflecting different market dynamics. Extensive experiments based on six commonly adopted benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;</title><link>http://arxiv.org/abs/2305.06657</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65306;&#30456;&#37051;&#19981;&#30830;&#23450;&#24615;&#38598;&#21644;&#21452;&#20195;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On practical robust reinforcement learning: adjacent uncertainty set and double-agent algorithm. (arXiv:2305.06657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ARQ-Learning&#30340;&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#19968;&#20010;&#26356;&#21152;&#23454;&#38469;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#32780;&#19988;&#65292;&#26412;&#25991;&#36824;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;PR-DQN&#21644;PR-DDPG&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#19978;&#20248;&#21270;&#26368;&#24046;&#24615;&#33021;&#12290;&#32473;&#23450;&#19968;&#20010;&#20135;&#29983;&#35757;&#32451;&#26679;&#26412;&#30340;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;N-MDP&#65289;&#65292;&#35813;&#38598;&#21512;&#21253;&#21547;&#36890;&#36807;&#23545;N-MDP&#36827;&#34892;&#26576;&#20123;&#25200;&#21160;&#32780;&#33719;&#24471;&#30340;MDP&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#27604;&#29616;&#26377;&#38598;&#21512;&#26356;&#23454;&#38469;&#30340;MDP&#12290;&#20351;&#29992;&#36825;&#20010;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#40065;&#26834;RL&#31639;&#27861;&#65292;&#21517;&#20026;ARQ-Learning&#65292;&#29992;&#20110;&#34920;&#26684;&#21270;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#35823;&#24046;&#30028;&#24182;&#35777;&#26126;&#23427;&#19982;Q-Learning&#21644;&#40065;&#26834;Q-Learning&#65288;&#21363;&#29616;&#26377;&#30340;&#40065;&#26834;RL&#26041;&#27861;&#65289;&#19968;&#26679;&#24555;&#22320;&#25910;&#25947;&#65292;&#21516;&#26102;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24754;&#35266;&#20195;&#29702;&#8221;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23558;ARQ-Learning&#25193;&#23637;&#21040;&#22823;&#22411;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#30340;&#20851;&#38190;&#29942;&#39048;&#12290;&#21033;&#29992;&#36825;&#19968;&#25216;&#26415;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;PRQ-Learning&#12290;&#25509;&#30528;&#65292;&#23558;&#20854;&#19982;DQN&#21644;DDPG&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#20998;&#21035;&#24320;&#21457;&#20102;PR-DQN&#21644;PR-DDPG&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#28145;&#24230;&#30830;&#23450;&#31574;&#30053;&#26799;&#24230;&#30340;&#40065;&#26834;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#39046;&#22495;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust reinforcement learning (RL) aims at learning a policy that optimizes the worst-case performance over an uncertainty set. Given nominal Markov decision process (N-MDP) that generates samples for training, the set contains MDPs obtained by some perturbations from N-MDP. In this paper, we introduce a new uncertainty set containing more realistic MDPs in practice than the existing sets. Using this uncertainty set, we present a robust RL, named ARQ-Learning, for tabular cases. Also, we characterize the finite-time error bounds and prove that it converges as fast as Q-Learning and robust Q-Learning (i.e., the state-of-the-art robust RL method) while providing better robustness for real applications. We propose {\em pessimistic agent} that efficiently tackles the key bottleneck for the extension of ARQ-Learning into large or continuous state spaces. Using this technique, we first propose PRQ-Learning. To the next, combining this with DQN and DDPG, we develop PR-DQN and PR-DDPG, respect
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HAHE&#27169;&#22411;&#65292;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#23398;&#20064;&#20102;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#39034;&#24207;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06588</link><description>&lt;p&gt;
HAHE: &#22522;&#20110;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#20998;&#23618;&#27880;&#24847;&#21147;&#27169;&#22411;&#29992;&#20110;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level. (arXiv:2305.06588v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06588
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HAHE&#27169;&#22411;&#65292;&#20351;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#23398;&#20064;&#20102;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#39034;&#24207;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#26159;&#20540;&#24471;&#23581;&#35797;&#30340;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#8212;&#8212;HAHE&#65292;&#21253;&#25324;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#34920;&#31034;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#37319;&#29992;&#36229;&#22270;&#21452;&#37325;&#27880;&#24847;&#21147;&#23618;&#65292;&#20840;&#23616;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21487;&#20197;&#24314;&#27169;&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;&#35889;&#30340;&#22270;&#24418;&#32467;&#26500;&#65307;&#32780;&#37319;&#29992;&#24322;&#36136;&#24615;&#33258;&#27880;&#24847;&#23618;&#65292;&#23616;&#37096;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21017;&#21487;&#20197;&#23398;&#20064;H-Facts&#20869;&#37096;&#30340;&#39034;&#24207;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HAHE&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs' representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06360</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#36951;&#24536;&#30340;&#39046;&#22495;&#65306;&#19968;&#31687;&#32508;&#36848;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#38656;&#35201;&#21024;&#38500;&#25110;&#20462;&#25913;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#21644;&#20934;&#30830;&#65292;&#20294;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#65289;&#65292;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#33879;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25915;&#20987;&#22797;&#26434;&#24615;&#12289;&#26631;&#20934;&#21270;&#12289;&#21487;&#36716;&#31227;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#35752;&#35770;MU&#30340;&#28508;&#22312;&#30410;&#22788;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#22788;&#29702;&#23454;&#20363;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#24456;&#22909;&#22320;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#28508;&#22312;&#22240;&#32032;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06247</link><description>&lt;p&gt;
&#20851;&#20110;&#23454;&#20363;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#20215;&#20540;&#30340;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Value of Labels for Instance-Dependent Label Noise Learning. (arXiv:2305.06247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#22788;&#29702;&#23454;&#20363;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#24456;&#22909;&#22320;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#28508;&#22312;&#22240;&#32032;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#65292;&#26631;&#31614;&#22122;&#22768;&#26222;&#36941;&#23384;&#22312;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#23454;&#20363;&#30456;&#20851;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#30340;&#19981;&#21487;&#35782;&#21035;&#24615;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#31639;&#27861;&#36890;&#36807;&#20551;&#35774;&#22122;&#22768;&#26631;&#31614;&#29983;&#25104;&#36807;&#31243;&#19982;&#23454;&#20363;&#29305;&#24449;&#26080;&#20851;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#22122;&#22768;&#26631;&#31614;&#36890;&#24120;&#21462;&#20915;&#20110;&#30495;&#23454;&#26631;&#31614;&#21644;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#22788;&#29702;&#23454;&#20363;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#36991;&#20813;&#20102;&#26126;&#30830;&#24314;&#27169;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#65292;&#21516;&#26102;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#28508;&#22312;&#22240;&#32032;&#12290;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#30340;&#22122;&#22768;&#26631;&#31614;&#30417;&#30563;&#20449;&#24687;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#20363;&#30456;&#20851;&#26631;&#31614;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26174;&#33879;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label noise widely exists in large-scale datasets and significantly degenerates the performances of deep learning algorithms. Due to the non-identifiability of the instance-dependent noise transition matrix, most existing algorithms address the problem by assuming the noisy label generation process to be independent of the instance features. Unfortunately, noisy labels in real-world applications often depend on both the true label and the features. In this work, we tackle instance-dependent label noise with a novel deep generative model that avoids explicitly modeling the noise transition matrix. Our algorithm leverages casual representation learning and simultaneously identifies the high-level content and style latent factors from the data. By exploiting the supervision information of noisy labels with structural causal models, our empirical evaluations on a wide range of synthetic and real-world instance-dependent label noise datasets demonstrate that the proposed algorithm significa
&lt;/p&gt;</description></item><item><title>&#34917;&#19969;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06217</link><description>&lt;p&gt;
&#34917;&#19969;&#23398;&#20064;&#65306;&#23454;&#29616;&#36328;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#28304;&#30340;&#32508;&#21512;&#20998;&#26512;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources. (arXiv:2305.06217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06217
&lt;/p&gt;
&lt;p&gt;
&#34917;&#19969;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25552;&#20379;&#20102;&#35768;&#22810;&#22686;&#24378;&#24739;&#32773;&#25252;&#29702;&#12289;&#20154;&#21475;&#20581;&#24247;&#21644;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#24037;&#20316;&#27969;&#31243;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#65292;&#29616;&#23454;&#20013;&#30340;&#20020;&#24202;&#21644;&#25104;&#26412;&#25928;&#30410;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#36825;&#31687;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;&#34917;&#19969;&#23398;&#20064;&#8221;&#65288;PL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#38598;&#25104;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#65288;&#20363;&#22914;&#65292;&#20020;&#24202;&#20813;&#36153;&#25991;&#26412;&#12289;&#21307;&#23398;&#22270;&#20687;&#12289;&#32452;&#23398;&#65289;&#21644;&#20998;&#24067;&#22312;&#19981;&#21516;&#23433;&#20840;&#31449;&#28857;&#19978;&#30340;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;PL&#20801;&#35768;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#34917;&#19969;&#23398;&#20064;&#30340;&#27010;&#24565;&#20197;&#21450;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24403;&#21069;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#30340;&#28508;&#22312;&#26426;&#20250;&#21644;&#36866;&#29992;&#25968;&#25454;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) in healthcare presents numerous opportunities for enhancing patient care, population health, and healthcare providers' workflows. However, the real-world clinical and cost benefits remain limited due to challenges in data privacy, heterogeneous data sources, and the inability to fully leverage multiple data modalities. In this perspective paper, we introduce "patchwork learning" (PL), a novel paradigm that addresses these limitations by integrating information from disparate datasets composed of different data modalities (e.g., clinical free-text, medical images, omics) and distributed across separate and secure sites. PL allows the simultaneous utilization of complementary data sources while preserving data privacy, enabling the development of more holistic and generalizable ML models. We present the concept of patchwork learning and its current implementations in healthcare, exploring the potential opportunities and applicable data sources for addressing various
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#30452;&#25509;&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#28779;&#28798;&#20256;&#25773;&#27010;&#29575;&#65292;&#30465;&#21435;&#20102;&#32321;&#37325;&#30340;&#27169;&#25311;&#38598;&#21512;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23545;&#28779;&#28798;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.06139</link><description>&lt;p&gt;
&#28779;&#28798;&#20256;&#25773;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#31070;&#32463;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Neural Emulator for Uncertainty Estimation of Fire Propagation. (arXiv:2305.06139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#30452;&#25509;&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#28779;&#28798;&#20256;&#25773;&#27010;&#29575;&#65292;&#30465;&#21435;&#20102;&#32321;&#37325;&#30340;&#27169;&#25311;&#38598;&#21512;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23545;&#28779;&#28798;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26862;&#26519;&#28779;&#28798;&#30340;&#20256;&#25773;&#26159;&#19968;&#20010;&#39640;&#24230;&#38543;&#26426;&#30340;&#36807;&#31243;&#65292;&#29615;&#22659;&#26465;&#20214;&#30340;&#24494;&#23567;&#21464;&#21270;&#65288;&#20363;&#22914;&#39118;&#36895;&#21644;&#26041;&#21521;&#65289;&#20250;&#23548;&#33268;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#21457;&#29983;&#24040;&#22823;&#21464;&#21270;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#38598;&#21512;&#29983;&#25104;&#27010;&#29575;&#22270;&#26469;&#37327;&#21270;&#28779;&#32447;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20294;&#20351;&#29992;&#38598;&#21512;&#36890;&#24120;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#20250;&#38480;&#21046;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#30340;&#33539;&#22260;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26041;&#27861;&#65292;&#30452;&#25509;&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#28779;&#28798;&#20256;&#25773;&#27010;&#29575;&#12290;&#36890;&#36807;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#24847;&#35797;&#22270;&#25200;&#21160;&#36755;&#20837;&#30340;&#22825;&#27668;&#39044;&#25253;&#26469;&#34920;&#31034;&#19981;&#30830;&#23450;&#24615;&#12290;&#35745;&#31639;&#36127;&#36733;&#38598;&#20013;&#20110;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#20801;&#35768;&#22312;&#37096;&#32626;&#26399;&#38388;&#25506;&#32034;&#26356;&#22823;&#30340;&#27010;&#29575;&#31354;&#38388;&#12290;&#32463;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#19982;&#20256;&#32479; S*t&#27169;&#25311;&#25152;&#20135;&#29983;&#30340;&#30456;&#24403;&#28779;&#28798;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wildfire propagation is a highly stochastic process where small changes in environmental conditions (such as wind speed and direction) can lead to large changes in observed behaviour. A traditional approach to quantify uncertainty in fire-front progression is to generate probability maps via ensembles of simulations. However, use of ensembles is typically computationally expensive, which can limit the scope of uncertainty analysis. To address this, we explore the use of a spatio-temporal neural-based modelling approach to directly estimate the likelihood of fire propagation given uncertainty in input parameters. The uncertainty is represented by deliberately perturbing the input weather forecast during model training. The computational load is concentrated in the model training process, which allows larger probability spaces to be explored during deployment. Empirical evaluations indicate that the proposed model achieves comparable fire boundaries to those produced by the traditional S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35748;&#20026;&#22806;&#37096;&#39564;&#35777;&#26080;&#27861;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25110;&#23454;&#29992;&#24615;&#65292;&#25552;&#20986;&#20102;&#24490;&#29615;&#26412;&#22320;&#39564;&#35777;&#30340;MLOps&#21551;&#21457;&#24335;&#33539;&#24335;&#20316;&#20026;&#26032;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#24378;&#35843;&#23545;&#21508;&#20010;&#26412;&#22320;&#37096;&#32626;&#30340;&#27169;&#22411;&#36827;&#34892;&#30417;&#27979;&#21644;&#26356;&#26032;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23545;&#40784;&#20020;&#24202;&#21644;&#21307;&#30103;&#29305;&#23450;&#38656;&#27714;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39564;&#35777;&#31574;&#30053;&#65292;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03219</link><description>&lt;p&gt;
&#25152;&#26377;&#30340;&#27169;&#22411;&#37117;&#26159;&#23616;&#37096;&#30340;: &#29992;&#24490;&#29615;&#26412;&#22320;&#39564;&#35777;&#21462;&#20195;&#22806;&#37096;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
All models are local: time to replace external validation with recurrent local validation. (arXiv:2305.03219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35748;&#20026;&#22806;&#37096;&#39564;&#35777;&#26080;&#27861;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25110;&#23454;&#29992;&#24615;&#65292;&#25552;&#20986;&#20102;&#24490;&#29615;&#26412;&#22320;&#39564;&#35777;&#30340;MLOps&#21551;&#21457;&#24335;&#33539;&#24335;&#20316;&#20026;&#26032;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#24378;&#35843;&#23545;&#21508;&#20010;&#26412;&#22320;&#37096;&#32626;&#30340;&#27169;&#22411;&#36827;&#34892;&#30417;&#27979;&#21644;&#26356;&#26032;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23545;&#40784;&#20020;&#24202;&#21644;&#21307;&#30103;&#29305;&#23450;&#38656;&#27714;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39564;&#35777;&#31574;&#30053;&#65292;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#39564;&#35777;&#32463;&#24120;&#34987;&#25512;&#33616;&#29992;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#26082;&#19981;&#33021;&#20445;&#35777;&#27867;&#21270;&#33021;&#21147;&#65292;&#20063;&#19981;&#33021;&#31561;&#20215;&#20110;&#27169;&#22411;&#30340;&#20020;&#24202;&#23454;&#29992;&#24615;&#65288;&#20219;&#20309;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;&#26368;&#32456;&#30446;&#26631;&#65289;&#12290;&#22806;&#37096;&#39564;&#35777;&#19982;&#24403;&#21069;&#21307;&#30103;&#20445;&#20581;&#26426;&#22120;&#23398;&#20064;&#30340;&#38656;&#35201;&#19981;&#19968;&#33268;&#12290;&#20854;&#27425;&#65292;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12289;&#24403;&#21069;&#30340;&#24066;&#22330;&#21147;&#37327;&#21644;&#26356;&#26032;&#30340;&#30417;&#31649;&#26694;&#26550;&#27491;&#22312;&#20419;&#36827;&#23545;&#20010;&#20307;&#37096;&#32626;&#30340;&#27169;&#22411;&#23454;&#20363;&#30340;&#39057;&#32321;&#26356;&#26032;&#21644;&#30417;&#25511;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22806;&#37096;&#39564;&#35777;&#19981;&#36275;&#20197;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25110;&#23454;&#29992;&#24615;&#12290;&#20462;&#22797;&#22806;&#37096;&#39564;&#35777;&#33539;&#24335;&#30340;&#24314;&#35758;&#19981;&#22815;&#24443;&#24213;&#12290;&#32487;&#32493;&#20381;&#36182;&#23427;&#20316;&#20026;&#26368;&#32456;&#27979;&#35797;&#24456;&#21487;&#33021;&#20250;&#20351;&#25105;&#20204;&#36208;&#19978;&#38169;&#35823;&#36947;&#36335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; MLOps &#21551;&#21457;&#24335;&#33539;&#24335;&#30340;&#24490;&#29615;&#26412;&#22320;&#39564;&#35777;&#20316;&#20026;&#26032;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#24378;&#35843;&#30417;&#27979;&#21644;&#26356;&#26032;&#21508;&#20010;&#26412;&#22320;&#37096;&#32626;&#30340;&#27169;&#22411;&#12290;&#37319;&#29992;&#36825;&#31181;&#33539;&#24335;&#23558;&#26356;&#22909;&#22320;&#23545;&#40784;&#20020;&#24202;&#21644;&#21307;&#30103;&#29305;&#23450;&#38656;&#27714;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39564;&#35777;&#31574;&#30053;&#65292;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
External validation is often recommended to ensure the generalizability of ML models. However, it neither guarantees generalizability nor equates to a model's clinical usefulness (the ultimate goal of any clinical decision-support tool). External validation is misaligned with current healthcare ML needs. First, patient data changes across time, geography, and facilities. These changes create significant volatility in the performance of a single fixed model (especially for deep learning models, which dominate clinical ML). Second, newer ML techniques, current market forces, and updated regulatory frameworks are enabling frequent updating and monitoring of individual deployed model instances. We submit that external validation is insufficient to establish ML models' safety or utility. Proposals to fix the external validation paradigm do not go far enough. Continued reliance on it as the ultimate test is likely to lead us astray. We propose the MLOps-inspired paradigm of recurring local v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#20197;&#21450;&#25945;&#23398;&#20064;&#32773;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2305.02749</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#23454;&#29616;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explainable Reinforcement Learning via a Causal World Model. (arXiv:2305.02749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#20197;&#21450;&#25945;&#23398;&#20064;&#32773;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#35299;&#37322;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#34892;&#21160;&#21487;&#33021;&#23545;&#26410;&#26469;&#20135;&#29983;&#38271;&#26399;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65306;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;&#32780;&#19981;&#39044;&#20808;&#30693;&#36947;&#29615;&#22659;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#21040;&#21160;&#20316;&#30340;&#24433;&#21709;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#22240;&#26524;&#38142;&#26469;&#35299;&#37322;&#34892;&#21160;&#30340;&#38271;&#26399;&#24433;&#21709;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#34892;&#21160;&#26159;&#22914;&#20309;&#24433;&#21709;&#29615;&#22659;&#21464;&#37327;&#24182;&#26368;&#32456;&#23548;&#33268;&#22870;&#21169;&#30340;&#12290;&#19982;&#22823;&#22810;&#25968;&#35299;&#37322;&#24615;&#27169;&#22411;&#30340;&#20302;&#20934;&#30830;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#35299;&#37322;&#24615;&#21644;&#23398;&#20064;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating explanations for reinforcement learning (RL) is challenging as actions may produce long-term effects on the future. In this paper, we develop a novel framework for explainable RL by learning a causal world model without prior knowledge of the causal structure of the environment. The model captures the influence of actions, allowing us to interpret the long-term effects of actions through causal chains, which present how actions influence environmental variables and finally lead to rewards. Different from most explanatory models which suffer from low accuracy, our model remains accurate while improving explainability, making it applicable in model-based learning. As a result, we demonstrate that our causal model can serve as the bridge between explainability and learning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;RePU&#28608;&#27963;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#36817;&#20284;$C^s$&#24179;&#28369;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#21516;&#26102;&#24314;&#31435;&#20102;&#19979;&#38480;&#35823;&#24046;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RePU&#32593;&#32476;&#30340;&#24809;&#32602;&#20445;&#24207;&#22238;&#24402;(PDIR)&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00608</link><description>&lt;p&gt;
&#20351;&#29992;RePU&#28608;&#27963;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#65306;&#22312;&#24471;&#20998;&#20272;&#35745;&#21644;&#20445;&#24207;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable Neural Networks with RePU Activation: with Applications to Score Estimation and Isotonic Regression. (arXiv:2305.00608v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00608
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;RePU&#28608;&#27963;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#36817;&#20284;$C^s$&#24179;&#28369;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#21516;&#26102;&#24314;&#31435;&#20102;&#19979;&#38480;&#35823;&#24046;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RePU&#32593;&#32476;&#30340;&#24809;&#32602;&#20445;&#24207;&#22238;&#24402;(PDIR)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#20462;&#27491;&#21518;&#30340;&#24130;&#21333;&#20803;&#65288;RePU&#65289;&#20989;&#25968;&#28608;&#27963;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RePU&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#23548;&#25968;&#21487;&#20197;&#30001;&#28151;&#21512;&#28608;&#27963;RePU&#32593;&#32476;&#26469;&#34920;&#31034;&#65292;&#24182;&#25512;&#23548;&#20102;&#23548;&#25968;RePU&#32593;&#32476;&#20989;&#25968;&#31867;&#30340;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#12290;&#22312;&#20351;&#29992;RePU&#28608;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21516;&#26102;&#36817;&#20284;$C^s$&#24179;&#28369;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#35823;&#24046;&#30028;&#12290;&#27492;&#22806;&#65292;&#24403;&#25968;&#25454;&#20855;&#26377;&#36817;&#20284;&#20302;&#32500;&#25903;&#25345;&#26102;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#25913;&#36827;&#30340;&#36924;&#36817;&#35823;&#24046;&#30028;&#65292;&#35777;&#26126;&#20102;RePU&#32593;&#32476;&#20943;&#32531;&#32500;&#24230;&#28798;&#38590;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#28145;&#24230;&#24471;&#20998;&#21305;&#37197;&#20272;&#35745;&#22120;(DSME)&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RePU&#32593;&#32476;&#30340;&#24809;&#32602;&#20445;&#24207;&#22238;&#24402;(PDIR)&#12290;&#25105;&#20204;&#22312;&#20551;&#23450;&#30446;&#26631;&#20989;&#25968;&#23646;&#20110;$C^s$&#24179;&#28369;&#20989;&#25968;&#31867;&#30340;&#24773;&#20917;&#19979;&#20026;DSME&#21644;PDIR&#24314;&#31435;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the properties of differentiable neural networks activated by rectified power unit (RePU) functions. We show that the partial derivatives of RePU neural networks can be represented by RePUs mixed-activated networks and derive upper bounds for the complexity of the function class of derivatives of RePUs networks. We establish error bounds for simultaneously approximating $C^s$ smooth functions and their derivatives using RePU-activated deep neural networks. Furthermore, we derive improved approximation error bounds when data has an approximate low-dimensional support, demonstrating the ability of RePU networks to mitigate the curse of dimensionality. To illustrate the usefulness of our results, we consider a deep score matching estimator (DSME) and propose a penalized deep isotonic regression (PDIR) using RePU networks. We establish non-asymptotic excess risk bounds for DSME and PDIR under the assumption that the target functions belong to a class of $C^s$ smooth functions. We 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20808;&#39564;&#30340;&#30697;&#27861;&#36712;&#36947;&#24674;&#22797;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#21442;&#29031;&#38754;&#23545;&#40784;&#21644;&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#24314;&#27169;&#31561;&#38382;&#39064;&#65292;&#20855;&#26377;&#25233;&#21046;&#22122;&#22768;&#30340;&#20248;&#21183;.</title><link>http://arxiv.org/abs/2304.14604</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20808;&#39564;&#30340;&#30697;&#27861;&#36712;&#36947;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Deep Neural-network Prior for Orbit Recovery from Method of Moments. (arXiv:2304.14604v1 [stat.ME] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14604
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20808;&#39564;&#30340;&#30697;&#27861;&#36712;&#36947;&#24674;&#22797;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#21442;&#29031;&#38754;&#23545;&#40784;&#21644;&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#24314;&#27169;&#31561;&#38382;&#39064;&#65292;&#20855;&#26377;&#25233;&#21046;&#22122;&#22768;&#30340;&#20248;&#21183;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36947;&#24674;&#22797;&#38382;&#39064;&#26159;&#19968;&#31867;&#32463;&#24120;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#26377;&#22810;&#31181;&#24418;&#24335;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#32463;&#36807;&#32676;&#20316;&#29992;&#25197;&#26354;&#24182;&#36890;&#36807;&#24050;&#30693;&#31639;&#23376;&#35266;&#23519;&#21518;&#65292;&#20272;&#35745;&#26410;&#30693;&#20989;&#25968;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#35266;&#27979;&#20540;&#20250;&#21463;&#21040;&#38750;&#24179;&#20961;&#27700;&#24179;&#30340;&#22122;&#22768;&#27745;&#26579;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#29305;&#23450;&#30340;&#36712;&#36947;&#24674;&#22797;&#38382;&#39064;&#65292;&#21363;&#22810;&#21442;&#29031;&#38754;&#23545;&#40784;&#21644;&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#24314;&#27169;&#12290;&#20026;&#20102;&#25233;&#21046;&#22122;&#22768;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20004;&#20010;&#38382;&#39064;&#20013;&#37117;&#20351;&#29992;&#30697;&#27861;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20808;&#39564;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#36755;&#20986;&#20449;&#21495;&#21644;&#32676;&#20803;&#32032;&#30340;&#20998;&#24067;&#65292;&#32780;&#30697;&#21017;&#20026;&#36755;&#20837;&#12290;&#22312;&#22810;&#21442;&#29031;&#38754;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20174;&#30697;&#20013;&#37325;&#24314;&#20449;&#21495;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#37325;&#24314;&#20102;&#20919;&#20923;&#30005;&#38236;&#27169;&#25311;&#21644;&#29983;&#29289;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orbit recovery problems are a class of problems that often arise in practice and in various forms. In these problems, we aim to estimate an unknown function after being distorted by a group action and observed via a known operator. Typically, the observations are contaminated with a non-trivial level of noise. Two particular orbit recovery problems of interest in this paper are multireference alignment and single-particle cryo-EM modeling. In order to suppress the noise, we suggest using the method of moments approach for both problems while introducing deep neural network priors. In particular, our neural networks should output the signals and the distribution of group elements, with moments being the input. In the multireference alignment case, we demonstrate the advantage of using the NN to accelerate the convergence for the reconstruction of signals from the moments. Finally, we use our method to reconstruct simulated and biological volumes in the cryo-EM setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#35745;&#31639;&#26725;&#25509;&#24230;&#35782;&#21035;&#37325;&#35201;&#33410;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;GRAPH-wGD&#65292;&#26377;&#25928;&#22320;&#25552;&#20379;&#20840;&#23616;&#24615;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.12036</link><description>&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#26725;&#25509;&#24230;&#37325;&#35201;&#33410;&#28857;&#29983;&#25104;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#21518;&#32493;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness. (arXiv:2304.12036v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#35745;&#31639;&#26725;&#25509;&#24230;&#35782;&#21035;&#37325;&#35201;&#33410;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;GRAPH-wGD&#65292;&#26377;&#25928;&#22320;&#25552;&#20379;&#20840;&#23616;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26159;&#32534;&#30721;&#36830;&#32493;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#20851;&#31995;&#20449;&#24687;&#21516;&#26102;&#20445;&#30041;&#32593;&#32476;&#22266;&#26377;&#23646;&#24615;&#21644;&#32467;&#26500;&#30340;&#37325;&#35201;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;DeepWalk&#12289;LINE&#12289;struc2vec&#12289;PTE&#12289;UserItem2vec&#21644;RWJBG&#31561;&#26080;&#30417;&#30563;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#20174;Skip-gram&#27169;&#22411;&#20013;&#20986;&#29616;&#65292;&#24182;&#22312;&#35832;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#29992;&#20110;&#23884;&#20837;&#30340;&#35299;&#37322;&#26041;&#27861;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#25552;&#20379;Skip-gram&#23884;&#20837;&#30340;&#21518;&#32493;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#22312;&#35889;&#32858;&#31867;&#24863;&#30693;&#23616;&#37096;&#25200;&#21160;&#19979;&#35745;&#31639;&#26725;&#25509;&#24230;&#26469;&#25214;&#21040;Skip-gram&#23884;&#20837;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRAPH-wGD&#30340;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#20801;&#35768;&#26816;&#32034;top-q&#20840;&#23616;&#24615;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node representation learning in a network is an important machine learning technique for encoding relational information in a continuous vector space while preserving the inherent properties and structures of the network. Recently, unsupervised node embedding methods such as DeepWalk, LINE, struc2vec, PTE, UserItem2vec, and RWJBG have emerged from the Skip-gram model and perform better performance in several downstream tasks such as node classification and link prediction than the existing relational models. However, providing post-hoc explanations of Skip-gram-based embeddings remains a challenging problem because of the lack of explanation methods and theoretical studies applicable for embeddings. In this paper, we first show that global explanations to the Skip-gram-based embeddings can be found by computing bridgeness under a spectral cluster-aware local perturbation. Moreover, a novel gradient-based explanation method, which we call GRAPH-wGD, is proposed that allows the top-q glo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22235;&#38754;&#20307;&#21644; Delaunay &#34920;&#31034;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#21487;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#25509;&#36817;&#34920;&#38754;&#30340;&#32454;&#33410;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#27604;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.09987</link><description>&lt;p&gt;
Tetra-NeRF&#65306;&#20351;&#29992;&#22235;&#38754;&#20307;&#34920;&#31034;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra. (arXiv:2304.09987v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22235;&#38754;&#20307;&#21644; Delaunay &#34920;&#31034;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#21487;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#22810;&#25509;&#36817;&#34920;&#38754;&#30340;&#32454;&#33410;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#27604;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330; (NeRF) &#26159;&#19968;&#31181;&#38750;&#24120;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#21644;&#19977;&#32500;&#37325;&#26500;&#38382;&#39064;&#12290;NeRF &#24120;&#29992;&#30340;&#22330;&#26223;&#34920;&#31034;&#26159;&#23558;&#22330;&#26223;&#30340;&#19968;&#33268;&#30340;&#22522;&#20110;&#20307;&#32032;&#30340;&#32454;&#20998;&#19982; MLP &#32467;&#21512;&#36215;&#26469;&#12290;&#26412;&#25991;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#30340;&#65288;&#31232;&#30095;&#65289;&#28857;&#20113;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Delaunay &#34920;&#31034;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#32780;&#38750;&#19968;&#33268;&#30340;&#32454;&#20998;&#25110;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#34920;&#31034;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24039;&#22937;&#22320;&#32467;&#21512;&#20102;&#19977;&#32500;&#20960;&#20309;&#22788;&#29702;&#12289;&#19977;&#35282;&#24418;&#28210;&#26579;&#21644;&#29616;&#20195;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#27010;&#24565;&#12290;&#19982;&#22522;&#20110;&#20307;&#32032;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22810;&#25509;&#36817;&#34920;&#38754;&#30340;&#22330;&#26223;&#32454;&#33410;&#12290;&#19982;&#22522;&#20110;&#28857;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRFs) are a very recent and very popular approach for the problems of novel view synthesis and 3D reconstruction. A popular scene representation used by NeRFs is to combine a uniform, voxel-based subdivision of the scene with an MLP. Based on the observation that a (sparse) point cloud of the scene is often available, this paper proposes to use an adaptive representation based on tetrahedra and a Delaunay representation instead of the uniform subdivision or point-based representations. We show that such a representation enables efficient training and leads to state-of-the-art results. Our approach elegantly combines concepts from 3D geometry processing, triangle-based rendering, and modern neural radiance fields. Compared to voxel-based representations, ours provides more detail around parts of the scene likely to be close to the surface. Compared to point-based representations, our approach achieves better performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.09868</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?. (arXiv:2304.09868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#20854;&#35745;&#31639;&#26114;&#36149;&#30340;&#31751;&#20998;&#37197;&#27493;&#39588;&#65292;&#23427;&#38754;&#20020;&#30528;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#23569;&#37327;&#35889;&#34920;&#31034;&#30340;&#32858;&#21512;&#25968;&#25454;&#28857;&#65292;&#28982;&#21518;&#22312;&#21387;&#32553;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#26631;&#20934;&#30340;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#26368;&#21518;&#23558;&#21387;&#32553;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#32467;&#26524;&#26144;&#23556;&#22238;&#21407;&#22987;&#25968;&#25454;&#38598;&#20197;&#21457;&#29616;&#31751;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support vector clustering is an important clustering method. However, it suffers from a scalability issue due to its computational expensive cluster assignment step. In this paper we accelertate the support vector clustering via spectrum-preserving data compression. Specifically, we first compress the original data set into a small amount of spectrally representative aggregated data points. Then, we perform standard support vector clustering on the compressed data set. Finally, we map the clustering results of the compressed data set back to discover the clusters in the original data set. Our extensive experimental results on real-world data set demonstrate dramatically speedups over standard support vector clustering without sacrificing clustering quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#19979;&#28216;&#19987;&#23478;&#25968;&#25454;&#20174;&#31163;&#32447;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24615;&#22320;&#26597;&#35810;&#30456;&#20851;&#34892;&#20026;&#65288;&#21253;&#25324;&#35768;&#22810;&#27425;&#20248;&#34892;&#20026;&#65289;&#65292;&#23454;&#29616;&#20102;&#34892;&#20026;&#26816;&#32034;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.08742</link><description>&lt;p&gt;
&#34892;&#20026;&#26816;&#32034;&#65306;&#36890;&#36807;&#26597;&#35810;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#23454;&#29616;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets. (arXiv:2304.08742v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#19979;&#28216;&#19987;&#23478;&#25968;&#25454;&#20174;&#31163;&#32447;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24615;&#22320;&#26597;&#35810;&#30456;&#20851;&#34892;&#20026;&#65288;&#21253;&#25324;&#35768;&#22810;&#27425;&#20248;&#34892;&#20026;&#65289;&#65292;&#23454;&#29616;&#20102;&#34892;&#20026;&#26816;&#32034;&#65292;&#36827;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#20351;&#26426;&#22120;&#20154;&#23398;&#20064;&#26032;&#30340;&#35270;&#35273;&#21160;&#20316;&#25216;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#65292;&#26377;&#35768;&#22810;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#33539;&#24335;&#26159;&#21033;&#29992;&#22823;&#22411;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#35768;&#22810;&#34892;&#20026;&#65292;&#28982;&#21518;&#20351;&#29992;&#23569;&#37327;&#20219;&#21153;&#29305;&#23450;&#30340;&#20154;&#31867;&#30417;&#30563;&#65288;&#21363;&#20171;&#20837;&#25110;&#28436;&#31034;&#65289;&#26469;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;&#20294;&#26159;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;&#29421;&#31364;&#30340;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#24182;&#23558;&#20854;&#19982;&#31163;&#32447;&#25968;&#25454;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#22312;&#20110;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#19981;&#20165;&#20026;&#20195;&#29702;&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36824;&#21487;&#20197;&#20026;&#20195;&#29702;&#30340;&#23398;&#20064;&#25552;&#20379;&#26377;&#20851;&#20808;&#21069;&#25968;&#25454;&#31867;&#22411;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#19979;&#28216;&#19987;&#23478;&#25968;&#25454;&#20174;&#31163;&#32447;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#24615;&#22320;&#26597;&#35810;&#30456;&#20851;&#34892;&#20026;&#65288;&#21253;&#25324;&#35768;&#22810;&#27425;&#20248;&#34892;&#20026;&#65289;&#12290;&#28982;&#21518;&#20195;&#29702;&#34987;&#32852;&#21512;&#35757;&#32451;&#22312;&#19987;&#23478;&#21644;&#26597;&#35810;&#25968;&#25454;&#19978;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25105;&#20204;&#30340;
&lt;/p&gt;
&lt;p&gt;
Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert data to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07689</link><description>&lt;p&gt;
&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#29992;&#20110;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Empirical Bregman Divergence for Uncertain Distance Representation. (arXiv:2304.07689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;Deep Metric Learning&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#36317;&#31163;&#34920;&#31034;&#65292;&#33021;&#22815;&#26377;&#25928;&#30340;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25216;&#26415;&#24050;&#24212;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#26679;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#26041;&#27861;&#37319;&#29992;&#22266;&#23450;&#36317;&#31163;&#24230;&#37327;&#20316;&#20026;&#20004;&#20010;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21487;&#33021;&#23548;&#33268;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#20122;&#26368;&#20248;&#24615;&#33021;&#12290;Bregman&#25955;&#24230;&#27010;&#25324;&#20102;&#21508;&#31181;&#36317;&#31163;&#24230;&#37327;&#30340;&#24230;&#37327;&#65292;&#24182;&#22312;&#35768;&#22810;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#39046;&#22495;&#20013;&#20135;&#29983;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;Bregman&#25955;&#24230;&#33719;&#24471;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#25439;&#22833;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#32463;&#39564;Bregman&#25955;&#24230;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#35774;&#32622;&#23545;Bregman&#25955;&#24230;&#19979;&#30340;&#20984;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;SOTA&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#27969;&#34892;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#24335;&#35782;&#21035;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#20998;&#24067;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#21033;&#29992;&#35813;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#23884;&#20837;&#65292;&#22312;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03376</link><description>&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#21160;&#24577;&#21644;&#20960;&#20309;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpretable statistical representations of neural population dynamics and geometry. (arXiv:2304.03376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#20998;&#24067;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#21033;&#29992;&#35813;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#23884;&#20837;&#65292;&#22312;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#31070;&#32463;&#20803;&#32676;&#20307;&#30340;&#21160;&#24577;&#36890;&#24120;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#20960;&#20309;&#21644;&#21160;&#24577;&#23545;&#32534;&#30721;&#30456;&#20851;&#34892;&#20026;&#21464;&#37327;&#30340;&#36129;&#29486;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#30456;&#36712;&#29305;&#24449;&#30340;&#32479;&#35745;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32479;&#35745;&#34920;&#31034;&#21487;&#20197;&#27178;&#36328;&#31070;&#32463;&#32593;&#32476;&#23454;&#20363;&#36827;&#34892;&#25512;&#24191;&#65292;&#20197;&#21306;&#20998;&#35745;&#31639;&#26426;&#21046;&#65292;&#22312;&#20855;&#26377;&#20960;&#20309;&#23545;&#24212;&#30340;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#35299;&#37322;&#23884;&#20837;&#31070;&#32463;&#21160;&#21147;&#23398;&#65292;&#24182;&#24320;&#21457;&#20855;&#26377;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20351;&#29992;&#20869;&#22312;&#27969;&#24418;&#32467;&#26500;&#20248;&#20110;&#26102;&#38388;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamics of neuron populations during diverse tasks often evolve on low-dimensional manifolds. However, it remains challenging to discern the contributions of geometry and dynamics for encoding relevant behavioural variables. Here, we introduce an unsupervised geometric deep learning framework for representing non-linear dynamical systems based on statistical distributions of local phase portrait features. Our method provides robust geometry-aware or geometry-agnostic representations for the unbiased comparison of dynamics based on measured trajectories. We demonstrate that our statistical representation can generalise across neural network instances to discriminate computational mechanisms, obtain interpretable embeddings of neural dynamics in a primate reaching task with geometric correspondence to hand kinematics, and develop a decoding algorithm with state-of-the-art accuracy. Our results highlight the importance of using the intrinsic manifold structure over temporal informati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#25972;&#21512;&#20102;EHR&#20013;&#30340;&#25104;&#20687;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.02836</link><description>&lt;p&gt;
&#38271;&#26399;&#30340;&#22810;&#27169;&#24335;&#21464;&#21387;&#22120;&#25972;&#21512;EHR&#20013;&#25104;&#20687;&#21644;&#28508;&#22312;&#20020;&#24202;&#29305;&#24449;&#65292;&#29992;&#20110;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal Multimodal Transformer Integrating Imaging and Latent Clinical Signatures From Routine EHRs for Pulmonary Nodule Classification. (arXiv:2304.02836v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32954;&#37096;&#32467;&#33410;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#25972;&#21512;&#20102;EHR&#20013;&#30340;&#25104;&#20687;&#21644;&#20020;&#24202;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#37325;&#22797;&#25104;&#20687;&#21644;&#21307;&#30103;&#32972;&#26223;&#65288;&#22914;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65289;&#32435;&#20837;&#39044;&#27979;&#24615;&#23396;&#31435;&#24615;&#32954;&#37096;&#32467;&#33410;&#65288;SPN&#65289;&#35786;&#26029;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20687;&#25104;&#20687;&#21644;&#35786;&#26029;&#20195;&#30721;&#36825;&#26679;&#30340;&#20020;&#24202;&#24120;&#35268;&#27169;&#24335;&#21487;&#33021;&#26159;&#24322;&#27493;&#30340;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#19981;&#35268;&#21017;&#37319;&#26679;&#65292;&#36825;&#26159;&#38271;&#26399;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#27169;&#24577;&#31574;&#30053;&#65292;&#23558;&#37325;&#22797;&#25104;&#20687;&#19982;&#26085;&#24120;&#25910;&#38598;&#30340;EHR&#20013;&#30340;&#38271;&#26399;&#20020;&#24202;&#29305;&#24449;&#30456;&#25972;&#21512;&#65292;&#20197;&#36827;&#34892;SPN&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;&#28508;&#22312;&#20020;&#24202;&#29305;&#24449;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#35299;&#32544;&#32538;&#65292;&#24182;&#21033;&#29992;&#26102;&#38388;&#36317;&#31163;&#32553;&#25918;&#33258;&#27880;&#24847;&#21147;&#26469;&#32852;&#21512;&#23398;&#20064;&#20020;&#24202;&#29305;&#24449;&#34920;&#36798;&#21644;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#26159;&#22312;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;2,668&#20010;&#25195;&#25551;&#21644;1,149&#21517;&#24535;&#24895;&#32773;&#30340;&#38271;&#26399;&#33016;&#37096;CT&#12289;&#36134;&#21333;&#20195;&#30721;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#35760;&#24405;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy of predictive models for solitary pulmonary nodule (SPN) diagnosis can be greatly increased by incorporating repeat imaging and medical context, such as electronic health records (EHRs). However, clinically routine modalities such as imaging and diagnostic codes can be asynchronous and irregularly sampled over different time scales which are obstacles to longitudinal multimodal learning. In this work, we propose a transformer-based multimodal strategy to integrate repeat imaging with longitudinal clinical signatures from routinely collected EHRs for SPN classification. We perform unsupervised disentanglement of latent clinical signatures and leverage time-distance scaled self-attention to jointly learn from clinical signatures expressions and chest computed tomography (CT) scans. Our classifier is pretrained on 2,668 scans from a public dataset and 1,149 subjects with longitudinal chest CTs, billing codes, medications, and laboratory tests from EHRs of our home institution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#65292;&#23454;&#29616;&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39640;&#25928;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.17907</link><description>&lt;p&gt;
&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39044;&#27979;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#37325;&#23450;&#21521;&#27493;&#34892;
&lt;/p&gt;
&lt;p&gt;
Predictive Context-Awareness for Full-Immersive Multiuser Virtual Reality with Redirected Walking. (arXiv:2303.17907v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#65292;&#23454;&#29616;&#38754;&#21521;&#20840;&#27785;&#28024;&#22810;&#29992;&#25143;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#39640;&#25928;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#27491;&#26397;&#30528;&#22686;&#24378;&#27785;&#28024;&#24863;&#12289;&#25903;&#25345;&#22810;&#29992;&#25143;&#20307;&#39564;&#21644;&#22312;&#34394;&#25311;&#20307;&#39564;&#20013;&#25903;&#25345;&#26080;&#38480;&#21046;&#30340;&#31227;&#21160;&#65292;&#32780;&#36890;&#36807;&#37325;&#23450;&#21521;&#27493;&#34892;&#23558;&#29992;&#25143;&#38480;&#21046;&#22312;&#19987;&#38376;&#30340;VR&#35774;&#32622;&#20869;&#12290;&#20026;&#20102;&#28385;&#36275;&#26410;&#26469;VR&#31995;&#32479;&#30340;&#26497;&#31471;&#25968;&#25454;&#36895;&#29575;&#21644;&#24310;&#36831;&#35201;&#27714;&#65292;&#25903;&#25345;&#26080;&#32447;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#23558;&#22312;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#39057;&#29575;&#19978;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#23454;&#29616;&#39640;&#24230;&#23450;&#21521;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#39044;&#27979;&#24615;&#19978;&#19979;&#25991;&#24863;&#30693;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#21644;&#25509;&#25910;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35748;&#20026;&#36890;&#36807;&#30701;&#26399;&#39044;&#27979;&#22810;&#29992;&#25143;VR&#35774;&#32622;&#20013;&#29992;&#25143;&#30340;&#27178;&#21521;&#31227;&#21160;&#65292;&#21487;&#20197;&#21033;&#29992;&#29992;&#25143;&#26041;&#21521;&#19978;&#30340;&#30452;&#32447;&#35270;&#36317;&#65288;LoS&#65289;&#8220;&#36319;&#36394;&#8221;&#26469;&#20248;&#21270;&#21457;&#23556;&#31471;&#30340;&#27874;&#26463;&#25104;&#24418;&#21644;&#27874;&#26463;&#23548;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual Reality (VR) technology is being advanced along the lines of enhancing its immersiveness, enabling multiuser Virtual Experiences (VEs), and supporting unconstrained mobility of the users in their VEs, while constraining them within specialized VR setups through Redirected Walking (RDW). For meeting the extreme data-rate and latency requirements of future VR systems, supporting wireless networking infrastructures will operate in millimeter Wave (mmWave) frequencies and leverage highly directional communication in both transmission and reception through beamforming and beamsteering. We propose to leverage predictive context-awareness for optimizing transmitter and receiver-side beamforming and beamsteering. In particular, we argue that short-term prediction of users' lateral movements in multiuser VR setups with RDW can be utilized for optimizing transmitter-side beamforming and beamsteering through Line-of-Sight (LoS) "tracking" in the users' directions. At the same time, short-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HCNN&#65292;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#35774;&#35745;&#30340;&#23436;&#20840;&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#27931;&#20262;&#20857;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;CNN&#30340;&#22522;&#26412;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#22312;&#23436;&#20840;&#21452;&#26354;&#35774;&#32622;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;HCNN&#26694;&#26550;&#21644;&#27931;&#20262;&#20857;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.15919</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21452;&#26354;&#20960;&#20309;&#65306;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks. (arXiv:2303.15919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HCNN&#65292;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#35774;&#35745;&#30340;&#23436;&#20840;&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#27931;&#20262;&#20857;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;CNN&#30340;&#22522;&#26412;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#22312;&#23436;&#20840;&#21452;&#26354;&#35774;&#32622;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;HCNN&#26694;&#26550;&#21644;&#27931;&#20262;&#20857;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#25968;&#25454;&#21576;&#29616;&#20986;&#22266;&#26377;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#36825;&#20123;&#32467;&#26500;&#21487;&#20197;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#26377;&#25928;&#22320;&#34920;&#31034;&#12290;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#26159;&#22312;&#36825;&#31181;&#31354;&#38388;&#20013;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#24403;&#21069;&#26041;&#27861;&#20381;&#36182;&#20110;&#27431;&#20960;&#37324;&#24471;&#39592;&#24178;&#65292;&#24182;&#19988;&#20165;&#22312;&#20219;&#21153;&#22836;&#20013;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#21452;&#26354;&#20960;&#20309;&#30340;&#22909;&#22788;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HCNN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#35774;&#35745;&#30340;&#23436;&#20840;&#21452;&#26354;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#22522;&#20110;&#27931;&#20262;&#20857;&#27169;&#22411;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;CNN&#30340;&#22522;&#26412;&#32452;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#21367;&#31215;&#23618;&#12289;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#22810;&#39033;&#24335;&#36923;&#36753;&#22238;&#24402;&#65288;MLR&#65289;&#30340;&#26032;&#20844;&#24335;&#12290;&#22312;&#26631;&#20934;&#35270;&#35273;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;HCNN&#26694;&#26550;&#21644;&#27931;&#20262;&#20857;&#27169;&#22411;&#22312;&#28151;&#21512;&#21644;&#23436;&#20840;&#21452;&#26354;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#23558;&#26469;&#22312;&#21452;&#26354;&#20960;&#20309;&#20013;&#36827;&#34892;&#30340;&#30740;&#31350;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current methods in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, the first fully hyperbolic convolutional neural network (CNN) designed for computer vision tasks. Based on the Lorentz model, we generalize fundamental components of CNNs and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression (MLR). Experimentation on standard vision tasks demonstrates the effectiveness of our HCNN framework and the Lorentz model in both hybrid and fully hyperbolic settings. Overall, we aim to pave the way for future research in h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10650</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#36923;&#36753;&#30340;&#36923;&#36753;&#65306;&#36208;&#21521;DL&#30340;&#32479;&#19968;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#65292;&#29992;&#20110;&#23450;&#20041;DL&#65292;&#35813;&#20803;&#35821;&#35328;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#21487;&#24494;&#20998;&#36923;&#36753;&#65288;DL&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#28385;&#36275;&#36923;&#36753;&#35268;&#33539;&#30340;&#26041;&#27861;&#12290;DL&#21253;&#25324;&#35821;&#27861;&#21644;&#23558;&#35821;&#27861;&#20013;&#30340;&#34920;&#36798;&#24335;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#35299;&#37322;&#20989;&#25968;&#12290;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;&#26631;&#20934;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290; &#29616;&#26377;DL&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#20854;&#24418;&#24335;&#21270;&#31243;&#24230;&#30340;&#19981;&#21516;&#22788;&#29702;&#20351;&#24471;&#23545;&#23427;&#20204;&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#21464;&#24471;&#22256;&#38590;&#12290;&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20803;&#35821;&#35328;&#8212;&#8212;LDL&#20316;&#20026;DL&#23450;&#20041;&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#20174;&#35821;&#27861;&#21644;&#35821;&#20041;&#20004;&#26041;&#38754;&#19978;&#25552;&#39640;DL&#30340;&#24418;&#24335;&#21270;&#31243;&#24230;&#65292;&#20351;&#24471;&#23545;DL&#30340;&#24615;&#36136;&#21644;&#23454;&#29616;&#36827;&#34892;&#31995;&#32479;&#27604;&#36739;&#30740;&#31350;&#25104;&#20026;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38050;&#29748;&#21367;&#24088;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21327;&#35843;&#12289;&#29983;&#25104;&#12289;&#23436;&#21892;&#38899;&#20048;&#65307;&#20195;&#30721;&#24050;&#20844;&#24320;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2303.08385</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;
&lt;/p&gt;
&lt;p&gt;
Generating symbolic music using diffusion models. (arXiv:2303.08385v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38050;&#29748;&#21367;&#24088;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21327;&#35843;&#12289;&#29983;&#25104;&#12289;&#23436;&#21892;&#38899;&#20048;&#65307;&#20195;&#30721;&#24050;&#20844;&#24320;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;&#25193;&#25955;&#27169;&#22411;&#19981;&#20250;&#20986;&#29616;&#27169;&#24335;&#23849;&#28291;&#65292;&#20063;&#19981;&#38656;&#35201;&#36776;&#21035;&#22120;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20108;&#39033;&#20808;&#39564;&#20998;&#24067;&#26469;&#29983;&#25104;&#38050;&#29748;&#21367;&#24088;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#21644;&#29983;&#25104;&#26679;&#26412;&#12290;&#29983;&#25104;&#30340;&#38899;&#20048;&#20855;&#26377;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#21487;&#20197;&#36798;&#21040;&#35757;&#32451;&#38050;&#29748;&#21367;&#24088;&#27573;&#30340;&#38271;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#26159;&#22914;&#20309;&#22312;&#36755;&#20837;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#24037;&#20316;&#65292;&#24182;&#21487;&#29992;&#20110;&#21327;&#35843;&#32473;&#23450;&#30340;&#26059;&#24459;&#65292;&#23436;&#25104;&#19981;&#23436;&#25972;&#30340;&#38050;&#29748;&#21367;&#24088;&#25110;&#29983;&#25104;&#32473;&#23450;&#20048;&#26354;&#30340;&#21464;&#21270;&#12290;&#20195;&#30721;&#26159;&#20844;&#24320;&#20849;&#20139;&#30340;&#65292;&#20197;&#40723;&#21169;&#31038;&#21306;&#20351;&#29992;&#21644;&#24320;&#21457;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Denoising Diffusion models have emerged as simple yet very powerful generative models. Diffusion models unlike other generative models do not suffer from mode collapse nor require a discriminator to generate high quality samples. In this paper, we propose a diffusion model that uses a binomial prior distribution to generate piano-rolls. The paper also proposes an efficient method to train the model and generate samples. The generated music has coherence at time scales up to the length of the training piano-roll segments. We show how such a model is conditioned on the input and can be used to harmonize a given melody, complete an incomplete piano-roll or generate a variation of a given piece. The code is shared publicly to encourage the use and development of the method by the community.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.06318</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#19987;&#23478;&#28151;&#21512;&#24182;&#34892;&#26041;&#27861;&#26469;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixture-of-Experts&#65288;MoE&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#28155;&#21152;&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#22359;&#26469;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#65288;&#22522;&#26412;&#27169;&#22411;&#65289;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#25913;&#21464;&#35757;&#32451;&#25110;&#25512;&#29702;&#30340;&#24635;&#28014;&#28857;&#25805;&#20316;&#25968;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#31181;&#26550;&#26500;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#20219;&#24847;&#22823;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19982;&#22522;&#26412;&#27169;&#22411;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;64&#21040;128&#20010;&#19987;&#23478;&#22359;&#20043;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35266;&#23519;&#21040;&#36825;&#20123;MoE&#27169;&#22411;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#36882;&#20943;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;MoE&#27169;&#22411;&#38656;&#35201;&#25105;&#20204;&#25193;&#23637;&#22522;&#26412;&#27169;&#22411;&#30340;&#22823;&#23567;&#20197;&#21450;&#19987;&#23478;&#22359;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#32500;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#27493;&#39588;&#20013;&#25552;&#20986;&#20102;&#20869;&#23384;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#20462;&#22797;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;DDMs&#12290;&#36890;&#36807;&#25972;&#21512;IR&#25991;&#29486;&#65292;&#25105;&#20204;&#20351;&#29992;&#26367;&#20195;&#30446;&#26631;&#21644;&#22810;&#26679;&#30340;&#21069;&#21521;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;MAP&#20808;&#39564;&#25439;&#22833;&#20989;&#25968;&#30340;&#22522;&#30784;&#65292;&#28040;&#38500;&#20102;DDMs&#20013;&#26114;&#36149;&#30340;&#37319;&#26679;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21453;&#38382;&#39064;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#30456;&#20449;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#38138;&#24179;&#24555;&#36895;&#22270;&#20687;&#21512;&#25104;&#21644;&#20462;&#22797;&#30340;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.05456</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#20462;&#22797;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Restoration based Generative Models. (arXiv:2303.05456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#20462;&#22797;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;DDMs&#12290;&#36890;&#36807;&#25972;&#21512;IR&#25991;&#29486;&#65292;&#25105;&#20204;&#20351;&#29992;&#26367;&#20195;&#30446;&#26631;&#21644;&#22810;&#26679;&#30340;&#21069;&#21521;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;MAP&#20808;&#39564;&#25439;&#22833;&#20989;&#25968;&#30340;&#22522;&#30784;&#65292;&#28040;&#38500;&#20102;DDMs&#20013;&#26114;&#36149;&#30340;&#37319;&#26679;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21453;&#38382;&#39064;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#30456;&#20449;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#38138;&#24179;&#24555;&#36895;&#22270;&#20687;&#21512;&#25104;&#21644;&#20462;&#22797;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#21512;&#25104;&#36136;&#37327;&#65292;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;DDMs&#24314;&#31435;&#22312;&#23558;&#25968;&#25454;&#25512;&#21521;&#22122;&#22768;&#20998;&#24067;&#30340;&#25193;&#25955;&#36807;&#31243;&#19978;&#65292;&#27169;&#22411;&#23398;&#20064;&#21435;&#22122;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;DDMs&#22312;&#22270;&#20687;&#20462;&#22797;&#65288;IR&#65289;&#26041;&#38754;&#30340;&#35299;&#37322;&#12290;&#25972;&#21512;IR&#25991;&#29486;&#20351;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26367;&#20195;&#30446;&#26631;&#21644;&#22810;&#26679;&#30340;&#21069;&#21521;&#36807;&#31243;&#65292;&#32780;&#19981;&#20165;&#23616;&#38480;&#20110;&#25193;&#25955;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#22522;&#20110;MAP&#20272;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#19978;&#26045;&#21152;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#28040;&#38500;&#20102;DDMs&#20013;&#26114;&#36149;&#30340;&#37319;&#26679;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#35757;&#32451;&#65292;&#36890;&#36807;&#21033;&#29992;&#21069;&#21521;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21453;&#38382;&#39064;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#38138;&#24179;&#20102;&#24555;&#36895;&#22270;&#20687;&#21512;&#25104;&#21644;&#20462;&#22797;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models (DDMs) have recently attracted increasing attention by showing impressive synthesis quality. DDMs are built on a diffusion process that pushes data to the noise distribution and the models learn to denoise. In this paper, we establish the interpretation of DDMs in terms of image restoration (IR). Integrating IR literature allows us to use an alternative objective and diverse forward processes, not confining to the diffusion process. By imposing prior knowledge on the loss function grounded on MAP-based estimation, we eliminate the need for the expensive sampling of DDMs. Also, we propose a multi-scale training, which improves the performance compared to the diffusion process, by taking advantage of the flexibility of the forward process. Experimental results demonstrate that our model improves the quality and efficiency of both training and inference. Furthermore, we show the applicability of our model to inverse problems. We believe that our framework paves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24314;&#27169;&#26694;&#26550;&#8212;&#8212;CoolPINNs&#65292;&#29992;&#20110;&#26088;&#22312;&#39640;&#25928;&#28909;&#35843;&#33410;&#30340;&#34880;&#31649;&#31995;&#32479;&#12290;&#23427;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#22797;&#26434;&#34880;&#31649;&#24067;&#23616;&#20013;&#30340;&#28909;&#27969;&#24613;&#21095;&#36339;&#21464;&#65292;&#22788;&#29702;&#21253;&#25324;&#20999;&#21521;&#21644;&#27861;&#21521;&#20998;&#37327;&#30340;&#26012;&#29575;&#23548;&#25968;&#65292;&#35299;&#20915;&#30001;&#20110;&#36752;&#23556;&#28909;&#20256;&#36882;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#65292;&#24182;&#20026;&#23454;&#26102;&#30417;&#27979;&#25552;&#20379;&#39640;&#36895;&#39044;&#27979;&#65292;&#26356;&#20415;&#20110;&#31283;&#20581;&#30340;&#21453;&#28436;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.05300</link><description>&lt;p&gt;
CoolPINNs: &#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#34880;&#31649;&#31995;&#32479;&#20013;&#30340;&#20027;&#21160;&#20919;&#21364;
&lt;/p&gt;
&lt;p&gt;
CoolPINNs: A Physics-informed Neural Network Modeling of Active Cooling in Vascular Systems. (arXiv:2303.05300v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24314;&#27169;&#26694;&#26550;&#8212;&#8212;CoolPINNs&#65292;&#29992;&#20110;&#26088;&#22312;&#39640;&#25928;&#28909;&#35843;&#33410;&#30340;&#34880;&#31649;&#31995;&#32479;&#12290;&#23427;&#33021;&#22815;&#20934;&#30830;&#25429;&#25417;&#22797;&#26434;&#34880;&#31649;&#24067;&#23616;&#20013;&#30340;&#28909;&#27969;&#24613;&#21095;&#36339;&#21464;&#65292;&#22788;&#29702;&#21253;&#25324;&#20999;&#21521;&#21644;&#27861;&#21521;&#20998;&#37327;&#30340;&#26012;&#29575;&#23548;&#25968;&#65292;&#35299;&#20915;&#30001;&#20110;&#36752;&#23556;&#28909;&#20256;&#36882;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#65292;&#24182;&#20026;&#23454;&#26102;&#30417;&#27979;&#25552;&#20379;&#39640;&#36895;&#39044;&#27979;&#65292;&#26356;&#20415;&#20110;&#31283;&#20581;&#30340;&#21453;&#28436;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#25216;&#26415;&#22914;&#39640;&#36229;&#22768;&#36895;&#39134;&#26426;&#12289;&#31354;&#38388;&#25506;&#32034;&#36710;&#21644;&#30005;&#27744;&#21033;&#29992;&#23884;&#20837;&#24335;&#24494;&#32454;&#24490;&#29615;&#36827;&#34892;&#39640;&#25928;&#28909;&#35843;&#33410;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#24037;&#31243;&#31995;&#32479;&#30340;&#35774;&#35745;&#21644;&#25805;&#20316;&#38454;&#27573;&#36827;&#34892;&#24314;&#27169;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#24320;&#21457;&#24314;&#27169;&#26694;&#26550;&#26041;&#38754;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#23398;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#24212;&#29992;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;CoolPINNs&#30340;&#24555;&#36895;&#12289;&#21487;&#38752;&#12289;&#20934;&#30830;&#30340;&#22522;&#20110;PINNs&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#34880;&#31649;&#31995;&#32479;&#30340;&#28909;&#35843;&#33410;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging technologies like hypersonic aircraft, space exploration vehicles, and batteries avail fluid circulation in embedded microvasculatures for efficient thermal regulation. Modeling is vital during these engineered systems' design and operational phases. However, many challenges exist in developing a modeling framework. What is lacking is an accurate framework that (i) captures sharp jumps in the thermal flux across complex vasculature layouts, (ii) deals with oblique derivatives (involving tangential and normal components), (iii) handles nonlinearity because of radiative heat transfer, (iv) provides a high-speed forecast for real-time monitoring, and (v) facilitates robust inverse modeling. This paper addresses these challenges by availing the power of physics-informed neural networks (PINNs). We develop a fast, reliable, and accurate Scientific Machine Learning (SciML) framework for vascular-based thermal regulation -- called CoolPINNs: a PINNs-based modeling framework for activ
&lt;/p&gt;</description></item><item><title>QuickSRNet &#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#31227;&#21160;&#24179;&#21488;&#19978;&#30340;&#23454;&#26102;&#24212;&#29992;&#30340;&#39640;&#25928;&#36229;&#20998;&#36776;&#29575;&#26550;&#26500;&#65292;&#23427;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#30340;&#28145;&#24230;&#23398;&#20064;&#36229;&#20998;&#36776;&#29575;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#31070;&#32463;&#26550;&#26500;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#19982;&#24310;&#36831;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.04336</link><description>&lt;p&gt;
QuickSRNet&#65306;&#36866;&#29992;&#20110;&#31227;&#21160;&#24179;&#21488;&#24555;&#36895;&#25512;&#29702;&#30340;&#31616;&#21333;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
QuickSRNet: Plain Single-Image Super-Resolution Architecture for Faster Inference on Mobile Platforms. (arXiv:2303.04336v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04336
&lt;/p&gt;
&lt;p&gt;
QuickSRNet &#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#31227;&#21160;&#24179;&#21488;&#19978;&#30340;&#23454;&#26102;&#24212;&#29992;&#30340;&#39640;&#25928;&#36229;&#20998;&#36776;&#29575;&#26550;&#26500;&#65292;&#23427;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#30340;&#28145;&#24230;&#23398;&#20064;&#36229;&#20998;&#36776;&#29575;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#31070;&#32463;&#26550;&#26500;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#19982;&#24310;&#36831;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36229;&#20998;&#36776;&#29575;&#26550;&#26500;QuickSRNet&#65292;&#36866;&#29992;&#20110;&#31227;&#21160;&#24179;&#21488;&#19978;&#30340;&#23454;&#26102;&#24212;&#29992;&#12290;&#36229;&#20998;&#36776;&#29575;&#21487;&#20197;&#20351;&#22270;&#20687;&#26356;&#21152;&#28165;&#26224;&#12289;&#38160;&#21033;&#24182;&#25552;&#39640;&#20998;&#36776;&#29575;&#12290;&#28216;&#25103;&#21644;&#35270;&#39057;&#25773;&#25918;&#24212;&#29992;&#20197;&#21450;&#19981;&#26029;&#25552;&#21319;&#30340;&#30005;&#35270;&#12289;&#26234;&#33021;&#25163;&#26426;&#21644;VR&#22836;&#26174;&#30340;&#26174;&#31034;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;&#23545;&#39640;&#25928;&#25552;&#21319;&#20998;&#36776;&#29575;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#22312;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#22312;&#35745;&#31639;&#12289;&#28909;&#37327;&#21644;&#33021;&#28304;&#28040;&#32791;&#26041;&#38754;&#65292;&#23454;&#29616;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#26102;&#30340;&#28145;&#24230;&#23398;&#20064;&#36229;&#20998;&#36776;&#29575;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;QuickSRNet&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26550;&#26500;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#19982;&#24310;&#36831;&#30340;&#26435;&#34913;&#65292;&#29992;&#20110;&#21333;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#25216;&#24039;&#65292;&#21487;&#20197;&#21152;&#36895;&#29616;&#26377;&#30340;&#22522;&#20110;&#27531;&#24046;&#30340;&#36229;&#20998;&#36776;&#29575;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#37327;&#21270;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present QuickSRNet, an efficient super-resolution architecture for real-time applications on mobile platforms. Super-resolution clarifies, sharpens, and upscales an image to higher resolution. Applications such as gaming and video playback along with the ever-improving display capabilities of TVs, smartphones, and VR headsets are driving the need for efficient upscaling solutions. While existing deep learning-based super-resolution approaches achieve impressive results in terms of visual quality, enabling real-time DL-based super-resolution on mobile devices with compute, thermal, and power constraints is challenging. To address these challenges, we propose QuickSRNet, a simple yet effective architecture that provides better accuracy-to-latency trade-offs than existing neural architectures for single-image super resolution. We present training tricks to speed up existing residual-based super-resolution architectures while maintaining robustness to quantization. Our pro
&lt;/p&gt;</description></item><item><title>&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#20250;&#20135;&#29983;&#39044;&#27979;&#22810;&#26679;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#30456;&#21516;&#36755;&#20837;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#24615;&#20063;&#20250;&#24471;&#21040;&#19981;&#21516;&#30340;&#36755;&#20986;&#65292;&#36825;&#19968;&#25104;&#26412;&#19981;&#20165;&#26410;&#34987;&#30740;&#31350;&#36824;&#26410;&#34987;&#23457;&#26680;&#25110;&#20256;&#36798;&#32473;&#27169;&#22411;&#35774;&#35745;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;</title><link>http://arxiv.org/abs/2302.14517</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#20219;&#24847;&#20915;&#31574;&#26159;&#19968;&#20010;&#38544;&#34255;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Arbitrary Decisions are a Hidden Cost of Differentially Private Training. (arXiv:2302.14517v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14517
&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#20250;&#20135;&#29983;&#39044;&#27979;&#22810;&#26679;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#30456;&#21516;&#36755;&#20837;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#38543;&#26426;&#24615;&#20063;&#20250;&#24471;&#21040;&#19981;&#21516;&#30340;&#36755;&#20986;&#65292;&#36825;&#19968;&#25104;&#26412;&#19981;&#20165;&#26410;&#34987;&#30740;&#31350;&#36824;&#26410;&#34987;&#23457;&#26680;&#25110;&#20256;&#36798;&#32473;&#27169;&#22411;&#35774;&#35745;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#26426;&#21046;&#36890;&#24120;&#26088;&#22312;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;(DP)&#12290;&#22312;&#23558;&#27169;&#22411;&#21442;&#25968;&#25311;&#21512;&#21040;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#26102;&#65292;&#23454;&#36341;&#20013;&#20351;&#29992;&#38543;&#26426;&#21270;&#26041;&#27861;(&#20363;&#22914;&#65292;&#22312;&#25130;&#26029;&#30340;&#26799;&#24230;&#19978;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;)&#20197;&#30830;&#20445;DP&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#38543;&#26426;&#21270;&#20250;&#20135;&#29983;&#39044;&#27979;&#22810;&#26679;&#24615;&#65306;&#23545;&#20110;&#32473;&#23450;&#30340;&#36755;&#20837;&#31034;&#20363;&#65292;&#30001;&#21516;&#26679;DP-&#20445;&#35777;&#30340;&#27169;&#22411;&#39044;&#27979;&#30340;&#36755;&#20986;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#38543;&#26426;&#24615;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#36755;&#20837;&#65292;&#21363;&#20351;&#20351;&#29992;&#30456;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#39044;&#27979;&#36755;&#20986;&#20063;&#21487;&#33021;&#21457;&#29983;&#24040;&#22823;&#21464;&#21270;&#12290;&#23578;&#26410;&#30740;&#31350;&#30001;DP&#35757;&#32451;&#24341;&#36215;&#30340;&#22810;&#26679;&#24615;&#25104;&#26412;&#65292;&#24182;&#19988;&#30446;&#21069;&#36824;&#26410;&#32463;&#36807;&#23457;&#26680;&#25110;&#21521;&#27169;&#22411;&#35774;&#35745;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#20256;&#36798;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#31181;&#22312;&#21487;&#38752;&#22320;&#20272;&#35745;&#39044;&#27979;&#22810;&#26679;&#24615;&#25152;&#38656;&#30340;&#37325;&#26032;&#35757;&#32451;&#27425;&#25968;&#30340;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#19977;&#31181;DP-&#30830;&#20445;&#35757;&#32451;&#26041;&#27861;&#30340;&#39044;&#27979;&#22810;&#26679;&#24615;&#25104;&#26412;&#65292;&#21253;&#25324;&#29702;&#35770;&#21644;&#23454;&#39564;&#20004;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanisms used in privacy-preserving machine learning often aim to guarantee differential privacy (DP) during model training. Practical DP-ensuring training methods use randomization when fitting model parameters to privacy-sensitive data (e.g., adding Gaussian noise to clipped gradients). We demonstrate that such randomization incurs predictive multiplicity: for a given input example, the output predicted by equally-private models depends on the randomness used in training. Thus, for a given input, the predicted output can vary drastically if a model is re-trained, even if the same training dataset is used. The predictive-multiplicity cost of DP training has not been studied, and is currently neither audited for nor communicated to model designers and stakeholders. We derive a bound on the number of re-trainings required to estimate predictive multiplicity reliably. We analyze--both theoretically and through extensive experiments--the predictive-multiplicity cost of three DP-ensuring
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25935;&#25463;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;&#20219;&#20309;&#20027;&#35266;&#35270;&#35273;&#27010;&#24565;&#36716;&#21270;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;30&#20998;&#38047;&#20869;&#36731;&#26494;&#21019;&#24314;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12948</link><description>&lt;p&gt;
&#25935;&#25463;&#24314;&#27169;&#65306;&#20174;&#27010;&#24565;&#21040;&#20998;&#31867;&#22120;&#21482;&#38656;&#20960;&#20998;&#38047;
&lt;/p&gt;
&lt;p&gt;
Agile Modeling: From Concept to Classifier in Minutes. (arXiv:2302.12948v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25935;&#25463;&#24314;&#27169;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;&#20219;&#20309;&#20027;&#35266;&#35270;&#35273;&#27010;&#24565;&#36716;&#21270;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;30&#20998;&#38047;&#20869;&#36731;&#26494;&#21019;&#24314;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#20027;&#35266;&#32454;&#24494;&#24212;&#29992;&#26041;&#38754;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#34429;&#28982;&#20247;&#21253;&#23545;&#20110;&#22823;&#22810;&#25968;&#23458;&#35266;&#20219;&#21153;&#65288;&#22914;&#26631;&#35760;&#8220;&#26001;&#39532;&#8221;&#65289;&#24050;&#32463;&#20026;&#35270;&#35273;&#31038;&#21306;&#26381;&#21153;&#24471;&#24456;&#22909;&#65292;&#20294;&#22312;&#27010;&#24565;&#20855;&#26377;&#23454;&#36136;&#24615;&#20027;&#35266;&#24615;&#30340;&#20219;&#21153;&#65288;&#20363;&#22914;&#35782;&#21035;&#8220;&#32654;&#39135;&#37329;&#26538;&#40060;&#8221;&#65289;&#19978;&#65292;&#23427;&#29616;&#22312;&#38754;&#20020;&#22833;&#36133;&#12290;&#28982;&#32780;&#65292;&#35753;&#20219;&#20309;&#29992;&#25143;&#24320;&#21457;&#20854;&#27010;&#24565;&#30340;&#20998;&#31867;&#22120;&#22312;&#25216;&#26415;&#19978;&#26159;&#22256;&#38590;&#30340;&#65306;&#29992;&#25143;&#26082;&#19981;&#26159;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#65292;&#20063;&#27809;&#26377;&#32784;&#24515;&#26631;&#35760;&#25968;&#21315;&#20010;&#31034;&#20363;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25935;&#25463;&#24314;&#27169;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#23454;&#26102;&#29992;&#25143;&#21442;&#19982;&#23558;&#20219;&#20309;&#20027;&#35266;&#35270;&#35273;&#27010;&#24565;&#36716;&#21270;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20026;&#22270;&#20687;&#20998;&#31867;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#25935;&#25463;&#24314;&#27169;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65288;N=14&#65289;&#34920;&#26126;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;30&#20998;&#38047;&#20869;&#36731;&#26494;&#21019;&#24314;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#29992;&#25143;&#39537;&#21160;&#30340;&#36807;&#31243;&#19982;&#20256;&#32479;&#30340;&#20247;&#21253;&#33539;&#24335;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#21457;&#29616;&#32676;&#20307;&#30340;&#35266;&#24565;&#24120;&#24120;&#19982;&#29992;&#25143;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of computer vision to nuanced subjective use cases is growing. While crowdsourcing has served the vision community well for most objective tasks (such as labeling a "zebra"), it now falters on tasks where there is substantial subjectivity in the concept (such as identifying "gourmet tuna"). However, empowering any user to develop a classifier for their concept is technically difficult: users are neither machine learning experts, nor have the patience to label thousands of examples. In reaction, we introduce the problem of Agile Modeling: the process of turning any subjective visual concept into a computer vision model through a real-time user-in-the-loop interactions. We instantiate an Agile Modeling prototype for image classification and show through a user study (N=14) that users can create classifiers with minimal effort under 30 minutes. We compare this user driven process with the traditional crowdsourcing paradigm and find that the crowd's notion often differs fro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Metropolis-adjusted Langevin&#31639;&#27861;&#26469;&#22788;&#29702;&#24102;&#26377;&#32422;&#26463;&#26465;&#20214;&#30340;&#30446;&#26631;&#20998;&#24067;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#26102;&#25928;&#26524;&#20248;&#20110;&#31454;&#20105;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.11971</link><description>&lt;p&gt;
&#29992;Metropolis-adjusted Langevin&#31639;&#27861;&#39640;&#25928;&#22788;&#29702;&#32422;&#26463;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Efficiently handling constraints with Metropolis-adjusted Langevin algorithm. (arXiv:2302.11971v2 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Metropolis-adjusted Langevin&#31639;&#27861;&#26469;&#22788;&#29702;&#24102;&#26377;&#32422;&#26463;&#26465;&#20214;&#30340;&#30446;&#26631;&#20998;&#24067;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#31639;&#27861;&#22312;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#26102;&#25928;&#26524;&#20248;&#20110;&#31454;&#20105;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#30446;&#26631;&#20998;&#24067;&#25903;&#25745;&#38598;&#19978;&#26045;&#21152;&#32422;&#26463;&#26465;&#20214;&#26102;&#65292;Metropolis-adjusted Langevin&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#20854;&#24471;&#20986;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#35777;&#26126;&#20854;&#25910;&#25947;&#24615;&#24182;&#25512;&#23548;&#20102;&#20854;&#28151;&#21512;&#26102;&#38388;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Metropolis-adjusted Langevin&#31639;&#27861;&#22312;&#22788;&#29702;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65306;&#25105;&#20204;&#24471;&#21040;&#30340;&#28151;&#21512;&#26102;&#38388;&#19978;&#30028;&#20248;&#20110;&#26080;&#25509;&#21463;-&#25298;&#32477;&#27493;&#39588;&#30340;&#31454;&#20105;&#31639;&#27861;&#30340;&#26368;&#20339;&#24050;&#30693;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#25903;&#25345;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#65292;&#34920;&#26126;Metropolis-adjusted Langevin&#31639;&#27861;&#22312;&#22788;&#29702;&#30446;&#26631;&#20998;&#24067;&#25903;&#25745;&#38598;&#32422;&#26463;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate the performance of the Metropolis-adjusted Langevin algorithm in a setting with constraints on the support of the target distribution. We provide a rigorous analysis of the resulting Markov chain, establishing its convergence and deriving an upper bound for its mixing time. Our results demonstrate that the Metropolis-adjusted Langevin algorithm is highly effective in handling this challenging situation: the mixing time bound we obtain is superior to the best known bounds for competing algorithms without an accept-reject step. Our numerical experiments support these theoretical findings, indicating that the Metropolis-adjusted Langevin algorithm shows promising performance when dealing with constraints on the support of the target distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#28145;&#24230;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28145;&#24230;&#26680;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#22312;&#20248;&#21270;&#20013;&#25351;&#23548;&#28145;&#24230;&#26680;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36935;&#21040;&#26032;&#25968;&#25454;&#28857;&#26102;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#32622;&#20449;&#24230;&#65292;&#26082;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#34892;&#20026;&#65292;&#21448;&#20445;&#25345;&#20102;&#28145;&#24230;&#26680;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09574</link><description>&lt;p&gt;
&#24341;&#23548;&#28145;&#24230;&#26680;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Guided Deep Kernel Learning. (arXiv:2302.09574v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#28145;&#24230;&#26680;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28145;&#24230;&#26680;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#22312;&#20248;&#21270;&#20013;&#25351;&#23548;&#28145;&#24230;&#26680;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36935;&#21040;&#26032;&#25968;&#25454;&#28857;&#26102;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#32622;&#20449;&#24230;&#65292;&#26082;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#34892;&#20026;&#65292;&#21448;&#20445;&#25345;&#20102;&#28145;&#24230;&#26680;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#36890;&#24120;&#36890;&#36807;&#28145;&#24230;&#26680;&#23398;&#20064; (DKL) &#23558;&#39640;&#26031;&#36807;&#31243;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#32467;&#21512;&#36215;&#26469;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#26680;&#20248;&#21270;&#36807;&#31243;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#20250;&#22833;&#21435;&#23427;&#20204;&#30340;&#36125;&#21494;&#26031;&#20248;&#21183;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28145;&#24230;&#26680;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243; (NNGP) &#27169;&#22411;&#20316;&#20026; DKl &#27169;&#22411;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992; NNGP &#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20197;&#20351; DKL &#22312;&#36935;&#21040;&#26032;&#25968;&#25454;&#28857;&#26102;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#32622;&#20449;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26082;&#21033;&#29992;&#20102; NNGP &#30340;&#36125;&#21494;&#26031;&#34892;&#20026; (&#21363;&#20854;&#25239;&#36807;&#25311;&#21512;&#24615;&#21644;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;)&#65292;&#21448;&#20445;&#25345;&#20102;&#28145;&#24230;&#26680;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Gaussian processes with the expressive power of deep neural networks is commonly done nowadays through deep kernel learning (DKL). Unfortunately, due to the kernel optimization process, this often results in losing their Bayesian benefits. In this study, we present a novel approach for learning deep kernels by utilizing infinite-width neural networks. We propose to use the Neural Network Gaussian Process (NNGP) model as a guide to the DKL model in the optimization process. Our approach harnesses the reliable uncertainty estimation of the NNGPs to adapt the DKL target confidence when it encounters novel data points. As a result, we get the best of both worlds, we leverage the Bayesian behavior of the NNGP, namely its robustness to overfitting, and accurate uncertainty estimation, while maintaining the generalization abilities, scalability, and flexibility of deep kernels. Empirically, we show on multiple benchmark datasets of varying sizes and dimensionality, that our method i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31934;&#32454;&#30340;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#35270;&#35273;&#20998;&#31867;&#20013;&#21487;&#35270;&#21270;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#26356;&#24544;&#23454;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08508</link><description>&lt;p&gt;
&#21407;&#22411;&#22270;&#20687;&#20998;&#31867;&#20013;&#34917;&#19969;&#21487;&#35270;&#21270;&#30340;&#21512;&#29702;&#24615;&#26816;&#26597;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Sanity checks and improvements for patch visualisation in prototype-based image classification. (arXiv:2302.08508v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31934;&#32454;&#30340;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;&#35270;&#35273;&#20998;&#31867;&#20013;&#21487;&#35270;&#21270;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#26356;&#24544;&#23454;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#21407;&#22411;&#30340;&#35270;&#35273;&#20998;&#31867;&#20013;&#23454;&#26045;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20351;&#29992;&#20004;&#20010;&#31934;&#32454;&#30340;&#25968;&#25454;&#38598;&#65288;CUB-200-2011 &#21644; Stanford Cars&#65289;&#39318;&#20808;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#33021;&#27491;&#30830;&#22320;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#22240;&#27492;&#19981;&#33021;&#21453;&#26144;&#20986;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#21024;&#38500;&#24230;&#37327;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#35777;&#26126;&#20102; Smoothgrads &#25110; PRP &#31561;&#26174;&#33879;&#24615;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#24544;&#23454;&#30340;&#22270;&#20687;&#34917;&#19969;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26576;&#20123;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914; CUB-200-2011&#65289;&#20013;&#25552;&#20379;&#30340;&#23545;&#35937;&#20998;&#21106;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#65292;&#24182;&#23637;&#31034;&#20102; ProtoPNet &#21644; ProtoTree &#20135;&#29983;&#30340;&#19981;&#31934;&#30830;&#30340;&#34917;&#19969;&#21487;&#35270;&#21270;&#21487;&#33021;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#20559;&#35265;&#24863;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26356;&#24544;&#23454;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#20854;&#20182;&#20351;&#29992;&#30456;&#21516;&#21487;&#35270;&#21270;&#26041;&#27861;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we perform an in-depth analysis of the visualisation methods implemented in two popular self-explaining models for visual classification based on prototypes - ProtoPNet and ProtoTree. Using two fine-grained datasets (CUB-200-2011 and Stanford Cars), we first show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour. Secondly, using a deletion metric, we demonstrate quantitatively that saliency methods such as Smoothgrads or PRP provide more faithful image patches. We also propose a new relevance metric based on the segmentation of the object provided in some datasets (e.g. CUB-200-2011) and show that the imprecise patch visualisations generated by ProtoPNet and ProtoTree can create a false sense of bias that can be mitigated by the use of more faithful methods. Finally, we discuss the implications of our findings for other prototype-based models sharing the same visualisation method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#24314;&#27169;&#26234;&#33021;&#26448;&#26009;&#28382;&#21518;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#23545;&#21387;&#30005;&#26448;&#26009;&#30340;&#28382;&#21518;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#21516;&#26102;&#22312;&#30913;&#24615;&#26448;&#26009;&#26041;&#38754;&#25552;&#20379;&#20102;&#31232;&#30095;&#30333;&#30418;&#24314;&#27169;&#28382;&#21518;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.05313</link><description>&lt;p&gt;
&#26234;&#33021;&#26448;&#26009;&#20013;&#31232;&#30095;&#28382;&#21518;&#27169;&#22411;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discovering sparse hysteresis models for smart materials. (arXiv:2302.05313v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#24314;&#27169;&#26234;&#33021;&#26448;&#26009;&#28382;&#21518;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#23545;&#21387;&#30005;&#26448;&#26009;&#30340;&#28382;&#21518;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#21516;&#26102;&#22312;&#30913;&#24615;&#26448;&#26009;&#26041;&#38754;&#25552;&#20379;&#20102;&#31232;&#30095;&#30333;&#30418;&#24314;&#27169;&#28382;&#21518;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#24314;&#27169;&#26234;&#33021;&#26448;&#26009;&#65288;&#23588;&#20854;&#26159;&#21387;&#30005;&#26448;&#26009;&#65289;&#28382;&#21518;&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#37319;&#29992;&#20102;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#21644;&#39034;&#24207;&#38408;&#20540;&#26041;&#27861;&#23545;&#36127;&#36131;&#28382;&#21518;&#30340;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#24471;&#21040;&#20102;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#27169;&#25311;&#21644;&#23454;&#39564;&#21387;&#30005;&#26448;&#26009;&#25968;&#25454;&#30340;&#28382;&#21518;&#29616;&#35937;&#12290;&#25991;&#31456;&#36824;&#27169;&#25311;&#20102;&#19981;&#21516;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#21253;&#25324;&#23398;&#20064;&#34676;&#34678;&#24418;&#28382;&#21518;&#12289;&#23545;&#21387;&#30005;&#33268;&#21160;&#22120;&#30340;&#30495;&#23454;&#28382;&#21518;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#36824;&#36890;&#36807;&#20197;&#38750;&#21462;&#21521;&#30005;&#24037;&#38050;&#20026;&#20363;&#65292;&#25552;&#20379;&#20102;&#23545;&#30913;&#24615;&#26448;&#26009;&#31232;&#30095;&#30333;&#30418;&#24314;&#27169;&#28382;&#21518;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents an approach for modelling hysteresis in smart materials, specifically piezoelectric materials, that leverages recent advancements in machine learning, particularly in sparse-regression techniques. While sparse regression has previously been used to model various scientific and engineering phenomena, its application to nonlinear hysteresis modelling in piezoelectric materials has yet to be explored. The study employs the least-squares algorithm with a sequential threshold to model the dynamic system responsible for hysteresis, resulting in a concise model that accurately predicts hysteresis for both simulated and experimental piezoelectric material data. Several numerical experiments are performed, including learning butterfly-shaped hysteresis and modelling real-world hysteresis data for a piezoelectric actuator. Additionally, insights are provided on sparse white-box modelling of hysteresis for magnetic materials taking non-oriented electrical steel as an example
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;</title><link>http://arxiv.org/abs/2302.05045</link><description>&lt;p&gt;
&#21033;&#29992;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31232;&#30095;&#24615;&#26469;&#20248;&#21270;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training. (arXiv:2302.05045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach that exploits sparse subnetworks to optimize memory utilization and communication in two popular algorithms for parallel deep learning, and demonstrates significant reductions in memory consumption and communication time on a 2.7 billion parameter model using 512 NVIDIA V100 GPUs.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36890;&#20449;&#24320;&#38144;&#30340;&#26174;&#33879;&#22686;&#21152;&#65292;&#35268;&#27169;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#35757;&#32451;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#21098;&#26525;&#31639;&#27861;&#65292;&#33021;&#22815;&#21098;&#26525;&#65288;&#21363;&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#65289;80-90&#65285;&#30340;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#19982;&#26410;&#21098;&#26525;&#29238;&#32593;&#32476;&#30456;&#31561;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;AxoNN&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20381;&#36182;&#20110;&#25968;&#25454;&#21644;&#23618;&#38388;&#24182;&#34892;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#20449;&#26102;&#38388;&#21644;&#20869;&#23384;&#21033;&#29992;&#30340;&#20943;&#23569;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26657;&#20934;&#35780;&#20272;&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;&#26657;&#20934;&#20998;&#25968;&#35774;&#35745;&#20013;&#30340;&#19981;&#21516;&#36873;&#25321;&#65292;&#24182;&#30740;&#31350;&#20102;&#26681;&#25454;&#36755;&#20837;&#29305;&#24449;&#32780;&#19981;&#26159;&#39044;&#27979;&#32467;&#26524;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#20998;&#32452;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#24110;&#21161;&#21046;&#23450;&#20855;&#26377;&#29702;&#24819;&#25968;&#23398;&#29305;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.04118</link><description>&lt;p&gt;
&#35770;&#26657;&#20934;&#30340;&#31934;&#32454;&#24230;
&lt;/p&gt;
&lt;p&gt;
On the Richness of Calibration. (arXiv:2302.04118v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26657;&#20934;&#35780;&#20272;&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;&#26657;&#20934;&#20998;&#25968;&#35774;&#35745;&#20013;&#30340;&#19981;&#21516;&#36873;&#25321;&#65292;&#24182;&#30740;&#31350;&#20102;&#26681;&#25454;&#36755;&#20837;&#29305;&#24449;&#32780;&#19981;&#26159;&#39044;&#27979;&#32467;&#26524;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#20998;&#32452;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#24110;&#21161;&#21046;&#23450;&#20855;&#26377;&#29702;&#24819;&#25968;&#23398;&#29305;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;&#35266;&#27979;&#26631;&#31614;&#39057;&#29575;&#30340;&#27604;&#36739;&#65292;&#21363;&#36890;&#36807;&#26657;&#20934;&#30340;&#26041;&#27861;&#21487;&#20197;&#35780;&#20272;&#27010;&#29575;&#39044;&#27979;&#65292;&#26368;&#36817;&#65292;&#31639;&#27861;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#23398;&#32773;&#24320;&#22987;&#30740;&#31350;&#19968;&#20010;&#21517;&#20026;&#22810;&#26657;&#20934;&#30340;&#26657;&#20934;&#22522;&#20934;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#22810;&#26679;&#24615;&#65292;&#20294;&#36824;&#26159;&#27604;&#36739;&#23616;&#38480;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26126;&#30830;&#35774;&#35745;&#26657;&#20934;&#24230;&#37327;&#26102;&#28041;&#21450;&#30340;&#36873;&#25321;&#65292;&#25506;&#32034;&#21644;&#20998;&#26512;&#20102;&#26657;&#20934;&#35780;&#20272;&#30340;&#21508;&#31181;&#24418;&#24335;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#36873;&#25321;&#20998;&#20026;&#19977;&#20010;&#20998;&#32452;&#36873;&#25321;&#21644;&#19968;&#20010;&#20851;&#20110;&#32452;&#38169;&#35823;&#21512;&#24182;&#30340;&#36873;&#25321;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#27604;&#36739;&#20197;&#21069;&#25552;&#20986;&#30340;&#26657;&#20934;&#20998;&#25968;&#65292;&#24182;&#26377;&#21161;&#20110;&#21046;&#23450;&#20855;&#26377;&#29702;&#24819;&#25968;&#23398;&#29305;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26681;&#25454;&#36755;&#20837;&#29305;&#24449;&#32780;&#19981;&#26159;&#39044;&#27979;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#20998;&#32452;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#27491;&#24335;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#36824;&#23545;&#36866;&#21512;&#30340;&#32452;&#38169;&#35823;&#21512;&#24182;&#20989;&#25968;&#30340;&#31354;&#38388;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic predictions can be evaluated through comparisons with observed label frequencies, that is, through the lens of calibration. Recent scholarship on algorithmic fairness has started to look at a growing variety of calibration-based objectives under the name of multi-calibration but has still remained fairly restricted. In this paper, we explore and analyse forms of evaluation through calibration by making explicit the choices involved in designing calibration scores. We organise these into three grouping choices and a choice concerning the agglomeration of group errors. This provides a framework for comparing previously proposed calibration scores and helps to formulate novel ones with desirable mathematical properties. In particular, we explore the possibility of grouping datapoints based on their input features rather than on predictions and formally demonstrate advantages of such approaches. We also characterise the space of suitable agglomeration functions for group erro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#22238;&#39038;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.03122</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
State-wise Safe Reinforcement Learning: A Survey. (arXiv:2302.03122v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#22238;&#39038;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20173;&#28982;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#23433;&#20840;&#24615;&#65292;&#20063;&#23601;&#26159;&#32422;&#26463;&#28385;&#36275;&#12290;&#29366;&#24577;&#32422;&#26463;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#26368;&#24120;&#35265;&#19988;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#32422;&#26463;&#20043;&#19968;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#32780;&#35328;&#26159;&#24517;&#35201;&#21644;&#20851;&#38190;&#30340;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#35299;&#20915;&#22522;&#20110;&#29366;&#24577;&#30340;&#32422;&#26463;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#29366;&#24577;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#19979;&#65292;&#20174;&#23433;&#20840;&#20445;&#38556;&#21644;&#21487;&#25193;&#23637;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#22870;&#21169;&#34920;&#29616;&#12289;&#25910;&#25947;&#21518;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#65292;&#35752;&#35770;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32852;&#31995;&#12289;&#24046;&#24322;&#21644;&#26435;&#34913;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potentia
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#65292;&#25552;&#20986;&#22235;&#20010;&#35823;&#24046;&#37096;&#20998;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;169&#20010;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20026;SSL&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.03068</link><description>&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03068
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#65292;&#25552;&#20986;&#22235;&#20010;&#35823;&#24046;&#37096;&#20998;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;169&#20010;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20026;SSL&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#27969;&#31243;&#35774;&#35745;&#28041;&#21450;&#26550;&#26500;&#12289;&#22686;&#24378;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#31561;&#35832;&#22810;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;SSL&#36890;&#24120;&#20351;&#29992;&#21333;&#19968;&#24230;&#37327;&#26469;&#35780;&#20272;&#65292;&#36825;&#24182;&#19981;&#33021;&#25552;&#20379;&#28145;&#20837;&#30340;&#27934;&#23519;&#21644;&#25913;&#36827;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SSL&#39118;&#38505;&#20998;&#35299;&#65292;&#20174;&#36924;&#36817;&#12289;&#34920;&#31034;&#21487;&#29992;&#24615;&#12289;&#25506;&#38024;&#27867;&#21270;&#21644;&#32534;&#30721;&#22120;&#27867;&#21270;&#31561;&#35282;&#24230;&#23545;&#38169;&#35823;&#36827;&#34892;&#20998;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;30&#20010;&#35774;&#35745;&#36873;&#25321;&#23545;169&#20010;&#22312;ImageNet&#19978;&#35780;&#20272;&#30340;SSL&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#20026;&#27599;&#20010;&#32452;&#20214;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#65292;&#20026;SSL&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. Yet SSL is typically evaluated using a single metric: linear probing on ImageNet. This does not provide much insight into why or when a model is better, now how to improve it. To address this, we propose an SSL risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. Our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main sources of error and shows how to improve SSL in specific settings (full- vs 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#26367;&#20195;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#30830;&#20445;&#39044;&#27979;&#30340;&#36712;&#36857;&#22312;&#21160;&#21147;&#23398;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#21516;&#26102;&#36890;&#36807;&#26500;&#24314;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33258;&#20027;&#36187;&#36710;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#31934;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.01060</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#32422;&#26463;&#19979;&#24102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Physics Constrained Motion Prediction with Uncertainty Quantification. (arXiv:2302.01060v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#26367;&#20195;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#30830;&#20445;&#39044;&#27979;&#30340;&#36712;&#36857;&#22312;&#21160;&#21147;&#23398;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#21516;&#26102;&#36890;&#36807;&#26500;&#24314;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33258;&#20027;&#36187;&#36710;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#39044;&#27979;&#31934;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21160;&#24577;&#29289;&#20307;&#30340;&#36816;&#21160;&#23545;&#20110;&#20445;&#35777;&#33258;&#20027;&#31995;&#32479;&#30340;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#25361;&#25112;&#26159;&#36816;&#21160;&#39044;&#27979;&#31639;&#27861;&#24212;&#36981;&#23432;&#21160;&#21147;&#23398;&#32422;&#26463;&#65292;&#24182;&#23558;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20026;&#32622;&#20449;&#24230;&#30340;&#34913;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#26367;&#20195;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#30830;&#20445;&#39044;&#27979;&#30340;&#36712;&#36857;&#22312;&#21160;&#21147;&#23398;&#19978;&#26159;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#38598;&#25104;&#36807;&#31243;&#65292;&#21253;&#25324;&#24847;&#22270;&#39044;&#27979;&#21644;&#36712;&#36857;&#39044;&#27979;&#65292;&#21516;&#26102;&#28385;&#36275;&#20102;&#21160;&#21147;&#23398;&#32422;&#26463;&#12290;&#36890;&#36807;&#20351;&#29992;&#27969;&#34892;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#33258;&#20027;&#36187;&#36710;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#29289;&#29702;&#23398;&#32422;&#26463;&#19979;&#24102;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#36816;&#21160;&#39044;&#27979;&#23454;&#29616;&#20102;41%&#26356;&#22909;&#30340;ADE&#65292;56%&#26356;&#22909;&#30340;FDE&#21644;19%&#26356;&#22909;&#30340;IoU&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the motion of dynamic agents is a critical task for guaranteeing the safety of autonomous systems. A particular challenge is that motion prediction algorithms should obey dynamics constraints and quantify prediction uncertainty as a measure of confidence. We present a physics-constrained approach for motion prediction which uses a surrogate dynamical model to ensure that predicted trajectories are dynamically feasible. We propose a two-step integration consisting of intent and trajectory prediction subject to dynamics constraints. We also construct prediction regions that quantify uncertainty and are tailored for autonomous driving by using conformal prediction, a popular statistical tool. Physics Constrained Motion Prediction achieves a 41% better ADE, 56% better FDE, and 19% better IoU over a baseline in experiments using an autonomous racing dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#23884;&#20837;&#20013;&#25237;&#24433;&#20986;&#20559;&#21521;&#26041;&#21521;&#26469;&#26657;&#20934;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#20559;&#24046;&#65292;&#20165;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;&#21435;&#20559;&#25191;&#23601;&#36275;&#20197;&#29983;&#25104;&#20005;&#35880;&#30340;&#20998;&#31867;&#22120;&#21644;&#20844;&#27491;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#31038;&#20132;&#20559;&#35265;&#21644;&#34394;&#20551;&#30456;&#20851;&#65292;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25110;&#22521;&#35757;&#12290;</title><link>http://arxiv.org/abs/2302.00070</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#21521;&#26657;&#20934;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Debiasing Vision-Language Models via Biased Prompts. (arXiv:2302.00070v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#23884;&#20837;&#20013;&#25237;&#24433;&#20986;&#20559;&#21521;&#26041;&#21521;&#26469;&#26657;&#20934;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#20559;&#24046;&#65292;&#20165;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;&#21435;&#20559;&#25191;&#23601;&#36275;&#20197;&#29983;&#25104;&#20005;&#35880;&#30340;&#20998;&#31867;&#22120;&#21644;&#20844;&#27491;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#31038;&#20132;&#20559;&#35265;&#21644;&#34394;&#20551;&#30456;&#20851;&#65292;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25110;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20250;&#20174;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#32487;&#25215;&#20559;&#35265;&#12290;&#23545;&#20110;&#20174;&#20114;&#32852;&#32593;&#19978;&#29228;&#21462;&#30340;&#26410;&#21152;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#20250;&#29305;&#21035;&#20855;&#26377;&#38382;&#39064;&#12290;&#20559;&#35265;&#21487;&#33021;&#20250;&#34987;&#25918;&#22823;&#24182;&#20256;&#25773;&#21040;&#20854;&#20182;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25991;&#26412;&#23884;&#20837;&#20013;&#25237;&#24433;&#20986;&#20559;&#21521;&#26041;&#21521;&#26469;&#26657;&#20934;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#25237;&#24433;&#30697;&#38453;&#23545;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#21435;&#20559;&#25191;&#23601;&#36275;&#20197;&#29983;&#25104;&#20005;&#35880;&#30340;&#20998;&#31867;&#22120;&#21644;&#20844;&#27491;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#20351;&#24471;&#26131;&#20110;&#22312;&#22823;&#35268;&#27169;&#31649;&#36947;&#20013;&#36827;&#34892;&#38598;&#25104;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#21306;&#20998;&#24615;&#21644;&#29983;&#25104;&#24615;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20132;&#20559;&#35265;&#21644;&#34394;&#20551;&#30456;&#20851;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#25110;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#23454;&#29992;&#30340;&#26041;&#27861;{\sc LegendreTron}&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#22810;&#31867;&#21035;&#38382;&#39064;&#30340;&#27491;&#30830;&#26631;&#20934;&#25439;&#22833;&#21644;&#27010;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#32463;&#24120;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.11695</link><description>&lt;p&gt;
LegendreTron&#65306;&#21319;&#32423;&#29256;&#22810;&#31867;&#21035;&#27491;&#30830;&#22810;&#39033;&#25439;&#22833;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LegendreTron: Uprising Proper Multiclass Loss Learning. (arXiv:2301.11695v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#23454;&#29992;&#30340;&#26041;&#27861;{\sc LegendreTron}&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#22810;&#31867;&#21035;&#38382;&#39064;&#30340;&#27491;&#30830;&#26631;&#20934;&#25439;&#22833;&#21644;&#27010;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#32463;&#24120;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25439;&#22833;&#20989;&#25968;&#26159;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#36890;&#24120;&#22312;&#27169;&#22411;&#24320;&#21457;&#20043;&#21069;&#36873;&#25321;&#12290;&#20026;&#36991;&#20813;&#36873;&#25321;&#25439;&#22833;&#20989;&#25968;&#21487;&#33021;&#20986;&#29616;&#30340;&#29305;&#23450;&#36873;&#25321;&#65292;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#25551;&#36848;&#20102;&#25439;&#22833;&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#31216;&#20026;&#8220;&#27491;&#30830;&#24615;&#8221;&#65292;&#23427;&#26029;&#35328;&#36125;&#21494;&#26031;&#35268;&#21017;&#26159;&#26368;&#20248;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#32852;&#21512;&#23398;&#20064;&#25439;&#22833;&#21644;&#27169;&#22411;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#25311;&#21512;&#19968;&#20010;&#23558;$\mathbb{R}$&#21333;&#35843;&#26144;&#23556;&#21040;$[0,1]$&#30340;&#21453;&#35299;&#26631;&#20934;&#38142;&#25509;&#20989;&#25968;&#26469;&#20272;&#35745;&#20108;&#20803;&#38382;&#39064;&#30340;&#27010;&#29575;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20984;&#20989;&#25968;&#26799;&#24230;&#30340;&#21333;&#35843;&#24615;&#23558;&#21333;&#35843;&#24615;&#25193;&#23637;&#21040;$\mathbb{R}^{C-1}$&#21040;&#27010;&#29575;&#30340;&#27491;&#25237;&#24433;$\tilde{\Delta}^{C-1}$&#30340;&#26144;&#23556;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#23454;&#29992;&#30340;&#26041;&#27861;{\sc LegendreTron}&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#22810;&#31867;&#21035;&#38382;&#39064;&#30340;&#27491;&#30830;&#26631;&#20934;&#25439;&#22833;&#21644;&#27010;&#29575;&#12290;&#22312;&#26368;&#22810;1,000&#31181;&#31867;&#21035;&#30340;&#39046;&#22495;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loss functions serve as the foundation of supervised learning and are often chosen prior to model development. To avoid potentially ad hoc choices of losses, statistical decision theory describes a desirable property for losses known as \emph{properness}, which asserts that Bayes' rule is optimal. Recent works have sought to \emph{learn losses} and models jointly. Existing methods do this by fitting an inverse canonical link function which monotonically maps $\mathbb{R}$ to $[0,1]$ to estimate probabilities for binary problems. In this paper, we extend monotonicity to maps between $\mathbb{R}^{C-1}$ and the projected probability simplex $\tilde{\Delta}^{C-1}$ by using monotonicity of gradients of convex functions. We present {\sc LegendreTron} as a novel and practical method that jointly learns \emph{proper canonical losses} and probabilities for multiclass problems. Tested on a benchmark of domains with up to 1,000 classes, our experimental results show that our method consistently ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25429;&#33719;&#26680;&#20381;&#36182;&#20851;&#31995;&#30340;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#21644;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#20381;&#36182;&#24615;&#30340;&#25968;&#25454;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.09870</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Context-specific kernel-based hidden Markov model for time series analysis. (arXiv:2301.09870v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25429;&#33719;&#26680;&#20381;&#36182;&#20851;&#31995;&#30340;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#21644;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#20381;&#36182;&#24615;&#30340;&#25968;&#25454;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26159;&#29702;&#35299;&#21644;&#24314;&#27169;&#38543;&#26426;&#21160;&#24577;&#25968;&#25454;&#30340;&#26377;&#29992;&#24037;&#20855;&#65307;&#22312;&#38750;&#39640;&#26031;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#31867;&#20284;&#39640;&#26031;&#28151;&#21512;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#21463;&#21040;&#31934;&#24230;&#30697;&#38453;&#30340;&#35745;&#31639;&#20197;&#21450;&#20855;&#26377;&#24456;&#22810;&#19981;&#24517;&#35201;&#30340;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#20551;&#23450;&#25152;&#26377;&#21464;&#37327;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#20063;&#33021;&#22815;&#24314;&#27169;&#38750;&#39640;&#26031;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#20551;&#23450;&#21464;&#37327;&#20043;&#38388;&#26159;&#29420;&#31435;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#26032;&#22411;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#19978;&#19979;&#25991;&#30456;&#20851;&#36125;&#21494;&#26031;&#32593;&#32476;&#25429;&#33719;&#26680;&#20381;&#36182;&#20851;&#31995;&#12290;&#20171;&#32461;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#21450;&#20854;&#22522;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#23558;&#35813;&#27169;&#22411;&#19982;&#30456;&#20851;&#30340;HMM&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional hidden Markov models have been a useful tool to understand and model stochastic dynamic data; in the case of non-Gaussian data, models such as mixture of Gaussian hidden Markov models can be used. However, these suffer from the computation of precision matrices and have a lot of unnecessary parameters. As a consequence, such models often perform better when it is assumed that all variables are independent, a hypothesis that may be unrealistic. Hidden Markov models based on kernel density estimation are also capable of modeling non-Gaussian data, but they assume independence between variables. In this article, we introduce a new hidden Markov model based on kernel density estimation, which is capable of capturing kernel dependencies using context-specific Bayesian networks. The proposed model is described, together with a learning algorithm based on the expectation-maximization algorithm. Additionally, the model is compared to related HMMs on synthetic and real data. From th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#36731;&#37327;&#32423;&#27169;&#22411;&#24182;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#37096;&#32626;&#65292;&#21516;&#26102;&#23454;&#29616;&#23545;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#31934;&#32454;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2301.08143</link><description>&lt;p&gt;
&#32852;&#37030;&#25512;&#33616;&#20013;&#30340;&#21452;&#37325;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dual Personalization on Federated Recommendation. (arXiv:2301.08143v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#25512;&#33616;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#36731;&#37327;&#32423;&#27169;&#22411;&#24182;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#37096;&#32626;&#65292;&#21516;&#26102;&#23454;&#29616;&#23545;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#31934;&#32454;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#25512;&#33616;&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;&#32852;&#37030;&#29615;&#22659;&#19979;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#25512;&#33616;&#26381;&#21153;&#30340;&#26032;&#22411;Internet&#26381;&#21153;&#26550;&#26500;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;&#32452;&#21512;&#20998;&#24067;&#24335;&#25512;&#33616;&#31639;&#27861;&#21644;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#65292;&#22240;&#27492;&#20174;&#26681;&#26412;&#19978;&#37319;&#29992;&#26381;&#21153;&#22120;&#19978;&#30340;&#37325;&#37327;&#32423;&#27169;&#22411;&#65292;&#38459;&#30861;&#20102;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#26234;&#33021;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#25512;&#33616;&#65288;PFedRec&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#35768;&#22810;&#29992;&#25143;&#29305;&#23450;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#20197;&#20415;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#37096;&#32626;&#65292;&#32780;&#19981;&#26159;&#22312;&#26381;&#21153;&#22120;&#19978;&#20351;&#29992;&#37325;&#37327;&#32423;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#20010;&#24615;&#21270;&#26426;&#21046;&#65292;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#32454;&#31890;&#24230;&#20010;&#24615;&#21270;&#12290;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#32852;&#37030;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated recommendation is a new Internet service architecture that aims to provide privacy-preserving recommendation services in federated settings. Existing solutions are used to combine distributed recommendation algorithms and privacy-preserving mechanisms. Thus it inherently takes the form of heavyweight models at the server and hinders the deployment of on-device intelligent models to end-users. This paper proposes a novel Personalized Federated Recommendation (PFedRec) framework to learn many user-specific lightweight models to be deployed on smart devices rather than a heavyweight model on a server. Moreover, we propose a new dual personalization mechanism to effectively learn fine-grained personalization on both users and items. The overall learning process is formulated into a unified federated optimization framework. Specifically, unlike previous methods that share exactly the same item embeddings across users in a federated system, dual personalization allows mild finetuni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#36127;&#36890;&#37327;&#32858;&#21512;&#65288;NeFLAG&#65289;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36755;&#20837;&#29305;&#24449;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#25311;&#21512;&#26367;&#20195;&#27169;&#22411;&#25110;&#36335;&#24452;&#31215;&#20998;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.06989</link><description>&lt;p&gt;
&#29992;&#36127;&#36890;&#37327;&#32858;&#21512;&#20272;&#35745;&#29305;&#24449;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Negative Flux Aggregation to Estimate Feature Attributions. (arXiv:2301.06989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#36127;&#36890;&#37327;&#32858;&#21512;&#65288;NeFLAG&#65289;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36755;&#20837;&#29305;&#24449;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#25311;&#21512;&#26367;&#20195;&#27169;&#22411;&#25110;&#36335;&#24452;&#31215;&#20998;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22686;&#38271;&#20013;&#30340;&#23433;&#20840;&#21644;/&#25110;&#36879;&#26126;&#24230;&#38382;&#39064;&#65292;&#23545;&#20110;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#34892;&#20026;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290; &#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#22810;&#23618;&#38750;&#32447;&#24615;&#65292;&#35299;&#37322;DNN&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#26426;&#21046;&#12290;&#20026;&#20102;&#22686;&#24378;DNN&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#21644;&#36890;&#37327;&#26469;&#20272;&#35745;&#39044;&#27979;&#20219;&#21153;&#30340;&#36755;&#20837;&#29305;&#24449;&#30340;&#24402;&#22240;&#12290;&#21463;&#30690;&#37327;&#20998;&#26512;&#20013;&#30340;&#25955;&#24230;&#23450;&#29702;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#36127;&#36890;&#37327;&#32858;&#21512;&#65288;NeFLAG&#65289;&#20844;&#24335;&#21644;&#26377;&#25928;&#30340;&#36817;&#20284;&#31639;&#27861;&#26469;&#20272;&#35745;&#24402;&#22240;&#22270;&#12290;&#19982;&#20808;&#21069;&#30340;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25311;&#21512;&#26367;&#20195;&#27169;&#22411;&#65292;&#20063;&#19981;&#38656;&#35201;&#26799;&#24230;&#30340;&#36335;&#24452;&#31215;&#20998;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#37117;&#35777;&#26126;&#20102;NeFLAG&#22312;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#24402;&#22240;&#22270;&#26041;&#38754;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; \url{https://git
&lt;/p&gt;
&lt;p&gt;
There are increasing demands for understanding deep neural networks' (DNNs) behavior spurred by growing security and/or transparency concerns. Due to multi-layer nonlinearity of the deep neural network architectures, explaining DNN predictions still remains as an open problem, preventing us from gaining a deeper understanding of the mechanisms. To enhance the explainability of DNNs, we estimate the input feature's attributions to the prediction task using divergence and flux. Inspired by the divergence theorem in vector analysis, we develop a novel Negative Flux Aggregation (NeFLAG) formulation and an efficient approximation algorithm to estimate attribution map. Unlike the previous techniques, ours doesn't rely on fitting a surrogate model nor need any path integration of gradients. Both qualitative and quantitative experiments demonstrate a superior performance of NeFLAG in generating more faithful attribution maps than the competing methods. Our code is available at \url{https://git
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#23558;&#20840;&#29699;&#28418;&#27969;&#22120;&#25968;&#25454;&#38598;&#19982;&#28023;&#27915;&#39044;&#27979;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#21160;&#24577;&#21644;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#28023;&#27915;&#28201;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.05551</link><description>&lt;p&gt;
MPAS-O&#19982;&#20840;&#29699;&#28418;&#27969;&#22120;&#25968;&#25454;&#38598;&#30340;&#21160;&#24577;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Data Assimilation of MPAS-O and the Global Drifter Dataset. (arXiv:2301.05551v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#23558;&#20840;&#29699;&#28418;&#27969;&#22120;&#25968;&#25454;&#38598;&#19982;&#28023;&#27915;&#39044;&#27979;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#21160;&#24577;&#21644;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#28023;&#27915;&#28201;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29616;&#22330;&#28014;&#26631;&#27979;&#37327;&#32467;&#26524;&#19982;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65288;ESMs&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#28023;&#27915;&#28201;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#25216;&#26415;&#21033;&#29992;ESMs&#20013;&#30830;&#23450;&#30340;&#21160;&#24577;&#21644;&#27169;&#24335;&#26469;&#25913;&#21892;&#28014;&#26631;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23395;&#33410;&#24615;&#31561;&#29305;&#24449;&#12290;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#32416;&#27491;MPAS-O&#27169;&#22411;&#20135;&#29983;&#30340;&#26412;&#22320;&#28201;&#24230;&#39044;&#27979;&#35823;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#25554;&#20540;&#21644;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#23558;&#36328;&#23610;&#24230;&#28023;&#27915;&#39044;&#27979;&#27169;&#22411;&#65288;MPAS-O&#65289;&#19982;&#20840;&#29699;&#28418;&#27969;&#22120;&#35745;&#21010;&#30340;&#29616;&#22330;&#28023;&#27915;&#28014;&#26631;&#25968;&#25454;&#38598;&#21516;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a new method for combining in situ buoy measurements with Earth system models (ESMs) to improve the accuracy of temperature predictions in the ocean. The technique utilizes the dynamics and modes identified in ESMs to improve the accuracy of buoy measurements while still preserving features such as seasonality. Using this technique, errors in localized temperature predictions made by the MPAS-O model can be corrected. We demonstrate that our approach improves accuracy compared to other interpolation and data assimilation methods. We apply our method to assimilate the Model for Prediction Across Scales Ocean component (MPAS-O) with the Global Drifter Program's in-situ ocean buoy dataset.
&lt;/p&gt;</description></item><item><title>gRoMA&#26159;&#19968;&#31181;&#34913;&#37327;DNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#21019;&#26032;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#35780;&#20272;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36973;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#35813;&#24037;&#20855;&#21487;&#36816;&#34892;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#24182;&#23545;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20135;&#29983;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.02288</link><description>&lt;p&gt;
gRoMA: &#19968;&#31181;&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
gRoMA: a Tool for Measuring Deep Neural Networks Global Robustness. (arXiv:2301.02288v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02288
&lt;/p&gt;
&lt;p&gt;
gRoMA&#26159;&#19968;&#31181;&#34913;&#37327;DNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#21019;&#26032;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#35780;&#20272;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36973;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#35813;&#24037;&#20855;&#21487;&#36816;&#34892;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#19978;&#65292;&#24182;&#23545;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20135;&#29983;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#21069;&#27839;&#25216;&#26415;&#30340;&#20195;&#34920;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#65288;&#22914;&#33322;&#31354;&#25110;&#27773;&#36710;&#39046;&#22495;&#65289;&#26102;&#65292;&#30001;&#20110;&#23545;&#25239;&#24615;&#36755;&#20837;&#65288;&#21363;&#21487;&#33021;&#23548;&#33268;DNN&#29359;&#38169;&#30340;&#36755;&#20837;&#25200;&#21160;&#65289;&#30340;&#23041;&#32961;&#65292;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#21363;&#20415;&#26159;&#29616;&#20195;DNN&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#24517;&#39035;&#27979;&#37327;&#24182;&#38477;&#20302;&#36825;&#31181;&#39118;&#38505;&#25165;&#33021;&#22312;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#37096;&#32626;DNN&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;gRoMA&#65288;&#20840;&#23616;&#40065;&#26834;&#24615;&#27979;&#37327;&#21644;&#35780;&#20272;&#65289;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#27010;&#29575;&#39564;&#35777;&#26041;&#27861;&#26469;&#27979;&#37327;DNN&#30340;&#20840;&#23616;&#20998;&#31867;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;gRoMA&#27979;&#37327;&#29305;&#23450;&#36755;&#20986;&#31867;&#21035;&#36935;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#65292;&#20135;&#29983;&#25972;&#20010;&#27169;&#22411;&#21644;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#27979;&#37327;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;DNN&#22312;&#28909;&#38376;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#40065;&#26834;&#24615;&#24182;&#20998;&#26512;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are at the forefront of cutting-edge technology, and have been achieving remarkable performance in a variety of complex tasks. Nevertheless, their integration into safety-critical systems, such as in the aerospace or automotive domains, poses a significant challenge due to the threat of adversarial inputs: perturbations in inputs that might cause the DNN to make grievous mistakes. Multiple studies have demonstrated that even modern DNNs are susceptible to adversarial inputs; and this risk must thus be measured and mitigated to allow the deployment of DNNs in safety-critical systems.  Here, we present gRoMA (global Robustness Measurement and Assessment), an innovative and scalable tool that implements a probabilistic verification approach to measure the global categorial robustness of a DNN. Specifically, gRoMA measures the probability of encountering adversarial inputs for a specific output category. Our tool operates on pre-trained, black-box classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#20102;&#36873;&#25321;&#20381;&#36182;&#20998;&#31867;&#65288;SDC&#65289;&#21464;&#20307;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#28436;&#31034;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#26080;&#35823;&#20294;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#22810;&#31181;&#38169;&#35823;&#27169;&#24335;&#12290;&#35813;&#30740;&#31350;&#20026;&#35780;&#20272;SDC&#27169;&#22411;&#21450;&#20854;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14776</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Interpretability of Attention Networks. (arXiv:2212.14776v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#20102;&#36873;&#25321;&#20381;&#36182;&#20998;&#31867;&#65288;SDC&#65289;&#21464;&#20307;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#28436;&#31034;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#26080;&#35823;&#20294;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#22810;&#31181;&#38169;&#35823;&#27169;&#24335;&#12290;&#35813;&#30740;&#31350;&#20026;&#35780;&#20272;SDC&#27169;&#22411;&#21450;&#20854;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#22810;&#20010;&#25104;&#21151;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#24819;&#27861;&#65306;&#8220;&#36755;&#20986;&#20165;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#19968;&#20010;&#23567;&#65288;&#20294;&#26410;&#30693;&#65289;&#37096;&#20998;&#8221;&#12290;&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35821;&#35328;&#32763;&#35793;&#31561;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#36890;&#24120;&#26159;&#27491;&#30830;&#30340;&#12290;&#22312;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#32534;&#30721;&#19982;&#36755;&#20986;&#30456;&#20851;&#30340;&#36755;&#20837;&#27573;&#30340;&#20013;&#38388;&#27169;&#22359;&#30340;&#36755;&#20986;&#36890;&#24120;&#34987;&#29992;&#20316;&#31397;&#35270;&#32593;&#32476;&#8220;&#25512;&#29702;&#8221;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#27880;&#24847;&#21147;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#35299;&#20915;&#19968;&#20010;&#21464;&#20307;&#20998;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36873;&#25321;&#20381;&#36182;&#20998;&#31867;&#65288;SDC&#65289;&#65292;&#20174;&#32780;&#26356;&#21152;&#28165;&#26224;&#22320;&#38416;&#36848;&#20102;&#36825;&#31181;&#27010;&#24565;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22810;&#31181;&#38169;&#35823;&#27169;&#24335;&#65292;&#20854;&#20013;&#27880;&#24847;&#21147;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#26080;&#35823;&#20294;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#27169;&#22411;&#22240;&#35757;&#32451;&#32780;&#20986;&#29616;&#12290;&#25105;&#20204;&#36824;&#38416;&#36848;&#20102;&#21487;&#20197;&#21152;&#24378;&#21644;&#20943;&#36731;&#27492;&#34892;&#20026;&#30340;&#22810;&#31181;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#30446;&#26631;&#23450;&#20041;&#20102;&#19968;&#31181;&#23545;&#20110;SDC&#27169;&#22411;&#21450;&#20854;&#35299;&#37322;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanisms form a core component of several successful deep learning architectures, and are based on one key idea: ''The output depends only on a small (but unknown) segment of the input.'' In several practical applications like image captioning and language translation, this is mostly true. In trained models with an attention mechanism, the outputs of an intermediate module that encodes the segment of input responsible for the output is often used as a way to peek into the `reasoning` of the network. We make such a notion more precise for a variant of the classification problem that we term selective dependence classification (SDC) when used with attention model architectures. Under such a setting, we demonstrate various error modes where an attention model can be accurate but fail to be interpretable, and show that such models do occur as a result of training. We illustrate various situations that can accentuate and mitigate this behaviour. Finally, we use our objective def
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#32447;&#24615;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#25214;&#21040;&#20102;&#39044;&#27979;&#21518;&#39564;&#21644;&#36125;&#21494;&#26031;&#27169;&#22411;&#35777;&#25454;&#30340;&#38750;&#28176;&#36817;&#34920;&#36798;&#65292;&#24182;&#36890;&#36807;&#36825;&#20123;&#34920;&#36798;&#24335;&#24471;&#21040;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#32852;&#21512;&#20316;&#29992;&#30340;&#26032;&#22270;&#20687;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#32447;&#24615;&#32593;&#32476;&#22312;&#26080;&#38480;&#28145;&#24230;&#26102;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#39044;&#27979;&#65292;&#24182;&#25512;&#23548;&#20102;&#26377;&#38480;&#32593;&#32476;&#30340;&#23574;&#38160;&#22823;&#20559;&#24046;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2212.14457</link><description>&lt;p&gt;
&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Bayesian Interpolation with Deep Linear Networks. (arXiv:2212.14457v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#32447;&#24615;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#29702;&#25214;&#21040;&#20102;&#39044;&#27979;&#21518;&#39564;&#21644;&#36125;&#21494;&#26031;&#27169;&#22411;&#35777;&#25454;&#30340;&#38750;&#28176;&#36817;&#34920;&#36798;&#65292;&#24182;&#36890;&#36807;&#36825;&#20123;&#34920;&#36798;&#24335;&#24471;&#21040;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#32852;&#21512;&#20316;&#29992;&#30340;&#26032;&#22270;&#20687;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#32447;&#24615;&#32593;&#32476;&#22312;&#26080;&#38480;&#28145;&#24230;&#26102;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#39044;&#27979;&#65292;&#24182;&#25512;&#23548;&#20102;&#26377;&#38480;&#32593;&#32476;&#30340;&#23574;&#38160;&#22823;&#20559;&#24046;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#22914;&#20309;&#20849;&#21516;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#26159;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#32447;&#24615;&#32593;&#32476;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20855;&#26377;&#39640;&#26031;&#26435;&#37325;&#20808;&#39564;&#21644;&#24179;&#22343;&#24179;&#26041;&#35823;&#24046;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#23545;&#21333;&#36755;&#20986;&#32500;&#24230;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#32593;&#32476;&#28145;&#24230;&#21644;&#38544;&#34255;&#23618;&#23485;&#24230;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#39044;&#27979;&#21518;&#39564;&#21644;&#36125;&#21494;&#26031;&#27169;&#22411;&#35777;&#25454;&#30340;&#38750;&#28176;&#36817;&#34920;&#36798;&#65292;&#36825;&#20123;&#34920;&#36798;&#24335;&#26159;&#19968;&#31867;&#20851;&#20110;Meijer-G&#20989;&#25968;&#30340;&#20122;&#32431;&#29305;&#27530;&#20989;&#25968;&#12290;&#36890;&#36807;&#36825;&#20123;Meijer-G&#20989;&#25968;&#30340;&#26032;&#22411;&#28176;&#36817;&#23637;&#24320;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#32852;&#21512;&#20316;&#29992;&#30340;&#20016;&#23500;&#26032;&#22270;&#20687;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32447;&#24615;&#32593;&#32476;&#22312;&#26080;&#38480;&#28145;&#24230;&#26102;&#21487;&#20197;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#26368;&#20248;&#39044;&#27979;&#65306;&#20855;&#26377;&#25968;&#25454;&#19981;&#21487;&#30693;&#20808;&#39564;&#30340;&#26080;&#38480;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30340;&#21518;&#39564;&#27010;&#29575;&#19982;&#20855;&#26377;&#26368;&#22823;&#21270;&#25968;&#25454;&#20381;&#36182;&#20808;&#39564;&#20449;&#24687;&#30340;&#27973;&#32593;&#32476;&#30340;&#21518;&#39564;&#27010;&#29575;&#30456;&#21516;&#65292;&#19988;&#21518;&#39564;&#27010;&#29575;&#38598;&#20013;&#20110;&#32447;&#24615;&#20989;&#25968;&#12290;&#24403;&#32593;&#32476;&#26159;&#26377;&#38480;&#30340;&#26102;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20102;&#21518;&#39564;&#36317;&#31163;&#32447;&#24615;&#20989;&#25968;&#30340;&#23574;&#38160;&#22823;&#20559;&#24046;&#36793;&#30028;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#36793;&#30028;&#20197;&#39640;&#24230;&#38169;&#32508;&#22797;&#26434;&#30340;&#26041;&#24335;&#21462;&#20915;&#20110;&#32593;&#32476;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#26368;&#21518;&#65292;&#22312;&#22823;&#25968;&#25454;&#38598;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#36125;&#21494;&#26031;&#27169;&#22411;&#35777;&#25454;&#30340;&#28176;&#36817;&#23637;&#24320;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#22266;&#23450;&#23485;&#24230;&#65292;&#35777;&#25454;&#26159;&#28145;&#24230;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22810;&#39033;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Characterizing how neural network depth, width, and dataset size jointly impact model quality is a central problem in deep learning theory. We give here a complete solution in the special case of linear networks with output dimension one trained using zero noise Bayesian inference with Gaussian weight priors and mean squared error as a negative log-likelihood. For any training dataset, network depth, and hidden layer widths, we find non-asymptotic expressions for the predictive posterior and Bayesian model evidence in terms of Meijer-G functions, a class of meromorphic special functions of a single complex variable. Through novel asymptotic expansions of these Meijer-G functions, a rich new picture of the joint role of depth, width, and dataset size emerges. We show that linear networks make provably optimal predictions at infinite depth: the posterior of infinitely deep linear networks with data-agnostic priors is the same as that of shallow networks with evidence-maximizing data-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20026;&#32852;&#37030;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#30340;&#22122;&#22768;&#35780;&#20272;&#38382;&#39064;&#25552;&#20379;&#20102;&#31532;&#19968;&#27425;&#31995;&#32479;&#30740;&#31350;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#23567;&#37327;&#30340;&#22122;&#22768;&#20063;&#20250;&#26174;&#33879;&#24433;&#21709;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20844;&#20849;&#20195;&#29702;&#25968;&#25454;&#26469;&#25552;&#39640;&#35780;&#20272;&#20449;&#21495;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08930</link><description>&lt;p&gt;
&#20851;&#20110;&#32852;&#37030;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#30340;&#22122;&#22768;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
On Noisy Evaluation in Federated Hyperparameter Tuning. (arXiv:2212.08930v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20026;&#32852;&#37030;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#30340;&#22122;&#22768;&#35780;&#20272;&#38382;&#39064;&#25552;&#20379;&#20102;&#31532;&#19968;&#27425;&#31995;&#32479;&#30740;&#31350;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#23567;&#37327;&#30340;&#22122;&#22768;&#20063;&#20250;&#26174;&#33879;&#24433;&#21709;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20844;&#20849;&#20195;&#29702;&#25968;&#25454;&#26469;&#25552;&#39640;&#35780;&#20272;&#20449;&#21495;&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#35843;&#25972;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#32593;&#32476;&#20013;&#36866;&#24403;&#22320;&#36873;&#25321;&#36229;&#21442;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#35268;&#27169;&#12289;&#38544;&#31169;&#21644;&#24322;&#26500;&#24615;&#31561;&#38382;&#39064;&#20250;&#23548;&#33268;&#35843;&#25972;&#36807;&#31243;&#20013;&#20135;&#29983;&#22122;&#22768;&#65292;&#24182;&#20351;&#35780;&#20272;&#21508;&#31181;&#36229;&#21442;&#25968;&#30340;&#24615;&#33021;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;&#32852;&#37030;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#22122;&#22768;&#35780;&#20272;&#24433;&#21709;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#24182;&#20005;&#26684;&#25506;&#32034;&#20851;&#38190;&#22122;&#22768;&#28304;&#65292;&#21253;&#25324;&#23458;&#25143;&#31471;&#23376;&#37319;&#26679;&#12289;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#20197;&#21450;&#25968;&#25454;&#38544;&#31169;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#23567;&#37327;&#30340;&#22122;&#22768;&#20063;&#20250;&#26174;&#33879;&#24433;&#21709;&#35843;&#25972;&#26041;&#27861;&#65292;&#23558;&#23574;&#31471;&#26041;&#27861;&#30340;&#24615;&#33021;&#38477;&#20302;&#21040;&#24188;&#31258;&#30340;&#22522;&#32447;&#27700;&#24179;&#12290;&#38024;&#23545;&#27492;&#31867;&#24773;&#20917;&#20013;&#30340;&#22122;&#22768;&#35780;&#20272;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20844;&#20849;&#20195;&#29702;&#25968;&#25454;&#26469;&#25552;&#39640;&#35780;&#20272;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#35299;&#20915;&#32852;&#37030;&#36229;&#21442;&#25968;&#35843;&#25972;&#20013;&#30340;&#22122;&#22768;&#35780;&#20272;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#33324;&#24615;&#25361;&#25112;&#12289;&#22522;&#32447;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter tuning is critical to the success of federated learning applications. Unfortunately, appropriately selecting hyperparameters is challenging in federated networks. Issues of scale, privacy, and heterogeneity introduce noise in the tuning process and make it difficult to evaluate the performance of various hyperparameters. In this work, we perform the first systematic study on the effect of noisy evaluation in federated hyperparameter tuning. We first identify and rigorously explore key sources of noise, including client subsampling, data and systems heterogeneity, and data privacy. Surprisingly, our results indicate that even small amounts of noise can significantly impact tuning methods-reducing the performance of state-of-the-art approaches to that of naive baselines. To address noisy evaluation in such scenarios, we propose a simple and effective approach that leverages public proxy data to boost the evaluation signal. Our work establishes general challenges, baselines
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2212.07530</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07530
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#20174;&#19981;&#21516;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#21327;&#21516;&#20013;&#33719;&#30410;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#21463;&#21040;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#20808;&#36827;&#26041;&#27861;&#26088;&#22312;&#28040;&#38500;&#24178;&#25200;&#65292;&#20294;&#25105;&#20204;&#23545;&#24178;&#25200;&#29616;&#35937;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#23548;&#33268;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#24178;&#25200;&#65288;&#25110;&#21327;&#21516;&#65289;&#20027;&#35201;&#30001;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#22823;&#23567;&#21644;&#27599;&#20010;&#35821;&#35328;&#23545;&#22312;&#24635;&#25968;&#25454;&#38598;&#20013;&#25152;&#21344;&#27604;&#20363;&#26469;&#20915;&#23450;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#27169;&#22411;&#30456;&#23545;&#20110;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#23567;&#30340;&#26102;&#20505;&#65292;&#20250;&#20986;&#29616;&#20005;&#37325;&#30340;&#24178;&#25200;&#65292;&#32780;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;</title><link>http://arxiv.org/abs/2212.06951</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;AI&#20262;&#29702;: &#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#21306;&#22359;&#38142;&#23433;&#20840;&#20027;&#39064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#20351;&#29992;&#20998;&#24067;&#24335;&#32593;&#32476;&#35753;&#35745;&#31639;&#26426;&#31995;&#32479;&#26356;&#23433;&#20840;&#65292;&#20294;&#24403;&#21069;&#30340;&#21306;&#22359;&#38142;&#35774;&#35745;&#22312;&#20132;&#26131;&#39034;&#24207;&#26041;&#38754;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#30719;&#24037;&#21487;&#20197;&#37325;&#26032;&#25490;&#24207;&#20132;&#26131;&#20197;&#29983;&#25104;&#21033;&#28070;&#65292;&#36825;&#34987;&#31216;&#20026;&#30719;&#24037;&#21487;&#25552;&#21462;&#20215;&#20540;&#65288;MEV&#65289;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#35748;&#20026;MEV&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#30693;&#21517;&#30340;Flashbots&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#20998;&#26512;&#20102;&#21306;&#22359;&#38142;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;MEV&#22312;&#26356;&#24191;&#27867;&#30340;AI&#31038;&#20250;&#20013;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#20840;&#38754;&#20998;&#26512;&#20102;MEV&#25512;&#25991;&#20013;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#36229;&#36807;20,000&#20010;MEV&#21644;Flashbots&#26631;&#31614;&#30340;&#25512;&#25991;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21306;&#22359;&#38142;&#19978;MEV&#27963;&#21160;&#30340;&#20849;&#21516;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#35299;&#37322;&#19982;&#39044;&#27979;&#30340;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#20851;&#31995;&#36828;&#19981;&#22914;&#29702;&#24819;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.06925</link><description>&lt;p&gt;
&#35770;&#35299;&#37322;&#19982;&#39044;&#27979;&#30340;&#20851;&#31995;&#65306;&#19968;&#31181;&#22240;&#26524;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On the Relationship Between Explanation and Prediction: A Causal View. (arXiv:2212.06925v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#29992;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#35299;&#37322;&#19982;&#39044;&#27979;&#30340;&#20851;&#31995;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#20851;&#31995;&#36828;&#19981;&#22914;&#29702;&#24819;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#27169;&#22411;&#20915;&#31574;&#35299;&#37322;&#30340;&#33021;&#21147;&#25104;&#20026;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#12289;&#37096;&#32626;&#21644;&#24212;&#29992;&#30340;&#26680;&#24515;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23578;&#26410;&#29702;&#35299;&#35299;&#37322;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;&#25968;&#25454;&#12289;&#27169;&#22411;&#39044;&#27979;&#12289;&#36229;&#21442;&#25968;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#31561;&#19978;&#28216;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#30340;&#35299;&#37322;&#65311;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#35299;&#37322;&#19982;&#39044;&#27979;&#20043;&#38388;&#20851;&#31995;&#36739;&#23567;&#30340;&#25285;&#24551;&#65292;&#20294;&#32570;&#20047;&#30830;&#23450;&#24615;&#30340;&#30740;&#31350;&#26469;&#37327;&#21270;&#36825;&#31181;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20511;&#37492;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#36825;&#31181;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#24178;&#39044;&#35299;&#37322;&#21644;&#39044;&#27979;&#30340;&#22240;&#26524;&#31062;&#20808;&#65292;&#22312;&#20351;&#29992;&#20197;&#26174;&#30524;&#24230;&#20026;&#22522;&#30784;&#30340;&#35299;&#37322;&#25110;&#39044;&#27979;&#26102;&#23545;&#36229;&#21442;&#25968;&#21644;&#36755;&#20837;&#36827;&#34892;&#27979;&#37327;&#65292;&#26469;&#30740;&#31350;&#35299;&#37322;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#31995;&#36828;&#38750;&#29702;&#24819;&#12290;&#20107;&#23454;&#19978;&#65292;&#8220;&#29702;&#24819;&#8221;&#24773;&#20917;&#19979;&#30340;&#24046;&#36317;&#21482;&#20250;&#22312;&#26356;&#39640;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being able to provide explanations for a model's decision has become a central requirement for the development, deployment, and adoption of machine learning models. However, we are yet to understand what explanation methods can and cannot do. How do upstream factors such as data, model prediction, hyperparameters, and random initialization influence downstream explanations? While previous work raised concerns that explanations (E) may have little relationship with the prediction (Y), there is a lack of conclusive study to quantify this relationship. Our work borrows tools from causal inference to systematically assay this relationship. More specifically, we study the relationship between E and Y by measuring the treatment effect when intervening on their causal ancestors, i.e., on hyperparameters and inputs used to generate saliency-based Es or Ys. Our results suggest that the relationships between E and Y is far from ideal. In fact, the gap between 'ideal' case only increase in higher
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22870;&#21169;&#32806;&#21512;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2212.06357</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#20998;&#24067;&#24335;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable and Sample Efficient Distributed Policy Gradient Algorithms in Multi-Agent Networked Systems. (arXiv:2212.06357v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22870;&#21169;&#32806;&#21512;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#25509;&#25910;&#21040;&#30340;&#22870;&#21169;&#21462;&#20915;&#20110;&#20854;&#20182;&#20195;&#29702;&#30340;&#29366;&#24577;&#65292;&#20294;&#19979;&#19968;&#20010;&#29366;&#24577;&#20165;&#21462;&#20915;&#20110;&#20195;&#29702;&#33258;&#24049;&#30340;&#24403;&#21069;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#25105;&#20204;&#31216;&#20854;&#20026;REC-MARL&#65292;&#20195;&#34920;&#22870;&#21169;&#32806;&#21512;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;REC-MARL&#20855;&#26377;&#19968;&#31995;&#21015;&#37325;&#35201;&#24212;&#29992;&#65292;&#22914;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#23454;&#26102;&#35775;&#38382;&#25511;&#21046;&#21644;&#20998;&#24067;&#24335;&#21151;&#29575;&#25511;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;REC-MARL&#30340;&#20998;&#24067;&#24335;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#20004;&#20010;&#26041;&#38754;&#19978;&#26159;&#20998;&#24067;&#24335;&#30340;&#65306;&#65288;i&#65289;&#23398;&#20064;&#30340;&#31574;&#30053;&#26159;&#20998;&#24067;&#24335;&#31574;&#30053;&#65292;&#23558;&#20195;&#29702;&#30340;&#26412;&#22320;&#29366;&#24577;&#26144;&#23556;&#21040;&#20854;&#26412;&#22320;&#21160;&#20316;&#65292;&#65288;ii&#65289;&#35757;&#32451;&#26159;&#20998;&#24067;&#24335;&#30340;&#65292;&#27599;&#20010;&#20195;&#29702;&#22522;&#20110;&#33258;&#24049;&#21644;&#37051;&#23621;&#30340;&#20449;&#24687;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#25152;&#23398;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#31283;&#23450;&#31574;&#30053;&#65292;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#36793;&#30028;&#21462;&#20915;&#20110;&#26412;&#22320;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#32500;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a class of multi-agent reinforcement learning (MARL) problems where the reward that an agent receives depends on the states of other agents, but the next state only depends on the agent's own current state and action. We name it REC-MARL standing for REward-Coupled Multi-Agent Reinforcement Learning. REC-MARL has a range of important applications such as real-time access control and distributed power control in wireless networks. This paper presents a distributed policy gradient algorithm for REC-MARL. The proposed algorithm is distributed in two aspects: (i) the learned policy is a distributed policy that maps a local state of an agent to its local action and (ii) the learning/training is distributed, during which each agent updates its policy based on its own and neighbors' information. The learned algorithm achieves a stationary policy and its iterative complexity bounds depend on the dimension of local states and actions. The experimental results of our algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#28145;&#24230;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#20272;&#35745;&#24178;&#20928;&#35821;&#38899;&#30340;&#23436;&#25972;&#21518;&#39564;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#30340;&#20056;&#24615;&#25513;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26041;&#38754;&#37117;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.04831</link><description>&lt;p&gt;
&#22522;&#20110;&#22797;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#28145;&#24230;&#35821;&#38899;&#22686;&#24378;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation in Deep Speech Enhancement Using Complex Gaussian Mixture Models. (arXiv:2212.04831v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#28145;&#24230;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#20272;&#35745;&#24178;&#20928;&#35821;&#38899;&#30340;&#23436;&#25972;&#21518;&#39564;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#30340;&#20056;&#24615;&#25513;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26041;&#38754;&#37117;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#36890;&#36947;&#28145;&#24230;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#36890;&#24120;&#20272;&#35745;&#21333;&#19968;&#30340;&#20056;&#24615;&#25513;&#30721;&#20197;&#25552;&#21462;&#24178;&#20928;&#30340;&#35821;&#38899;&#20449;&#21495;&#65292;&#20294;&#32570;&#20047;&#20854;&#20934;&#30830;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#22312;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#22686;&#24378;&#20013;&#37327;&#21270;&#19982;&#24178;&#20928;&#35821;&#38899;&#20272;&#35745;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36890;&#24120;&#20998;&#20026;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#21069;&#32773;&#35299;&#37322;&#20102;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21518;&#32773;&#21017;&#23545;&#24212;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#40065;&#26834;&#30340;&#24178;&#20928;&#35821;&#38899;&#20272;&#35745;&#21644;&#39640;&#25928;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#32479;&#35745;&#22797;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;CGMM&#65289;&#38598;&#25104;&#21040;&#28145;&#24230;&#35821;&#38899;&#22686;&#24378;&#26694;&#26550;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#26465;&#20214;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#38543;&#26426;&#24314;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23558;&#22122;&#22768;&#36755;&#20837;&#26144;&#23556;&#21040;&#24178;&#20928;&#35821;&#38899;&#30340;&#23436;&#25972;&#21518;&#39564;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#24314;&#27169;&#20026;&#22810;&#20010;&#22797;&#39640;&#26031;&#20998;&#24067;&#30340;&#28151;&#21512;&#12290;&#22312;CHiME-5&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#21487;&#38752;&#30340;&#39044;&#27979;&#24178;&#20928;&#35821;&#38899;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-channel deep speech enhancement approaches often estimate a single multiplicative mask to extract clean speech without a measure of its accuracy. Instead, in this work, we propose to quantify the uncertainty associated with clean speech estimates in neural network-based speech enhancement. Predictive uncertainty is typically categorized into aleatoric uncertainty and epistemic uncertainty. The former accounts for the inherent uncertainty in data and the latter corresponds to the model uncertainty. Aiming for robust clean speech estimation and efficient predictive uncertainty quantification, we propose to integrate statistical complex Gaussian mixture models (CGMMs) into a deep speech enhancement framework. More specifically, we model the dependency between input and output stochastically by means of a conditional probability density and train a neural network to map the noisy input to the full posterior distribution of clean speech, modeled as a mixture of multiple complex Gauss
&lt;/p&gt;</description></item><item><title>&#20174;&#29289;&#29702;&#23398;&#30340;&#35282;&#24230;&#23558;&#36830;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;Franz-Parisi&#28909;&#21147;&#23398;&#21183;&#30340;&#26694;&#26550;&#65292;&#23558;&#20043;&#21069;&#23398;&#20064;&#21040;&#30340;&#20219;&#21153;&#20316;&#20026;&#20808;&#39564;&#21644;&#21442;&#32771;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22330;&#31354;&#38388;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#23398;&#20064;&#35774;&#32622;&#65292;&#29992;&#20110;&#35843;&#33410;&#20219;&#21153;&#38388;&#30340;&#31361;&#35302;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2212.02846</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#32479;&#35745;&#21147;&#23398;:&#21464;&#20998;&#21407;&#29702;&#21644;&#24179;&#22343;&#22330;&#21183;
&lt;/p&gt;
&lt;p&gt;
Statistical mechanics of continual learning: variational principle and mean-field potential. (arXiv:2212.02846v3 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02846
&lt;/p&gt;
&lt;p&gt;
&#20174;&#29289;&#29702;&#23398;&#30340;&#35282;&#24230;&#23558;&#36830;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;Franz-Parisi&#28909;&#21147;&#23398;&#21183;&#30340;&#26694;&#26550;&#65292;&#23558;&#20043;&#21069;&#23398;&#20064;&#21040;&#30340;&#20219;&#21153;&#20316;&#20026;&#20808;&#39564;&#21644;&#21442;&#32771;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#22330;&#31354;&#38388;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#23398;&#20064;&#35774;&#32622;&#65292;&#29992;&#20110;&#35843;&#33410;&#20219;&#21153;&#38388;&#30340;&#31361;&#35302;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#19968;&#20010;&#38556;&#30861;&#26159;&#22810;&#31181;&#19981;&#21516;&#20219;&#21153;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#28041;&#21450;&#26426;&#22120;&#23398;&#20064;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#21508;&#31181;&#21551;&#21457;&#24615;&#25216;&#24039;&#34987;&#25552;&#20986;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#26412;&#25991;&#20851;&#27880;&#20108;&#20803;&#26435;&#37325;&#30340;&#21333;&#23618;&#21644;&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#20998;&#36125;&#21494;&#26031;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#22312;&#22330;&#31354;&#38388;&#32780;&#19981;&#26159;&#28176;&#21464;&#26410;&#23450;&#20041;&#30340;&#31163;&#25955;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33258;&#28982;&#22320;&#23558;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#21512;&#24182;&#65292;&#24182;&#35843;&#33410;&#20219;&#21153;&#38388;&#30340;&#31361;&#35302;&#36164;&#28304;&#12290;&#20174;&#29289;&#29702;&#23398;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#23558;&#21464;&#20998;&#30340;&#36830;&#32493;&#23398;&#20064;&#36716;&#21270;&#20026;Franz-Parisi&#28909;&#21147;&#23398;&#21183;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#20197;&#21069;&#30340;&#20219;&#21153;&#30693;&#35782;&#20805;&#24403;&#20808;&#39564;&#21644;&#21442;&#32771;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#25945;&#24072;-&#23398;&#29983;&#35774;&#32622;&#20013;&#30340;&#20108;&#20803;&#24863;&#30693;&#22120;&#30340;&#36830;&#32493;&#23398;&#20064;&#35299;&#37322;&#20026;&#19968;&#20010;Franz-Parisi&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
An obstacle to artificial general intelligence is set by the continual learning of multiple tasks of different nature. Recently, various heuristic tricks, both from machine learning and from neuroscience angles, were proposed, but they lack a unified theory ground. Here, we focus on the continual learning in single-layered and multi-layered neural networks of binary weights. A variational Bayesian learning setting is thus proposed, where the neural network is trained in a field-space, rather than the gradient-ill-defined discrete-weight space, and furthermore, the weight uncertainty is naturally incorporated, and modulates the synaptic resources among tasks. From a physics perspective, we translate the variational continual learning into the Franz-Parisi thermodynamic potential framework, where the previous task knowledge acts as a prior and a reference as well. We thus interprete the continual learning of the binary perceptron in a teacher-student setting as a Franz-Parisi potential c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.02733</link><description>&lt;p&gt;
&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for Relative Overgeneralization. (arXiv:2212.02733v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064; (MARL) &#20013;&#65292;&#35768;&#22810;&#27969;&#34892;&#26041;&#27861;&#22914; VDN &#21644; QMIX&#65292;&#37117;&#23481;&#26131;&#21463;&#21040;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270; (RO) &#36825;&#19968;&#20851;&#38190;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#30149;&#29702;&#30340;&#24433;&#21709;&#12290;&#24403;&#21512;&#20316;&#20219;&#21153;&#20013;&#26368;&#20339;&#32852;&#21512;&#34892;&#21160;&#30340;&#25928;&#29992;&#20302;&#20110;&#27425;&#20248;&#32852;&#21512;&#34892;&#21160;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;RO&#12290;RO&#21487;&#33021;&#23548;&#33268;&#26234;&#33021;&#20307;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#25110;&#26080;&#27861;&#35299;&#20915;&#38656;&#35201;&#26234;&#33021;&#20307;&#20043;&#38388;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#38271;&#20869;&#36827;&#34892;&#22823;&#37327;&#21327;&#35843;&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#20215;&#20540;&#30340;MARL&#31639;&#27861;&#65292;&#22914;QPLEX&#21644;WQMIX&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20811;&#26381;RO&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#21512;&#20316;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30456;&#23545;&#36807;&#24230;&#27867;&#21270;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;CURO&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20811;&#26381;RO&#12290;&#22312;CURO&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#29983;&#25104;&#36866;&#21512;&#24403;&#21069;&#33021;&#21147;&#30340;&#28304;&#20219;&#21153;&#26469;&#35299;&#20915;&#23637;&#31034;&#24378;RO&#30340;&#30446;&#26631;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent reinforcement learning (MARL), many popular methods, such as VDN and QMIX, are susceptible to a critical multi-agent pathology known as relative overgeneralization (RO), which arises when the optimal joint action's utility falls below that of a sub-optimal joint action in cooperative tasks. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks that require significant coordination between agents within a given timestep. Recent value-based MARL algorithms such as QPLEX and WQMIX can overcome RO to some extent. However, our experimental results show that they can still fail to solve cooperative tasks that exhibit strong RO. In this work, we propose a novel approach called curriculum learning for relative overgeneralization (CURO) to better overcome RO. To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks that are tailored to the current ability of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#29615;&#22659;&#27169;&#22411;&#29983;&#25104;&#36712;&#36857;&#26469;&#26356;&#26032;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#27604;&#20351;&#29992;&#26356;&#20934;&#30830;&#30340;p&#22411;&#28151;&#21512;&#33258;&#21160;&#32534;&#30721;&#22120;&#21160;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#35201;&#24046;&#12290;</title><link>http://arxiv.org/abs/2212.02179</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#20449;&#35299;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Model-Based Reinforcement Learning. (arXiv:2212.02179v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#29615;&#22659;&#27169;&#22411;&#29983;&#25104;&#36712;&#36857;&#26469;&#26356;&#26032;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#27604;&#20351;&#29992;&#26356;&#20934;&#30830;&#30340;p&#22411;&#28151;&#21512;&#33258;&#21160;&#32534;&#30721;&#22120;&#21160;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#35201;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#20256;&#32479;RL&#31639;&#27861;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#26679;&#26412;&#25928;&#29575;&#36739;&#20302;&#12290;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#12290;&#22312;&#25105;&#20204;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#23398;&#20064;&#29615;&#22659;&#30340;&#27169;&#22411;&#65292;&#20027;&#35201;&#26159;&#20854;&#36716;&#25442;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#21033;&#29992;&#20854;&#29983;&#25104;&#34394;&#25311;&#36712;&#36857;&#24182;&#36890;&#36807;&#20854;&#21453;&#21521;&#20256;&#25773;&#26469;&#26356;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#27169;&#22411;&#30340;&#21487;&#24494;&#24615;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#23398;&#20064;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#24212;&#35813;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#21033;&#29992;&#24213;&#23618;&#29289;&#29702;&#32467;&#26500;&#24320;&#21457;&#26356;&#22909;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29289;&#29702;&#31995;&#32479;&#21160;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#27809;&#26377;&#32852;&#31995;&#30340;&#21018;&#20307;&#36816;&#21160;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#31639;&#27861;&#30340;&#20004;&#20010;&#29256;&#26412;&#65292;&#19968;&#20010;&#20351;&#29992;&#26631;&#20934;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#21478;&#19968;&#20010;&#20351;&#29992;&#26356;&#20934;&#30830;&#30340;p&#22411;&#28151;&#21512;&#33258;&#21160;&#32534;&#30721;&#22120;&#21160;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply reinforcement learning (RL) to robotics tasks. One of the drawbacks of traditional RL algorithms has been their poor sample efficiency. One approach to improve the sample efficiency is model-based RL. In our model-based RL algorithm, we learn a model of the environment, essentially its transition dynamics and reward function, use it to generate imaginary trajectories and backpropagate through them to update the policy, exploiting the differentiability of the model. Intuitively, learning more accurate models should lead to better model-based RL performance. Recently, there has been growing interest in developing better deep neural network based dynamics models for physical systems, by utilizing the structure of the underlying physics. We focus on robotic systems undergoing rigid body motion without contacts. We compare two versions of our model-based RL algorithm, one which uses a standard deep neural network based dynamics model and the other which uses a much more accurate, p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21475;-&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#26469;&#35757;&#32451;&#21644;&#32452;&#21512;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#36991;&#20813;&#38656;&#35201;&#26356;&#22810;&#22797;&#21512;&#31995;&#32479;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.00893</link><description>&lt;p&gt;
&#20351;&#29992;&#31471;&#21475;-&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#30340;&#32452;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositional Learning of Dynamical System Models Using Port-Hamiltonian Neural Networks. (arXiv:2212.00893v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21475;-&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#26469;&#35757;&#32451;&#21644;&#32452;&#21512;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#36991;&#20813;&#38656;&#35201;&#26356;&#22810;&#22797;&#21512;&#31995;&#32479;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21160;&#24577;&#31995;&#32479;&#37117;&#28041;&#21450;&#35768;&#22810;&#30456;&#20114;&#20316;&#29992;&#30340;&#23376;&#31995;&#32479;&#65292;&#20174;&#19982;&#20854;&#21608;&#22260;&#29615;&#22659;&#20132;&#20114;&#30340;&#26426;&#22120;&#20154;&#21040;&#22823;&#35268;&#27169;&#30340;&#22810;&#29289;&#29702;&#31995;&#32479;&#12290;&#20026;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#31181;&#22797;&#21512;&#31995;&#32479;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20197;&#21450;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#32452;&#21512;&#24050;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#26368;&#32456;&#32467;&#26524;&#26159;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65306;&#31070;&#32463;&#32593;&#32476;&#23376;&#27169;&#22411;&#26159;&#22312;&#30456;&#23545;&#31616;&#21333;&#30340;&#23376;&#31995;&#32479;&#29983;&#25104;&#30340;&#36712;&#36857;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#28982;&#21518;&#39044;&#27979;&#26356;&#22797;&#26434;&#30340;&#22797;&#21512;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#32780;&#19981;&#38656;&#35201;&#35201;&#27714;&#22797;&#21512;&#31995;&#32479;&#26412;&#36523;&#29983;&#25104;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many dynamical systems -- from robots interacting with their surroundings to large-scale multiphysics systems -- involve a number of interacting subsystems. Toward the objective of learning composite models of such systems from data, we present i) a framework for compositional neural networks, ii) algorithms to train these models, iii) a method to compose the learned models, iv) theoretical results that bound the error of the resulting composite models, and v) a method to learn the composition itself, when it is not known a priori. The end result is a modular approach to learning: neural network submodels are trained on trajectory data generated by relatively simple subsystems, and the dynamics of more complex composite systems are then predicted without requiring additional data generated by the composite systems themselves. We achieve this compositionality by representing the system of interest, as well as each of its subsystems, as a port-Hamiltonian neural network (PHNN) -- a class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;LieFVIN&#65292;&#36890;&#36807;&#23545;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32467;&#26500;&#29305;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#23398;&#20064;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25511;&#21046;Lagrangian&#25110;Hamiltonian&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#21508;&#31181;&#27169;&#25311;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2211.16006</link><description>&lt;p&gt;
&#22522;&#20110;&#26446;&#32676;&#21147;&#23398;&#21464;&#20998;&#31215;&#20998;&#22120;&#32593;&#32476;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#23398;&#20064;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Lie Group Forced Variational Integrator Networks for Learning and Control of Robot Systems. (arXiv:2211.16006v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;LieFVIN&#65292;&#36890;&#36807;&#23545;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#32467;&#26500;&#29305;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#21487;&#20197;&#23398;&#20064;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#25511;&#21046;Lagrangian&#25110;Hamiltonian&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#21508;&#31181;&#27169;&#25311;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25226;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#29289;&#29702;&#23450;&#24459;&#21644;&#32467;&#26500;&#29305;&#24615;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#30340;&#35774;&#35745;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#20854;&#35745;&#31639;&#25928;&#29575;&#21644;&#27010;&#25324;&#33021;&#21147;&#30340;&#24378;&#26377;&#21147;&#30340;&#25216;&#26415;&#12290;&#23398;&#20064;&#31934;&#30830;&#30340;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#27169;&#22411;&#23545;&#20110;&#23433;&#20840;&#21644;&#31283;&#23450;&#25511;&#21046;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#20445;&#25345;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21363;Lie Group Forced Variational Integrator Network (LieFVIN)&#65292;&#33021;&#22815;&#20174;&#20301;&#32622;-&#36895;&#24230;&#25110;&#20165;&#20301;&#32622;&#25968;&#25454;&#20013;&#23398;&#20064;&#25511;&#21046;&#30340;Lagrangian&#25110;Hamiltonian&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#35774;&#35745;&#65292;LieFVINs&#33021;&#22815;&#20445;&#25345;&#21160;&#21147;&#23398;&#28436;&#21464;&#30340;&#26446;&#32676;&#32467;&#26500;&#21644;&#24213;&#23618;&#30340;Hamiltonian&#25110;Lagrangian&#31995;&#32479;&#20013;&#30340;&#36763;&#32467;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21487;&#29992;&#20110;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#25511;&#21046;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#27169;&#25311;&#26426;&#22120;&#20154;&#31995;&#32479;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22235;&#36724;&#39134;&#34892;&#22120;&#12289;&#27700;&#19979;&#28369;&#32724;&#26426;&#21644;&#20154;&#24418;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating prior knowledge of physics laws and structural properties of dynamical systems into the design of deep learning architectures has proven to be a powerful technique for improving their computational efficiency and generalization capacity. Learning accurate models of robot dynamics is critical for safe and stable control. Autonomous mobile robots, including wheeled, aerial, and underwater vehicles, can be modeled as controlled Lagrangian or Hamiltonian rigid-body systems evolving on matrix Lie groups. In this paper, we introduce a new structure-preserving deep learning architecture, the Lie group Forced Variational Integrator Network (LieFVIN), capable of learning controlled Lagrangian or Hamiltonian dynamics on Lie groups, either from position-velocity or position-only data. By design, LieFVINs preserve both the Lie group structure on which the dynamics evolve and the symplectic structure underlying the Hamiltonian or Lagrangian systems of interest. The proposed architectu
&lt;/p&gt;</description></item><item><title>PipeFisher&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;K-FAC&#30340;&#24037;&#20316;&#20998;&#37197;&#32473;&#31649;&#36947;&#38388;&#38553;&#65292;&#20197;&#21152;&#36895;&#25910;&#25947;&#21644;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14133</link><description>&lt;p&gt;
PipeFisher: &#21033;&#29992;&#31649;&#36947;&#24182;&#32467;&#21512;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#39640;&#25928;&#22823;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices. (arXiv:2211.14133v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14133
&lt;/p&gt;
&lt;p&gt;
PipeFisher&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;K-FAC&#30340;&#24037;&#20316;&#20998;&#37197;&#32473;&#31649;&#36947;&#38388;&#38553;&#65292;&#20197;&#21152;&#36895;&#25910;&#25947;&#21644;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#36947;&#24182;&#34892;&#25216;&#26415;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#21152;&#36895;&#22120;&#38598;&#32676;&#19978;&#39640;&#25928;&#22320;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#21551;&#21160;&#21644;&#20851;&#38381;&#26399;&#38388;&#30340;&#31649;&#36947;&#38388;&#38553;&#20250;&#38477;&#20302;&#21152;&#36895;&#22120;&#30340;&#21033;&#29992;&#29575;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#39640;&#25928;&#30340;&#31649;&#36947;&#26041;&#26696;&#26469;&#26368;&#22823;&#21270;&#21033;&#29992;&#29575;&#65292;&#22914;&#24494;&#25209;&#22788;&#29702;&#21644;&#21452;&#21521;&#31649;&#36947;&#65292;&#20294;&#20173;&#26377;&#22823;&#37327;&#38388;&#38553;&#26080;&#27861;&#20351;&#29992;&#21516;&#27493;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#22635;&#20805;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#39069;&#22806;&#30340;&#24037;&#20316;&#20998;&#37197;&#32473;&#31649;&#36947;&#38388;&#38553;&#65292;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#33719;&#24471;&#36741;&#21161;&#25928;&#30410;&#12290;&#20316;&#20026;&#36825;&#31181;&#26041;&#21521;&#30340;&#19968;&#20010;&#20363;&#23376;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PipeFisher&#65292;&#23427;&#23558;&#22522;&#20110;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;K-FAC&#30340;&#24037;&#20316;&#20998;&#37197;&#32473;&#31649;&#36947;&#38388;&#38553;&#65292;&#21152;&#36895;&#25910;&#25947;&#12290;&#22312;BERT-Base&#21644;-Large&#27169;&#22411;&#30340;&#31532;&#19968;&#38454;&#27573;&#39044;&#35757;&#32451;&#20013;&#65292;PipeFisher&#23558;&#65288;&#27169;&#25311;&#30340;&#65289;&#35757;&#32451;&#26102;&#38388;&#32553;&#30701;&#20102;50-75&#65285;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#22120;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#21152;&#36895;&#22120;&#30340;&#21033;&#29992;&#29575;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pipeline parallelism enables efficient training of Large Language Models (LLMs) on large-scale distributed accelerator clusters. Yet, pipeline bubbles during startup and tear-down reduce the utilization of accelerators. Although efficient pipeline schemes with micro-batching and bidirectional pipelines have been proposed to maximize utilization, a significant number of bubbles cannot be filled using synchronous forward and backward passes. To address this problem, we suggest that extra work be assigned to the bubbles to gain auxiliary benefits in LLM training. As an example in this direction, we propose PipeFisher, which assigns the work of K-FAC, a second-order optimization method based on the Fisher information matrix, to the bubbles to accelerate convergence. In Phase 1 pretraining of BERT-Base and -Large models, PipeFisher reduces the (simulated) training time to 50-75% compared to training with a first-order optimizer by greatly improving the accelerator utilization and benefiting
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#65292;&#23545;&#39044;&#27979;&#36712;&#36857;&#30340;&#21518;&#32493;&#29366;&#24577;&#20043;&#38388;&#29420;&#31435;&#24615;&#30340;&#20551;&#35774;&#26159;&#38169;&#35823;&#30340;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#20998;&#27573;&#32447;&#24615;&#36817;&#20284;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.11103</link><description>&lt;p&gt;
&#36807;&#21435;&#30340;&#20107;&#24773;&#24456;&#37325;&#35201;&#65306;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#36712;&#36857;&#39044;&#27979;&#20013;&#21518;&#32493;&#29366;&#24577;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Past Does Matter: Correlation of Subsequent States in Trajectory Predictions of Gaussian Process Models. (arXiv:2211.11103v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11103
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#65292;&#23545;&#39044;&#27979;&#36712;&#36857;&#30340;&#21518;&#32493;&#29366;&#24577;&#20043;&#38388;&#29420;&#31435;&#24615;&#30340;&#20551;&#35774;&#26159;&#38169;&#35823;&#30340;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#20998;&#27573;&#32447;&#24615;&#36817;&#20284;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#21160;&#24577;&#31995;&#32479;&#30340;&#36712;&#36857;&#20998;&#24067;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22312;&#32771;&#34385;&#21040;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#20043;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27169;&#22411;&#36755;&#20986;&#21644;&#36712;&#36857;&#20998;&#24067;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#24037;&#20316;&#65292;&#37325;&#28857;&#25918;&#22312;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19978;&#65292;&#38169;&#35823;&#22320;&#21253;&#21547;&#20102;&#23545;&#39044;&#27979;&#36712;&#36857;&#30340;&#21518;&#32493;&#29366;&#24577;&#20043;&#38388;&#29420;&#31435;&#24615;&#30340;&#20551;&#35774;&#12290;&#23558;&#36825;&#20123;&#24605;&#24819;&#25193;&#23637;&#21040;&#36830;&#32493;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#20551;&#35774;&#30340;&#21547;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#20998;&#27573;&#32447;&#24615;&#36817;&#20284;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computing the distribution of trajectories from a Gaussian Process model of a dynamical system is an important challenge in utilizing such models. Motivated by the computational cost of sampling-based approaches, we consider approximations of the model's output and trajectory distribution. We show that previous work on uncertainty propagation, focussed on discrete state-space models, incorrectly included an independence assumption between subsequent states of the predicted trajectories. Expanding these ideas to continuous ordinary differential equation models, we illustrate the implications of this assumption and propose a novel piecewise linear approximation of Gaussian Processes to mitigate them.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#8212;&#8212;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#65288;ConvGNP&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25918;&#32622;&#25928;&#29575;&#12290;ConvGNP&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#32852;&#21512;&#39640;&#26031;&#20998;&#24067;&#65292;&#36890;&#36807;&#23398;&#20064;&#31354;&#38388;&#21644;&#23395;&#33410;&#24615;&#38750;&#24179;&#31283;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38750;&#24179;&#31283;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.10381</link><description>&lt;p&gt;
&#24102;&#26377;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#30340;&#29615;&#22659;&#20256;&#24863;&#22120;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Environmental Sensor Placement with Convolutional Gaussian Neural Processes. (arXiv:2211.10381v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#8212;&#8212;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#65288;ConvGNP&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#29615;&#22659;&#20256;&#24863;&#22120;&#30340;&#25918;&#32622;&#25928;&#29575;&#12290;ConvGNP&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#32852;&#21512;&#39640;&#26031;&#20998;&#24067;&#65292;&#36890;&#36807;&#23398;&#20064;&#31354;&#38388;&#21644;&#23395;&#33410;&#24615;&#38750;&#24179;&#31283;&#24615;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38750;&#24179;&#31283;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#20256;&#24863;&#22120;&#23545;&#20110;&#30417;&#27979;&#22825;&#27668;&#21644;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#21335;&#26497;&#36825;&#26679;&#30340;&#20559;&#36828;&#22320;&#21306;&#65292;&#26368;&#22823;&#21270;&#27979;&#37327;&#20449;&#24687;&#21644;&#26377;&#25928;&#25918;&#32622;&#20256;&#24863;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#26032;&#20256;&#24863;&#22120;&#25552;&#20379;&#30340;&#19981;&#30830;&#23450;&#24615;&#20943;&#23569;&#26469;&#35780;&#20272;&#25918;&#32622;&#20449;&#24687;&#12290;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#20294;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#38750;&#24179;&#31283;&#34892;&#20026;&#24182;&#32553;&#25918;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21367;&#31215;&#39640;&#26031;&#31070;&#32463;&#36807;&#31243;&#65288;ConvGNP&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;ConvGNP&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#20219;&#24847;&#30446;&#26631;&#20301;&#32622;&#30340;&#32852;&#21512;&#39640;&#26031;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20351;&#29992;&#27169;&#25311;&#30340;&#21335;&#26497;&#22320;&#21306;&#22320;&#38754;&#28201;&#24230;&#24322;&#24120;&#20316;&#20026;&#30495;&#23454;&#25968;&#25454;&#65292;ConvGNP&#23398;&#20064;&#20102;&#31354;&#38388;&#21644;&#23395;&#33410;&#24615;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#20248;&#20110;&#38750;&#24179;&#31283;GP&#22522;&#32447;&#12290;&#22312;&#27169;&#25311;&#30340;s&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Environmental sensors are crucial for monitoring weather conditions and the impacts of climate change. However, it is challenging to maximise measurement informativeness and place sensors efficiently, particularly in remote regions like Antarctica. Probabilistic machine learning models can evaluate placement informativeness by predicting the uncertainty reduction provided by a new sensor. Gaussian process (GP) models are widely used for this purpose, but they struggle with capturing complex non-stationary behaviour and scaling to large datasets. This paper proposes using a convolutional Gaussian neural process (ConvGNP) to address these issues. A ConvGNP uses neural networks to parameterise a joint Gaussian distribution at arbitrary target locations, enabling flexibility and scalability. Using simulated surface air temperature anomaly over Antarctica as ground truth, the ConvGNP learns spatial and seasonal non-stationarities, outperforming a non-stationary GP baseline. In a simulated s
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#38024;&#23545;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#38656;&#35201;&#19981;&#21516;&#30340;&#25915;&#20987;&#21644;&#26816;&#27979;&#26041;&#27861;&#12290;&#35770;&#25991;&#38024;&#23545;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#36827;&#34892;&#20102;&#32479;&#35745;&#27979;&#35797;&#65292;&#30830;&#23450;&#20102;&#19982;&#25968;&#25454;&#20860;&#23481;&#30340;&#27169;&#22411;&#38598;&#26159;&#21542;&#21253;&#25324;&#31995;&#32479;&#30340;&#30495;&#23454;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#34109;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2211.08804</link><description>&lt;p&gt;
&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#31163;&#32447;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#20998;&#26512;&#21644;&#21487;&#26816;&#27979;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysis and Detectability of Offline Data Poisoning Attacks on Linear Dynamical Systems. (arXiv:2211.08804v4 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08804
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#38024;&#23545;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#38656;&#35201;&#19981;&#21516;&#30340;&#25915;&#20987;&#21644;&#26816;&#27979;&#26041;&#27861;&#12290;&#35770;&#25991;&#38024;&#23545;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#36827;&#34892;&#20102;&#32479;&#35745;&#27979;&#35797;&#65292;&#30830;&#23450;&#20102;&#19982;&#25968;&#25454;&#20860;&#23481;&#30340;&#27169;&#22411;&#38598;&#26159;&#21542;&#21253;&#25324;&#31995;&#32479;&#30340;&#30495;&#23454;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#34109;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26041;&#27861;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#24433;&#21709;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#24050;&#32463;&#29087;&#30693;&#20102;&#27602;&#21270;&#25915;&#20987;&#65292;&#20294;&#36825;&#20123;&#25915;&#20987;&#36890;&#24120;&#20351;&#29992;&#20132;&#21449;&#26679;&#26412;&#29420;&#31435;&#31561;&#20551;&#35774;&#65292;&#32780;&#36825;&#20123;&#20551;&#35774;&#22312;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#20013;&#36890;&#24120;&#19981;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#19982;i.i.d.&#35774;&#32622;&#19979;&#38024;&#23545;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#24320;&#21457;&#30340;&#25915;&#20987;&#21644;&#26816;&#27979;&#26041;&#27861;&#19981;&#21516;&#30340;&#25915;&#20987;&#21644;&#26816;&#27979;&#26041;&#27861;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#31639;&#27861;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#27979;&#35797;&#26469;&#30740;&#31350;&#27745;&#26579;&#22914;&#20309;&#24433;&#21709;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#24182;&#36136;&#30097;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#21487;&#20197;&#20197;&#20160;&#20040;&#26041;&#24335;&#34987;&#26816;&#27979;&#21040;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#19982;&#25968;&#25454;&#20860;&#23481;&#30340;&#27169;&#22411;&#38598;&#21253;&#21547;&#31995;&#32479;&#30340;&#30495;&#23454;&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#20102;&#25915;&#20987;&#32773;&#30340;&#19981;&#21516;&#27745;&#26579;&#31574;&#30053;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#34109;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing interest in the effects of data poisoning attacks on data-driven control methods. Poisoning attacks are well-known to the Machine Learning community, which, however, make use of assumptions, such as cross-sample independence, that in general do not hold for linear dynamical systems. Consequently, these systems require different attack and detection methods than those developed for supervised learning problems in the i.i.d.\ setting. Since most data-driven control algorithms make use of the least-squares estimator, we study how poisoning impacts the least-squares estimate through the lens of statistical testing, and question in what way data poisoning attacks can be detected. We establish under which conditions the set of models compatible with the data includes the true model of the system, and we analyze different poisoning strategies for the attacker. On the basis of the arguments hereby presented, we propose a stealthy data poisoning attack 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#31354;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#32771;&#34385; COLREG &#35268;&#21017;&#21644;&#24341;&#20837;&#26368;&#20808;&#36827;&#30340;&#30896;&#25758;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#33258;&#20027;&#33337;&#33334;&#30340;&#28145;&#24230; Q &#32593;&#32476;&#12290;&#22312;&#30495;&#23454;&#29615;&#22659;&#21644;&#27169;&#25311;&#29615;&#22659;&#20013;&#39564;&#35777;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#22312;&#28023;&#19978;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#28508;&#21147;&#24182;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01004</link><description>&lt;p&gt;
&#33258;&#20027;&#33337;&#33334;&#30340;&#26102;&#31354;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal recurrent reinforcement learning for autonomous ships. (arXiv:2211.01004v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#31354;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#32771;&#34385; COLREG &#35268;&#21017;&#21644;&#24341;&#20837;&#26368;&#20808;&#36827;&#30340;&#30896;&#25758;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#33258;&#20027;&#33337;&#33334;&#30340;&#28145;&#24230; Q &#32593;&#32476;&#12290;&#22312;&#30495;&#23454;&#29615;&#22659;&#21644;&#27169;&#25311;&#29615;&#22659;&#20013;&#39564;&#35777;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#22312;&#28023;&#19978;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#28508;&#21147;&#24182;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31354;&#38388;-&#26102;&#38388;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#33258;&#20027;&#33337;&#33334;&#30340;&#28145;&#24230; Q &#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#35774;&#35745;&#20351;&#24471;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#25968;&#37327;&#30340;&#21608;&#22260;&#30446;&#26631;&#33337;&#21482;&#65292;&#24182;&#23545;&#23616;&#37096;&#35266;&#27979;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#30896;&#25758;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#20415;&#26234;&#33021;&#20307;&#26356;&#23481;&#26131;&#35780;&#20272;&#19981;&#21516;&#24773;&#20917;&#12290;&#28023;&#19978;&#20132;&#36890; COLREG &#35268;&#21017;&#26126;&#30830;&#22320;&#32771;&#34385;&#22312;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#20013;&#12290;&#26368;&#32456;&#31574;&#30053;&#22312;&#33258;&#23450;&#20041;&#30340; Around the Clock &#21333;&#33337;&#36973;&#36935;&#38382;&#39064;&#38598;&#21644;&#21253;&#25324; 18 &#20010;&#22810;&#33337;&#24773;&#26223;&#30340;&#24120;&#29992; Imazu (1987) &#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#19982;&#20154;&#24037;&#21183;&#22330;&#21644;&#36895;&#24230;&#38556;&#30861;&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#28023;&#19978;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#26032;&#26550;&#26500;&#22312;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#20063;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a spatial-temporal recurrent neural network architecture for deep $Q$-networks that can be used to steer an autonomous ship. The network design makes it possible to handle an arbitrary number of surrounding target ships while offering robustness to partial observability. Furthermore, a state-of-the-art collision risk metric is proposed to enable an easier assessment of different situations by the agent. The COLREG rules of maritime traffic are explicitly considered in the design of the reward function. The final policy is validated on a custom set of newly created single-ship encounters called `Around the Clock' problems and the commonly used Imazu (1987) problems, which include 18 multi-ship scenarios. Performance comparisons with artificial potential field and velocity obstacle methods demonstrate the potential of the proposed approach for maritime path planning. Furthermore, the new architecture exhibits robustness when it is deployed in multi-agent scenarios and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#22312;&#32447;&#25511;&#21046;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#36873;&#25321;&#21551;&#21457;&#24335;&#31574;&#30053;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#65292;&#20197;&#33719;&#24471;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#65292;&#23545;&#24212;&#29992;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2211.00759</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#22312;&#32447;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning. (arXiv:2211.00759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#22312;&#32447;&#25511;&#21046;&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#36873;&#25321;&#21551;&#21457;&#24335;&#31574;&#30053;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#65292;&#20197;&#33719;&#24471;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#65292;&#23545;&#24212;&#29992;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#22823;&#37051;&#22495;&#25628;&#32034;&#65288;ALNS&#65289;&#31639;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65288;COPs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#25104;&#21151;&#12290;ALNS&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#21508;&#31181;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#21183;&#26469;&#25214;&#21040;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#12290;&#28982;&#32780;&#65292;ALNS&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20854;&#36873;&#25321;&#21644;&#25509;&#21463;&#21442;&#25968;&#30340;&#27491;&#30830;&#37197;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#36873;&#25321;&#21551;&#21457;&#24335;&#12289;&#35843;&#25972;&#21442;&#25968;&#21644;&#25511;&#21046;&#25509;&#21463;&#26631;&#20934;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#22522;&#20110;&#25628;&#32034;&#30340;&#29366;&#24577;&#23398;&#20064;&#22914;&#20309;&#37197;&#32622;&#19979;&#19968;&#27425;ALNS&#36845;&#20195;&#20197;&#33719;&#24471;&#22909;&#30340;&#20248;&#21270;&#38382;&#39064;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;&#21547;&#26377;&#38543;&#26426;&#26435;&#37325;&#21644;&#26102;&#38388;&#31383;&#21475;&#30340;&#23548;&#33322;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#35813;&#38382;&#39064;&#29992;&#20110;IJCAI&#31454;&#36187;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26222;&#36890;&#30340;ALNS&#21644;&#20855;&#26377;&#40664;&#35748;&#21442;&#25968;&#35774;&#32622;&#30340;ALNS&#65292;&#23637;&#31034;&#20102;DRL&#26041;&#27861;&#22312;&#22312;&#32447;&#25511;&#21046;ALNS&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive Large Neighborhood Search (ALNS) algorithm has shown considerable success in solving complex combinatorial optimization problems (COPs). ALNS selects various heuristics adaptively during the search process, leveraging their strengths to find good solutions for optimization problems. However, the effectiveness of ALNS depends on the proper configuration of its selection and acceptance parameters. To address this limitation, we propose a Deep Reinforcement Learning (DRL) approach that selects heuristics, adjusts parameters, and controls the acceptance criteria during the search process. The proposed method aims to learn, based on the state of the search, how to configure the next iteration of the ALNS to obtain good solutions to the underlying optimization problem. We evaluate the proposed method on a time-dependent orienteering problem with stochastic weights and time windows, used in an IJCAI competition. The results show that our approach outperforms vanilla ALNS and ALNS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LiDAR&#28857;&#20113;&#34920;&#31034;&#30340;&#35745;&#31639;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20998;&#26512;&#20102;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;LiDAR&#28857;&#20113;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35774;&#35745;&#29992;&#20110;LiDAR&#28857;&#20113;&#30340;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#65292;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#30340;&#23454;&#26102;&#24863;&#30693;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.14612</link><description>&lt;p&gt;
&#20998;&#26512;&#28145;&#24230;&#23398;&#20064;&#23545;&#36710;&#20869;&#23454;&#26102;LiDAR&#24863;&#30693;&#30340;&#28857;&#20113;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Analyzing Deep Learning Representations of Point Clouds for Real-Time In-Vehicle LiDAR Perception. (arXiv:2210.14612v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LiDAR&#28857;&#20113;&#34920;&#31034;&#30340;&#35745;&#31639;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20998;&#26512;&#20102;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;LiDAR&#28857;&#20113;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35774;&#35745;&#29992;&#20110;LiDAR&#28857;&#20113;&#30340;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#65292;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#30340;&#23454;&#26102;&#24863;&#30693;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LiDAR&#20256;&#24863;&#22120;&#20316;&#20026;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#12289;&#39640;&#20998;&#36776;&#29575;&#30340;&#36710;&#36742;&#21608;&#22260;&#19977;&#32500;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#22810;&#20010;&#39640;&#20998;&#36776;&#29575;LiDAR&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#37327;&#36880;&#28176;&#22686;&#21152;&#65292;&#20854;&#35745;&#31639;&#38590;&#24230;&#20063;&#38543;&#20043;&#22686;&#21152;&#65292;&#36825;&#26679;&#30340;&#28857;&#20113;&#38656;&#35201;&#23454;&#26102;&#22788;&#29702;&#24182;&#20174;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#24433;&#21709;&#22522;&#20110;&#36825;&#20123;&#28857;&#20113;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36816;&#34892;&#24615;&#33021;&#21644;&#20934;&#30830;&#24230;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#26159;&#20854;&#24213;&#23618;&#25968;&#25454;&#34920;&#31034;&#26041;&#24335;&#21450;&#35745;&#31639;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LiDAR&#28857;&#20113;&#34920;&#31034;&#30340;&#35745;&#31639;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20998;&#26512;&#20102;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;LiDAR&#28857;&#20113;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35745;&#31639;&#20998;&#31867;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#35774;&#35745;&#29992;&#20110;LiDAR&#28857;&#20113;&#30340;&#28145;&#24230;&#23398;&#20064;&#34920;&#31034;&#65292;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#30340;&#23454;&#26102;&#24863;&#30693;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LiDAR sensors are an integral part of modern autonomous vehicles as they provide an accurate, high-resolution 3D representation of the vehicle's surroundings. However, it is computationally difficult to make use of the ever-increasing amounts of data from multiple high-resolution LiDAR sensors. As frame-rates, point cloud sizes and sensor resolutions increase, real-time processing of these point clouds must still extract semantics from this increasingly precise picture of the vehicle's environment. One deciding factor of the run-time performance and accuracy of deep neural networks operating on these point clouds is the underlying data representation and the way it is computed. In this work, we examine the relationship between the computational representations used in neural networks and their performance characteristics. To this end, we propose a novel computational taxonomy of LiDAR point cloud representations used in modern deep neural networks for 3D point cloud processing. Using t
&lt;/p&gt;</description></item><item><title>CEGARETTE&#26159;&#19968;&#31181;&#26032;&#30340;&#39564;&#35777;&#26426;&#21046;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#31995;&#32479;&#21644;&#23646;&#24615;&#36827;&#34892;&#25277;&#35937;&#21644;&#32454;&#21270;&#65292;&#20174;&#32780;&#20135;&#29983;&#22823;&#23567;&#36866;&#20013;&#19988;&#36275;&#22815;&#20934;&#30830;&#30340;&#25277;&#35937;&#32593;&#32476;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26356;&#21152;&#24555;&#36895;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.12871</link><description>&lt;p&gt;
&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20013;&#30340;&#25277;&#35937;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Tighter Abstract Queries in Neural Network Verification. (arXiv:2210.12871v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12871
&lt;/p&gt;
&lt;p&gt;
CEGARETTE&#26159;&#19968;&#31181;&#26032;&#30340;&#39564;&#35777;&#26426;&#21046;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#31995;&#32479;&#21644;&#23646;&#24615;&#36827;&#34892;&#25277;&#35937;&#21644;&#32454;&#21270;&#65292;&#20174;&#32780;&#20135;&#29983;&#22823;&#23567;&#36866;&#20013;&#19988;&#36275;&#22815;&#20934;&#30830;&#30340;&#25277;&#35937;&#32593;&#32476;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26356;&#21152;&#24555;&#36895;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#21508;&#20010;&#39046;&#22495;&#21453;&#24212;&#24615;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23613;&#31649;&#20854;&#24615;&#33021;&#20986;&#33394;&#65292;&#20294;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20250;&#24102;&#26469;&#35768;&#22810;&#39118;&#38505;&#65292;&#36825;&#20123;&#39118;&#38505;&#28304;&#20110;&#25105;&#20204;&#26080;&#27861;&#29702;&#35299;&#21644;&#25512;&#29702;&#20854;&#34892;&#20026;&#12290;&#38024;&#23545;&#36825;&#20123;&#39118;&#38505;&#65292;&#24050;&#25552;&#20986;&#20102;&#21508;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26469;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#65307;&#20294;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38556;&#30861;&#12290;&#26368;&#36817;&#30340;&#23581;&#35797;&#34920;&#26126;&#65292;&#25277;&#35937;&#32454;&#21270;&#26041;&#27861;&#22312;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#26041;&#38754;&#21487;&#33021;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65307;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#20135;&#29983;&#36807;&#20110;&#25277;&#35937;&#30340;&#32593;&#32476;&#65292;&#20351;&#20854;&#19981;&#36866;&#21512;&#36827;&#34892;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CEGARETTE&#65292;&#19968;&#31181;&#26032;&#30340;&#39564;&#35777;&#26426;&#21046;&#65292;&#20854;&#20013;&#31995;&#32479;&#21644;&#23646;&#24615;&#21516;&#26102;&#36827;&#34892;&#25277;&#35937;&#21644;&#32454;&#21270;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#26082;&#23567;&#21448;&#36275;&#22815;&#20934;&#30830;&#30340;&#25277;&#35937;&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have become critical components of reactive systems in various domains within computer science. Despite their excellent performance, using neural networks entails numerous risks that stem from our lack of ability to understand and reason about their behavior. Due to these risks, various formal methods have been proposed for verifying neural networks; but unfortunately, these typically struggle with scalability barriers. Recent attempts have demonstrated that abstraction-refinement approaches could play a significant role in mitigating these limitations; but these approaches can often produce networks that are so abstract, that they become unsuitable for verification. To deal with this issue, we present CEGARETTE, a novel verification mechanism where both the system and the property are abstracted and refined simultaneously. We observe that this approach allows us to produce abstract networks which are both small and sufficiently accurate, allowing for quick verification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20809;&#35889;&#26412;&#36523;&#30340;&#33258;&#30456;&#20851;&#20449;&#24687;&#26469;&#28040;&#38500;&#20809;&#35889;&#20013;&#30340;&#32593;&#26684;&#32467;&#26500;&#21644;&#22122;&#22768;&#65292;&#20174;&#32780;&#20248;&#21270;&#20809;&#35889;&#36136;&#37327;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#25193;&#23637;&#21040;&#20854;&#20182;&#20809;&#35889;&#27979;&#37327;&#20013;&#20197;&#28040;&#38500;&#22806;&#37096;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2210.11200</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#28040;&#38500;&#35282;&#20998;&#36776;&#20809;&#30005;&#23376;&#33021;&#35889;&#20013;&#30340;&#32593;&#26684;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Removing grid structure in angle-resolved photoemission spectra via deep learning method. (arXiv:2210.11200v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20809;&#35889;&#26412;&#36523;&#30340;&#33258;&#30456;&#20851;&#20449;&#24687;&#26469;&#28040;&#38500;&#20809;&#35889;&#20013;&#30340;&#32593;&#26684;&#32467;&#26500;&#21644;&#22122;&#22768;&#65292;&#20174;&#32780;&#20248;&#21270;&#20809;&#35889;&#36136;&#37327;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#26377;&#28508;&#21147;&#25193;&#23637;&#21040;&#20854;&#20182;&#20809;&#35889;&#27979;&#37327;&#20013;&#20197;&#28040;&#38500;&#22806;&#37096;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#35889;&#25968;&#25454;&#24120;&#24120;&#21253;&#21547;&#19981;&#24517;&#35201;&#30340;&#22806;&#37096;&#20449;&#21495;&#12290;&#20363;&#22914;&#65292;&#22312;ARPES&#23454;&#39564;&#20013;&#65292;&#36890;&#24120;&#20250;&#22312;CCD&#21069;&#25918;&#32622;&#19968;&#26681;&#37329;&#23646;&#32593;&#26684;&#20197;&#38459;&#25377;&#26434;&#25955;&#30340;&#20809;&#30005;&#23376;&#65292;&#20294;&#22312;&#24555;&#36895;&#27979;&#37327;&#27169;&#24335;&#19979;&#20250;&#24341;&#36215;&#20809;&#35889;&#20013;&#30340;&#32593;&#26684;&#32467;&#26500;&#12290;&#36807;&#21435;&#65292;&#36825;&#31181;&#32467;&#26500;&#36890;&#24120;&#26159;&#36890;&#36807;&#25968;&#23398;&#20613;&#31435;&#21494;&#28388;&#27874;&#26041;&#27861;&#36890;&#36807;&#21024;&#38500;&#21608;&#26399;&#24615;&#32467;&#26500;&#26469;&#28040;&#38500;&#30340;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20809;&#35889;&#20013;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#31354;&#32570;&#65292;&#22240;&#20026;&#32593;&#26684;&#32467;&#26500;&#19981;&#26159;&#20005;&#26684;&#32447;&#24615;&#21472;&#21152;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20809;&#35889;&#26412;&#36523;&#20869;&#37096;&#30340;&#33258;&#30456;&#20851;&#20449;&#24687;&#65292;&#21487;&#20197;&#21516;&#26102;&#28040;&#38500;&#32593;&#26684;&#32467;&#26500;&#21644;&#22122;&#22768;&#65292;&#22823;&#22823;&#20248;&#21270;&#20809;&#35889;&#36136;&#37327;&#12290;&#23427;&#26377;&#28508;&#21147;&#25193;&#23637;&#21040;&#25152;&#26377;&#20809;&#35889;&#27979;&#37327;&#20013;&#65292;&#20197;&#28040;&#38500;&#20854;&#20182;&#22806;&#37096;&#20449;&#21495;&#24182;&#26681;&#25454;&#33258;&#30456;&#20851;&#24615;&#22686;&#24378;&#20809;&#35889;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectroscopic data may often contain unwanted extrinsic signals. For example, in ARPES experiment, a wire mesh is typically placed in front of the CCD to block stray photo-electrons, but could cause a grid-like structure in the spectra during quick measurement mode. In the past, this structure was often removed using the mathematical Fourier filtering method by erasing the periodic structure. However, this method may lead to information loss and vacancies in the spectra because the grid structure is not strictly linearly superimposed. Here, we propose a deep learning method to effectively overcome this problem. Our method takes advantage of the self-correlation information within the spectra themselves and can greatly optimize the quality of the spectra while removing the grid structure and noise simultaneously. It has the potential to be extended to all spectroscopic measurements to eliminate other extrinsic signals and enhance the spectral quality based on the self-correlation of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#35889;&#33258;&#36523;&#20449;&#24687;&#30340;&#21435;&#22122;&#26041;&#27861;&#65292;&#26080;&#38656;&#35757;&#32451;&#38598;&#65292;&#33021;&#22815;&#25552;&#21462;&#20809;&#35889;&#30340;&#20869;&#22312;&#20449;&#24687;&#65292;&#20445;&#30041;&#20102;&#33021;&#24102;&#29305;&#24449;&#65292;&#21487;&#25193;&#23637;&#24615;&#24378;&#12290;</title><link>http://arxiv.org/abs/2210.10494</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#38656;&#35757;&#32451;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20809;&#35889;&#25968;&#25454;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Spectroscopic data de-noising via training-set-free deep learning method. (arXiv:2210.10494v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#35889;&#33258;&#36523;&#20449;&#24687;&#30340;&#21435;&#22122;&#26041;&#27861;&#65292;&#26080;&#38656;&#35757;&#32451;&#38598;&#65292;&#33021;&#22815;&#25552;&#21462;&#20809;&#35889;&#30340;&#20869;&#22312;&#20449;&#24687;&#65292;&#20445;&#30041;&#20102;&#33021;&#24102;&#29305;&#24449;&#65292;&#21487;&#25193;&#23637;&#24615;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#22312;&#20809;&#35889;&#21518;&#22788;&#29702;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#25552;&#21462;&#20869;&#22312;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#38598;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#23454;&#39564;&#27979;&#37327;&#20013;&#36890;&#24120;&#26159;&#19981;&#21487;&#33719;&#24471;&#30340;&#12290;&#26412;&#25991;&#20197;&#35282;&#20998;&#36776;&#20809;&#30005;&#23376;&#33021;&#35889;&#65288;ARPES&#65289;&#20013;&#30340;&#20809;&#35889;&#20026;&#20363;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#21435;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#21462;&#20869;&#22312;&#30340;&#20809;&#35889;&#20449;&#24687;&#32780;&#26080;&#38656;&#35757;&#32451;&#38598;&#12290;&#36825;&#26159;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20809;&#35889;&#33258;&#36523;&#30340;&#33258;&#30456;&#20851;&#20449;&#24687;&#12290;&#23427;&#20445;&#30041;&#20102;&#20869;&#22312;&#30340;&#33021;&#24102;&#29305;&#24449;&#65292;&#20174;&#32780;&#26377;&#21033;&#20110;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#21644;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21463;&#20197;&#21069;&#26041;&#27861;&#35757;&#32451;&#38598;&#29305;&#23450;&#23646;&#24615;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#23427;&#21487;&#33021;&#20250;&#25193;&#23637;&#21040;&#22312;&#33719;&#21462;&#39640;&#36136;&#37327;&#22810;&#32500;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20854;&#20182;&#39046;&#22495;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
De-noising plays a crucial role in the post-processing of spectra. Machine learning-based methods show good performance in extracting intrinsic information from noisy data, but often require a high-quality training set that is typically inaccessible in real experimental measurements. Here, using spectra in angle-resolved photoemission spectroscopy (ARPES) as an example, we develop a de-noising method for extracting intrinsic spectral information without the need for a training set. This is possible as our method leverages the self-correlation information of the spectra themselves. It preserves the intrinsic energy band features and thus facilitates further analysis and processing. Moreover, since our method is not limited by specific properties of the training set compared to previous ones, it may well be extended to other fields and application scenarios where obtaining high-quality multidimensional training data is challenging.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21333;&#35843;&#29992;&#21333;&#25237;&#24433;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#20248;&#21270;&#26799;&#24230;&#26041;&#27861;&#21644;&#21152;&#36895;&#21453;&#23556;&#26799;&#24230;&#26041;&#27861;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#28385;&#36275;&#24369;Minty&#21464;&#20998;&#19981;&#31561;&#24335;&#21644;&#36127;&#20849;&#21516;&#21333;&#35843;&#24615;&#30340;&#21253;&#21547;&#38382;&#39064;&#65292;&#21487;&#20197;&#21152;&#36895;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#65292;&#24182;&#20998;&#21035;&#21462;&#24471;&#20102;$O(\frac{1}{\sqrt{T}})$&#21644;$O(\frac{1}{T})$&#30340;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.03096</link><description>&lt;p&gt;
&#21152;&#36895;&#21463;&#38480;&#21046;&#30340;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#30340;&#21333;&#35843;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerated Single-Call Methods for Constrained Min-Max Optimization. (arXiv:2210.03096v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21333;&#35843;&#29992;&#21333;&#25237;&#24433;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#20248;&#21270;&#26799;&#24230;&#26041;&#27861;&#21644;&#21152;&#36895;&#21453;&#23556;&#26799;&#24230;&#26041;&#27861;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#28385;&#36275;&#24369;Minty&#21464;&#20998;&#19981;&#31561;&#24335;&#21644;&#36127;&#20849;&#21516;&#21333;&#35843;&#24615;&#30340;&#21253;&#21547;&#38382;&#39064;&#65292;&#21487;&#20197;&#21152;&#36895;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#65292;&#24182;&#20998;&#21035;&#21462;&#24471;&#20102;$O(\frac{1}{\sqrt{T}})$&#21644;$O(\frac{1}{T})$&#30340;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32422;&#26463;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#30340;&#19968;&#38454;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#27599;&#27425;&#36845;&#20195;&#35201;&#27714;&#20004;&#20010;&#26799;&#24230;&#35843;&#29992;&#25110;&#20004;&#20010;&#25237;&#24433;&#65292;&#36825;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#19968;&#31181;&#20048;&#35266;&#26799;&#24230;&#26041;&#27861;&#30340;&#21464;&#31181;&#65292;&#21363;&#21333;&#27425;&#35843;&#29992;&#21333;&#27425;&#25237;&#24433;&#31639;&#27861;&#65292;&#23545;&#20110;&#28385;&#36275;&#24369;Minty&#21464;&#20998;&#19981;&#31561;&#24335;&#65288;MVI&#65289;&#30340;&#21253;&#21547;&#38382;&#39064;&#65292;&#20855;&#26377;$O(\frac{1}{\sqrt{T}})$&#30340;&#26368;&#20339;&#36845;&#20195;&#25910;&#25947;&#29575;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#32467;&#26524;&#26159;&#31532;&#19968;&#20010;&#21333;&#35843;&#29992;&#21333;&#25237;&#24433;&#31639;&#27861;&#8212;&#8212;&#21152;&#36895;&#21453;&#23556;&#26799;&#24230;&#65288;ARG&#65289;&#26041;&#27861;&#65292;&#23427;&#23545;&#20110;&#28385;&#36275;&#36127;&#20849;&#21516;&#21333;&#35843;&#24615;&#30340;&#21253;&#21547;&#38382;&#39064;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;$O(\frac{1}{T})$&#21518;&#24335;&#25910;&#25947;&#29575;&#12290;&#24369;MVI&#21644;&#36127;&#20849;&#21516;&#21333;&#35843;&#24615;&#37117;&#26159;&#28145;&#20837;&#30740;&#31350;&#36807;&#30340;&#20551;&#35774;&#65292;&#28085;&#30422;&#20102;&#19968;&#32452;&#20016;&#23500;&#30340;&#38750;&#20984;&#38750;&#20985;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#21478;&#19968;&#31181;&#21333;&#35843;&#29992;&#21333;&#25237;&#24433;&#31639;&#27861;&#8212;&#8212;&#21453;&#23556;&#26799;&#24230;&#65288;RG&#65289;&#26041;&#27861;&#65292;&#20855;&#26377;$O(\frac{1}{\sqrt{T}})$&#30340;&#26368;&#20339;&#36845;&#20195;&#25910;&#25947;&#29575;&#65292;&#20294;&#38656;&#35201;&#20004;&#20010;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study first-order methods for constrained min-max optimization. Existing methods either require two gradient calls or two projections in each iteration, which may be costly in some applications. In this paper, we first show that a variant of the Optimistic Gradient (OG) method, a single-call single-projection algorithm, has $O(\frac{1}{\sqrt{T}})$ best-iterate convergence rate for inclusion problems with operators that satisfy the weak Minty variation inequality (MVI). Our second result is the first single-call single-projection algorithm -- the Accelerated Reflected Gradient (ARG) method that achieves the optimal $O(\frac{1}{T})$ last-iterate convergence rate for inclusion problems that satisfy negative comonotonicity. Both the weak MVI and negative comonotonicity are well-studied assumptions and capture a rich set of non-convex non-concave min-max optimization problems. Finally, we show that the Reflected Gradient (RG) method, another single-call single-projection algorithm, has $
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;CMA-MAE&#21464;&#20307;&#65292;&#21033;&#29992;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#25552;&#39640;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#65292;&#30456;&#36739;&#20110;ES&#22522;&#32447;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#36798;&#21040;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.02622</link><description>&lt;p&gt;
&#36890;&#36807;&#32553;&#25918;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#27169;&#25311;&#26469;&#35757;&#32451;&#22810;&#26679;&#21270;&#30340;&#39640;&#32500;&#24230;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Training Diverse High-Dimensional Controllers by Scaling Covariance Matrix Adaptation MAP-Annealing. (arXiv:2210.02622v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;CMA-MAE&#21464;&#20307;&#65292;&#21033;&#29992;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#25552;&#39640;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#65292;&#30456;&#36739;&#20110;ES&#22522;&#32447;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#36798;&#21040;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#25311;&#20013;&#39044;&#20808;&#35757;&#32451;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#36866;&#24212;&#36816;&#21160;&#20219;&#21153;&#20013;&#30340;&#25439;&#20260;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#22810;&#26679;&#24615;&#39640;&#19988;&#24615;&#33021;&#20248;&#24322;&#30340;&#25511;&#21046;&#22120;&#38656;&#35201;&#26114;&#36149;&#30340;&#32593;&#32476;&#35757;&#32451;&#21644;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#30456;&#27604;&#32780;&#35328;&#65292;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#8220;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#27169;&#25311;&#8221;&#27809;&#26377;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#22312;&#26631;&#20934;&#30340;QD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;CMA-MAE&#26080;&#27861;&#25193;&#23637;&#21040;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#21033;&#29992;ES&#20013;&#30340;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;CMA-MAE&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#33021;&#22815;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#21464;&#20307;&#22312;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;ES&#22522;&#32447;&#65292;&#21516;&#26102;&#21487;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#23218;&#32654;&#25110;&#36229;&#36234;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training a diverse set of neural network controllers in simulation has enabled robots to adapt online to damage in robot locomotion tasks. However, finding diverse, high-performing controllers requires expensive network training and extensive tuning of a large number of hyperparameters. On the other hand, Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), an evolution strategies (ES)-based quality diversity algorithm, does not have these limitations and has achieved state-of-the-art performance on standard QD benchmarks. However, CMA-MAE cannot scale to modern neural network controllers due to its quadratic complexity. We leverage efficient approximation methods in ES to propose three new CMA-MAE variants that scale to high dimensions. Our experiments show that the variants outperform ES-based baselines in benchmark robotic locomotion tasks, while being comparable with or exceeding state-of-the-art deep reinforcement learning-based quality diversity algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25913;&#21892;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#23454;&#29992;&#21644;&#20934;&#30830;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.15315</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#21453;&#24212;&#23454;&#29616;&#20998;&#23376;&#34920;&#31034;&#34701;&#21512;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
FusionRetro: Molecule Representation Fusion via In-context Reactions for Retrosynthetic Planning. (arXiv:2209.15315v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25913;&#21892;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#23454;&#29992;&#21644;&#20934;&#30830;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21512;&#25104;&#35268;&#21010;&#30340;&#30446;&#26631;&#26159;&#20174;&#36215;&#22987;&#29289;&#36136;&#21040;&#30446;&#26631;&#20998;&#23376;&#35774;&#35745;&#20986;&#19968;&#20010;&#23436;&#25972;&#30340;&#22810;&#27493;&#21512;&#25104;&#36335;&#32447;&#12290;&#24403;&#21069;&#31574;&#30053;&#37319;&#29992;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#21363;&#21333;&#27493;&#21453;&#21512;&#25104;&#27169;&#22411;&#21644;&#25628;&#32034;&#31639;&#27861;&#65292;&#21482;&#23558;&#20135;&#29289;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#27599;&#20010;&#35268;&#21010;&#27493;&#39588;&#30340;&#21453;&#24212;&#29289;&#65292;&#24182;&#24573;&#30053;&#20102;&#27839;&#30528;&#21512;&#25104;&#36335;&#32447;&#30340;&#26377;&#20215;&#20540;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#25913;&#21892;&#21453;&#21512;&#25104;&#35268;&#21010;&#12290;&#25105;&#20204;&#23558;&#21512;&#25104;&#36335;&#32447;&#35270;&#20026;&#21453;&#24212;&#22270;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#19977;&#20010;&#21407;&#21017;&#27493;&#39588;&#26469;&#25972;&#21512;&#20449;&#24687;&#21644;&#39044;&#27979;&#21453;&#24212;&#29289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#19978;&#19979;&#25991;&#21453;&#24212;&#36827;&#34892;&#21453;&#21512;&#25104;&#35268;&#21010;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25972;&#20010;&#26694;&#26550;&#21487;&#20197;&#39640;&#25928;&#22320;&#31471;&#21040;&#31471;&#22320;&#20248;&#21270;&#65292;&#20135;&#29983;&#26356;&#23454;&#29992;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#20013;&#34701;&#21512;&#20998;&#23376;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#30340;&#21453;&#21512;&#25104;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthetic planning aims to devise a complete multi-step synthetic route from starting materials to a target molecule. Current strategies use a decoupled approach of single-step retrosynthesis models and search algorithms, taking only the product as the input to predict the reactants for each planning step and ignoring valuable context information along the synthetic route. In this work, we propose a novel framework that utilizes context information for improved retrosynthetic planning. We view synthetic routes as reaction graphs and propose to incorporate context through three principled steps: encode molecules into embeddings, aggregate information over routes, and readout to predict reactants. Our approach is the first attempt to utilize in-context reactions for retrosynthetic planning. The entire framework can be efficiently optimized in an end-to-end fashion and produce more practical and accurate predictions. Comprehensive experiments demonstrate that by fusing in the context
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31215;&#20998;&#22120;&#31934;&#30830;&#20445;&#23432;&#24459;&#30340;&#26041;&#27861;&#12290;&#30456;&#23545;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#20123;&#20445;&#23432;&#24459;&#65292;&#26412;&#25991;&#21033;&#29992;&#35834;&#29305;&#23450;&#29702;&#23558;&#20854;&#22266;&#26377;&#22320;&#32467;&#21512;&#21040;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#20013;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#39044;&#27979;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2209.11661</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#31215;&#20998;&#22120;&#31934;&#30830;&#20445;&#23432;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exact conservation laws for neural network integrators of dynamical systems. (arXiv:2209.11661v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31215;&#20998;&#22120;&#31934;&#30830;&#20445;&#23432;&#24459;&#30340;&#26041;&#27861;&#12290;&#30456;&#23545;&#20110;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#20123;&#20445;&#23432;&#24459;&#65292;&#26412;&#25991;&#21033;&#29992;&#35834;&#29305;&#23450;&#29702;&#23558;&#20854;&#22266;&#26377;&#22320;&#32467;&#21512;&#21040;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#20013;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#39044;&#27979;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26102;&#21464;&#24494;&#20998;&#26041;&#31243;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25511;&#21046;&#35299;&#30340;&#28436;&#21270;&#30340;&#35268;&#24459;&#65292;&#20854;&#20013;&#25968;&#25454;&#21487;&#33021;&#20250;&#21463;&#21040;&#38543;&#26426;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#20294;&#26159;&#65292;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#19981;&#21516;&#30340;&#26159;&#65292;&#36890;&#24120;&#24050;&#32463;&#20102;&#35299;&#20102;&#35813;&#31995;&#32479;&#30340;&#35768;&#22810;&#20449;&#24687;&#65292;&#20363;&#22914;&#65292;&#23545;&#20110;&#35768;&#22810;&#21160;&#21147;&#23398;&#31995;&#32479;&#65292;&#33021;&#37327;&#25110;&#32773;&#65288;&#35282;&#65289;&#21160;&#37327;&#31561;&#29289;&#29702;&#37327;&#23558;&#34987;&#23436;&#20840;&#20445;&#30041;&#12290;&#22240;&#27492;&#65292;&#31070;&#32463;&#32593;&#32476;&#24517;&#39035;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#20123;&#23432;&#24658;&#23450;&#24459;&#65292;&#24182;&#19988;&#30001;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#38543;&#26426;&#22122;&#22768;&#65292;&#36825;&#20123;&#23432;&#24658;&#23450;&#24459;&#21482;&#33021;&#36817;&#20284;&#28385;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21478;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#35834;&#29305;&#23450;&#29702;&#23558;&#23432;&#24658;&#23450;&#24459;&#22266;&#26377;&#22320;&#32467;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#19977;&#20010;&#27169;&#22411;&#31995;&#32479;&#30340;&#39044;&#27979;&#25928;&#26524;&#26356;&#22909;&#65306;&#38750;&#30456;&#23545;&#35770;&#24773;&#20917;&#19979;&#19977;&#32500;&#29275;&#39039;&#24341;&#21147;&#21183;&#20013;&#29289;&#36136;&#31890;&#23376;&#30340;&#36816;&#21160;&#65292;&#21463;&#21046;&#21160;&#21147;&#23398;&#23450;&#24459;&#25903;&#37197;&#30340;&#26438;&#20174;&#25903;&#25745;&#28857;&#24748;&#25346;&#24182;&#26045;&#21152;&#24352;&#21147;&#65292;&#20197;&#21450;&#32447;&#24615;&#27874;&#21160;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution of time dependent differential equations with neural networks has attracted a lot of attention recently. The central idea is to learn the laws that govern the evolution of the solution from data, which might be polluted with random noise. However, in contrast to other machine learning applications, usually a lot is known about the system at hand. For example, for many dynamical systems physical quantities such as energy or (angular) momentum are exactly conserved. Hence, the neural network has to learn these conservation laws from data and they will only be satisfied approximately due to finite training time and random noise. In this paper we present an alternative approach which uses Noether's Theorem to inherently incorporate conservation laws into the architecture of the neural network. We demonstrate that this leads to better predictions for three model systems: the motion of a non-relativistic particle in a three-dimensional Newtonian gravitational potential, the moti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;&#30340;&#20122;&#23395;&#33410;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#28201;&#24230;&#21644;&#38477;&#27700;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.10666</link><description>&lt;p&gt;
&#25913;&#36827;&#20122;&#23395;&#33410;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Bias Correction for Improved Subseasonal Forecasting. (arXiv:2209.10666v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;&#30340;&#20122;&#23395;&#33410;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#28201;&#24230;&#21644;&#38477;&#27700;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20122;&#23395;&#33410;&#39044;&#27979;&#26159;&#39044;&#27979;&#26410;&#26469;2&#21040;6&#21608;&#28201;&#24230;&#21644;&#38477;&#27700;&#30340;&#37325;&#35201;&#25163;&#27573;&#65292;&#23545;&#20110;&#26377;&#25928;&#30340;&#27700;&#36164;&#28304;&#20998;&#37197;&#12289;&#37326;&#28779;&#31649;&#29702;&#20197;&#21450;&#24178;&#26097;&#21644;&#27946;&#28061;&#28798;&#23475;&#30340;&#32531;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20559;&#24046;&#26657;&#27491;&#65288;ABC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#65292;&#26088;&#22312;&#23545;&#27668;&#35937;&#21160;&#21147;&#23398;&#21644;&#29289;&#29702;&#23398;&#27169;&#22411;&#20013;&#30340;&#22266;&#26377;&#35823;&#24046;&#36827;&#34892;&#20462;&#27491;&#12290;&#22312;&#32654;&#22269;&#36830;&#32493;&#21306;&#22495;&#65292;&#24403;&#25105;&#20204;&#23558;ABC&#26041;&#27861;&#24212;&#29992;&#20110;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;&#65288;ECMWF&#65289;&#30340;&#39046;&#20808;&#20122;&#23395;&#33410;&#27169;&#22411;&#26102;&#65292;&#21457;&#29616;&#28201;&#24230;&#39044;&#27979;&#25216;&#24039;&#25552;&#39640;&#20102;60-90&#65285;&#65288;&#22522;&#32447;&#25216;&#24039;&#22312;0.18-0.25&#20043;&#38388;&#65289;&#65292;&#38477;&#27700;&#39044;&#27979;&#25216;&#24039;&#25552;&#39640;&#20102;40-69&#65285;&#65288;&#22522;&#32447;&#25216;&#24039;&#22312;0.11-0.15&#20043;&#38388;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subseasonal forecasting -- predicting temperature and precipitation 2 to 6 weeks ahead -- is critical for effective water allocation, wildfire management, and drought and flood mitigation. Recent international research efforts have advanced the subseasonal capabilities of operational dynamical models, yet temperature and precipitation prediction skills remain poor, partly due to stubborn errors in representing atmospheric dynamics and physics inside dynamical models. Here, to counter these errors, we introduce an adaptive bias correction (ABC) method that combines state-of-the-art dynamical forecasts with observations using machine learning. We show that, when applied to the leading subseasonal model from the European Centre for Medium-Range Weather Forecasts (ECMWF), ABC improves temperature forecasting skill by 60-90% (over baseline skills of 0.18-0.25) and precipitation forecasting skill by 40-69% (over baseline skills of 0.11-0.15) in the contiguous U.S. We couple these performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#26032;&#30340;&#20559;&#35265;&#38382;&#39064;&#65306;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#37325;&#22797;&#20351;&#29992;&#24863;&#30693;&#37325;&#35201;&#24615;&#21152;&#26435;&#65288;RAW&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;RAW&#26174;&#33879;&#25552;&#39640;&#20102;&#31163;&#32447;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.07074</link><description>&lt;p&gt;
&#20851;&#20110;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Reuse Bias in Off-Policy Reinforcement Learning. (arXiv:2209.07074v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#26032;&#30340;&#20559;&#35265;&#38382;&#39064;&#65306;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#37325;&#22797;&#20351;&#29992;&#24863;&#30693;&#37325;&#35201;&#24615;&#21152;&#26435;&#65288;RAW&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;RAW&#26174;&#33879;&#25552;&#39640;&#20102;&#31163;&#32447;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#37319;&#26679;&#26159;&#31163;&#32447;&#35780;&#20272;&#20013;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#36712;&#36857;&#25910;&#30410;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35757;&#32451;&#21487;&#33021;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#20197;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#23581;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#26512;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26041;&#24046;&#19978;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#20063;&#19982;&#19968;&#20010;&#26032;&#30340;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#26377;&#20851;&#8212;&#8212;&#30001;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#35780;&#20272;&#21644;&#20248;&#21270;&#37325;&#22797;&#20351;&#29992;&#36896;&#25104;&#30340;&#31163;&#32447;&#35780;&#20272;&#20013;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#24403;&#21069;&#31574;&#30053;&#30340;&#31163;&#32447;&#35780;&#20272;&#21644;&#20248;&#21270;&#19982;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#25968;&#25454;&#23548;&#33268;&#30446;&#26631;&#30340;&#36807;&#39640;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#26799;&#24230;&#26356;&#26032;&#24182;&#36864;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#19968;&#20010;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#30340;&#39640;&#27010;&#29575;&#19978;&#38480;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#24341;&#20837;&#31163;&#32447;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#27010;&#24565;&#65292;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;&#19978;&#38480;&#30340;&#26576;&#19968;&#39033;&#26469;&#25511;&#21046;&#37325;&#22797;&#20351;&#29992;&#20559;&#24046;&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#37325;&#22797;&#20351;&#29992;&#24863;&#30693;&#37325;&#35201;&#24615;&#21152;&#26435;&#65288;RAW&#65289;&#65292;&#26469;&#32416;&#27491;&#37325;&#22797;&#20351;&#29992;&#20559;&#35265;&#24182;&#25552;&#39640;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#26469;&#35777;&#26126;&#65292;RAW&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#31163;&#32447;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;DDPG&#12289;SAC&#21644;TD3&#12290;
&lt;/p&gt;
&lt;p&gt;
Importance sampling (IS) is a popular technique in off-policy evaluation, which re-weights the return of trajectories in the replay buffer to boost sample efficiency. However, training with IS can be unstable and previous attempts to address this issue mainly focus on analyzing the variance of IS. In this paper, we reveal that the instability is also related to a new notion of Reuse Bias of IS -- the bias in off-policy evaluation caused by the reuse of the replay buffer for evaluation and optimization. We theoretically show that the off-policy evaluation and optimization of the current policy with the data from the replay buffer result in an overestimation of the objective, which may cause an erroneous gradient update and degenerate the performance. We further provide a high-probability upper bound of the Reuse Bias, and show that controlling one term of the upper bound can control the Reuse Bias by introducing the concept of stability for off-policy algorithms. Based on these analyses
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.06049</link><description>&lt;p&gt;
&#38754;&#21521;&#27861;&#24459;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65306;&#20197;&#21360;&#24230;&#27861;&#24459;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22686;&#22810;&#65292;&#29305;&#21035;&#26159;&#22312;&#27431;&#32654;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#65292;PLMs&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21360;&#24230;&#31561;&#20854;&#20182;&#22269;&#23478;&#30340;&#27861;&#24459;&#25991;&#26412;&#20855;&#26377;&#24456;&#22810;&#29305;&#27530;&#29305;&#24449;&#65292;&#22240;&#27492;&#20063;&#38656;&#35201;&#22312;&#36825;&#20123;&#26041;&#38754;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21360;&#24230;&#27861;&#24459;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#65288;&#32487;&#32493;&#39044;&#35757;&#32451;&#65289;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#27861;&#24459;PLMs, LegalBERT&#21644;CaseLawBERT&#65292;&#20197;&#21450;&#20351;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;PLMs&#24212;&#29992;&#20110;&#19977;&#20010;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#8212;&#8212;&#20174;&#20107;&#23454;&#20013;&#35782;&#21035;&#27861;&#24459;&#27861;&#35268;&#12289;&#23545;&#27861;&#38498;&#21028;&#20915;&#25991;&#20214;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#20197;&#21450;&#39044;&#27979;&#27861;&#38498;&#19978;&#35785;&#21028;&#20915;--&#22312;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#25991;&#26412;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23884;&#20837;&#22270;&#24418;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24352;&#37327;&#31215;&#20197;&#21450;&#29699;&#24418;&#30721;&#23454;&#29616;&#39640;&#25928;&#21387;&#32553;&#21644;&#34920;&#24449;&#65292;&#22312;&#31232;&#30095;&#22270;&#34920;&#31034;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#25216;&#26415;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2208.10917</link><description>&lt;p&gt;
&#24352;&#37327;&#31215;&#19982;&#36817;&#20284;&#27491;&#20132;&#30721;&#30340;&#22270;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Embeddings via Tensor Products and Approximately Orthonormal Codes. (arXiv:2208.10917v4 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23884;&#20837;&#22270;&#24418;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24352;&#37327;&#31215;&#20197;&#21450;&#29699;&#24418;&#30721;&#23454;&#29616;&#39640;&#25928;&#21387;&#32553;&#21644;&#34920;&#24449;&#65292;&#22312;&#31232;&#30095;&#22270;&#34920;&#31034;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#22312;&#25216;&#26415;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#20197;&#20445;&#25345;&#32467;&#26500;&#26041;&#24335;&#26469;&#23884;&#20837;&#22270;&#24418;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#20016;&#23500;&#30340;&#34920;&#24449;&#33021;&#21147;&#24182;&#24314;&#31435;&#20102;&#19968;&#20123;&#29702;&#35770;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#23646;&#20110;&#32465;&#23450;&#21644;&#27714;&#21644;&#26041;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#26174;&#31034;&#20102;&#24352;&#37327;&#31215;&#26159;&#23562;&#37325;&#21472;&#21152;&#21407;&#29702;&#30340;&#26368;&#19968;&#33324;&#30340;&#32465;&#23450;&#25805;&#20316;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20123;&#31934;&#30830;&#30340;&#32467;&#26524;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#20351;&#29992;&#30340;&#29699;&#24418;&#30721;&#23454;&#29616;&#20102;&#19968;&#20010;&#35013;&#31665;&#19978;&#38480;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19982;&#37051;&#25509;&#30697;&#38453;&#30340;&#32852;&#31995;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#26159;&#19968;&#31181;&#37051;&#25509;&#30697;&#38453;&#30340;&#21387;&#32553;&#65292;&#20855;&#26377;&#31232;&#30095;&#22270;&#34920;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze a method for embedding graphs as vectors in a structure-preserving manner, showcasing its rich representational capacity and establishing some of its theoretical properties. Our procedure falls under the bind-and-sum approach, and we show that the tensor product is the most general binding operation that respects the superposition principle. We also establish some precise results characterizing the behavior of our method, and we show that our use of spherical codes achieves a packing upper bound. We establish a link to adjacency matrices, showing that our method is, in some sense, a compression of adjacency matrices with applications towards sparse graph representations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24050;&#22833;&#25928;&#65292;&#30456;&#20851;&#20869;&#23481;&#24050;&#34987;&#21512;&#24182;&#21040;&#21478;&#19968;&#31687;&#35770;&#25991;&#20013;&#12290;</title><link>http://arxiv.org/abs/2208.08769</link><description>&lt;p&gt;
&#22270;&#23884;&#20837;&#26041;&#27861;&#30340;&#35760;&#24518;&#19982;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Memory and Capacity of Graph Embedding Methods. (arXiv:2208.08769v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24050;&#22833;&#25928;&#65292;&#30456;&#20851;&#20869;&#23481;&#24050;&#34987;&#21512;&#24182;&#21040;&#21478;&#19968;&#31687;&#35770;&#25991;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24050;&#22833;&#25928;&#65306;&#35831;&#26597;&#30475;&#8220;&#36890;&#36807;&#24352;&#37327;&#31215;&#21644;&#36817;&#20284;&#27491;&#20132;&#30721;&#23454;&#29616;&#22270;&#23884;&#20837;&#8221;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#24050;&#23558;&#26412;&#25991;&#21512;&#24182;&#20026;&#19968;&#31687;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
THIS PAPER IS NOW DEFUNCT: Check out "Graph Embeddings via Tensor Products and Approximately Orthonormal Codes", where it has been combined into one paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#21450;&#20854;&#26816;&#27979;&#21644;&#32531;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.08085</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#26816;&#27979;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Detection and Mitigation of Byzantine Attacks in Distributed Training. (arXiv:2208.08085v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#21450;&#20854;&#26816;&#27979;&#21644;&#32531;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#38598;&#32676;&#20316;&#20026;&#35757;&#32451;&#27969;&#31243;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#20294;&#26159;&#24037;&#20316;&#33410;&#28857;&#30340;&#24322;&#24120;&#25308;&#21344;&#24237;&#34892;&#20026;&#21487;&#33021;&#20250;&#30772;&#22351;&#35757;&#32451;&#24182;&#21361;&#21450;&#25512;&#29702;&#30340;&#36136;&#37327;&#12290;&#27492;&#31867;&#34892;&#20026;&#21487;&#20197;&#24402;&#22240;&#20110;&#26080;&#24847;&#30340;&#31995;&#32479;&#25925;&#38556;&#25110;&#26377;&#32452;&#32455;&#30340;&#25915;&#20987;&#65292;&#32467;&#26524;&#21487;&#33021;&#26159;&#19968;&#20123;&#33410;&#28857;&#21521;&#21327;&#35843;&#35757;&#32451;&#30340;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#36820;&#22238;&#20219;&#24847;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#21508;&#31181;&#25915;&#20987;&#27169;&#22411;&#24182;&#25506;&#32034;&#20102;&#24378;&#22823;&#30340;&#32858;&#21512;&#21644;/&#25110;&#35745;&#31639;&#20887;&#20313;&#26469;&#32416;&#27491;&#25197;&#26354;&#30340;&#26799;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#25915;&#20987;&#27169;&#22411;&#20174;&#24378;&#21040;&#24369;&#19981;&#31561;&#65306;$ q $&#20855;&#26377;&#23436;&#20840;&#20102;&#35299;&#38450;&#24481;&#21327;&#35758;&#24182;&#19988;&#27599;&#36845;&#20195;&#37117;&#21487;&#20197;&#26356;&#25913;&#30340;&#20840;&#30693;&#23545;&#25163;&#21040;$ q $&#20855;&#26377;&#26377;&#38480;&#21246;&#32467;&#33021;&#21147;&#24182;&#19988;&#20165;&#27599;&#38548;&#20960;&#27425;&#36845;&#20195;&#26356;&#25913;&#30340;&#38543;&#26426;&#23545;&#25163;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#20887;&#20313;&#20219;&#21153;&#20998;&#37197;&#12289;&#22522;&#20110;&#38408;&#20540;&#30340;&#25509;&#25910;&#26799;&#24230;&#36807;&#28388;&#21644;&#22522;&#20110;&#26102;&#26399;&#30340;&#24037;&#20316;&#33410;&#28857;&#37325;&#21551;&#65292;&#20197;&#35782;&#21035;&#21644;&#32531;&#35299;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
A plethora of modern machine learning tasks require the utilization of large-scale distributed clusters as a critical component of the training pipeline. However, abnormal Byzantine behavior of the worker nodes can derail the training and compromise the quality of the inference. Such behavior can be attributed to unintentional system malfunctions or orchestrated attacks; as a result, some nodes may return arbitrary results to the parameter server (PS) that coordinates the training. Recent work considers a wide range of attack models and has explored robust aggregation and/or computational redundancy to correct the distorted gradients.  In this work, we consider attack models ranging from strong ones: $q$ omniscient adversaries with full knowledge of the defense protocol that can change from iteration to iteration to weak ones: $q$ randomly chosen adversaries with limited collusion abilities which only change every few iterations at a time. Our algorithms rely on redundant task assignme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#30005;&#36335;&#20986;&#29983;&#26426;&#22120;&#23545;&#22522;&#25968;&#21463;&#38480;&#20998;&#24067;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#25506;&#31350;&#20102;&#27492;&#33021;&#21147;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#22797;&#26434;&#20998;&#24067;.</title><link>http://arxiv.org/abs/2207.13645</link><description>&lt;p&gt;
&#37327;&#23376;&#30005;&#36335;&#20986;&#29983;&#26426;&#22120;&#26159;&#21542;&#20855;&#26377;&#25512;&#24191;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Quantum Circuit Born Machines Generalize?. (arXiv:2207.13645v4 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#30005;&#36335;&#20986;&#29983;&#26426;&#22120;&#23545;&#22522;&#25968;&#21463;&#38480;&#20998;&#24067;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#25506;&#31350;&#20102;&#27492;&#33021;&#21147;&#30340;&#36164;&#28304;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#22797;&#26434;&#20998;&#24067;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#30340;&#25552;&#35758;&#20013;&#65292;&#20851;&#20110;&#23427;&#20204;&#30340;&#34920;&#29616;&#30340;&#35752;&#35770;&#20165;&#38480;&#20110;&#23427;&#20204;&#37325;&#29616;&#24050;&#30693;&#30446;&#26631;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#34920;&#36798;&#21147;&#24378;&#30340;&#27169;&#22411;&#31995;&#21015;&#65292;&#20363;&#22914;&#37327;&#23376;&#30005;&#36335;&#20986;&#29983;&#26426;&#22120;&#65288;QCBMs&#65289;&#20960;&#20046;&#23436;&#20840;&#34987;&#35780;&#20272;&#20854;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#32473;&#23450;&#30446;&#26631;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36825;&#26041;&#38754;&#23545;&#20110;&#26576;&#20123;&#20219;&#21153;&#21487;&#33021;&#26159;&#29702;&#24819;&#30340;&#65292;&#20294;&#23427;&#38480;&#21046;&#20102;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#33539;&#22260;&#65292;&#32780;&#38750;&#20854;&#35760;&#24518;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20197;&#21450;&#27492;&#31867;&#33021;&#21147;&#19982;&#36164;&#28304;&#38656;&#27714;&#20043;&#38388;&#30340;&#20851;&#31995;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#27867;&#21270;&#24615;&#35780;&#20272;&#26694;&#26550;&#26469;&#24320;&#22987;&#35299;&#20915;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;QCBM&#23545;&#22522;&#25968;&#21463;&#38480;&#20998;&#24067;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#38543;&#30528;&#22522;&#25968;&#22686;&#21152;&#65292;&#27867;&#21270;&#24615;&#33021;&#26377;&#25152;&#25552;&#39640;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#39640;&#27867;&#21270;&#33021;&#21147;&#30340;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#23567;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25506;&#31350;&#20102;&#27492;&#33021;&#21147;&#30340;&#36164;&#28304;&#38656;&#27714;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20855;&#26377;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#22797;&#26434;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent proposals of quantum circuit models for generative tasks, the discussion about their performance has been limited to their ability to reproduce a known target distribution. For example, expressive model families such as Quantum Circuit Born Machines (QCBMs) have been almost entirely evaluated on their capability to learn a given target distribution with high accuracy. While this aspect may be ideal for some tasks, it limits the scope of a generative model's assessment to its ability to memorize data rather than generalize. As a result, there has been little understanding of a model's generalization performance and the relation between such capability and the resource requirements, e.g., the circuit depth and the amount of training data. In this work, we leverage upon a recently proposed generalization evaluation framework to begin addressing this knowledge gap. We first investigate the QCBM's learning process of a cardinality-constrained distribution and see an increase in ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21450;&#20854;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20511;&#21161;&#20960;&#20309;&#27979;&#37327;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#23454;&#34892;&#19968;&#20010;&#31616;&#21333;&#30340;&#31751;&#32467;&#26500;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.10541</link><description>&lt;p&gt;
&#25581;&#31034;&#25512;&#36827;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Latent Space Geometry of Push-Forward Generative Models. (arXiv:2207.10541v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21450;&#20854;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20511;&#21161;&#20960;&#20309;&#27979;&#37327;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20248;&#21270;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#23454;&#34892;&#19968;&#20010;&#31616;&#21333;&#30340;&#31751;&#32467;&#26500;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#37117;&#26159;&#36890;&#36807;&#36830;&#32493;&#29983;&#25104;&#22120;&#25512;&#36827;&#39640;&#26031;&#27979;&#37327;&#32780;&#23450;&#20041;&#30340;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#25110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#26412;&#25991;&#25506;&#31350;&#20102;&#36825;&#20123;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#22312;&#23398;&#20064;&#19981;&#36830;&#36890;&#20998;&#24067;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#36755;&#20986;&#36229;&#20986;&#30446;&#26631;&#20998;&#24067;&#25903;&#25345;&#33539;&#22260;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#23427;&#20204;&#30340;&#28508;&#22312;&#31354;&#38388;&#20960;&#20309;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20511;&#21161;&#20960;&#20309;&#27979;&#37327;&#29702;&#35770;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25105;&#20204;&#22312;&#28508;&#22312;&#31354;&#38388;&#30340;&#32500;&#24230;&#22823;&#20110;&#27169;&#30340;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#20248;&#21270;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#36890;&#36807;&#23545;GAN&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#29702;&#35770;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20960;&#20309;&#32467;&#26500;&#33719;&#24471;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24378;&#21046;&#23454;&#34892;&#19968;&#20010;&#31616;&#21333;&#30340;&#31751;&#32467;&#26500;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many deep generative models are defined as a push-forward of a Gaussian measure by a continuous generator, such as Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). This work explores the latent space of such deep generative models. A key issue with these models is their tendency to output samples outside of the support of the target distribution when learning disconnected distributions. We investigate the relationship between the performance of these models and the geometry of their latent space. Building on recent developments in geometric measure theory, we prove a sufficient condition for optimality in the case where the dimension of the latent space is larger than the number of modes. Through experiments on GANs, we demonstrate the validity of our theoretical results and gain new insights into the latent space geometry of these models. Additionally, we propose a truncation method that enforces a simplicial cluster structure in the latent space and improve
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#26085;&#24535;&#25991;&#20214;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#27169;&#22411;&#12289;&#25968;&#25454;&#39044;&#22788;&#29702;&#26426;&#21046;&#12289;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#30456;&#23545;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2207.03820</link><description>&lt;p&gt;
&#26085;&#24535;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Anomaly Detection in Log Data: A Survey. (arXiv:2207.03820v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#26085;&#24535;&#25991;&#20214;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#27169;&#22411;&#12289;&#25968;&#25454;&#39044;&#22788;&#29702;&#26426;&#21046;&#12289;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#30456;&#23545;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26085;&#24535;&#25991;&#20214;&#20998;&#26512;&#21487;&#20197;&#25552;&#21069;&#26816;&#27979;&#37325;&#35201;&#20107;&#20214;&#65292;&#22914;&#31995;&#32479;&#25925;&#38556;&#12290;&#23588;&#20854;&#26159;&#65292;&#33258;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#25429;&#25417;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#65292;&#24182;&#38543;&#21518;&#21521;&#31995;&#32479;&#25805;&#20316;&#21592;&#25253;&#21578;&#24847;&#22806;&#26085;&#24535;&#20107;&#20214;&#21457;&#29983;&#65292;&#26080;&#38656;&#25552;&#21069;&#25552;&#20379;&#25110;&#25163;&#21160;&#24314;&#27169;&#24322;&#24120;&#22330;&#26223;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#23436;&#25104;&#27492;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#27604;&#36739;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26816;&#27979;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#21516;&#26102;&#35299;&#20915;&#20102;&#25968;&#25454;&#26684;&#24335;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26377;&#35768;&#22810;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23558;&#26410;&#21152;&#22788;&#29702;&#30340;&#38750;&#32467;&#26500;&#21270;&#26085;&#24535;&#25968;&#25454;&#32534;&#30721;&#20197;&#20379;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#27010;&#36848;&#20102;&#37096;&#32626;&#30340;&#27169;&#22411;&#12289;&#25968;&#25454;&#39044;&#22788;&#29702;&#26426;&#21046;&#12289;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic log file analysis enables early detection of relevant incidents such as system failures. In particular, self-learning anomaly detection techniques capture patterns in log data and subsequently report unexpected log event occurrences to system operators without the need to provide or manually model anomalous scenarios in advance. Recently, an increasing number of approaches leveraging deep learning neural networks for this purpose have been presented. These approaches have demonstrated superior detection performance in comparison to conventional machine learning techniques and simultaneously resolve issues with unstable data formats. However, there exist many different architectures for deep learning and it is non-trivial to encode raw and unstructured log data to be analyzed by neural networks. We therefore carry out a systematic literature review that provides an overview of deployed models, data pre-processing mechanisms, anomaly detection techniques, and evaluations. The s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#30446;&#26631;&#30340;&#25299;&#25169;&#32467;&#26500;&#20197;&#21450;&#23618;&#27425;&#20998;&#21106;&#21644;&#24369;&#24179;&#28369;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#23458;&#25143;&#31471;&#25968;&#37327;&#21644;&#35780;&#20272;&#39044;&#31639;&#30456;&#20851;&#30340;&#27425;&#32447;&#24615;&#32047;&#35745;&#36951;&#25022;&#24230;&#65292;&#23545;&#25968;&#36890;&#20449;&#21482;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2205.15268</link><description>&lt;p&gt;
&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Federated X-Armed Bandit. (arXiv:2205.15268v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#30446;&#26631;&#30340;&#25299;&#25169;&#32467;&#26500;&#20197;&#21450;&#23618;&#27425;&#20998;&#21106;&#21644;&#24369;&#24179;&#28369;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#19982;&#23458;&#25143;&#31471;&#25968;&#37327;&#21644;&#35780;&#20272;&#39044;&#31639;&#30456;&#20851;&#30340;&#27425;&#32447;&#24615;&#32047;&#35745;&#36951;&#25022;&#24230;&#65292;&#23545;&#25968;&#36890;&#20449;&#21482;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#32852;&#37030; $\mathcal{X}$-armed bandit &#26694;&#26550;&#65292;&#19981;&#21516;&#23458;&#25143;&#31471;&#38754;&#20020;&#22312;&#30456;&#21516;&#22495;&#19978;&#23450;&#20041;&#30340;&#24322;&#26500;&#23616;&#37096;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#38656;&#35201;&#21327;&#20316;&#22320;&#25214;&#20986;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#27492;&#31867;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#32852;&#37030;&#31639;&#27861;&#65292;&#31216;&#20026; \texttt{Fed-PNE}&#12290;&#36890;&#36807;&#21033;&#29992;&#20840;&#23616;&#30446;&#26631;&#30340;&#25299;&#25169;&#32467;&#26500;&#20197;&#21450;&#23618;&#27425;&#20998;&#21106;&#21644;&#24369;&#24179;&#28369;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19982;&#23458;&#25143;&#31471;&#25968;&#37327;&#21644;&#35780;&#20272;&#39044;&#31639;&#30456;&#20851;&#30340;&#27425;&#32447;&#24615;&#32047;&#35745;&#36951;&#25022;&#24230;&#12290;&#21516;&#26102;&#65292;&#23427;&#21482;&#38656;&#35201;&#20013;&#22830;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#23545;&#25968;&#36890;&#20449;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#30340;&#38544;&#31169;&#12290;&#21512;&#25104;&#20989;&#25968;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102; \texttt{Fed-PNE} &#30456;&#23545;&#20110;&#21508;&#31181;&#38598;&#20013;&#24335;&#21644;&#32852;&#37030;&#22522;&#32447;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work establishes the first framework of federated $\mathcal{X}$-armed bandit, where different clients face heterogeneous local objective functions defined on the same domain and are required to collaboratively figure out the global optimum. We propose the first federated algorithm for such problems, named \texttt{Fed-PNE}. By utilizing the topological structure of the global objective inside the hierarchical partitioning and the weak smoothness property, our algorithm achieves sublinear cumulative regret with respect to both the number of clients and the evaluation budget. Meanwhile, it only requires logarithmic communications between the central server and clients, protecting the client privacy. Experimental results on synthetic functions and real datasets validate the advantages of \texttt{Fed-PNE} over various centralized and federated baseline algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#25554;&#20540;&#30340;&#20923;&#38684;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#27668;&#35937;&#31449;&#30340;&#27668;&#20505;&#25968;&#25454;&#12289;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#21644;&#26893;&#34987;&#25351;&#25968;&#25968;&#25454;&#65292;&#39044;&#27979;&#30446;&#26631;&#22330;&#22320;&#30340;&#19979;&#19968;&#23567;&#26102;&#26368;&#20302;&#28201;&#24230;&#65292;&#36798;&#21040;&#20102;92.55%&#30340;&#25506;&#27979;&#29575;&#12290;</title><link>http://arxiv.org/abs/2204.08465</link><description>&lt;p&gt;
&#22522;&#20110;&#26234;&#33021;&#31354;&#38388;&#25554;&#20540;&#30340;&#20923;&#38684;&#39044;&#27979;&#26041;&#27861;&#23398;&#8212;&#8212;&#26377;&#38480;&#23616;&#37096;&#25968;&#25454;&#19978;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Intelligent Spatial Interpolation-based Frost Prediction Methodology using Artificial Neural Networks with Limited Local Data. (arXiv:2204.08465v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#25554;&#20540;&#30340;&#20923;&#38684;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#27668;&#35937;&#31449;&#30340;&#27668;&#20505;&#25968;&#25454;&#12289;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#21644;&#26893;&#34987;&#25351;&#25968;&#25968;&#25454;&#65292;&#39044;&#27979;&#30446;&#26631;&#22330;&#22320;&#30340;&#19979;&#19968;&#23567;&#26102;&#26368;&#20302;&#28201;&#24230;&#65292;&#36798;&#21040;&#20102;92.55%&#30340;&#25506;&#27979;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20923;&#38684;&#22825;&#27668;&#29616;&#35937;&#23545;&#20892;&#19994;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#29616;&#22330;&#21382;&#21490;&#25968;&#25454;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#22240;&#27492;&#22312;&#26032;&#22330;&#22320;&#20013;&#38656;&#35201;&#32791;&#36153;&#39069;&#22806;&#30340;&#26102;&#38388;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#37096;&#32626;&#12290;&#26412;&#25991;&#26088;&#22312;&#28040;&#38500;&#20923;&#38684;&#39044;&#27979;&#26041;&#27861;&#23545;&#29616;&#22330;&#21382;&#21490;&#25968;&#25454;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31354;&#38388;&#25554;&#20540;&#30340;&#20923;&#38684;&#39044;&#27979;&#26041;&#27861;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#29616;&#26377;&#27668;&#35937;&#31449;&#30340;&#27668;&#20505;&#25968;&#25454;&#12289;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#21208;&#27979;&#21644;&#24402;&#19968;&#21270;&#24046;&#24322;&#26893;&#34987;&#25351;&#25968;&#25968;&#25454;&#26469;&#39044;&#27979;&#30446;&#26631;&#22330;&#22320;&#30340;&#19979;&#19968;&#23567;&#26102;&#26368;&#20302;&#28201;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#27668;&#20505;&#25968;&#25454;&#38598;&#26469;&#33258;&#28595;&#22823;&#21033;&#20122;&#26032;&#21335;&#23041;&#23572;&#22763;&#21644;&#39318;&#37117;&#39046;&#22320;&#22320;&#21306;&#30340;75&#20010;&#27668;&#35937;&#31449;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#30340;&#25506;&#27979;&#29575;&#26368;&#39640;&#21487;&#36798;92.55%&#12290;
&lt;/p&gt;
&lt;p&gt;
The weather phenomenon of frost poses great threats to agriculture. As recent frost prediction methods are based on on-site historical data and sensors, extra development and deployment time are required for data collection in any new site. The aim of this article is to eliminate the dependency on on-site historical data and sensors for frost prediction methods. In this article, a frost prediction method based on spatial interpolation is proposed. The models use climate data from existing weather stations, digital elevation models surveys, and normalized difference vegetation index data to estimate a target site's next hour minimum temperature. The proposed method utilizes ensemble learning to increase the model accuracy. Climate datasets are obtained from 75 weather stations across New South Wales and Australian Capital Territory areas of Australia. The results show that the proposed method reached a detection rate up to 92.55%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PADA&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#38754;&#21152;&#20837;&#21098;&#26525;&#31574;&#30053;&#65292;&#20351;&#29992;CD-TAW&#26041;&#27861;&#20174;&#31934;&#32454;&#35843;&#25972;&#30340;OOT&#27169;&#22411;&#20013;&#33719;&#24471;&#21021;&#22987;&#21098;&#26525;&#25513;&#30721;&#65292;&#24182;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.16965</link><description>&lt;p&gt;
PADA: &#22522;&#20110;&#21098;&#26525;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PADA&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#38754;&#21152;&#20837;&#21098;&#26525;&#31574;&#30053;&#65292;&#20351;&#29992;CD-TAW&#26041;&#27861;&#20174;&#31934;&#32454;&#35843;&#25972;&#30340;OOT&#27169;&#22411;&#20013;&#33719;&#24471;&#21021;&#22987;&#21098;&#26525;&#25513;&#30721;&#65292;&#24182;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#21487;&#29992;&#20110;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#20250;&#20986;&#29616;&#23545;&#26410;&#26631;&#27880;&#25968;&#25454;&#26469;&#28304;&#39046;&#22495;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#20026;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PADA (Pruning Assisted Domain Adaptation)&#65292;&#24182;&#20174;&#32463;&#36807;&#22823;&#37327;OOT(Out-of-domain)&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#20943;&#21435;&#22810;&#20313;&#30340;&#26435;&#37325;&#65292;&#20026;&#30446;&#26631;&#39046;&#22495;&#30340;ASR&#24494;&#35843;&#33150;&#20986;&#31354;&#38388;&#12290;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#21098;&#26525;&#31574;&#30053;&#26469;&#35782;&#21035;&#20887;&#20313;&#26435;&#37325;&#65292;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#26368;&#36817;&#21457;&#29616;&#30340;Task-Agnostic&#21644;Task-Aware&#21098;&#26525;&#23545;PADA&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21518;&#32773;&#30340;&#21098;&#26525;&#33539;&#24335;&#65292;&#31216;&#20026;Cross-Domain Task-Aware Pruning(CD-TAW)&#12290;CD-TAW&#20174;&#31934;&#32454;&#35843;&#25972;&#30340;OOT&#27169;&#22411;&#20013;&#33719;&#24471;&#21021;&#22987;&#21098;&#26525;&#25513;&#30721;&#65292;&#36825;&#20351;&#20854;&#19982;&#26412;&#25991;&#20013;&#35752;&#35770;&#30340;&#21098;&#26525;&#31574;&#30053;&#25130;&#28982;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;PADA&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#21033;&#29992;&#21098;&#26525;&#25216;&#26415;&#26469;&#28040;&#38500;&#19981;&#24517;&#35201;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20351;&#29992;CD-TAW&#21098;&#26525;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-supervised speech representation learning (SSL) models serve a variety of downstream tasks, these models have been observed to overfit to the domain from which the unlabelled data originates. To alleviate this issue, we propose PADA (Pruning Assisted Domain Adaptation) and zero out redundant weights from models pre-trained on large amounts of out-of-domain (OOD) data. Intuitively, this helps to make space for the target-domain ASR finetuning. The redundant weights can be identified through various pruning strategies which have been discussed in detail as a part of this work. Specifically, we investigate the effect of the recently discovered Task-Agnostic and Task-Aware pruning on PADA and propose a new pruning paradigm based on the latter, which we call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial pruning mask from a well fine-tuned OOD model, which makes it starkly different from the rest of the pruning strategies discussed in the paper. Our proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28789;&#27963;&#35774;&#35745;&#30340;&#22810;&#37327;&#23376;&#27604;&#29305;&#38376;&#25805;&#20316;&#30340;&#37327;&#23376;&#21464;&#20998;&#25351;&#20196;&#38598;(QuVIS)&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#37327;&#23376;&#35745;&#31639;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20986;&#20102;&#26356;&#20302;&#30340;&#35823;&#24046;&#31215;&#32047;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2203.15574</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#20998;&#25351;&#20196;&#38598;&#30340;&#37327;&#23376;&#32534;&#35793;&#20248;&#21270;&#65306;&#23454;&#29616;&#39640;&#31934;&#24230;&#19982;&#24555;&#36895;&#37327;&#23376;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Quantum compiling with variational instruction set for accurate and fast quantum computing. (arXiv:2203.15574v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28789;&#27963;&#35774;&#35745;&#30340;&#22810;&#37327;&#23376;&#27604;&#29305;&#38376;&#25805;&#20316;&#30340;&#37327;&#23376;&#21464;&#20998;&#25351;&#20196;&#38598;(QuVIS)&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#20248;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#37327;&#23376;&#35745;&#31639;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20986;&#20102;&#26356;&#20302;&#30340;&#35823;&#24046;&#31215;&#32047;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#25351;&#20196;&#38598;(QIS)&#23450;&#20041;&#20026;&#22312;&#25511;&#21046;&#37327;&#23376;&#27604;&#29305;&#29366;&#24577;&#19979;&#21487;&#20197;&#29289;&#29702;&#23454;&#29616;&#30340;&#19968;&#31995;&#21015;&#37327;&#23376;&#38376;&#25805;&#20316;&#65292;&#20854;&#22312;&#37327;&#23376;&#35745;&#31639;&#20013;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28789;&#27963;&#35774;&#35745;&#30340;&#22810;&#37327;&#23376;&#27604;&#29305;&#38376;&#25805;&#20316;&#30340;&#37327;&#23376;&#21464;&#20998;&#25351;&#20196;&#38598;(QuVIS)&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#20248;&#21270;&#31639;&#27861;&#26469;&#21464;&#20998;&#23454;&#29616;&#37327;&#23376;&#27604;&#29305;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#21644;&#31934;&#30830;&#30340;&#37327;&#23376;&#35745;&#31639;&#12290;&#19982;&#26631;&#20934;QIS &#22914;&#37327;&#23376;&#24494;&#25351;&#20196;&#38598;(QuMIS)&#30456;&#27604;&#65292;QuVIS &#29992;&#20110;&#22810;&#37327;&#23376;&#27604;&#29305;&#20132;&#25442;&#21644;&#37327;&#23376;&#20613;&#37324;&#21494;&#21464;&#25442;&#31561;&#38376;&#25805;&#20316;&#20855;&#26377;&#26356;&#20302;&#30340;&#35823;&#24046;&#31215;&#32047;&#21644;&#26102;&#38388;&#25104;&#26412;&#12290;&#22312;&#30456;&#21516;&#37327;&#23376;&#30828;&#20214;&#35201;&#27714;&#19979;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#21487;&#22823;&#24133;&#25552;&#21319;&#37327;&#23376;&#35745;&#31639;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantum instruction set (QIS) is defined as the quantum gates that are physically realizable by controlling the qubits in a quantum hardware. Compiling quantum circuits into the product of the gates in a properly-defined QIS is a fundamental step in quantum computing. We here propose the \R{quantum variational instruction set (QuVIS)} formed by flexibly-designed multi-qubit gates for higher speed and accuracy of quantum computing. The controlling of qubits for realizing the gates in a QuVIS are variationally achieved using the fine-grained time optimization algorithm. Significant reductions on both the error accumulation and time cost are demonstrated in realizing the swaps of multiple qubits and quantum Fourier transformations, compared with the compiling by the standard QIS such as \RR{the quantum microinstruction set} (QuMIS, formed by several one- and two-qubit gates including the one-qubit rotations and controlled-NOT gate). With the same requirement on quantum hardware, the t
&lt;/p&gt;</description></item><item><title>CD-GAN&#26159;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#24322;&#26500;&#20256;&#24863;&#22120;&#30340;&#36965;&#24863;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#34701;&#21512;&#25216;&#26415;&#36827;&#23637;&#21644;&#23545;&#25239;&#24615;&#32593;&#32476;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#20256;&#24863;&#22120;&#31354;&#38388;&#21644;/&#25110;&#20809;&#35889;&#20998;&#36776;&#29575;&#19981;&#21516;&#36896;&#25104;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2203.00948</link><description>&lt;p&gt;
CD-GAN:&#19968;&#31181;&#22522;&#20110;&#34701;&#21512;&#30340;&#24378;&#22823;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20855;&#26377;&#24322;&#26500;&#20256;&#24863;&#22120;&#30340;&#26080;&#30417;&#30563;&#36965;&#24863;&#21464;&#21270;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
CD-GAN: a robust fusion-based generative adversarial network for unsupervised remote sensing change detection with heterogeneous sensors. (arXiv:2203.00948v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00948
&lt;/p&gt;
&lt;p&gt;
CD-GAN&#26159;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#24322;&#26500;&#20256;&#24863;&#22120;&#30340;&#36965;&#24863;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#34701;&#21512;&#25216;&#26415;&#36827;&#23637;&#21644;&#23545;&#25239;&#24615;&#32593;&#32476;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#20013;&#20256;&#24863;&#22120;&#31354;&#38388;&#21644;/&#25110;&#20809;&#35889;&#20998;&#36776;&#29575;&#19981;&#21516;&#36896;&#25104;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#29699;&#35266;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#21464;&#21270;&#26816;&#27979;&#26159;&#36890;&#36807;&#22810;&#26102;&#30456;&#22270;&#20687;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#20123;&#22270;&#20687;&#30001;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#31354;&#38388;&#21644;/&#25110;&#20809;&#35889;&#20998;&#36776;&#29575;&#29978;&#33267;&#19981;&#21516;&#27169;&#24577;&#65288;&#20363;&#22914;&#20809;&#23398;&#65292;&#38647;&#36798;&#65289;&#30340;&#20256;&#24863;&#22120;&#33719;&#21462;&#12290; &#21363;&#20351;&#38480;&#21046;&#20026;&#20809;&#23398;&#27169;&#24577;&#65292;&#21482;&#35201;&#20256;&#24863;&#22120;&#20855;&#26377;&#19981;&#21516;&#30340;&#31354;&#38388;&#21644;/&#25110;&#20809;&#35889;&#20998;&#36776;&#29575;&#65292;&#36825;&#39033;&#20219;&#21153;&#23601;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#36825;&#31181;&#25152;&#35859;&#30340;&#24322;&#26500;&#20809;&#23398;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#22270;&#20687;&#30340;&#26080;&#30417;&#30563;&#21464;&#21270;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#23558;&#21464;&#21270;&#26816;&#27979;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#24378;&#22823;&#30340;&#34701;&#21512;&#26694;&#26550;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28145;&#24230;&#23545;&#25239;&#32593;&#32476;&#65292;&#26088;&#22312;&#20107;&#20808;&#35774;&#35745;&#21644;&#35757;&#32451;&#20197;&#34701;&#21512;&#19968;&#23545;&#22810;&#27874;&#27573;&#20809;&#23398;&#22270;&#20687;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36890;&#36807;&#20855;&#26377;&#30456;&#21516;&#26550;&#26500;&#30340;&#32593;&#32476;&#26469;&#34917;&#20805;&#25191;&#34892;&#21464;&#21270;&#26816;&#27979;&#12290;&#29983;&#25104;&#30340;&#25972;&#20307;&#26550;&#26500;&#26412;&#36523;&#36981;&#24490;&#23545;&#25239;&#24615;&#31574;&#30053;&#65292;&#20854;&#20013;&#34701;&#21512;&#32593;&#32476;&#21644;&#20854;&#20182;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of Earth observation, the detection of changes is performed from multitemporal images acquired by sensors with possibly different spatial and/or spectral resolutions or even different modalities (e.g. optical, radar). Even limiting to the optical modality, this task has proved to be challenging as soon as the sensors have different spatial and/or spectral resolutions. This paper proposes a novel unsupervised change detection method dedicated to images acquired with such so-called heterogeneous optical sensors. This method capitalizes on recent advances which frame the change detection problem into a robust fusion framework. More precisely, we show that a deep adversarial network designed and trained beforehand to fuse a pair of multiband optical images can be easily complemented by a network with the same architecture to perform change detection. The resulting overall architecture itself follows an adversarial strategy where the fusion network and the additional network 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25237;&#24433;SARSA&#21040;&#26377;&#38480;&#21306;&#22495;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#25506;&#31350;&#65292;&#21462;&#24471;&#20102;&#22312;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;SARSA&#31639;&#27861;&#25910;&#25947;&#24615;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21457;&#29616;&#25910;&#25947;&#21306;&#22495;&#27604;&#24819;&#35937;&#30340;&#35201;&#23567;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2202.06828</link><description>&lt;p&gt;
&#20851;&#20110;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;SARSA&#31639;&#27861;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of SARSA with Linear Function Approximation. (arXiv:2202.06828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25237;&#24433;SARSA&#21040;&#26377;&#38480;&#21306;&#22495;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#25506;&#31350;&#65292;&#21462;&#24471;&#20102;&#22312;&#24102;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;SARSA&#31639;&#27861;&#25910;&#25947;&#24615;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21457;&#29616;&#25910;&#25947;&#21306;&#22495;&#27604;&#24819;&#35937;&#30340;&#35201;&#23567;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SARSA&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#22686;&#24378;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#19982;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30456;&#32467;&#21512;&#26102;&#20250;&#20986;&#29616;&#38663;&#33633;&#29616;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#25237;&#24433;SARSA&#21040;&#26377;&#38480;&#21306;&#22495;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21462;&#24471;&#20102;&#22312;&#27492;&#26041;&#38754;&#30340;&#19968;&#23450;&#36827;&#23637;&#12290;&#24778;&#20154;&#30340;&#26159;&#65292;&#21482;&#35201;&#22870;&#21169;&#30340;&#22823;&#23567;&#19981;&#26159;&#22826;&#22823;&#65292;&#25910;&#25947;&#21306;&#22495;&#27604;&#24819;&#35937;&#20013;&#30340;&#35201;&#23567;&#24471;&#22810;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#20851;&#20110;&#32447;&#24615;SARSA&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;&#37117;&#38656;&#35201;SARSA&#31574;&#30053;&#25913;&#36827;&#31639;&#23376;&#30340;Lipschitz&#24120;&#25968;&#36275;&#22815;&#23567;&#65307;&#19982;&#20043;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36866;&#29992;&#20110;&#20219;&#24847;Lipschitz&#24120;&#25968;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#32447;&#24615;SARSA&#30340;&#26032;&#34892;&#20026;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
SARSA, a classical on-policy control algorithm for reinforcement learning, is known to chatter when combined with linear function approximation: SARSA does not diverge but oscillates in a bounded region. However, little is known about how fast SARSA converges to that region and how large the region is. In this paper, we make progress towards this open problem by showing the convergence rate of projected SARSA to a bounded region. Importantly, the region is much smaller than the region that we project into, provided that the magnitude of the reward is not too large. Existing works regarding the convergence of linear SARSA to a fixed point all require the Lipschitz constant of SARSA's policy improvement operator to be sufficiently small; our analysis instead applies to arbitrary Lipschitz constants and thus characterizes the behavior of linear SARSA for a new regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#25209;&#27425;&#22823;&#23567;&#19982;&#35757;&#32451;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20026;&#20102;&#25214;&#21040;&#31283;&#23450;&#28857;&#65292;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20250;&#20943;&#23569;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2201.11989</link><description>&lt;p&gt;
&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#30340;&#23384;&#22312;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#25209;&#27425;&#22823;&#23567;&#19982;&#35757;&#32451;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20026;&#20102;&#25214;&#21040;&#31283;&#23450;&#28857;&#65292;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20250;&#20943;&#23569;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#65292;&#22914;&#19981;&#21516;&#30340;&#24658;&#23450;&#29575;&#25110;&#19981;&#21516;&#30340;&#34928;&#20943;&#29575;&#31561;&#65292;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#26377;&#21161;&#20110;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#27492;&#22806;&#65292;&#25209;&#27425;&#22823;&#23567;&#23545;&#20110;&#20351;&#29992;TTUR&#35757;&#32451;GANs&#20063;&#24456;&#37325;&#35201;&#65292;&#20004;&#32773;&#37117;&#24433;&#21709;&#20102;&#35757;&#32451;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#37327;&#12290;&#26412;&#25991;&#22522;&#20110;&#24658;&#23450;&#23398;&#20064;&#29575;&#30740;&#31350;&#20102;&#25209;&#27425;&#22823;&#23567;&#19982;&#20351;&#29992;TTUR&#35757;&#32451;GANs&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;TTUR&#65292;&#20026;&#20102;&#25214;&#21040;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#65292;&#25152;&#38656;&#27493;&#39588;&#25968;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Fr'echet Inception Distance&#65288;FID&#65289;&#20316;&#20026;&#35757;&#32451;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#19979;Bayes&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#21487;&#20197;&#20026;&#30740;&#31350;&#23545;&#25239;&#24615;&#20195;&#29702;&#25439;&#22833;&#21644;&#20854;&#19968;&#33268;&#24615;&#23646;&#24615;&#25552;&#20379;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2112.01694</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#25239;&#24615;Bayes&#20998;&#31867;&#22120;&#23384;&#22312;&#24615;&#30340;&#30740;&#31350;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
On the Existence of the Adversarial Bayes Classifier (Extended Version). (arXiv:2112.01694v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#19979;Bayes&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#33324;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#21487;&#20197;&#20026;&#30740;&#31350;&#23545;&#25239;&#24615;&#20195;&#29702;&#25439;&#22833;&#21644;&#20854;&#19968;&#33268;&#24615;&#23646;&#24615;&#25552;&#20379;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#26368;&#36817;&#24050;&#32463;&#26377;&#22810;&#39033;&#29702;&#35770;&#30740;&#31350;&#65292;&#20294;&#19982;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#30456;&#20851;&#30340;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#20173;&#28982;&#26410;&#34987;&#35299;&#20915;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20851;&#20110;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#19979;Bayes&#26368;&#20248;&#20998;&#31867;&#22120;&#23384;&#22312;&#24615;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33324;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20445;&#35777;&#23384;&#22312;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#19979;&#30340;Bayes&#26368;&#20248;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#20026;&#23545;&#21518;&#32493;&#23545;&#25239;&#35757;&#32451;&#20581;&#22766;&#24615;&#19979;&#20195;&#29702;&#25439;&#22833;&#21644;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#23646;&#24615;&#30340;&#30740;&#31350;&#25552;&#20379;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#26159;&#8220;&#20851;&#20110;&#23545;&#25239;&#24615;Bayes&#20998;&#31867;&#22120;&#23384;&#22312;&#24615;&#8221;&#30340;&#30699;&#27491;&#21644;&#25193;&#23637;&#29256;&#26412;&#65292;&#35813;&#31295;&#20214;&#24050;&#21457;&#34920;&#22312;NeurIPS 2021&#19978;&#12290;&#21407;&#22987;&#35770;&#25991;&#20013;&#26377;&#20004;&#22788;&#23450;&#29702;&#38169;&#35823;&#65292;&#19968;&#22788;&#26159;&#23545;&#20266;&#21487;&#35777;&#20581;&#22766;&#24615;&#30340;&#23450;&#20041;&#65292;&#21478;&#19968;&#22788;&#26159;&#38024;&#23545;&#20219;&#24847;&#24230;&#37327;&#31354;&#38388;&#30340;$A^\e$&#21487;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness is a critical property in a variety of modern machine learning applications. While it has been the subject of several recent theoretical studies, many important questions related to adversarial robustness are still open. In this work, we study a fundamental question regarding Bayes optimality for adversarial robustness. We provide general sufficient conditions under which the existence of a Bayes optimal classifier can be guaranteed for adversarial robustness. Our results can provide a useful tool for a subsequent study of surrogate losses in adversarial robustness and their consistency properties. This manuscript is the extended and corrected version of the paper \emph{On the Existence of the Adversarial Bayes Classifier} published in NeurIPS 2021. There were two errors in theorem statements in the original paper -- one in the definition of pseudo-certifiable robustness and the other in the measurability of $A^\e$ for arbitrary metric spaces. In this version we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;&#21487;&#31359;&#25140;&#35774;&#22791;&#30417;&#27979;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#25214;&#21040;&#19982;&#25351;&#23450;&#20027;&#39064;&#30456;&#20851;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#30561;&#30496;&#36136;&#37327;&#27963;&#21160;&#24314;&#35758;&#65292;&#20026;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2110.13745</link><description>&lt;p&gt;
PARIS&#65306;&#29992;&#20110;&#25913;&#21892;&#30561;&#30496;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PARIS: Personalized Activity Recommendation for Improving Sleep Quality. (arXiv:2110.13745v1 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#21512;&#21487;&#31359;&#25140;&#35774;&#22791;&#30417;&#27979;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#25214;&#21040;&#19982;&#25351;&#23450;&#20027;&#39064;&#30456;&#20851;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#29983;&#25104;&#30456;&#24212;&#30340;&#30561;&#30496;&#36136;&#37327;&#27963;&#21160;&#24314;&#35758;&#65292;&#20026;&#25552;&#39640;&#30561;&#30496;&#36136;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#36136;&#37327;&#23545;&#20154;&#20204;&#30340;&#36523;&#20307;&#21644;&#24515;&#29702;&#20581;&#24247;&#26377;&#28145;&#36828;&#24433;&#21709;&#12290;&#30561;&#30496;&#19981;&#36275;&#30340;&#20154;&#26356;&#23481;&#26131;&#25253;&#21578;&#36523;&#20307;&#21644;&#24515;&#29702;&#22256;&#25200;&#12289;&#27963;&#21160;&#21463;&#38480;&#12289;&#28966;&#34385;&#21644;&#30140;&#30171;&#12290;&#27492;&#22806;&#65292;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#27963;&#21160;&#30417;&#27979;&#21644;&#20581;&#24247;&#36319;&#36394;&#30340;&#24212;&#29992;&#21644;&#35774;&#22791;&#26041;&#20852;&#26410;&#33406;&#12290;&#20174;&#36825;&#20123;&#21487;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#21040;&#30340;&#20449;&#21495;&#21487;&#29992;&#20110;&#30740;&#31350;&#21644;&#25913;&#21892;&#30561;&#30496;&#36136;&#37327;&#12290;&#26412;&#25991;&#21033;&#29992;&#23454;&#20307;&#27963;&#21160;&#21644;&#30561;&#30496;&#36136;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25214;&#21040;&#21327;&#21161;&#20154;&#20204;&#25913;&#21892;&#30561;&#30496;&#30340;&#26041;&#27861;&#12290;&#23545;&#27963;&#21160;&#25968;&#25454;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#65292;&#25105;&#20204;&#25214;&#21040;&#19982;&#29305;&#23450;&#20027;&#39064;&#26368;&#26174;&#30528;&#30340;&#34892;&#20026;&#27169;&#24335;&#30456;&#20851;&#30340;&#31751;&#20013;&#24515;&#12290;&#28982;&#21518;&#20026;&#27599;&#20010;&#34892;&#20026;&#27169;&#24335;&#20013;&#30340;&#27599;&#20010;&#31751;&#29983;&#25104;&#26377;&#21161;&#20110;&#33391;&#22909;&#30561;&#30496;&#36136;&#37327;&#30340;&#27963;&#21160;&#24314;&#35758;&#12290;&#36825;&#20123;&#27963;&#21160;&#24314;&#35758;&#20379;&#24212;&#32473;&#27599;&#20301;&#29992;&#25143;&#30340;&#20010;&#24615;&#21270;&#27963;&#21160;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of sleep has a deep impact on people's physical and mental health. People with insufficient sleep are more likely to report physical and mental distress, activity limitation, anxiety, and pain. Moreover, in the past few years, there has been an explosion of applications and devices for activity monitoring and health tracking. Signals collected from these wearable devices can be used to study and improve sleep quality. In this paper, we utilize the relationship between physical activity and sleep quality to find ways of assisting people improve their sleep using machine learning techniques. People usually have several behavior modes that their bio-functions can be divided into. Performing time series clustering on activity data, we find cluster centers that would correlate to the most evident behavior modes for a specific subject. Activity recipes are then generated for good sleep quality for each behavior mode within each cluster. These activity recipes are supplied to an a
&lt;/p&gt;</description></item><item><title>AxoNN&#26159;&#19968;&#31181;&#21033;&#29992;&#24322;&#27493;&#24615;&#21644;&#28040;&#24687;&#39537;&#21160;&#25191;&#34892;&#35843;&#24230;&#27599;&#20010;GPU&#19978;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;CPU&#20869;&#23384;&#20316;&#20026;&#20887;&#20313;&#31354;&#38388;&#65292;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#23558;&#27599;&#20010;GPU&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#24517;&#38656;&#30340;GPU&#25968;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2110.13005</link><description>&lt;p&gt;
AxoNN: &#19968;&#31181;&#24322;&#27493;&#12289;&#28040;&#24687;&#39537;&#21160;&#30340;&#26497;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13005
&lt;/p&gt;
&lt;p&gt;
AxoNN&#26159;&#19968;&#31181;&#21033;&#29992;&#24322;&#27493;&#24615;&#21644;&#28040;&#24687;&#39537;&#21160;&#25191;&#34892;&#35843;&#24230;&#27599;&#20010;GPU&#19978;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;CPU&#20869;&#23384;&#20316;&#20026;&#20887;&#20313;&#31354;&#38388;&#65292;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#23558;&#27599;&#20010;GPU&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#24517;&#38656;&#30340;GPU&#25968;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#23384;&#20648;&#22120;&#23481;&#37327;&#24050;&#36828;&#36828;&#36229;&#20986;&#29616;&#20195;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;DRAM&#23481;&#37327;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#22522;&#20110;GPU&#30340;&#38598;&#32676;&#19978;&#24320;&#21457;&#39640;&#25928;&#31639;&#27861;&#24182;&#34892;&#35757;&#32451;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#29616;&#20195;GPU&#19978;&#65292;&#35745;&#31639;&#30456;&#23545;&#24265;&#20215;&#65292;&#20026;&#20102;&#25552;&#21462;&#26368;&#22823;&#24615;&#33021;&#65292;&#35774;&#35745;&#21644;&#23454;&#29616;&#36825;&#20123;&#24182;&#34892;&#35757;&#32451;&#31639;&#27861;&#20013;&#26497;&#20854;&#39640;&#25928;&#30340;&#36890;&#20449;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AxoNN&#65292;&#19968;&#31181;&#21033;&#29992;&#24322;&#27493;&#24615;&#21644;&#28040;&#24687;&#39537;&#21160;&#25191;&#34892;&#35843;&#24230;&#27599;&#20010;GPU&#19978;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#31354;&#38386;&#26102;&#38388;&#65292;&#26368;&#22823;&#21270;&#30828;&#20214;&#25928;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;CPU&#20869;&#23384;&#20316;&#20026;&#20887;&#20313;&#31354;&#38388;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#23450;&#26399;&#21368;&#36733;&#25968;&#25454;&#65292;AxoNN&#33021;&#22815;&#23558;GPU&#20869;&#23384;&#28040;&#32791;&#38477;&#20302;4&#20493;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#27599;&#20010;GPU&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#24517;&#38656;&#30340;GPU&#25968;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, the memory requirements to train state-of-the-art neural networks have far exceeded the DRAM capacities of modern hardware accelerators. This has necessitated the development of efficient algorithms to train these neural networks in parallel on large-scale GPU-based clusters. Since computation is relatively inexpensive on modern GPUs, designing and implementing extremely efficient communication in these parallel training algorithms is critical for extracting the maximum performance. This paper presents AxoNN, a parallel deep learning framework that exploits asynchrony and message-driven execution to schedule neural network operations on each GPU, thereby reducing GPU idle time and maximizing hardware efficiency. By using the CPU memory as a scratch space for offloading data periodically during training, AxoNN is able to reduce GPU memory consumption by four times. This allows us to increase the number of parameters per GPU by four times, thus reducing the amount 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2107.11972</link><description>&lt;p&gt;
&#24403;&#26426;&#20250;&#26469;&#20020;&#26102;&#36827;&#34892;&#20132;&#26131;&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#30340;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Iterative Refinement Labeling. (arXiv:2107.11972v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26088;&#22312;&#26681;&#25454;&#24403;&#21069;&#24066;&#22330;&#24773;&#20917;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#39044;&#27979;&#37329;&#34701;&#36164;&#20135;&#30340;&#26410;&#26469;&#36235;&#21183;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37329;&#34701;&#25968;&#25454;&#30340;&#20302;&#20449;&#22122;&#27604;&#21644;&#38543;&#26426;&#24615;&#26497;&#24378;&#65292;&#22909;&#30340;&#20132;&#26131;&#26426;&#20250;&#26497;&#20026;&#31232;&#23569;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#19981;&#20180;&#32454;&#36873;&#25321;&#28508;&#22312;&#30340;&#30408;&#21033;&#26679;&#26412;&#65292;&#36825;&#20123;ML&#26041;&#27861;&#23481;&#26131;&#25429;&#25417;&#21040;&#22122;&#22768;&#32780;&#19981;&#26159;&#30495;&#23454;&#20449;&#21495;&#30340;&#27169;&#24335;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#65288;LA-Attention&#65289;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#65288;IRL&#65289;&#12290;LA-Attention&#26088;&#22312;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#37329;&#34701;&#25968;&#25454;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#32780;IRL&#21017;&#26088;&#22312;&#36845;&#20195;&#22320;&#32454;&#21270;&#26631;&#27880;&#36807;&#31243;&#65292;&#36807;&#28388;&#25481;&#22122;&#22768;&#21644;&#26080;&#20851;&#26679;&#26412;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Price movement forecasting aims at predicting the future trends of financial assets based on the current market conditions and other relevant information. Recently, machine learning (ML) methods have become increasingly popular and achieved promising results for price movement forecasting in both academia and industry. Most existing ML solutions formulate the forecasting problem as a classification (to predict the direction) or a regression (to predict the return) problem over the entire set of training data. However, due to the extremely low signal-to-noise ratio and stochastic nature of financial data, good trading opportunities are extremely scarce. As a result, without careful selection of potentially profitable samples, such ML methods are prone to capture the patterns of noises instead of real signals. To address this issue, we propose a novel price movement forecasting framework named LARA consisting of two main components: Locality-Aware Attention (LA-Attention) and Iterative R
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#20010;&#26174;&#24335;&#36229;&#21442;&#25968;&#39044;&#27979;&#20989;&#25968;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#26597;&#35810;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2107.02378</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30340;&#26174;&#24335;&#36229;&#21442;&#25968;&#39044;&#27979;&#27169;&#22411;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning an Explicit Hyperparameter Prediction Function Conditioned on Tasks. (arXiv:2107.02378v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.02378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#20010;&#26174;&#24335;&#36229;&#21442;&#25968;&#39044;&#27979;&#20989;&#25968;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#26597;&#35810;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#26426;&#22120;&#23398;&#20064;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#20869;&#22312;&#30340;&#39044;&#27979;&#35268;&#21017;&#20197;&#39044;&#27979;&#26032;&#30340;&#26597;&#35810;&#25968;&#25454;&#26631;&#31614;&#19981;&#21516;&#65292;&#20803;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#26426;&#22120;&#23398;&#20064;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#21033;&#29992;&#20803;&#23398;&#20064;&#30340;&#23398;&#20064;&#26041;&#27861;&#23545;&#26032;&#30340;&#26597;&#35810;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#23398;&#20064;&#26041;&#27861;&#35299;&#37322;&#20026;&#23398;&#20064;&#19968;&#20010;&#26174;&#24335;&#36229;&#21442;&#25968;&#39044;&#27979;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#30001;&#25152;&#26377;&#35757;&#32451;&#20219;&#21153;&#20849;&#20139;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20010;&#20989;&#25968;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;&#21442;&#25968;&#21270;&#20989;&#25968;&#65292;&#31216;&#20026;&#20803;&#23398;&#20064;&#22120;&#65292;&#23558;&#35757;&#32451;/&#27979;&#35797;&#20219;&#21153;&#26144;&#23556;&#21040;&#20854;&#21512;&#36866;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#65292;&#25552;&#21462;&#33258;&#31216;&#20026;&#20803;&#23398;&#20064;&#26426;&#30340;&#39044;&#20808;&#25351;&#23450;&#30340;&#20989;&#25968;&#38598;&#12290;&#36825;&#31181;&#35774;&#32622;&#20445;&#35777;&#20102;&#20803;&#23398;&#20064;&#30340;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#26597;&#35810;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20687;&#35768;&#22810;&#29616;&#26377;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#19968;&#26679;&#65292;&#21482;&#33719;&#24471;&#22266;&#23450;&#30340;&#36229;&#21442;&#25968;&#65292;&#36866;&#24212;&#24615;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta learning has attracted much attention recently in machine learning community. Contrary to conventional machine learning aiming to learn inherent prediction rules to predict labels for new query data, meta learning aims to learn the learning methodology for machine learning from observed tasks, so as to generalize to new query tasks by leveraging the meta-learned learning methodology. In this study, we interpret such learning methodology as learning an explicit hyper-parameter prediction function shared by all training tasks. Specifically, this function is represented as a parameterized function called meta-learner, mapping from a training/test task to its suitable hyper-parameter setting, extracted from a pre-specified function set called meta learning machine. Such setting guarantees that the meta-learned learning methodology is able to flexibly fit diverse query tasks, instead of only obtaining fixed hyper-parameters by many current meta learning methods, with less adaptability 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#23398;&#20064;&#38598;&#21512;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#30340;&#35823;&#24046;&#26816;&#27979;&#21644;&#20934;&#30830;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.15728</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#23398;&#20064;&#38598;&#21512;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#26816;&#27979;&#38169;&#35823;&#24182;&#20272;&#35745;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Detecting Errors and Estimating Accuracy on Unlabeled Data with Self-training Ensembles. (arXiv:2106.15728v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.15728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#23398;&#20064;&#38598;&#21512;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#30340;&#35823;&#24046;&#26816;&#27979;&#21644;&#20934;&#30830;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26410;&#30693;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#36816;&#34892;&#26102;&#65292;&#23427;&#20250;&#36973;&#36935;&#21040;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#23433;&#20840;&#37096;&#32626;&#65292;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#20272;&#31639;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#27979;&#35797;&#38598;&#30340;&#26631;&#27880;&#36890;&#24120;&#19981;&#20250;&#31435;&#21363;&#25552;&#20379;&#65292;&#24182;&#19988;&#33719;&#24471;&#23427;&#20204;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#12289;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20004;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#65306;&#65288;1&#65289;&#26080;&#30417;&#30563;&#20934;&#30830;&#24615;&#20272;&#35745;&#65292;&#30446;&#30340;&#26159;&#20272;&#35745;&#39044;&#20808;&#35757;&#32451;&#20998;&#31867;&#22120;&#22312;&#19968;&#32452;&#26080;&#26631;&#31614;&#27979;&#35797;&#26679;&#26412;&#19978;&#30340;&#20934;&#30830;&#24615;&#65307;&#65288;2&#65289;&#38169;&#35823;&#26816;&#27979;&#65292;&#30446;&#30340;&#26159;&#35782;&#21035;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#27979;&#35797;&#26679;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36845;&#20195;&#22320;&#23398;&#20064;&#20102;&#19968;&#20010;&#38598;&#21512;&#27169;&#22411;&#26469;&#35782;&#21035;&#34987;&#38169;&#35823;&#20998;&#31867;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#36890;&#36807;&#33258;&#23398;&#20064;&#26469;&#25913;&#36827;&#38598;&#21512;&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#28436;&#31034;&#20102;&#35813;&#26694;&#26550;&#30340;&#21253;&#23481;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a deep learning model is deployed in the wild, it can encounter test data drawn from distributions different from the training data distribution and suffer drop in performance. For safe deployment, it is essential to estimate the accuracy of the pre-trained model on the test data. However, the labels for the test inputs are usually not immediately available in practice, and obtaining them can be expensive. This observation leads to two challenging tasks: (1) unsupervised accuracy estimation, which aims to estimate the accuracy of a pre-trained classifier on a set of unlabeled test inputs; (2) error detection, which aims to identify mis-classified test inputs. In this paper, we propose a principled and practically effective framework that simultaneously addresses the two tasks. The proposed framework iteratively learns an ensemble of models to identify mis-classified data points and performs self-training to improve the ensemble with the identified points. Theoretical analysis demo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36127;&#30456;&#20851;&#23398;&#20064;&#30340;&#28151;&#21512;&#38598;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#21152;&#26435;&#23376;&#27169;&#22411;&#26469;&#35299;&#20915;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2104.02317</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#36127;&#30456;&#20851;&#23398;&#20064;&#30340;&#28151;&#21512;&#38598;&#25104;&#31639;&#27861;&#36827;&#34892;&#22238;&#24402;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A hybrid ensemble method with negative correlation learning for regression. (arXiv:2104.02317v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.02317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#36127;&#30456;&#20851;&#23398;&#20064;&#30340;&#28151;&#21512;&#38598;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#21152;&#26435;&#23376;&#27169;&#22411;&#26469;&#35299;&#20915;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#38598;&#25104;&#26159;&#38598;&#25104;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#65292;&#22312;&#22238;&#24402;&#39046;&#22495;&#24471;&#21040;&#20102;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#35777;&#23454;&#20102;&#22810;&#26679;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#38598;&#25104;&#26041;&#27861;&#20027;&#35201;&#26159;&#22312;&#23376;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#32771;&#34385;&#22810;&#26679;&#24615;&#65292;&#25913;&#21892;&#25928;&#26524;&#26377;&#38480;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36825;&#39033;&#30740;&#31350;&#20174;&#24322;&#26500;&#27169;&#22411;&#27744;&#20013;&#33258;&#21160;&#36873;&#25321;&#21644;&#21152;&#26435;&#23376;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#20869;&#28857;&#36807;&#28388;&#32447;&#24615;&#25628;&#32034;&#31639;&#27861;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#30446;&#26631;&#20989;&#25968;&#21019;&#26032;&#22320;&#23558;&#36127;&#30456;&#20851;&#23398;&#20064;&#20316;&#20026;&#19968;&#20010;&#24809;&#32602;&#39033;&#65292;&#24182;&#36873;&#25321;&#22810;&#26679;&#21270;&#23376;&#27169;&#22411;&#23376;&#38598;&#12290;&#36873;&#25321;&#27599;&#20010;&#27169;&#22411;&#31867;&#30340;&#26368;&#20339;&#23376;&#27169;&#22411;&#26469;&#26500;&#24314;NCL&#38598;&#25104;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#31616;&#21333;&#24179;&#22343;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#21152;&#26435;&#26041;&#27861;&#12290;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;NCL&#38598;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hybrid ensemble, an essential branch of ensembles, has flourished in the regression field, with studies confirming diversity's importance. However, previous ensembles consider diversity in the sub-model training stage, with limited improvement compared to single models. In contrast, this study automatically selects and weights sub-models from a heterogeneous model pool. It solves an optimization problem using an interior-point filtering linear-search algorithm. The objective function innovatively incorporates negative correlation learning as a penalty term, with which a diverse model subset can be selected. The best sub-models from each model class are selected to build the NCL ensemble, which performance is better than the simple average and other state-of-the-art weighting methods. It is also possible to improve the NCL ensemble with a regularization term in the objective function. In practice, it is difficult to conclude the optimal sub-model for a dataset prior due to the model unc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21463;&#25511;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#27169;&#22411;&#65288;CGPDM&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#12290;&#35813;&#27169;&#22411;&#23558;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#25237;&#24433;&#21040;&#36739;&#23567;&#30340;&#32500;&#24230;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;CGPDM&#30001;&#19968;&#20010;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#32452;&#25104;&#65292;&#20855;&#26377;&#30456;&#20851;&#30340;&#21160;&#24577;&#65292;&#22806;&#37096;&#25511;&#21046;&#21464;&#37327;&#21487;&#20197;&#20316;&#29992;&#20110;&#35813;&#31354;&#38388;&#65292;&#24182;&#26144;&#23556;&#21040;&#35266;&#27979;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2103.06615</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#24067;&#26009;&#25805;&#20316;&#30340;&#21463;&#25511;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation. (arXiv:2103.06615v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.06615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21463;&#25511;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#27169;&#22411;&#65288;CGPDM&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#12290;&#35813;&#27169;&#22411;&#23558;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#25237;&#24433;&#21040;&#36739;&#23567;&#30340;&#32500;&#24230;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;CGPDM&#30001;&#19968;&#20010;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#32452;&#25104;&#65292;&#20855;&#26377;&#30456;&#20851;&#30340;&#21160;&#24577;&#65292;&#22806;&#37096;&#25511;&#21046;&#21464;&#37327;&#21487;&#20197;&#20316;&#29992;&#20110;&#35813;&#31354;&#38388;&#65292;&#24182;&#26144;&#23556;&#21040;&#35266;&#27979;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#22788;&#29702;&#38750;&#21018;&#24615;&#29289;&#20307;&#65288;&#22914;&#24067;&#26009;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#19982;&#38750;&#21018;&#24615;&#29289;&#20307;&#36827;&#34892;&#29289;&#29702;&#20132;&#20114;&#26159;&#19981;&#30830;&#23450;&#21644;&#22797;&#26434;&#30340;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#20174;&#26679;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#24314;&#27169;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29366;&#24577;&#34920;&#31034;&#30340;&#39640;&#32500;&#24615;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#35757;&#32451;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#25511;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#27169;&#22411;&#65288;CGPDM&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#23558;&#20854;&#23884;&#20837;&#21040;&#20302;&#32500;&#27969;&#24418;&#20013;&#23398;&#20064;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#12290; CGPDM&#30001;&#19968;&#20010;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#32452;&#25104;&#65292;&#20855;&#26377;&#30456;&#20851;&#30340;&#21160;&#24577;&#65292;&#22806;&#37096;&#25511;&#21046;&#21464;&#37327;&#21487;&#20197;&#20316;&#29992;&#20110;&#35813;&#31354;&#38388;&#65292;&#24182;&#26144;&#23556;&#21040;&#35266;&#27979;&#31354;&#38388;&#12290;&#32771;&#34385;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20808;&#39564;&#20998;&#24067;&#26469;&#36793;&#32536;&#21270;&#20004;&#20010;&#26144;&#23556;&#30340;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;CGPDM&#23558;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#25237;&#24433;&#21040;&#36739;&#23567;&#30340;&#32500;&#24230;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last years, significant advances have been made in robotic manipulation, but still, the handling of non-rigid objects, such as cloth garments, is an open problem. Physical interaction with non-rigid objects is uncertain and complex to model. Thus, extracting useful information from sample data can considerably improve modeling performance. However, the training of such models is a challenging task due to the high-dimensionality of the state representation. In this paper, we propose Controlled Gaussian Process Dynamical Model (CGPDM) for learning high-dimensional, nonlinear dynamics by embedding it in a low-dimensional manifold. A CGPDM is constituted by a low-dimensional latent space, with an associated dynamics where external control variables can act and a mapping to the observation space. The parameters of both maps are marginalized out by considering Gaussian Process (GP) priors. Hence, a CGPDM projects a high-dimensional state space into a smaller dimension latent space, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2102.12227</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#29992;&#20110;&#35770;&#36848;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#22312;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#26550;&#26500;&#65292;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#19981;&#23545;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#29992;&#25143;&#29983;&#25104;&#35780;&#35770;&#12289;&#31185;&#23398;&#20986;&#29256;&#29289;&#21644;&#21149;&#35828;&#24615;&#35770;&#25991;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#20855;&#26377;&#26356;&#39640;&#35745;&#31639;&#21360;&#35760;&#25110;&#29305;&#23450;&#20110;&#35821;&#26009;&#24211;&#35774;&#35745;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#30340;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#20195;&#34920;&#20102;&#36890;&#29992;&#24615;&#12289;&#24615;&#33021;&#31934;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#20043;&#38388;&#30340;&#26377;&#36259;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;DAG&#22240;&#26524;&#27169;&#22411;&#30340;&#20302;&#31209;&#20551;&#35774;&#26469;&#35299;&#20915;&#39640;&#32500;&#24773;&#20917;&#19979;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#38590;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#23558;&#29616;&#26377;&#30340;&#20302;&#31209;&#25216;&#26415;&#24212;&#29992;&#21040;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#65292;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#31264;&#23494;&#22270;&#30340;&#25968;&#25454;&#27169;&#22411;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2006.05691</link><description>&lt;p&gt;
&#20302;&#31209;&#26377;&#21521;&#26080;&#29615;&#22270;&#19982;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On Low Rank Directed Acyclic Graphs and Causal Structure Learning. (arXiv:2006.05691v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.05691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;DAG&#22240;&#26524;&#27169;&#22411;&#30340;&#20302;&#31209;&#20551;&#35774;&#26469;&#35299;&#20915;&#39640;&#32500;&#24773;&#20917;&#19979;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#38590;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#23558;&#29616;&#26377;&#30340;&#20302;&#31209;&#25216;&#26415;&#24212;&#29992;&#21040;&#20102;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#65292;&#23454;&#39564;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#31264;&#23494;&#22270;&#30340;&#25968;&#25454;&#27169;&#22411;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23398;&#20064;&#30001;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#34920;&#31034;&#30340;&#22240;&#26524;&#32467;&#26500;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#24403;&#35201;&#23398;&#20064;&#30340;&#22270;&#19981;&#26159;&#31232;&#30095;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20851;&#20110;DAG&#22240;&#26524;&#27169;&#22411;&#30340;&#65288;&#21152;&#26435;&#65289;&#37051;&#25509;&#30697;&#38453;&#30340;&#20302;&#31209;&#20551;&#35774;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#20302;&#31209;&#25216;&#26415;&#26469;&#35843;&#25972;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#24314;&#31435;&#19968;&#20123;&#26377;&#29992;&#30340;&#32467;&#26524;&#65292;&#23558;&#21487;&#35299;&#37322;&#30340;&#22270;&#24418;&#26465;&#20214;&#19982;&#20302;&#31209;&#20551;&#35774;&#30456;&#20851;&#32852;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#26368;&#22823;&#31209;&#19982;&#20013;&#24515;&#33410;&#28857;&#39640;&#24230;&#30456;&#20851;&#65292;&#36825;&#34920;&#26126;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#26080;&#26631;&#24230;&#32593;&#32476;&#24448;&#24448;&#26159;&#20302;&#31209;&#30340;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20302;&#31209;&#36866;&#24212;&#23545;&#20110;&#21508;&#31181;&#25968;&#25454;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30456;&#23545;&#22823;&#19988;&#23494;&#38598;&#30340;&#22270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#39564;&#35777;&#36807;&#31243;&#65292;&#36866;&#24212;&#24615;&#26041;&#27861;&#22987;&#32456;&#20445;&#25345;&#21331;&#36234;&#25110;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite several advances in recent years, learning causal structures represented by directed acyclic graphs (DAGs) remains a challenging task in high dimensional settings when the graphs to be learned are not sparse. In this paper, we propose to exploit a low rank assumption regarding the (weighted) adjacency matrix of a DAG causal model to help address this problem. We utilize existing low rank techniques to adapt causal structure learning methods to take advantage of this assumption and establish several useful results relating interpretable graphical conditions to the low rank assumption. Specifically, we show that the maximum rank is highly related to hubs, suggesting that scale-free networks, which are frequently encountered in practice, tend to be low rank. Our experiments demonstrate the utility of the low rank adaptations for a variety of data models, especially with relatively large and dense graphs. Moreover, with a validation procedure, the adaptations maintain a superior or
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaUSM&#30340;AdaGrad&#21464;&#20307;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#32479;&#19968;AdaGrad&#12289;AccAdaGrad&#12289;Adam&#21644;RMSProp&#30340;&#23398;&#20064;&#29575;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#21160;&#37327;&#26041;&#26696;&#65292;&#35206;&#30422;&#20102;&#37325;&#29699;&#21160;&#37327;&#21644;Nesterov&#21152;&#36895;&#26799;&#24230;&#21160;&#37327;&#65307;&#22312;&#38750;&#20984;&#38543;&#26426;&#35774;&#32622;&#20013;&#30340;&#25910;&#25947;&#29575;&#20026;$\mathcal{O}(\log(T)/\sqrt{T})$&#12290;</title><link>http://arxiv.org/abs/1808.03408</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#26435;&#37325;&#32858;&#38598;&#21644;&#21160;&#37327;&#21152;&#36895;&#30340;AdaGrad&#30340;&#32479;&#19968;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Unified Analysis of AdaGrad with Weighted Aggregation and Momentum Acceleration. (arXiv:1808.03408v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1808.03408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaUSM&#30340;AdaGrad&#21464;&#20307;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#32479;&#19968;AdaGrad&#12289;AccAdaGrad&#12289;Adam&#21644;RMSProp&#30340;&#23398;&#20064;&#29575;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#21160;&#37327;&#26041;&#26696;&#65292;&#35206;&#30422;&#20102;&#37325;&#29699;&#21160;&#37327;&#21644;Nesterov&#21152;&#36895;&#26799;&#24230;&#21160;&#37327;&#65307;&#22312;&#38750;&#20984;&#38543;&#26426;&#35774;&#32622;&#20013;&#30340;&#25910;&#25947;&#29575;&#20026;$\mathcal{O}(\log(T)/\sqrt{T})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#21644;&#21160;&#37327;&#25216;&#26415;&#38598;&#25104;&#21040;SGD&#20013;&#20250;&#23548;&#33268;&#19968;&#31867;&#39640;&#25928;&#21152;&#36895;&#33258;&#36866;&#24212;&#38543;&#26426;&#31639;&#27861;&#65292;&#22914;AdaGrad&#65292;RMSProp&#65292;Adam&#65292;AccAdaGrad&#31561;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#30340;&#25910;&#25947;&#29702;&#35770;&#20173;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#20984;&#38543;&#26426;&#35774;&#32622;&#20013;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24102;&#26377;&#32479;&#19968;&#21160;&#37327;&#30340;&#21152;&#26435;AdaGrad&#8221;&#65292;&#31216;&#20026;AdaUSM&#65292;&#23427;&#20855;&#26377;&#20197;&#19979;&#20027;&#35201;&#29305;&#24449;&#65306;(1) &#23427;&#34701;&#21512;&#20102;&#32479;&#19968;&#30340;&#21160;&#37327;&#26041;&#26696;&#65292;&#28085;&#30422;&#20102;&#37325;&#29699;&#21160;&#37327;&#21644;Nesterov&#21152;&#36895;&#26799;&#24230;&#21160;&#37327;&#65307;(2) &#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#32479;&#19968;AdaGrad&#65292;AccAdaGrad&#65292;Adam&#21644;RMSProp&#30340;&#23398;&#20064;&#29575;&#12290;&#27492;&#22806;&#65292;&#24403;&#25105;&#20204;&#22312;AdaUSM&#20013;&#37319;&#29992;&#22810;&#39033;&#24335;&#22686;&#38271;&#30340;&#26435;&#37325;&#26102;&#65292;&#22312;&#38750;&#20984;&#38543;&#26426;&#35774;&#32622;&#20013;&#21487;&#20197;&#24471;&#21040;&#20854;&#25910;&#25947;&#29575;&#20026;$\mathcal{O}(\log(T)/\sqrt{T})$ &#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;Adam&#21644;RMSProp&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#22312;&#37325;&#21152;&#26435;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating adaptive learning rate and momentum techniques into SGD leads to a large class of efficiently accelerated adaptive stochastic algorithms, such as AdaGrad, RMSProp, Adam, AccAdaGrad, \textit{etc}. In spite of their effectiveness in practice, there is still a large gap in their theories of convergences, especially in the difficult non-convex stochastic setting. To fill this gap, we propose \emph{weighted AdaGrad with unified momentum}, dubbed AdaUSM, which has the main characteristics that (1) it incorporates a unified momentum scheme which covers both the heavy ball momentum and the Nesterov accelerated gradient momentum; (2) it adopts a novel weighted adaptive learning rate that can unify the learning rates of AdaGrad, AccAdaGrad, Adam, and RMSProp. Moreover, when we take polynomially growing weights in AdaUSM, we obtain its $\mathcal{O}(\log(T)/\sqrt{T})$ convergence rate in the non-convex stochastic setting. We also show that the adaptive learning rates of Adam and RMSPro
&lt;/p&gt;</description></item></channel></rss>