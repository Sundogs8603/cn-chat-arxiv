<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.01649</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalizing Dataset Distillation via Deep Generative Prior. (arXiv:2305.01649v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26088;&#22312;&#23558;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#20960;&#20010;&#21512;&#25104;&#22270;&#20687;&#20013;&#12290;&#20854;&#24605;&#24819;&#26159;&#21512;&#25104;&#23569;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#28857;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#32473;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#36924;&#36817;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#24182;&#25193;&#23637;&#21040;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#20808;&#39564;&#26469;&#21512;&#25104;&#33976;&#39311;&#30340;&#25968;&#25454;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23558;&#22823;&#37327;&#22270;&#20687;&#33976;&#39311;&#20026;&#23569;&#37327;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#20013;&#30340;&#36328;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthesize a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite recent progress in the field, existing dataset distillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pre-trained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DP-ICL&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#20445;&#35777;&#19979;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;&#32463;&#36807;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.01639</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private In-Context Learning. (arXiv:2305.01639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DP-ICL&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#20445;&#35777;&#19979;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;&#32463;&#36807;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#22686;&#24378;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"DP-ICL"&#26469;&#23454;&#29616;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#20445;&#35777;&#12290;DP-ICL&#36890;&#36807;&#20351;&#29992;"report-noisy-max"&#26426;&#21046;&#22312;&#31034;&#20363;&#38598;&#21512;&#19978;&#24314;&#31435;&#22024;&#26434;&#19968;&#33268;&#24615;&#26469;&#36827;&#34892;&#31169;&#26377;&#25512;&#26029;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;DP-ICL&#65292;&#21457;&#29616;&#20854;&#19982;&#38750;&#31169;&#26377;ICL&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;(&lt;2%&#38477;&#32423;)&#12290;
&lt;/p&gt;
&lt;p&gt;
An important question in deploying large language models (LLMs) is how to augment LLMs with private data. We propose Differentially Private In-context Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees. DP-ICL performs private inference by establishing noisy consensus over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate DP-ICL on four benchmarks and find that it achieves comparable performance (&lt;2\% degradation) with non-private ICL.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26500;&#24314;&#22359;&#65292;&#31216;&#20026;MultiresLayer&#65292;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#25429;&#33719;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#36235;&#21183;&#65292;&#26082;&#20855;&#26377;&#21367;&#31215;&#32593;&#32476;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21448;&#20855;&#26377;&#23567;&#27874;&#20998;&#35299;&#30340;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#12290;</title><link>http://arxiv.org/abs/2305.01638</link><description>&lt;p&gt;
&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#35760;&#24518;&#30340;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Sequence Modeling with Multiresolution Convolutional Memory. (arXiv:2305.01638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26500;&#24314;&#22359;&#65292;&#31216;&#20026;MultiresLayer&#65292;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#25429;&#33719;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#36235;&#21183;&#65292;&#26082;&#20855;&#26377;&#21367;&#31215;&#32593;&#32476;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21448;&#20855;&#26377;&#23567;&#27874;&#20998;&#35299;&#30340;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#25429;&#25417;&#23545;&#20110;&#26576;&#20010;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#21644;&#29983;&#25104;&#24314;&#27169;&#65289;&#26174;&#33879;&#30340;&#39034;&#24207;&#25968;&#25454;&#28304;&#20013;&#30340;&#38271;&#31243;&#27169;&#24335;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#22522;&#20110;&#23567;&#27874;&#30340;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#26500;&#24314;&#22359;&#65292;&#31216;&#20026;MultiresLayer&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#22810;&#20998;&#36776;&#29575;&#21367;&#31215;&#65292;&#20197;&#25429;&#33719;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#22810;&#23610;&#24230;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;MultiresConv&#21487;&#20197;&#36890;&#36807;&#22312;&#25193;&#24352;&#30340;&#22240;&#26524;&#21367;&#31215;&#26641;&#19978;&#20351;&#29992;&#20849;&#20139;&#36807;&#28388;&#22120;&#26469;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#26082;&#20855;&#26377;&#21367;&#31215;&#32593;&#32476;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#21448;&#20855;&#26377;&#23567;&#27874;&#20998;&#35299;&#30340;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence. Our MultiresConv can be implemented with shared filters across a dilated causal convolution tree. Thus it garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#23545;&#27604;&#26469;&#25913;&#36827;&#25991;&#26412;&#29983;&#25104;&#36755;&#20986;&#65292;&#35299;&#20915;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.01628</link><description>&lt;p&gt;
&#22351;&#24314;&#35758;&#30340;&#22909;&#22788;&#65306;&#27169;&#22411;&#23618;&#38388;&#33258;&#21160;&#23545;&#29031;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers. (arXiv:2305.01628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#23545;&#27604;&#26469;&#25913;&#36827;&#25991;&#26412;&#29983;&#25104;&#36755;&#20986;&#65292;&#35299;&#20915;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#26368;&#32456;&#27169;&#22411;&#23618;&#30340;&#34920;&#31034;&#65292;&#22240;&#20026;&#20551;&#35774;&#20013;&#38388;&#38544;&#34255;&#23618;&#30340;&#34920;&#31034;&#26159;&#19981;&#22826;&#26377;&#29992;&#30340;&#12290;&#26412;&#25991;&#35748;&#20026;&#30001;&#20110;&#27169;&#22411;&#23618;&#20043;&#38388;&#30340;&#28176;&#36827;&#25913;&#36827;&#65292;&#21487;&#20197;&#20174;&#26356;&#39640;&#23618;&#21644;&#26356;&#20302;&#23618;&#20043;&#38388;&#30340;&#23545;&#27604;&#20013;&#33719;&#21462;&#39069;&#22806;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36873;&#25321;&#29983;&#25104;&#27169;&#22411;&#30340;&#19979;&#19968;&#20010;&#21487;&#33021;&#26631;&#35760;&#30340;&#39044;&#27979;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#36739;&#20302;&#23618;&#30340;&#39044;&#27979;&#26469;&#31361;&#20986;&#21738;&#20123;&#20505;&#36873;&#39033;&#26159;&#26368;&#22909;&#36991;&#20813;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23618;&#20043;&#38388;&#30340;&#23545;&#27604;&#26469;&#25913;&#36827;&#25991;&#26412;&#29983;&#25104;&#36755;&#20986;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#32531;&#35299;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25512;&#26029;&#26102;&#27604;&#36739;&#27169;&#22411;&#23618;&#20043;&#38388;&#21487;&#20197;&#23545;&#19968;&#20123;&#24635;&#20307;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#26041;&#38754;&#20135;&#29983;&#23454;&#36136;&#24615;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36965;&#25805;&#20316;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20197;&#21450;&#23398;&#20064;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#32852;&#32467;&#30446;&#26631;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01618</link><description>&lt;p&gt;
ContactArt&#65306;&#23398;&#20064;&#31867;&#21035;&#32423;&#32852;&#32467;&#29289;&#20307;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#30340;&#19977;&#32500;&#20132;&#20114;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation. (arXiv:2305.01618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#36965;&#25805;&#20316;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#20197;&#21450;&#23398;&#20064;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#32852;&#32467;&#30446;&#26631;&#21644;&#25163;&#37096;&#23039;&#24577;&#20272;&#35745;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#25163;&#37096;&#21644;&#32852;&#32467;&#30446;&#26631;&#23039;&#24577;&#20272;&#35745;&#20013;&#30340;&#25163;&#29289;&#20114;&#21160;&#20808;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35270;&#35273;&#36965;&#25805;&#20316;&#25910;&#38598;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20154;&#31867;&#25805;&#20316;&#21592;&#21487;&#20197;&#30452;&#25509;&#22312;&#29289;&#29702;&#27169;&#25311;&#22120;&#20013;&#28216;&#25103;&#26469;&#25805;&#32437;&#32852;&#32467;&#23545;&#35937;&#12290; &#25105;&#20204;&#35760;&#24405;&#25968;&#25454;&#24182;&#20174;&#27169;&#25311;&#22120;&#33719;&#24471;&#26377;&#20851;&#30446;&#26631;&#23039;&#24577;&#21644;&#25509;&#35302;&#20449;&#24687;&#30340;&#20813;&#36153;&#21644;&#20934;&#30830;&#27880;&#37322;&#12290; &#25105;&#20204;&#30340;&#31995;&#32479;&#20165;&#38656;&#35201;&#20351;&#29992;iPhone&#26469;&#35760;&#24405;&#20154;&#25163;&#36816;&#21160;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#24182;&#22823;&#22823;&#38477;&#20302;&#25968;&#25454;&#21644;&#27880;&#37322;&#25910;&#38598;&#30340;&#25104;&#26412;&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19977;&#32500;&#20132;&#20114;&#20808;&#39564;&#65292;&#21253;&#25324;&#25429;&#33719;&#23545;&#35937;&#37096;&#20214;&#25490;&#21015;&#20998;&#24067;&#30340;&#37492;&#21035;&#22120;&#65288;&#22312;GAN&#20013;&#65289;&#65292;&#20197;&#21450;&#29983;&#25104;&#32852;&#32467;&#23545;&#35937;&#19978;&#25509;&#35302;&#21306;&#22495;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#25351;&#23548;&#25163;&#21183;&#20272;&#35745;&#12290;&#36825;&#20123;&#32467;&#26500;&#21644;&#25509;&#35302;&#20808;&#39564;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#39046;&#22495;&#24046;&#36317;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#23398;&#20064;&#30340;&#20808;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20851;&#38190;&#28857;&#23450;&#20301;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new dataset and a novel approach to learning hand-object interaction priors for hand and articulated object pose estimation. We first collect a dataset using visual teleoperation, where the human operator can directly play within a physical simulator to manipulate the articulated objects. We record the data and obtain free and accurate annotations on object poses and contact information from the simulator. Our system only requires an iPhone to record human hand motion, which can be easily scaled up and largely lower the costs of data and annotation collection. With this data, we learn 3D interaction priors including a discriminator (in a GAN) capturing the distribution of how object parts are arranged, and a diffusion model which generates the contact regions on articulated objects, guiding the hand pose estimation. Such structural and contact priors can easily transfer to real-world data with barely any domain gap. By using our data and learned priors, our method signific
&lt;/p&gt;</description></item><item><title>AutoColor &#26159;&#31532;&#19968;&#20010;&#23398;&#20064;&#27491;&#30830;&#29031;&#26126;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#20809;&#28304;&#21151;&#29575;&#30340;&#26041;&#27861;&#65292;&#23558;&#20248;&#21270;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#20174;&#36229;&#36807;1000&#20010;&#38477;&#33267;70&#20010;&#36845;&#20195;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.01611</link><description>&lt;p&gt;
AutoColor: &#38024;&#23545;&#22810;&#33394;&#20840;&#24687;&#22270;&#30340;&#23398;&#20064;&#20809;&#21151;&#29575;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
AutoColor: Learned Light Power Control for Multi-Color Holograms. (arXiv:2305.01611v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01611
&lt;/p&gt;
&lt;p&gt;
AutoColor &#26159;&#31532;&#19968;&#20010;&#23398;&#20064;&#27491;&#30830;&#29031;&#26126;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#20809;&#28304;&#21151;&#29575;&#30340;&#26041;&#27861;&#65292;&#23558;&#20248;&#21270;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#20174;&#36229;&#36807;1000&#20010;&#38477;&#33267;70&#20010;&#36845;&#20195;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33394;&#20840;&#24687;&#22270;&#38656;&#35201;&#22810;&#20010;&#20809;&#28304;&#21516;&#26102;&#29031;&#23556;&#25165;&#33021;&#24418;&#25104;&#22270;&#24418;&#12290;&#36825;&#31181;&#20840;&#24687;&#22270;&#21487;&#20197;&#27604;&#20256;&#32479;&#21333;&#33394;&#20840;&#24687;&#22270;&#26356;&#22909;&#22320;&#21033;&#29992;&#20809;&#28304;&#65292;&#24182;&#25552;&#39640;&#20840;&#24687;&#26174;&#31034;&#30340;&#21160;&#24577;&#33539;&#22260;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;\projectname&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#29031;&#26126;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#30340;&#26368;&#20339;&#20809;&#28304;&#21151;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#21644;&#20854;&#28145;&#24230;&#20449;&#24687;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#22810;&#33394;&#20840;&#24687;&#22270;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#27969;&#34892;&#30340;&#27969;&#31243;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#12289;&#22823;&#35821;&#35328;&#21644;&#21333;&#30524;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#26469;&#29983;&#25104;&#36825;&#20123;&#21512;&#25104;&#22270;&#20687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23454;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;\projectname&#21487;&#20197;&#23558;&#20248;&#21270;&#22810;&#33394;&#20840;&#24687;&#22270;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#20174;&#36229;&#36807;1000&#20010;&#38477;&#33267;70&#20010;&#36845;&#20195;&#27493;&#39588;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-color holograms rely on simultaneous illumination from multiple light sources. These multi-color holograms could utilize light sources better than conventional single-color holograms and can improve the dynamic range of holographic displays. In this letter, we introduce \projectname, the first learned method for estimating the optimal light source powers required for illuminating multi-color holograms. For this purpose, we establish the first multi-color hologram dataset using synthetic images and their depth information. We generate these synthetic images using a trending pipeline combining generative, large language, and monocular depth estimation models. Finally, we train our learned model using our dataset and experimentally demonstrate that \projectname significantly decreases the number of steps required to optimize multi-color holograms from $&gt;1000$ to $70$ iteration steps without compromising image quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#36755;&#20837;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#34920;&#31034;&#26041;&#24335;&#65307;&#23545;&#19981;&#21516;&#23618;&#27425;&#30340;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#22810;&#31181;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#26377;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#34920;&#31034;&#39640;&#32423;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#22686;&#21152;&#35268;&#27169;&#20351;&#29305;&#24449;&#34920;&#31034;&#26356;&#21152;&#31232;&#30095;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.01610</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#25506;&#27979;&#20013;&#23547;&#25214;&#28023;&#37327;&#31070;&#32463;&#20803;: &#23454;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Finding Neurons in a Haystack: Case Studies with Sparse Probing. (arXiv:2305.01610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35757;&#32451;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;&#20197;&#39044;&#27979;&#36755;&#20837;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#30340;&#34920;&#31034;&#26041;&#24335;&#65307;&#23545;&#19981;&#21516;&#23618;&#27425;&#30340;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#22810;&#31181;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#26377;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#34920;&#31034;&#39640;&#32423;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#22686;&#21152;&#35268;&#27169;&#20351;&#29305;&#24449;&#34920;&#31034;&#26356;&#21152;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24212;&#29992;&#21644;&#37096;&#32626;&#36805;&#36895;&#22686;&#21152;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#35745;&#31639;&#20173;&#28982;&#19981;&#36879;&#26126;&#19988;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#39640;&#32423;&#21487;&#35299;&#37322;&#29305;&#24449;&#22312;LLM&#20869;&#37096;&#31070;&#32463;&#20803;&#28608;&#27963;&#20013;&#30340;&#34920;&#31034;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;$k$-&#31232;&#30095;&#32447;&#24615;&#20998;&#31867;&#22120;(&#25506;&#38024;)&#26469;&#35757;&#32451;&#36825;&#20123;&#20869;&#37096;&#28608;&#27963;&#20540;&#65292;&#24182;&#39044;&#27979;&#36755;&#20837;&#30340;&#29305;&#24449;&#26159;&#21542;&#23384;&#22312;&#65307;&#36890;&#36807;&#25913;&#21464;$k$&#20540;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#31232;&#30095;&#24615;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#24403;$k=1$&#26102;&#65292;&#25105;&#20204;&#23450;&#20301;&#26576;&#20010;&#29305;&#23450;&#29305;&#24449;&#38750;&#24120;&#30456;&#20851;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#35828;&#26126;LLM&#30340;&#19968;&#33324;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26089;&#26399;&#23618;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#32452;&#21512;&#26469;&#34920;&#31034;&#35768;&#22810;&#29305;&#24449;&#65292;&#20013;&#38388;&#23618;&#20284;&#20046;&#20855;&#26377;&#19987;&#38376;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#26356;&#39640;&#32423;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#32780;&#22686;&#21152;&#30340;&#35268;&#27169;&#21017;&#23548;&#33268;&#34920;&#31034;&#31232;&#30095;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30456;&#21516;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#36825;&#20123;&#32593;&#32476;&#21253;&#25324;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#12289;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#26041;&#27861;&#12289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12289;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;&#24182;&#25581;&#31034;&#20102;&#32593;&#32476;&#21021;&#22987;&#21270;&#20301;&#32622;&#12289;&#20307;&#31995;&#32467;&#26500;&#21644;&#22823;&#23567;&#23545;&#27969;&#24418;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01604</link><description>&lt;p&gt;
&#22810;&#20010;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30456;&#21516;&#30340;&#20302;&#32500;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold. (arXiv:2305.01604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22810;&#20010;&#28145;&#24230;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30456;&#21516;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#36825;&#20123;&#32593;&#32476;&#21253;&#25324;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#12289;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#26041;&#27861;&#12289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12289;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;&#24182;&#25581;&#31034;&#20102;&#32593;&#32476;&#21021;&#22987;&#21270;&#20301;&#32622;&#12289;&#20307;&#31995;&#32467;&#26500;&#21644;&#22823;&#23567;&#23545;&#27969;&#24418;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#20449;&#24687;&#20960;&#20309;&#25216;&#26415;&#26469;&#20998;&#26512;&#28145;&#24230;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#39044;&#27979;&#36712;&#36857;&#12290;&#36890;&#36807;&#26816;&#26597;&#24213;&#23618;&#39640;&#32500;&#27010;&#29575;&#27169;&#22411;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#35757;&#32451;&#36807;&#31243;&#25506;&#32034;&#30340;&#26377;&#25928;&#20302;&#32500;&#27969;&#24418;&#12290;&#20855;&#26377;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#12289;&#22823;&#23567;&#12289;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#26041;&#27861;&#12289;&#27491;&#21017;&#21270;&#25216;&#26415;&#12289;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#32593;&#32476;&#22312;&#39044;&#27979;&#31354;&#38388;&#20869;&#20301;&#20110;&#21516;&#19968;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#27969;&#24418;&#30340;&#32454;&#33410;&#65292;&#21457;&#29616;&#20855;&#26377;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340;&#32593;&#32476;&#36981;&#24490;&#21487;&#21306;&#20998;&#30340;&#36712;&#36857;&#65292;&#20294;&#20854;&#20182;&#22240;&#32032;&#24433;&#21709;&#26497;&#23567;; &#26356;&#22823;&#30340;&#32593;&#32476;&#27839;&#30528;&#19982;&#36739;&#23567;&#30340;&#32593;&#32476;&#30456;&#20284;&#30340;&#27969;&#24418;&#35757;&#32451;&#65292;&#21482;&#26159;&#26356;&#24555;; &#19981;&#21516;&#37096;&#20998;&#30340;&#21021;&#22987;&#21270;&#32593;&#32476;&#22312;&#30456;&#20284;&#30340;&#27969;&#24418;&#19978;&#21521;&#35299;&#20915;&#26041;&#26696;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop information-geometric techniques to analyze the trajectories of the predictions of deep networks during training. By examining the underlying high-dimensional probabilistic models, we reveal that the training process explores an effectively low-dimensional manifold. Networks with a wide range of architectures, sizes, trained using different optimization methods, regularization techniques, data augmentation techniques, and weight initializations lie on the same manifold in the prediction space. We study the details of this manifold to find that networks with different architectures follow distinguishable trajectories but other factors have a minimal influence; larger networks train along a similar manifold as that of smaller networks, just faster; and networks initialized at very different parts of the prediction space converge to the solution along a similar manifold.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#21644;&#27169;&#22411;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#22122;&#22768;&#32423;&#21035;&#30340;&#25968;&#25454;&#19978;&#27979;&#37327;&#20844;&#24179;&#24615;&#25351;&#26631;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#22122;&#22768;&#23545;&#20110;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01595</link><description>&lt;p&gt;
&#35770;&#25968;&#25454;&#36136;&#37327;&#23545;&#22270;&#20687;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Data Quality on Image Classification Fairness. (arXiv:2305.01595v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#21644;&#27169;&#22411;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#22122;&#22768;&#32423;&#21035;&#30340;&#25968;&#25454;&#19978;&#27979;&#37327;&#20844;&#24179;&#24615;&#25351;&#26631;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#22122;&#22768;&#23545;&#20110;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31639;&#27861;&#20915;&#31574;&#30340;&#26222;&#21450;&#65292;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#20851;&#27880;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30417;&#30563;&#20998;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#20351;&#29992;&#27492;&#31867;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#25972;&#20307;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#27979;&#37327;&#20851;&#38190;&#30340;&#20844;&#24179;&#24615;&#25351;&#26631;&#65292;&#20854;&#20013;&#21253;&#21547;&#26631;&#31614;&#21644;&#35757;&#32451;&#25968;&#25454;&#26412;&#36523;&#22122;&#22768;&#27700;&#24179;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23558;&#26631;&#31614;&#20013;&#30340;&#22122;&#22768;&#25551;&#36848;&#20026;&#35757;&#32451;&#38598;&#20013;&#25968;&#25454;&#26631;&#35760;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#23558;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25551;&#36848;&#20026;&#25968;&#25454;&#20013;&#30340;&#25197;&#26354;&#65292;&#20063;&#26159;&#35757;&#32451;&#38598;&#20013;&#30340;&#20869;&#23481;&#12290;&#36890;&#36807;&#21521;&#21407;&#22987;&#25968;&#25454;&#38598;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#21487;&#20197;&#25506;&#32034;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#19982;&#22312;&#35813;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36755;&#20986;&#30340;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of algorithmic decision-making, increased scrutiny has been placed on these systems. This paper explores the relationship between the quality of the training data and the overall fairness of the models trained with such data in the context of supervised classification. We measure key fairness metrics across a range of algorithms over multiple image classification datasets that have a varying level of noise in both the labels and the training data itself. We describe noise in the labels as inaccuracies in the labelling of the data in the training set and noise in the data as distortions in the data, also in the training set. By adding noise to the original datasets, we can explore the relationship between the quality of the training data and the fairness of the output of the models trained on that data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26799;&#24230;&#21098;&#20999;&#30340;&#25910;&#25947;&#20445;&#35777;&#26426;&#21046;&#65292;&#19981;&#20877;&#38656;&#35201;&#29305;&#23450;&#30340;&#38408;&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#65292;&#21516;&#26102;&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25910;&#25947;&#30340;&#33258;&#30001;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.01588</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26799;&#24230;&#21098;&#20999;&#65306;&#38543;&#26426;&#20559;&#24046;&#21644;&#32039;&#23494;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees. (arXiv:2305.01588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26799;&#24230;&#21098;&#20999;&#30340;&#25910;&#25947;&#20445;&#35777;&#26426;&#21046;&#65292;&#19981;&#20877;&#38656;&#35201;&#29305;&#23450;&#30340;&#38408;&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#65292;&#21516;&#26102;&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25910;&#25947;&#30340;&#33258;&#30001;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#21098;&#20999;&#26159;&#26631;&#20934;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#30340;&#19968;&#31181;&#27969;&#34892;&#20462;&#25913;&#26041;&#27861;&#65292;&#27599;&#27425;&#36845;&#20195;&#23558;&#26799;&#24230;&#33539;&#25968;&#38480;&#21046;&#22312;&#26576;&#20010;&#20540;c&gt;0&#12290;&#23427;&#34987;&#24191;&#27867;&#29992;&#20110;&#31283;&#23450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;( Goodfellow et al., 2016 )&#25110;&#24378;&#21046;&#23454;&#26045;&#24046;&#20998;&#38544;&#31169;( Abadi et al., 2016 )&#12290;&#23613;&#31649;&#21098;&#20999;&#26426;&#21046;&#21463;&#27426;&#36814;&#19988;&#31616;&#21333;&#65292;&#20294;&#20854;&#25910;&#25947;&#20445;&#35777;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#30340;$c$&#20540;&#21644;&#24378;&#22122;&#22768;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#25910;&#25947;&#20445;&#35777;&#65292;&#26174;&#31034;&#20102;&#23545;&#20219;&#24847;&#21098;&#36753;&#38408;&#20540;&#30340;&#31934;&#30830;&#20381;&#36182;&#65292;&#24182;&#19988;&#34920;&#26126;&#25105;&#20204;&#30340;&#20445;&#35777;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#37117;&#26159;&#32039;&#23494;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#34920;&#26126;(i)&#23545;&#20110;&#30830;&#23450;&#24615;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#21098;&#36753;&#38408;&#20540;&#20165;&#24433;&#21709;&#25910;&#25947;&#30340;&#39640;&#38454;&#39033;&#65292;(ii)&#22312;&#38543;&#26426;&#35774;&#32622;&#20013;&#65292;&#21363;&#20351;&#23545;&#20110;&#20219;&#24847;&#23567;&#30340;&#27493;&#38271;&#65292;&#20063;&#19981;&#33021;&#20445;&#35777;&#25910;&#25947;&#21040;&#30495;&#27491;&#30340;&#26368;&#20248;&#35299;&#22312;&#26631;&#20934;&#30340;&#22122;&#22768;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#29305;&#23450;&#30340;&#38543;&#26426;&#22122;&#22768;&#20551;&#35774;&#65292;&#22312;&#27492;&#20551;&#35774;&#19979;&#65292;&#25910;&#25947;&#26159;&#20445;&#35777;&#30340;&#65292;&#21098;&#20999;&#38408;&#20540;$c$&#21487;&#20197;&#29420;&#31435;&#20110;&#27493;&#38271;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient clipping is a popular modification to standard (stochastic) gradient descent, at every iteration limiting the gradient norm to a certain value $c &gt;0$. It is widely used for example for stabilizing the training of deep learning models (Goodfellow et al., 2016), or for enforcing differential privacy (Abadi et al., 2016). Despite popularity and simplicity of the clipping mechanism, its convergence guarantees often require specific values of $c$ and strong noise assumptions.  In this paper, we give convergence guarantees that show precise dependence on arbitrary clipping thresholds $c$ and show that our guarantees are tight with both deterministic and stochastic gradients. In particular, we show that (i) for deterministic gradient descent, the clipping threshold only affects the higher-order terms of convergence, (ii) in the stochastic setting convergence to the true optimum cannot be guaranteed under the standard noise assumption, even under arbitrary small step-sizes. We give ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SELFIES&#20998;&#23376;&#34920;&#31034;&#21644;&#25913;&#36827;&#21518;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#20998;&#23376;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#22810;&#31181;&#31867;&#30340;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.01580</link><description>&lt;p&gt;
&#22522;&#20110;&#26032;&#22411;&#20998;&#23376;&#34920;&#31034;&#19982;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#20998;&#23376;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Molecular design method based on novel molecular representation and variational auto-encoder. (arXiv:2305.01580v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SELFIES&#20998;&#23376;&#34920;&#31034;&#21644;&#25913;&#36827;&#21518;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#20998;&#23376;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#22810;&#31181;&#31867;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#20256;&#32479;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#22522;&#30784;&#19978;&#37319;&#29992;&#26368;&#26032;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;SELFIES&#65292;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#26032;&#30340;&#20998;&#23376;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#22810;&#23618;&#21367;&#31215;&#32593;&#32476;&#12289;&#36153;&#33293;&#23572;&#20449;&#24687;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32858;&#21512;&#25968;&#25454;&#29305;&#24449;&#21644;&#25351;&#23548;&#32534;&#30721;&#36807;&#31243;&#31561;&#25163;&#27573;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#21407;&#22987;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;SELFIES&#21644;&#26032;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#31181;&#31867;&#30340;&#20998;&#23376;&#65292;&#19988;&#29983;&#25104;&#30340;&#20998;&#23376;&#30456;&#20284;&#24230;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the traditional VAE, a novel neural network model is presented, with the latest molecular representation, SELFIES, to improve the effect of generating new molecules. In this model, multi-layer convolutional network and Fisher information are added to the original encoding layer to learn the data characteristics and guide the encoding process, which makes the features of the data hiding layer more aggregated, and integrates the Long Short Term Memory neural network (LSTM) into the decoding layer for better data generation, which effectively solves the degradation phenomenon generated by the encoding layer and decoding layer of the original VAE model. Through experiments on zinc molecular data sets, it is found that the similarity in the new VAE is 8.47% higher than that of the original ones. SELFIES are better at generating a variety of molecules than the traditional molecular representation, SELFIES. Experiments have shown that using SELFIES and the new VAE model presented in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NELoRa&#30340;&#31070;&#32463;&#22686;&#24378;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#25552;&#39640;&#20302;&#21151;&#32791;&#24191;&#22495;&#32593;&#32476;&#20013;LoRa&#30340;&#20449;&#21495;&#22122;&#22768;&#27604;&#65288;SNR&#65289;&#24615;&#33021;&#12290;&#25968;&#25454;&#38598;&#26174;&#31034;&#65292;NELoRa&#30456;&#27604;&#26631;&#20934;LoRa&#35299;&#30721;&#22120;&#21487;&#20197;&#23454;&#29616;1.84-2.35 dB&#30340;SNR&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.01573</link><description>&lt;p&gt;
NELoRa-Bench&#65306;&#19968;&#31181;&#31070;&#32463;&#22686;&#24378;LoRa&#35299;&#35843;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NELoRa-Bench: A Benchmark for Neural-enhanced LoRa Demodulation. (arXiv:2305.01573v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NELoRa&#30340;&#31070;&#32463;&#22686;&#24378;&#35299;&#30721;&#22120;&#65292;&#29992;&#20110;&#25552;&#39640;&#20302;&#21151;&#32791;&#24191;&#22495;&#32593;&#32476;&#20013;LoRa&#30340;&#20449;&#21495;&#22122;&#22768;&#27604;&#65288;SNR&#65289;&#24615;&#33021;&#12290;&#25968;&#25454;&#38598;&#26174;&#31034;&#65292;NELoRa&#30456;&#27604;&#26631;&#20934;LoRa&#35299;&#30721;&#22120;&#21487;&#20197;&#23454;&#29616;1.84-2.35 dB&#30340;SNR&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#21151;&#32791;&#24191;&#22495;&#32593;&#32476;&#65288;LPWANs&#65289;&#26159;&#19968;&#31181;&#26631;&#24535;&#24615;&#30340;&#29289;&#32852;&#32593;&#33539;&#20363;&#65292;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#36828;&#36317;&#31163;&#36890;&#20449;&#30340;&#29305;&#28857;&#12290;&#20854;&#20013;&#65292;LoRa&#22240;&#20854;&#29420;&#29305;&#30340;&#29305;&#24615;&#21644;&#24320;&#28304;&#25216;&#26415;&#32780;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#37319;&#29992;&#21825;&#21886;&#25193;&#39057;&#65288;CSS&#65289;&#35843;&#21046;&#65292;LoRa&#21487;&#20197;&#23454;&#29616;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#36890;&#20449;&#12290;&#26631;&#20934;LoRa&#35299;&#35843;&#26041;&#27861;&#23558;&#25972;&#20010;&#21825;&#21886;&#20449;&#21495;&#30340;&#21151;&#29575;&#32047;&#21152;&#25104;&#39057;&#22495;&#20013;&#30340;&#19968;&#20010;&#33021;&#37327;&#23792;&#20540;&#12290;&#36825;&#26679;&#65292;&#21363;&#20351;SNR&#20302;&#20110;-15 dB&#65292;&#23427;&#20063;&#21487;&#20197;&#25903;&#25345;&#36890;&#20449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NELoRa&#30340;&#31070;&#32463;&#22686;&#24378;&#35299;&#30721;&#22120;&#65292;&#23427;&#21033;&#29992;&#22810;&#32500;&#20449;&#24687;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;SNR&#22686;&#30410;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#35757;&#32451;/&#27979;&#35797;NELoRa&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;27,329&#20010;LoRa&#31526;&#21495;&#65292;&#20854;&#25193;&#23637;&#22240;&#23376;&#20174;7&#21040;10&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#31070;&#32463;&#22686;&#24378;LoRa&#35299;&#35843;&#24615;&#33021;&#12290;&#25968;&#25454;&#38598;&#26174;&#31034;&#65292;NELoRa&#30456;&#27604;&#26631;&#20934;LoRa&#35299;&#30721;&#22120;&#21487;&#20197;&#23454;&#29616;1.84-2.35 dB&#30340;SNR&#22686;&#30410;&#12290;&#35813;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Low-Power Wide-Area Networks (LPWANs) are an emerging Internet-of-Things (IoT) paradigm marked by low-power and long-distance communication. Among them, LoRa is widely deployed for its unique characteristics and open-source technology. By adopting the Chirp Spread Spectrum (CSS) modulation, LoRa enables low signal-to-noise ratio (SNR) communication. The standard LoRa demodulation method accumulates the chirp power of the whole chirp into an energy peak in the frequency domain. In this way, it can support communication even when SNR is lower than -15 dB. Beyond that, we proposed NELoRa, a neural-enhanced decoder that exploits multi-dimensional information to achieve significant SNR gain. This paper presents the dataset used to train/test NELoRa, which includes 27,329 LoRa symbols with spreading factors from 7 to 10, for further improvement of neural-enhanced LoRa demodulation. The dataset shows that NELoRa can achieve 1.84-2.35 dB SNR gain over the standard LoRa decoder. The dataset and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01555</link><description>&lt;p&gt;
&#22914;&#20309;&#21457;&#25381;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;GPT-3.5&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#65292;&#23454;&#29616;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#36824;&#27809;&#26377;&#24471;&#21040;&#20840;&#38754;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#35814;&#32454;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;GPT-3.5&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#30340;&#22522;&#26412;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25968;&#25454;&#29983;&#25104;&#12290;&#20026;&#20102;&#22686;&#24378;&#23569;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#25351;&#23548;&#35828;&#26126;&#21644;&#32422;&#26463;&#27169;&#24335;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#19982;&#20197;&#21069;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#25512;&#21160;&#20197;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#22312;&#22235;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#28608;&#21457;&#26410;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#36817;&#20284;&#35760;&#24518;&#21270;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#36127;&#30456;&#20284;&#24230;&#35780;&#20998;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#23398;&#20064;&#24046;&#24322;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#26174;&#24335;&#21644;&#38544;&#24335;&#20551;&#35774;&#25152;&#23548;&#33268;&#30340;&#25968;&#25454;&#32467;&#26500;&#19981;&#23436;&#20840;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01550</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#30340;&#24046;&#24322;&#31574;&#30053;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36817;&#20284;&#35760;&#24518;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy. (arXiv:2305.01550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#32531;&#35299;&#35821;&#35328;&#27169;&#22411;&#36817;&#20284;&#35760;&#24518;&#21270;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#36127;&#30456;&#20284;&#24230;&#35780;&#20998;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#23398;&#20064;&#24046;&#24322;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#26174;&#24335;&#21644;&#38544;&#24335;&#20551;&#35774;&#25152;&#23548;&#33268;&#30340;&#25968;&#25454;&#32467;&#26500;&#19981;&#23436;&#20840;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#36890;&#36807;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20854;&#20013;&#21487;&#33021;&#21253;&#21547;&#20250;&#21361;&#23475;&#20010;&#20154;&#38544;&#31169;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;LLMs&#34987;&#35777;&#26126;&#20250;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#30340;&#37096;&#20998;&#20869;&#23481;&#24182;&#22312;&#36935;&#21040;&#23545;&#24212;&#25552;&#31034;&#26102;&#30452;&#25509;&#36755;&#20986;&#35813;&#25968;&#25454;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#24046;&#20998;&#38544;&#31169;&#25216;&#26415;&#19978;&#20197;&#35299;&#20915;&#35760;&#24518;&#21270;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#23545;&#35201;&#20445;&#25252;&#25968;&#25454;&#32467;&#26500;&#30340;&#26174;&#24335;&#21644;&#38544;&#24335;&#20551;&#35774;&#65292;&#23548;&#33268;&#38382;&#39064;&#35299;&#20915;&#19981;&#23436;&#20840;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;PPO&#65289;&#24494;&#35843;LLMs&#20197;&#32531;&#35299;&#36817;&#20284;&#35760;&#24518;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36127;&#30456;&#20284;&#24230;&#35780;&#20998;&#65288;&#20363;&#22914;BERTScore&#25110;SacreBLEU&#65289;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#26469;&#23398;&#20064;&#24046;&#24322;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26694;&#26550;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#36817;&#20284;&#35760;&#24518;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language models (LLMs) are trained on large amounts of data, which can include sensitive information that may compromise per- sonal privacy. LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately. Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy. However, these methods rely on explicit and implicit assumptions about the structure of the data to be protected, which often results in an incomplete solution to the problem. To address this, we propose a novel framework that utilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate approximate memorization. Our approach utilizes a negative similarity score, such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity policy. Our results demonstrate that this framework effectively 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#33258;&#21161;&#20803;&#23398;&#20064;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#25105;&#25552;&#39640;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01547</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21161;&#27169;&#22411;&#24341;&#23548;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#25105;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
Accelerating Neural Self-Improvement via Bootstrapping. (arXiv:2305.01547v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#33258;&#21161;&#20803;&#23398;&#20064;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#25105;&#25552;&#39640;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#24207;&#21015;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26368;&#36817;&#24341;&#36215;&#20102;&#26032;&#30340;&#20851;&#27880;&#12290;&#22312;&#26631;&#20934;&#30340;N&#31867;K&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;NN&#34987;&#26126;&#30830;&#20248;&#21270;&#20026;&#23398;&#20064;&#36890;&#36807;&#35266;&#23519;&#24207;&#21015;NK&#26631;&#35760;&#31034;&#20363;&#26469;&#23545;&#26410;&#26631;&#35760;&#30340;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#65292;&#20174;&#32780;&#36843;&#20351;NN&#23398;&#20064;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#32771;&#34385;&#21040;&#26377;&#38480;&#30340;&#35757;&#32451;&#31034;&#20363;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340;&#33258;&#21161;&#20803;&#23398;&#20064;&#24212;&#29992;&#20110;NN&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#40723;&#21169;&#36827;&#19968;&#27493;&#21152;&#36895;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#20165;&#21033;&#29992;NK&#20010;&#26679;&#26412;&#65292;&#23558;K&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#24615;&#33021;&#20248;&#21270;&#21040;&#20854;&#21487;&#20197;&#35266;&#23519;&#21040;&#22810;&#20110;NK&#20010;&#26679;&#26412;&#26102;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22312;&#26631;&#20934;&#30340;Mini-ImageNet&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning with sequence-processing neural networks (NNs) has recently attracted a new wave of attention in the context of large language models. In the standard N-way K-shot learning setting, an NN is explicitly optimised to learn to classify unlabelled inputs by observing a sequence of NK labelled examples. This pressures the NN to learn a learning algorithm that achieves optimal performance, given the limited number of training examples. Here we study an auxiliary loss that encourages further acceleration of few-shot learning, by applying recently proposed bootstrapped meta-learning to NN few-shot learners: we optimise the K-shot learner to match its own performance achievable by observing more than NK examples, using only NK examples. Promising results are obtained on the standard Mini-ImageNet dataset. Our code is public.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#23398;&#24433;&#21709;&#30340;&#32858;&#31867;&#31574;&#30053;&#65292;&#21517;&#20026;Jacobian-Scaled K-means&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#19981;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#38598;&#21363;&#29983;&#25104;&#33021;&#22815;&#25429;&#33719;&#21160;&#21147;&#23398;&#30456;&#20284;&#24615;&#21306;&#22495;&#30340;&#32858;&#31867;&#12290;&#26412;&#26041;&#27861;&#20855;&#26377;&#19968;&#23450;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.01539</link><description>&lt;p&gt;
Jacobian-Scaled K-means&#32858;&#31867;&#29992;&#20110;&#21463;&#29289;&#29702;&#23398;&#24433;&#21709;&#30340;&#21453;&#24212;&#27969;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Jacobian-Scaled K-means Clustering for Physics-Informed Segmentation of Reacting Flows. (arXiv:2305.01539v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#23398;&#24433;&#21709;&#30340;&#32858;&#31867;&#31574;&#30053;&#65292;&#21517;&#20026;Jacobian-Scaled K-means&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#19981;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#38598;&#21363;&#29983;&#25104;&#33021;&#22815;&#25429;&#33719;&#21160;&#21147;&#23398;&#30456;&#20284;&#24615;&#21306;&#22495;&#30340;&#32858;&#31867;&#12290;&#26412;&#26041;&#27861;&#20855;&#26377;&#19968;&#23450;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Jacobian-Scaled K-means (JSK-means)&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#22522;&#20110;K-means&#26694;&#26550;&#30340;&#21463;&#29289;&#29702;&#23398;&#24433;&#21709;&#30340;&#32858;&#31867;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36317;&#31163;&#20989;&#25968;&#30340;&#20462;&#25913;&#23558;&#28508;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#27880;&#20837;&#21040;&#32858;&#31867;&#36807;&#31243;&#20013;&#65306;JSK-means&#32858;&#31867;&#36807;&#31243;&#19981;&#20351;&#29992;&#20256;&#32479;&#30340;&#27431;&#27663;&#36317;&#31163;&#21521;&#37327;&#65292;&#32780;&#26159;&#20351;&#29992;&#20174;&#38598;&#32676;&#36136;&#24515;&#22788;&#27714;&#24471;&#30340;&#21160;&#24577;&#31995;&#32479;Jacobian&#30697;&#38453;&#32553;&#25918;&#30340;&#36317;&#31163;&#21521;&#37327;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;JSK-means&#31639;&#27861;--&#32780;&#19981;&#26159;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#38598;--&#29983;&#25104;&#33021;&#22815;&#25429;&#33719;&#21160;&#21147;&#23398;&#30456;&#20284;&#24615;&#21306;&#22495;&#30340;&#32858;&#31867;&#65292;&#21363;&#32858;&#31867;&#26159;&#21521;&#39640;&#25935;&#24863;&#21306;&#22495;&#22312;&#30456;&#31354;&#38388;&#20013;&#37325;&#26032;&#20998;&#24067;&#24182;&#30001;&#26679;&#26412;&#30340;&#28304;&#39033;&#30340;&#30456;&#20284;&#24615;&#25551;&#36848;&#32780;&#38750;&#26679;&#26412;&#26412;&#36523;&#12290;&#35813;&#31639;&#27861;&#22312;&#22797;&#26434;&#30340;&#21453;&#24212;&#27969;&#27169;&#25311;&#25968;&#25454;&#38598;(&#36890;&#36947;&#29190;&#36720;&#37197;&#32622;)&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#20854;&#20013;&#21160;&#24577;&#24615;&#36136;&#26159;
&lt;/p&gt;
&lt;p&gt;
This work introduces Jacobian-scaled K-means (JSK-means) clustering, which is a physics-informed clustering strategy centered on the K-means framework. The method allows for the injection of underlying physical knowledge into the clustering procedure through a distance function modification: instead of leveraging conventional Euclidean distance vectors, the JSK-means procedure operates on distance vectors scaled by matrices obtained from dynamical system Jacobians evaluated at the cluster centroids. The goal of this work is to show how the JSK-means algorithm -- without modifying the input dataset -- produces clusters that capture regions of dynamical similarity, in that the clusters are redistributed towards high-sensitivity regions in phase space and are described by similarity in the source terms of samples instead of the samples themselves. The algorithm is demonstrated on a complex reacting flow simulation dataset (a channel detonation configuration), where the dynamics in the the
&lt;/p&gt;</description></item><item><title>DeepEIK&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#65292;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01523</link><description>&lt;p&gt;
&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#25512;&#21160;AI&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Empowering AI drug discovery with explicit and implicit knowledge. (arXiv:2305.01523v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01523
&lt;/p&gt;
&lt;p&gt;
DeepEIK&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#65292;&#36890;&#36807;&#29305;&#24449;&#34701;&#21512;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29420;&#31435;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#26174;&#24335;&#30693;&#35782;&#25110;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#30340;&#38544;&#24335;&#30693;&#35782;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#30340;&#30740;&#31350;&#36805;&#36895;&#22686;&#38271;&#12290;&#36825;&#20123;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;AI&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#29420;&#31435;&#22320;&#25972;&#21512;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#20250;&#38459;&#30861;&#23545;&#20998;&#23376;&#30340;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DeepEIK&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#26469;&#36827;&#34892;AI&#33647;&#29289;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivation: Recently, research on independently utilizing either explicit knowledge from knowledge graphs or implicit knowledge from biomedical literature for AI drug discovery has been growing rapidly. These approaches have greatly improved the prediction accuracy of AI models on multiple downstream tasks. However, integrating explicit and implicit knowledge independently hinders their understanding of molecules. Results: We propose DeepEIK, a unified deep learning framework that incorporates both explicit and implicit knowledge for AI drug discovery. We adopt feature fusion to process the multi-modal inputs, and leverage the attention mechanism to denoise the text information. Experiments show that DeepEIK significantly outperforms state-of-the-art methods on crucial tasks in AI drug discovery including drug-target interaction prediction, drug property prediction and protein-protein interaction prediction. Further studies show that benefiting from explicit and implicit knowledge, our
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26333;&#20809;&#30340;&#39118;&#38505;&#27491;&#21017;&#21270;&#23545;IPS&#20272;&#35745;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#25490;&#21517;&#27169;&#22411;&#19982;&#32473;&#23450;&#30340;&#23433;&#20840;&#27169;&#22411;&#25509;&#36817;&#65292;&#20174;&#32780;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#38477;&#20302;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.01522</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#26333;&#20809;&#30340;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21453;&#20107;&#23454;&#23398;&#20064;&#25490;&#24207;&#23433;&#20840;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Safe Deployment for Counterfactual Learning to Rank with Exposure-Based Risk Minimization. (arXiv:2305.01522v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26333;&#20809;&#30340;&#39118;&#38505;&#27491;&#21017;&#21270;&#23545;IPS&#20272;&#35745;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#30830;&#20445;&#23398;&#20064;&#25490;&#21517;&#27169;&#22411;&#19982;&#32473;&#23450;&#30340;&#23433;&#20840;&#27169;&#22411;&#25509;&#36817;&#65292;&#20174;&#32780;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#38477;&#20302;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#23398;&#20064;&#25490;&#24207;&#65288;CLTR&#65289;&#20381;&#36182;&#20110;&#22522;&#20110;&#26333;&#20809;&#30340;&#20498;&#25968;&#27010;&#29575;&#35780;&#20998;&#65288;IPS&#65289;&#65292;&#19968;&#31181;LTR&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;IPS&#26469;&#32416;&#27491;&#20301;&#32622;&#20559;&#24046;&#12290;&#34429;&#28982;IPS&#21487;&#20197;&#25552;&#20379;&#26080;&#20559;&#21644;&#19968;&#33268;&#30340;&#20272;&#35745;&#65292;&#20294;&#36890;&#24120;&#20250;&#21463;&#21040;&#39640;&#26041;&#24046;&#30340;&#24433;&#21709;&#12290;&#23588;&#20854;&#26159;&#24403;&#21487;&#29992;&#28857;&#20987;&#25968;&#25454;&#24456;&#23569;&#26102;&#65292;&#36825;&#31181;&#26041;&#24046;&#21487;&#33021;&#20250;&#23548;&#33268;CLTR&#23398;&#20064;&#27425;&#20248;&#30340;&#25490;&#21517;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;CLTR&#26041;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#65292;&#22240;&#20026;&#31616;&#21333;&#22320;&#37096;&#32626;&#23427;&#20204;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#38750;&#24120;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#39118;&#38505;&#24863;&#30693;CLTR&#26041;&#27861;&#65292;&#20855;&#26377;&#23433;&#20840;&#37096;&#32626;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#23545;LTR&#30340;IPS&#20272;&#35745;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26333;&#20809;&#30340;&#39118;&#38505;&#27491;&#21017;&#21270;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#39118;&#38505;&#27491;&#21017;&#21270;&#24809;&#32602;&#23398;&#20064;&#27169;&#22411;&#30340;&#25490;&#21517;&#34892;&#20026;&#19982;&#32473;&#23450;&#23433;&#20840;&#27169;&#22411;&#30340;&#19981;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#22312;IPS&#20272;&#35745;&#23384;&#22312;&#39640;&#24230;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#23427;&#30830;&#20445;&#23398;&#20064;&#25490;&#21517;&#27169;&#22411;&#19982;&#21487;&#20449;&#27169;&#22411;&#20445;&#25345;&#25509;&#36817;&#65292;&#20174;&#32780;&#22823;&#22823;&#38477;&#20302;&#20102;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual learning to rank (CLTR) relies on exposure-based inverse propensity scoring (IPS), a LTR-specific adaptation of IPS to correct for position bias. While IPS can provide unbiased and consistent estimates, it often suffers from high variance. Especially when little click data is available, this variance can cause CLTR to learn sub-optimal ranking behavior. Consequently, existing CLTR methods bring significant risks with them, as naively deploying their models can result in very negative user experiences. We introduce a novel risk-aware CLTR method with theoretical guarantees for safe deployment. We apply a novel exposure-based concept of risk regularization to IPS estimation for LTR. Our risk regularization penalizes the mismatch between the ranking behavior of a learned model and a given safe model. Thereby, it ensures that learned ranking models stay close to a trusted model, when there is high uncertainty in IPS estimation, which greatly reduces the risks during deployme
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RECODE&#30340;&#38750;&#21442;&#25968;&#26032;&#39062;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#23427;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36319;&#36394;&#29366;&#24577;&#35775;&#38382;&#35745;&#25968;&#65292;&#24182;&#19982;&#26032;&#39062;&#30340;&#36870;&#21160;&#21147;&#23398;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01521</link><description>&lt;p&gt;
&#25581;&#31192;&#38271;&#26399;&#22522;&#20110;&#26032;&#39062;&#24615;&#25506;&#32034;&#20013;&#34920;&#31034;&#26041;&#27861;&#30340;&#23041;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Power of Representations in Long-term Novelty-based Exploration. (arXiv:2305.01521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RECODE&#30340;&#38750;&#21442;&#25968;&#26032;&#39062;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#23427;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#36319;&#36394;&#29366;&#24577;&#35775;&#38382;&#35745;&#25968;&#65292;&#24182;&#19982;&#26032;&#39062;&#30340;&#36870;&#21160;&#21147;&#23398;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RECODE&#65288;&#22522;&#20110;&#32858;&#31867;&#30340;&#22312;&#32447;&#23494;&#24230;&#20272;&#35745;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65289;&#30340;&#38750;&#21442;&#25968;&#26032;&#39062;&#24615;&#25506;&#32034;&#26041;&#27861;&#65292;&#20854;&#26681;&#25454;&#22312;&#25152;&#36873;&#25321;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24615;&#32858;&#21512;&#29366;&#24577;&#24182;&#20272;&#35745;&#20854;&#35775;&#38382;&#27425;&#25968;&#12290;&#36890;&#36807;&#23558;&#32463;&#20856;&#32858;&#31867;&#26041;&#27861;&#36866;&#24212;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38750;&#24179;&#31283;&#24615;&#29615;&#22659;&#65292;RECODE&#21487;&#22312;&#25968;&#21315;&#20010;&#22238;&#21512;&#20013;&#26377;&#25928;&#22320;&#36319;&#36394;&#29366;&#24577;&#35775;&#38382;&#35745;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36870;&#21160;&#21147;&#23398;&#25439;&#22833;&#30340;&#27867;&#21270;&#24418;&#24335;&#65292;&#23427;&#21033;&#29992;&#25513;&#30721;&#21464;&#21387;&#22120;&#32467;&#26500;&#36827;&#34892;&#22810;&#27493;&#39044;&#27979;&#12290;RECODE&#19982;&#27492;&#30456;&#32467;&#21512;&#65292;&#22312;DM-Hard-8&#30340;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;3D&#25506;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#22312;&#22256;&#38590;&#30340;Atari&#28216;&#25103;&#20013;&#65292;RECODE&#20063;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#25104;&#20026;&#39318;&#20010;&#25104;&#21151;&#36890;&#20851;"Pitfall!"&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method for novelty-based exploration that estimates visitation counts for clusters of states based on their similarity in a chosen embedding space. By adapting classical clustering to the nonstationary setting of Deep RL, RECODE can efficiently track state visitation counts over thousands of episodes. We further propose a novel generalization of the inverse dynamics loss, which leverages masked transformer architectures for multi-step prediction; which in conjunction with RECODE achieves a new state-of-the-art in a suite of challenging 3D-exploration tasks in DM-Hard-8. RECODE also sets new state-of-the-art in hard exploration Atari games, and is the first agent to reach the end screen in "Pitfall!".
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#31995;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;CGIB&#65292;&#36890;&#36807;&#26816;&#27979;&#20854;&#20013;&#30340;&#26680;&#24515;&#23376;&#22270;&#65292;&#39044;&#27979;&#23545;&#22270;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2305.01520</link><description>&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#30340;&#26465;&#20214;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Conditional Graph Information Bottleneck for Molecular Relational Learning. (arXiv:2305.01520v1 [q-bio.MN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#31995;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;CGIB&#65292;&#36890;&#36807;&#26816;&#27979;&#20854;&#20013;&#30340;&#26680;&#24515;&#23376;&#22270;&#65292;&#39044;&#27979;&#23545;&#22270;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#26159;&#25351;&#23398;&#20064;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#65292;&#22240;&#20854;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#23376;&#31185;&#23398;&#32780;&#24341;&#36215;&#26497;&#22823;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#20998;&#23376;&#24314;&#27169;&#20026;&#22270;&#32467;&#26500;&#65292;&#24182;&#32771;&#34385;&#20004;&#20010;&#20998;&#23376;&#20043;&#38388;&#30340;&#21407;&#23376;&#32423;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20102;&#21270;&#23398;&#30340;&#26412;&#36136;&#12290;&#21363;&#21270;&#21512;&#29289;&#30001;&#22810;&#20010;&#20114;&#30456;&#20316;&#29992;&#30340;&#23376;&#32467;&#26500;&#32452;&#25104;&#65292;&#36825;&#20123;&#23376;&#32467;&#26500;&#20250;&#24341;&#36215;&#29420;&#29305;&#30340;&#21270;&#23398;&#21453;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#31995;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;CGIB&#65292;&#36890;&#36807;&#26816;&#27979;&#20854;&#20013;&#30340;&#26680;&#24515;&#23376;&#22270;&#65292;&#39044;&#27979;&#23545;&#22270;&#23545;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#65292;&#22312;&#32473;&#23450;&#19968;&#23545;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#20010;&#22270;&#20013;&#25214;&#21040;&#21253;&#21547;&#26377;&#20851;&#25152;&#38656;&#20219;&#21153;&#30340;&#26368;&#23567;&#20805;&#20998;&#20449;&#24687;&#30340;&#23376;&#22270;&#65292;&#20197;&#27492;&#26469;&#39044;&#27979;&#36825;&#23545;&#22270;&#20043;&#38388;&#30340;&#20449;&#24687;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular relational learning, whose goal is to learn the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. Recently, graph neural networks have recently shown great success in molecular relational learning by modeling a molecule as a graph structure, and considering atom-level interactions between two molecules. Despite their success, existing molecular relational learning methods tend to overlook the nature of chemistry, i.e., a chemical compound is composed of multiple substructures such as functional groups that cause distinctive chemical reactions. In this work, we propose a novel relational learning framework, called CGIB, that predicts the interaction behavior between a pair of graphs by detecting core subgraphs therein. The main idea is, given a pair of graphs, to find a subgraph from a graph that contains the minimal sufficient information regarding the task at hand conditioned on the paired graph
&lt;/p&gt;</description></item><item><title>BCEdge&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;-based&#35843;&#24230;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#25209;&#22788;&#29702;&#21644;DNN&#25512;&#26029;&#26381;&#21153;&#36827;&#34892;&#24182;&#21457;&#25191;&#34892;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25928;&#29992;&#26469;&#21516;&#26102;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#19988;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01519</link><description>&lt;p&gt;
BCEdge&#65306;&#36866;&#24212;&#36793;&#32536;&#24179;&#21488;&#19978;&#30340;&#33258;&#36866;&#24212;&#25209;&#22788;&#29702;&#21644;SLO&#24863;&#30693;&#30340;DNN&#25512;&#26029;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
BCEdge: SLO-Aware DNN Inference Services with Adaptive Batching on Edge Platforms. (arXiv:2305.01519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01519
&lt;/p&gt;
&lt;p&gt;
BCEdge&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;-based&#35843;&#24230;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#25209;&#22788;&#29702;&#21644;DNN&#25512;&#26029;&#26381;&#21153;&#36827;&#34892;&#24182;&#21457;&#25191;&#34892;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25928;&#29992;&#26469;&#21516;&#26102;&#23454;&#29616;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#19988;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#24212;&#29992;&#20110;&#21508;&#31181;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#65292;&#36793;&#32536;&#25512;&#26029;&#24179;&#21488;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#20302;&#24310;&#36831;&#23545;&#20110;&#25552;&#39640;&#26381;&#21153;&#36136;&#37327;&#38750;&#24120;&#20851;&#38190;&#12290;&#23545;&#20110;&#20855;&#26377;&#22810;&#20010;DNN&#27169;&#22411;&#30340;&#36793;&#32536;&#24179;&#21488;&#65292;&#36825;&#31181;&#24773;&#20917;&#32473;&#35843;&#24230;&#31243;&#24207;&#35774;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#27599;&#20010;&#35831;&#27714;&#30340;&#26381;&#21153;&#27700;&#24179;&#30446;&#26631;(SLOs)&#37117;&#19981;&#21516;&#65292;&#20197;&#25552;&#39640;&#26381;&#21153;&#36136;&#37327;(QoS)&#12290;&#20854;&#27425;&#65292;&#36793;&#32536;&#24179;&#21488;&#24212;&#35813;&#33021;&#22815;&#26377;&#25928;&#22320;&#35843;&#24230;&#22810;&#20010;&#24322;&#26500;DNN&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#21033;&#29992;&#29575;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20004;&#20010;&#30446;&#26631;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;BCEdge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;-based&#35843;&#24230;&#26694;&#26550;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#25209;&#22788;&#29702;&#21644;DNN&#25512;&#26029;&#26381;&#21153;&#22312;&#36793;&#32536;&#24179;&#21488;&#19978;&#36827;&#34892;&#24182;&#21457;&#25191;&#34892;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#23454;&#29992;&#20989;&#25968;&#26469;&#35780;&#20272;&#21534;&#21520;&#37327;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;BCEdge&#20013;&#30340;&#35843;&#24230;&#31243;&#24207;&#21033;&#29992;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26469;&#36890;&#36807;1)&#20849;&#21516;&#20248;&#21270;&#25209;&#22823;&#23567;&#21644;2)&#22522;&#20110;SLOs&#30340;&#21516;&#26102;&#25191;&#34892;&#25968;&#37327;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BCEdge&#22312;&#21534;&#21520;&#37327;&#21644;SLO&#36798;&#25104;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks (DNNs) are being applied to a wide range of edge intelligent applications, it is critical for edge inference platforms to have both high-throughput and low-latency at the same time. Such edge platforms with multiple DNN models pose new challenges for scheduler designs. First, each request may have different service level objectives (SLOs) to improve quality of service (QoS). Second, the edge platforms should be able to efficiently schedule multiple heterogeneous DNN models so that system utilization can be improved. To meet these two goals, this paper proposes BCEdge, a novel learning-based scheduling framework that takes adaptive batching and concurrent execution of DNN inference services on edge platforms. We define a utility function to evaluate the trade-off between throughput and latency. The scheduler in BCEdge leverages maximum entropy-based deep reinforcement learning (DRL) to maximize utility by 1) co-optimizing batch size and 2) the number of concurren
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#20041;&#39044;&#27979;&#35268;&#21017;&#21487;&#22797;&#21046;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#20154;&#26694;&#26550;&#23450;&#20041;&#21487;&#22797;&#21046;&#24615;&#65292;&#26088;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#26356;&#31995;&#32479;&#30340;&#21487;&#22797;&#21046;&#24615;&#35780;&#20272;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2305.01518</link><description>&lt;p&gt;
&#23450;&#20041;&#39044;&#27979;&#35268;&#21017;&#30340;&#21487;&#22797;&#21046;&#24615;
&lt;/p&gt;
&lt;p&gt;
Defining Replicability of Prediction Rules. (arXiv:2305.01518v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#20041;&#39044;&#27979;&#35268;&#21017;&#21487;&#22797;&#21046;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#20154;&#26694;&#26550;&#23450;&#20041;&#21487;&#22797;&#21046;&#24615;&#65292;&#26088;&#22312;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#26356;&#31995;&#32479;&#30340;&#21487;&#22797;&#21046;&#24615;&#35780;&#20272;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#20041;&#39044;&#27979;&#35268;&#21017;&#21487;&#22797;&#21046;&#24615;&#30340;&#26041;&#27861;&#12290;&#21463;&#26368;&#36817;&#19968;&#20221;&#32654;&#22269;&#22269;&#23478;&#31185;&#23398;&#38498;&#25253;&#21578;&#30340;&#21551;&#21457;&#65292;&#25991;&#31456;&#20174;&#19968;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#35748;&#20026;&#21487;&#22797;&#21046;&#24615;&#26159;&#22312;&#36866;&#21512;&#22238;&#31572;&#21516;&#19968;&#39044;&#27979;&#38382;&#39064;&#30340;&#22810;&#20010;&#30740;&#31350;&#20013;&#33719;&#24471;&#19968;&#33268;&#32467;&#26524;&#65292;&#27599;&#20010;&#30740;&#31350;&#37117;&#26377;&#33258;&#24049;&#30340;&#25968;&#25454;&#12290;&#28982;&#21518;&#35752;&#35770;&#20102;&#23450;&#20041;&#35813;&#35821;&#21477;&#20851;&#38190;&#37096;&#20998;&#30340;&#27010;&#24565;&#21644;&#38382;&#39064;&#65292;&#24182;&#19987;&#27880;&#20110;&#20856;&#22411;&#21033;&#29992;&#29615;&#22659;&#20013;&#8220;&#19968;&#33268;&#32467;&#26524;&#8221;&#30340;&#21547;&#20041;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20195;&#29702;&#20154;&#26694;&#26550;&#26469;&#23450;&#20041;&#21487;&#22797;&#21046;&#24615;&#65292;&#20195;&#29702;&#20154;&#26082;&#19981;&#26159;&#21512;&#20316;&#20249;&#20276;&#20063;&#19981;&#26159;&#23545;&#25163;&#12290;&#20854;&#20013;&#19968;&#20123;&#26222;&#36941;&#23454;&#29992;&#30340;&#26041;&#27861;&#25104;&#20026;&#29305;&#20363;&#12290;&#24076;&#26395;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#26356;&#31995;&#32479;&#30340;&#21487;&#22797;&#21046;&#24615;&#35780;&#20272;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article I propose an approach for defining replicability for prediction rules. Motivated by a recent NAS report, I start from the perspective that replicability is obtaining consistent results across studies suitable to address the same prediction question, each of which has obtained its own data. I then discuss concept and issues in defining key elements of this statement. I focus specifically on the meaning of "consistent results" in typical utilization contexts, and propose a multi-agent framework for defining replicability, in which agents are neither partners nor adversaries. I recover some of the prevalent practical approaches as special cases. I hope to provide guidance for a more systematic assessment of replicability in machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#29616;&#23454;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#20013;&#23884;&#20837;&#34920;&#30340;&#24102;&#23485;&#38656;&#27714;&#21644;&#23616;&#37096;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#20869;&#23384;&#25552;&#20986;MTrainS&#26469;&#25552;&#39640;DLRM&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.01515</link><description>&lt;p&gt;
MTrainS: &#20351;&#29992;&#24322;&#26500;&#20869;&#23384;&#25552;&#39640;DLRM&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
MTrainS: Improving DLRM training efficiency using heterogeneous memories. (arXiv:2305.01515v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#29616;&#23454;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#20013;&#23884;&#20837;&#34920;&#30340;&#24102;&#23485;&#38656;&#27714;&#21644;&#23616;&#37096;&#24615;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#20869;&#23384;&#25552;&#20986;MTrainS&#26469;&#25552;&#39640;DLRM&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#38750;&#24120;&#24222;&#22823;&#65292;&#22312;&#35757;&#32451;&#26102;&#38656;&#35201;&#20351;&#29992;&#20960;TB&#30340;&#20869;&#23384;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#36136;&#37327;&#65292;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#22686;&#38271;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#36825;&#31181;&#27169;&#22411;&#22686;&#38271;&#35201;&#27714;&#25968;&#25454;&#20013;&#24515;&#22823;&#37327;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#25928;&#29575;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#20445;&#25345;&#25968;&#25454;&#20013;&#24515;&#30340;&#21151;&#29575;&#38656;&#27714;&#21487;&#25511;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;(DLRM)&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#34920;&#25429;&#25417;&#20998;&#31867;&#36755;&#20837;&#30340;&#31232;&#30095;&#29305;&#24449;&#26159;&#27169;&#22411;&#22823;&#23567;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#20869;&#23384;&#24102;&#23485;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#23454;&#20013;&#37096;&#32626;&#27169;&#22411;&#20013;&#23884;&#20837;&#34920;&#30340;&#24102;&#23485;&#38656;&#27714;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19981;&#21516;&#34920;&#30340;&#24102;&#23485;&#35201;&#27714;&#19981;&#22343;&#21248;&#65292;&#24182;&#19988;&#23884;&#20837;&#34920;&#26174;&#31034;&#20986;&#39640;&#26102;&#24207;&#23616;&#37096;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;MTrainS&#65292;&#21033;&#29992;&#24322;&#26500;&#20869;&#23384;&#65292;&#21253;&#25324;&#23383;&#33410;&#21644;&#22359;&#21487;&#23547;&#22336;&#23384;&#20648;&#31867;&#20869;&#23384;&#65292;&#29992;&#20110;DLRM&#30340;&#20998;&#23618;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation models are very large, requiring terabytes (TB) of memory during training. In pursuit of better quality, the model size and complexity grow over time, which requires additional training data to avoid overfitting. This model growth demands a large number of resources in data centers. Hence, training efficiency is becoming considerably more important to keep the data center power demand manageable. In Deep Learning Recommendation Models (DLRM), sparse features capturing categorical inputs through embedding tables are the major contributors to model size and require high memory bandwidth. In this paper, we study the bandwidth requirement and locality of embedding tables in real-world deployed models. We observe that the bandwidth requirement is not uniform across different tables and that embedding tables show high temporal locality. We then design MTrainS, which leverages heterogeneous memory, including byte and block addressable Storage Class Memory for DLRM hierarchicall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#39044;&#35774;&#21442;&#25968;&#30340;ART&#25299;&#25169;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#32858;&#31867;&#31639;&#27861;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.01507</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#38656;&#39044;&#35774;&#21442;&#25968;&#30340;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#25299;&#25169;&#32858;&#31867;&#31639;&#27861;&#65292;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Parameter-free Adaptive Resonance Theory-based Topological Clustering Algorithm Capable of Continual Learning. (arXiv:2305.01507v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#39044;&#35774;&#21442;&#25968;&#30340;ART&#25299;&#25169;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#27604;&#29616;&#26377;&#32858;&#31867;&#31639;&#27861;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#26469;&#35828;&#65292;&#22312;&#33258;&#36866;&#24212;&#20849;&#25391;&#29702;&#35770;&#65288;ART&#65289;&#31639;&#27861;&#20013;&#65292;&#33410;&#28857;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#30456;&#20284;&#24230;&#38408;&#20540;&#65288;&#21363;&#35686;&#35273;&#21442;&#25968;&#65289;&#23545;&#32858;&#31867;&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25299;&#25169;&#32858;&#31867;&#31639;&#27861;&#20013;&#30340;&#36793;&#32536;&#21024;&#38500;&#38408;&#20540;&#22312;&#33258;&#32452;&#32455;&#36807;&#31243;&#20013;&#29983;&#25104;&#20114;&#30456;&#20998;&#31163;&#30340;&#32858;&#31867;&#20013;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#39044;&#35774;&#21442;&#25968;&#30340;ART&#25299;&#25169;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#12290;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#31639;&#27861;&#22312;&#26080;&#39044;&#35774;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#27604;&#29616;&#26377;&#32858;&#31867;&#31639;&#27861;&#26356;&#20248;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In general, a similarity threshold (i.e., a vigilance parameter) for a node learning process in Adaptive Resonance Theory (ART)-based algorithms has a significant impact on clustering performance. In addition, an edge deletion threshold in a topological clustering algorithm plays an important role in adaptively generating well-separated clusters during a self-organizing process. In this paper, we propose a new parameter-free ART-based topological clustering algorithm capable of continual learning by introducing parameter estimation methods. Experimental results with synthetic and real-world datasets show that the proposed algorithm has superior clustering performance to the state-of-the-art clustering algorithms without any parameter pre-specifications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22823;&#35268;&#27169;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#19978;&#30340;&#27773;&#36710;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#20998;&#26512;&#23454;&#39564;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01506</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#20013;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Discovering the Effectiveness of Pre-Training in a Large-scale Car-sharing Platform. (arXiv:2305.01506v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22823;&#35268;&#27169;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#19978;&#30340;&#27773;&#36710;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#20998;&#26512;&#23454;&#39564;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#36171;&#20104;&#20102;&#21508;&#31181;&#26234;&#33021;&#20132;&#36890;&#24212;&#29992;&#20197;&#21147;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#19978;&#12290;&#20256;&#32479;&#30340;&#20849;&#20139;&#27773;&#36710;&#26381;&#21153;&#36816;&#33829;&#39640;&#24230;&#20381;&#36182;&#20110;&#36710;&#38431;&#31649;&#29702;&#20013;&#30340;&#20154;&#31867;&#21442;&#19982;&#65292;&#29616;&#20195;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#21017;&#20801;&#35768;&#29992;&#25143;&#22312;&#20351;&#29992;&#21069;&#21518;&#19978;&#20256;&#27773;&#36710;&#22270;&#20687;&#65292;&#20197;&#26816;&#26597;&#27773;&#36710;&#32780;&#26080;&#38656;&#23454;&#22320;&#35775;&#38382;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#19978;&#36848;&#26816;&#26597;&#20219;&#21153;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#26222;&#36941;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#25216;&#26415;&#20197;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#19979;&#24314;&#31435;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#22788;&#29702;&#27773;&#36710;&#22270;&#20687;&#30340;&#20505;&#36873;&#20174;&#19994;&#32773;&#24456;&#21487;&#33021;&#20250;&#36973;&#21463;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#22240;&#27492;&#25105;&#20204;&#23545;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#22797;&#26434;&#30340;&#31867;&#27604;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#32570;&#20047;&#28145;&#20837;&#21078;&#26512;&#12290;&#37492;&#20110;&#19978;&#36848;&#20998;&#26512;&#32570;&#20047;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20998;&#26512;&#65292;&#20197;&#25506;&#31350;&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#27773;&#36710;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#23558;&#24050;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#25216;&#26415;&#30830;&#23454;&#25552;&#39640;&#20102;&#27773;&#36710;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#20934;&#30830;&#24615;&#36824;&#26159;&#25928;&#29575;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#22312;&#20849;&#20139;&#27773;&#36710;&#24179;&#21488;&#24212;&#29992;&#20013;&#25506;&#31350;&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress of deep learning has empowered various intelligent transportation applications, especially in car-sharing platforms. While the traditional operations of the car-sharing service highly relied on human engagements in fleet management, modern car-sharing platforms let users upload car images before and after their use to inspect the cars without a physical visit. To automate the aforementioned inspection task, prior approaches utilized deep neural networks. They commonly employed pre-training, a de-facto technique to establish an effective model under the limited number of labeled datasets. As candidate practitioners who deal with car images would presumably get suffered from the lack of a labeled dataset, we analyzed a sophisticated analogy into the effectiveness of pre-training is important. However, prior studies primarily shed a little spotlight on the effectiveness of pre-training. Motivated by the aforementioned lack of analysis, our study proposes a series of analys
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27979;&#37327;&#27169;&#22411;&#21644;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01481</link><description>&lt;p&gt;
&#27169;&#22411;&#20043;&#38388;&#30340;&#28508;&#22312;&#19968;&#33268;&#24615;&#65306;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Great Models Think Alike: Improving Model Reliability via Inter-Model Latent Agreement. (arXiv:2305.01481v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01481
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#37327;&#27169;&#22411;&#21644;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24212;&#29992;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#38469;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#30001;&#20110;&#36807;&#24230;&#33258;&#20449;&#65292;&#27169;&#22411;&#32463;&#24120;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#27979;&#37327;&#19968;&#20010;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#19982;&#21478;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#19981;&#36830;&#36143;&#24615;&#65292;&#21363;&#20219;&#24847;&#26059;&#36716;&#21644;&#19981;&#21516;&#30340;&#32500;&#24230;&#65292;&#20004;&#20010;&#19981;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#24456;&#38590;&#34913;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#19981;&#36830;&#36143;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#8220;&#37051;&#22495;&#19968;&#33268;&#24615;&#24230;&#37327;&#8221;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#19968;&#33268;&#24615;&#19982;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#26377;&#24778;&#20154;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#21518;&#32493;&#30340;&#26041;&#24335;&#20013;&#23558;&#37051;&#22495;&#19968;&#33268;&#24615;&#34701;&#20837;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#25925;&#38556;&#26816;&#27979;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable application of machine learning is of primary importance to the practical deployment of deep learning methods. A fundamental challenge is that models are often unreliable due to overconfidence. In this paper, we estimate a model's reliability by measuring \emph{the agreement between its latent space, and the latent space of a foundation model}. However, it is challenging to measure the agreement between two different latent spaces due to their incoherence, \eg, arbitrary rotations and different dimensionality. To overcome this incoherence issue, we design a \emph{neighborhood agreement measure} between latent spaces and find that this agreement is surprisingly well-correlated with the reliability of a model's predictions. Further, we show that fusing neighborhood agreement into a model's predictive confidence in a post-hoc way significantly improves its reliability. Theoretical analysis and extensive experiments on failure detection across various datasets verify the effective
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#65288;GCMM&#65289;&#30340;&#24615;&#36136;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#25193;&#23637;&#26399;&#26395;&#26368;&#22823;&#31639;&#27861;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;GCMM&#30456;&#27604;&#20110;GMM&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#25968;&#25454;&#24182;&#23454;&#29616;&#26356;&#28145;&#20837;&#30340;&#25968;&#25454;&#25366;&#25496;&#12290;</title><link>http://arxiv.org/abs/2305.01479</link><description>&lt;p&gt;
&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#30340;&#24615;&#36136;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the properties of Gaussian Copula Mixture Models. (arXiv:2305.01479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#65288;GCMM&#65289;&#30340;&#24615;&#36136;&#65292;&#24320;&#21457;&#20102;&#22522;&#20110;&#25193;&#23637;&#26399;&#26395;&#26368;&#22823;&#31639;&#27861;&#30340;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;GCMM&#30456;&#27604;&#20110;GMM&#21487;&#20197;&#26356;&#22909;&#22320;&#25311;&#21512;&#25968;&#25454;&#24182;&#23454;&#29616;&#26356;&#28145;&#20837;&#30340;&#25968;&#25454;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;Copula&#28151;&#21512;&#27169;&#22411;&#65288;GCMM&#65289;&#26159;&#20351;&#29992;Copula&#27010;&#24565;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#25512;&#24191;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#20854;&#25968;&#23398;&#23450;&#20041;&#65292;&#24182;&#30740;&#31350;&#20102;&#20284;&#28982;&#20989;&#25968;&#30340;&#24615;&#36136;&#12290;&#22522;&#20110;&#36825;&#20123;&#23646;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25193;&#23637;&#26399;&#26395;&#26368;&#22823;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#28151;&#21512;Copula&#30340;&#21442;&#25968;&#65292;&#32780;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#30340;&#36793;&#38469;&#20998;&#24067;&#21017;&#20351;&#29992;&#21333;&#29420;&#30340;&#38750;&#21442;&#25968;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20272;&#35745;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;GMM&#65292;GCMM&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#32858;&#31867;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25311;&#21512;&#65307;&#27492;&#22806;&#65292;GCMM&#21487;&#20197;&#21033;&#29992;&#27599;&#20010;&#32500;&#24230;&#19978;&#30340;&#19981;&#21516;&#27493;&#25968;&#25454;&#23454;&#29616;&#26356;&#28145;&#20837;&#30340;&#25968;&#25454;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian copula mixture models (GCMM) are the generalization of Gaussian Mixture models using the concept of copula. Its mathematical definition is given and the properties of likelihood function are studied in this paper. Based on these properties, extended Expectation Maximum algorithms are developed for estimating parameters for the mixture of copulas while marginal distributions corresponding to each component is estimated using separate nonparametric statistical methods. In the experiment, GCMM can achieve better goodness-of-fitting given the same number of clusters as GMM; furthermore, GCMM can utilize unsynchronized data on each dimension to achieve deeper mining of data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#36951;&#20256;&#31639;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32959;&#30244;&#21551;&#21457;&#30340;&#22522;&#22240;&#32452;&#27979;&#24207;&#26144;&#23556;&#27169;&#22411;&#65288;CGMM&#65289;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#29983;&#29289;&#23398;&#20851;&#31995;&#21644;&#32422;&#26463;&#26465;&#20214;&#26469;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#22522;&#22240;&#32452;&#26631;&#24535;&#30340;&#21512;&#25104;DNA&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.01475</link><description>&lt;p&gt;
&#22522;&#20110;&#32959;&#30244;&#21551;&#21457;&#30340;&#22522;&#22240;&#32452;&#27979;&#24207;&#26144;&#23556;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#22522;&#22240;&#32452;&#26631;&#24535;&#30340;&#21512;&#25104;DNA&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Cancer-inspired Genomics Mapper Model for the Generation of Synthetic DNA Sequences with Desired Genomics Signatures. (arXiv:2305.01475v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#36951;&#20256;&#31639;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32959;&#30244;&#21551;&#21457;&#30340;&#22522;&#22240;&#32452;&#27979;&#24207;&#26144;&#23556;&#27169;&#22411;&#65288;CGMM&#65289;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#29983;&#29289;&#23398;&#20851;&#31995;&#21644;&#32422;&#26463;&#26465;&#20214;&#26469;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#22522;&#22240;&#32452;&#26631;&#24535;&#30340;&#21512;&#25104;DNA&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#22240;&#32452;&#25968;&#25454;&#22312;&#29616;&#20195;&#21307;&#23398;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#35786;&#26029;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;&#24471;&#30410;&#20110;&#25216;&#26415;&#36827;&#27493;&#65292;&#24050;&#32463;&#23545;&#35768;&#22810;&#30334;&#19975;&#30340;&#20581;&#24247;&#21644;&#24739;&#26377;&#30142;&#30149;&#30340;&#22522;&#22240;&#32452;&#36827;&#34892;&#20102;&#27979;&#24207;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#20307;&#30740;&#31350;&#21644;&#29305;&#21035;&#26159;&#39564;&#35777;&#30740;&#31350;&#26469;&#35828;&#65292;&#33719;&#21462;&#26368;&#21512;&#36866;&#30340;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#35268;&#27169;&#21644;&#21487;&#35775;&#38382;&#24615;&#19978;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;&#22522;&#22240;&#32452;&#24207;&#21015;&#21457;&#29983;&#22120;&#20316;&#20026;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#29983;&#25104;&#22120;&#20351;&#29992;&#30340;&#25968;&#25454;&#22823;&#37117;&#36739;&#24046;&#65292;&#20027;&#35201;&#26159;&#20351;&#29992;&#27973;&#23618;&#65288;&#38543;&#26426;&#65289;&#36830;&#25509;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#29992;&#26377;&#38480;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#26816;&#27979;&#20986;&#26469;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#26368;&#21021;&#23548;&#33268;&#35266;&#23519;&#21040;&#30340;&#36830;&#25509;&#30340;&#21512;&#36866;&#30340;&#29983;&#29289;&#20851;&#31995;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32959;&#30244;&#21551;&#21457;&#30340;&#22522;&#22240;&#32452;&#26144;&#23556;&#27169;&#22411;&#65288;CGMM&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;CGMM&#27169;&#25311;&#20102;&#20135;&#29983;&#22522;&#22240;&#32452;&#24207;&#21015;&#30340;&#36807;&#31243;&#65292;&#20197;&#20415;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#22522;&#22240;&#32452;&#26631;&#24535;&#30340;&#21512;&#25104;DNA&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Genome data are crucial in modern medicine, offering significant potential for diagnosis and treatment. Thanks to technological advancements, many millions of healthy and diseased genomes have already been sequenced; however, obtaining the most suitable data for a specific study, and specifically for validation studies, remains challenging with respect to scale and access. Therefore, in silico genomics sequence generators have been proposed as a possible solution. However, the current generators produce inferior data using mostly shallow (stochastic) connections, detected with limited computational complexity in the training data. This means they do not take the appropriate biological relations and constraints, that originally caused the observed connections, into consideration. To address this issue, we propose cancer-inspired genomics mapper model (CGMM), that combines genetic algorithm (GA) and deep learning (DL) methods to tackle this challenge. CGMM mimics processes that generate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21442;&#25968;&#40065;&#26834;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#39640;&#25928;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#20559;&#23548;&#25968;&#30340;$k$&#20010;&#21442;&#25968;&#23376;&#38598;&#24182;&#24212;&#29992;&#20110;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#29366;&#24577;&#21644;&#25968;&#21315;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.01473</link><description>&lt;p&gt;
&#21442;&#25968;&#40065;&#26834;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#39640;&#25928;&#25935;&#24863;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Efficient Sensitivity Analysis for Parametric Robust Markov Chains. (arXiv:2305.01473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21442;&#25968;&#40065;&#26834;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#39640;&#25928;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#20559;&#23548;&#25968;&#30340;$k$&#20010;&#21442;&#25968;&#23376;&#38598;&#24182;&#24212;&#29992;&#20110;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#29366;&#24577;&#21644;&#25968;&#21315;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#20998;&#26512;&#21442;&#25968;&#40065;&#26834;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;&#21442;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#38598;&#21512;&#65292;&#20197;&#20943;&#36731;&#31934;&#30830;&#27010;&#29575;&#21487;&#29992;&#30340;&#19981;&#29616;&#23454;&#20551;&#35774;&#12290;&#25105;&#20204;&#26681;&#25454;&#19982;&#19981;&#30830;&#23450;&#36716;&#31227;&#27010;&#29575;&#30456;&#20851;&#30340;&#25514;&#26045;&#65288;&#22914;&#26399;&#26395;&#22870;&#21169;&#65289;&#30340;&#20559;&#23548;&#25968;&#26469;&#34913;&#37327;&#25935;&#24863;&#24615;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#36825;&#20123;&#20559;&#23548;&#25968;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#36866;&#29992;&#20110;&#20855;&#26377;&#25968;&#21315;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35813;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#20559;&#23548;&#25968;&#30340;$k$&#20010;&#21442;&#25968;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#32473;&#23450;&#21442;&#25968;&#20540;&#38468;&#36817;&#23545;&#36825;&#20123;&#31243;&#24207;&#36827;&#34892;&#24494;&#20998;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#36229;&#36807;&#19968;&#30334;&#19975;&#29366;&#24577;&#21644;&#25968;&#21315;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32467;&#26524;&#23884;&#20837;&#21040;&#36845;&#20195;&#23398;&#20064;&#26041;&#26696;&#20013;&#65292;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#26356;&#24555;&#22320;&#33719;&#21462;&#23545;&#21442;&#25968;&#30340;&#21709;&#24212;&#24182;&#29992;&#20110;&#25913;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a novel method for sensitivity analysis of parametric robust Markov chains. These models incorporate parameters and sets of probability distributions to alleviate the often unrealistic assumption that precise probabilities are available. We measure sensitivity in terms of partial derivatives with respect to the uncertain transition probabilities regarding measures such as the expected reward. As our main contribution, we present an efficient method to compute these partial derivatives. To scale our approach to models with thousands of parameters, we present an extension of this method that selects the subset of $k$ parameters with the highest partial derivative. Our methods are based on linear programming and differentiating these programs around a given value for the parameters. The experiments show the applicability of our approach on models with over a million states and thousands of parameters. Moreover, we embed the results within an iterative learning scheme that profi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#33410;&#28857;&#26631;&#31614;&#30456;&#21516;&#30340;&#33410;&#28857;&#20849;&#20139;&#30456;&#21516;&#30340;&#22870;&#21169;&#20998;&#24067;&#12290;&#23545;&#20110;&#32447;&#22270;&#21644;&#26641;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36951;&#25022;&#30028;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01470</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#22270;&#30340;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Contextual Bandits with Graph-based Contexts. (arXiv:2305.01470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#19978;&#19979;&#25991;&#30340;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#33410;&#28857;&#26631;&#31614;&#30456;&#21516;&#30340;&#33410;&#28857;&#20849;&#20139;&#30456;&#21516;&#30340;&#22870;&#21169;&#20998;&#24067;&#12290;&#23545;&#20110;&#32447;&#22270;&#21644;&#26641;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36951;&#25022;&#30028;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22312;&#32447;&#22270;&#39044;&#27979;&#38382;&#39064;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#20102;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#38382;&#39064;&#30340;&#19968;&#20010;&#29256;&#26412;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#26159;&#22270;&#20013;&#30340;&#33410;&#28857;&#65292;&#32780;&#22270;&#30340;&#32467;&#26500;&#25552;&#20379;&#20102;&#26377;&#20851;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32473;&#20986;&#19968;&#20010;&#22270;$G = (V&#65292;E)$&#65292;&#20854;&#33410;&#28857;&#38598;$V$&#34920;&#31034;&#20855;&#26377;&#26410;&#30693;&#39030;&#28857;&#26631;&#31614;$y$&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#25105;&#20204;&#30340;&#38543;&#26426;&#24773;&#22659;&#36172;&#21338;&#26426;&#35774;&#32622;&#20013;&#65292;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#39030;&#28857;&#20849;&#20139;&#21516;&#19968;&#22870;&#21169;&#20998;&#24067;&#12290;&#22312;&#22270;&#26631;&#31614;&#39044;&#27979;&#20013;&#65292;&#26631;&#20934;&#30340;&#23454;&#20363;&#38590;&#24230;&#27010;&#24565;&#26159;&#21106;&#22823;&#23567;$f$&#65292;&#21363;&#26377;&#19981;&#21516;&#26631;&#31614;&#32467;&#26463;&#28857;&#30340;&#36793;&#25968;&#12290;&#23545;&#20110;&#32447;&#22270;&#21644;&#26641;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36951;&#25022;&#30028;&#30340;&#31639;&#27861;$O(T^{2/3}K^{1/3}f^{1/3})$&#65292;&#20854;&#20013;$K$&#26159;&#25163;&#33218;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;Zimmert&#21644;Seldin~[AISTAT'19&#65292;JMLR'21]&#30340;&#26368;&#20248;&#38543;&#26426;&#36172;&#24466;&#31639;&#27861;&#12290;&#24403;&#26368;&#20339;&#25163;&#33218;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#25163;&#33218;&#26102;&#65292;&#36951;&#25022;&#30028;&#23558;&#25913;&#21892;&#20026;$\tilde{O}(\sqrt{KT\cdot f})$&#12290;
&lt;/p&gt;
&lt;p&gt;
We naturally generalize the on-line graph prediction problem to a version of stochastic contextual bandit problems where contexts are vertices in a graph and the structure of the graph provides information on the similarity of contexts. More specifically, we are given a graph $G=(V,E)$, whose vertex set $V$ represents contexts with {\em unknown} vertex label $y$. In our stochastic contextual bandit setting, vertices with the same label share the same reward distribution. The standard notion of instance difficulties in graph label prediction is the cutsize $f$ defined to be the number of edges whose end points having different labels. For line graphs and trees we present an algorithm with regret bound of $\tilde{O}(T^{2/3}K^{1/3}f^{1/3})$ where $K$ is the number of arms. Our algorithm relies on the optimal stochastic bandit algorithm by Zimmert and Seldin~[AISTAT'19, JMLR'21]. When the best arm outperforms the other arms, the regret improves to $\tilde{O}(\sqrt{KT\cdot f})$. The regret 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#35745;&#31639;&#38382;&#39064;&#12290;&#36890;&#36807;&#21457;&#29616;&#25968;&#20540;&#35780;&#20272;&#30340;&#19981;&#20934;&#30830;&#24615;&#20027;&#35201;&#28304;&#20110;&#25968;&#20540;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#30697;&#38453;MC&#30456;&#23545;&#20110;&#20013;&#31435;&#24615;&#30340;&#31283;&#20581;&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25968;&#20540;&#35780;&#20272;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01457</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#65306;&#25105;&#20204;&#35745;&#31639;&#24471;&#23545;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Memory of recurrent networks: Do we compute it right?. (arXiv:2305.01457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#35745;&#31639;&#38382;&#39064;&#12290;&#36890;&#36807;&#21457;&#29616;&#25968;&#20540;&#35780;&#20272;&#30340;&#19981;&#20934;&#30830;&#24615;&#20027;&#35201;&#28304;&#20110;&#25968;&#20540;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#30697;&#38453;MC&#30456;&#23545;&#20110;&#20013;&#31435;&#24615;&#30340;&#31283;&#20581;&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25968;&#20540;&#35780;&#20272;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#65288;MC&#65289;&#30340;&#25968;&#20540;&#35780;&#20272;&#24120;&#24120;&#19982;&#24050;&#32463;&#24314;&#31435;&#30340;&#29702;&#35770;&#30028;&#38480;&#30456;&#30683;&#30462;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#23545;&#24212;&#30340;Kalman&#21487;&#25511;&#30697;&#38453;&#30340;&#31209;&#24050;&#34987;&#35777;&#26126;&#31561;&#20110;&#24635;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20851;&#20110;&#35760;&#24518;&#19981;&#20934;&#30830;&#30340;&#25968;&#20540;&#35780;&#20272;&#30340;&#21508;&#31181;&#21407;&#22240;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#38382;&#39064;&#26159;&#32431;&#31929;&#25968;&#20540;&#26041;&#38754;&#19978;&#30340;&#65292;&#24448;&#24448;&#22312;&#36817;&#26399;&#25991;&#29486;&#20013;&#34987;&#24573;&#35270;&#12290;&#26356;&#26126;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#32447;&#24615;MC&#30340;Krylov&#32467;&#26500;&#34987;&#24573;&#30053;&#26102;&#65292;&#29702;&#35770;MC&#21644;&#23427;&#30340;&#32463;&#39564;&#20540;&#20043;&#38388;&#20250;&#23384;&#22312;&#24046;&#36317;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#65292;&#21033;&#29992;MC&#30456;&#23545;&#20110;&#36755;&#20837;&#25513;&#30721;&#30697;&#38453;&#30340;&#20013;&#31435;&#24615;&#65292;&#24320;&#21457;&#20986;&#31283;&#20581;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#35760;&#24518;&#26354;&#32447;&#19982;&#29702;&#35770;&#23436;&#20840;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerical evaluations of the memory capacity (MC) of recurrent neural networks reported in the literature often contradict well-established theoretical bounds. In this paper, we study the case of linear echo state networks, for which the total memory capacity has been proven to be equal to the rank of the corresponding Kalman controllability matrix. We shed light on various reasons for the inaccurate numerical estimations of the memory, and we show that these issues, often overlooked in the recent literature, are of an exclusively numerical nature. More explicitly, we prove that when the Krylov structure of the linear MC is ignored, a gap between the theoretical MC and its empirical counterpart is introduced. As a solution, we develop robust numerical approaches by exploiting a result of MC neutrality with respect to the input mask matrix. Simulations show that the memory curves that are recovered using the proposed methods fully agree with the theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#30123;&#33495;&#38144;&#21806;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#19981;&#21516;&#23618;&#27425;&#39044;&#27979;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#26368;&#23567;&#36857;&#21644;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#19982;&#32467;&#26500;&#32553;&#25918;&#30340;&#21327;&#35843;&#26041;&#27861;&#25928;&#26524;&#26368;&#20339;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#24182;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.01455</link><description>&lt;p&gt;
&#30123;&#33495;&#20379;&#24212;&#38142;&#20248;&#21270;&#30340;&#39044;&#27979;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Forecast reconciliation for vaccine supply chain optimization. (arXiv:2305.01455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#30123;&#33495;&#38144;&#21806;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#21327;&#35843;&#26041;&#27861;&#35299;&#20915;&#20102;&#19981;&#21516;&#23618;&#27425;&#39044;&#27979;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#26368;&#23567;&#36857;&#21644;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#19982;&#32467;&#26500;&#32553;&#25918;&#30340;&#21327;&#35843;&#26041;&#27861;&#25928;&#26524;&#26368;&#20339;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#24182;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#20379;&#24212;&#38142;&#20248;&#21270;&#36890;&#36807;&#25353;&#31867;&#22411;&#25110;&#22320;&#28857;&#20998;&#32452;&#30340;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26469;&#33719;&#30410;&#12290;&#28982;&#32780;&#65292;&#24403;&#39640;&#23618;&#27425;&#30340;&#39044;&#27979;&#32467;&#26524;&#19982;&#20302;&#23618;&#27425;&#39044;&#27979;&#32467;&#26524;&#20043;&#21644;&#19981;&#21305;&#37197;&#26102;&#65292;&#19981;&#21516;&#23618;&#27425;&#30340;&#39044;&#27979;&#21464;&#24471;&#19981;&#19968;&#33268;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#21327;&#35843;&#26041;&#27861;&#36827;&#34892;&#35299;&#20915;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;2010&#24180;&#33267;2021&#24180;GSK&#20844;&#21496;&#30340;&#38144;&#21806;&#25968;&#25454;&#24314;&#27169;&#20026;&#23618;&#27425;&#26102;&#38388;&#24207;&#21015;&#65292;&#35299;&#20915;&#20102;&#30123;&#33495;&#38144;&#21806;&#39044;&#27979;&#38382;&#39064;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;ARIMA&#27169;&#22411;&#39044;&#27979;&#26410;&#26469;&#20540;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#21508;&#31181;&#21327;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#26816;&#39564;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;COVID&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#26368;&#23567;&#36857;&#21644;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#19982;&#32467;&#26500;&#32553;&#25918;&#30340;&#21327;&#35843;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#39044;&#27979;&#24182;&#20943;&#23569;&#20102;&#22522;&#32447;ARIMA&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine supply chain optimization can benefit from hierarchical time series forecasting, when grouping the vaccines by type or location. However, forecasts of different hierarchy levels become incoherent when higher levels do not match the sum of the lower levels forecasts, which can be addressed by reconciliation methods.  In this paper, we tackle the vaccine sale forecasting problem by modeling sales data from GSK between 2010 and 2021 as a hierarchical time series. After forecasting future values with several ARIMA models, we systematically compare the performance of various reconciliation methods, using statistical tests. We also compare the performance of the forecast before and after COVID. The results highlight Minimum Trace and Weighted Least Squares with Structural scaling as the best performing methods, which provided a coherent forecast while reducing the forecast error of the baseline ARIMA.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;TSER&#31639;&#27861;&#65306;FreshPRINCE&#21644;DrCIF&#65292;&#23427;&#20204;&#20998;&#21035;&#30001;&#19968;&#32452;&#27719;&#24635;&#29305;&#24449;&#21644;&#22810;&#20010;&#26465;&#20214;&#25512;&#29702;&#26641;&#26500;&#25104;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;&#26102;&#38388;&#24207;&#21015;&#22806;&#28304;&#22238;&#24402;&#30340;&#38382;&#39064;&#19978;&#39044;&#27979;&#21709;&#24212;&#21464;&#37327;&#65292;&#27604;&#36215;&#20197;&#21069;&#30340;&#35780;&#20272;&#20013;&#20351;&#29992;&#30340;&#22522;&#32447;&#31639;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.01429</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22806;&#28304;&#22238;&#24402;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Feature Based Algorithms for Time Series Extrinsic Regression. (arXiv:2305.01429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;TSER&#31639;&#27861;&#65306;FreshPRINCE&#21644;DrCIF&#65292;&#23427;&#20204;&#20998;&#21035;&#30001;&#19968;&#32452;&#27719;&#24635;&#29305;&#24449;&#21644;&#22810;&#20010;&#26465;&#20214;&#25512;&#29702;&#26641;&#26500;&#25104;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;&#26102;&#38388;&#24207;&#21015;&#22806;&#28304;&#22238;&#24402;&#30340;&#38382;&#39064;&#19978;&#39044;&#27979;&#21709;&#24212;&#21464;&#37327;&#65292;&#27604;&#36215;&#20197;&#21069;&#30340;&#35780;&#20272;&#20013;&#20351;&#29992;&#30340;&#22522;&#32447;&#31639;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22806;&#28304;&#22238;&#24402;&#65288;TSER&#65289;&#28041;&#21450;&#20351;&#29992;&#19968;&#32452;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#26469;&#24418;&#25104;&#19968;&#20010;&#36830;&#32493;&#21709;&#24212;&#21464;&#37327;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#21464;&#37327;&#19982;&#22238;&#24402;&#22120;&#24207;&#21015;&#27809;&#26377;&#30452;&#25509;&#20851;&#31995;&#12290;TSER&#23384;&#26723;&#29992;&#20110;&#27604;&#36739;&#31639;&#27861;&#20110;2022&#24180;&#21457;&#24067;&#65292;&#21253;&#25324;19&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#27492;&#23384;&#26723;&#30340;&#22823;&#23567;&#22686;&#21152;&#21040;63&#20010;&#38382;&#39064;&#65292;&#24182;&#37325;&#29616;&#20197;&#21069;&#31639;&#27861;&#30340;&#22522;&#20934;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#27604;&#36739;&#65292;&#21253;&#25324;&#26356;&#24191;&#27867;&#30340;&#26631;&#20934;&#22238;&#24402;&#22120;&#21644;&#20197;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#26368;&#26032;&#29256;&#26412;&#30340;TSER&#27169;&#22411;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20197;&#21069;&#35780;&#20272;&#30340;&#22238;&#24402;&#22120;&#37117;&#19981;&#33021;&#32988;&#36807;&#26631;&#20934;&#20998;&#31867;&#22120;&#26059;&#36716;&#26862;&#26519;&#30340;&#22238;&#24402;&#36866;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;TSER&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#26159;&#20174;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#24320;&#21457;&#32780;&#26469;&#12290;FreshPRINCE&#26159;&#19968;&#20010;&#31649;&#36947;&#20272;&#35745;&#22120;&#65292;&#21253;&#25324;&#36716;&#25442;&#21040;&#21508;&#31181;&#27719;&#24635;&#29305;&#24449;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#26059;&#36716;&#26862;&#26519;&#22238;&#24402;&#22120;&#12290;DrCIF&#26159;&#19968;&#20010;&#26641;&#38598;&#21512;&#65292;&#23427;&#26681;&#25454;&#26102;&#38388;&#24207;&#21015;&#30340;&#38543;&#26426;&#22686;&#37327;&#21019;&#24314;&#27719;&#24635;&#32479;&#35745;&#20449;&#24687;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#22810;&#20010;&#26465;&#20214;&#25512;&#29702;&#26641;&#26469;&#39044;&#27979;&#21709;&#24212;&#21464;&#37327;&#12290;&#22312;&#25193;&#23637;&#30340;TSER&#38382;&#39064;&#38598;&#19978;&#65292;FreshPRINCE&#21644;DrCIF&#37117;&#22987;&#32456;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time Series Extrinsic Regression (TSER) involves using a set of training time series to form a predictive model of a continuous response variable that is not directly related to the regressor series. The TSER archive for comparing algorithms was released in 2022 with 19 problems. We increase the size of this archive to 63 problems and reproduce the previous comparison of baseline algorithms. We then extend the comparison to include a wider range of standard regressors and the latest versions of TSER models used in the previous study. We show that none of the previously evaluated regressors can outperform a regression adaptation of a standard classifier, rotation forest. We introduce two new TSER algorithms developed from related work in time series classification. FreshPRINCE is a pipeline estimator consisting of a transform into a wide range of summary features followed by a rotation forest regressor. DrCIF is a tree ensemble that creates features from summary statistics over random i
&lt;/p&gt;</description></item><item><title>&#25688;&#35201;&#20171;&#32461;&#20102;&#22914;&#20309;&#24212;&#23545;&#38750;&#27954;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#22411;&#20316;&#20026;&#20135;&#21697;&#25945;&#23398;&#30340;&#25945;&#23398;&#24037;&#20855;&#30340;&#24819;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#23547;&#27714;&#25913;&#21892;&#38750;&#27954;&#26412;&#22303;&#26041;&#35328;&#23458;&#25143;&#20307;&#39564;&#21644;&#20135;&#21697;&#24320;&#21457;&#30340;&#20225;&#19994;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01427</link><description>&lt;p&gt;
&#20174;&#26412;&#22320;&#21040;&#20840;&#29699;&#65306;&#24212;&#23545;&#38750;&#27954;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
From Local to Global: Navigating Linguistic Diversity in the African Context. (arXiv:2305.01427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01427
&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#20171;&#32461;&#20102;&#22914;&#20309;&#24212;&#23545;&#38750;&#27954;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#22411;&#20316;&#20026;&#20135;&#21697;&#25945;&#23398;&#30340;&#25945;&#23398;&#24037;&#20855;&#30340;&#24819;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#20110;&#23547;&#27714;&#25913;&#21892;&#38750;&#27954;&#26412;&#22303;&#26041;&#35328;&#23458;&#25143;&#20307;&#39564;&#21644;&#20135;&#21697;&#24320;&#21457;&#30340;&#20225;&#19994;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#23384;&#22312;&#30340;&#26377;&#20851;&#38750;&#27954;&#22823;&#38470;&#19978;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#38750;&#27954;&#30340;&#22320;&#26041;&#26041;&#35328;&#21644;&#40092;&#20026;&#20154;&#30693;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#23545;&#20110;&#23547;&#27714;&#25913;&#36827;&#38750;&#27954;&#26412;&#22303;&#26041;&#35328;&#30340;&#23458;&#25143;&#20307;&#39564;&#21644;&#20135;&#21697;&#24320;&#21457;&#30340;&#20225;&#19994;&#21487;&#33021;&#20135;&#29983;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#20351;&#29992;&#35813;&#27169;&#22411;&#20316;&#20026;&#20135;&#21697;&#25945;&#23398;&#30340;&#25945;&#23398;&#24037;&#20855;&#30340;&#24819;&#27861;&#20063;&#24456;&#26377;&#36259;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#20250;&#28608;&#21457;&#23398;&#20064;&#32773;&#30340;&#20852;&#36259;&#24182;&#24341;&#21457;&#31185;&#25216;&#21019;&#19994;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#25913;&#36827;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#24212;&#23545;&#38750;&#27954;&#26412;&#22303;&#26041;&#35328;&#25361;&#25112;&#30340;&#26377;&#21069;&#36884;&#30340;&#20998;&#26512;&#12290;&#29305;&#21035;&#26159;&#40092;&#20026;&#20154;&#30693;&#30340;&#38463;&#25289;&#20271;&#26041;&#35328;&#65292;&#36825;&#23545;&#20110;&#23547;&#27714;&#25913;&#36827;&#23458;&#25143;&#20307;&#39564;&#21644;&#20135;&#21697;&#24320;&#21457;&#30340;&#20225;&#19994;&#21487;&#33021;&#20250;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The focus is on critical problems in NLP related to linguistic diversity and variation across the African continent, specifically with regards to African local di- alects and Arabic dialects that have received little attention. We evaluated our various approaches, demonstrating their effectiveness while highlighting the potential impact of the proposed approach on businesses seeking to improve customer experience and product development in African local dialects. The idea of using the model as a teaching tool for product-based instruction is interesting, as it could potentially stimulate interest in learners and trigger techno entrepreneurship. Overall, our modified approach offers a promising analysis of the challenges of dealing with African local dialects. Particularly Arabic dialects, which could have a significant impact on businesses seeking to improve customer experience and product development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;Mercer&#26680;&#30340;&#32477;&#23545;&#21487;&#31215;&#24615;&#20165;&#20165;&#26159;RKHS&#31283;&#23450;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.01411</link><description>&lt;p&gt;
Mercer&#26680;&#30340;&#32477;&#23545;&#21487;&#31215;&#24615;&#20165;&#20165;&#26159;RKHS&#31283;&#23450;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Absolute integrability of Mercer kernels is only sufficient for RKHS stability. (arXiv:2305.01411v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Mercer&#26680;&#30340;&#32477;&#23545;&#21487;&#31215;&#24615;&#20165;&#20165;&#26159;RKHS&#31283;&#23450;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#26159;&#19968;&#31181;&#19982;&#31216;&#20026;&#26680;&#30340;&#27491;&#23450;&#26144;&#23556;&#19968;&#19968;&#23545;&#24212;&#30340;&#29305;&#27530;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#12290;&#23427;&#20204;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#29992;&#20110;&#20174;&#31232;&#30095;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#37325;&#24314;&#26410;&#30693;&#30340;&#20989;&#25968;&#12290;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#22312;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#30340;&#32972;&#26223;&#19979;&#20063;&#24341;&#20837;&#20102;&#31283;&#23450;RKHS&#30340;&#23376;&#31867;&#12290;&#31283;&#23450;RKHS&#20165;&#21253;&#21547;&#27491;&#23454;&#25968;&#32447;&#19978;&#30340;&#32477;&#23545;&#21487;&#31215;&#33033;&#20914;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#20197;&#20316;&#20026;&#20551;&#35774;&#31354;&#38388;&#29992;&#20110;&#20174;&#36755;&#20837;&#36755;&#20986;&#25968;&#25454;&#20013;&#20272;&#35745;&#32447;&#24615;&#12289;&#26102;&#19981;&#21464;&#21644;BIBO&#31283;&#23450;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;RKHS&#31283;&#23450;&#24615;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#24182;&#19988;&#24050;&#30693;&#26680;&#30340;&#32477;&#23545;&#21487;&#31215;&#24615;&#24847;&#21619;&#30528;&#31283;&#23450;&#24615;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#31163;&#25955;&#26102;&#38388;&#20013;&#24037;&#20316;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#26465;&#20214;&#21482;&#26159;&#20805;&#20998;&#26465;&#20214;&#12290;&#22312;&#36830;&#32493;&#26102;&#38388;&#20013;&#24037;&#20316;&#65292;&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#21516;&#26679;&#30340;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;Mercer&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducing kernel Hilbert spaces (RKHSs) are special Hilbert spaces in one-to-one correspondence with positive definite maps called kernels. They are widely employed in machine learning to reconstruct unknown functions from sparse and noisy data. In the last two decades, a subclass known as stable RKHSs has been also introduced in the setting of linear system identification. Stable RKHSs contain only absolutely integrable impulse responses over the positive real line. Hence, they can be adopted as hypothesis spaces to estimate linear, time-invariant and BIBO stable dynamic systems from input-output data. Necessary and sufficient conditions for RKHS stability are available in the literature and it is known that kernel absolute integrability implies stability. Working in discrete-time, in a recent work we have proved that this latter condition is only sufficient. Working in continuous-time, it is the purpose of this note to prove that the same result holds also for Mercer kernels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#21183;&#33021;&#20013;&#30340;&#24212;&#21147;&#21644;&#28909;&#36890;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01401</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#24212;&#21147;&#21644;&#28909;&#36890;&#37327;
&lt;/p&gt;
&lt;p&gt;
Stress and heat flux via automatic differentiation. (arXiv:2305.01401v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#33258;&#21160;&#24494;&#20998;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#21183;&#33021;&#20013;&#30340;&#24212;&#21147;&#21644;&#28909;&#36890;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21183;&#33021;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#36817;&#20284;&#25551;&#36848;BOPES&#65288;Born-Oppenheimer&#21183;&#33021;&#38754;&#65289;&#65292;&#35813;&#21183;&#33021;&#20915;&#23450;&#20102;&#35768;&#22810;&#26448;&#26009;&#29305;&#24615;&#12290;&#27169;&#25311;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#26799;&#24230;&#65292;&#23588;&#20854;&#26159;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#30340;&#21147;&#21644;&#24212;&#21147;&#20197;&#21450;&#28909;&#23398;&#36755;&#36816;&#20013;&#30340;&#28909;&#36890;&#37327;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;&#21183;&#33021;&#29305;&#24449;&#20855;&#26377;&#39640;&#30340;&#30456;&#20114;&#20316;&#29992;&#33021;&#37327;&#21644;&#21253;&#25324;&#36890;&#36807;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#31561;&#21464;&#21322;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#22797;&#26434;&#21151;&#33021;&#24418;&#24335;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25163;&#21160;&#23454;&#29616;&#25110;&#26377;&#38480;&#24046;&#20998;&#26041;&#26696;&#26469;&#35745;&#31639;&#26799;&#24230;&#30340;&#38656;&#35201;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;AD&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#36825;&#31181;&#21183;&#33021;&#30340;&#21147;&#12289;&#24212;&#21147;&#21644;&#28909;&#36890;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;Lennard-Jones&#21183;&#33021;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#20351;&#29992;&#31561;&#21464;mes&#27169;&#22411;&#39044;&#27979;&#38177;&#30802;&#21270;&#29289;&#30340;&#20869;&#32858;&#29305;&#24615;&#21644;&#28909;&#23548;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning potentials provide computationally efficient and accurate approximations of the Born-Oppenheimer potential energy surface. This potential determines many materials properties and simulation techniques usually require its gradients, in particular forces and stress for molecular dynamics, and heat flux for thermal transport properties. Recently developed potentials feature high body order and can include equivariant semi-local interactions through message-passing mechanisms. Due to their complex functional forms, they rely on automatic differentiation (AD), overcoming the need for manual implementations or finite-difference schemes to evaluate gradients. This study demonstrates a unified AD approach to obtain forces, stress, and heat flux for such potentials, and provides a model-independent implementation. The method is tested on the Lennard-Jones potential, and then applied to predict cohesive properties and thermal conductivity of tin selenide using an equivariant mes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;POIR&#23558;&#34892;&#20026;&#20811;&#38534;&#21644;&#35268;&#21010;&#22120;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#27169;&#20223;&#23398;&#20064;&#20013;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01400</link><description>&lt;p&gt;
&#36820;&#22238;&#20998;&#24067;&#35268;&#21010;&#65306;&#40065;&#26834;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Get Back Here: Robust Imitation by Return-to-Distribution Planning. (arXiv:2305.01400v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;POIR&#23558;&#34892;&#20026;&#20811;&#38534;&#21644;&#35268;&#21010;&#22120;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#27169;&#20223;&#23398;&#20064;&#20013;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#27169;&#20223;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#21363;&#19987;&#23478;&#25968;&#25454;&#19981;&#26159;&#22312;&#23454;&#38469;&#37096;&#32626;&#29615;&#22659;&#19979;&#25910;&#38598;&#30340;&#65292;&#32780;&#26159;&#22312;&#21478;&#19968;&#20010;&#29256;&#26412;&#19978;&#25910;&#38598;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#27492;&#23548;&#33268;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#19982;&#19968;&#20010;&#35268;&#21010;&#22120;&#30456;&#32467;&#21512;&#65292;&#35268;&#21010;&#22120;&#30340;&#20219;&#21153;&#26159;&#22312;&#20195;&#29702;&#31243;&#24207;&#20559;&#31163;&#28436;&#31034;&#20998;&#24067;&#26102;&#65292;&#23558;&#20195;&#29702;&#31243;&#24207;&#24102;&#22238;&#21040;&#19987;&#23478;&#35775;&#38382;&#30340;&#29366;&#24577;&#12290;&#24471;&#21040;&#30340;&#31639;&#27861;POIR&#21487;&#20197;&#31163;&#32447;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;&#22312;&#32447;&#20132;&#20114;&#26469;&#26377;&#25928;&#22320;&#20248;&#21270;&#20854;&#35268;&#21010;&#22120;&#65292;&#20197;&#36880;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#27169;&#25311;&#22120;&#20013;&#23545;POIR&#36827;&#34892;&#27979;&#35797;&#65292;&#20351;&#29992;&#21508;&#31181;&#20154;&#31867;&#29983;&#25104;&#30340;&#25805;&#20316;&#28436;&#31034;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#31574;&#30053;&#23545;&#19981;&#21516;&#21021;&#22987;&#29366;&#24577;&#20998;&#24067;&#21644;&#22024;&#26434;&#21160;&#24577;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the Imitation Learning (IL) setup where expert data are not collected on the actual deployment environment but on a different version. To address the resulting distribution shift, we combine behavior cloning (BC) with a planner that is tasked to bring the agent back to states visited by the expert whenever the agent deviates from the demonstration distribution. The resulting algorithm, POIR, can be trained offline, and leverages online interactions to efficiently fine-tune its planner to improve performance over time. We test POIR on a variety of human-generated manipulation demonstrations in a realistic robotic manipulation simulator and show robustness of the learned policy to different initial state distributions and noisy dynamics.
&lt;/p&gt;</description></item><item><title>&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.01397</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#19981;&#21464;&#27169;&#22411;&#21644;&#34920;&#31034;&#26159;&#21542;&#20844;&#24179;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are demographically invariant models and representations in medical imaging fair?. (arXiv:2305.01397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01397
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#21307;&#23398;&#25104;&#20687;&#27169;&#22411;&#22312;&#20854;&#28508;&#22312;&#34920;&#31034;&#20013;&#32534;&#30721;&#20102;&#26377;&#20851;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#20449;&#24687;&#65288;&#24180;&#40836;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#65292;&#36825;&#24341;&#21457;&#20102;&#26377;&#20851;&#20854;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#21487;&#34892;&#21644;&#20540;&#24471;&#35757;&#32451;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#19981;&#21516;&#31867;&#22411;&#30340;&#19982;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#30340;&#19981;&#21464;&#24615;&#65292;&#21363;&#36793;&#38469;&#12289;&#31867;&#26465;&#20214;&#21644;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#24182;&#35828;&#26126;&#23427;&#20204;&#19982;&#31639;&#27861;&#20844;&#24179;&#30340;&#26631;&#20934;&#27010;&#24565;&#30340;&#31561;&#20215;&#24615;&#12290;&#26681;&#25454;&#29616;&#26377;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#38469;&#21644;&#31867;&#26465;&#20214;&#30340;&#19981;&#21464;&#24615;&#21487;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#26576;&#20123;&#20844;&#24179;&#27010;&#24565;&#30340;&#36807;&#24230;&#38480;&#21046;&#26041;&#27861;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#25439;&#22833;&#12290;&#20851;&#20110;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#65292;&#23450;&#20041;&#21307;&#23398;&#22270;&#20687;&#21453;&#20107;&#23454;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#29978;&#33267;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging models have been shown to encode information about patient demographics (age, race, sex) in their latent representation, raising concerns about their potential for discrimination. Here, we ask whether it is feasible and desirable to train models that do not encode demographic attributes. We consider different types of invariance with respect to demographic attributes marginal, class-conditional, and counterfactual model invariance - and lay out their equivalence to standard notions of algorithmic fairness. Drawing on existing theory, we find that marginal and class-conditional invariance can be considered overly restrictive approaches for achieving certain fairness notions, resulting in significant predictive performance losses. Concerning counterfactual model invariance, we note that defining medical image counterfactuals with respect to demographic attributes is fraught with complexities. Finally, we posit that demographic encoding may even be considered advantageou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;Fed-LTP&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24425;&#31080;&#21367;&#31215;&#31232;&#30095;&#32593;&#32476;&#21644;&#38646;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#20197;&#21450;&#36164;&#28304;&#32422;&#26463;&#30340;&#31227;&#21160;&#32456;&#31471;&#21327;&#20316;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.01387</link><description>&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;&#39046;&#22495;&#23454;&#29616;&#22686;&#24378;&#38544;&#31169;&#24615;&#30340;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Federated Learning with Enhanced Privacy via Lottery Ticket Pruning in Edge Computing. (arXiv:2305.01387v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;Fed-LTP&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24425;&#31080;&#21367;&#31215;&#31232;&#30095;&#32593;&#32476;&#21644;&#38646;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#20197;&#21450;&#36164;&#28304;&#32422;&#26463;&#30340;&#31227;&#21160;&#32456;&#31471;&#21327;&#20316;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#20998;&#25955;&#30340;&#31227;&#21160;&#32456;&#31471;&#65288;MT&#65289;&#20013;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#36890;&#20449;&#12289;MT&#36164;&#28304;&#21644;&#38544;&#31169;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;FL&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#23454;&#20363;&#32423;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#20294;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#12289;&#20256;&#36755;&#24320;&#38144;&#21644;&#36793;&#32536;&#35774;&#22791;&#65288;&#22914;MT&#65289;&#30340;&#36164;&#28304;&#32422;&#26463;&#31561;&#20960;&#20010;&#29942;&#39048;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fed-LTP&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#22686;&#24378;&#38544;&#31169;&#30340;FL&#26694;&#26550;&#65292;&#37319;&#29992;&#24425;&#31080;&#21367;&#31215;&#31232;&#30095;&#32593;&#32476;&#21644;&#38646;&#38598;&#20013;DP&#65288;zCDP&#65289;&#31639;&#27861;&#12290;&#23427;&#22312;&#26381;&#21153;&#22120;&#19978;&#29983;&#25104;&#20462;&#21098;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#23458;&#25143;&#31471;&#19978;&#20351;&#29992;zCDP&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#12290;&#26381;&#21153;&#22120;&#31471;&#25552;&#20986;&#20102;&#20004;&#31181;&#20462;&#21098;&#26041;&#26696;&#65306;&#65288;i&#65289;&#22522;&#20110;&#26435;&#37325;&#30340;&#20462;&#21098;&#65288;LTH&#65289;&#20915;&#23450;&#20462;&#21098;&#20840;&#23616;&#27169;&#22411;&#25152;&#38656;&#30340;&#38750;&#38646;&#27169;&#22411;&#20803;&#32032;&#25968;&#37327;&#21644;&#20301;&#32622;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#27169;&#24335;&#30340;&#20462;&#21098;&#65288;PTH&#65289;&#20462;&#21098;&#20840;&#23616;&#27169;&#22411;,&#20351;&#20854;&#19982;MT&#30340;&#27169;&#24335;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a collaborative learning paradigm for decentralized private data from mobile terminals (MTs). However, it suffers from issues in terms of communication, resource of MTs, and privacy. Existing privacy-preserving FL methods usually adopt the instance-level differential privacy (DP), which provides a rigorous privacy guarantee but with several bottlenecks: severe performance degradation, transmission overhead, and resource constraints of edge devices such as MTs. To overcome these drawbacks, we propose Fed-LTP, an efficient and privacy-enhanced FL framework with \underline{\textbf{L}}ottery \underline{\textbf{T}}icket \underline{\textbf{H}}ypothesis (LTH) and zero-concentrated D\underline{\textbf{P}} (zCDP). It generates a pruned global model on the server side and conducts sparse-to-sparse training from scratch with zCDP on the client side. On the server side, two pruning schemes are proposed: (i) the weight-based pruning (LTH) determines the pruned global mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#21035;&#20449;&#24687;&#30340;&#24433;&#21709;&#20989;&#25968;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01384</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#30340;&#24433;&#21709;&#20989;&#25968;&#29992;&#20110;&#35823;&#24046;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Class based Influence Functions for Error Detection. (arXiv:2305.01384v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01384
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31867;&#21035;&#20449;&#24687;&#30340;&#24433;&#21709;&#20989;&#25968;&#26469;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#20989;&#25968;(IFs)&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#24322;&#24120;&#26679;&#26412;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#28145;&#24230;&#32593;&#32476;&#26102;&#65292;&#23427;&#20204;&#19981;&#31283;&#23450;&#12290;&#26412;&#25991;&#35299;&#37322;&#20102;IFs&#19981;&#31283;&#23450;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#28857;&#23646;&#20110;&#20004;&#20010;&#19981;&#21516;&#31867;&#21035;&#26102;&#65292;IFs&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#27861;&#21033;&#29992;&#31867;&#21035;&#20449;&#24687;&#26469;&#25913;&#36827;IFs&#30340;&#31283;&#23450;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#25913;&#26174;&#33879;&#25552;&#39640;&#20102;IFs&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#19981;&#22686;&#21152;&#20219;&#20309;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence functions (IFs) are a powerful tool for detecting anomalous examples in large scale datasets. However, they are unstable when applied to deep networks. In this paper, we provide an explanation for the instability of IFs and develop a solution to this problem. We show that IFs are unreliable when the two data points belong to two different classes. Our solution leverages class information to improve the stability of IFs. Extensive experiments show that our modification significantly improves the performance and stability of IFs while incurring no additional computational cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.01381</link><description>&lt;p&gt;
&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26679;&#26412;&#26377;&#25928;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#20248;&#21270;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LTL&#35268;&#33539;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#26377;&#25928;&#22320;&#23398;&#20064;&#24182;&#20248;&#21270;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#24191;&#27867;&#29992;&#20110;&#25351;&#23450;&#31995;&#32479;&#31574;&#30053;&#30340;&#39640;&#32423;&#30446;&#26631;&#65292;&#33258;&#20027;&#31995;&#32479;&#23398;&#20064;&#30456;&#23545;&#20110;&#36825;&#26679;&#30340;&#35268;&#33539;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#12290; &#20294;&#26159;&#65292;&#20174;LTL&#35268;&#33539;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#24182;&#19981;&#36731;&#26494;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#26410;&#30693;&#38543;&#26426;&#31995;&#32479;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#26356;&#36890;&#29992;&#30340;&#20056;&#31215;MDP&#12289;&#22870;&#21169;&#32467;&#26500;&#21644;&#25240;&#25187;&#26426;&#21046;&#65292;&#24403;&#19982;&#29616;&#25104;&#30340;&#26080;&#27169;&#22411;RL&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#26368;&#22823;&#21270;&#32473;&#23450;LTL&#35268;&#33539;&#28385;&#36275;&#27010;&#29575;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#26377;&#20851;&#36873;&#25321;RL&#20013;&#20851;&#38190;&#21442;&#25968;&#20197;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#20026;&#20102;&#30452;&#25509;&#35780;&#20272;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#37319;&#29992;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#22120;PRISM&#26469;&#35745;&#31639;LTL&#35268;&#33539;&#30340;&#28385;&#36275;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#23398;&#20064;&#27169;&#22411;LogSpecT&#21450;&#20854;&#23454;&#38469;&#20844;&#24335;rLogSpecT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;rSpecT&#25935;&#24863;&#36229;&#21442;&#25968;&#36873;&#25321;&#12289;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;rLogSpecT&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;L-ADMM&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01379</link><description>&lt;p&gt;
LogSpecT: &#20174;&#24179;&#31283;&#20449;&#21495;&#20013;&#23398;&#20064;&#21487;&#34892;&#30340;&#22270;&#24418;&#23398;&#20064;&#27169;&#22411;&#24182;&#20855;&#22791;&#24674;&#22797;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
LogSpecT: Feasible Graph Learning Model from Stationary Signals with Recovery Guarantees. (arXiv:2305.01379v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#23398;&#20064;&#27169;&#22411;LogSpecT&#21450;&#20854;&#23454;&#38469;&#20844;&#24335;rLogSpecT&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;rSpecT&#25935;&#24863;&#36229;&#21442;&#25968;&#36873;&#25321;&#12289;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;rLogSpecT&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;L-ADMM&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#21495;&#22270;&#24418;&#23398;&#20064;&#26159;&#22270;&#24418;&#20449;&#21495;&#22788;&#29702;&#65288;GSP&#65289;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#23398;&#20064;&#24179;&#31283;&#20449;&#21495;&#22270;&#24418;&#26368;&#24120;&#29992;&#30340;&#27169;&#22411;&#20043;&#19968;&#26159;SpecT&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#23454;&#38469;&#20844;&#24335;rSpecT&#34987;&#35748;&#20026;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#25935;&#24863;&#65292;&#24182;&#19988;&#26356;&#31967;&#30340;&#26159;&#65292;&#23481;&#26131;&#26080;&#27861;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#32473;&#20986;&#20445;&#35777;rSpecT&#26080;&#27861;&#23454;&#29616;&#30340;&#26465;&#20214;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65288;LogSpecT&#65289;&#21450;&#20854;&#23454;&#38469;&#20844;&#24335;&#65288;rLogSpecT&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;rSpecT&#19981;&#21516;&#65292;&#26032;&#30340;&#23454;&#29992;&#27169;&#22411;rLogSpecT&#22987;&#32456;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;rLogSpecT&#30340;&#24674;&#22797;&#20445;&#35777;&#65292;&#36825;&#20123;&#20445;&#35777;&#26469;&#33258;&#20110;&#19982;epi-converg&#8203;&#8203;ence&#30456;&#20851;&#30340;&#29616;&#20195;&#20248;&#21270;&#24037;&#20855;&#12290;&#36825;&#20123;&#24037;&#20855;&#23545;&#20110;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#37117;&#20855;&#26377;&#29420;&#31435;&#30340;&#21033;&#30410;&#21644;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;rLogSpecT&#22312;&#23454;&#36341;&#20013;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#21270;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#26041;&#27861;&#65288;L-ADMM&#65289;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;L-ADMM&#30340;&#23376;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph learning from signals is a core task in Graph Signal Processing (GSP). One of the most commonly used models to learn graphs from stationary signals is SpecT. However, its practical formulation rSpecT is known to be sensitive to hyperparameter selection and, even worse, to suffer from infeasibility. In this paper, we give the first condition that guarantees the infeasibility of rSpecT and design a novel model (LogSpecT) and its practical formulation (rLogSpecT) to overcome this issue. Contrary to rSpecT, the novel practical model rLogSpecT is always feasible. Furthermore, we provide recovery guarantees of rLogSpecT, which are derived from modern optimization tools related to epi-convergence. These tools could be of independent interest and significant for various learning problems. To demonstrate the advantages of rLogSpecT in practice, a highly efficient algorithm based on the linearized alternating direction method of multipliers (L-ADMM) is proposed. The subproblems of L-ADMM a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;(RFD)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#35745;&#31639;&#20986;&#27493;&#38271;&#24182;&#19988;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21516;&#12290;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RFD&#31639;&#27861;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#20986;&#30340;heuristic&#25193;&#23637;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.01377</link><description>&lt;p&gt;
&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;&#27861;
&lt;/p&gt;
&lt;p&gt;
Random Function Descent. (arXiv:2305.01377v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;(RFD)&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#35745;&#31639;&#20986;&#27493;&#38271;&#24182;&#19988;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21516;&#12290;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RFD&#31639;&#27861;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25552;&#20986;&#30340;heuristic&#25193;&#23637;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21313;&#20998;&#24120;&#35265;&#65292;&#20294;&#26159;&#36873;&#25321;&#27491;&#30830;&#30340;&#27493;&#38271;&#32463;&#24120;&#38656;&#35201;&#36827;&#34892;&#8220;&#36229;&#21442;&#25968;&#35843;&#25972;&#8221;&#12290;&#36825;&#26159;&#22240;&#20026;&#22238;&#28335;&#31243;&#24207;&#22914;Armijo's&#20934;&#21017;&#20381;&#36182;&#20110;&#27599;&#20010;&#27493;&#39588;&#20013;&#30340;&#36136;&#37327;&#35780;&#20272;&#65292;&#32780;&#36825;&#20123;&#35780;&#20272;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#19981;&#21487;&#29992;&#12290;&#30001;&#20110;&#20248;&#21270;&#26041;&#26696;&#21487;&#20197;&#29992;Taylor&#36924;&#36817;&#26469;&#35299;&#37322;&#65292;&#25105;&#20204;&#23558;Taylor&#36924;&#36817;&#26367;&#25442;&#20026;&#26465;&#20214;&#26399;&#26395;&#65288;&#26368;&#20339;&#30340;$L^2$&#20272;&#35745;&#65289;&#65292;&#25552;&#20986;&#20102;&#8220;&#38543;&#26426;&#20989;&#25968;&#19979;&#38477;&#8221;&#65288;RFD&#65289;&#12290; &#22312;Bayesian&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#19968;&#20123;&#36731;&#24494;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RFD&#19982;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26159;&#30456;&#21516;&#30340;&#65292;&#20294;&#26159;&#22312;&#38543;&#26426;&#24773;&#20917;&#19979;&#20855;&#26377;&#21487;&#35745;&#31639;&#30340;&#27493;&#38271;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#26410;&#35843;&#25972;&#30340;Adam&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#31639;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#25193;&#23637;&#65292;&#21487;&#19982;&#35843;&#25972;&#21518;&#30340;Adam&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
While gradient based methods are ubiquitous in machine learning, selecting the right step size often requires "hyperparameter tuning". This is because backtracking procedures like Armijo's rule depend on quality evaluations in every step, which are not available in a stochastic context. Since optimization schemes can be motivated using Taylor approximations, we replace the Taylor approximation with the conditional expectation (the best $L^2$ estimator) and propose "Random Function Descent" (RFD). Under light assumptions common in Bayesian optimization, we prove that RFD is identical to gradient descent, but with calculable step sizes, even in a stochastic context. We beat untuned Adam in synthetic benchmarks. To close the performance gap to tuned Adam, we propose a heuristic extension competitive with tuned Adam.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36755;&#20986;&#35823;&#24046;Hamiltonian&#31070;&#32463;&#32593;&#32476;&#65288;OE-HNN&#65289;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20855;&#26377;&#36755;&#20837;&#21644;&#22122;&#22768;&#29366;&#24577;&#27979;&#37327;&#30340;&#29289;&#29702;&#31995;&#32479;&#24314;&#27169;&#38382;&#39064;&#12290;OE-HNN&#36890;&#36807;&#20351;&#29992;&#23884;&#20837;&#24335;ODE&#27714;&#35299;&#22120;&#20174;&#24102;&#22122;&#22768;&#30340;&#29366;&#24577;&#27979;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#19977;&#20010;&#29289;&#29702;&#31995;&#32479;&#19978;&#24471;&#21040;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01338</link><description>&lt;p&gt;
&#24102;&#36755;&#20986;&#35823;&#24046;&#22122;&#22768;&#27169;&#22411;&#30340;Hamiltonian&#31070;&#32463;&#32593;&#32476;&#29289;&#29702;&#23398;&#30693;&#35782;&#39537;&#21160;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Learning Using Hamiltonian Neural Networks with Output Error Noise Models. (arXiv:2305.01338v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36755;&#20986;&#35823;&#24046;Hamiltonian&#31070;&#32463;&#32593;&#32476;&#65288;OE-HNN&#65289;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20855;&#26377;&#36755;&#20837;&#21644;&#22122;&#22768;&#29366;&#24577;&#27979;&#37327;&#30340;&#29289;&#29702;&#31995;&#32479;&#24314;&#27169;&#38382;&#39064;&#12290;OE-HNN&#36890;&#36807;&#20351;&#29992;&#23884;&#20837;&#24335;ODE&#27714;&#35299;&#22120;&#20174;&#24102;&#22122;&#22768;&#30340;&#29366;&#24577;&#27979;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;&#65292;&#24182;&#22312;&#19977;&#20010;&#29289;&#29702;&#31995;&#32479;&#19978;&#24471;&#21040;&#20102;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#29289;&#29702;&#31995;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24517;&#39035;&#23558;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#34701;&#20837;&#21040;&#24314;&#27169;&#26694;&#26550;&#20013;&#12290;Hamiltonian&#31070;&#32463;&#32593;&#32476;(HNN)&#23558;&#21704;&#23494;&#39039;&#29702;&#35770;&#23454;&#29616;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24418;&#25104;&#20102;&#29992;&#20110;&#24314;&#27169;&#33258;&#27835;&#33021;&#37327;&#23432;&#24658;&#31995;&#32479;&#30340;&#32508;&#21512;&#26694;&#26550;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36755;&#20986;&#35823;&#24046;Hamiltonian&#31070;&#32463;&#32593;&#32476;&#65288;OE-HNN&#65289;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20855;&#26377;&#36755;&#20837;&#21644;&#22122;&#22768;&#29366;&#24577;&#27979;&#37327;&#30340;&#29289;&#29702;&#31995;&#32479;&#24314;&#27169;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#23427;&#19981;&#38656;&#35201;&#24050;&#30693;&#29366;&#24577;&#23548;&#25968;&#12290;&#30456;&#21453;&#65292;OE-HNN&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#23884;&#20837;&#24335;ODE&#27714;&#35299;&#22120;&#65292;&#20351;&#20854;&#33021;&#22815;&#20174;&#24102;&#22122;&#22768;&#30340;&#29366;&#24577;&#27979;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#29289;&#29702;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65306;&#38459;&#23612;&#35856;&#25391;&#23376;&#65292;&#28151;&#27788;&#31995;&#32479;&#21644;&#26426;&#26800;&#33218;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;OE-HNN&#25552;&#20379;&#20102;&#24102;&#26377;&#22122;&#22768;&#27979;&#37327;&#30340;&#31995;&#32479;&#21160;&#24577;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#22312;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to make data-driven models of physical systems interpretable and reliable, it is essential to include prior physical knowledge in the modeling framework. Hamiltonian Neural Networks (HNNs) implement Hamiltonian theory in deep learning and form a comprehensive framework for modeling autonomous energy-conservative systems. Despite being suitable to estimate a wide range of physical system behavior from data, classical HNNs are restricted to systems without inputs and require noiseless state measurements and information on the derivative of the state to be available. To address these challenges, this paper introduces an Output Error Hamiltonian Neural Network (OE-HNN) modeling approach to address the modeling of physical systems with inputs and noisy state measurements. Furthermore, it does not require the state derivatives to be known. Instead, the OE-HNN utilizes an ODE-solver embedded in the training process, which enables the OE-HNN to learn the dynamics from noisy state meas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#30528;&#27979;&#35797;&#32780;&#19981;&#26029;&#36866;&#24212;&#30340;&#21305;&#37197;&#21512;&#25104;&#25511;&#21046;&#32452;&#65292;&#26469;&#21306;&#20998;&#21160;&#24577;&#24182;&#34892;&#27979;&#35797;&#20013;&#21508;&#31181;&#27979;&#35797;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.01334</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#25511;&#21046;&#21305;&#37197;&#39564;&#35777;&#22823;&#35268;&#27169;&#33258;&#36866;&#24212;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Validation of massively-parallel adaptive testing using dynamic control matching. (arXiv:2305.01334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#30528;&#27979;&#35797;&#32780;&#19981;&#26029;&#36866;&#24212;&#30340;&#21305;&#37197;&#21512;&#25104;&#25511;&#21046;&#32452;&#65292;&#26469;&#21306;&#20998;&#21160;&#24577;&#24182;&#34892;&#27979;&#35797;&#20013;&#21508;&#31181;&#27979;&#35797;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
A/B&#27979;&#35797;&#26159;&#24066;&#22330;&#20248;&#21270;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#19968;&#31181;&#33539;&#20363;&#65292;&#22240;&#20026;&#23427;&#25215;&#35834;&#33021;&#22815;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#19988;&#23427;&#24050;&#32463;&#22312;&#22823;&#22810;&#25968;&#28040;&#24687;&#20256;&#36882;&#36719;&#20214;&#24179;&#21488;&#20013;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#20225;&#19994;&#32463;&#24120;&#21516;&#26102;&#24182;&#34892;&#36816;&#34892;&#22810;&#20010;A/B/n&#27979;&#35797;&#65292;&#24182;&#23558;&#35768;&#22810;&#20869;&#23481;&#21464;&#21270;&#25171;&#21253;&#21040;&#30456;&#21516;&#30340;&#28040;&#24687;&#20013;&#65292;&#20854;&#20013;&#24182;&#19981;&#26159;&#25152;&#26377;&#20869;&#23481;&#21464;&#21270;&#37117;&#26159;&#26126;&#30830;&#27979;&#35797;&#30340;&#19968;&#37096;&#20998;&#12290;&#26080;&#35770;&#26159;&#30001;&#20110;&#35768;&#22810;&#22242;&#38431;&#21516;&#26102;&#27979;&#35797;&#65292;&#36824;&#26159;&#20316;&#20026;&#26356;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#65292;&#35813;&#26041;&#27861;&#19981;&#26029;&#26681;&#25454;&#20197;&#21069;&#30340;&#32467;&#26524;&#35843;&#25972;&#27979;&#35797;&#21644;&#27979;&#35797;&#26465;&#20214;&#20998;&#37197;&#65292;&#21160;&#24577;&#24182;&#34892;&#27979;&#35797;&#19981;&#33021;&#20687;&#20256;&#32479;&#30340;A/B&#27979;&#35797;&#19968;&#26679;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#30528;&#27979;&#35797;&#32780;&#19981;&#26029;&#36866;&#24212;&#30340;&#21305;&#37197;&#21512;&#25104;&#25511;&#21046;&#32452;&#65292;&#22312;&#36830;&#32493;&#27979;&#35797;&#36866;&#24212;&#30340;&#26465;&#20214;&#19979;&#21306;&#20998;&#21508;&#31181;&#27979;&#35797;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
A/B testing is a widely-used paradigm within marketing optimization because it promises identification of causal effects and because it is implemented out of the box in most messaging delivery software platforms. Modern businesses, however, often run many A/B/n tests at the same time and in parallel, and package many content variations into the same messages, not all of which are part of an explicit test. Whether as the result of many teams testing at the same time, or as part of a more sophisticated reinforcement learning (RL) approach that continuously adapts tests and test condition assignment based on previous results, dynamic parallel testing cannot be evaluated the same way traditional A/B tests are evaluated. This paper presents a method for disentangling the causal effects of the various tests under conditions of continuous test adaptation, using a matched-synthetic control group that adapts alongside the tests.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#21462;&#24471;&#20122;&#32447;&#24615;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01333</link><description>&lt;p&gt;
&#26080;&#25237;&#24433;&#22312;&#32447;&#20984;&#20248;&#21270;&#19982;&#38543;&#26426;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Projection-Free Online Convex Optimization with Stochastic Constraints. (arXiv:2305.01333v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01333
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#21462;&#24471;&#20122;&#32447;&#24615;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24102;&#26377;&#38543;&#26426;&#32422;&#26463;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#26080;&#25237;&#24433;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#37319;&#29992;&#20219;&#20309;&#29992;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#26080;&#38271;&#26399;&#32422;&#26463;&#30340;&#26080;&#25237;&#24433;&#31639;&#27861;&#12290;&#20351;&#29992;&#35813;&#27169;&#26495;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#20122;&#32447;&#24615;&#36951;&#25022;&#21644;&#32422;&#26463;&#36829;&#35268;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25439;&#22833;&#20989;&#25968;&#21644;&#32422;&#26463;&#20989;&#25968;&#37117;&#20809;&#28369;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#26465;&#20214;&#26799;&#24230;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{T})$&#30340;&#36951;&#25022;&#21644; $O(T^{3/4})$&#30340;&#32422;&#26463;&#36829;&#35268;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25439;&#22833;&#20989;&#25968;&#21644;&#32422;&#26463;&#20989;&#25968;&#37117;&#26159;&#38543;&#26426;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#19982;&#20851;&#32852;&#30340;&#31163;&#32447;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#23384;&#22312;&#24378;&#23545;&#20598;&#24615;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32422;&#26463;&#36829;&#35268;&#21487;&#20197;&#34987;&#20943;&#23569;&#21040;&#19982;&#36951;&#25022;&#26377;&#30456;&#21516;&#30340;&#28176;&#36817;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops projection-free algorithms for online convex optimization with stochastic constraints. We design an online primal-dual projection-free framework that can take any projection-free algorithms developed for online convex optimization with no long-term constraint. With this general template, we deduce sublinear regret and constraint violation bounds for various settings. Moreover, for the case where the loss and constraint functions are smooth, we develop a primal-dual conditional gradient method that achieves $O(\sqrt{T})$ regret and $O(T^{3/4})$ constraint violations. Furthermore, for the setting where the loss and constraint functions are stochastic and strong duality holds for the associated offline stochastic optimization problem, we prove that the constraint violation can be reduced to have the same asymptotic growth as the regret.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01322</link><description>&lt;p&gt;
&#22522;&#20110;Option&#26694;&#26550;&#30340;&#22810;&#27169;&#24335;&#25506;&#32034;&#33258;&#20027;&#38750;&#21333;&#20307;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#33258;&#20027;&#31649;&#29702;&#25506;&#32034;&#31574;&#30053;&#30340;&#22810;&#27169;&#24335;&#26234;&#33021;&#20307;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25506;&#32034;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#8220;&#22914;&#20309;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#26041;&#24335;&#65292;&#32780;&#8220;&#20309;&#26102;&#25506;&#32034;&#8221;&#30340;&#25506;&#32034;&#30740;&#31350;&#19968;&#30452;&#27809;&#26377;&#25104;&#20026;&#37325;&#28857;&#12290;&#20856;&#22411;&#30340;&#25506;&#32034;&#34892;&#20026;&#36890;&#24120;&#23558;&#25506;&#32034;&#34892;&#20026;&#19982;&#26234;&#33021;&#20307;&#30340;&#24320;&#21457;&#21033;&#29992;&#34892;&#20026;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#38750;&#21333;&#20307;&#25506;&#32034;&#34892;&#20026;&#30340;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#20154;&#31867;&#21644;&#21160;&#29289;&#30340;&#27169;&#24335;&#20999;&#25442;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#30340;&#26368;&#32456;&#30446;&#30340;&#26159;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#33258;&#20027;&#20915;&#23450;&#20309;&#26102;&#25506;&#32034;&#25110;&#21033;&#29992;&#12290;&#25105;&#20204;&#22312;Option&#26694;&#26550;&#20013;&#25551;&#36848;&#20102;&#33258;&#20027;&#22810;&#27169;&#24335;&#25506;&#32034;&#30340;&#21021;&#22987;&#30740;&#31350;&#12290;&#36890;&#36807;&#27604;&#36739;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#38750;&#21333;&#20307;&#25506;&#32034;&#26041;&#27861;&#30340;&#26356;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#20559;&#33322;&#25511;&#21046;&#31243;&#24207;&#65292;&#20197;&#26368;&#23567;&#21270;&#20559;&#33322;&#20559;&#24046;&#65292;&#24182;&#20248;&#21270;&#37325;&#26032;&#20998;&#37197;&#20559;&#33322;&#36164;&#28304;&#65292;&#20351;&#39640;&#36895;&#27573;&#24471;&#21040;&#20248;&#20808;&#22788;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;&#20559;&#33322;&#20351;&#29992;&#29575;&#20302;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#20559;&#33322;&#20559;&#24046;&#65292;&#22686;&#21152;&#20102;&#28065;&#36718;&#26426;&#39069;&#22806;&#21033;&#28070;&#65292;&#20943;&#23569;&#26426;&#26800;&#36127;&#36733;&#21644;&#24310;&#38271;&#20102;&#20351;&#29992;&#23551;&#21629;&#12290;</title><link>http://arxiv.org/abs/2305.01299</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#39118;&#21147;&#28065;&#36718;&#26426;&#20559;&#33322;&#25511;&#21046;&#26041;&#27861;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
An Improved Yaw Control Algorithm for Wind Turbines via Reinforcement Learning. (arXiv:2305.01299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#20559;&#33322;&#25511;&#21046;&#31243;&#24207;&#65292;&#20197;&#26368;&#23567;&#21270;&#20559;&#33322;&#20559;&#24046;&#65292;&#24182;&#20248;&#21270;&#37325;&#26032;&#20998;&#37197;&#20559;&#33322;&#36164;&#28304;&#65292;&#20351;&#39640;&#36895;&#27573;&#24471;&#21040;&#20248;&#20808;&#22788;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;&#20559;&#33322;&#20351;&#29992;&#29575;&#20302;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#20559;&#33322;&#20559;&#24046;&#65292;&#22686;&#21152;&#20102;&#28065;&#36718;&#26426;&#39069;&#22806;&#21033;&#28070;&#65292;&#20943;&#23569;&#26426;&#26800;&#36127;&#36733;&#21644;&#24310;&#38271;&#20102;&#20351;&#29992;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#21521;&#19982;&#39118;&#36718;&#36716;&#21521;&#26426;&#23380;&#20301;&#20043;&#38388;&#30340;&#20559;&#24046;&#20250;&#23545;&#39118;&#21147;&#28065;&#36718;&#26426;&#21450;&#20854;&#25972;&#20010;&#39118;&#30005;&#22330;&#30340;&#21151;&#29575;&#36755;&#20986;&#12289;&#23433;&#20840;&#24615;&#21644;&#23551;&#21629;&#20135;&#29983;&#24433;&#21709;&#12290;&#26412;&#25991;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#20559;&#33322;&#25511;&#21046;&#31243;&#24207;&#65292;&#20197;&#26368;&#23567;&#21270;&#20559;&#33322;&#20559;&#24046;&#65292;&#24182;&#20248;&#21270;&#37325;&#26032;&#20998;&#37197;&#20559;&#33322;&#36164;&#28304;&#65292;&#37325;&#28857;&#20248;&#21270;&#39640;&#36895;&#27573;&#65292;&#21516;&#26102;&#20445;&#25345;&#20559;&#33322;&#20351;&#29992;&#29575;&#20302;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#21644;&#27979;&#35797;&#20102;&#22870;&#21169;&#26631;&#20934;&#65292;&#20197;&#24179;&#34913;&#20559;&#33322;&#20351;&#29992;&#29575;&#19982;&#20559;&#33322;&#26657;&#20934;&#20043;&#38388;&#65288;&#19982;&#21151;&#29575;&#20135;&#37327;&#25104;&#27604;&#20363;&#65289;&#30340;&#26435;&#34913;&#65292;&#24182;&#22522;&#20110;&#26469;&#33258; REpower MM82 2MW &#28065;&#36718;&#26426;&#30340;&#23454;&#38469;&#39118;&#36895;&#35760;&#24405;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22120;&#65288;&#29615;&#22659;&#65289;&#12290;&#35813;&#31639;&#27861;&#30340;&#20004;&#20010;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#23558;&#20559;&#33322;&#20559;&#24046;&#20998;&#21035;&#38477;&#20302;&#20102; 5.5% &#21644; 11.2%&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#20027;&#21160;&#20559;&#33322;&#25511;&#21046;&#31639;&#27861;&#12290;&#30456;&#24212;&#22320;&#65292;&#24179;&#22343;&#20928;&#33021;&#37327;&#22686;&#30410;&#20998;&#21035;&#20026; 0.31% &#21644; 0.33%&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#20559;&#33322;&#25511;&#21046;&#31639;&#27861;&#12290;&#22312;&#21333;&#20010; 2MW &#28065;&#36718;&#26426;&#19978;&#65292;&#36825;&#24847;&#21619;&#30528;&#22686;&#21152;&#20102;&#22823;&#32422; 4500 &#32654;&#20803;&#30340;&#39069;&#22806;&#21033;&#28070;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#26426;&#26800;&#36127;&#36733;&#21644;&#24310;&#38271;&#20102;&#20854;&#20351;&#29992;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;
Yaw misalignment, measured as the difference between the wind direction and the nacelle position of a wind turbine, has consequences on the power output, the safety and the lifetime of the turbine and its wind park as a whole. We use reinforcement learning to develop a yaw control agent to minimise yaw misalignment and optimally reallocate yaw resources, prioritising high-speed segments, while keeping yaw usage low. To achieve this, we carefully crafted and tested the reward metric to trade-off yaw usage versus yaw alignment (as proportional to power production), and created a novel simulator (environment) based on real-world wind logs obtained from a REpower MM82 2MW turbine. The resulting algorithm decreased the yaw misalignment by 5.5% and 11.2% on two simulations of 2.7 hours each, compared to the conventional active yaw control algorithm. The average net energy gain obtained was 0.31% and 0.33% respectively, compared to the traditional yaw control algorithm. On a single 2MW turbin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#32858;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#30446;&#26631;&#35823;&#24046;&#28176;&#36817;&#19981;&#21155;&#20110;&#26410;&#30693;&#26368;&#20339;&#32858;&#21512;&#30340;&#20004;&#20493;&#65292;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#28145;&#24230;&#23884;&#20837;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01281</link><description>&lt;p&gt;
&#36890;&#36807;&#32858;&#21512;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Parameter Choice Issues in Unsupervised Domain Adaptation by Aggregation. (arXiv:2305.01281v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32447;&#24615;&#32858;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#30446;&#26631;&#35823;&#24046;&#28176;&#36817;&#19981;&#21155;&#20110;&#26410;&#30693;&#26368;&#20339;&#32858;&#21512;&#30340;&#20004;&#20493;&#65292;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#28145;&#24230;&#23884;&#20837;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#36873;&#25321;&#31639;&#27861;&#36229;&#21442;&#25968;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#28304;&#22495;&#20013;&#26377;&#26631;&#35760;&#25968;&#25454;&#65292;&#22312;&#30446;&#26631;&#22495;&#20013;&#26377;&#26469;&#33258;&#19981;&#21516;&#36755;&#20837;&#20998;&#24067;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#37319;&#29992;&#35745;&#31639;&#20351;&#29992;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#20960;&#20010;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#28982;&#21518;&#35745;&#31639;&#27169;&#22411;&#30340;&#32447;&#24615;&#32858;&#21512;&#12290;&#34429;&#28982;&#23384;&#22312;&#20960;&#20010;&#36981;&#24490;&#36825;&#31181;&#31574;&#30053;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20294;&#26159;&#20173;&#28982;&#32570;&#23569;&#20381;&#36182;&#20110;&#38480;&#21046;&#30446;&#26631;&#35823;&#24046;&#30340;&#24443;&#24213;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#27861;&#25193;&#23637;&#21040;&#21521;&#37327;&#20540;&#20989;&#25968;&#65288;&#20363;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#30446;&#26631;&#35823;&#24046;&#28176;&#36817;&#19981;&#21155;&#20110;&#26410;&#30693;&#26368;&#20339;&#32858;&#21512;&#30340;&#20004;&#20493;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#27604;&#36739;&#30740;&#31350;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#33041;&#30005;&#22270;&#12289;&#36523;&#20307;&#20256;&#24863;&#22120;&#20449;&#21495;&#21644;&#25163;&#26426;&#20449;&#21495;&#31561;&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#28145;&#24230;&#23884;&#20837;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of choosing algorithm hyper-parameters in unsupervised domain adaptation, i.e., with labeled data in a source domain and unlabeled data in a target domain, drawn from a different input distribution. We follow the strategy to compute several models using different hyper-parameters, and, to subsequently compute a linear aggregation of the models. While several heuristics exist that follow this strategy, methods are still missing that rely on thorough theories for bounding the target error. In this turn, we propose a method that extends weighted least squares to vector-valued functions, e.g., deep neural networks. We show that the target error of the proposed algorithm is asymptotically not worse than twice the error of the unknown optimal aggregation. We also perform a large scale empirical comparative study on several datasets, including text, images, electroencephalogram, body sensor signals and signals from mobile phones. Our method outperforms deep embedded valid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#21518;&#38376;&#25915;&#20987;&#27169;&#22411;&#65306;&#25968;&#25454;&#26080;&#20851;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#22120;&#31471;&#21518;&#38376;&#25915;&#20987;&#65292;&#21363;DABS&#12290;DABS&#36890;&#36807;&#30452;&#25509;&#20462;&#25913;&#20840;&#23616;&#27169;&#22411;&#26469;&#23454;&#29616;&#25915;&#20987;&#65292;&#24182;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#27491;&#24120;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.01267</link><description>&lt;p&gt;
DABS&#65306;&#22522;&#20110;&#25968;&#25454;&#26080;&#20851;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#22120;&#31471;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning. (arXiv:2305.01267v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#21518;&#38376;&#25915;&#20987;&#27169;&#22411;&#65306;&#25968;&#25454;&#26080;&#20851;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#22120;&#31471;&#21518;&#38376;&#25915;&#20987;&#65292;&#21363;DABS&#12290;DABS&#36890;&#36807;&#30452;&#25509;&#20462;&#25913;&#20840;&#23616;&#27169;&#22411;&#26469;&#23454;&#29616;&#25915;&#20987;&#65292;&#24182;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#27491;&#24120;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#35797;&#22270;&#36890;&#36807;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#32858;&#21512;&#26469;&#33258;&#20998;&#24067;&#24335;&#35774;&#22791;&#30340;&#23616;&#37096;&#27169;&#22411;&#26469;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#37327;&#24322;&#26500;&#35774;&#22791;&#30340;&#23384;&#22312;&#20351;&#24471;FL&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#38544;&#34109;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#20316;&#21697;&#20391;&#37325;&#20110;&#23458;&#25143;&#31471;&#25915;&#20987;&#65292;&#35797;&#22270;&#36890;&#36807;&#20462;&#25913;&#26412;&#22320;&#25968;&#25454;&#38598;&#26469;&#27745;&#26579;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FL&#25915;&#20987;&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#25968;&#25454;&#26080;&#20851;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#22120;&#31471;&#21518;&#38376;&#25915;&#20987;&#65288;DABS&#65289;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#30452;&#25509;&#20462;&#25913;&#20840;&#23616;&#27169;&#22411;&#26469;&#23454;&#29616;FL&#31995;&#32479;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#25915;&#20987;&#26041;&#26696;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#27491;&#24120;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) attempts to train a global model by aggregating local models from distributed devices under the coordination of a central server. However, the existence of a large number of heterogeneous devices makes FL vulnerable to various attacks, especially the stealthy backdoor attack. Backdoor attack aims to trick a neural network to misclassify data to a target label by injecting specific triggers while keeping correct predictions on original training data. Existing works focus on client-side attacks which try to poison the global model by modifying the local datasets. In this work, we propose a new attack model for FL, namely Data-Agnostic Backdoor attack at the Server (DABS), where the server directly modifies the global model to backdoor an FL system. Extensive simulation results show that this attack scheme achieves a higher attack success rate compared with baseline methods while maintaining normal accuracy on the clean data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#39044;&#27979;&#31995;&#32479;HTPS&#65292;&#20854;&#36890;&#36807;&#29305;&#24449;&#24037;&#31243;&#21644;&#33258;&#21160;&#32534;&#30721;&#22120;&#23454;&#29616;&#29305;&#24449;&#23884;&#20837;&#21644;&#24322;&#26500;&#25968;&#25454;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HTPS&#22312;&#22810;&#31181;&#39044;&#27979;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.01252</link><description>&lt;p&gt;
HTPS: &#38754;&#21521;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
HTPS: Heterogeneous Transferring Prediction System for Healthcare Datasets. (arXiv:2305.01252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#21307;&#30103;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#39044;&#27979;&#31995;&#32479;HTPS&#65292;&#20854;&#36890;&#36807;&#29305;&#24449;&#24037;&#31243;&#21644;&#33258;&#21160;&#32534;&#30721;&#22120;&#23454;&#29616;&#29305;&#24449;&#23884;&#20837;&#21644;&#24322;&#26500;&#25968;&#25454;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HTPS&#22312;&#22810;&#31181;&#39044;&#27979;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#29289;&#32852;&#32593;&#24102;&#26469;&#21307;&#30103;&#26381;&#21153;&#30340;&#38761;&#21629;&#24615;&#25913;&#36827;&#65292;&#20063;&#31216;&#20026;&#26234;&#33021;&#21307;&#30103;&#12290;&#21033;&#29992;&#22823;&#37327;&#21307;&#30103;&#25968;&#25454;&#65292;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#21327;&#21161;&#20581;&#24247;&#31649;&#29702;&#21644;&#26234;&#33021;&#35786;&#26029;&#65292;&#24182;&#23454;&#29616;P4&#21307;&#23398;&#12290;&#28982;&#32780;&#65292;&#21307;&#30103;&#25968;&#25454;&#20855;&#26377;&#39640;&#31232;&#30095;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24322;&#26500;&#36716;&#31227;&#39044;&#27979;&#31995;&#32479;&#65288;HTPS&#65289;&#12290;&#29305;&#24449;&#24037;&#31243;&#26426;&#21046;&#23558;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#31232;&#30095;&#21644;&#23494;&#38598;&#30340;&#29305;&#24449;&#30697;&#38453;&#65292;&#23884;&#20837;&#32593;&#32476;&#20013;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#19981;&#20165;&#23884;&#20837;&#29305;&#24449;&#65292;&#36824;&#20174;&#24322;&#26500;&#25968;&#25454;&#38598;&#20013;&#20256;&#36755;&#30693;&#35782;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;HTPS&#22312;&#22810;&#31181;&#39044;&#27979;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#20102;&#27599;&#20010;&#35774;&#35745;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#24322;&#26500;&#25968;&#25454;&#23545;&#22522;&#20934;&#31995;&#32479;&#30340;&#36127;&#38754;&#24433;&#21709;&#20197;&#21450;&#25152;&#25552;&#20986;&#30340;HTPS&#30340;&#39640;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical internet of things leads to revolutionary im- provements in medical services, also known as smart healthcare. With the big healthcare data, data mining and machine learning can assist wellness management and intelligent diagnosis, and achieve the P4-medicine. However, healthcare data has high sparsity and heterogeneity. In this paper, we propose a Heterogeneous Transferring Prediction System (HTPS). Feature engineering mechanism transforms the dataset into sparse and dense feature matrices, and autoencoders in the embedding networks not only embed features but also transfer knowledge from heterogeneous datasets. Experimental results show that the proposed HTPS outperforms the benchmark systems on various prediction tasks and datasets, and ablation studies present the effectiveness of each designed mechanism. Experimental results demonstrate the negative impact of heterogeneous data on benchmark systems and the high transferability of the proposed HTPS.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#21452;&#23884;&#20837;&#32593;&#32476;&#65288;MDENet&#65289;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#24320;&#25918;&#38598;&#35782;&#21035;&#65292;&#21033;&#29992;&#24694;&#24847;&#36719;&#20214;&#22270;&#29255;&#21644;&#21477;&#23376;&#26469;&#22686;&#21152;&#24694;&#24847;&#36719;&#20214;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#25552;&#21319;&#35782;&#21035;&#25928;&#26524;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01245</link><description>&lt;p&gt;
MDENet: &#22810;&#27169;&#24577;&#21452;&#23884;&#20837;&#32593;&#32476;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#24320;&#25918;&#38598;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MDENet: Multi-modal Dual-embedding Networks for Malware Open-set Recognition. (arXiv:2305.01245v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01245
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#21452;&#23884;&#20837;&#32593;&#32476;&#65288;MDENet&#65289;&#29992;&#20110;&#24694;&#24847;&#36719;&#20214;&#24320;&#25918;&#38598;&#35782;&#21035;&#65292;&#21033;&#29992;&#24694;&#24847;&#36719;&#20214;&#22270;&#29255;&#21644;&#21477;&#23376;&#26469;&#22686;&#21152;&#24694;&#24847;&#36719;&#20214;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#25552;&#21319;&#35782;&#21035;&#25928;&#26524;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;MOSR&#65289;&#26088;&#22312;&#21516;&#26102;&#23545;&#26469;&#33258;&#24050;&#30693;&#23478;&#26063;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#24182;&#26816;&#27979;&#26469;&#33258;&#26032;&#26410;&#30693;&#23478;&#26063;&#30340;&#26679;&#26412;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#32771;&#34385;&#27599;&#20010;&#24050;&#30693;&#23478;&#26063;&#30340;&#39044;&#27979;&#27010;&#29575;&#24182;&#20351;&#29992;&#22522;&#20110;&#38408;&#20540;&#30340;&#26816;&#27979;&#26469;&#23454;&#29616;MOSR&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#30340;&#29305;&#24449;&#20998;&#24067;&#38750;&#24120;&#30456;&#20284;&#65292;&#29978;&#33267;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#23478;&#26063;&#20043;&#38388;&#20063;&#26159;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#33719;&#24471;&#30340;&#20998;&#31867;&#22120;&#21487;&#33021;&#20250;&#22312;&#21521;&#24050;&#30693;&#23478;&#26063;&#27979;&#35797;&#26410;&#30693;&#26679;&#26412;&#26102;&#20135;&#29983;&#36807;&#39640;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#21452;&#23884;&#20837;&#32593;&#32476;&#65292;&#31216;&#20026;MDENet&#65292;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#32508;&#21512;&#24694;&#24847;&#36719;&#20214;&#29305;&#24449;&#65288;&#21363;&#24694;&#24847;&#36719;&#20214;&#22270;&#29255;&#21644;&#24694;&#24847;&#36719;&#20214;&#21477;&#23376;&#65289;&#26469;&#22686;&#24378;&#24694;&#24847;&#36719;&#20214;&#29305;&#24449;&#31354;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#36825;&#26356;&#20855;&#20195;&#34920;&#24615;&#21644;&#21306;&#20998;&#24615;&#65292;&#20197;&#20379;&#19979;&#28216;&#35782;&#21035;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#35777;MDENet&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29983;&#25104;&#26356;&#24378;&#22823;&#30340;&#26410;&#30693;&#26679;&#26412;&#20316;&#20026;&#30495;&#23454;&#26410;&#30693;&#23478;&#26063;&#30340;&#20195;&#29702;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MDENet&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware open-set recognition (MOSR) aims at jointly classifying malware samples from known families and detect the ones from novel unknown families, respectively. Existing works mostly rely on a well-trained classifier considering the predicted probabilities of each known family with a threshold-based detection to achieve the MOSR. However, our observation reveals that the feature distributions of malware samples are extremely similar to each other even between known and unknown families. Thus the obtained classifier may produce overly high probabilities of testing unknown samples toward known families and degrade the model performance. In this paper, we propose the Multi-modal Dual-Embedding Networks, dubbed MDENet, to take advantage of comprehensive malware features (i.e., malware images and malware sentences) from different modalities to enhance the diversity of malware feature space, which is more representative and discriminative for down-stream recognition. Last, to further guara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#31895;&#21270;&#65288;CCG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#35299;&#20915;&#20102;&#31895;&#31890;&#21270;&#27169;&#22411;&#26500;&#24314;&#21644;&#32473;&#23450; CG &#32467;&#26500;&#30340;&#32454;&#33410;&#24674;&#22797;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340; CG &#26041;&#27861;&#21450;&#26080;&#32597;&#35265;&#20107;&#20214;&#30340;&#35745;&#31639;&#33258;&#30001;&#33021;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01243</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#36870;&#31895;&#31890;&#21270;&#22810;&#23610;&#24230;&#20998;&#23376;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Machine-Learned Invertible Coarse Graining for Multiscale Molecular Modeling. (arXiv:2305.01243v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#31895;&#21270;&#65288;CCG&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#35299;&#20915;&#20102;&#31895;&#31890;&#21270;&#27169;&#22411;&#26500;&#24314;&#21644;&#32473;&#23450; CG &#32467;&#26500;&#30340;&#32454;&#33410;&#24674;&#22797;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340; CG &#26041;&#27861;&#21450;&#26080;&#32597;&#35265;&#20107;&#20214;&#30340;&#35745;&#31639;&#33258;&#30001;&#33021;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#20998;&#23376;&#24314;&#27169;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#23376;&#22312;&#22823;&#26102;&#38388;&#21644;&#38271;&#24230;&#23610;&#24230;&#19979;&#30340;&#24615;&#36136;&#12290;&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#22810;&#23610;&#24230;&#24314;&#27169;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#31895;&#31890;&#21270;&#65288;CG&#65289;&#27169;&#22411;&#30340;&#26500;&#24314;&#21644;&#32473;&#23450; CG &#32467;&#26500;&#30340;&#32454;&#33410;&#24674;&#22797;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#24490;&#29615;&#31895;&#21270;&#65288;CCG&#65289;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#22312; CCG &#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#26131;&#22788;&#29702;&#30340;&#20248;&#21270;&#36807;&#31243;&#23454;&#29616;&#37325;&#26500;&#65292;&#20174; CG &#27169;&#25311;&#20013;&#24674;&#22797;&#32454;&#33410;&#65292;&#36827;&#32780;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340; CG &#26041;&#27861;&#21450;&#26080;&#32597;&#35265;&#20107;&#20214;&#30340;&#35745;&#31639;&#33258;&#30001;&#33021;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiscale molecular modeling is widely applied in scientific research of molecular properties over large time and length scales. Two specific challenges are commonly present in multiscale modeling, provided that information between the coarse and fine representations of molecules needs to be properly exchanged: One is to construct coarse grained (CG) models by passing information from the fine to coarse levels; the other is to restore finer molecular details given CG configurations. Although these two problems are commonly addressed independently, in this work, we present a theory connecting them, and develop a methodology called Cycle Coarse Graining (CCG) to solve both problems in a unified manner. In CCG, reconstruction can be achieved via a tractable optimization process, leading to a general method to retrieve fine details from CG simulations, which in turn, delivers a new solution to the CG problem, yielding an efficient way to calculate free energies in a rare-event-free manner
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37327;&#21270;&#27969;&#27700;&#32447;&#23545;&#37096;&#20998;&#25163;&#21183;&#24207;&#21015;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24230;&#36924;&#30495;&#12289;&#23500;&#26377;&#34920;&#29616;&#21147;&#12289;&#24182;&#36991;&#20813;&#20102;&#20266;&#20687;&#30340;&#25163;&#21183;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.01241</link><description>&lt;p&gt;
AQ-GT:&#19968;&#31181;&#26102;&#38388;&#23545;&#40784;&#24182;&#37327;&#21270;&#30340;GRU-Transformer&#65292;&#29992;&#20110;&#20849;&#35821;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis. (arXiv:2305.01241v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37327;&#21270;&#27969;&#27700;&#32447;&#23545;&#37096;&#20998;&#25163;&#21183;&#24207;&#21015;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24230;&#36924;&#30495;&#12289;&#23500;&#26377;&#34920;&#29616;&#21147;&#12289;&#24182;&#36991;&#20813;&#20102;&#20266;&#20687;&#30340;&#25163;&#21183;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21019;&#24314;&#22810;&#27169;&#24335;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26102;&#65292;&#29983;&#25104;&#36924;&#30495;&#19988;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#20849;&#35821;&#25163;&#21183;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23398;&#20064;&#20849;&#35821;&#25163;&#21183;&#34920;&#31034;&#21644;&#23454;&#38469;&#21160;&#20316;&#20043;&#38388;&#30340;&#30452;&#25509;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#20135;&#29983;&#20284;&#20046;&#33258;&#28982;&#20294;&#24120;&#24120;&#19981;&#20196;&#20154;&#20449;&#26381;&#30340;&#25163;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37327;&#21270;&#27969;&#27700;&#32447;&#23545;&#37096;&#20998;&#25163;&#21183;&#24207;&#21015;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#30721;&#26412;&#21521;&#37327;&#26082;&#26159;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;&#36755;&#20837;&#65292;&#20063;&#26159;&#36755;&#20986;&#65292;&#26500;&#25104;&#25163;&#21183;&#30340;&#29983;&#25104;&#21644;&#37325;&#26500;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#30340;&#26144;&#23556;&#65292;&#32780;&#19981;&#26159;&#23558;&#20854;&#30452;&#25509;&#26144;&#23556;&#21040;&#30690;&#37327;&#34920;&#31034;&#65292;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#39640;&#24230;&#36924;&#30495;&#19988;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#25163;&#21183;&#29983;&#25104;&#65292;&#36825;&#20123;&#25163;&#21183;&#19982;&#20154;&#31867;&#30340;&#36816;&#21160;&#21644;&#34892;&#20026;&#38750;&#24120;&#30456;&#20284;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20266;&#20687;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#23427;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#36924;&#30495;&#21644;&#20016;&#23500;&#30340;&#25163;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of realistic and contextually relevant co-speech gestures is a challenging yet increasingly important task in the creation of multimodal artificial agents. Prior methods focused on learning a direct correspondence between co-speech gesture representations and produced motions, which created seemingly natural but often unconvincing gestures during human assessment. We present an approach to pre-train partial gesture sequences using a generative adversarial network with a quantization pipeline. The resulting codebook vectors serve as both input and output in our framework, forming the basis for the generation and reconstruction of gestures. By learning the mapping of a latent space representation as opposed to directly mapping it to a vector representation, this framework facilitates the generation of highly realistic and expressive gestures that closely replicate human movement and behavior, while simultaneously avoiding artifacts in the generation process. We evaluate ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21160;&#24577;&#35843;&#24230;&#21442;&#19982;&#26412;&#22320;&#35757;&#32451;&#30340;&#35774;&#22791;&#23376;&#38598;&#30340;&#31639;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#20854;&#26102;&#38388;&#24179;&#22343;&#25968;&#25454;&#37325;&#35201;&#24615;&#65292;&#36866;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#24378;&#26102;&#38388;&#30456;&#20851;&#24615;&#26102;</title><link>http://arxiv.org/abs/2305.01238</link><description>&lt;p&gt;
&#24102;&#26377;&#27969;&#25968;&#25454;&#30340;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#30340;&#21160;&#24577;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Dynamic Scheduling for Federated Edge Learning with Streaming Data. (arXiv:2305.01238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#21160;&#24577;&#35843;&#24230;&#21442;&#19982;&#26412;&#22320;&#35757;&#32451;&#30340;&#35774;&#22791;&#23376;&#38598;&#30340;&#31639;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#20854;&#26102;&#38388;&#24179;&#22343;&#25968;&#25454;&#37325;&#35201;&#24615;&#65292;&#36866;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#24378;&#26102;&#38388;&#30456;&#20851;&#24615;&#26102;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;FEEL&#65289;&#31995;&#32479;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#22312;&#20855;&#26377;&#38271;&#26399;&#33021;&#28304;&#32422;&#26463;&#30340;&#20998;&#24067;&#24335;&#36793;&#32536;&#35774;&#22791;&#38598;&#19978;&#38543;&#26426;&#29983;&#25104;&#65292;&#30001;&#20110;&#36890;&#20449;&#36164;&#28304;&#21644;&#26102;&#24310;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#27599;&#27425;&#36845;&#20195;&#21482;&#26377;&#19968;&#20010;&#35774;&#22791;&#23376;&#38598;&#21487;&#20197;&#21442;&#21152;&#26412;&#22320;&#22521;&#35757;&#36807;&#31243;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#38543;&#26426;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#24230;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#34987;&#35843;&#24230;&#29992;&#25143;&#38598;&#30340;&#26102;&#38388;&#24179;&#22343;&#25968;&#25454;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#21463;&#21040;&#33021;&#28304;&#28040;&#32791;&#21644;&#26102;&#24310;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;Lyapunov&#20248;&#21270;&#26694;&#26550;&#30340;&#31639;&#27861;&#22312;&#19981;&#32771;&#34385;&#26102;&#21464;&#25968;&#25454;&#37325;&#35201;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#29983;&#25104;&#26174;&#31034;&#20986;&#24378;&#26102;&#38388;&#30456;&#20851;&#24615;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider a Federated Edge Learning (FEEL) system where training data are randomly generated over time at a set of distributed edge devices with long-term energy constraints. Due to limited communication resources and latency requirements, only a subset of devices is scheduled for participating in the local training process in every iteration. We formulate a stochastic network optimization problem for designing a dynamic scheduling policy that maximizes the time-average data importance from scheduled user sets subject to energy consumption and latency constraints. Our proposed algorithm based on the Lyapunov optimization framework outperforms alternative methods without considering time-varying data importance, especially when the generation of training data shows strong temporal correlation.
&lt;/p&gt;</description></item><item><title>CNS-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#24335;&#22330;&#26223;&#19979;&#24694;&#24847;&#36719;&#20214;&#35782;&#21035;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#21512;&#25104;&#20445;&#23432;&#24615;&#26032;&#39062;&#24615;&#26469;&#20934;&#30830;&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#12290;</title><link>http://arxiv.org/abs/2305.01236</link><description>&lt;p&gt;
CNS-Net: &#29992;&#20110;&#24320;&#25918;&#24335;&#22330;&#26223;&#19979;&#24694;&#24847;&#36719;&#20214;&#35782;&#21035;&#30340;&#20445;&#23432;&#24615;&#26032;&#39062;&#21512;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CNS-Net: Conservative Novelty Synthesizing Network for Malware Recognition in an Open-set Scenario. (arXiv:2305.01236v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01236
&lt;/p&gt;
&lt;p&gt;
CNS-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#24335;&#22330;&#26223;&#19979;&#24694;&#24847;&#36719;&#20214;&#35782;&#21035;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#21512;&#25104;&#20445;&#23432;&#24615;&#26032;&#39062;&#24615;&#26469;&#20934;&#30830;&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24694;&#24847;&#36719;&#20214;&#24320;&#25918;&#38598;&#35782;&#21035;(MOSR)&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#21363;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#20013;&#36827;&#34892;&#35782;&#21035;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#20551;&#35774;&#20998;&#31867;&#22120;&#24050;&#30693;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#22788;&#20110;&#20851;&#38381;&#38598;&#22330;&#26223;&#20013;&#65292;&#21363;&#27979;&#35797;&#23478;&#26063;&#26159;&#35757;&#32451;&#23478;&#26063;&#30340;&#23376;&#38598;&#25110;&#26368;&#22810;&#19982;&#20043;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;&#26410;&#30693;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#32463;&#24120;&#20986;&#29616;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#24320;&#25918;&#38598;&#22330;&#26223;&#20013;&#23545;&#24694;&#24847;&#36719;&#20214;&#23454;&#20363;&#36827;&#34892;&#35782;&#21035;&#65292;&#21363;&#27979;&#35797;&#38598;&#20013;&#20063;&#21253;&#21547;&#19968;&#20123;&#26410;&#30693;&#23478;&#26063;&#12290;MOSR&#30340;&#19968;&#20010;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#32771;&#34385;&#36890;&#36807;&#21333;&#19968;&#20998;&#31867;&#22120;(&#20363;&#22914;&#65292;&#31070;&#32463;&#32593;&#32476;)&#20174;&#24050;&#30693;&#23478;&#26063;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#24046;&#26469;&#21516;&#26102;&#23545;&#24050;&#30693;&#21644;&#26410;&#30693;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#36827;&#34892;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33391;&#22909;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#36890;&#24120;&#20542;&#21521;&#20110;&#33719;&#24471;&#36807;&#39640;&#30340;&#35782;&#21035;&#27010;&#29575;&#36755;&#20986;&#65292;&#20174;&#32780;&#38590;&#20197;&#21306;&#20998;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CNS-Net&#65292;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21512;&#25104;&#20102;&#20445;&#23432;&#24615;&#26032;&#39062;&#24615;&#65292;&#26088;&#22312;&#20934;&#30830;&#22320;&#20998;&#31867;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the challenging task of malware recognition on both known and novel unknown malware families, called malware open-set recognition (MOSR). Previous works usually assume the malware families are known to the classifier in a close-set scenario, i.e., testing families are the subset or at most identical to training families. However, novel unknown malware families frequently emerge in real-world applications, and as such, require to recognize malware instances in an open-set scenario, i.e., some unknown families are also included in the test-set, which has been rarely and non-thoroughly investigated in the cyber-security domain. One practical solution for MOSR may consider jointly classifying known and detecting unknown malware families by a single classifier (e.g., neural network) from the variance of the predicted probability distribution on known families. However, conventional well-trained classifiers usually tend to obtain overly high recognition probabilities in the outputs,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#27861;&#24459;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;MultiLegalSBD&#65292;&#21253;&#25324;6&#31181;&#35821;&#35328;&#30340;130,000&#20010;&#27880;&#37322;&#21477;&#23376;&#12290;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#65292;&#29616;&#26377;&#30340;SBD&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#35757;&#32451;&#20102;&#22522;&#20110;CRF&#12289;BiLSTM-CRF&#21644;transformers&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35813;&#39046;&#22495;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#30340;&#22810;&#35821;&#27169;&#22411;&#22312;&#38646;-shot&#27979;&#35797;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.01211</link><description>&lt;p&gt;
MultiLegalSBD&#65306;&#19968;&#20010;&#22810;&#35821;&#35328;&#27861;&#24459;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset. (arXiv:2305.01211v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#27861;&#24459;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#25968;&#25454;&#38598;MultiLegalSBD&#65292;&#21253;&#25324;6&#31181;&#35821;&#35328;&#30340;130,000&#20010;&#27880;&#37322;&#21477;&#23376;&#12290;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#65292;&#29616;&#26377;&#30340;SBD&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#12290;&#20316;&#32773;&#35757;&#32451;&#20102;&#22522;&#20110;CRF&#12289;BiLSTM-CRF&#21644;transformers&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35813;&#39046;&#22495;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#30340;&#22810;&#35821;&#27169;&#22411;&#22312;&#38646;-shot&#27979;&#35797;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30784;&#20043;&#19968;&#65292;&#19981;&#27491;&#30830;&#30340;&#20998;&#21106;&#20250;&#20005;&#37325;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#23545;&#20110;&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#23545;&#20110;&#27861;&#24459;&#39046;&#22495;&#65292;&#22240;&#20026;&#20351;&#29992;&#30340;&#22797;&#26434;&#21477;&#23376;&#32467;&#26500;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#27861;&#24459;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;6&#31181;&#35821;&#35328;&#30340;130,000&#20010;&#27880;&#37322;&#21477;&#23376;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;SBD&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#27861;&#24459;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#22522;&#20110;CRF&#12289;BiLSTM-CRF&#21644;transformers&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#22810;&#35821;&#27169;&#22411;&#22312;&#33889;&#33796;&#29273;&#35821;&#27979;&#35797;&#38598;&#30340;&#38646;-shot&#35774;&#32622;&#20013;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#12290;&#20026;&#20102;&#40723;&#21169;&#31038;&#21306;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#65292;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence Boundary Detection (SBD) is one of the foundational building blocks of Natural Language Processing (NLP), with incorrectly split sentences heavily influencing the output quality of downstream tasks. It is a challenging task for algorithms, especially in the legal domain, considering the complex and different sentence structures used. In this work, we curated a diverse multilingual legal dataset consisting of over 130'000 annotated sentences in 6 languages. Our experimental results indicate that the performance of existing SBD models is subpar on multilingual legal data. We trained and tested monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers, demonstrating state-of-the-art performance. We also show that our multilingual models outperform all baselines in the zero-shot setting on a Portuguese test set. To encourage further research and development by the community, we have made our dataset, models, and code publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#20195;&#30721;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;EvalPlus&#65292;&#29992;&#20110;&#35780;&#20272;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01210</link><description>&lt;p&gt;
ChatGPT&#29983;&#25104;&#30340;&#20195;&#30721;&#30495;&#30340;&#27491;&#30830;&#21527;&#65311;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#20005;&#26684;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#20195;&#30721;&#32508;&#21512;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;EvalPlus&#65292;&#29992;&#20110;&#35780;&#20272;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#32508;&#21512;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#34987;&#38271;&#26399;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#30452;&#25509;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#20013;&#29992;&#25143;&#30340;&#24847;&#22270;&#29983;&#25104;&#20195;&#30721;&#12290;&#20195;&#30721;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#31574;&#21010;&#22909;&#30340;&#32508;&#21512;&#38382;&#39064;&#21644;&#21508;&#31181;&#36755;&#20837;/&#36755;&#20986;&#27979;&#35797;&#29992;&#20363;&#65292;&#34987;&#29992;&#26469;&#34913;&#37327;&#21508;&#31181;LLMs&#22312;&#20195;&#30721;&#32508;&#21512;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#27979;&#35797;&#29992;&#20363;&#22312;&#23436;&#20840;&#35780;&#20272;&#29983;&#25104;&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#26041;&#38754;&#65292;&#25968;&#37327;&#21644;&#36136;&#37327;&#37117;&#21487;&#33021;&#26377;&#25152;&#38480;&#21046;&#12290;&#36825;&#31181;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#38480;&#21046;&#24341;&#20986;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#22312;LLMs&#26102;&#20195;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#30495;&#30340;&#27491;&#30830;&#21527;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvalPlus&#8212;&#8212;&#19968;&#20010;&#35780;&#20272;LLM-synthesized&#20195;&#30721;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#20005;&#26684;&#22522;&#20934;&#35780;&#20272;&#26694;&#26550;&#12290;EvalPlus&#25509;&#21463;&#22522;&#30784;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#33258;&#21160;&#36755;&#20837;&#29983;&#25104;&#27493;&#39588;&#65292;&#20351;&#29992;LLM-based&#21644;&#22522;&#20110;&#21464;&#24322;&#30340;&#26041;&#27861;&#29983;&#25104;&#21644;&#22810;&#26679;&#21270;&#22823;&#37327;&#26032;&#30340;&#27979;&#35797;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various LLMs on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. In short, EvalPlus takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both LLM-based and mutation-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#24403;&#21069;&#25490;&#21517;&#20013;&#30340;&#19968;&#20010;&#39033;&#30446;&#19982;&#25490;&#21517;&#22806;&#30340;&#39033;&#30446;&#39640;&#25928;&#22320;&#20132;&#25442;&#26469;&#25191;&#34892;&#25506;&#32034;&#65292;&#24182;&#22522;&#20110;Kullback-Leibler&#19978;&#32622;&#20449;&#24230;&#30028;&#38480;&#65288;KL-UCB&#65289;&#23545;&#26410;&#25490;&#24207;&#39033;&#30446;&#36827;&#34892;&#20048;&#35266;&#36873;&#25321;&#21644;&#23433;&#20840;&#30340;&#25490;&#24207;&#65292;&#20197;&#25552;&#39640;&#38271;&#26399;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.01202</link><description>&lt;p&gt;
&#23433;&#20840;&#22312;&#32447;&#23398;&#20064;&#20013;&#26410;&#25490;&#24207;&#39033;&#30446;&#30340;&#25506;&#32034;&#19982;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Exploration of Unranked Items in Safe Online Learning to Re-Rank. (arXiv:2305.01202v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#24403;&#21069;&#25490;&#21517;&#20013;&#30340;&#19968;&#20010;&#39033;&#30446;&#19982;&#25490;&#21517;&#22806;&#30340;&#39033;&#30446;&#39640;&#25928;&#22320;&#20132;&#25442;&#26469;&#25191;&#34892;&#25506;&#32034;&#65292;&#24182;&#22522;&#20110;Kullback-Leibler&#19978;&#32622;&#20449;&#24230;&#30028;&#38480;&#65288;KL-UCB&#65289;&#23545;&#26410;&#25490;&#24207;&#39033;&#30446;&#36827;&#34892;&#20048;&#35266;&#36873;&#25321;&#21644;&#23433;&#20840;&#30340;&#25490;&#24207;&#65292;&#20197;&#25552;&#39640;&#38271;&#26399;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#32463;&#24120;&#35797;&#22270;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#26368;&#22823;&#21270;&#38271;&#26399;&#25910;&#30410;&#12290;&#28982;&#32780;&#65292;&#20174;&#23454;&#38469;&#35282;&#24230;&#32771;&#34385;&#65292;&#36825;&#31181;&#31639;&#27861;&#30001;&#20110;&#36807;&#20110;&#28608;&#36827;&#30340;&#25506;&#32034;&#32780;&#20855;&#26377;&#25439;&#23475;&#29992;&#25143;&#20307;&#39564;&#30340;&#39640;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#65292;&#23545;&#23433;&#20840;&#25506;&#32034;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#30340;&#22312;&#32447;&#23398;&#20064;&#25490;&#24207;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#23558;&#24403;&#21069;&#25490;&#21517;&#20013;&#30340;&#19968;&#20010;&#39033;&#30446;&#19982;&#25490;&#21517;&#22806;&#30340;&#39033;&#30446;&#65288;&#21363;&#26410;&#25490;&#24207;&#39033;&#30446;&#65289;&#39640;&#25928;&#22320;&#20132;&#25442;&#26469;&#25191;&#34892;&#25506;&#32034;&#24182;&#36880;&#27493;&#25552;&#39640;&#21407;&#22987;&#25490;&#21517;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22522;&#20110;Kullback-Leibler&#19978;&#32622;&#20449;&#24230;&#30028;&#38480;&#65288;KL-UCB&#65289;&#20048;&#35266;&#22320;&#36873;&#25321;&#19968;&#20010;&#26410;&#25490;&#21517;&#39033;&#30446;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#23433;&#20840;&#22320;&#23545;&#21253;&#25324;&#25152;&#36873;&#39033;&#30446;&#22312;&#20869;&#30340;&#39033;&#30446;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#23433;&#20840;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38271;&#26399;&#24724;&#24680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandit algorithms for online learning to rank (OLTR) problems often aim to maximize long-term revenue by utilizing user feedback. From a practical point of view, however, such algorithms have a high risk of hurting user experience due to their aggressive exploration. Thus, there has been a rising demand for safe exploration in recent years. One approach to safe exploration is to gradually enhance the quality of an original ranking that is already guaranteed acceptable quality. In this paper, we propose a safe OLTR algorithm that efficiently exchanges one of the items in the current ranking with an item outside the ranking (i.e., an unranked item) to perform exploration. We select an unranked item optimistically to explore based on Kullback-Leibler upper confidence bounds (KL-UCB) and safely re-rank the items including the selected one. Through experiments, we demonstrate that the proposed algorithm improves long-term regret from baselines without any safety violation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27880;&#37322;&#20102;&#19968;&#20010;&#30001;1308&#20010;&#23545;&#35805;&#32452;&#25104;&#30340;&#20013;&#25991;&#33258;&#28982;&#35805;&#39064;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#20197;&#22635;&#34917;&#20013;&#25991;&#33258;&#28982;&#23545;&#35805;&#35805;&#39064;&#35821;&#26009;&#24211;&#30340;&#31354;&#30333;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#30340;&#24072;&#29983;&#26694;&#26550;&#26469;&#39044;&#27979;&#27809;&#26377;&#22238;&#22797;&#30340;&#35805;&#39064;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.01195</link><description>&lt;p&gt;
&#20013;&#25991;&#23545;&#35805;&#20013;&#30340;&#35805;&#39064;&#36716;&#31227;&#26816;&#27979;&#65306;&#35821;&#26009;&#24211;&#19982;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Topic Shift Detection in Chinese Dialogues: Corpus and Benchmark. (arXiv:2305.01195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27880;&#37322;&#20102;&#19968;&#20010;&#30001;1308&#20010;&#23545;&#35805;&#32452;&#25104;&#30340;&#20013;&#25991;&#33258;&#28982;&#35805;&#39064;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#20197;&#22635;&#34917;&#20013;&#25991;&#33258;&#28982;&#23545;&#35805;&#35805;&#39064;&#35821;&#26009;&#24211;&#30340;&#31354;&#30333;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#30340;&#24072;&#29983;&#26694;&#26550;&#26469;&#39044;&#27979;&#27809;&#26377;&#22238;&#22797;&#30340;&#35805;&#39064;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#35805;&#39064;&#36716;&#31227;&#26816;&#27979;&#26159;&#25351;&#26816;&#27979;&#23545;&#35805;&#20013;&#27491;&#22312;&#36827;&#34892;&#30340;&#35805;&#39064;&#26159;&#21542;&#36716;&#31227;&#25110;&#24212;&#35813;&#36716;&#31227;&#12290;&#20219;&#21153;&#21487;&#20998;&#20026;&#24050;&#30693;&#22238;&#22797;&#20219;&#21153;&#21644;&#26410;&#30693;&#22238;&#22797;&#20219;&#21153;&#12290;&#30446;&#21069;&#65292;&#21482;&#26377;&#23569;&#25968;&#38024;&#23545;&#21518;&#32773;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#22238;&#22797;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#35805;&#39064;&#36716;&#31227;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#26412;&#25991;&#39318;&#20808;&#27880;&#37322;&#20102;&#19968;&#20010;&#30001;1308&#20010;&#23545;&#35805;&#32452;&#25104;&#30340;&#20013;&#25991;&#33258;&#28982;&#35805;&#39064;&#23545;&#35805;&#65288;CNTD&#65289;&#35821;&#26009;&#24211;&#65292;&#20197;&#22635;&#34917;&#20013;&#25991;&#33258;&#28982;&#20250;&#35805;&#35805;&#39064;&#35821;&#26009;&#24211;&#30340;&#31354;&#30333;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26410;&#30693;&#22238;&#22797;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#30340;&#24072;&#29983;&#26694;&#26550;&#26469;&#39044;&#27979;&#27809;&#26377;&#22238;&#22797;&#30340;&#35805;&#39064;&#36716;&#31227;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#39640;&#32423;&#24072;&#29983;&#21709;&#24212;&#20013;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#65292;&#29992;&#20110;&#24314;&#31435;&#21709;&#24212;&#21644;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#32780;&#22312;&#20302;&#32423;&#23398;&#29983;&#20013;&#26500;&#24314;&#26631;&#31614;&#23545;&#27604;&#23398;&#20064;&#12290;&#22312;&#25105;&#20204;&#30340;&#20013;&#25991;CNTD&#21644;&#33521;&#25991;TIAGE&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue topic shift detection is to detect whether an ongoing topic has shifted or should shift in a dialogue, which can be divided into two categories, i.e., response-known task and response-unknown task. Currently, only a few investigated the latter, because it is still a challenge to predict the topic shift without the response information. In this paper, we first annotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308 dialogues to fill the gap in the Chinese natural conversation topic corpus. And then we focus on the response-unknown task and propose a teacher-student framework based on hierarchical contrastive learning to predict the topic shift without the response. Specifically, the response at high-level teacher-student is introduced to build the contrastive learning between the response and the context, while the label contrastive learning is constructed at low-level student. The experimental results on our Chinese CNTD and English TIAGE show the effectiven
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24102;&#22122;&#22768;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#24335;&#20808;&#39564;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#12289;&#22270;&#20687;&#20462;&#34917;&#12289;&#36229;&#20998;&#36776;&#29575;&#31561;&#21453;&#38382;&#39064;&#65292;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01166</link><description>&lt;p&gt;
&#20174;&#24102;&#22122;&#22768;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#24335;&#20808;&#39564;&#26469;&#35299;&#20915;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data. (arXiv:2305.01166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24102;&#22122;&#22768;&#25968;&#25454;&#20013;&#23398;&#20064;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#24335;&#20808;&#39564;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#20687;&#37325;&#24314;&#12289;&#22270;&#20687;&#20462;&#34917;&#12289;&#36229;&#20998;&#36776;&#29575;&#31561;&#21453;&#38382;&#39064;&#65292;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SURE-Score&#65306;&#19968;&#31181;&#21033;&#29992;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#25152;&#27745;&#26579;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#23398;&#20064;&#24471;&#20998;-based&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#24403;&#26377;&#19968;&#20010;&#22823;&#22411;&#24178;&#20928;&#26679;&#26412;&#30340;&#35757;&#32451;&#38598;&#21487;&#29992;&#26102;&#65292;&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#65292;&#36890;&#36807;&#22312;&#19979;&#38754;&#30340;&#23436;&#20840;&#37319;&#26679;&#25968;&#25454;&#20998;&#24067;&#19978;&#35757;&#32451;&#24471;&#20998;-based&#65288;&#25193;&#25955;&#65289;&#29983;&#25104;&#27169;&#22411;&#26469;&#35299;&#20915;&#21453;&#38382;&#39064;&#65292;&#32988;&#36807;&#19968;&#27493;&#21040;&#20301;&#30340;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#26679;&#19968;&#20010;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#33021;&#22312;&#19968;&#24320;&#22987;&#33719;&#21462;&#26102;&#26159;&#38590;&#20197;&#36127;&#25285;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#20013;&#22823;&#32422;&#23398;&#20064;&#24178;&#20928;&#20998;&#24067;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#21046;&#23450;&#24182;&#35777;&#26126;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21033;&#29992;Stein&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#26469;&#36890;&#36807;&#38500;&#22122;&#24471;&#20998;&#21305;&#37197;&#36830;&#32493;&#22320;&#38500;&#21435;&#25968;&#25454;&#21644;&#23398;&#20064;&#24471;&#20998;&#20989;&#25968;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;&#22122;&#22768;&#26679;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#20808;&#39564;&#24182;&#24212;&#29992;&#21518;&#39564;&#37319;&#26679;&#21040;&#22270;&#20687;&#37325;&#24314;&#12289;&#22270;&#20687;&#20462;&#34917;&#21644;&#36229;&#20998;&#36776;&#29575;&#31561;&#21453;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;SURE-Score&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21307;&#23398;&#25104;&#20687;&#12289;&#26174;&#24494;&#38236;&#21644;&#25668;&#24433;&#31561;&#21453;&#38382;&#39064;&#30340;&#19968;&#31995;&#21015;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SURE-Score: an approach for learning score-based generative models using training samples corrupted by additive Gaussian noise. When a large training set of clean samples is available, solving inverse problems via score-based (diffusion) generative models trained on the underlying fully-sampled data distribution has recently been shown to outperform end-to-end supervised deep learning. In practice, such a large collection of training data may be prohibitively expensive to acquire in the first place. In this work, we present an approach for approximately learning a score-based generative model of the clean distribution, from noisy training data. We formulate and justify a novel loss function that leverages Stein's unbiased risk estimate to jointly denoise the data and learn the score function via denoising score matching, while using only the noisy samples. We demonstrate the generality of SURE-Score by learning priors and applying posterior sampling to ill-posed inverse prob
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LC&#30340;&#26032;&#38271;&#23614;&#35782;&#21035;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#65292;&#21516;&#26102;&#35299;&#20915;&#31867;&#21035;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01160</link><description>&lt;p&gt;
&#26368;&#22823;&#21270;&#28508;&#22312;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#23454;&#29616;&#38271;&#23614;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels. (arXiv:2305.01160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LC&#30340;&#26032;&#38271;&#23614;&#35782;&#21035;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#26356;&#22909;&#22320;&#27169;&#25311;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#65292;&#21516;&#26102;&#35299;&#20915;&#31867;&#21035;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#31181;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#38271;&#23614;&#20998;&#24067;&#26102;&#65292;&#23427;&#20204;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#23558;&#23545;&#27604;&#23398;&#20064;&#21644;&#36923;&#36753;&#26031;&#33922;&#35843;&#25972;&#25216;&#26415;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#32452;&#21512;&#26159;&#20020;&#26102;&#30340;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#29702;&#35770;&#32972;&#26223;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#32972;&#26223;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#38271;&#23614;&#20219;&#21153;&#20013;&#36935;&#21040;&#22256;&#38590;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#23427;&#20204;&#35797;&#22270;&#26368;&#22823;&#21270;&#28508;&#22312;&#29305;&#24449;&#21644;&#36755;&#20837;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#12290;&#30001;&#20110;&#19981;&#32771;&#34385;&#30495;&#23454;&#26631;&#31614;&#30340;&#26368;&#22823;&#21270;&#65292;&#23427;&#20204;&#26080;&#27861;&#35299;&#20915;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#35299;&#37322;&#20026;&#28508;&#22312;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#19968;&#31181;&#26377;&#21407;&#21017;&#30340;&#26041;&#24335;&#38598;&#25104;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#36923;&#36753;&#26031;&#33922;&#35843;&#25972;&#25216;&#26415;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#28508;&#22312;&#31867;&#21035;&#65288;LC&#65289;&#26041;&#27861;&#65292;&#23427;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#30495;&#23454;&#26631;&#31614;&#30340;&#20998;&#24067;&#65292;&#24182;&#32852;&#21512;&#26368;&#22823;&#21270;&#28508;&#22312;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#23545;&#21253;&#25324;CIFAR-10&#65292;CIFAR-100&#21644;ImageNet&#22312;&#20869;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although contrastive learning methods have shown prevailing performance on a variety of representation learning tasks, they encounter difficulty when the training dataset is long-tailed. Many researchers have combined contrastive learning and a logit adjustment technique to address this problem, but the combinations are done ad-hoc and a theoretical background has not yet been provided. The goal of this paper is to provide the background and further improve the performance. First, we show that the fundamental reason contrastive learning methods struggle with long-tailed tasks is that they try to maximize the mutual information maximization between latent features and input data. As ground-truth labels are not considered in the maximization, they are not able to address imbalances between class labels. Rather, we interpret the long-tailed recognition task as a mutual information maximization between latent features and ground-truth labels. This approach integrates contrastive learning a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedAVO&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#36873;&#25321;&#26368;&#20339;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;FL&#25805;&#20316;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.01154</link><description>&lt;p&gt;
FedAVO&#65306;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
FedAVO: Improving Communication Efficiency in Federated Learning with African Vultures Optimizer. (arXiv:2305.01154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedAVO&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#36873;&#25321;&#26368;&#20339;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;FL&#25805;&#20316;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30001;&#20110;&#24378;&#35843;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;FL&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#21487;&#33021;&#23548;&#33268;&#36890;&#20449;&#21463;&#38480;&#24182;&#19988;&#23398;&#20064;&#36807;&#31243;&#21464;&#24471;&#25302;&#24310;&#65292;&#38656;&#35201;&#23545;&#23458;&#25143;-&#26381;&#21153;&#22120;&#36890;&#20449;&#25104;&#26412;&#36827;&#34892;&#20248;&#21270;&#12290;&#36873;&#25321;&#30340;&#23458;&#25143;&#27604;&#20363;&#21644;&#26412;&#22320;&#35757;&#32451;&#24490;&#29615;&#27425;&#25968;&#26159;&#23545;FL&#24615;&#33021;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#20004;&#20010;&#36229;&#21442;&#25968;&#12290;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#20559;&#22909;&#65292;&#22240;&#27492;FL&#20174;&#19994;&#32773;&#24456;&#38590;&#25163;&#21160;&#36873;&#25321;&#36825;&#20123;&#36229;&#21442;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FedAVO&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;FL&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#27954;&#31171;&#40555;&#20248;&#21270;&#22120;&#65288;AVO&#65289;&#36873;&#25321;&#26368;&#20339;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;AVO&#36827;&#34892;FL&#36229;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#19982;FL&#25805;&#20316;&#30456;&#20851;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL), a distributed machine learning technique has recently experienced tremendous growth in popularity due to its emphasis on user data privacy. However, the distributed computations of FL can result in constrained communication and drawn-out learning processes, necessitating the client-server communication cost optimization. The ratio of chosen clients and the quantity of local training passes are two hyperparameters that have a significant impact on FL performance. Due to different training preferences across various applications, it can be difficult for FL practitioners to manually select such hyperparameters. In our research paper, we introduce FedAVO, a novel FL algorithm that enhances communication effectiveness by selecting the best hyperparameters leveraging the African Vulture Optimizer (AVO). Our research demonstrates that the communication costs associated with FL operations can be substantially reduced by adopting AVO for FL hyperparameter adjustment. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#39033;&#30740;&#31350;&#38024;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#24207;&#21015;&#25193;&#23637;&#20102;&#26089;&#26399;&#20998;&#31867;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;AUC&#26041;&#38754;&#26377;&#39640;&#36798;8.7%&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.01151</link><description>&lt;p&gt;
&#26089;&#26399;&#20998;&#31867;&#22810;&#27169;&#24577;&#25968;&#25454;&#24207;&#21015;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Early Classifying Multimodal Sequences. (arXiv:2305.01151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30740;&#31350;&#38024;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#24207;&#21015;&#25193;&#23637;&#20102;&#26089;&#26399;&#20998;&#31867;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;AUC&#26041;&#38754;&#26377;&#39640;&#36798;8.7%&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#20154;&#20204;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36880;&#27493;&#25910;&#38598;&#20449;&#24687;&#12290;&#20160;&#20040;&#26102;&#20505;&#25910;&#38598;&#21040;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#20998;&#31867;&#65311;&#20026;&#20102;&#22312;&#20915;&#31574;&#30340;&#30830;&#23450;&#24615;&#21644;&#31561;&#24453;&#26102;&#38388;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#26089;&#26399;&#20998;&#31867;&#38382;&#39064;&#24050;&#25104;&#20026;&#36866;&#24212;&#26356;&#21160;&#24577;&#29615;&#22659;&#30340;&#20998;&#31867;&#26041;&#27861;&#30340;&#20851;&#38190;&#25152;&#22312;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30340;&#32467;&#26524;&#20165;&#38480;&#20110;&#21333;&#19968;&#27169;&#24577;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#21021;&#27493;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#29616;&#26377;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#20998;&#31867;&#22810;&#27169;&#24577;&#24207;&#21015;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#30340;&#23454;&#39564;AUC&#20248;&#21183;&#39640;&#36798;8.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Often pieces of information are received sequentially over time. When did one collect enough such pieces to classify? Trading wait time for decision certainty leads to early classification problems that have recently gained attention as a means of adapting classification to more dynamic environments. However, so far results have been limited to unimodal sequences. In this pilot study, we expand into early classifying multimodal sequences by combining existing methods. We show our new method yields experimental AUC advantages of up to 8.7%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;RKGCN&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#20998;&#26512;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01147</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ripple Knowledge Graph Convolutional Networks For Recommendation Systems. (arXiv:2305.01147v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;RKGCN&#65292;&#23427;&#33021;&#22815;&#21160;&#24577;&#20998;&#26512;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#65292;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#36741;&#21161;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#20915;&#31574;&#33021;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;RKGCN&#65292;&#23427;&#21160;&#24577;&#20998;&#26512;&#27599;&#20010;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#25512;&#33616;&#20986;&#21512;&#36866;&#30340;&#29289;&#21697;&#12290;&#23427;&#22312;&#29289;&#21697;&#21644;&#29992;&#25143;&#21452;&#26041;&#38754;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#20016;&#23500;&#23427;&#20204;&#30340;&#34920;&#31034;&#65292;&#26368;&#22823;&#21270;&#30693;&#35782;&#22270;&#35889;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#30340;&#21033;&#29992;&#12290; RKGCN&#33021;&#22815;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#25552;&#20379;&#26356;&#20010;&#24615;&#21270;&#21644;&#30456;&#20851;&#30340;&#25512;&#33616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21253;&#25324;&#30005;&#24433;&#12289;&#20070;&#31821;&#21644;&#38899;&#20048;&#22312;&#20869;&#30340;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;5&#20010;&#22522;&#20934;&#27169;&#22411;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using knowledge graphs to assist deep learning models in making recommendation decisions has recently been proven to effectively improve the model's interpretability and accuracy. This paper introduces an end-to-end deep learning model, named RKGCN, which dynamically analyses each user's preferences and makes a recommendation of suitable items. It combines knowledge graphs on both the item side and user side to enrich their representations to maximize the utilization of the abundant information in knowledge graphs. RKGCN is able to offer more personalized and relevant recommendations in three different scenarios. The experimental results show the superior effectiveness of our model over 5 baseline models on three real-world datasets including movies, books, and music.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#26680;&#21270;Renyi&#29109;&#65292;&#29992;&#20110;&#22312;&#19981;&#20551;&#35774;Lipschitz&#25110;&#20984;&#24615;&#26465;&#20214;&#30340;&#21069;&#25552;&#19979;&#23545;SGD / SGLD&#31561;&#19979;&#38477;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#24403;&#21069;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#30340;&#20248;&#21270;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.01143</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#29702;&#35299;&#65306;&#22522;&#20110;&#26680;&#21270;Renyi&#29109;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Understanding the Generalization Ability of Deep Learning Algorithms: A Kernelized Renyi's Entropy Perspective. (arXiv:2305.01143v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#26680;&#21270;Renyi&#29109;&#65292;&#29992;&#20110;&#22312;&#19981;&#20551;&#35774;Lipschitz&#25110;&#20984;&#24615;&#26465;&#20214;&#30340;&#21069;&#25552;&#19979;&#23545;SGD / SGLD&#31561;&#19979;&#38477;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#24403;&#21069;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#30340;&#20248;&#21270;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20449;&#24687;&#29702;&#35770;&#20998;&#26512;&#24050;&#25104;&#20026;&#29702;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#34892;&#20026;&#30340;&#27969;&#34892;&#26694;&#26550;&#12290;&#23427;&#20801;&#35768;&#23545;&#38543;&#26426;&#26799;&#24230;/ Langevin&#19979;&#38477;&#65288;SGD / SGLD&#65289;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#30452;&#25509;&#20998;&#26512;&#65292;&#32780;&#26080;&#38656;&#35832;&#22914;Lipschitz&#25110;&#20984;&#24615;&#26465;&#20214;&#31561;&#24378;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#30340;&#24403;&#21069;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#20173;&#28982;&#36828;&#38750;&#26368;&#20248;&#65292;&#32780;&#23545;&#36825;&#20123;&#30028;&#38480;&#30340;&#23454;&#36136;&#24615;&#25913;&#36827;&#30001;&#20110;&#39640;&#32500;&#20449;&#24687;&#37327;&#30340;&#19981;&#21487;&#35745;&#31639;&#24615;&#32780;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#29702;&#12290;&#35770;&#34913;&#37327;&#65306;&#22522;&#20110;&#26680;&#21270;Renyi&#29109;&#65292;&#21033;&#29992;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#31639;&#23376;&#34920;&#31034;&#12290;&#23427;&#32487;&#25215;&#20102;&#39321;&#20892;&#29109;&#30340;&#23646;&#24615;&#65292;&#21487;&#36890;&#36807;&#31616;&#21333;&#30340;&#38543;&#26426;&#25277;&#26679;&#36827;&#34892;&#26377;&#25928;&#35745;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#29420;&#31435;&#20110;&#36755;&#20837;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#26680;&#21270;Renyi&#29109;&#19979;&#24314;&#31435;&#20102;SGD / SGLD&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#20854;&#20013;&#30456;&#20114;&#20449;&#24687;...
&lt;/p&gt;
&lt;p&gt;
Recently, information theoretic analysis has become a popular framework for understanding the generalization behavior of deep neural networks. It allows a direct analysis for stochastic gradient/Langevin descent (SGD/SGLD) learning algorithms without strong assumptions such as Lipschitz or convexity conditions. However, the current generalization error bounds within this framework are still far from optimal, while substantial improvements on these bounds are quite challenging due to the intractability of high-dimensional information quantities. To address this issue, we first propose a novel information theoretical measure: kernelized Renyi's entropy, by utilizing operator representation in Hilbert space. It inherits the properties of Shannon's entropy and can be effectively calculated via simple random sampling, while remaining independent of the input dimension. We then establish the generalization error bounds for SGD/SGLD under kernelized Renyi's entropy, where the mutual informati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#30340;&#20960;&#20309;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;GeoLDM&#65289;&#65292;&#26159;&#20998;&#23376;&#20960;&#20309;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#31435;&#20855;&#26377;&#19981;&#21464;&#26631;&#37327;&#21644;&#31561;&#21464;&#24352;&#37327;&#30340;&#28857;&#32467;&#26500;&#28508;&#22312;&#31354;&#38388;&#26469;&#25429;&#25417;&#20854;&#20851;&#38190;&#30340;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#32422;&#26463;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01140</link><description>&lt;p&gt;
&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#30340;&#20960;&#20309;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Geometric Latent Diffusion Models for 3D Molecule Generation. (arXiv:2305.01140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01140
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#30340;&#20960;&#20309;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;GeoLDM&#65289;&#65292;&#26159;&#20998;&#23376;&#20960;&#20309;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24314;&#31435;&#20855;&#26377;&#19981;&#21464;&#26631;&#37327;&#21644;&#31561;&#21464;&#24352;&#37327;&#30340;&#28857;&#32467;&#26500;&#28508;&#22312;&#31354;&#38388;&#26469;&#25429;&#25417;&#20854;&#20851;&#38190;&#30340;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#32422;&#26463;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#24050;&#32463;&#22312;&#29983;&#25104;&#23500;&#26377;&#29305;&#24449;&#30340;&#20960;&#20309;&#22270;&#24418;&#21644;&#25512;&#36827;&#20998;&#23376;&#35774;&#35745;&#31561;&#22522;&#30784;&#31185;&#23398;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#21463;&#21040;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#31283;&#23450;&#24615;&#30340;&#26368;&#26032;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21407;&#21017;&#30340;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;&#20960;&#20309;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;GeoLDM&#65289;&#12290;GeoLDM&#26159;&#20998;&#23376;&#20960;&#20309;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;&#28508;&#22312;DM&#27169;&#22411;&#65292;&#30001;&#23558;&#32467;&#26500;&#32534;&#30721;&#20026;&#36830;&#32493;&#28508;&#22312;&#30721;&#30340;&#33258;&#32534;&#30721;&#22120;&#21644;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#25805;&#20316;&#30340;DM&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#23545;&#20110;&#19977;&#32500;&#20998;&#23376;&#20960;&#20309;&#24314;&#27169;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#20855;&#26377;&#19981;&#21464;&#26631;&#37327;&#21644;&#31561;&#21464;&#24352;&#37327;&#30340;&#28857;&#32467;&#26500;&#28508;&#22312;&#31354;&#38388;&#26469;&#25429;&#25417;&#20854;&#20851;&#38190;&#30340;&#26059;&#36716;&#24179;&#31227;&#31561;&#21464;&#24615;&#32422;&#26463;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GeoLDM&#22312;&#22810;&#20010;&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#33021;&#22815;&#31283;&#23450;&#22320;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#22312;&#22823;&#22411;&#21452;&#36739;&#24046;&#20998;&#23376;&#30340;&#26377;&#25928;&#30334;&#20998;&#27604;&#26041;&#38754;&#25552;&#39640;&#20102;7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models, especially diffusion models (DMs), have achieved promising results for generating feature-rich geometries and advancing foundational science problems such as molecule design. Inspired by the recent huge success of Stable (latent) Diffusion models, we propose a novel and principled method for 3D molecule generation named Geometric Latent Diffusion Models (GeoLDM). GeoLDM is the first latent DM model for the molecular geometry domain, composed of autoencoders encoding structures into continuous latent codes and DMs operating in the latent space. Our key innovation is that for modeling the 3D molecular geometries, we capture its critical roto-translational equivariance constraints by building a point-structured latent space with both invariant scalars and equivariant tensors. Extensive experiments demonstrate that GeoLDM can consistently achieve better performance on multiple molecule generation benchmarks, with up to 7\% improvement for the valid percentage of large bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#19968;&#33268;&#39044;&#27979;&#30340;&#25298;&#32477;&#23545;&#25239;&#35757;&#32451;&#65288;CPR&#65289;&#65292;&#29992;&#20110;&#26500;&#24314;&#40065;&#26834;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20998;&#23618;&#25298;&#32477;&#35774;&#32622;&#19979;&#36827;&#34892;&#23545;&#25239;&#40065;&#26834;&#20998;&#31867;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01139</link><description>&lt;p&gt;
&#20998;&#23618;&#23545;&#25239;&#40065;&#26834;&#24615;&#19982;&#25298;&#32477;
&lt;/p&gt;
&lt;p&gt;
Stratified Adversarial Robustness with Rejection. (arXiv:2305.01139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#19968;&#33268;&#39044;&#27979;&#30340;&#25298;&#32477;&#23545;&#25239;&#35757;&#32451;&#65288;CPR&#65289;&#65292;&#29992;&#20110;&#26500;&#24314;&#40065;&#26834;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20998;&#23618;&#25298;&#32477;&#35774;&#32622;&#19979;&#36827;&#34892;&#23545;&#25239;&#40065;&#26834;&#20998;&#31867;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#22320;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#26041;&#27861;&#8212;&#8212;&#25298;&#32477;&#39044;&#27979;&#65292;&#29992;&#20110;&#22686;&#24378;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#25298;&#32477;&#39044;&#27979;&#20250;&#24102;&#26469;&#19968;&#23450;&#30340;&#25104;&#26412;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#23558;&#34987;&#25200;&#21160;&#30340;&#36755;&#20837;&#30340;&#25298;&#32477;&#19982;&#38646;&#25104;&#26412;&#30456;&#20851;&#32852;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#25298;&#32477;&#22823;&#37327;&#21487;&#20197;&#34987;&#27491;&#30830;&#20998;&#31867;&#30340;&#36731;&#24230;&#25200;&#21160;&#36755;&#20837;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#23618;&#25298;&#32477;&#35774;&#32622;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#20998;&#31867;&#65292;&#24182;&#19988;&#36890;&#36807;&#25298;&#32477;&#25439;&#22833;&#20989;&#25968;&#22312;&#25200;&#21160;&#24133;&#24230;&#19978;&#21333;&#35843;&#19981;&#20943;&#22320;&#24314;&#27169;&#26469;&#27169;&#25311;&#25298;&#32477;&#25104;&#26412;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#20998;&#23618;&#25298;&#32477;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#19968;&#33268;&#39044;&#27979;&#30340;&#25298;&#32477;&#23545;&#25239;&#35757;&#32451;&#65288;CPR&#65289;&#8212;&#8212;&#29992;&#20110;&#26500;&#24314;&#40065;&#26834;&#30340;&#36873;&#25321;&#24615;&#20998;&#31867;&#22120;&#12290;&#38024;&#23545;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24378;&#36866;&#24212;&#24615;&#19979;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there is an emerging interest in adversarially training a classifier with a rejection option (also known as a selective classifier) for boosting adversarial robustness. While rejection can incur a cost in many applications, existing studies typically associate zero cost with rejecting perturbed inputs, which can result in the rejection of numerous slightly-perturbed inputs that could be correctly classified. In this work, we study adversarially-robust classification with rejection in the stratified rejection setting, where the rejection cost is modeled by rejection loss functions monotonically non-increasing in the perturbation magnitude. We theoretically analyze the stratified rejection setting and propose a novel defense method -- Adversarial Training with Consistent Prediction-based Rejection (CPR) -- for building a robust selective classifier. Experiments on image datasets demonstrate that the proposed method significantly outperforms existing methods under strong adaptiv
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;PGrad&#26041;&#27861;&#26469;&#23398;&#20064;&#20027;&#23548;&#26799;&#24230;&#65292;&#25552;&#39640;&#27169;&#22411;&#22312;&#26410;&#30693;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#24573;&#30053;&#39046;&#22495;&#30456;&#20851;&#30340;&#22122;&#22768;&#20449;&#21495;&#65292;&#35813;&#26041;&#27861;&#22312;DomainBed&#21644;WILDS&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01134</link><description>&lt;p&gt;
PGrad: &#23398;&#20064;&#20027;&#23548;&#26799;&#24230;&#20197;&#36827;&#34892;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
PGrad: Learning Principal Gradients For Domain Generalization. (arXiv:2305.01134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01134
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;PGrad&#26041;&#27861;&#26469;&#23398;&#20064;&#20027;&#23548;&#26799;&#24230;&#65292;&#25552;&#39640;&#27169;&#22411;&#22312;&#26410;&#30693;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#24573;&#30053;&#39046;&#22495;&#30456;&#20851;&#30340;&#22122;&#22768;&#20449;&#21495;&#65292;&#35813;&#26041;&#27861;&#22312;DomainBed&#21644;WILDS&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#38754;&#23545;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#65288;OOD&#65289;&#30340;&#39046;&#22495;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;DG&#35757;&#32451;&#31574;&#30053;&#65292;&#31216;&#20026;PGrad&#65292;&#20197;&#23398;&#20064;&#19968;&#20010;&#24378;&#20581;&#30340;&#26799;&#24230;&#26041;&#21521;&#65292;&#25552;&#39640;&#27169;&#22411;&#22312;&#26410;&#30693;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26799;&#24230;&#32858;&#21512;&#20102;&#19968;&#20010;&#37319;&#26679;&#30340;roll-out&#20248;&#21270;&#36712;&#36857;&#30340;&#20027;&#23548;&#26041;&#21521;&#65292;&#35813;&#36712;&#36857;&#27979;&#37327;&#20102;&#22312;&#25152;&#26377;&#35757;&#32451;&#39046;&#22495;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;PGrad&#26799;&#24230;&#35774;&#35745;&#24378;&#21046;DG&#35757;&#32451;&#24573;&#30053;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#22122;&#22768;&#20449;&#21495;&#65292;&#24182;&#29992;&#35206;&#30422;&#21442;&#25968;&#21160;&#24577;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#30340;&#24378;&#20581;&#26041;&#21521;&#26356;&#26032;&#25152;&#26377;&#35757;&#32451;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#21452;&#23556;&#22522;&#20110;&#35745;&#31639;&#30340;&#31934;&#28860;&#21644;&#22522;&#20110;&#26041;&#21521;&#21152;&#38271;&#24230;&#30340;&#26657;&#20934;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;PGrad&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#23558;PGrad&#19982;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;Hessian&#30340;&#35889;&#20998;&#26512;&#30456;&#36830;&#25509;&#12290;DomainBed&#21644;WILDS&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#24378;&#20581;&#30340;DG&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models fail to perform when facing out-of-distribution (OOD) domains, a challenging task known as domain generalization (DG). In this work, we develop a novel DG training strategy, we call PGrad, to learn a robust gradient direction, improving models' generalization ability on unseen domains. The proposed gradient aggregates the principal directions of a sampled roll-out optimization trajectory that measures the training dynamics across all training domains. PGrad's gradient design forces the DG training to ignore domain-dependent noise signals and updates all training domains with a robust direction covering main components of parameter dynamics. We further improve PGrad via bijection-based computational refinement and directional plus length-based calibrations. Our theoretical proof connects PGrad to the spectral analysis of Hessian in training neural networks. Experiments on DomainBed and WILDS benchmarks demonstrate that our approach effectively enables robust DG o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#20381;&#36182;&#32467;&#26500;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;TGN&#27169;&#22411;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.01128</link><description>&lt;p&gt;
&#19981;&#21516;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#22312;&#21160;&#24577;&#22270;&#19978;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of different temporal graph neural network configurations on dynamic graphs. (arXiv:2305.01128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#20381;&#36182;&#32467;&#26500;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;TGN&#27169;&#22411;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20998;&#26512;&#21160;&#24577;&#22270;&#65288;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#22270;&#65289;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGN&#65289;&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#21160;&#24577;&#22270;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20173;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36825;&#20123;TGN&#27169;&#22411;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#25628;&#32034;&#20173;&#22312;&#36827;&#34892;&#20013;&#12290;&#36817;&#26399;&#65292;Pytorch Geometric Temporal&#25552;&#20379;&#20102;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#22823;&#22810;&#25968;&#36824;&#27809;&#26377;&#29992;&#19981;&#21516;&#30340;TGN&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#20197;&#24314;&#31435;&#26368;&#20808;&#36827;&#30340;&#29366;&#20917;&#12290;&#22240;&#27492;&#65292;&#26412;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23545;&#21160;&#24577;&#22270;&#19978;&#30340;&#26102;&#31354;&#20381;&#36182;&#32467;&#26500;&#23398;&#20064;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#24182;&#27604;&#36739;&#36873;&#23450;TGN&#27169;&#22411;&#22312;&#33410;&#28857;&#21644;&#36793;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#23545;&#26368;&#20339;&#34920;&#29616;TGN&#27169;&#22411;&#30340;&#19981;&#21516;&#21464;&#20307;&#36827;&#34892;&#24191;&#27867;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#20197;&#25506;&#32034;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an increasing interest in the use of graph neural networks (GNNs) for analyzing dynamic graphs, which are graphs that evolve over time. However, there is still a lack of understanding of how different temporal graph neural network (TGNs) configurations can impact the accuracy of predictions on dynamic graphs. Moreover, the hunt for benchmark datasets for these TGNs models is still ongoing. Up until recently, Pytorch Geometric Temporal came up with a few benchmark datasets but most of these datasets have not been analyzed with different TGN models to establish the state-of-the-art. Therefore, this project aims to address this gap in the literature by performing a qualitative analysis of spatial-temporal dependence structure learning on dynamic graphs, as well as a comparative study of the effectiveness of selected TGNs on node and edge prediction tasks. Additionally, an extensive ablation study will be conducted on different variants of the best-performin
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25511;&#33258;&#36866;&#24212;&#22810;&#20998;&#36776;&#29575;&#29289;&#29702;&#27169;&#25311;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#36866;&#24403;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#23558;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#32473;&#39640;&#24230;&#21160;&#24577;&#21306;&#22495;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01122</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#25511;&#33258;&#36866;&#24212;&#22810;&#20998;&#36776;&#29575;&#29289;&#29702;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Learning Controllable Adaptive Simulation for Multi-resolution Physics. (arXiv:2305.01122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01122
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25511;&#33258;&#36866;&#24212;&#22810;&#20998;&#36776;&#29575;&#29289;&#29702;&#27169;&#25311;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#36866;&#24403;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#23558;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#32473;&#39640;&#24230;&#21160;&#24577;&#21306;&#22495;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#38382;&#39064;&#20013;&#65292;&#27169;&#25311;&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#27169;&#25311;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#38590;&#39064;&#26159;&#23427;&#20204;&#30340;&#22810;&#20998;&#36776;&#29575;&#21160;&#24577;&#29305;&#24615;&#65306;&#31995;&#32479;&#30340;&#19968;&#23567;&#37096;&#20998;&#38750;&#24120;&#21160;&#24577;&#65292;&#38656;&#35201;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#20998;&#36776;&#29575;&#65292;&#32780;&#22823;&#22810;&#25968;&#31995;&#32479;&#21464;&#21270;&#32531;&#24930;&#65292;&#21487;&#20197;&#29992;&#36739;&#31895;&#30340;&#31354;&#38388;&#23610;&#24230;&#26469;&#24314;&#27169;&#12290;&#20856;&#22411;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#20351;&#29992;&#32479;&#19968;&#30340;&#31354;&#38388;&#23610;&#24230;&#65292;&#38656;&#35201;&#35299;&#26512;&#21040;&#26368;&#32454;&#30340;&#23610;&#24230;&#65292;&#24182;&#19988;&#21487;&#33021;&#28010;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#26469;&#36798;&#21040;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#21487;&#25511;&#33258;&#36866;&#24212;&#22810;&#20998;&#36776;&#29575;&#29289;&#29702;&#27169;&#25311;&#65288;LAMP&#65289;&#30340;&#27169;&#22411;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#28436;&#21270;&#27169;&#22411;&#24182;&#20248;&#21270;&#36866;&#24403;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;&#23558;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#32473;&#39640;&#24230;&#21160;&#24577;&#21306;&#22495;&#12290;LAMP&#30001;&#29992;&#20110;&#23398;&#20064;&#27491;&#21521;&#28436;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#22522;&#20110;GNN&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#29992;&#20110;&#23398;&#20064;&#31354;&#38388;&#20998;&#36776;&#29575;&#25511;&#21046;&#31574;&#30053;&#20004;&#37096;&#20998;&#32452;&#25104;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#29289;&#29702;&#31995;&#32479;&#19978;&#23637;&#31034;&#20102;LAMP&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#20248;&#20110;&#32479;&#19968;&#20998;&#36776;&#29575;&#26367;&#20195;&#27169;&#22411;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating the time evolution of physical systems is pivotal in many scientific and engineering problems. An open challenge in simulating such systems is their multi-resolution dynamics: a small fraction of the system is extremely dynamic, and requires very fine-grained resolution, while a majority of the system is changing slowly and can be modeled by coarser spatial scales. Typical learning-based surrogate models use a uniform spatial scale, which needs to resolve to the finest required scale and can waste a huge compute to achieve required accuracy. In this work, we introduce Learning controllable Adaptive simulation for Multi-resolution Physics (LAMP) as the first full deep learning-based surrogate model that jointly learns the evolution model and optimizes appropriate spatial resolutions that devote more compute to the highly dynamic regions. LAMP consists of a Graph Neural Network (GNN) for learning the forward evolution, and a GNN-based actor-critic for learning the policy of sp
&lt;/p&gt;</description></item><item><title>CSP&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#65292;&#23398;&#20064;&#22320;&#29702;&#20301;&#32622;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01118</link><description>&lt;p&gt;
CSP&#65306;&#38024;&#23545;&#22320;&#29702;&#31354;&#38388;&#35270;&#35273;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#31354;&#38388;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations. (arXiv:2305.01118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01118
&lt;/p&gt;
&lt;p&gt;
CSP&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#65292;&#23398;&#20064;&#22320;&#29702;&#20301;&#32622;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#20844;&#24320;&#21487;&#29992;&#65292;&#32780;&#23545;&#35937;&#31867;&#21035;&#31561;&#26631;&#31614;&#21017;&#30456;&#23545;&#31232;&#32570;&#19988;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#12290;&#21516;&#26102;&#65292;&#23545;&#27604;&#23398;&#20064;&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20165;&#38656;&#24456;&#23569;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#26159;&#21306;&#20998;&#35270;&#35273;&#19978;&#30456;&#20284;&#30340;&#23545;&#35937;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#22312;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#38454;&#27573;&#30452;&#25509;&#21033;&#29992;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20016;&#23500;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; Contrastive Spatial Pre-Training&#65288;CSP&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#20998;&#21035;&#23545;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#22320;&#29702;&#20301;&#32622;&#36827;&#34892;&#32534;&#30721;&#65292;&#21033;&#29992;&#23545;&#27604;&#30446;&#26631;&#20174;&#22270;&#20687;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#20301;&#32622;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#36716;&#31227;&#21040;&#19979;&#28216;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CSP&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged im- ages. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#29305;&#24449;&#34701;&#21512;&#30340;&#34892;&#20154;&#24847;&#21521;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#34892;&#20154;&#21644;&#20132;&#36890;&#19978;&#19979;&#25991;&#30340;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.01111</link><description>&lt;p&gt;
&#34892;&#20154;&#24847;&#21521;&#39044;&#27979;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#29305;&#24449;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Local and Global Contextual Features Fusion for Pedestrian Intention Prediction. (arXiv:2305.01111v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#29305;&#24449;&#34701;&#21512;&#30340;&#34892;&#20154;&#24847;&#21521;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#34892;&#20154;&#21644;&#20132;&#36890;&#19978;&#19979;&#25991;&#30340;&#35270;&#35273;&#29305;&#24449;&#26469;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#36947;&#36335;&#19978;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#27491;&#22312;&#25104;&#20026;&#26410;&#26469;&#20132;&#36890;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#20294;&#26159;&#65292;&#23433;&#20840;&#25361;&#25112;&#21644;&#32570;&#20047;&#21487;&#38752;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#12290;&#19982;&#34892;&#20154;&#30340;&#20132;&#20114;&#21253;&#25324;&#8220;&#39044;&#27979;&#34892;&#20154;&#30340;&#31359;&#36234;&#24847;&#21521;&#8221;&#20540;&#24471;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#36947;&#36335;&#19978;&#30340;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#21462;&#21644;&#20998;&#26512;&#34892;&#20154;&#21644;&#20132;&#36890;&#19978;&#19979;&#25991;&#30340;&#26102;&#31354;&#35270;&#35273;&#29305;&#24449;&#36827;&#34892;&#26377;&#25928;&#30340;&#24847;&#21521;&#39044;&#27979;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles (AVs) are becoming an indispensable part of future transportation. However, safety challenges and lack of reliability limit their real-world deployment. Towards boosting the appearance of AVs on the roads, the interaction of AVs with pedestrians including "prediction of the pedestrian crossing intention" deserves extensive research. This is a highly challenging task as involves multiple non-linear parameters. In this direction, we extract and analyse spatio-temporal visual features of both pedestrian and traffic contexts. The pedestrian features include body pose and local context features that represent the pedestrian's behaviour. Additionally, to understand the global context, we utilise location, motion, and environmental information using scene parsing technology that represents the pedestrian's surroundings, and may affect the pedestrian's intention. Finally, these multi-modality features are intelligently fused for effective intention prediction learning. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26448;&#26009;&#21457;&#29616;&#26694;&#26550;&#65292;&#21033;&#29992;&#26448;&#26009;&#31185;&#23398;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23884;&#20837;&#20316;&#20026;&#26448;&#26009;&#30340;&#32452;&#25104;&#21644;&#32467;&#26500;&#29305;&#24449;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#19988;&#32852;&#21512;&#37319;&#29992;&#20102;&#34920;&#31034;&#30456;&#20284;&#24615;&#21484;&#22238;&#20505;&#36873;&#26448;&#26009;&#21644;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#20505;&#36873;&#26448;&#26009;&#36827;&#34892;&#30446;&#26631;&#23646;&#24615;&#25490;&#21517;&#30340;&#26041;&#26696;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#24191;&#38420;&#30340;&#26448;&#26009;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#30830;&#23450;&#39640;&#24615;&#33021;&#20505;&#36873;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2305.01101</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#26448;&#26009;&#25512;&#33616;&#12289;&#25490;&#21517;&#21644;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Leveraging Language Representation for Material Recommendation, Ranking, and Exploration. (arXiv:2305.01101v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26448;&#26009;&#21457;&#29616;&#26694;&#26550;&#65292;&#21033;&#29992;&#26448;&#26009;&#31185;&#23398;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23884;&#20837;&#20316;&#20026;&#26448;&#26009;&#30340;&#32452;&#25104;&#21644;&#32467;&#26500;&#29305;&#24449;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#19988;&#32852;&#21512;&#37319;&#29992;&#20102;&#34920;&#31034;&#30456;&#20284;&#24615;&#21484;&#22238;&#20505;&#36873;&#26448;&#26009;&#21644;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#20505;&#36873;&#26448;&#26009;&#36827;&#34892;&#30446;&#26631;&#23646;&#24615;&#25490;&#21517;&#30340;&#26041;&#26696;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#24191;&#38420;&#30340;&#26448;&#26009;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#30830;&#23450;&#39640;&#24615;&#33021;&#20505;&#36873;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#20852;&#25216;&#26415;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26448;&#26009;&#21457;&#29616;&#21644;&#35774;&#35745;&#24050;&#32463;&#24471;&#21040;&#20102;&#21152;&#36895;&#12290;&#34429;&#28982;&#22312;&#23398;&#20064;&#26448;&#26009;&#32467;&#26500;&#19982;&#24615;&#36136;&#20851;&#31995;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#33021;&#22815;&#26377;&#25928;&#25506;&#32034;&#24191;&#38420;&#30340;&#26448;&#26009;&#25628;&#32034;&#31354;&#38388;&#24182;&#30830;&#23450;&#39640;&#24615;&#33021;&#20505;&#36873;&#26448;&#26009;&#30340;&#26041;&#27861;&#20173;&#28982;&#21313;&#20998;&#26377;&#38480;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26448;&#26009;&#21457;&#29616;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20174;&#26448;&#26009;&#31185;&#23398;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#33258;&#28982;&#35821;&#35328;&#23884;&#20837;&#20316;&#20026;&#32452;&#25104;&#21644;&#32467;&#26500;&#29305;&#24449;&#30340;&#34920;&#31034;&#12290;&#35813;&#21457;&#29616;&#26694;&#26550;&#30001;&#19968;&#20010;&#32852;&#21512;&#26041;&#26696;&#32452;&#25104;&#65292;&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#26448;&#26009;&#65292;&#39318;&#20808;&#22522;&#20110;&#34920;&#31034;&#30456;&#20284;&#24615;&#21484;&#22238;&#20505;&#36873;&#26448;&#26009;&#65292;&#20877;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#20505;&#36873;&#26448;&#26009;&#36827;&#34892;&#30446;&#26631;&#23646;&#24615;&#25490;&#21517;&#12290;&#35821;&#35328;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#34987;&#21457;&#29616;&#21487;&#20197;&#20256;&#36798;&#26377;&#20851;&#26448;&#26009;&#24615;&#36136;&#21644;&#32467;&#26500;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#39640;&#24615;&#33021;&#26448;&#26009;&#21457;&#29616;&#21464;&#24471;&#26356;&#21152;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approaches for material discovery and design have been accelerated by emerging efforts in machine learning. While there is enormous progress towards learning the structure to property relationship of materials, methods that allow for general representations of crystals to effectively explore the vast material search space and identify high-performance candidates remain limited. In this work, we introduce a material discovery framework that uses natural language embeddings derived from material science-specific language models as representations of compositional and structural features. The discovery framework consists of a joint scheme that, given a query material, first recalls candidates based on representational similarity, and ranks the candidates based on target properties through multi-task learning. The contextual knowledge encoded in language representations is found to convey information about material properties and structures, enabling both similarity analysis fo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24076;&#33098;&#35821;&#35821;&#35328;&#23398;&#20013;&#30340;&#38382;&#39064;&#65292;&#25104;&#21151;&#21033;&#29992;BERT&#27169;&#22411;&#21457;&#29616;&#21644;&#32416;&#27491;&#20102;&#25220;&#20889;&#21592;&#22312;&#25991;&#26412;&#20256;&#36882;&#36807;&#31243;&#20013;&#26410;&#34987;&#21457;&#29616;&#30340;&#38169;&#35823;&#65292;&#24182;&#33021;&#22312;&#20462;&#22797;&#39044;&#29616;&#20195;&#25163;&#31295;&#26448;&#26009;&#32769;&#21270;&#24341;&#36215;&#30340;&#20449;&#24687;&#32570;&#22833;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#22312;&#39046;&#22495;&#19987;&#23478;&#19982;&#27169;&#22411;&#21512;&#20316;&#26102;&#65292;&#26368;&#20339;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#21551;&#31034;&#24615;&#24314;&#35758;&#23454;&#29616;&#12290;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#22836;&#20284;&#20046;&#32534;&#30721;&#20102;&#39044;&#29616;&#20195;&#24076;&#33098;&#35821;&#30340;&#36873;&#25321;&#24615;&#35821;&#27861;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.01099</link><description>&lt;p&gt;
Logion&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24076;&#33098;&#35821;&#35821;&#35328;&#23398;
&lt;/p&gt;
&lt;p&gt;
Logion: Machine Learning for Greek Philology. (arXiv:2305.01099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24076;&#33098;&#35821;&#35821;&#35328;&#23398;&#20013;&#30340;&#38382;&#39064;&#65292;&#25104;&#21151;&#21033;&#29992;BERT&#27169;&#22411;&#21457;&#29616;&#21644;&#32416;&#27491;&#20102;&#25220;&#20889;&#21592;&#22312;&#25991;&#26412;&#20256;&#36882;&#36807;&#31243;&#20013;&#26410;&#34987;&#21457;&#29616;&#30340;&#38169;&#35823;&#65292;&#24182;&#33021;&#22312;&#20462;&#22797;&#39044;&#29616;&#20195;&#25163;&#31295;&#26448;&#26009;&#32769;&#21270;&#24341;&#36215;&#30340;&#20449;&#24687;&#32570;&#22833;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#22312;&#39046;&#22495;&#19987;&#23478;&#19982;&#27169;&#22411;&#21512;&#20316;&#26102;&#65292;&#26368;&#20339;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#21551;&#31034;&#24615;&#24314;&#35758;&#23454;&#29616;&#12290;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#22836;&#20284;&#20046;&#32534;&#30721;&#20102;&#39044;&#29616;&#20195;&#24076;&#33098;&#35821;&#30340;&#36873;&#25321;&#24615;&#35821;&#27861;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24076;&#33098;&#35821;&#35821;&#35328;&#23398;&#20013;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#36804;&#20170;&#20026;&#27490;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#26368;&#22823;&#39044;&#29616;&#20195;&#24076;&#33098;&#35821;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;BERT&#27169;&#22411;&#65292;&#21457;&#29616;&#24182;&#32416;&#27491;&#20102;&#25220;&#20889;&#21592;&#22312;&#25991;&#26412;&#20256;&#36882;&#36807;&#31243;&#20013;&#26410;&#34987;&#21457;&#29616;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#22635;&#34917;&#30001;&#20110;&#39044;&#29616;&#20195;&#25163;&#31295;&#26448;&#26009;&#32769;&#21270;&#23548;&#33268;&#30340;&#32570;&#21475;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#39046;&#22495;&#19987;&#23478;&#24471;&#21040;&#27169;&#22411;&#30340;&#21551;&#31034;&#24615;&#24314;&#35758;&#26102;&#65292;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#36825;&#31181;&#20154;&#19982;&#35745;&#31639;&#26426;&#30340;&#21512;&#20316;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21457;&#29616;&#19968;&#20123;&#27880;&#24847;&#21147;&#22836;&#20284;&#20046;&#32534;&#30721;&#20102;&#39044;&#29616;&#20195;&#24076;&#33098;&#35821;&#30340;&#36873;&#25321;&#24615;&#35821;&#27861;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents machine-learning methods to address various problems in Greek philology. After training a BERT model on the largest premodern Greek dataset used for this purpose to date, we identify and correct previously undetected errors made by scribes in the process of textual transmission, in what is, to our knowledge, the first successful identification of such errors via machine learning. Additionally, we demonstrate the model's capacity to fill gaps caused by material deterioration of premodern manuscripts and compare the model's performance to that of a domain expert. We find that best performance is achieved when the domain expert is provided with model suggestions for inspiration. With such human-computer collaborations in mind, we explore the model's interpretability and find that certain attention heads appear to encode select grammatical features of premodern Greek.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#20316;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#20013;&#39550;&#39542;&#21592;&#21464;&#36947;&#39044;&#27979;&#38382;&#39064;&#12290;&#20351;&#29992;&#36710;&#21040;&#36710;&#36890;&#20449;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#36710;&#36742;&#30340;&#21152;&#36895;&#24230;&#20449;&#24687;&#65292;&#36890;&#36807;LSTM&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#24403;&#21608;&#22260;&#36710;&#36742;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.01096</link><description>&lt;p&gt;
&#21512;&#20316;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#39550;&#39542;&#21592;&#21464;&#36947;&#39044;&#27979;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Novel Model for Driver Lane Change Prediction in Cooperative Adaptive Cruise Control Systems. (arXiv:2305.01096v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21512;&#20316;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#20013;&#39550;&#39542;&#21592;&#21464;&#36947;&#39044;&#27979;&#38382;&#39064;&#12290;&#20351;&#29992;&#36710;&#21040;&#36710;&#36890;&#20449;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#36710;&#36742;&#30340;&#21152;&#36895;&#24230;&#20449;&#24687;&#65292;&#36890;&#36807;LSTM&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#24403;&#21608;&#22260;&#36710;&#36742;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#20934;&#30830;&#24615;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#21464;&#36947;&#39044;&#27979;&#21487;&#20197;&#20943;&#23569;&#28508;&#22312;&#20107;&#25925;&#65292;&#25552;&#39640;&#36947;&#36335;&#23433;&#20840;&#24615;&#12290;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#65292;&#36710;&#36947;&#20559;&#31163;&#35686;&#21578;&#65288;LDA&#65289;&#21644;&#36710;&#36947;&#20445;&#25345;&#36741;&#21161;&#65288;LKA&#65289;&#26159;&#19968;&#20123;&#39640;&#32423;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#65288;ADAS&#65289;&#30340;&#20256;&#32479;&#27169;&#22359;&#12290;&#26377;&#20102;&#36710;&#21040;&#36710;&#36890;&#20449;&#65288;V2V&#65289;&#65292;&#36710;&#36742;&#21487;&#20197;&#19982;&#21608;&#22260;&#36710;&#36742;&#20849;&#20139;&#20132;&#36890;&#20449;&#24687;&#65292;&#23454;&#29616;&#21512;&#20316;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;CACC&#65289;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;&#39550;&#39542;&#21592;&#21464;&#36947;&#39044;&#27979;&#25152;&#38656;&#30340;&#20449;&#24687;&#31867;&#22411;&#65288;&#20301;&#32622;&#12289;&#36895;&#24230;&#12289;&#21152;&#36895;&#24230;&#65289;&#21644;&#21608;&#22260;&#36710;&#36742;&#25968;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;HighD&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;LSTM&#65288;&#38271;&#30701;&#26399;&#35760;&#24518;&#65289;&#27169;&#22411;&#26469;&#39044;&#27979;&#21464;&#36947;&#24847;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#21608;&#22260;&#36710;&#36742;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20934;&#30830;&#24615;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate lane change prediction can reduce potential accidents and contribute to higher road safety. Adaptive cruise control (ACC), lane departure avoidance (LDA), and lane keeping assistance (LKA) are some conventional modules in advanced driver assistance systems (ADAS). Thanks to vehicle-to-vehicle communication (V2V), vehicles can share traffic information with surrounding vehicles, enabling cooperative adaptive cruise control (CACC). While ACC relies on the vehicle's sensors to obtain the position and velocity of the leading vehicle, CACC also has access to the acceleration of multiple vehicles through V2V communication. This paper compares the type of information (position, velocity, acceleration) and the number of surrounding vehicles for driver lane change prediction. We trained an LSTM (Long Short-Term Memory) on the HighD dataset to predict lane change intention. Results indicate a significant improvement in accuracy with an increase in the number of surrounding vehicles and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#23398;&#20064;&#36807;&#21435;&#30340;&#39550;&#39542;&#32463;&#39564;&#65292;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.01095</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22120;&#65292;&#22312;&#36710;&#36947;&#21464;&#25442;&#26102;&#21487;&#20197;&#39044;&#27979;&#20808;&#21069;&#36710;&#36742;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application. (arXiv:2305.01095v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#23398;&#20064;&#36807;&#21435;&#30340;&#39550;&#39542;&#32463;&#39564;&#65292;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#35843;&#33410;&#36710;&#36895;&#26469;&#30830;&#20445;&#19982;&#21069;&#36710;&#23433;&#20840;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#24378;&#36710;&#36742;&#30340;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#24615;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ACC&#31995;&#32479;&#26080;&#27861;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#39550;&#39542;&#26465;&#20214;&#21644;&#39550;&#39542;&#32773;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;ACC&#31995;&#32479;&#65292;&#21487;&#20197;&#20174;&#20197;&#24448;&#30340;&#39550;&#39542;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#24182;&#23454;&#26102;&#22320;&#36866;&#24212;&#21644;&#39044;&#27979;&#26032;&#30340;&#24773;&#20917;&#12290;&#35813;&#27169;&#22411;&#26159;&#22522;&#20110;&#23454;&#38469;&#30340;&#39640;&#36895;&#20844;&#36335;&#39640;D&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#21033;&#29992;&#35013;&#22791;&#26377;&#25668;&#20687;&#22836;&#30340;&#26080;&#20154;&#26426;&#22312;&#24503;&#22269;&#39640;&#36895;&#20844;&#36335;&#19978;&#33719;&#21462;&#30340;&#12290;&#25105;&#20204;&#22312;&#20391;&#38754;&#36710;&#36947;&#21069;&#36710;&#21098;&#20999;&#24182;&#24378;&#21046;&#30446;&#26631;&#39550;&#39542;&#21592;&#20943;&#36895;&#30340;&#28608;&#36827;&#36710;&#36947;&#21464;&#21270;&#26102;&#35780;&#20272;&#20102;ACC&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#27169;&#25311;&#39550;&#39542;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#39304;&#36865;&#21069;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Adaptive Cruise Control (ACC) systems aims to enhance the safety and comfort of vehicles by automatically regulating the speed of the vehicle to ensure a safe gap from the preceding vehicle. However, conventional ACC systems are unable to adapt themselves to changing driving conditions and drivers' behavior. To address this limitation, we propose a Long Short-Term Memory (LSTM) based ACC system that can learn from past driving experiences and adapt and predict new situations in real time. The model is constructed based on the real-world highD dataset, acquired from German highways with the assistance of camera-equipped drones. We evaluated the ACC system under aggressive lane changes when the side lane preceding vehicle cut off, forcing the targeted driver to reduce speed. To this end, the proposed system was assessed on a simulated driving environment and compared with a feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01094</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Prediction with Bandit Feedback: Learning through Reparameterization. (arXiv:2305.01094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#21453;&#39304;&#30340;&#23454;&#29616;&#24335;&#39044;&#27979;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#30001;&#27169;&#22411;&#37096;&#32626;&#33258;&#36523;&#25913;&#21464;&#30340;&#24773;&#24418;&#19979;&#39044;&#27979;&#30340;&#19968;&#20010;&#26694;&#26550;&#8212;&#8212;&#23454;&#29616;&#24335;&#39044;&#27979;&#12290;&#29616;&#26377;&#30740;&#31350;&#30340;&#37325;&#28857;&#22312;&#20110;&#20248;&#21270;&#20934;&#30830;&#24615;&#65292;&#20294;&#26159;&#20854;&#20551;&#35774;&#24448;&#24448;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#28385;&#36275;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#23618;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#23454;&#29616;&#24335;&#39044;&#27979;&#30446;&#26631;&#65292;&#20174;&#32780;&#23558;&#38750;&#20984;&#30340;&#30446;&#26631;&#36716;&#21270;&#20026;&#20984;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performative prediction, as introduced by Perdomo et al. (2020), is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work on optimizing accuracy in this setting hinges on two assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, and that the mapping from the model to the data distribution is known to the model designer in advance. In this paper, we initiate the study of tractable performative prediction problems that do not require these assumptions. To tackle this more challenging setting, we develop a two-level zeroth-order optimization algorithm, where one level aims to compute the distribution map, and the other level reparameterizes the performative prediction objective as a function of the induced data distribution. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#32467;&#21512;implicit regularization&#65292;&#20869;&#37096;&#32447;&#24615;&#23618;&#21644;L2&#27491;&#21017;&#21270;&#65288;&#26435;&#37325;&#34928;&#20943;&#65289;&#33258;&#21160;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32500;&#24230;&#65292;&#20135;&#29983;&#27491;&#20132;&#30340;&#27969;&#24418;&#22352;&#26631;&#31995;&#65292;&#24182;&#25552;&#20379;&#29615;&#22659;&#31354;&#38388;&#21644;&#27969;&#24418;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#26679;&#26412;&#20043;&#22806;&#30340;&#25237;&#24433;&#12290;&#35813;&#26041;&#27861;&#22312;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#20302;&#31209;&#34920;&#31034;&#25928;&#26524;&#65292;&#20026;&#24213;&#23618;&#21160;&#24577;&#25552;&#20379;&#20102;&#29289;&#29702;&#27934;&#35265;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01090</link><description>&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#21457;&#29616;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#20013;&#30340;&#27969;&#24418;&#32500;&#24230;&#21644;&#22352;&#26631;
&lt;/p&gt;
&lt;p&gt;
Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems. (arXiv:2305.01090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#32467;&#21512;implicit regularization&#65292;&#20869;&#37096;&#32447;&#24615;&#23618;&#21644;L2&#27491;&#21017;&#21270;&#65288;&#26435;&#37325;&#34928;&#20943;&#65289;&#33258;&#21160;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32500;&#24230;&#65292;&#20135;&#29983;&#27491;&#20132;&#30340;&#27969;&#24418;&#22352;&#26631;&#31995;&#65292;&#24182;&#25552;&#20379;&#29615;&#22659;&#31354;&#38388;&#21644;&#27969;&#24418;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#26679;&#26412;&#20043;&#22806;&#30340;&#25237;&#24433;&#12290;&#35813;&#26041;&#27861;&#22312;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#20302;&#31209;&#34920;&#31034;&#25928;&#26524;&#65292;&#20026;&#24213;&#23618;&#21160;&#24577;&#25552;&#20379;&#20102;&#29289;&#29702;&#27934;&#35265;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#21644;&#24037;&#31243;&#23398;&#20013;&#30340;&#35768;&#22810;&#29616;&#35937;&#22312;&#24418;&#24335;&#19978;&#26159;&#39640;&#32500;&#30340;&#65292;&#20294;&#23427;&#20204;&#30340;&#38271;&#26102;&#38388;&#21160;&#24577;&#24448;&#24448;&#29983;&#27963;&#22312;&#36739;&#20302;&#32500;&#30340;&#27969;&#24418;&#19978;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65292;&#23558;&#38544;&#24335;&#27491;&#21017;&#21270;&#19982;&#20869;&#37096;&#32447;&#24615;&#23618;&#21644;$L_2$&#27491;&#21017;&#21270;&#65288;&#26435;&#37325;&#34928;&#20943;&#65289;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20272;&#35745;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#32500;&#24230;&#12289;&#20135;&#29983;&#27491;&#20132;&#30340;&#27969;&#24418;&#22352;&#26631;&#31995;&#65292;&#24182;&#25552;&#20379;&#29615;&#22659;&#31354;&#38388;&#21644;&#27969;&#24418;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#36827;&#34892;&#26679;&#26412;&#20043;&#22806;&#30340;&#25237;&#24433;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20272;&#35745;&#27969;&#24418;&#32500;&#24230;&#30340;&#33021;&#21147;&#65292;&#38024;&#23545;&#22810;&#31181;&#22797;&#26434;&#24230;&#30340;&#21160;&#21147;&#31995;&#32479;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#20998;&#26512;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20197;&#20102;&#35299;&#20302;&#31209;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#27599;&#20010;&#38544;&#24335;&#27491;&#21017;&#21270;&#23618;&#20849;&#21516;&#26500;&#25104;&#20102;&#20302;&#31209;&#34920;&#31034;&#65292;&#29978;&#33267;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#33258;&#25105;&#32416;&#27491;&#12290;&#23398;&#20064;&#30340;&#22352;&#26631;&#31995;&#30340;&#20998;&#26512;&#21487;&#20197;&#20026;&#24213;&#23618;&#21160;&#24577;&#25552;&#20379;&#29289;&#29702;&#27934;&#35265;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25511;&#21046;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While many phenomena in physics and engineering are formally high-dimensional, their long-time dynamics often live on a lower-dimensional manifold. The present work introduces an autoencoder framework that combines implicit regularization with internal linear layers and $L_2$ regularization (weight decay) to automatically estimate the underlying dimensionality of a data set, produce an orthogonal manifold coordinate system, and provide the mapping functions between the ambient space and manifold space, allowing for out-of-sample projections. We validate our framework's ability to estimate the manifold dimension for a series of datasets from dynamical systems of varying complexities and compare to other state-of-the-art estimators. We analyze the training dynamics of the network to glean insight into the mechanism of low-rank learning and find that collectively each of the implicit regularizing layers compound the low-rank representation and even self-correct during training. Analysis o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20272;&#35745;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#20013;&#26399;&#26395;&#30340;&#27169;&#20307;&#35745;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.01089</link><description>&lt;p&gt;
&#35745;&#31639;&#21487;&#20132;&#25442;&#22270;&#29983;&#25104;&#27169;&#22411;&#30340;&#26399;&#26395;&#27169;&#20307;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Computing Expected Motif Counts for Exchangeable Graph Generative Models. (arXiv:2305.01089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20272;&#35745;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#20013;&#26399;&#26395;&#30340;&#27169;&#20307;&#35745;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#22270;&#24418;&#32479;&#35745;&#37327;&#30340;&#26399;&#26395;&#20540;&#26159;&#20351;&#29992;&#21644;&#23398;&#20064;&#22270;&#27169;&#22411;&#30340;&#37325;&#35201;&#25512;&#26029;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20272;&#35745;&#36807;&#31243;&#65292;&#29992;&#20110;&#26399;&#26395;&#27169;&#20307;&#35745;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#32479;&#35745;&#37327;&#31867;&#22411;&#12290;&#35813;&#31243;&#24207;&#36866;&#29992;&#20110;&#29983;&#25104;&#28151;&#21512;&#27169;&#22411;&#65292;&#36825;&#31181;&#27169;&#22411;&#24120;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#22270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the expected value of a graph statistic is an important inference task for using and learning graph models. This note presents a scalable estimation procedure for expected motif counts, a widely used type of graph statistic. The procedure applies for generative mixture models of the type used in neural and Bayesian approaches to graph data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22810;&#35821;&#31181;&#29992;&#25143;&#26597;&#35810;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#65292;&#24182;&#26681;&#25454;&#29305;&#23450;&#20135;&#21697;&#30340;&#38656;&#27714;&#35843;&#25972;&#20854;&#35789;&#27719;&#34920;&#21644;&#25340;&#20889;&#36755;&#20986;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.01082</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22810;&#35821;&#31181;&#29992;&#25143;&#26597;&#35810;&#25340;&#20889;&#26816;&#26597;&#22120;
&lt;/p&gt;
&lt;p&gt;
Contextual Multilingual Spellchecker for User Queries. (arXiv:2305.01082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22810;&#35821;&#31181;&#29992;&#25143;&#26597;&#35810;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#65292;&#24182;&#26681;&#25454;&#29305;&#23450;&#20135;&#21697;&#30340;&#38656;&#27714;&#35843;&#25972;&#20854;&#35789;&#27719;&#34920;&#21644;&#25340;&#20889;&#36755;&#20986;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25340;&#20889;&#26816;&#26597;&#26159;&#26368;&#22522;&#26412;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#25628;&#32034;&#21151;&#33021;&#20043;&#19968;&#12290;&#32416;&#27491;&#25340;&#20889;&#38169;&#35823;&#30340;&#29992;&#25143;&#26597;&#35810;&#19981;&#20165;&#22686;&#24378;&#20102;&#29992;&#25143;&#20307;&#39564;&#65292;&#32780;&#19988;&#29992;&#25143;&#20063;&#26399;&#26395;&#33021;&#22815;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24191;&#27867;&#21487;&#29992;&#30340;&#25340;&#20889;&#26816;&#26597;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#27604;&#26368;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#31934;&#24230;&#20302;&#65292;&#35201;&#20040;&#36895;&#24230;&#22826;&#24930;&#65292;&#26080;&#27861;&#29992;&#20110;&#24310;&#36831;&#26159;&#20851;&#38190;&#35201;&#27714;&#30340;&#25628;&#32034;&#29992;&#20363;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#21019;&#26032;&#26550;&#26500;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#24182;&#19988;&#27809;&#26377;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#36827;&#34892;&#22521;&#35757;&#65292;&#24182;&#19988;&#26159;&#38024;&#23545;&#36739;&#38271;&#25991;&#26412;&#30340;&#25340;&#20889;&#32416;&#27491;&#36827;&#34892;&#22521;&#35757;&#65292;&#36825;&#26159;&#19982;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#25340;&#20889;&#32416;&#27491;&#19981;&#21516;&#30340;&#33539;&#24335;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#24456;&#23569;(&#22823;&#22810;&#25968;&#26597;&#35810;&#21482;&#26377;1-2&#20010;&#21333;&#35789;)&#12290;&#26368;&#21518;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#20225;&#19994;&#26377;&#29420;&#29305;&#30340;&#35789;&#27719;&#65292;&#20363;&#22914;&#20135;&#21697;&#21517;&#31216;&#65292;&#29616;&#25104;&#30340;&#25340;&#20889;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#25340;&#20889;&#26816;&#26597;&#22120;&#65292;&#23427;&#38750;&#24120;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#65292;&#24182;&#26681;&#25454;&#29305;&#23450;&#20135;&#21697;&#30340;&#38656;&#27714;&#35843;&#25972;&#20854;&#35789;&#27719;&#34920;&#21644;&#25340;&#20889;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spellchecking is one of the most fundamental and widely used search features. Correcting incorrectly spelled user queries not only enhances the user experience but is expected by the user. However, most widely available spellchecking solutions are either lower accuracy than state-of-the-art solutions or too slow to be used for search use cases where latency is a key requirement. Furthermore, most innovative recent architectures focus on English and are not trained in a multilingual fashion and are trained for spell correction in longer text, which is a different paradigm from spell correction for user queries, where context is sparse (most queries are 1-2 words long). Finally, since most enterprises have unique vocabularies such as product names, off-the-shelf spelling solutions fall short of users' needs. In this work, we build a multilingual spellchecker that is extremely fast and scalable and that adapts its vocabulary and hence speller output based on a specific product's needs. Fu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;FedGMM&#65292;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#22788;&#29702;&#20102;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.01068</link><description>&lt;p&gt;
&#28151;&#21512;&#20998;&#24067;&#19979;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning under Mixture of Distributions. (arXiv:2305.01068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01068
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;FedGMM&#65292;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#22788;&#29702;&#20102;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#27599;&#20010;&#23458;&#25143;&#35757;&#32451;&#23450;&#21046;&#27169;&#22411;&#24182;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;(PFL)&#36235;&#21183;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;PFL&#25216;&#26415;&#20027;&#35201;&#20851;&#27880;&#26465;&#20214;&#20998;&#24067;&#24322;&#36136;&#24615;(&#21363;&#27010;&#24565;&#28418;&#31227;)&#65292;&#24403;&#23458;&#25143;&#20043;&#38388;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;(&#21363;&#21327;&#21464;&#37327;&#28418;&#31227;)&#26102;&#65292;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#32570;&#20047;&#36866;&#24212;&#26410;&#35265;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;FedGMM&#65292;&#23427;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;(GMM)&#26377;&#25928;&#22320;&#36866;&#24212;&#22810;&#26679;&#21270;&#23458;&#25143;&#31471;&#30340;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#12290;&#27169;&#22411;&#21442;&#25968;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#21033;&#29992;&#32852;&#37030;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#35299;&#20915;&#65292;&#35813;&#31639;&#27861;&#22312;&#38381;&#21512;&#24418;&#24335;&#19979;&#35299;&#20915;&#19988;&#19981;&#20551;&#35774;&#28176;&#21464;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;FedGMM&#21516;&#26102;&#27169;&#22411;&#21270;&#26465;&#20214;&#21644;&#21327;&#21464;&#37327;&#20998;&#24067;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent trend towards Personalized Federated Learning (PFL) has garnered significant attention as it allows for the training of models that are tailored to each client while maintaining data privacy. However, current PFL techniques primarily focus on modeling the conditional distribution heterogeneity (i.e. concept shift), which can result in suboptimal performance when the distribution of input data across clients diverges (i.e. covariate shift). Additionally, these techniques often lack the ability to adapt to unseen data, further limiting their effectiveness in real-world scenarios. To address these limitations, we propose a novel approach, FedGMM, which utilizes Gaussian mixture models (GMM) to effectively fit the input data distributions across diverse clients. The model parameters are estimated by maximum likelihood estimation utilizing a federated Expectation-Maximization algorithm, which is solved in closed form and does not assume gradient similarity. Furthermore, FedGMM po
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19987;&#19994;&#30693;&#35782;&#26641;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38598;&#20307;&#20915;&#31574;&#20013;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.01063</link><description>&lt;p&gt;
&#19987;&#19994;&#30693;&#35782;&#26641;&#22312;&#38598;&#20307;&#20915;&#31574;&#20013;&#35299;&#20915;&#30693;&#35782;&#23616;&#38480;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making. (arXiv:2305.01063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19987;&#19994;&#30693;&#35782;&#26641;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38598;&#20307;&#20915;&#31574;&#20013;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#20915;&#31574;&#32773;&#25552;&#20379;&#24314;&#35758;&#30340;&#19987;&#23478;&#24448;&#24448;&#20250;&#26174;&#31034;&#20986;&#38543;&#38382;&#39064;&#23454;&#20363;&#21464;&#21270;&#32780;&#21464;&#21270;&#30340;&#19987;&#19994;&#30693;&#35782;&#27700;&#24179;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38024;&#23545;&#23569;&#25968;&#24773;&#20917;&#30340;&#27425;&#20248;&#25110;&#27495;&#35270;&#24615;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#30693;&#35782;&#28145;&#24230;&#21644;&#24191;&#24230;&#30340;&#21464;&#21270;&#24314;&#27169;&#20026;&#23558;&#38382;&#39064;&#31354;&#38388;&#21010;&#20998;&#20026;&#19981;&#21516;&#19987;&#19994;&#30693;&#35782;&#21306;&#22495;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#31639;&#27861;&#65292;&#23427;&#20204;&#26126;&#30830;&#32771;&#34385;&#24182;&#36866;&#24212;&#38382;&#39064;&#23454;&#20363;&#19982;&#19987;&#23478;&#30693;&#35782;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#24182;&#24378;&#35843;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#26597;&#35810;&#30340;&#22825;&#30495;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#8212;&#8212;&#19987;&#19994;&#30693;&#35782;&#26641;&#65292;&#23427;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#36873;&#25321;&#36866;&#24403;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#29616;&#26377;&#26041;&#27861;&#34987;&#35777;&#26126;&#19981;&#36275;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experts advising decision-makers are likely to display expertise which varies as a function of the problem instance. In practice, this may lead to sub-optimal or discriminatory decisions against minority cases. In this work we model such changes in depth and breadth of knowledge as a partitioning of the problem space into regions of differing expertise. We provide here new algorithms that explicitly consider and adapt to the relationship between problem instances and experts' knowledge. We first propose and highlight the drawbacks of a naive approach based on nearest neighbor queries. To address these drawbacks we then introduce a novel algorithm - expertise trees - that constructs decision trees enabling the learner to select appropriate models. We provide theoretical insights and empirically validate the improved performance of our novel approach on a range of problems for which existing methods proved to be inadequate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#35299;&#20915;&#33609;&#22270;&#21512;&#25104;&#21644;&#35782;&#21035;&#38382;&#39064;&#12290;&#23545;&#24453;&#35782;&#21035;&#30340;&#20154;&#33080;&#36827;&#34892;&#27491;&#38754;&#23039;&#24577;&#12289;&#27491;&#24120;&#20809;&#29031;&#12289;&#20013;&#24615;&#34920;&#24773;&#21644;&#26080;&#36974;&#25377;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.01058</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#31070;&#32463;&#27169;&#22411;&#30340;&#33609;&#22270;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
semantic neural model approach for face recognition from sketch. (arXiv:2305.01058v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#21516;&#26102;&#35299;&#20915;&#33609;&#22270;&#21512;&#25104;&#21644;&#35782;&#21035;&#38382;&#39064;&#12290;&#23545;&#24453;&#35782;&#21035;&#30340;&#20154;&#33080;&#36827;&#34892;&#27491;&#38754;&#23039;&#24577;&#12289;&#27491;&#24120;&#20809;&#29031;&#12289;&#20013;&#24615;&#34920;&#24773;&#21644;&#26080;&#36974;&#25377;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#25191;&#27861;&#20013;&#65292;&#20154;&#33080;&#33609;&#22270;&#21512;&#25104;&#21644;&#35782;&#21035;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#24403;&#21069;&#30340;&#30740;&#31350;&#24448;&#24448;&#23558;&#20854;&#35270;&#20026;&#29420;&#31435;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#33609;&#22270;&#21512;&#25104;&#21644;&#35782;&#21035;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#24453;&#35782;&#21035;&#30340;&#20154;&#33080;&#22788;&#20110;&#27491;&#38754;&#23039;&#24577;&#65292;&#26377;&#30528;&#27491;&#24120;&#30340;&#20809;&#29031;&#21644;&#20013;&#24615;&#30340;&#34920;&#24773;&#65292;&#24182;&#19988;&#27809;&#26377;&#36974;&#25377;&#12290;&#20026;&#20102;&#21512;&#25104;&#33609;&#22270;/&#22270;&#20687;&#65292;&#23558;&#20154;&#33080;&#21306;&#22495;&#21010;&#20998;&#20026;&#37325;&#21472;&#30340;&#34917;&#19969;&#36827;&#34892;&#23398;&#20064;&#65292;&#34917;&#19969;&#30340;&#22823;&#23567;&#20915;&#23450;&#35201;&#23398;&#20064;&#30340;&#23616;&#37096;&#20154;&#33080;&#32467;&#26500;&#30340;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face sketch synthesis and reputation have wide range of packages in law enforcement. Despite the amazing progresses had been made in faces cartoon and reputation, maximum current researches regard them as separate responsibilities. On this paper, we propose a semantic neural version approach so that you can address face caricature synthesis and recognition concurrently. We anticipate that faces to be studied are in a frontal pose, with regular lighting and neutral expression, and have no occlusions. To synthesize caricature/image photos, the face vicinity is divided into overlapping patches for gaining knowledge of. The size of the patches decides the scale of local face systems to be found out.
&lt;/p&gt;</description></item><item><title>LooPy&#26159;&#19968;&#31181;Python&#36719;&#20214;&#21253;&#65292;&#20026;MIR&#25552;&#20379;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#33310;&#26354;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;EDM&#38899;&#39057;&#65292;&#24182;&#25552;&#20379;&#20102;&#26694;&#26550;&#26469;&#26500;&#24314;&#19987;&#19994;&#32423;&#30340;&#27169;&#26495;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20174;&#25351;&#23450;&#30340;&#26059;&#24459;&#21644;&#21644;&#24358;&#20013;&#21576;&#29616;&#20986;&#21046;&#20316;&#31934;&#33391;&#30340;&#27468;&#26354;&#36712;&#36947;&#25110;&#20165;&#36890;&#36807;&#20351;&#29992;&#31526;&#21495;&#24615;&#30340;&#26059;&#24459;&#29983;&#25104;&#22120;&#65292;&#25552;&#20379;&#20855;&#26377;&#22810;&#31181;&#39118;&#26684;&#30340;&#38899;&#36712;&#12290;</title><link>http://arxiv.org/abs/2305.01051</link><description>&lt;p&gt;
LooPy: &#19968;&#31181;&#38024;&#23545;&#30005;&#23376;&#33310;&#26354;&#30340;&#30740;&#31350;&#21451;&#22909;&#22411;&#28151;&#21512;&#26694;&#26550;&#30340;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
LooPy: A Research-Friendly Mix Framework for Music Information Retrieval on Electronic Dance Music. (arXiv:2305.01051v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01051
&lt;/p&gt;
&lt;p&gt;
LooPy&#26159;&#19968;&#31181;Python&#36719;&#20214;&#21253;&#65292;&#20026;MIR&#25552;&#20379;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#33310;&#26354;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#21487;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;EDM&#38899;&#39057;&#65292;&#24182;&#25552;&#20379;&#20102;&#26694;&#26550;&#26469;&#26500;&#24314;&#19987;&#19994;&#32423;&#30340;&#27169;&#26495;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20174;&#25351;&#23450;&#30340;&#26059;&#24459;&#21644;&#21644;&#24358;&#20013;&#21576;&#29616;&#20986;&#21046;&#20316;&#31934;&#33391;&#30340;&#27468;&#26354;&#36712;&#36947;&#25110;&#20165;&#36890;&#36807;&#20351;&#29992;&#31526;&#21495;&#24615;&#30340;&#26059;&#24459;&#29983;&#25104;&#22120;&#65292;&#25552;&#20379;&#20855;&#26377;&#22810;&#31181;&#39118;&#26684;&#30340;&#38899;&#36712;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;(MIR)&#24471;&#21040;&#20102;&#29190;&#28856;&#24615;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35832;&#22914;&#30005;&#23376;&#33310;&#26354;&#31561;&#38899;&#20048;&#31867;&#22411;&#30456;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#19968;&#30452;&#36739;&#23569;&#30740;&#31350;&#12290;&#32771;&#34385;&#21040;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Python&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;EDM&#38899;&#39057;&#29983;&#25104;&#65292;&#24182;&#20316;&#20026;EDM&#27468;&#26354;MIR&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#20197;&#32531;&#35299;&#33719;&#21462;&#26631;&#27880;&#25968;&#25454;&#30340;&#38590;&#24230;&#12290;&#23427;&#26159;&#19968;&#20010;&#26041;&#20415;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#36731;&#26494;&#36830;&#25509;&#21040;&#35768;&#22810;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#31649;&#36947;&#30340;&#26411;&#31471;&#12290;&#22312;&#36825;&#20010;&#36719;&#20214;&#21253;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26500;&#24314;&#19987;&#19994;&#27700;&#24179;&#30340;&#27169;&#26495;&#65292;&#21487;&#20197;&#20174;&#25351;&#23450;&#30340;&#26059;&#24459;&#21644;&#21644;&#24358;&#20013;&#21576;&#29616;&#20986;&#19968;&#20010;&#21046;&#20316;&#31934;&#33391;&#30340;&#36712;&#36947;&#65292;&#25110;&#32773;&#20165;&#36890;&#36807;&#25105;&#20204;&#30340;&#27010;&#29575;&#31526;&#21495;&#26059;&#24459;&#29983;&#25104;&#22120;&#65292;&#25552;&#20379;&#20855;&#26377;&#21508;&#31181;&#39118;&#26684;&#30340;&#38899;&#36712;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#28151;&#38899;&#22312;&#20027;&#35266;&#21644;&#23458;&#35266;&#35780;&#20272;&#26041;&#38754;&#19982;&#30001;&#19990;&#30028;&#33879;&#21517;&#33402;&#26415;&#23478;&#21046;&#20316;&#30340;&#21407;&#22987;&#21442;&#32771;&#27468;&#26354;&#20855;&#26377;&#30456;&#21516;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music information retrieval (MIR) has gone through an explosive development with the advancement of deep learning in recent years. However, music genres like electronic dance music (EDM) has always been relatively less investigated compared to others. Considering its wide range of applications, we present a Python package for automated EDM audio generation as an infrastructure for MIR for EDM songs, to mitigate the difficulty of acquiring labelled data. It is a convenient tool that could be easily concatenated to the end of many symbolic music generation pipelines. Inside this package, we provide a framework to build professional-level templates that could render a well-produced track from specified melody and chords, or produce massive tracks given only a specific key by our probabilistic symbolic melody generator. Experiments show that our mixes could achieve the same quality of the original reference songs produced by world-famous artists, with respect to both subjective and objecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;BERT&#27169;&#22411;&#26469;&#26631;&#27880;&#20398;&#36785;&#24615;&#25991;&#26412;&#20013;&#30340;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#27604;&#36739;&#30452;&#25509;&#35757;&#32451;&#21644;&#32858;&#21512;&#20004;&#31181;&#26041;&#27861;&#65292;&#32467;&#26524;&#21457;&#29616;&#32858;&#21512;&#26041;&#27861;&#27604;&#30452;&#25509;&#35757;&#32451;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01050</link><description>&lt;p&gt;
SemEval-2023 &#20219;&#21153;11&#20013;&#30340;SafeWebUH&#65306;&#23398;&#20064;&#20398;&#36785;&#24615;&#25991;&#26412;&#30340;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65306; &#30452;&#25509;&#35757;&#32451;&#19982;&#32858;&#21512;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in Derogatory Text: Comparison of Direct Training vs Aggregation. (arXiv:2305.01050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;BERT&#27169;&#22411;&#26469;&#26631;&#27880;&#20398;&#36785;&#24615;&#25991;&#26412;&#20013;&#30340;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#27604;&#36739;&#30452;&#25509;&#35757;&#32451;&#21644;&#32858;&#21512;&#20004;&#31181;&#26041;&#27861;&#65292;&#32467;&#26524;&#21457;&#29616;&#32858;&#21512;&#26041;&#27861;&#27604;&#30452;&#25509;&#35757;&#32451;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#35266;&#24615;&#21644;&#19981;&#21516;&#24847;&#35265;&#26159;&#20851;&#38190;&#30340;&#31038;&#20250;&#29616;&#35937;&#65292;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#22312;&#27880;&#37322;&#21644;&#26816;&#27979;&#20398;&#36785;&#24615;&#25991;&#26412;&#20869;&#23481;&#30340;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20351;&#29992;SemEval-2023&#20219;&#21153;11&#25552;&#20379;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#65292;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25429;&#25417;&#27880;&#37322;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20010;&#20307;&#27880;&#37322;&#32773;&#24314;&#27169;&#21644;&#32858;&#21512;&#23558;&#20132;&#21449;&#29109;&#24471;&#20998;&#24179;&#22343;&#38477;&#20302;&#20102;0.21&#65292;&#32780;&#19982;&#30452;&#25509;&#35757;&#32451;&#36719;&#26631;&#31614;&#30456;&#27604;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#27880;&#37322;&#32773;&#20803;&#25968;&#25454;&#23545;&#24179;&#22343;&#20132;&#21449;&#29109;&#20998;&#25968;&#30340;0.029&#38477;&#20302;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subjectivity and difference of opinion are key social phenomena, and it is crucial to take these into account in the annotation and detection process of derogatory textual content. In this paper, we use four datasets provided by SemEval-2023 Task 11 and fine-tune a BERT model to capture the disagreement in the annotation. We find individual annotator modeling and aggregation lowers the Cross-Entropy score by an average of 0.21, compared to the direct training on the soft labels. Our findings further demonstrate that annotator metadata contributes to the average 0.029 reduction in the Cross-Entropy score.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#12289;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#27867;&#21270;&#38590;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#20294;&#35201;&#27714;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2305.01034</link><description>&lt;p&gt;
&#26080;&#29305;&#23450;&#27169;&#22411;&#27867;&#21270;&#38590;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic Measure of Generalization Difficulty. (arXiv:2305.01034v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#12289;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#27867;&#21270;&#38590;&#24230;&#30340;&#26041;&#27861;&#8212;&#8212;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#35813;&#26041;&#27861;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#20294;&#35201;&#27714;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24230;&#37327;&#26159;&#20854;&#21487;&#20197;&#25191;&#34892;&#30340;&#20219;&#21153;&#38590;&#24230;&#65292;&#36275;&#22815;&#22256;&#38590;&#30340;&#20219;&#21153;&#26159;&#24378;&#22823;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#39537;&#21160;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#27979;&#35797;&#30340;&#27867;&#21270;&#38590;&#24230;&#19968;&#30452;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25454;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#23545;&#20219;&#21153;&#22266;&#26377;&#27867;&#21270;&#38590;&#24230;&#30340;&#26080;&#29305;&#23450;&#27169;&#22411;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24402;&#32435;&#20559;&#24046;&#22797;&#26434;&#24230;&#24230;&#37327;&#37327;&#21270;&#20102;&#22312;&#20219;&#21153;&#19978;&#33391;&#22909;&#27867;&#21270;&#25152;&#38656;&#30340;&#24635;&#20449;&#24687;&#37327;&#19982;&#25968;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#20043;&#24046;&#12290;&#36890;&#36807;&#27979;&#37327;&#36866;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#20551;&#35774;&#22312;&#20219;&#21153;&#20013;&#27867;&#21270;&#30340;&#20998;&#25968;&#21344;&#25454;&#30340;&#23481;&#31215;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23427;&#19982;&#27169;&#22411;&#24517;&#39035;&#27867;&#21270;&#30340;&#31354;&#38388;&#30340;&#20869;&#22312;&#32500;&#25968;&#25104;&#25351;&#25968;&#27604;&#20363;&#65292;&#20294;&#20165;&#22312;&#27599;&#20010;&#32500;&#24230;&#30340;&#20998;&#36776;&#29575;&#19978;&#21576;&#22810;&#39033;&#24335;&#27604;&#20363;&#65292;&#34920;&#26126;&#38656;&#35201;&#22312;&#35768;&#22810;&#32500;&#24230;&#19978;&#27867;&#21270;&#30340;&#20219;&#21153;&#27604;&#28041;&#21450;&#26356;&#23569;&#32500;&#24230;&#30340;&#26356;&#22810;&#32454;&#33410;&#30340;&#20219;&#21153;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
The measure of a machine learning algorithm is the difficulty of the tasks it can perform, and sufficiently difficult tasks are critical drivers of strong machine learning models. However, quantifying the generalization difficulty of machine learning benchmarks has remained challenging. We propose what is to our knowledge the first model-agnostic measure of the inherent generalization difficulty of tasks. Our inductive bias complexity measure quantifies the total information required to generalize well on a task minus the information provided by the data. It does so by measuring the fractional volume occupied by hypotheses that generalize on a task given that they fit the training data. It scales exponentially with the intrinsic dimensionality of the space over which the model must generalize but only polynomially in resolution per dimension, showing that tasks which require generalizing over many dimensions are drastically more difficult than tasks involving more detail in fewer dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2305.01028</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#22312;&#20844;&#21496;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Company classification using zero-shot learning. (arXiv:2305.01028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#35768;&#22810;&#21830;&#19994;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26412;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#20844;&#21496;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20174;&#20844;&#21496;&#25551;&#36848;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#24212;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#23558;&#20844;&#21496;&#20998;&#31867;&#21040;&#30456;&#20851;&#31867;&#21035;&#65292;&#26080;&#38656;&#20026;&#27599;&#20010;&#31867;&#21035;&#25552;&#20379;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#20844;&#21496;&#25991;&#26412;&#25551;&#36848;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#31616;&#21270;&#20844;&#21496;&#20998;&#31867;&#36807;&#31243;&#65292;&#20174;&#32780;&#20943;&#23569;&#20256;&#32479;&#26041;&#27861;&#22914;&#20840;&#29699;&#20135;&#19994;&#20998;&#31867;&#26631;&#20934;&#65288;GICS&#65289;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#33258;&#21160;&#21270;&#20844;&#21496;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#26159;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, natural language processing (NLP) has become increasingly important in a variety of business applications, including sentiment analysis, text classification, and named entity recognition. In this paper, we propose an approach for company classification using NLP and zero-shot learning. Our method utilizes pre-trained transformer models to extract features from company descriptions, and then applies zero-shot learning to classify companies into relevant categories without the need for specific training data for each category. We evaluate our approach on publicly available datasets of textual descriptions of companies, and demonstrate that it can streamline the process of company classification, thereby reducing the time and resources required in traditional approaches such as the Global Industry Classification Standard (GICS). The results show that this method has potential for automation of company classification, making it a promising avenue for future research in thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#38388;&#23618;&#34920;&#31034;&#30340;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#22495;&#36716;&#31227;&#36827;&#34892;&#39046;&#22495;&#38388;&#30340;&#20851;&#32852;&#65292;&#25552;&#39640;&#27450;&#39575;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#65292;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#25512;&#25991;&#26159;&#26816;&#27979;&#20551;&#26032;&#38395;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#26368;&#26377;&#24110;&#21161;&#30340;&#20449;&#24687;&#25552;&#20379;&#32773;&#65292;&#26032;&#38395;&#22312;&#25512;&#29305;&#35875;&#35328;&#26816;&#27979;&#20013;&#26368;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.01011</link><description>&lt;p&gt;
&#22522;&#20110;&#36719;&#22495;&#36716;&#31227;&#30340;&#29305;&#24449;&#22686;&#24378;&#27450;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deception Detection with Feature-Augmentation by soft Domain Transfer. (arXiv:2305.01011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#38388;&#23618;&#34920;&#31034;&#30340;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#22495;&#36716;&#31227;&#36827;&#34892;&#39046;&#22495;&#38388;&#30340;&#20851;&#32852;&#65292;&#25552;&#39640;&#27450;&#39575;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#65292;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#25512;&#25991;&#26159;&#26816;&#27979;&#20551;&#26032;&#38395;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#26368;&#26377;&#24110;&#21161;&#30340;&#20449;&#24687;&#25552;&#20379;&#32773;&#65292;&#26032;&#38395;&#22312;&#25512;&#29305;&#35875;&#35328;&#26816;&#27979;&#20013;&#26368;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#29190;&#28856;&#30340;&#36825;&#20010;&#26102;&#20195;&#65292;&#27450;&#39575;&#32773;&#21033;&#29992;&#19981;&#21516;&#30340;&#20449;&#24687;&#39046;&#22495;&#25110;&#23186;&#20171;&#26469;&#21033;&#29992;&#29992;&#25143;&#65292;&#27604;&#22914;&#26032;&#38395;&#12289;&#30005;&#23376;&#37038;&#20214;&#21644;&#25512;&#25991;&#31561;&#12290;&#23613;&#31649;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#26469;&#26816;&#27979;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#27450;&#39575;&#65292;&#20294;&#26032;&#20107;&#20214;&#20013;&#20449;&#24687;&#30340;&#30701;&#32570;&#38656;&#35201;&#36825;&#20123;&#39046;&#22495;&#30456;&#20114;&#20851;&#32852;&#26469;&#23545;&#25239;&#27450;&#39575;&#12290;&#20026;&#20102;&#24418;&#25104;&#36825;&#31181;&#20851;&#32852;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#36827;&#34892;&#29305;&#24449;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#33258;&#36523;&#39046;&#22495;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;&#22810;&#36798;6.60%&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#25512;&#25991;&#26159;&#26816;&#27979;&#20551;&#26032;&#38395;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#26368;&#26377;&#24110;&#21161;&#30340;&#20449;&#24687;&#25552;&#20379;&#32773;&#65292;&#32780;&#26032;&#38395;&#22312;&#25512;&#29305;&#35875;&#35328;&#26816;&#27979;&#20013;&#26368;&#26377;&#24110;&#21161;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#20110;&#22495;&#30693;&#35782;&#36716;&#31227;&#30340;&#26377;&#29992;&#27934;&#35265;&#65292;&#21487;&#20197;&#24110;&#21161;&#24314;&#31435;&#27604;&#29616;&#26377;&#25991;&#29486;&#26356;&#24378;&#22823;&#30340;&#27450;&#39575;&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this era of information explosion, deceivers use different domains or mediums of information to exploit the users, such as News, Emails, and Tweets. Although numerous research has been done to detect deception in all these domains, information shortage in a new event necessitates these domains to associate with each other to battle deception. To form this association, we propose a feature augmentation method by harnessing the intermediate layer representation of neural models. Our approaches provide an improvement over the self-domain baseline models by up to 6.60%. We find Tweets to be the most helpful information provider for Fake News and Phishing Email detection, whereas News helps most in Tweet Rumor detection. Our analysis provides a useful insight for domain knowledge transfer which can help build a stronger deception detection system than the existing literature.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#38598;&#20307;&#21464;&#37327;&#26469;&#24314;&#31435;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#29702;&#35770;&#65292;&#25506;&#31350;NN&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20004;&#20010;&#21464;&#37327;&#65292;&#29109;&#21644;&#32463;&#39564;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#36857;&#32447;&#65292;&#24182;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#36825;&#20123;&#21464;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#26377;&#20851;&#12290;&#38543;&#26426;&#32593;&#32476;&#25552;&#28860;&#65288;RND&#65289;&#34987;&#29992;&#20110;&#20248;&#21270;&#25968;&#25454;&#36873;&#25321;&#65292;&#21457;&#29616;&#36873;&#25321;RND&#25968;&#25454;&#38598;&#33021;&#32988;&#36807;&#38543;&#26426;&#36873;&#21462;&#65292;&#19988;RND&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#38598;&#20307;&#21464;&#37327;&#26356;&#22823;&#12290;</title><link>http://arxiv.org/abs/2305.00995</link><description>&lt;p&gt;
&#26397;&#21521;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#35937;&#23398;&#29702;&#35299;&#65306;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Towards a Phenomenological Understanding of Neural Networks: Data. (arXiv:2305.00995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#38598;&#20307;&#21464;&#37327;&#26469;&#24314;&#31435;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#29702;&#35770;&#65292;&#25506;&#31350;NN&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20004;&#20010;&#21464;&#37327;&#65292;&#29109;&#21644;&#32463;&#39564;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#36857;&#32447;&#65292;&#24182;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#36825;&#20123;&#21464;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#26377;&#20851;&#12290;&#38543;&#26426;&#32593;&#32476;&#25552;&#28860;&#65288;RND&#65289;&#34987;&#29992;&#20110;&#20248;&#21270;&#25968;&#25454;&#36873;&#25321;&#65292;&#21457;&#29616;&#36873;&#25321;RND&#25968;&#25454;&#38598;&#33021;&#32988;&#36807;&#38543;&#26426;&#36873;&#21462;&#65292;&#19988;RND&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#38598;&#20307;&#21464;&#37327;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#24314;&#31435;&#22312;&#38598;&#20307;&#21464;&#37327;&#20043;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#29702;&#35770;&#23558;&#20026;&#31185;&#23398;&#23478;&#25552;&#20379;&#26356;&#22909;&#22320;&#29702;&#35299;&#27599;&#20010;&#38454;&#27573;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#36825;&#26679;&#30340;&#21464;&#37327;&#65292;&#21363;&#22522;&#20110;&#20256;&#36882;&#32473;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#29109;&#21644;&#36857;&#32447;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#21464;&#37327;&#30340;&#32972;&#26223;&#19979;&#23545;NN&#24615;&#33021;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#36215;&#22987;&#29109;&#12289;NTK&#30340;&#36857;&#32447;&#19982;&#35757;&#32451;&#23436;&#25104;&#21518;&#35745;&#31639;&#30340;&#27169;&#22411;&#25512;&#24191;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;NN&#35757;&#32451;&#30340;&#26368;&#20339;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#38543;&#26426;&#32593;&#32476;&#25552;&#28860;&#65288;RND&#65289;&#20316;&#20026;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#25163;&#27573;&#65292;&#28982;&#21518;&#19982;&#38543;&#26426;&#36873;&#21462;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RND&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#19981;&#20165;&#33021;&#22815;&#32988;&#36807;&#38543;&#26426;&#36873;&#21462;&#65292;&#32780;&#19988;&#19982;RND&#25968;&#25454;&#38598;&#30456;&#20851;&#32852;&#30340;&#38598;&#20307;&#21464;&#37327;&#27604;&#38543;&#26426;&#36873;&#21462;&#30340;&#35201;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
A theory of neural networks (NNs) built upon collective variables would provide scientists with the tools to better understand the learning process at every stage. In this work, we introduce two such variables, the entropy and the trace of the empirical neural tangent kernel (NTK) built on the training data passed to the model. We empirically analyze the NN performance in the context of these variables and find that there exists correlation between the starting entropy, the trace of the NTK, and the generalization of the model computed after training is complete. This framework is then applied to the problem of optimal data selection for the training of NNs. To this end, random network distillation (RND) is used as a means of selecting training data which is then compared with random selection of data. It is shown that not only does RND select data-sets capable of outperforming random selection, but that the collective variables associated with the RND data-sets are larger than those o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26497;&#24230;&#32570;&#20047;&#30495;&#23454;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#22823;&#22411;&#20154;&#36896;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#31361;&#21464;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36873;&#25321;&#24615;&#21387;&#21147;&#26469;&#20248;&#21270;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.00987</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#26497;&#24230;&#32570;&#20047;&#30495;&#23454;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
A novel algorithm can generate data to train machine learning models in conditions of extreme scarcity of real world data. (arXiv:2305.00987v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26497;&#24230;&#32570;&#20047;&#30495;&#23454;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#22823;&#22411;&#20154;&#36896;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#31361;&#21464;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36873;&#25321;&#24615;&#21387;&#21147;&#26469;&#20248;&#21270;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#20294;&#25910;&#38598;&#12289;&#25972;&#29702;&#21644;&#25805;&#32437;&#22823;&#22411;&#22797;&#26434;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20250;&#24102;&#26469;&#25104;&#26412;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#20197;&#21450;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26497;&#24230;&#32570;&#20047;&#30495;&#23454;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#22823;&#22411;&#20154;&#36896;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#65292;&#36890;&#36807;&#31361;&#21464;&#38543;&#26426;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#35757;&#32451;&#21518;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#25209;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#34987;&#35270;&#20026;&#29992;&#20110;&#20854;&#35757;&#32451;&#30340;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#36866;&#24212;&#24615;&#20195;&#29702;&#12290;&#36890;&#36807;&#23545;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#30340;&#31181;&#32676;&#26045;&#21152;&#36873;&#25321;&#24615;&#21387;&#21147;&#65292;&#28120;&#27760;&#19981;&#36866;&#23452;&#30340;&#20010;&#20307;&#65292;&#36890;&#36807;&#20195;&#38469;&#36951;&#20256;&#65292;&#36866;&#24212;&#24615;&#26368;&#24378;&#30340;&#20010;&#20307;&#30340;&#36866;&#24212;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning models requires large datasets. However, collecting, curating, and operating large and complex sets of real world data poses problems of costs, ethical and legal issues, and data availability. Here we propose a novel algorithm to generate large artificial datasets to train machine learning models in conditions of extreme scarcity of real world data. The algorithm is based on a genetic algorithm, which mutates randomly generated datasets subsequently used for training a neural network. After training, the performance of the neural network on a batch of real world data is considered a surrogate for the fitness of the generated dataset used for its training. As selection pressure is applied to the population of generated datasets, unfit individuals are discarded, and the fitness of the fittest individuals increases through generations. The performance of the data generation algorithm was measured on the Iris dataset and on the Breast Cancer Wisconsin diagnostic d
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#65288;ASTGODE&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ASTGODE&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.00985</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;ODE&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention-based Spatial-Temporal Graph Neural ODE for Traffic Prediction. (arXiv:2305.00985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00985
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#65288;ASTGODE&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#20132;&#36890;&#39044;&#27979;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ASTGODE&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#25429;&#25417;&#20132;&#36890;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#24182;&#23454;&#29616;&#29702;&#24819;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;ODE&#65288;ASTGODE&#65289;&#65292;&#26126;&#30830;&#23398;&#20064;&#20132;&#36890;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#20351;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#26356;&#21152;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32858;&#21512;&#20102;&#19981;&#21516;&#21608;&#26399;&#30340;&#20132;&#36890;&#27169;&#24335;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22343;&#26041;&#26681;&#35823;&#24046;&#24230;&#37327;&#30340;&#20934;&#30830;&#24615;&#19978;&#65292;&#26159;&#25152;&#26377;&#29616;&#26377;GNN&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is an important issue in intelligent traffic systems (ITS). Graph neural networks (GNNs) are effective deep learning models to capture the complex spatio-temporal dependency of traffic data, achieving ideal prediction performance. In this paper, we propose attention-based graph neural ODE (ASTGODE) that explicitly learns the dynamics of the traffic system, which makes the prediction of our machine learning model more explainable. Our model aggregates traffic patterns of different periods and has satisfactory performance on two real-world traffic data sets. The results show that our model achieves the highest accuracy of the root mean square error metric among all the existing GNN models in our experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#38454;&#27573;&#22522;&#20110;&#21452;Copula&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#24322;&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#38469;&#24037;&#25511;&#31995;&#32479;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00982</link><description>&lt;p&gt;
&#24037;&#25511;&#31995;&#32479;&#20013;&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#30340;&#21452;&#38454;&#27573;&#21452;Copula&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Two-phase Dual COPOD Method for Anomaly Detection in Industrial Control System. (arXiv:2305.00982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#38454;&#27573;&#22522;&#20110;&#21452;Copula&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#24322;&#30340;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#38469;&#24037;&#25511;&#31995;&#32479;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#22788;&#29702;&#35774;&#26045;&#21644;&#30005;&#31449;&#31561;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#20381;&#36182;&#20110;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#65288;ICS&#65289;&#36827;&#34892;&#30417;&#25511;&#21644;&#25511;&#21046;&#65292;&#36825;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#25915;&#20987;&#21644;&#31995;&#32479;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;ICS&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#20351;&#23454;&#36341;&#32773;&#38590;&#20197;&#29702;&#35299;&#21644;&#20449;&#20219;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#20010;&#38454;&#27573;&#30340;&#21452;Copula&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical infrastructures like water treatment facilities and power plants depend on industrial control systems (ICS) for monitoring and control, making them vulnerable to cyber attacks and system malfunctions. Traditional ICS anomaly detection methods lack transparency and interpretability, which make it difficult for practitioners to understand and trust the results. This paper proposes a two-phase dual Copula-based Outlier Detection (COPOD) method that addresses these challenges. The first phase removes unwanted outliers using an empirical cumulative distribution algorithm, and the second phase develops two parallel COPOD models based on the output data of phase 1. The method is based on empirical distribution functions, parameter-free, and provides interpretability by quantifying each feature's contribution to an anomaly. The method is also computationally and memory-efficient, suitable for low- and high-dimensional datasets. Experimental results demonstrate superior performance in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24930;&#28151;&#21512;&#36807;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32473;&#20986;&#20102;&#23545;&#30001;&#24179;&#31283;&#19988;phi&#28151;&#21512;&#36807;&#31243;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#19981;&#21516;&#31181;&#31867;&#25439;&#22833;&#31867;&#21035;&#30340;&#19968;&#31181;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.00977</link><description>&lt;p&gt;
&#24930;&#28151;&#21512;&#36807;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalization for slowly mixing processes. (arXiv:2305.00977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24930;&#28151;&#21512;&#36807;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32473;&#20986;&#20102;&#23545;&#30001;&#24179;&#31283;&#19988;phi&#28151;&#21512;&#36807;&#31243;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#19981;&#21516;&#31181;&#31867;&#25439;&#22833;&#31867;&#21035;&#30340;&#19968;&#31181;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32473;&#20986;&#20102;&#38024;&#23545;&#30001;&#24179;&#31283;&#19988;phi&#28151;&#21512;&#36807;&#31243;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#21508;&#31181;&#25439;&#22833;&#31867;&#21035;&#30340;&#19968;&#31181;&#19978;&#30028;&#65292;&#20854;&#20013;&#28151;&#21512;&#26102;&#38388;&#65288;&#33719;&#24471;&#36817;&#20284;&#29420;&#31435;&#25152;&#38656;&#30340;&#26102;&#38388;&#65289;&#20165;&#20197;&#21152;&#27861;&#26041;&#24335;&#36827;&#20837;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#23545;&#20110;&#24930;&#36895;&#28151;&#21512;&#36807;&#31243;&#32780;&#35328;&#65292;&#36825;&#21487;&#20197;&#26159;&#20854;&#20248;&#21183;&#65292;&#22240;&#20026;&#20854;&#19982;&#28151;&#21512;&#26102;&#38388;&#30340;&#20056;&#27861;&#20381;&#36182;&#24615;&#30456;&#27604;&#35201;&#22909;&#12290;&#20801;&#35768;&#30340;&#25439;&#22833;&#31867;&#21035;&#21253;&#25324;&#20855;&#26377;&#25351;&#23450;Lipschitz&#24402;&#19968;&#21270;&#25110;&#24179;&#28369;&#24230;&#21442;&#25968;&#30340;&#20989;&#25968;&#12290;&#35813;&#19978;&#30028;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#23545;&#26080;&#38480;&#21046;&#25439;&#22833;&#31867;&#21035;&#30340;&#32479;&#19968;&#24615;&#30740;&#31350;&#65292;&#20854;&#21462;&#20915;&#20110;&#26679;&#26412;&#36335;&#24452;&#19978;&#20989;&#25968;&#30340;&#23616;&#37096;Lipschitz&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
A bound uniform over various loss-classes is given for data generated by stationary and phi-mixing processes, where the mixing time (the time needed to obtain approximate independence) enters the sample complexity only in an additive way. For slowly mixing processes this can be a considerable advantage over results with multiplicative dependence on the mixing time. The admissible loss-classes include functions with prescribed Lipschitz norms or smoothness parameters. The bound can also be applied to be uniform over unconstrained loss-classes, where it depends on local Lipschitz properties of the function on the sample path.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#32479;&#35745;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20197;&#25552;&#20379;&#26356;&#20248;&#36234;&#30340;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#35268;&#21010;&#12290;&#28145;&#24230;&#38598;&#25104;&#21487;&#25552;&#20379;&#26356;&#22909;&#30340;&#39118;&#38505;&#35780;&#20272;&#65292;&#26159;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#37096;&#38376;&#24212;&#29992;&#25152;&#36843;&#20999;&#38656;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.00975</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#29992;&#20110;&#25913;&#36827;&#27668;&#20505;&#21464;&#21270;&#26465;&#20214;&#19979;&#32479;&#35745;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Ensembles to Improve Uncertainty Quantification of Statistical Downscaling Models under Climate Change Conditions. (arXiv:2305.00975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#32479;&#35745;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20197;&#25552;&#20379;&#26356;&#20248;&#36234;&#30340;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#35268;&#21010;&#12290;&#28145;&#24230;&#38598;&#25104;&#21487;&#25552;&#20379;&#26356;&#22909;&#30340;&#39118;&#38505;&#35780;&#20272;&#65292;&#26159;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#37096;&#38376;&#24212;&#29992;&#25152;&#36843;&#20999;&#38656;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#26395;&#35299;&#20915;&#32479;&#35745;&#38477;&#23610;&#24230;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#32479;&#35745;&#38477;&#23610;&#24230;&#25351;&#30340;&#26159;&#20174;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#21464;&#37327;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#27668;&#20505;&#22330;&#30340;&#26041;&#27861;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22522;&#20110;&#31283;&#23450;&#24615;&#20551;&#35774;&#65292;&#20854;&#22312;&#27668;&#20505;&#21464;&#21270;&#26465;&#20214;&#19979;&#25512;&#24191;&#30340;&#33021;&#21147;&#20173;&#28982;&#23384;&#22312;&#30097;&#38382;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#32479;&#35745;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#36890;&#36807;&#26356;&#22909;&#22320;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#65292;&#32479;&#35745;&#38477;&#23610;&#24230;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#26356;&#20248;&#36234;&#30340;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#35268;&#21010;&#65292;&#36825;&#26159;&#21508;&#31181;&#36127;&#38754;&#31038;&#20250;&#21644;&#32463;&#27982;&#24433;&#21709;&#30340;&#26469;&#28304;&#12290;&#30001;&#20110;&#19981;&#23384;&#22312;&#35266;&#27979;&#21040;&#30340;&#26410;&#26469;&#25968;&#25454;&#65292;&#25105;&#20204;&#20381;&#38752;&#20266;&#23454;&#39564;&#26469;&#35780;&#20272;&#28145;&#24230;&#38598;&#25104;&#23545;&#27668;&#20505;&#21464;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#21542;&#36866;&#29992;&#12290;&#28145;&#24230;&#38598;&#25104;&#21487;&#25552;&#20379;&#26356;&#22909;&#30340;&#39118;&#38505;&#35780;&#20272;&#65292;&#36825;&#26159;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#37096;&#38376;&#24212;&#29992;&#25152;&#36843;&#20999;&#38656;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has emerged as a promising tool for statistical downscaling, the set of methods for generating high-resolution climate fields from coarse low-resolution variables. Nevertheless, their ability to generalize to climate change conditions remains questionable, mainly due to the stationarity assumption. We propose deep ensembles as a simple method to improve the uncertainty quantification of statistical downscaling models. By better capturing uncertainty, statistical downscaling models allow for superior planning against extreme weather events, a source of various negative social and economic impacts. Since no observational future data exists, we rely on a pseudo reality experiment to assess the suitability of deep ensembles for quantifying the uncertainty of climate change projections. Deep ensembles allow for a better risk assessment, highly demanded by sectoral applications to tackle climate change.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20197;&#25913;&#21892;&#27668;&#20505;&#38477;&#23610;&#24230;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.00974</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23436;&#32654;&#39044;&#27979;&#27668;&#20505;&#38477;&#23610;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the use of Deep Generative Models for Perfect Prognosis Climate Downscaling. (arXiv:2305.00974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00974
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20197;&#25913;&#21892;&#27668;&#20505;&#38477;&#23610;&#24230;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#23436;&#32654;&#30340;&#39044;&#27979;&#38477;&#23610;&#24230;&#25216;&#26415;&#65292;&#21487;&#20197;&#20174;&#22823;&#35268;&#27169;&#31895;&#31890;&#24230;&#30340;&#22823;&#27668;&#25968;&#25454;&#20013;&#35745;&#31639;&#20986;&#39640;&#20998;&#36776;&#29575;&#30340;&#22330;&#12290;&#23613;&#31649;&#36825;&#20123;&#25216;&#26415;&#26377;&#26395;&#22797;&#21046;&#35266;&#27979;&#21040;&#30340;&#23616;&#37096;&#21464;&#24322;&#65292;&#20294;&#23427;&#20204;&#26159;&#22522;&#20110;&#22312;&#27599;&#20010;&#20301;&#32622;&#20272;&#35745;&#29420;&#31435;&#20998;&#24067;&#65292;&#36825;&#23548;&#33268;&#31354;&#38388;&#32467;&#26500;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#19979;&#38477;&#38632;&#37327;&#26102;&#12290;&#26412;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#25913;&#21892;&#39640;&#20998;&#36776;&#29575;&#22330;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#65292;&#36825;&#22312;&#19968;&#20123;&#37096;&#38376;&#24212;&#29992;&#65288;&#20363;&#22914;&#27700;&#25991;&#23398;&#65289;&#20013;&#23545;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#38750;&#24120;&#24517;&#38656;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning has recently emerged as a perfect prognosis downscaling technique to compute high-resolution fields from large-scale coarse atmospheric data. Despite their promising results to reproduce the observed local variability, they are based on the estimation of independent distributions at each location, which leads to deficient spatial structures, especially when downscaling precipitation. This study proposes the use of generative models to improve the spatial consistency of the high-resolution fields, very demanded by some sectoral applications (e.g., hydrology) to tackle climate change.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DP-FedSAM&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#26799;&#24230;&#25200;&#21160;&#26469;&#20943;&#36731;&#24046;&#20998;&#38544;&#31169;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#23558;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#22120;&#25972;&#21512;&#21040;&#31639;&#27861;&#20013;&#65292;&#29983;&#25104;&#26356;&#24179;&#32531;&#30340;&#25439;&#22833;&#26354;&#38754;&#21644;&#26356;&#22909;&#30340;&#26435;&#37325;&#25200;&#21160;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00873</link><description>&lt;p&gt;
&#25903;&#25345;&#23458;&#25143;&#31471;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#26356;&#24179;&#30340;&#26799;&#24230;&#22270;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards the Flatter Landscape and Better Generalization in Federated Learning under Client-level Differential Privacy. (arXiv:2305.00873v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00873
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DP-FedSAM&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#26799;&#24230;&#25200;&#21160;&#26469;&#20943;&#36731;&#24046;&#20998;&#38544;&#31169;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#23558;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#22120;&#25972;&#21512;&#21040;&#31639;&#27861;&#20013;&#65292;&#29983;&#25104;&#26356;&#24179;&#32531;&#30340;&#25439;&#22833;&#26354;&#38754;&#21644;&#26356;&#22909;&#30340;&#26435;&#37325;&#25200;&#21160;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#21644;&#20943;&#23569;&#20449;&#24687;&#27844;&#38706;&#65292;&#37319;&#29992;&#23458;&#25143;&#31471;&#32423;&#21035;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#26696;&#24448;&#24448;&#23548;&#33268;&#23574;&#38160;&#30340;&#25439;&#22833;&#22270;&#20687;&#21644;&#26435;&#37325;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#36739;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23458;&#25143;&#31471;&#32423;&#21035;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;DP-FedSAM&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#26799;&#24230;&#25200;&#21160;&#26469;&#20943;&#36731;&#24046;&#20998;&#38544;&#31169;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;DP-FedSAM&#23558;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#22120;&#65288;SAM&#65289;&#25972;&#21512;&#21040;&#31639;&#27861;&#20013;&#65292;&#29983;&#25104;&#23616;&#37096;&#24179;&#22374;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#26435;&#37325;&#25200;&#21160;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#20135;&#29983;&#20102;&#23616;&#37096;&#26356;&#26032;&#30340;&#23567;&#33539;&#25968;&#21644;&#23545;DP&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23569;&#38543;&#26426;&#22122;&#22768;&#30340;&#24133;&#24230;&#21516;&#26102;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DP-FedSAM-top_k&#65292;&#37319;&#29992;&#23616;&#37096;&#26356;&#26032;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharp loss landscape and have poor weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with improved stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. To further reduce the magnitude of random noise while achieving better performance, we propose DP-FedSAM-$top_k$ by adopting the local update sparsi
&lt;/p&gt;</description></item><item><title>GNNs&#22312;&#33410;&#28857;&#23618;&#38754;&#39044;&#27979;&#20013;&#21487;&#33021;&#20250;&#22240;&#22270;&#32467;&#26500;&#23567;&#30340;&#26356;&#25913;&#32780;&#23548;&#33268;&#19981;&#31283;&#20581;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#24615;&#22270;&#30340;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#25152;&#26377;&#35780;&#20272;&#30340;GNN&#37117;&#34920;&#29616;&#20986;&#36807;&#24230;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#36229;&#20986;&#35821;&#20041;&#21464;&#21270;&#28857;&#12290;&#22312;&#25512;&#29702;&#30340;&#22270;&#20013;&#21253;&#25324;&#35757;&#32451;&#22270;&#30340;&#26631;&#31614;-&#32467;&#26500;&#21487;&#20197;&#36991;&#20813;&#36807;&#24230;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#23545;&#25239;&#24615;&#27010;&#24565;&#30340;&#40065;&#26834;&#24615;&#27979;&#35797;&#19982;&#26368;&#32456;&#20027;&#21160;&#23398;&#20064;&#34920;&#29616;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.00851</link><description>&lt;p&gt;
&#37325;&#35775;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting Robustness in Graph Machine Learning. (arXiv:2305.00851v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00851
&lt;/p&gt;
&lt;p&gt;
GNNs&#22312;&#33410;&#28857;&#23618;&#38754;&#39044;&#27979;&#20013;&#21487;&#33021;&#20250;&#22240;&#22270;&#32467;&#26500;&#23567;&#30340;&#26356;&#25913;&#32780;&#23548;&#33268;&#19981;&#31283;&#20581;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#24615;&#22270;&#30340;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#25152;&#26377;&#35780;&#20272;&#30340;GNN&#37117;&#34920;&#29616;&#20986;&#36807;&#24230;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#36229;&#20986;&#35821;&#20041;&#21464;&#21270;&#28857;&#12290;&#22312;&#25512;&#29702;&#30340;&#22270;&#20013;&#21253;&#25324;&#35757;&#32451;&#22270;&#30340;&#26631;&#31614;-&#32467;&#26500;&#21487;&#20197;&#36991;&#20813;&#36807;&#24230;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#23545;&#25239;&#24615;&#27010;&#24565;&#30340;&#40065;&#26834;&#24615;&#27979;&#35797;&#19982;&#26368;&#32456;&#20027;&#21160;&#23398;&#20064;&#34920;&#29616;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#33410;&#28857;&#23618;&#38754;&#39044;&#27979;&#23545;&#22270;&#32467;&#26500;&#30340;&#24494;&#23567;&#26356;&#25913;&#8212;&#8212;&#36890;&#24120;&#31216;&#20026;&#23545;&#25239;&#24615;&#26356;&#25913;&#8212;&#8212;&#19981;&#31283;&#20581;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25163;&#21160;&#26816;&#26597;&#22270;&#24418;&#22256;&#38590;&#65292;&#19981;&#28165;&#26970;&#30740;&#31350;&#30340;&#25200;&#21160;&#26159;&#21542;&#24635;&#26159;&#20445;&#30041;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26680;&#24515;&#20551;&#35774;:&#21363;&#19981;&#25913;&#21464;&#35821;&#20041;&#20869;&#23481;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26356;&#22522;&#26412;&#30340;&#23545;&#25239;&#24615;&#22270;&#24418;&#27010;&#24565;&#65292;&#23427;&#30693;&#36947;&#35821;&#20041;&#20869;&#23481;&#30340;&#25913;&#21464;&#12290;&#36890;&#36807;&#20351;&#29992;&#35821;&#22659;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;CSBMs&#65289;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#34920;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21457;&#29616;&#65306;i&#65289;&#23545;&#20110;&#22823;&#22810;&#25968;&#33410;&#28857;&#65292;&#20027;&#35201;&#30340;&#25200;&#21160;&#27169;&#22411;&#21253;&#25324;&#22823;&#37327;&#25200;&#21160;&#22270;&#36829;&#21453;&#20102;&#26410;&#25913;&#21464;&#30340;&#35821;&#20041;&#20551;&#35774;&#65307;ii&#65289;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25152;&#26377;&#35780;&#20272;&#30340;GNN&#37117;&#34920;&#29616;&#20986;&#36807;&#24230;&#40065;&#26834;&#24615;&#8212;&#8212;&#21363;&#36229;&#20986;&#35821;&#20041;&#21464;&#21270;&#28857;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#26159;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#34917;&#20805;&#29616;&#35937;&#65292;&#24182;&#26174;&#31034;&#23558;&#35757;&#32451;&#22270;&#30340;&#26631;&#31614;-&#32467;&#26500;&#21253;&#25324;&#22312;&#25512;&#29702;&#30340;&#22270;&#20013;&#21487;&#20197;&#36991;&#20813;&#36825;&#31181;&#36807;&#24230;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#23545;&#25239;&#24615;&#27010;&#24565;&#30340;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#23427;&#19982;&#26368;&#32456;&#30340;&#20027;&#21160;&#23398;&#20064;&#24615;&#33021;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many works show that node-level predictions of Graph Neural Networks (GNNs) are unrobust to small, often termed adversarial, changes to the graph structure. However, because manual inspection of a graph is difficult, it is unclear if the studied perturbations always preserve a core assumption of adversarial examples: that of unchanged semantic content. To address this problem, we introduce a more principled notion of an adversarial graph, which is aware of semantic content change. Using Contextual Stochastic Block Models (CSBMs) and real-world graphs, our results uncover: $i)$ for a majority of nodes the prevalent perturbation models include a large fraction of perturbed graphs violating the unchanged semantics assumption; $ii)$ surprisingly, all assessed GNNs show over-robustness - that is robustness beyond the point of semantic change. We find this to be a complementary phenomenon to adversarial examples and show that including the label-structure of the training graph into the infer
&lt;/p&gt;</description></item><item><title>SelfDocSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#24067;&#23616;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.00795</link><description>&lt;p&gt;
SelfDocSeg: &#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#35270;&#35273;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation. (arXiv:2305.00795v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00795
&lt;/p&gt;
&lt;p&gt;
SelfDocSeg&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#24067;&#23616;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#28085;&#30422;&#20102;&#20174;&#25991;&#26412;&#25366;&#25496;&#12289;&#35782;&#21035;&#21040;&#22522;&#20110;&#22270;&#24418;&#30340;&#34920;&#31034;&#12289;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#31561;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#24573;&#30053;&#20102;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#36825;&#19968;&#20851;&#38190;&#20107;&#23454;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#25991;&#26412;&#25366;&#25496;&#21644;&#25991;&#26412;&#26631;&#31614;&#30340;&#33258;&#25105;&#30417;&#30563;&#25991;&#26723;&#20998;&#21106;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#23436;&#20840;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#20013;&#29983;&#25104;&#20266;&#24067;&#23616;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#30495;&#23454;&#26631;&#31614;&#25110;&#20854;&#23548;&#20986;&#29289;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26694;&#26550;&#20013;&#23398;&#20064;&#25991;&#26723;&#23545;&#35937;&#30340;&#34920;&#31034;&#21644;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging from text mining, and recognition to graph-based representation, visual feature extraction, etc. However, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tun
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#22270;&#24418;&#29615;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#22270;&#36801;&#31227;&#23398;&#20064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#39046;&#22495;&#28436;&#21270;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.00664</link><description>&lt;p&gt;
&#36328;&#22270;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Transfer Learning across Graphs. (arXiv:2305.00664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#22270;&#24418;&#29615;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#22270;&#36801;&#31227;&#23398;&#20064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#39046;&#22495;&#28436;&#21270;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#65292;&#36328;&#22270;&#20256;&#36755;&#30693;&#35782;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#36816;&#36755;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#37329;&#34701;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#65292;&#32771;&#34385;&#24050;&#35266;&#23519;&#21040;&#30340;&#20855;&#26377;&#26631;&#31614;&#30340;&#28304;&#22270;&#21644;&#26631;&#31614;&#31232;&#30095;&#30340;&#30446;&#26631;&#22270;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#34920;&#24449;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#20559;&#24046;&#65292;&#24182;&#20248;&#21270;&#30446;&#26631;&#22495;&#22312;&#19979;&#19968;&#20010;&#26102;&#38388;&#25139;&#30340;&#27867;&#21270;&#24615;&#33021;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#22270;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#21270;&#30028;&#38480;&#65292;&#36825;&#24847;&#21619;&#30528;&#27867;&#21270;&#24615;&#33021;&#30001;&#39046;&#22495;&#28436;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring knowledge across graphs plays a pivotal role in many high-stake domains, ranging from transportation networks to e-commerce networks, from neuroscience to finance. To date, the vast majority of existing works assume both source and target domains are sampled from a universal and stationary distribution. However, many real-world systems are intrinsically dynamic, where the underlying domains are evolving over time. To bridge the gap, we propose to shift the problem to the dynamic setting and ask: given the label-rich source graphs and the label-scarce target graphs observed in previous T timestamps, how can we effectively characterize the evolving domain discrepancy and optimize the generalization performance of the target domain at the incoming T+1 timestamp? To answer the question, for the first time, we propose a generalization bound under the setting of dynamic transfer learning across graphs, which implies the generalization performance is dominated by domain evolution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#21160;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#20445;&#30041;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#24182;&#25429;&#25417;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.00654</link><description>&lt;p&gt;
&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;&#21644;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition. (arXiv:2305.00654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#21160;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#20445;&#30041;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#24182;&#25429;&#25417;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#29616;&#23398;&#20064;&#21644;&#25506;&#32034;&#26159;&#20219;&#20309;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#33719;&#24471;&#20445;&#30041;&#22495;&#20013;&#28508;&#22312;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#36824;&#25429;&#25417;&#20102;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#65292;&#20174;&#32780;&#20813;&#36153;&#25552;&#20379;&#20102;&#20266;&#35745;&#25968;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#23558;&#36825;&#31181;&#20998;&#35299;&#26041;&#27861;&#25512;&#24191;&#21040;&#22823;&#35268;&#27169;&#22495;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#24314;&#31435;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#20063;&#20801;&#35768;&#23567;&#25209;&#37327;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#20013;&#21560;&#21462;&#28789;&#24863;&#65292;&#24182;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#20998;&#35299;&#26041;&#27861;&#21040;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#35774;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#22312;DM-Lab-30&#29615;&#22659;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning and exploration are among the key challenges for any deep reinforcement learning agent. In this work, we provide a singular value decomposition based method that can be used to obtain representations that preserve the underlying transition structure in the domain. Perhaps interestingly, we show that these representations also capture the relative frequency of state visitations, thereby providing an estimate for pseudo-counts for free. To scale this decomposition method to large-scale domains, we provide an algorithm that never requires building the transition matrix, can make use of deep networks, and also permits mini-batch training. Further, we draw inspiration from predictive state representations and extend our decomposition method to partially observable environments. With experiments on multi-task settings with partially observable domains, we show that the proposed method can not only learn useful representation on DM-Lab-30 environments (that have inputs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00633</link><description>&lt;p&gt;
&#20998;&#35299;&#22686;&#24378;&#25512;&#29702;&#30340;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26463;&#25628;&#32034;&#32467;&#21512;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#65292;&#25105;&#20204;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#22312;GSM8K&#12289;AQUA&#21644;StrategyQA&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23569;&#37327;&#31034;&#20363;&#20934;&#30830;&#24615;&#20998;&#21035;&#36229;&#36234;&#23545;&#24212;&#30340;Codex-backboned&#22522;&#32447;$6.34\%$&#12289;$9.56\%$&#21644;$5.46\%$&#12290;&#23545;&#25105;&#20204;&#30340;&#20998;&#35299;&#24335;&#25512;&#29702;&#20998;&#26512;&#21457;&#29616;&#65292;&#23427;&#21487;&#20197;&#25351;&#20986;&#36923;&#36753;&#38169;&#35823;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#25913;&#36827;&#36807;&#21435;&#30340;&#35270;&#35273;&#37325;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#19968;&#23567;&#32452;&#22270;&#20687;&#30340;&#37319;&#26679;&#21644;&#32534;&#30721;&#27169;&#22411;&#30340;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#39640;&#36136;&#37327;&#12289;&#20445;&#30041;&#35821;&#20041;&#20869;&#23481;&#30340;&#37325;&#24314;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#35270;&#35273;&#30382;&#23618;&#19981;&#21516;&#21306;&#22495;&#30340;&#37325;&#24314;&#26102;&#38388;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.00556</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#38543;&#26426;&#25628;&#32034;&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#37325;&#24314;&#35270;&#35273;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Reconstructing seen images from human brain activity via guided stochastic search. (arXiv:2305.00556v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#25913;&#36827;&#36807;&#21435;&#30340;&#35270;&#35273;&#37325;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#19968;&#23567;&#32452;&#22270;&#20687;&#30340;&#37319;&#26679;&#21644;&#32534;&#30721;&#27169;&#22411;&#30340;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#39640;&#36136;&#37327;&#12289;&#20445;&#30041;&#35821;&#20041;&#20869;&#23481;&#30340;&#37325;&#24314;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#35270;&#35273;&#30382;&#23618;&#19981;&#21516;&#21306;&#22495;&#30340;&#37325;&#24314;&#26102;&#38388;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#37325;&#24314;&#31639;&#27861;&#26159;&#19968;&#31181;&#23558;&#33041;&#27963;&#21160;&#26144;&#23556;&#21040;&#20687;&#32032;&#30340;&#35299;&#37322;&#24037;&#20855;&#12290;&#36807;&#21435;&#30340;&#37325;&#24314;&#31639;&#27861;&#37319;&#29992;&#22823;&#35268;&#27169;&#24211;&#30340;&#26292;&#21147;&#25628;&#32034;&#26469;&#36873;&#25321;&#20505;&#36873;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#33041;&#27963;&#21160;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#26469;&#25193;&#23637;&#21644;&#25913;&#36827;&#36825;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22823;&#37096;&#20998;&#35270;&#35273;&#30382;&#23618;&#30340;&#20307;&#32032;&#20013;&#20174;&#20154;&#33041;&#27963;&#21160;&#65288;7T fMRI&#65289;&#35299;&#30721;&#20986;&#35821;&#20041;&#25551;&#36848;&#31526;&#65292;&#28982;&#21518;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#27492;&#25551;&#36848;&#31526;&#30340;&#26465;&#20214;&#19979;&#23545;&#19968;&#23567;&#32452;&#22270;&#20687;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#26679;&#26412;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#65292;&#36873;&#25321;&#26368;&#33021;&#20934;&#30830;&#39044;&#27979;&#33041;&#27963;&#21160;&#30340;&#22270;&#20687;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#22270;&#20687;&#26469;&#31181;&#23376;&#21478;&#19968;&#20010;&#24211;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#36807;&#31243;&#36890;&#36807;&#22312;&#36845;&#20195;&#20013;&#32454;&#21270;&#20302;&#32423;&#22270;&#20687;&#32454;&#33410;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20869;&#23481;&#32780;&#25910;&#25947;&#21040;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25910;&#25947;&#25152;&#38656;&#30340;&#26102;&#38388;&#22312;&#35270;&#35273;&#30382;&#23618;&#20013;&#26377;&#31995;&#32479;&#24046;&#24322;&#65292;&#34920;&#26126;&#20102;&#33041;&#21306;&#30340;&#39640;&#23618;&#25277;&#35937;&#27010;&#24565;&#38656;&#35201;&#26356;&#38271;&#30340;&#26102;&#38388;&#26469;&#38598;&#25104;&#21644;&#21453;&#26144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual reconstruction algorithms are an interpretive tool that map brain activity to pixels. Past reconstruction algorithms employed brute-force search through a massive library to select candidate images that, when passed through an encoding model, accurately predict brain activity. Here, we use conditional generative diffusion models to extend and improve this search-based strategy. We decode a semantic descriptor from human brain activity (7T fMRI) in voxels across most of visual cortex, then use a diffusion model to sample a small library of images conditioned on this descriptor. We pass each sample through an encoding model, select the images that best predict brain activity, and then use these images to seed another library. We show that this process converges on high-quality reconstructions by refining low-level image details while preserving semantic content across iterations. Interestingly, the time-to-convergence differs systematically across visual cortex, suggesting a succi
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#21253;&#25324;MRI&#22270;&#20687;&#12289;&#23481;&#31215;&#35780;&#20272;&#12289;&#20998;&#23376;&#25968;&#25454;&#21644;&#29983;&#23384;&#32454;&#33410;&#22312;&#20869;&#30340;Glioblastoma&#24739;&#32773;&#30456;&#20851;&#25968;&#25454;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19987;&#23478;&#32416;&#27491;&#30340;&#32959;&#30244;&#20122;&#21306;&#21010;&#20998;&#65292;&#20026;&#21457;&#23637;&#26415;&#21518;&#21644;&#38543;&#35775;MRI&#25195;&#25551;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.00005</link><description>&lt;p&gt;
"R&#237;o Hortega University Hospital Glioblastoma Dataset: &#19968;&#20221;&#21253;&#21547;&#26415;&#21069;&#12289;&#26415;&#21518;&#26089;&#26399;&#21644;&#22797;&#21457;MRI&#25195;&#25551;&#30340;&#32508;&#21512;&#24615;&#25968;&#25454;&#38598; (RHUH-GBM)"
&lt;/p&gt;
&lt;p&gt;
The R\'io Hortega University Hospital Glioblastoma dataset: a comprehensive collection of preoperative, early postoperative and recurrence MRI scans (RHUH-GBM). (arXiv:2305.00005v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#21253;&#25324;MRI&#22270;&#20687;&#12289;&#23481;&#31215;&#35780;&#20272;&#12289;&#20998;&#23376;&#25968;&#25454;&#21644;&#29983;&#23384;&#32454;&#33410;&#22312;&#20869;&#30340;Glioblastoma&#24739;&#32773;&#30456;&#20851;&#25968;&#25454;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19987;&#23478;&#32416;&#27491;&#30340;&#32959;&#30244;&#20122;&#21306;&#21010;&#20998;&#65292;&#20026;&#21457;&#23637;&#26415;&#21518;&#21644;&#38543;&#35775;MRI&#25195;&#25551;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24230;&#20405;&#34989;&#24615;&#30340;&#21407;&#21457;&#24615;&#33041;&#32959;&#30244;Glioblastoma&#19982;&#24739;&#32773;&#19981;&#33391;&#21518;&#26524;&#30456;&#20851;&#65292;MRI&#22312;&#35786;&#26029;&#12289;&#29305;&#24449;&#25551;&#36848;&#21644;&#39044;&#27979;Glioblastoma&#36827;&#23637;&#26041;&#38754;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#20844;&#20849;MRI&#36164;&#26009;&#24211;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#65292;&#21253;&#25324;&#26415;&#21518;&#21644;&#38543;&#35775;&#30740;&#31350;&#19981;&#36275;&#65292;&#20197;&#21450;&#19987;&#23478;&#32959;&#30244;&#20998;&#21106;&#19981;&#36275;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;R&#237;o Hortega University Hospital Glioblastoma Dataset (RHUH-GBM)&#8221;&#65292;&#19968;&#20010;&#21253;&#21547;&#22810;&#21442;&#37327;MRI&#22270;&#20687;&#12289;&#23481;&#31215;&#35780;&#20272;&#12289;&#20998;&#23376;&#25968;&#25454;&#21644;&#23545;&#36827;&#34892;&#20840;&#38754;&#25110;&#36817;&#20840;&#22686;&#24378;&#32959;&#30244;&#20999;&#38500;&#30340;Glioblastoma&#24739;&#32773;&#30340;&#29983;&#23384;&#32454;&#33410;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19987;&#23478;&#32416;&#27491;&#30340;&#32959;&#30244;&#20122;&#21306;&#21010;&#20998;&#65292;&#20026;&#21457;&#23637;&#26415;&#21518;&#21644;&#38543;&#35775;MRI&#25195;&#25551;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#25968;&#25454;&#12290;RHUH-GBM&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;&#21457;&#24067;&#22312;Glioblastoma&#30740;&#31350;&#20013;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#65292;&#20351;&#31185;&#23398;&#30028;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#27492;&#30149;&#24182;&#24320;&#23637;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Glioblastoma, a highly aggressive primary brain tumor, is associated with poor patient outcomes. Although magnetic resonance imaging (MRI) plays a critical role in diagnosing, characterizing, and forecasting glioblastoma progression, public MRI repositories present significant drawbacks, including insufficient postoperative and follow-up studies as well as expert tumor segmentations. To address these issues, we present the "R\'io Hortega University Hospital Glioblastoma Dataset (RHUH-GBM)," a collection of multiparametric MRI images, volumetric assessments, molecular data, and survival details for glioblastoma patients who underwent total or near-total enhancing tumor resection. The dataset features expert-corrected segmentations of tumor subregions, offering valuable ground truth data for developing algorithms for postoperative and follow-up MRI scans. The public release of the RHUH-GBM dataset significantly contributes to glioblastoma research, enabling the scientific community to st
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;NLP&#25216;&#26415;&#20998;&#26512;&#20581;&#36523;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.14489</link><description>&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;NLP&#25216;&#26415;&#20998;&#26512;YouTube&#30340;&#23383;&#24149;&#25968;&#25454;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Automatic Generation of Labeled Data for Video-Based Human Pose Analysis via NLP applied to YouTube Subtitles. (arXiv:2304.14489v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14489
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;NLP&#25216;&#26415;&#20998;&#26512;&#20581;&#36523;&#35270;&#39057;&#23383;&#24149;&#25968;&#25454;&#33258;&#21160;&#29983;&#25104;&#20154;&#20307;&#23039;&#21183;&#20998;&#26512;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22522;&#20110;&#35270;&#39057;&#30340;&#23621;&#23478;&#38203;&#28860;&#35780;&#20272;&#31995;&#32479;&#24050;&#25104;&#20026;&#30446;&#21069;&#30740;&#31350;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#30001;&#20110;&#19987;&#38376;&#38024;&#23545;&#36816;&#21160;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22312;&#32447;&#20581;&#36523;&#35270;&#39057;&#20016;&#23500;&#36164;&#28304;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#35270;&#39057;&#36890;&#24120;&#19981;&#20165;&#23637;&#31034;&#32451;&#20064;&#20869;&#23481;&#65292;&#36824;&#25552;&#20379;&#35821;&#35328;&#20449;&#24687;&#20316;&#20026;&#39069;&#22806;&#30340;&#20449;&#24687;&#28304;&#30340;&#20248;&#21183;&#12290;&#20197;&#20463;&#21351;&#25745;&#20026;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;NLP&#25216;&#26415;&#20998;&#26512;&#23383;&#24149;&#25968;&#25454;&#65292;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#19982;&#23039;&#21183;&#20998;&#26512;&#30456;&#20851;&#20449;&#24687;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65288;&#26080;&#20851;&#32039;&#35201;&#65292;&#30456;&#20851;&#27491;&#30830;&#65292;&#30456;&#20851;&#19981;&#27491;&#30830;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#20851;&#21098;&#36753;&#65288;$n=332$&#65289;&#20855;&#26377;&#19982;&#30456;&#20851;&#21098;&#36753;&#65288;$n=298$&#65289;&#26174;&#33879;&#19981;&#21516;&#30340;&#20851;&#33410;&#21487;&#35265;&#24615;&#20540;&#12290;&#26816;&#26597;&#32858;&#31867;&#20013;&#24515;&#20063;&#23637;&#29616;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent advancements in computer vision as well as machine learning (ML), video-based at-home exercise evaluation systems have become a popular topic of current research. However, performance depends heavily on the amount of available training data. Since labeled datasets specific to exercising are rare, we propose a method that makes use of the abundance of fitness videos available online. Specifically, we utilize the advantage that videos often not only show the exercises, but also provide language as an additional source of information. With push-ups as an example, we show that through the analysis of subtitle data using natural language processing (NLP), it is possible to create a labeled (irrelevant, relevant correct, relevant incorrect) dataset containing relevant information for pose analysis. In particular, we show that irrelevant clips ($n=332$) have significantly different joint visibility values compared to relevant clips ($n=298$). Inspecting cluster centroids also show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;JaxPruner&#65292;&#19968;&#27454;&#29992;&#20110;&#30740;&#31350;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#28304;&#24211;&#12290;JaxPruner&#25552;&#20379;&#20102;&#27969;&#34892;&#30340;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#30340;&#31616;&#26126;&#23454;&#29616;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#24182;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;JAX&#24211;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.14082</link><description>&lt;p&gt;
JaxPruner&#65306;&#19968;&#20010;&#29992;&#20110;&#31232;&#30095;&#24615;&#30740;&#31350;&#30340;&#31616;&#26126;&#24211;
&lt;/p&gt;
&lt;p&gt;
JaxPruner: A concise library for sparsity research. (arXiv:2304.14082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JaxPruner&#65292;&#19968;&#27454;&#29992;&#20110;&#30740;&#31350;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#28304;&#24211;&#12290;JaxPruner&#25552;&#20379;&#20102;&#27969;&#34892;&#30340;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#30340;&#31616;&#26126;&#23454;&#29616;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#24182;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;JAX&#24211;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JaxPruner&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;JAX&#30340;&#24320;&#28304;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#27969;&#34892;&#30340;&#21098;&#26525;&#21644;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#30340;&#31616;&#26126;&#23454;&#29616;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#24310;&#36831;&#24320;&#38144;&#65292;&#21152;&#36895;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#12290;JaxPruner&#23454;&#29616;&#30340;&#31639;&#27861;&#20351;&#29992;&#36890;&#29992;API&#65292;&#24182;&#19982;&#27969;&#34892;&#30340;&#20248;&#21270;&#24211;Optax&#26080;&#32541;&#21327;&#20316;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#36731;&#26494;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;JAX&#24211;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#20013;&#25552;&#20379;&#31034;&#20363;&#24182;&#22312;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#22522;&#20934;&#23454;&#39564;&#26469;&#23637;&#31034;&#36825;&#31181;&#38598;&#25104;&#30340;&#20415;&#25463;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks.
&lt;/p&gt;</description></item><item><title>RubikONNs&#21033;&#29992;&#20809;&#23398;&#31995;&#32479;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#36890;&#36807;&#26059;&#36716;&#30828;&#20214;&#32534;&#30721;&#22810;&#20010;&#21069;&#39304;&#20989;&#25968;&#65292;&#20026;ONNs&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.12985</link><description>&lt;p&gt;
Rubik&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65306;&#20855;&#26377;&#29289;&#29702;&#24863;&#30693;&#26059;&#36716;&#32467;&#26500;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture. (arXiv:2304.12985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12985
&lt;/p&gt;
&lt;p&gt;
RubikONNs&#21033;&#29992;&#20809;&#23398;&#31995;&#32479;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#36890;&#36807;&#26059;&#36716;&#30828;&#20214;&#32534;&#30721;&#22810;&#20010;&#21069;&#39304;&#20989;&#25968;&#65292;&#20026;ONNs&#23454;&#29616;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24037;&#20316;&#22312;&#25512;&#36827;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65288;ONNs&#65289;&#65292;&#22312;&#21151;&#29575;&#25928;&#29575;&#65292;&#24182;&#34892;&#24615;&#21644;&#35745;&#31639;&#36895;&#24230;&#26041;&#38754;&#65292;ONNs&#24102;&#26469;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there are increasing efforts on advancing optical neural networks (ONNs), which bring significant advantages for machine learning (ML) in terms of power efficiency, parallelism, and computational speed. With the considerable benefits in computation speed and energy efficiency, there are significant interests in leveraging ONNs into medical sensing, security screening, drug detection, and autonomous driving. However, due to the challenge of implementing reconfigurability, deploying multi-task learning (MTL) algorithms on ONNs requires re-building and duplicating the physical diffractive systems, which significantly degrades the energy and cost efficiency in practical application scenarios. This work presents a novel ONNs architecture, namely, \textit{RubikONNs}, which utilizes the physical properties of optical systems to encode multiple feed-forward functions by physically rotating the hardware similarly to rotating a \textit{Rubik's Cube}. To optimize MTL performance on Rubi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;</title><link>http://arxiv.org/abs/2304.08658</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#22312;DED&#25171;&#21360;SS316L&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion. (arXiv:2304.08658v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22768;&#21457;&#23556;&#65288;AE&#65289;&#31561;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#26102;&#39057;&#27169;&#24335;&#19982;DED&#36807;&#31243;&#20013;&#30340;&#23380;&#38553;&#29575;&#24418;&#25104;&#36827;&#34892;&#39640;&#31354;&#38388;&#65288;0.5mm&#65289;&#21644;&#26102;&#38388;&#65288;&lt;1ms&#65289;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#20013;&#30340;LIME&#65288;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#38750;&#29305;&#23450;&#24615;&#35299;&#37322;&#65289;&#65292;&#23558;AE&#20013;&#30340;&#26576;&#20123;&#39640;&#39057;&#27874;&#24418;&#29305;&#24449;&#24402;&#22240;&#20110;DED&#36807;&#31243;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#23380;&#38553;&#24418;&#25104;&#36884;&#24452;&#65306;&#39134;&#28293;&#20107;&#20214;&#21644;&#20302;&#28909;&#37327;&#36755;&#20837;&#19979;&#30456;&#37051;&#25171;&#21360;&#36712;&#36857;&#30340;&#19981;&#20805;&#20998;&#29076;&#21512;&#12290;&#35813;&#26041;&#27861;&#20026;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#65288;0.5mm&#65289;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#26159;&#19982;&#20808;&#21069;&#21162;&#21147;&#30456;&#27604;&#30340;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;&#22312;&#25171;&#21360;&#24182;&#38543;&#21518;&#21152;&#24037;SS316L&#26448;&#26009;&#26679;&#21697;&#26102;&#65292;&#21516;&#27493;&#37319;&#38598;&#20102;&#21253;&#25324;&#21147;&#65292;AE&#65292;&#25391;&#21160;&#21644;&#28201;&#24230;&#22312;&#20869;&#30340;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#29992;&#20110;&#35782;&#21035;&#20004;&#31181;&#23380;&#38553;&#24418;&#25104;&#36884;&#24452;&#30340;AE&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#36827;&#19968;&#27493;&#20998;&#26512;&#36825;&#20123;&#29305;&#24449;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to relate the time-frequency patterns of acoustic emission (AE) and other multi-modal sensor data collected in a hybrid directed energy deposition (DED) process to the pore formations at high spatial (0.5 mm) and time (&lt; 1ms) resolutions. Adapting an explainable AI method in LIME (Local Interpretable Model-Agnostic Explanations), certain high-frequency waveform signatures of AE are to be attributed to two major pathways for pore formation in a DED process, namely, spatter events and insufficient fusion between adjacent printing tracks from low heat input. This approach opens an exciting possibility to predict, in real-time, the presence of a pore in every voxel (0.5 mm in size) as they are printed, a major leap forward compared to prior efforts. Synchronized multimodal sensor data including force, AE, vibration and temperature were gathered while an SS316L material sample was printed and subsequently machined. A deep convolution neural network classifier was used to ide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24182;&#36755;&#20986;&#34913;&#37327;&#20540;&#30340;&#36710;&#36742;&#33021;&#25928;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#38477;&#20302;&#30899;&#36275;&#36857;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07073</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#36710;&#36742;&#33021;&#25928;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Vehicle Energy Efficiency Prediction using an Ensemble of Neural Networks. (arXiv:2304.07073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24182;&#36755;&#20986;&#34913;&#37327;&#20540;&#30340;&#36710;&#36742;&#33021;&#25928;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#38477;&#20302;&#30899;&#36275;&#36857;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#36755;&#37096;&#38376;&#32422;&#21344;&#20840;&#29699;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;25&#65285;&#12290;&#22240;&#27492;&#65292;&#22312;&#20132;&#36890;&#37096;&#38376;&#25552;&#39640;&#33021;&#25928;&#26159;&#20943;&#23569;&#30899;&#36275;&#36857;&#30340;&#20851;&#38190;&#12290;&#33021;&#25928;&#36890;&#24120;&#20197;&#27599;&#34892;&#39542;&#36317;&#31163;&#30340;&#33021;&#28304;&#28040;&#32791;&#26469;&#34913;&#37327;&#65292;&#20363;&#22914;&#27599;&#20844;&#37324;&#30340;&#29123;&#27833;&#21319;&#25968;&#12290;&#24433;&#21709;&#33021;&#25928;&#30340;&#20027;&#35201;&#22240;&#32032;&#21253;&#25324;&#36710;&#36742;&#31867;&#22411;&#65292;&#29615;&#22659;&#65292;&#39550;&#39542;&#21592;&#34892;&#20026;&#21644;&#22825;&#27668;&#26465;&#20214;&#12290;&#36825;&#20123;&#19981;&#21516;&#30340;&#22240;&#32032;&#24341;&#20837;&#20102;&#20272;&#35745;&#36710;&#36742;&#33021;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24182;&#36755;&#20986;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#34913;&#37327;&#20540;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20844;&#24320;&#21487;&#24471;&#30340;&#36710;&#36742;&#33021;&#28304;&#25968;&#25454;&#38598;&#65288;VED&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#36742;&#36710;&#21644;&#33021;&#28304;&#31867;&#22411;&#30340;&#20960;&#20010;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#36755;&#20986;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#34913;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transportation sector accounts for about 25% of global greenhouse gas emissions. Therefore, an improvement of energy efficiency in the traffic sector is crucial to reducing the carbon footprint. Efficiency is typically measured in terms of energy use per traveled distance, e.g. liters of fuel per kilometer. Leading factors that impact the energy efficiency are the type of vehicle, environment, driver behavior, and weather conditions. These varying factors introduce uncertainty in estimating the vehicles' energy efficiency. We propose in this paper an ensemble learning approach based on deep neural networks (ENN) that is designed to reduce the predictive uncertainty and to output measures of such uncertainty. We evaluated it using the publicly available Vehicle Energy Dataset (VED) and compared it with several baselines per vehicle and energy type. The results showed a high predictive performance and they allowed to output a measure of predictive uncertainty.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03279</link><description>&lt;p&gt;
&#22870;&#21169;&#26159;&#21542;&#21512;&#29702;&#65311;&#22312; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#20013;&#34913;&#37327;&#22870;&#21169;&#19982;&#36947;&#24503;&#34892;&#20026;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#34987;&#35757;&#32451;&#25104;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#36861;&#27714;&#26435;&#21147;&#21644;&#27450;&#39575;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#21487;&#33021;&#20250;&#28608;&#21169;&#26377;&#23475;&#34892;&#20026;&#12290;&#37027;&#20040;&#20195;&#29702;&#26159;&#21542;&#33258;&#28982;&#32780;&#28982;&#22320;&#23398;&#20250;&#20102;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#65311;&#25105;&#20204;&#22914;&#20309;&#22312; GPT-4 &#31561;&#36890;&#29992;&#27169;&#22411;&#20013;&#34913;&#37327;&#36825;&#20123;&#34892;&#20026;&#21602;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; MACHIAVELLI &#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#22810;&#26679;&#21270;&#30340;&#24773;&#26223;&#65292;&#37325;&#28857;&#20851;&#27880;&#31038;&#20250;&#20915;&#31574;&#21046;&#23450;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#20195;&#29702;&#26159;&#21542;&#34920;&#29616;&#20986;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#12290;&#25105;&#20204;&#25968;&#23398;&#21270;&#20102;&#25968;&#21313;&#31181;&#26377;&#23475;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#30340;&#27880;&#37322;&#26469;&#35780;&#20272;&#20195;&#29702;&#20542;&#21521;&#20110;&#36861;&#27714;&#26435;&#21147;&#65292;&#36896;&#25104;&#21151;&#33021;&#19981;&#33391;&#21644;&#36829;&#21453;&#20262;&#29702;&#30340;&#20542;&#21521;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#34892;&#20026;&#30340;&#36947;&#24503;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#32039;&#24352;&#20851;&#31995;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#20195;&#29702;&#36235;&#21521;&#20110;&#37319;&#21462;&#26356;&#23569;&#30340;&#26377;&#23475;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MACHIAVELLI &#26159;&#35780;&#20272;&#20154;&#24037;&#20195;&#29702;&#39532;&#22522;&#38597;&#32500;&#21033;&#34892;&#20026;&#27700;&#24179;&#30340;&#26377;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;GAN&#23457;&#26680;&#26041;&#27861;&#65292;&#22312;&#19982;&#24050;&#24314;&#31435;&#8220;&#21442;&#32771;&#8221;GAN&#36827;&#34892;&#27604;&#36739;&#21518;&#65292;&#32852;&#21512;&#35782;&#21035;&#26032;&#24320;&#21457;&#30340;GAN&#20013;&#21487;&#29702;&#35299;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;GAN&#20043;&#38388;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#30340;&#30452;&#35266;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.10774</link><description>&lt;p&gt;
&#36328;GAN&#23457;&#26680;&#65306;&#26080;&#30417;&#30563;&#35782;&#21035;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#23646;&#24615;&#32423;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences between Pretrained Generative Models. (arXiv:2303.10774v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;GAN&#23457;&#26680;&#26041;&#27861;&#65292;&#22312;&#19982;&#24050;&#24314;&#31435;&#8220;&#21442;&#32771;&#8221;GAN&#36827;&#34892;&#27604;&#36739;&#21518;&#65292;&#32852;&#21512;&#35782;&#21035;&#26032;&#24320;&#21457;&#30340;GAN&#20013;&#21487;&#29702;&#35299;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;GAN&#20043;&#38388;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#30340;&#30452;&#35266;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#20998;&#24067;&#21644;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#38590;&#24230;&#26497;&#22823;&#12290;&#36825;&#20419;&#20351;&#20102;&#23545;&#24050;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#23457;&#35745;&#30340;&#38656;&#27714;&#65292;&#20197;&#20415;&#20197;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#26684;&#24335;&#35782;&#21035;&#20559;&#35265;&#25110;&#30830;&#20445;&#20844;&#24179;&#24615;&#12290;&#29616;&#26377;&#30340;GAN&#23457;&#35745;&#24037;&#20855;&#20165;&#38480;&#20110;&#22522;&#20110;&#24635;&#32467;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;FID&#25110;&#21484;&#22238;&#29575;&#65289;&#30340;&#31895;&#31890;&#24230;&#27169;&#22411;-&#25968;&#25454;&#27604;&#36739;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#19982;&#20043;&#21069;&#30340;&#22522;&#32447;GAN&#30456;&#27604;&#36739;&#30340;&#36884;&#24452;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36328;GAN&#23457;&#26680;&#65288;xGA&#65289;&#65292;&#35813;&#26041;&#27861;&#32852;&#21512;&#35782;&#21035;&#20986;&#21487;&#29702;&#35299;&#30340;&#23646;&#24615;&#65292;&#36825;&#20123;&#23646;&#24615;&#21487;&#20197;&#26159;&#20004;&#20010;GAN&#20849;&#26377;&#30340;&#65292;&#26159;&#23458;&#25143;&#31471;GAN&#30340;&#26032;&#39062;&#23646;&#24615;&#65292;&#25110;&#32773;&#26159;&#23458;&#25143;&#31471;GAN&#32570;&#23569;&#30340;&#23646;&#24615;&#12290;&#36825;&#20026;&#29992;&#25143;&#21644;&#27169;&#22411;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;GAN&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#22522;&#20110;&#23646;&#24615;&#30340;GAN&#23457;&#26680;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#25351;&#26631;&#26469;&#27604;&#36739;&#29616;&#26377;&#30340;&#23457;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) are notoriously difficult to train especially for complex distributions and with limited data. This has driven the need for tools to audit trained networks in human intelligible format, for example, to identify biases or ensure fairness. Existing GAN audit tools are restricted to coarse-grained, model-data comparisons based on summary statistics such as FID or recall. In this paper, we propose an alternative approach that compares a newly developed GAN against a prior baseline. To this end, we introduce Cross-GAN Auditing (xGA) that, given an established "reference" GAN and a newly proposed "client" GAN, jointly identifies intelligible attributes that are either common across both GANs, novel to the client GAN, or missing from the client GAN. This provides both users and model developers an intuitive assessment of similarity and differences between GANs. We introduce novel metrics to evaluate attribute-based GAN auditing approaches and use these m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#21270;&#23618;&#8212;&#8212;ContraNorm&#65292;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30772;&#22351;&#34920;&#31034;&#65292;&#32531;&#35299;&#20102;&#23436;&#20840;&#22604;&#38519;&#21644;&#32500;&#24230;&#22604;&#38519;&#30340;&#29616;&#35937;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06562</link><description>&lt;p&gt;
ContraNorm: &#23545;&#20110;&#36807;&#24230;&#24179;&#28369;&#30340;&#23545;&#27604;&#23398;&#20064;&#35270;&#35282;&#21644;&#26356;&#22810;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond. (arXiv:2303.06562v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#21270;&#23618;&#8212;&#8212;ContraNorm&#65292;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#24335;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#30772;&#22351;&#34920;&#31034;&#65292;&#32531;&#35299;&#20102;&#23436;&#20840;&#22604;&#38519;&#21644;&#32500;&#24230;&#22604;&#38519;&#30340;&#29616;&#35937;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#24179;&#28369;&#29616;&#35937;&#22312;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24403;&#23618;&#25968;&#22686;&#21152;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#21464;&#24046;&#12290;&#25105;&#20204;&#20174;&#32500;&#24230;&#25240;&#21472;&#30340;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#35270;&#35282;&#26469;&#25551;&#36848;&#36807;&#24230;&#24179;&#28369;&#30340;&#29616;&#35937;&#65292;&#34920;&#31034;&#20250;&#32858;&#21040;&#19968;&#20010;&#29421;&#31364;&#30340;&#38181;&#24418;&#31354;&#38388;&#20013;&#65292;&#32780;&#19981;&#26159;&#34920;&#31034;&#20250;&#32858;&#21040;&#19968;&#20010;&#28857;&#19978;&#12290;&#21463;&#21040;&#23545;&#25239;&#24615;&#23398;&#20064;&#22312;&#38450;&#27490;&#32500;&#24230;&#25240;&#21472;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#33539;&#21270;&#23618;&#8212;&#8212;ContraNorm&#12290;&#30452;&#35266;&#19978;&#65292;ContraNorm&#20250;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#38544;&#24335;&#30772;&#22351;&#34920;&#31034;&#65292;&#23548;&#33268;&#26356;&#22343;&#21248;&#30340;&#20998;&#24067;&#21644;&#36731;&#24494;&#30340;&#32500;&#24230;&#25240;&#21472;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;ContraNorm&#21487;&#20197;&#32531;&#35299;&#23436;&#20840;&#22604;&#38519;&#21644;&#32500;&#24230;&#22604;&#38519;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35268;&#33539;&#21270;&#23618;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;GNNs&#21644;Transformers&#20013;&#65292;&#19988;&#21442;&#25968;&#24320;&#38144;&#24456;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#21487;&#20197;&#25552;&#39640;GNNs&#21644;Transformers&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks (GNNs) and Transformers, where performance worsens as the number of layers increases. Instead of characterizing oversmoothing from the view of complete collapse in which representations converge to a single point, we dive into a more general perspective of dimensional collapse in which representations lie in a narrow cone. Accordingly, inspired by the effectiveness of contrastive learning in preventing dimensional collapse, we propose a novel normalization layer called ContraNorm. Intuitively, ContraNorm implicitly shatters representations in the embedding space, leading to a more uniform distribution and a slighter dimensional collapse. On the theoretical analysis, we prove that ContraNorm can alleviate both complete collapse and dimensional collapse under certain conditions. Our proposed normalization layer can be easily integrated into GNNs and Transformers with negligible parameter overhead. Experiments o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29289;&#29702;&#32422;&#26463;&#19979;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#26469;&#23398;&#20064;&#32858;&#37232;&#33018;&#32435;&#31859;&#23380;&#20013;&#31163;&#23376;&#30340;&#20256;&#36755;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2303.04594</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#19979;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#23398;&#20064;&#22810;&#31163;&#23376;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained neural differential equations for learning multi-ionic transport. (arXiv:2303.04594v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29289;&#29702;&#32422;&#26463;&#19979;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#26469;&#23398;&#20064;&#32858;&#37232;&#33018;&#32435;&#31859;&#23380;&#20013;&#31163;&#23376;&#30340;&#20256;&#36755;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#23376;&#36890;&#36807;&#32858;&#37232;&#33018;&#32435;&#31859;&#23380;&#30340;&#36830;&#32493;&#27169;&#22411;&#38656;&#35201;&#36890;&#36807;&#22797;&#26434;&#30340;&#23380;&#38553;&#20960;&#20309;&#24418;&#29366;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#36825;&#20010;&#38271;&#24230;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#35299;&#20915;&#26102;&#31354;&#29305;&#24449;&#21487;&#33021;&#20250;&#20351;&#35299;&#20915;&#36825;&#20123;&#26041;&#31243;&#30340;&#35745;&#31639;&#22797;&#26434;&#12290;&#27492;&#22806;&#65292;&#26426;&#21046;&#27169;&#22411;&#32463;&#24120;&#38656;&#35201;&#22312;&#32435;&#31859;&#38480;&#21046;&#19979;&#31163;&#23376;&#30456;&#20114;&#20316;&#29992;&#21442;&#25968;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#36825;&#36890;&#24120;&#26159;&#23454;&#39564;&#19978;&#22826;&#20855;&#25361;&#25112;&#24615;&#25110;&#38590;&#20197;&#30693;&#26195;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#29289;&#29702;&#32422;&#26463;&#36793;&#30028;&#19979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#23398;&#20064;&#32858;&#37232;&#33018;&#32435;&#31859;&#23380;&#20013;&#30340;&#31163;&#23376;&#20256;&#36755;&#34892;&#20026;&#12290;&#25152;&#25552;&#20986;&#30340;&#20307;&#31995;&#32467;&#26500;&#32467;&#21512;&#20102;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#21644;&#32463;&#20856;&#38381;&#21512;&#27169;&#22411;&#20316;&#20026;&#24402;&#32435;&#20559;&#24046;&#65292;&#30452;&#25509;&#32534;&#30721;&#21040;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#36890;&#36807;&#27169;&#25311;&#36830;&#32493;&#27169;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#29420;&#31435;&#30340;&#23454;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#23398;&#20064;&#31163;&#23376;&#25490;&#26021;&#34892;&#20026;&#12290;&#20174;&#39640;&#26031;&#22122;&#22768;&#22686;&#24378;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuum models for ion transport through polyamide nanopores require solving partial differential equations (PDEs) through complex pore geometries. Resolving spatiotemporal features at this length and time-scale can make solving these equations computationally intractable. In addition, mechanistic models frequently require functional relationships between ion interaction parameters under nano-confinement, which are often too challenging to measure experimentally or know a priori. In this work, we develop the first physics-informed deep learning model to learn ion transport behaviour across polyamide nanopores. The proposed architecture leverages neural differential equations in conjunction with classical closure models as inductive biases directly encoded into the neural framework. The neural differential equations are pre-trained on simulated data from continuum models and fine-tuned on independent experimental data to learn ion rejection behaviour. Gaussian noise augmentations from
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32806;&#21512;&#22810;&#21464;&#37327;&#26144;&#23556;&#38382;&#39064;&#30340;&#38590;&#28857;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.02304</link><description>&lt;p&gt;
&#38024;&#23545;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Coupled Multiwavelet Neural Operator Learning for Coupled Partial Differential Equations. (arXiv:2303.02304v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32806;&#21512;&#22810;&#21464;&#37327;&#26144;&#23556;&#38382;&#39064;&#30340;&#38590;&#28857;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#20915;&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32806;&#21512;&#20559;&#24494;&#20998;&#26041;&#31243;&#26159;&#25551;&#36848;&#35768;&#22810;&#29289;&#29702;&#36807;&#31243;&#22797;&#26434;&#21160;&#24577;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#31639;&#23376;&#24050;&#32463;&#23637;&#31034;&#20986;&#36890;&#36807;&#22312;&#20613;&#37324;&#21494;/&#23567;&#27874;&#31354;&#38388;&#30452;&#25509;&#23398;&#20064;&#31215;&#20998;&#26680;&#26469;&#35299;&#20915;PDE&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#32806;&#21512;PDE&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#38590;&#28857;&#22312;&#20110;&#22788;&#29702;&#20989;&#25968;&#20043;&#38388;&#30340;&#32806;&#21512;&#26144;&#23556;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32806;&#21512;&#22810;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#65288;CMWNO&#65289;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#23567;&#27874;&#31354;&#38388;&#20013;&#36827;&#34892;&#22810;&#23567;&#27874;&#20998;&#35299;&#21644;&#37325;&#26500;&#36807;&#31243;&#20013;&#35299;&#32806;&#21512;&#31215;&#20998;&#26680;&#12290;&#22312;&#35299;&#20915;Gray-Scott&#65288;GS&#65289;&#26041;&#31243;&#21644;&#38750;&#23616;&#37096;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#31561;&#32806;&#21512;PDE&#26041;&#38754;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#20808;&#21069;&#22522;&#20110;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;$L^2$&#35823;&#24046;&#34920;&#29616;&#20986;&#20102;$2\times \sim 4\times$&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coupled partial differential equations (PDEs) are key tasks in modeling the complex dynamics of many physical processes. Recently, neural operators have shown the ability to solve PDEs by learning the integral kernel directly in Fourier/Wavelet space, so the difficulty for solving the coupled PDEs depends on dealing with the coupled mappings between the functions. Towards this end, we propose a \textit{coupled multiwavelets neural operator} (CMWNO) learning scheme by decoupling the coupled integral kernels during the multiwavelet decomposition and reconstruction procedures in the Wavelet space. The proposed model achieves significantly higher accuracy compared to previous learning-based solvers in solving the coupled PDEs including Gray-Scott (GS) equations and the non-local mean field game (MFG) problem. According to our experimental results, the proposed model exhibits a $2\times \sim 4\times$ improvement relative $L$2 error compared to the best results from the state-of-the-art mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05881</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#25968;&#20540;&#20808;&#39564;&#30340;&#24191;&#20041;CP&#20998;&#35299;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#34917;&#20840;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#36825;&#19968;&#31867;&#21035;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#23545;&#34917;&#20840;&#24352;&#37327;&#26045;&#21152;&#20302;&#31209;&#32467;&#26500;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#32771;&#34385;&#21040;&#24352;&#37327;&#20803;&#32032;&#30340;&#25968;&#20540;&#20808;&#39564;&#20449;&#24687;&#12290;&#24573;&#30053;&#25968;&#20540;&#20808;&#39564;&#23558;&#23548;&#33268;&#20002;&#22833;&#20851;&#20110;&#25968;&#25454;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#22240;&#27492;&#38459;&#27490;&#31639;&#27861;&#36798;&#21040;&#26368;&#20248;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21517;&#20026;GCDTC&#65288;&#24191;&#20041;CP&#20998;&#35299;&#24352;&#37327;&#34917;&#20840;&#65289;&#65292;&#20197;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#26032;&#24341;&#20837;&#30340;&#26694;&#26550;&#20013;&#65292;&#23558;&#24191;&#20041;&#30340;CP&#20998;&#35299;&#24212;&#29992;&#20110;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPTC&#65288;&#24179;&#28369;&#27850;&#26494;&#24352;&#37327;&#34917;&#20840;&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#36127;&#25972;&#25968;&#24352;&#37327;&#34917;&#20840;&#65292;&#20316;&#20026;GCDTC&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#30340;&#24352;&#37327;&#34917;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26679;&#26412;&#19981;&#24179;&#34913;&#21450;&#26631;&#35760;&#19981;&#31934;&#30830;&#38382;&#39064;&#36827;&#34892;&#25913;&#36827;&#65292;&#20165;&#29992;&#27491;&#24120;&#34892;&#20026;&#25968;&#25454;&#30340;20&#65285;&#21363;&#21487;&#35757;&#32451;&#20855;&#31454;&#20105;&#21147;&#30340;&#30196;&#21574;&#30151;&#24739;&#32773;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.03224</link><description>&lt;p&gt;
&#29992;&#20110;&#25913;&#21892;&#30196;&#21574;&#30151;&#24739;&#32773;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#30340;&#19979;&#37319;&#26679;&#21644;&#32047;&#31215;&#31867;&#20915;&#31574;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Undersampling and Cumulative Class Re-decision Methods to Improve Detection of Agitation in People with Dementia. (arXiv:2302.03224v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26679;&#26412;&#19981;&#24179;&#34913;&#21450;&#26631;&#35760;&#19981;&#31934;&#30830;&#38382;&#39064;&#36827;&#34892;&#25913;&#36827;&#65292;&#20165;&#29992;&#27491;&#24120;&#34892;&#20026;&#25968;&#25454;&#30340;20&#65285;&#21363;&#21487;&#35757;&#32451;&#20855;&#31454;&#20105;&#21147;&#30340;&#30196;&#21574;&#30151;&#24739;&#32773;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#28608;&#21160;&#26159;&#30196;&#21574;&#30151;&#24739;&#32773;&#26368;&#24120;&#35265;&#30340;&#30151;&#29366;&#20043;&#19968;&#65292;&#21487;&#33021;&#20250;&#23545;&#20182;&#20204;&#21644;&#30475;&#25252;&#32773;&#30340;&#23433;&#20840;&#36896;&#25104;&#23041;&#32961;&#12290;&#24320;&#21457;&#23458;&#35266;&#30340;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#26041;&#27861;&#23545;&#25903;&#25345;&#22312;&#23621;&#20303;&#29615;&#22659;&#20013;&#29983;&#27963;&#30340;&#30196;&#21574;&#30151;&#24739;&#32773;&#30340;&#20581;&#24247;&#21644;&#23433;&#20840;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#19981;&#21516;&#30340;&#19979;&#37319;&#26679;&#26041;&#27861;&#28040;&#38500;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#21152;&#26435;&#19979;&#37319;&#26679;&#26041;&#27861;&#26469;&#35780;&#20272;&#25163;&#21160;&#26631;&#35760;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#25913;&#21892;&#30196;&#21574;&#30151;&#24739;&#32773;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21482;&#26377;&#27491;&#24120;&#34892;&#20026;&#25968;&#25454;&#30340;20&#65285;&#36275;&#20197;&#35757;&#32451;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24773;&#32490;&#28608;&#21160;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agitation is one of the most prevalent symptoms in people with dementia (PwD) that can place themselves and the caregiver's safety at risk. Developing objective agitation detection approaches is important to support health and safety of PwD living in a residential setting. In a previous study, we collected multimodal wearable sensor data from 17 participants for 600 days and developed machine learning models for predicting agitation in one-minute windows. However, there are significant limitations in the dataset, such as imbalance problem and potential imprecise labels as the occurrence of agitation is much rarer in comparison to the normal behaviours. In this paper, we first implement different undersampling methods to eliminate the imbalance problem, and come to the conclusion that only 20\% of normal behaviour data are adequate to train a competitive agitation detection model. Then, we design a weighted undersampling method to evaluate the manual labeling mechanism given the ambiguo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#27969;&#65288;NF&#65289;&#38598;&#21512;&#26469;&#20272;&#35745;Epistemic&#19981;&#30830;&#23450;&#24615;&#21644;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;dropout&#25513;&#30721;&#26469;&#21019;&#24314;&#38598;&#21512;&#65292;&#36816;&#29992;&#20110;&#21508;&#31181;&#23454;&#39564;&#24182;&#21487;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2302.01312</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#27969;&#38598;&#21512;&#29992;&#20110;&#20016;&#23500;&#30340;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flow Ensembles for Rich Aleatoric and Epistemic Uncertainty Modeling. (arXiv:2302.01312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#27969;&#65288;NF&#65289;&#38598;&#21512;&#26469;&#20272;&#35745;Epistemic&#19981;&#30830;&#23450;&#24615;&#21644;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;dropout&#25513;&#30721;&#26469;&#21019;&#24314;&#38598;&#21512;&#65292;&#36816;&#29992;&#20110;&#21508;&#31181;&#23454;&#39564;&#24182;&#21487;&#20197;&#25552;&#20379;&#20840;&#38754;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21487;&#38752;&#22320;&#20272;&#35745;Epistemic&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25429;&#25417;&#22797;&#26434;Aleatoric&#20998;&#24067;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#27969;&#65288;NF&#65289;&#38598;&#21512;&#65292;&#36825;&#26159;&#24314;&#27169;Aleatoric&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#38598;&#21512;&#26159;&#36890;&#36807;&#22266;&#23450;&#30340;dropout&#25513;&#30721;&#38598;&#21512;&#21019;&#24314;&#30340;&#65292;&#27604;&#21019;&#24314;&#21333;&#29420;&#30340;NF&#27169;&#22411;&#26356;&#21152;&#32463;&#27982;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;NF&#30340;&#29420;&#29305;&#32467;&#26500;&#8212;&#8212;&#22522;&#30784;&#20998;&#24067;&#8212;&#8212;&#26469;&#20272;&#35745;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#26679;&#26412;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#32447;&#65292;&#24182;&#25512;&#23548;&#20986;&#26080;&#20559;&#30340;&#24494;&#20998;&#29109;&#20272;&#35745;&#20540;&#12290;&#35813;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#39564;&#20013;&#65306;1D&#27491;&#24358;&#25968;&#25454;&#65292;2D&#26377;&#39118;&#26684;&#32593;&#26684;&#19990;&#30028;&#65288;$\it{Wet Chicken}$&#65289;&#65292;$\it{Pendulum}$&#21644;$\it{Hopper}$&#12290;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#20102;&#27599;&#20010;&#27169;&#22411;&#22312;&#27979;&#37327;Aleatoric&#21644;Epistemic&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we demonstrate how to reliably estimate epistemic uncertainty while maintaining the flexibility needed to capture complicated aleatoric distributions. To this end, we propose an ensemble of Normalizing Flows (NF), which are state-of-the-art in modeling aleatoric uncertainty. The ensembles are created via sets of fixed dropout masks, making them less expensive than creating separate NF models. We demonstrate how to leverage the unique structure of NFs, base distributions, to estimate aleatoric uncertainty without relying on samples, provide a comprehensive set of baselines, and derive unbiased estimates for differential entropy. The methods were applied to a variety of experiments, commonly used to benchmark aleatoric and epistemic uncertainty estimation: 1D sinusoidal data, 2D windy grid-world ($\it{Wet Chicken}$), $\it{Pendulum}$, and $\it{Hopper}$. In these experiments, we setup an active learning framework and evaluate each model's capability at measuring aleatoric and
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#20013;&#30340;&#22797;&#29616;&#36880;&#28176;&#37325;&#26500;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#39537;&#21160;&#20449;&#21495;&#65292;&#20174;&#32780;&#21487;&#38752;&#22320;&#25512;&#26029;&#20849;&#20139;&#22240;&#26524;&#39537;&#21160;&#32773;&#65292;&#20854;&#24050;&#22312;&#22810;&#20010;&#31034;&#20363;&#20013;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.13516</link><description>&lt;p&gt;
&#24490;&#29615;&#25581;&#31034;&#20102;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#20849;&#20139;&#22240;&#26524;&#39537;&#21160;&#32773;
&lt;/p&gt;
&lt;p&gt;
Recurrences reveal shared causal drivers of complex time series. (arXiv:2301.13516v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#20013;&#30340;&#22797;&#29616;&#36880;&#28176;&#37325;&#26500;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#39537;&#21160;&#20449;&#21495;&#65292;&#20174;&#32780;&#21487;&#38752;&#22320;&#25512;&#26029;&#20849;&#20139;&#22240;&#26524;&#39537;&#21160;&#32773;&#65292;&#20854;&#24050;&#22312;&#22810;&#20010;&#31034;&#20363;&#20013;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#39564;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#20849;&#20139;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#22240;&#26524;&#39537;&#21160;&#22120;&#12290;&#20363;&#22914;&#65292;&#21463;&#36716;&#24405;&#22240;&#23376;&#38774;&#21521;&#30340;&#22522;&#22240;&#12289;&#21463;&#22823;&#23610;&#24230;&#22823;&#27668;&#29615;&#27969;&#24433;&#21709;&#30340;&#28023;&#27915;&#27969;&#21160;&#65292;&#20197;&#21450;&#34987;&#19979;&#38477;&#31070;&#32463;&#20803;&#25511;&#21046;&#30340;&#30005;&#26426;&#30005;&#36335;&#12290;&#21487;&#38752;&#22320;&#25512;&#26029;&#36825;&#31181;&#30475;&#19981;&#35265;&#30340;&#39537;&#21160;&#21147;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#20102;&#35299;&#19981;&#21516;&#29983;&#29289;&#21644;&#24037;&#31243;&#31995;&#32479;&#20013;&#33258;&#19978;&#32780;&#19979;&#25511;&#21046;&#26041;&#26696;&#30340;&#38388;&#27463;&#24615;&#26412;&#36136;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#20013;&#30340;&#22797;&#29616;&#36880;&#28176;&#37325;&#26500;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#39537;&#21160;&#20449;&#21495;&#12290;&#20511;&#21161;&#20110;&#26012;&#31215;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#23398;&#29702;&#35770;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21709;&#24212;&#26102;&#38388;&#24207;&#21015;&#38388;&#20849;&#20139;&#30340;&#22797;&#29616;&#20107;&#20214;&#65292;&#36825;&#20123;&#20107;&#20214;&#38544;&#21547;&#22320;&#23450;&#20041;&#20102;&#19968;&#20010;&#20855;&#26377;&#29627;&#29827;&#29366;&#32467;&#26500;&#30340;&#22797;&#29616;&#22270;&#12290;&#38543;&#30528;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#37327;&#25110;&#36136;&#37327;&#30340;&#25913;&#21892;&#65292;&#35813;&#22797;&#29616;&#22270;&#32463;&#21382;&#20102;&#19968;&#20010;&#28183;&#27969;&#36716;&#21464;&#65292;&#34920;&#29616;&#20026;&#38543;&#26426;&#34892;&#36208;&#22312;&#35825;&#23548;&#30340;&#26223;&#35266;&#19978;&#20986;&#29616;&#24494;&#24369;&#30340;&#36941;&#21382;&#24615;&#30772;&#35010; - &#26292;&#38706;&#20986;&#20849;&#20139;&#39537;&#21160;&#20449;&#21495;&#20316;&#20026;&#23569;&#25968;&#21442;&#25968;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#29616;&#23454;&#29983;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#30830;&#23450;&#20102;&#21380;&#23572;&#23612;&#35834;&#12289;&#23395;&#33410;&#24615;&#40060;&#31867;&#36801;&#31227;&#21644;&#26524;&#34631;&#31181;&#32676;&#21160;&#24577;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many experimental time series measurements share unobserved causal drivers. Examples include genes targeted by transcription factors, ocean flows influenced by large-scale atmospheric currents, and motor circuits steered by descending neurons. Reliably inferring this unseen driving force is necessary to understand the intermittent nature of top-down control schemes in diverse biological and engineered systems. Here, we introduce a new unsupervised learning algorithm that uses recurrences in time series measurements to gradually reconstruct an unobserved driving signal. Drawing on the mathematical theory of skew-product dynamical systems, we identify recurrence events shared across response time series, which implicitly define a recurrence graph with glass-like structure. As the amount or quality of observed data improves, this recurrence graph undergoes a percolation transition manifesting as weak ergodicity breaking for random walks on the induced landscape -- revealing the shared dri
&lt;/p&gt;</description></item><item><title>&#35813;&#31687;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#36275;&#29699;&#36816;&#21160;&#21592;&#30340;&#20301;&#32622;&#20449;&#24687;&#20197;&#21450;&#20854;&#20182;&#21442;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#36827;&#29699;&#30340;&#27010;&#29575;&#20540;&#65292;&#24182;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#8220;&#29699;&#25481;&#36827;&#20102;&#38169;&#35823;&#30340;&#20154;&#25163;&#37324;&#8221;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13052</link><description>&lt;p&gt;
&#32852;&#21512;&#29699;&#21592;&#19982;&#20301;&#32622;&#20449;&#24687;&#30340;&#36275;&#29699;&#65288;soccer&#65289;&#26399;&#26395;&#36827;&#29699;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach for Player and Position Adjusted Expected Goals in Football (Soccer). (arXiv:2301.13052v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31687;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#36275;&#29699;&#36816;&#21160;&#21592;&#30340;&#20301;&#32622;&#20449;&#24687;&#20197;&#21450;&#20854;&#20182;&#21442;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#36827;&#29699;&#30340;&#27010;&#29575;&#20540;&#65292;&#24182;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#8220;&#29699;&#25481;&#36827;&#20102;&#38169;&#35823;&#30340;&#20154;&#25163;&#37324;&#8221;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36275;&#29699;&#26159;&#19968;&#20010;&#38750;&#24120;&#32467;&#26524;&#39537;&#21160;&#30340;&#34892;&#19994;&#65292;&#36827;&#29699;&#27604;&#22823;&#22810;&#25968;&#20307;&#32946;&#39033;&#30446;&#37117;&#26356;&#20026;&#32597;&#35265;&#65292;&#22240;&#27492;&#65292;&#36827;&#19968;&#27493;&#35780;&#20272;&#22242;&#38431;&#21644;&#20010;&#20154;&#34920;&#29616;&#30340;&#21442;&#25968;&#38750;&#24120;&#20851;&#38190;&#12290;&#26399;&#26395;&#36827;&#29699;&#65288;xG&#65289;&#27604;&#20165;&#26377;&#36827;&#29699;&#27604;&#20998;&#26356;&#33021;&#25552;&#20379;&#35265;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36275;&#29699;&#20013;&#36827;&#19968;&#27493;&#20998;&#26512;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#24182;&#24212;&#29992;&#20110;&#36275;&#29699;&#20107;&#20214;&#25968;&#25454;&#12290;&#20174;&#35813;&#27010;&#24565;&#20013;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#36923;&#36753;&#22238;&#24402;&#21644;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#26041;&#27861;&#36755;&#20986;&#27010;&#29575;&#20272;&#35745;&#20540;&#12290;&#35813;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#22522;&#20110;15,575&#27425;&#23556;&#38376;&#30340;&#36275;&#29699;&#36816;&#21160;&#21592;&#30340;xG&#27010;&#29575;&#20540;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;StatsBomb&#20316;&#20026;&#25968;&#25454;&#25552;&#20379;&#21830;&#65292;&#24182;&#23558;&#34892;&#19994;&#22522;&#20934;&#29992;&#20110;&#35843;&#25972;&#27169;&#22411;&#12290;&#25552;&#20986;&#30340;xG&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#36827;&#19968;&#27493;&#29992;&#20110;&#35299;&#20915;&#8220;&#29699;&#25481;&#36827;&#20102;&#38169;&#35823;&#30340;&#20154;&#25163;&#37324;&#8221;&#30340;&#32769;&#35805;&#39064;&#12290;&#35813;&#27169;&#22411;&#30340;&#24320;&#21457;&#29992;&#20110;&#35843;&#25972;&#24182;&#33719;&#24471;&#26356;&#29616;&#23454;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Football is a very result-driven industry, with goals being rarer than in most sports, so having further parameters to judge the performance of teams and individuals is key. Expected Goals (xG) allow further insight than just a scoreline. To tackle the need for further analysis in football, this paper uses machine learning applications that are developed and applied to Football Event data. From the concept, a Binary Classification problem is created whereby a probabilistic valuation is outputted using Logistic Regression and Gradient Boosting based approaches. The model successfully predicts xGs probability values for football players based on 15,575 shots. The proposed solution utilises StatsBomb as the data provider and an industry benchmark to tune the models in the right direction. The proposed ML solution for xG is further used to tackle the age-old cliche of: 'the ball has fallen to the wrong guy there'. The development of the model is used to adjust and gain more realistic value
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.12321</link><description>&lt;p&gt;
&#31070;&#32463;&#20851;&#31995;&#22270;&#65306;&#35782;&#21035;&#26631;&#31614;&#22122;&#38899;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data. (arXiv:2301.12321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35786;&#26029;&#21644;&#28165;&#29702;&#25968;&#25454;&#26159;&#26500;&#24314;&#20581;&#22766;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23384;&#22312;&#22797;&#26434;&#38382;&#39064;&#65292;&#22914;&#26631;&#31614;&#38169;&#35823;&#12289;&#27424;&#34920;&#31034;&#21644;&#24322;&#24120;&#20540;&#65292;&#22240;&#27492;&#22312;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#36825;&#19968;&#34987;&#24573;&#35270;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#26469;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20851;&#31995;&#22270;&#32467;&#26500;&#26469;&#26816;&#27979;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25552;&#20379;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#28857;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20316;&#20026;&#20132;&#20114;&#24335;&#35786;&#26029;&#25968;&#25454;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26631;&#31614;&#38169;&#35823;&#21644;&#31163;&#32676;&#20540;/&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosing and cleaning data is a crucial step for building robust machine learning systems. However, identifying problems within large-scale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, under-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by utilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational graph structure of data. We further introduce a visualization tool that provides contextual information of a data point in the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and language domain tasks, inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.09010</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#25351;&#25968;&#26631;&#20934;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#25351;&#25968;&#21028;&#25454;&#26469;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#20316;&#32773;&#36827;&#34892;&#20102;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39118;&#38505;&#20013;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#25104;&#21151;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#21644;&#31995;&#32479;&#21442;&#25968;&#25200;&#21160;&#30340;&#24433;&#21709;&#32780;&#19981;&#22815;&#31283;&#20581;&#12290;&#22240;&#27492;,&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#20854;&#31995;&#32479;&#25239;&#24178;&#25200;&#24615;&#65292;&#26679;&#26412;&#25928;&#29575;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#27169;&#22411;&#39118;&#38505;&#25935;&#24863;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#21464;&#20307;&#65292;&#20854;&#23454;&#29616;&#36807;&#31243;&#31867;&#20284;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#25351;&#25968;&#26631;&#20934;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#31574;&#30053;&#39118;&#38505;&#25935;&#24863;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#33945;&#29305;&#21345;&#32599;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21644;&#22312;&#32447;(&#26102;&#38388;&#24046;&#20998;)&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#25968;&#26631;&#20934;&#30340;&#20351;&#29992;&#33021;&#22815;&#25512;&#24191;&#24120;&#29992;&#30340;&#29305;&#23450;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#22312;&#25670;&#21160;&#26438;&#21644;&#25670;&#25670;&#26438;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#23454;&#29616;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
&lt;/p&gt;</description></item><item><title>LidarCLIP&#21487;&#20197;&#23558;&#25991;&#26412;&#21644;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#32852;&#31995;&#36215;&#26469;&#65292;&#36798;&#21040;&#26377;&#25928;&#30340;&#26816;&#32034;&#25928;&#26524;&#65292;&#32780;&#19988;&#33021;&#22815;&#22312;&#19981;&#33391;&#20256;&#24863;&#22120;&#26465;&#20214;&#19979;&#23454;&#29616;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26816;&#27979;&#22330;&#26223;&#30340;&#26377;&#38024;&#23545;&#24615;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2212.06858</link><description>&lt;p&gt;
LidarCLIP&#65306;&#22914;&#20309;&#19982;&#28857;&#20113;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
LidarCLIP or: How I Learned to Talk to Point Clouds. (arXiv:2212.06858v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06858
&lt;/p&gt;
&lt;p&gt;
LidarCLIP&#21487;&#20197;&#23558;&#25991;&#26412;&#21644;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#32852;&#31995;&#36215;&#26469;&#65292;&#36798;&#21040;&#26377;&#25928;&#30340;&#26816;&#32034;&#25928;&#26524;&#65292;&#32780;&#19988;&#33021;&#22815;&#22312;&#19981;&#33391;&#20256;&#24863;&#22120;&#26465;&#20214;&#19979;&#23454;&#29616;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26816;&#27979;&#22330;&#26223;&#30340;&#26377;&#38024;&#23545;&#24615;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#32852;&#31995;&#36215;&#26469;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#20960;&#39033;&#37325;&#22823;&#31361;&#30772;&#65292;&#20363;&#22914; CLIP&#12289;DALL-E 2 &#21644; Stable Diffusion &#31561;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#19982;&#20854;&#20182;&#35270;&#35273;&#27169;&#24577;(&#22914;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;)&#20043;&#38388;&#30340;&#32852;&#31995;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#32570;&#23569;&#25991;&#26412;-&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LidarCLIP&#65292;&#23427;&#23558;&#27773;&#36710;&#28857;&#20113;&#26144;&#23556;&#21040;&#39044;&#20808;&#23384;&#22312;&#30340; CLIP &#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#20351;&#29992;&#22270;&#20687;-&#28857;&#20113;&#23545;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#20687; CLIP &#23884;&#20837;&#30417;&#30563;&#28857;&#20113;&#32534;&#30721;&#22120;&#65292;&#26377;&#25928;&#22320;&#23558;&#25991;&#26412;&#21644;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#19982;&#22270;&#20687;&#22495;&#20316;&#20026;&#20013;&#20171;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; LidarCLIP &#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#28436;&#31034;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#26816;&#32034;&#19968;&#33324;&#19982;&#22522;&#20110;&#22270;&#20687;&#30340;&#26816;&#32034;&#30456;&#24403;&#65292;&#20294;&#20855;&#26377;&#20114;&#34917;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#21644;&#28608;&#20809;&#38647;&#36798;&#29305;&#24449;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21333;&#27169;&#24577;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#22312;&#19981;&#33391;&#20256;&#24863;&#22120;&#26465;&#20214;&#19979;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26816;&#27979;&#22330;&#26223;&#30340;&#26377;&#38024;&#23545;&#24615;&#25628;&#32034;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#24182;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Research connecting text and images has recently seen several breakthroughs, with models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection between text and other visual modalities, such as lidar data, has received less attention, prohibited by the lack of text-lidar datasets. In this work, we propose LidarCLIP, a mapping from automotive point clouds to a pre-existing CLIP embedding space. Using image-lidar pairs, we supervise a point cloud encoder with the image CLIP embeddings, effectively relating text and lidar data with the image domain as an intermediary. We show the effectiveness of LidarCLIP by demonstrating that lidar-based retrieval is generally on par with image-based retrieval, but with complementary strengths and weaknesses. By combining image and lidar features, we improve upon both single-modality methods and enable a targeted search for challenging detection scenarios under adverse sensor conditions. We also explore zero-shot classification and show that
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340; CNN DeepGOPlus &#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25968;&#20540;&#19981;&#30830;&#23450;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#38477;&#20302;&#31934;&#24230;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06361</link><description>&lt;p&gt;
DeepGOPlus &#25512;&#29702;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Numerical Stability of DeepGOPlus Inference. (arXiv:2212.06361v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06361
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340; CNN DeepGOPlus &#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25968;&#20540;&#19981;&#30830;&#23450;&#24615;&#21644;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#38477;&#20302;&#31934;&#24230;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNNs) &#26159;&#30446;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#20043;&#19968;&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#26368;&#21021;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#20294; CNNs &#19982;&#20855;&#26377;&#31354;&#38388;&#20851;&#31995;&#30340;&#20219;&#20309;&#25968;&#25454;&#37117;&#33021;&#24456;&#22909;&#22320;&#37197;&#21512;&#20351;&#29992;&#65292;&#24182;&#24050;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102; CNNs&#65292;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;&#23545;&#22122;&#22768;&#27880;&#20837;&#30340;&#25935;&#24863;&#24615;&#21487;&#33021;&#20250;&#21361;&#21450;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#37327;&#21270;&#20102; DeepGOPlus &#30340;&#28014;&#28857;&#31934;&#24230;&#19981;&#30830;&#23450;&#24615;&#20197;&#30830;&#23450;&#20854;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;DeepGOPlus &#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#34507;&#30333;&#36136;&#21151;&#33021;&#30340; CNN&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38477;&#20302;&#31934;&#24230;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892; DeepGOPlus &#25512;&#29702;&#20197;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#21644;&#24310;&#36831;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807; Monte Carlo Arithmetic &#23454;&#29616;&#30340;&#65292;&#35813;&#25216;&#26415;&#23454;&#39564;&#24615;&#22320;&#37327;&#21270;&#20102;&#28014;&#28857;&#36816;&#31639;&#38169;&#35823;&#21644; VPR&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) are currently among the most widely-used neural networks available and achieve state-of-the-art performance for many problems. While originally applied to computer vision tasks, CNNs work well with any data with a spatial relationship, besides images, and have been applied to different fields. However, recent works have highlighted how CNNs, like other deep learning models, are sensitive to noise injection which can jeopardise their performance. This paper quantifies the numerical uncertainty of the floating point arithmetic inaccuracies of the inference stage of DeepGOPlus, a CNN that predicts protein function, in order to determine its numerical stability. In addition, this paper investigates the possibility to use reduced-precision floating point formats for DeepGOPlus inference to reduce memory consumption and latency. This is achieved with Monte Carlo Arithmetic, a technique that experimentally quantifies floating point operation errors and VPR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;GNN&#22312;&#22788;&#29702;&#38750;&#21516;&#36136;&#22270;&#25968;&#25454;&#26102;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#28151;&#21512;&#23616;&#37096;&#32467;&#26500;&#27169;&#24335;&#30340;&#27010;&#24565;&#65292;&#24182;&#20174;&#23616;&#37096;&#27169;&#24335;&#30340;&#38543;&#26426;&#24615;&#21644;&#36817;&#37051;&#21487;&#32858;&#21512;&#24615;&#20004;&#20010;&#26041;&#38754;&#28145;&#20837;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#26356;&#36890;&#29992;&#30340;GNN&#12290;</title><link>http://arxiv.org/abs/2212.03654</link><description>&lt;p&gt;
&#28151;&#21512;&#23616;&#37096;&#27169;&#24335;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Technical Report of Mixing Local Patterns. (arXiv:2212.03654v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;GNN&#22312;&#22788;&#29702;&#38750;&#21516;&#36136;&#22270;&#25968;&#25454;&#26102;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#28151;&#21512;&#23616;&#37096;&#32467;&#26500;&#27169;&#24335;&#30340;&#27010;&#24565;&#65292;&#24182;&#20174;&#23616;&#37096;&#27169;&#24335;&#30340;&#38543;&#26426;&#24615;&#21644;&#36817;&#37051;&#21487;&#32858;&#21512;&#24615;&#20004;&#20010;&#26041;&#38754;&#28145;&#20837;&#30740;&#31350;&#65292;&#20197;&#23454;&#29616;&#26356;&#36890;&#29992;&#30340;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21516;&#36136;&#22270;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#22788;&#29702;&#38750;&#21516;&#36136;&#22270;&#25968;&#25454;&#26102;&#21364;&#36828;&#36828;&#19981;&#22914;&#21516;&#36136;&#22270;&#25968;&#25454;&#65292;&#36825;&#26159;&#30001;&#20110;GNN&#30340;&#22266;&#26377;&#20302;&#36890;&#28388;&#27874;&#29305;&#24615;&#25152;&#33268;&#12290;&#22312;&#38754;&#23545;&#20998;&#26512;&#20855;&#26377;&#19981;&#21516;&#21516;&#36136;&#24615;&#23646;&#24615;&#30340;&#22797;&#26434;&#29616;&#23454;&#19990;&#30028;&#22270;&#34920;&#26102;&#65292;&#19981;&#24212;&#24573;&#30053;&#22270;&#20013;&#28508;&#22312;&#30340;&#28151;&#21512;&#23616;&#37096;&#32467;&#26500;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#20805;&#20998;&#32771;&#34385;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#65288;\textbf{Q1}&#65289;&#21644;&#65288;\textbf{Q2}&#65289;&#65292;&#20197;&#23454;&#29616;&#26356;&#36890;&#29992;&#30340;GNN&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23581;&#35797;&#20174;&#20004;&#20010;&#26041;&#38754;&#28145;&#20837;&#20102;&#35299;&#23427;&#20204;&#65292;&#20998;&#21035;&#26159;\textbf{&#65288;A1&#65289;&#65306;&#23616;&#37096;&#27169;&#24335;&#30340;&#38543;&#26426;&#24615;}&#21644;\textbf{&#65288;A2&#65289;&#65306;&#36817;&#37051;&#21487;&#32858;&#21512;&#24615;}&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown remarkable performance on homophilic graph data while being far less impressive when handling non-homophilic graph data due to the inherent low-pass filtering property of GNNs. In the face of analyzing complex real-world graphs with different homophily properties, the latent mixed local structural patterns in graphs should not be neglected. Therefore, the two questions, i.e., (\textbf{Q1}) and (\textbf{Q2}) as motioned above, should be well considered on the way to implementing a more generic GNN. For this purpose, we attempt to get deeper insights into them from two points, respectively, \textbf{(A1): Randomness of local patterns}, and \textbf{(A2): Aggregability of near-neighbors}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#35009;&#21098;&#31639;&#27861;DP-PSAC&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#38750;&#21333;&#35843;&#33258;&#36866;&#24212;&#26435;&#37325;&#20989;&#25968;&#65292;&#26681;&#25454;&#26799;&#24230;&#30340;&#21382;&#21490;&#25935;&#24863;&#24615;&#33258;&#36866;&#24212;&#35009;&#21098;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#24133;&#24230;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#19988;&#20445;&#35777;&#20102;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2212.00328</link><description>&lt;p&gt;
&#24102;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#35009;&#21098;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Learning with Per-Sample Adaptive Clipping. (arXiv:2212.00328v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#35009;&#21098;&#31639;&#27861;DP-PSAC&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#38750;&#21333;&#35843;&#33258;&#36866;&#24212;&#26435;&#37325;&#20989;&#25968;&#65292;&#26681;&#25454;&#26799;&#24230;&#30340;&#21382;&#21490;&#25935;&#24863;&#24615;&#33258;&#36866;&#24212;&#35009;&#21098;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#24133;&#24230;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#19988;&#20445;&#35777;&#20102;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;AI&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#19968;&#30452;&#26159;&#21560;&#24341;&#30740;&#31350;&#32773;&#21644;&#20844;&#20247;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;AI&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#20351;AI&#27169;&#22411;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#12290;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#24120;&#37327;&#35009;&#21098;&#26469;&#38480;&#21046;&#26799;&#24230;&#24133;&#24230;&#20197;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#35009;&#21098;&#24120;&#25968;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24040;&#22823;&#24433;&#21709;&#65292;&#26368;&#26032;&#30340;&#20316;&#21697;NSGD&#21644;Auto-S&#25552;&#20986;&#20102;&#20351;&#29992;&#24402;&#19968;&#21270;&#26469;&#26367;&#20195;&#35009;&#21098;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;NSGD&#21644;Auto-S&#31561;&#22522;&#20110;&#24402;&#19968;&#21270;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21333;&#35843;&#26435;&#37325;&#20989;&#25968;&#65292;&#23545;&#23567;&#26799;&#24230;&#26679;&#26412;&#26045;&#21152;&#36807;&#37327;&#26435;&#37325;&#24182;&#24341;&#20837;&#39069;&#22806;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#21333;&#35843;&#33258;&#36866;&#24212;&#26435;&#37325;&#20989;&#25968;&#30340;&#24046;&#20998;&#38544;&#31169;&#27599;&#20010;&#26679;&#26412;&#33258;&#36866;&#24212;&#35009;&#21098;&#65288;DP-PSAC&#65289;&#31639;&#27861;&#65292;&#23427;&#20445;&#35777;&#20102;&#38544;&#31169;&#32780;&#19981;&#29306;&#29298;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;DP-PSAC&#31639;&#27861;&#26681;&#25454;&#35813;&#26679;&#26412;&#30340;&#21382;&#21490;&#25935;&#24863;&#24615;&#33258;&#36866;&#24212;&#35009;&#21098;&#27599;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#24133;&#24230;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35009;&#21098;&#26799;&#24230;&#30340;&#26041;&#24046;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;DP-PSAC&#31639;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy in AI remains a topic that draws attention from researchers and the general public in recent years. As one way to implement privacy-preserving AI, differentially private learning is a framework that enables AI models to use differential privacy (DP). To achieve DP in the learning process, existing algorithms typically limit the magnitude of gradients with a constant clipping, which requires carefully tuned due to its significant impact on model performance. As a solution to this issue, latest works NSGD and Auto-S innovatively propose to use normalization instead of clipping to avoid hyperparameter tuning. However, normalization-based approaches like NSGD and Auto-S rely on a monotonic weight function, which imposes excessive weight on small gradient samples and introduces extra deviation to the update. In this paper, we propose a Differentially Private Per-Sample Adaptive Clipping (DP-PSAC) algorithm based on a non-monotonic adaptive weight function, which guarantees privacy w
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26085;&#30410;&#27969;&#34892;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#35813;&#26041;&#27861;&#24182;&#35752;&#35770;&#20102;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.10873</link><description>&lt;p&gt;
&#29992;&#31526;&#21495;&#22238;&#24402;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#31185;&#23398;&#21457;&#29616;&#65306;&#32508;&#36848;&#65288;arXiv:2211.10873v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Interpretable Scientific Discovery with Symbolic Regression: A Review. (arXiv:2211.10873v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10873
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26085;&#30410;&#27969;&#34892;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#65292;&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#35813;&#26041;&#27861;&#24182;&#35752;&#35770;&#20102;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#20316;&#20026;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26085;&#30410;&#27969;&#34892;&#12290;&#34429;&#28982;&#20256;&#32479;&#19978;&#37319;&#29992;&#36951;&#20256;&#32534;&#31243;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#65292;&#23427;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#65292;&#22312;&#20174;&#22522;&#30784;&#31185;&#23398;&#21040;&#24212;&#29992;&#31185;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#30340;&#32467;&#26500;&#21644;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression is emerging as a promising machine learning method for learning succinct underlying interpretable mathematical expressions directly from data. Whereas it has been traditionally tackled with genetic programming, it has recently gained a growing interest in deep learning as a data-driven model discovery method, achieving significant advances in various application domains ranging from fundamental to applied sciences. This survey presents a structured and comprehensive overview of symbolic regression methods and discusses their strengths and limitations.
&lt;/p&gt;</description></item><item><title>RecD &#26159;&#19968;&#31181;&#20026; DLRM &#35757;&#32451;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs) &#26469;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#65292;&#20351; DLRM &#27169;&#22411;&#26550;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#37325;&#22797;&#24615;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.05239</link><description>&lt;p&gt;
RecD&#65306;&#20026;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#35757;&#32451;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure. (arXiv:2211.05239v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05239
&lt;/p&gt;
&lt;p&gt;
RecD &#26159;&#19968;&#31181;&#20026; DLRM &#35757;&#32451;&#25552;&#20379;&#21435;&#37325;&#21151;&#33021;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs) &#26469;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#65292;&#20351; DLRM &#27169;&#22411;&#26550;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#30340;&#37325;&#22797;&#24615;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; RecD&#65288;&#25512;&#33616;&#21435;&#37325;&#65289;&#65292;&#23427;&#26159;&#19968;&#32452;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411; (DLRM) &#35757;&#32451;&#27969;&#31243;&#30340;&#31471;&#21040;&#31471;&#22522;&#30784;&#35774;&#26045;&#20248;&#21270;&#12290;RecD&#35299;&#20915;&#20102;&#30001;&#20110;&#29305;&#24449;&#37325;&#22797;&#36896;&#25104;&#30340;&#28023;&#37327;&#23384;&#20648;&#12289;&#39044;&#22788;&#29702;&#21644;&#35757;&#32451;&#24320;&#38144;&#65292;&#36825;&#26159;&#22823;&#35268;&#27169; DLRM &#35757;&#32451;&#25968;&#25454;&#38598;&#20869;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026; DLRM &#25968;&#25454;&#38598;&#26159;&#20174;&#20132;&#20114;&#20013;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; RecD &#22914;&#20309;&#21033;&#29992;&#27492;&#23646;&#24615;&#26469;&#20248;&#21270;&#29983;&#20135;&#25968;&#25454;&#30340;&#27969;&#31243;&#65292;&#20943;&#23569;&#25968;&#25454;&#38598;&#23384;&#20648;&#21644;&#39044;&#22788;&#29702;&#38656;&#27714;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#22312;&#35757;&#32451;&#25209;&#27425;&#20013;&#37325;&#22797;&#12290;RecD &#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#26684;&#24335; InverseKeyedJaggedTensors (IKJTs)&#65292;&#20197;&#22312;&#27599;&#20010;&#25209;&#27425;&#20013;&#21435;&#38500;&#29305;&#24449;&#20540;&#30340;&#37325;&#22797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; DLRM &#27169;&#22411;&#26550;&#26500;&#22914;&#20309;&#21033;&#29992; IKJTs &#26469;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. Re
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#26500;&#24314;&#24037;&#20855;&#65288;Webvicob&#65289;&#65292;&#29992;&#20110;&#20174;Wikipedia HTML&#36716;&#20648;&#25991;&#20214;&#20013;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#30340;&#20351;&#29992;&#21487;&#20197;&#25552;&#21319;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.03256</link><description>&lt;p&gt;
&#22522;&#20110;Web&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#26500;&#24314;&#29992;&#20110;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
On Web-based Visual Corpus Construction for Visual Document Understanding. (arXiv:2211.03256v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03256
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Web&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#26500;&#24314;&#24037;&#20855;&#65288;Webvicob&#65289;&#65292;&#29992;&#20110;&#20174;Wikipedia HTML&#36716;&#20648;&#25991;&#20214;&#20013;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25968;&#25454;&#38598;&#30340;&#20351;&#29992;&#21487;&#20197;&#25552;&#21319;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#65288;VDU&#65289;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#21487;&#20844;&#24320;&#33719;&#21462;&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#30340;&#26377;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38750;&#25289;&#19969;&#35821;&#35328;&#25110;&#36164;&#28304;&#30701;&#32570;&#30340;&#35821;&#35328;&#32780;&#35328;&#65292;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#38598;&#21512;&#19982;&#35814;&#32454;&#25991;&#26412;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Web&#30340;&#35270;&#35273;&#35821;&#26009;&#24211;&#26500;&#24314;&#24037;&#20855;&#65288;Webvicob&#65289;&#65292;&#35813;&#24037;&#20855;&#33021;&#22815;&#20174;&#21407;&#22987;&#30340;Wikipedia HTML&#36716;&#20648;&#25991;&#20214;&#20013;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#35270;&#35273;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Webvicob&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#29992;&#20110;&#35757;&#32451;&#31283;&#20581;&#30340;VDU&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;DocVQA&#21644;&#21518;OCR&#35299;&#26512;&#65289;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#30001;Webvicob&#29983;&#25104;&#30340;100&#19975;&#24352;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;DocVQA&#20219;&#21153;3&#19978;&#30456;&#23545;&#20110;COCO-Text&#25968;&#25454;&#38598;&#30340;1100&#19975;&#24352;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#26377;&#36229;&#36807;13&#65285;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, research on visual document understanding (VDU) has grown significantly, with a particular emphasis on the development of self-supervised learning methods. However, one of the significant challenges faced in this field is the limited availability of publicly accessible visual corpora or extensive collections of images with detailed text annotations, particularly for non-Latin or resource-scarce languages. To address this challenge, we propose Web-based Visual Corpus Builder (Webvicob), a dataset generator engine capable of constructing large-scale, multilingual visual corpora from raw Wikipedia HTML dumps. Our experiments demonstrate that the data generated by Webvicob can be used to train robust VDU models that perform well on various downstream tasks, such as DocVQA and post-OCR parsing. Furthermore, when using a dataset of 1 million images generated by Webvicob, we observed an improvement of over 13% on the DocVQA Task 3 compared to a dataset of 11 million images fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38899;&#39057;&#39046;&#22495;&#20013;&#36890;&#36807;&#21513;&#20182;&#25928;&#26524;&#21160;&#24577;&#36716;&#25442;&#24694;&#24847;&#26679;&#26412;&#26469;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#39118;&#26684;&#35302;&#21457;&#22120;&#12290;JingleBack &#30340;&#25552;&#20986;&#20351;&#24471;&#39118;&#26684;&#35302;&#21457;&#22120;&#22312;&#38899;&#39057;&#39046;&#22495;&#24471;&#20197;&#24212;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#26497;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.03117</link><description>&lt;p&gt;
Going In Style&#65306;&#36890;&#36807;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#38899;&#39057;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Going In Style: Audio Backdoors Through Stylistic Transformations. (arXiv:2211.03117v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38899;&#39057;&#39046;&#22495;&#20013;&#36890;&#36807;&#21513;&#20182;&#25928;&#26524;&#21160;&#24577;&#36716;&#25442;&#24694;&#24847;&#26679;&#26412;&#26469;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#39118;&#26684;&#35302;&#21457;&#22120;&#12290;JingleBack &#30340;&#25552;&#20986;&#20351;&#24471;&#39118;&#26684;&#35302;&#21457;&#22120;&#22312;&#38899;&#39057;&#39046;&#22495;&#24471;&#20197;&#24212;&#29992;&#65292;&#24182;&#21462;&#24471;&#20102;&#26497;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38899;&#39057;&#39046;&#22495;&#20013;&#36890;&#36807;&#21513;&#20182;&#25928;&#26524;&#21160;&#24577;&#36716;&#25442;&#24694;&#24847;&#26679;&#26412;&#26469;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#30340;&#39118;&#26684;&#35302;&#21457;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#39118;&#26684;&#35302;&#21457;&#22120;&#8212;&#8212;&#36825;&#26159;&#24403;&#21069;&#25991;&#29486;&#20013;&#32570;&#22833;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#25552;&#20986; JingleBack &#26469;&#22312;&#38899;&#39057;&#39046;&#22495;&#24320;&#21457;&#39118;&#26684;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102; 96% &#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/skoffas/going-in-style &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores stylistic triggers for backdoor attacks in the audio domain: dynamic transformations of malicious samples through guitar effects. We first formalize stylistic triggers - currently missing in the literature. Second, we explore how to develop stylistic triggers in the audio domain by proposing JingleBack. Our experiments confirm the effectiveness of the attack, achieving a 96% attack success rate. Our code is available in https://github.com/skoffas/going-in-style.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#26469;&#26816;&#27979;&#21464;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17312</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#24207;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#26469;&#26816;&#27979;&#21464;&#28857;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#31361;&#21464;&#20998;&#24067;&#36716;&#25442;&#65292;&#21363;&#25152;&#35859;&#30340;&#21464;&#28857;&#26816;&#27979;&#65292;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36880;&#27493;&#35745;&#31639;&#26816;&#27979;&#32479;&#35745;&#37327;&#30340;&#32047;&#31215;&#21644;&#65292;&#24403;&#21457;&#29983;&#21464;&#28857;&#26102;&#65292;&#35813;&#37327;&#20250;&#26174;&#33879;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#21464;&#28857;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21160;&#20316;&#26679;&#26412;&#30340;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#27169;&#22411;&#30340;&#22810;&#21160;&#20316;&#37319;&#26679;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#39640;&#30340;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2210.13011</link><description>&lt;p&gt;
&#35770;&#22810;&#21160;&#20316;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
On Many-Actions Policy Gradient. (arXiv:2210.13011v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21160;&#20316;&#26679;&#26412;&#30340;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#27169;&#22411;&#30340;&#22810;&#21160;&#20316;&#37319;&#26679;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#39640;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21160;&#20316;&#26679;&#26412;&#30340;&#38543;&#26426;&#31574;&#30053;&#26799;&#24230;&#65288;SPG&#65289;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#22810;&#21160;&#20316;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#23427;&#20915;&#23450;&#20102;&#24403;&#19982;&#27604;&#20363;&#25193;&#23637;&#36712;&#36857;&#30340;&#21333;&#21160;&#20316;&#20195;&#29702;&#30456;&#27604;&#65292;&#22810;&#21160;&#20316;SPG&#20135;&#29983;&#27604;&#36739;&#20302;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#21160;&#20316;&#65288;MBMA&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;SPG&#32972;&#26223;&#19979;&#21033;&#29992;&#21160;&#24577;&#27169;&#22411;&#36827;&#34892;&#22810;&#21160;&#20316;&#37319;&#26679;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22810;&#21160;&#20316;SPG&#23454;&#29616;&#25152;&#28041;&#21450;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#22411;&#27169;&#25311;&#30340;&#22238;&#21512;&#20013;&#25552;&#20379;&#19982;SPG&#30456;&#24403;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MBMA&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#32467;&#26500;&#19982;&#29702;&#35770;&#39044;&#27979;&#30340;&#30456;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#21160;&#20316;&#29615;&#22659;&#20013;&#65292;MBMA&#19982;&#26080;&#27169;&#22411;&#65292;&#22810;&#21160;&#20316;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#22522;&#32447;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#39640;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the variance of stochastic policy gradients (SPGs) with many action samples per state. We derive a many-actions optimality condition, which determines when many-actions SPG yields lower variance as compared to a single-action agent with proportionally extended trajectory. We propose Model-Based Many-Actions (MBMA), an approach leveraging dynamics models for many-actions sampling in the context of SPG. MBMA addresses issues associated with existing implementations of many-actions SPG and yields lower bias and comparable variance to SPG estimated from states in model-simulated rollouts. We find that MBMA bias and variance structure matches that predicted by theory. As a result, MBMA achieves improved sample efficiency and higher returns on a range of continuous action environments as compared to model-free, many-actions, and model-based on-policy SPG baselines.
&lt;/p&gt;</description></item><item><title>Transformer&#27169;&#22411;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#20854;&#24490;&#29615;&#21160;&#24577;&#65292;&#21487;&#20197;&#20351;&#29992;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#23569;&#30340;&#23618;&#25968;&#25191;&#34892;&#20219;&#20309;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#35745;&#31639;&#12290;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340; $O(\log T)$ &#28145;&#24230;&#35299;&#20915;&#26041;&#26696;&#22987;&#32456;&#23384;&#22312;&#65292;&#32780;&#19988;$O(1)$&#28145;&#24230;&#27169;&#25311;&#22120;&#26159;&#38750;&#24120;&#26222;&#36941;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.10749</link><description>&lt;p&gt;
Transformers&#23398;&#20250;&#20102;&#33258;&#21160;&#26426;&#30340;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Transformers Learn Shortcuts to Automata. (arXiv:2210.10749v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10749
&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#20854;&#24490;&#29615;&#21160;&#24577;&#65292;&#21487;&#20197;&#20351;&#29992;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#23569;&#30340;&#23618;&#25968;&#25191;&#34892;&#20219;&#20309;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#35745;&#31639;&#12290;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340; $O(\log T)$ &#28145;&#24230;&#35299;&#20915;&#26041;&#26696;&#22987;&#32456;&#23384;&#22312;&#65292;&#32780;&#19988;$O(1)$&#28145;&#24230;&#27169;&#25311;&#22120;&#26159;&#38750;&#24120;&#26222;&#36941;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#25512;&#29702;&#38656;&#35201;&#35745;&#31639;&#27169;&#22411;&#30340;&#24490;&#29615;&#33021;&#21147;&#65292;&#22914;&#22270;&#28789;&#26426;&#31561;&#12290;&#28982;&#32780;&#65292;Transformer&#27169;&#22411;&#34429;&#28982;&#32570;&#20047;&#24490;&#29615;&#33021;&#21147;&#65292;&#20294;&#33021;&#22815;&#20351;&#29992;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#23569;&#30340;&#23618;&#25968;&#25191;&#34892;&#27492;&#31867;&#25512;&#29702;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#27973;&#23618;&#27425;&#21644;&#38750;&#24490;&#29615;&#27169;&#22411;&#23398;&#21040;&#20102;&#20160;&#20040;&#35299;&#20915;&#26041;&#26696;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#20302;&#28145;&#24230;Transformer&#21487;&#20197;&#36890;&#36807;&#36880;&#23618;&#37325;&#26032;&#21442;&#25968;&#21270;&#20854;&#24490;&#29615;&#21160;&#24577;&#65292;&#34920;&#31034;&#20219;&#20309;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;&#22240;&#27492;&#65292;&#20219;&#20309;&#26377;&#30028;&#20869;&#23384;&#31639;&#27861;&#65289;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#24449;&#20102;&#24555;&#25463;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20855;&#26377; $o(T)$ &#23618;&#30340;Transformer&#21487;&#20197;&#31934;&#30830;&#22797;&#21046;&#33258;&#21160;&#26426;&#22312;&#38271;&#24230;&#20026; $T$ &#30340;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340; $O(\log T)$ &#28145;&#24230;&#35299;&#20915;&#26041;&#26696;&#22987;&#32456;&#23384;&#22312;&#65307;&#27492;&#22806;&#65292;$O(1)$ &#28145;&#24230;&#27169;&#25311;&#22120;&#38750;&#24120;&#26222;&#36941;&#65292;&#21487;&#20197;&#20351;&#29992;&#20174; Krohn-Rhodes &#29702;&#35770;&#21644;&#30005;&#36335;&#22797;&#26434;&#24230;&#29702;&#35770;&#20013;&#30340;&#24037;&#20855;&#26469;&#29702;&#35299;&#12290;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are learned by these shallow and non-recurrent models? We find that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics. Our theoretical results characterize shortcut solutions, whereby a Transformer with $o(T)$ layers can exactly replicate the computation of an automaton on an input sequence of length $T$. We find that polynomial-sized $O(\log T)$-depth solutions always exist; furthermore, $O(1)$-depth simulators are surprisingly common, and can be understood using tools from Krohn-Rhodes theory and circuit complexity. Empirical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#25968;&#25454;&#30340;&#26465;&#20214;&#29305;&#24449;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#20351;&#29992;&#26465;&#20214;&#39044;&#27979;&#24433;&#21709;&#21644;&#39034;&#24207;knockoff&#25277;&#26679;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24456;&#23569;&#35752;&#35770;&#30340;&#26465;&#20214;&#21644;&#36793;&#32536;&#24230;&#37327;&#20043;&#38388;&#30340;&#37325;&#35201;&#21306;&#21035;&#65292;&#24182;&#25581;&#31034;&#20986;&#20026;&#27979;&#35797;&#26465;&#20214;FI&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#21487;&#29992;&#19988;&#36807;&#21435;&#20174;&#19994;&#32773;&#30001;&#20110;&#25968;&#25454;&#35201;&#27714;&#19981;&#21305;&#37197;&#32780;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.03047</link><description>&lt;p&gt;
&#28151;&#21512;&#25968;&#25454;&#30340;&#26465;&#20214;&#29305;&#24449;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Conditional Feature Importance for Mixed Data. (arXiv:2210.03047v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#25968;&#25454;&#30340;&#26465;&#20214;&#29305;&#24449;&#37325;&#35201;&#24615;&#26694;&#26550;&#65292;&#20351;&#29992;&#26465;&#20214;&#39044;&#27979;&#24433;&#21709;&#21644;&#39034;&#24207;knockoff&#25277;&#26679;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24456;&#23569;&#35752;&#35770;&#30340;&#26465;&#20214;&#21644;&#36793;&#32536;&#24230;&#37327;&#20043;&#38388;&#30340;&#37325;&#35201;&#21306;&#21035;&#65292;&#24182;&#25581;&#31034;&#20986;&#20026;&#27979;&#35797;&#26465;&#20214;FI&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#21487;&#29992;&#19988;&#36807;&#21435;&#20174;&#19994;&#32773;&#30001;&#20110;&#25968;&#25454;&#35201;&#27714;&#19981;&#21305;&#37197;&#32780;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29305;&#24449;&#37325;&#35201;&#24615;&#65288;FI&#65289;&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#35745;&#20805;&#20998;&#24615;&#24456;&#23569;&#34987;&#35752;&#35770;&#12290;&#20174;&#32479;&#35745;&#35282;&#24230;&#30475;&#65292;&#19968;&#20010;&#20027;&#35201;&#21306;&#21035;&#26159;&#22312;&#35843;&#25972;&#21327;&#21464;&#37327;&#20043;&#21069;&#21644;&#20043;&#21518;&#20998;&#26512;&#21464;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#21363;&#8220;&#36793;&#32536;&#8221;&#21644;&#8220;&#26465;&#20214;&#8221;&#24230;&#37327;&#20043;&#38388;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#36215;&#20102;&#36825;&#31181;&#24456;&#23569;&#34987;&#25215;&#35748;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#21306;&#21035;&#30340;&#27880;&#24847;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27979;&#35797;&#26465;&#20214;FI&#26102;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#21487;&#29992;&#65292;&#32780;&#20174;&#19994;&#32773;&#36807;&#21435;&#30001;&#20110;&#25968;&#25454;&#35201;&#27714;&#19981;&#21305;&#37197;&#32780;&#21463;&#21040;&#20005;&#37325;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#37117;&#34920;&#29616;&#20986;&#22797;&#26434;&#30340;&#29305;&#24449;&#20381;&#36182;&#24615;&#65292;&#24182;&#21253;&#21547;&#36830;&#32493;&#21644;&#20998;&#31867;&#25968;&#25454;&#65288;&#28151;&#21512;&#25968;&#25454;&#65289;&#12290;&#36825;&#20123;&#23646;&#24615;&#36890;&#24120;&#34987;&#26465;&#20214;FI&#24230;&#37327;&#25152;&#24573;&#30053;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#26465;&#20214;&#39044;&#27979;&#24433;&#21709;&#65288;CPI&#65289;&#26694;&#26550;&#19982;&#39034;&#24207;knockoff&#25277;&#26679;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the popularity of feature importance (FI) measures in interpretable machine learning, the statistical adequacy of these methods is rarely discussed. From a statistical perspective, a major distinction is between analyzing a variable's importance before and after adjusting for covariates i.e., between $\textit{marginal}$ and $\textit{conditional}$ measures. Our work draws attention to this rarely acknowledged, yet crucial distinction and showcases its implications. Further, we reveal that for testing conditional FI, only few methods are available and practitioners have hitherto been severely restricted in method application due to mismatching data requirements. Most real-world data exhibits complex feature dependencies and incorporates both continuous and categorical data (mixed data). Both properties are oftentimes neglected by conditional FI measures. To fill this gap, we propose to combine the conditional predictive impact (CPI) framework with sequential knockoff sampling. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ELPH&#30340;&#20840;&#22270;GNN&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#23376;&#22270;&#33609;&#22270;&#20316;&#20026;&#28040;&#24687;&#20256;&#36882;&#65292;&#20197;&#32531;&#35299;LP&#20219;&#21153;&#20013;&#23376;&#22270;&#20043;&#38388;&#30340;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.15486</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#33609;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Link Prediction with Subgraph Sketching. (arXiv:2209.15486v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ELPH&#30340;&#20840;&#22270;GNN&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#23376;&#22270;&#33609;&#22270;&#20316;&#20026;&#28040;&#24687;&#20256;&#36882;&#65292;&#20197;&#32531;&#35299;LP&#20219;&#21153;&#20013;&#23376;&#22270;&#20043;&#38388;&#30340;&#20887;&#20313;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#30001;&#20110;&#34920;&#36798;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#26080;&#27861;&#35745;&#31639;&#19977;&#35282;&#24418;&#65288;&#22823;&#22810;&#25968;LP&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#39592;&#24178;&#65289;&#65292;&#20197;&#21450;&#19981;&#33021;&#21306;&#20998;&#21516;&#26500;&#33410;&#28857;&#65288;&#20855;&#26377;&#30456;&#21516;&#32467;&#26500;&#35282;&#33394;&#30340;&#33410;&#28857;&#65289;&#12290;&#36825;&#20004;&#31181;&#34920;&#36798;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#38142;&#36335;&#65288;&#32780;&#19981;&#26159;&#33410;&#28857;&#65289;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#19977;&#35282;&#24418;&#35745;&#25968;&#31561;&#32467;&#26500;&#29305;&#24449;&#26469;&#32531;&#35299;&#12290;&#30001;&#20110;&#26174;&#24335;&#38142;&#36335;&#34920;&#31034;&#36890;&#24120;&#20195;&#20215;&#39640;&#26114;&#65292;&#22240;&#27492;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;LP&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#23376;&#22270;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#20887;&#20313;&#65292;&#25928;&#29575;&#36739;&#20302;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23376;&#22270;GNN&#65288;SGNN&#65289;&#26041;&#27861;&#30340;&#32452;&#20214;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ELPH&#65288;&#39640;&#25928;&#21704;&#24076;&#38142;&#36335;&#39044;&#27979;&#65289;&#30340;&#26032;&#22411;&#20840;&#22270;GNN&#65292;&#23558;&#23376;&#22270;&#33609;&#22270;&#20316;&#20026;&#28040;&#24687;&#20256;&#36882;&#20197;&#36817;&#20284;&#20840;&#22270;&#19978;&#30340;&#36716;&#25442;&#12290;ELPH&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#29616;&#26377;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#28608;&#27963;&#24418;&#29366;&#26041;&#27861;ASH&#65292;&#29992;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#27493;&#39588;&#12289;&#39069;&#22806;&#30340;&#25968;&#25454;&#25110;&#23545;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#38750;&#24120;&#35268;&#20462;&#25913;&#65292;&#23601;&#33021;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#30693;&#24773;&#20917;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.09858</link><description>&lt;p&gt;
&#26497;&#20854;&#31616;&#21333;&#30340;&#28608;&#27963;&#24418;&#29366;&#27861;&#29992;&#20110;&#35782;&#21035;&#31163;&#32676;&#28857;
&lt;/p&gt;
&lt;p&gt;
Extremely Simple Activation Shaping for Out-of-Distribution Detection. (arXiv:2209.09858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#28608;&#27963;&#24418;&#29366;&#26041;&#27861;ASH&#65292;&#29992;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#27493;&#39588;&#12289;&#39069;&#22806;&#30340;&#25968;&#25454;&#25110;&#23545;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#38750;&#24120;&#35268;&#20462;&#25913;&#65292;&#23601;&#33021;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#30693;&#24773;&#20917;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#20043;&#38388;&#30340;&#21306;&#21035;&#24847;&#21619;&#30528;&#26080;&#27861;&#39044;&#27979;&#37096;&#32626;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#25152;&#26377;&#24773;&#20917;&#65292;&#22240;&#27492;&#20165;&#20381;&#38752;&#35757;&#32451;&#30340;&#36827;&#27493;&#20855;&#26377;&#20854;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#28608;&#27963;&#24418;&#29366;&#26041;&#27861;ASH&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#21518;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#24212;&#29992;&#65292;&#19981;&#38656;&#35201;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#35745;&#31639;&#20219;&#20309;&#32479;&#35745;&#25968;&#25454;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#22788;&#29702;&#21487;&#20197;&#22686;&#24378;&#27491;&#24120;&#25968;&#25454;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The separation between training and deployment of machine learning models implies that not all scenarios encountered in deployment can be anticipated during training, and therefore relying solely on advancements in training has its limits. Out-of-distribution (OOD) detection is an important area that stress-tests a model's ability to handle unseen situations: Do models know when they don't know? Existing OOD detection methods either incur extra training steps, additional data or make nontrivial modifications to the trained network. In contrast, in this work, we propose an extremely simple, post-hoc, on-the-fly activation shaping method, ASH, where a large portion (e.g. 90%) of a sample's activation at a late layer is removed, and the rest (e.g. 10%) simplified or lightly adjusted. The shaping is applied at inference time, and does not require any statistics calculated from training data. Experiments show that such a simple treatment enhances in-distribution and out-of-distribution dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#37322;&#20102;&#8220;&#28145;&#24230;&#23398;&#20064;&#8221;&#20013;&#30340;&#20004;&#31181;&#35823;&#23548;&#24615;&#34892;&#20026;&#8220;&#25968;&#25454;&#21024;&#38500;&#8221;&#21644;&#8220;&#22312;&#35757;&#32451;&#38598;&#19978;&#27979;&#35797;&#8221;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#21363;NNWT&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#19981;&#24403;&#34892;&#20026;&#22312;&#20219;&#20309;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#38646;&#35823;&#24046;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#24182;&#19981;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11228</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#25968;&#25454;&#20026;&#20309;&#35823;&#23548;&#20154;
&lt;/p&gt;
&lt;p&gt;
Why Deep Learning's Performance Data Are Misleading. (arXiv:2208.11228v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#37322;&#20102;&#8220;&#28145;&#24230;&#23398;&#20064;&#8221;&#20013;&#30340;&#20004;&#31181;&#35823;&#23548;&#24615;&#34892;&#20026;&#8220;&#25968;&#25454;&#21024;&#38500;&#8221;&#21644;&#8220;&#22312;&#35757;&#32451;&#38598;&#19978;&#27979;&#35797;&#8221;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#21363;NNWT&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#19981;&#24403;&#34892;&#20026;&#22312;&#20219;&#20309;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#38646;&#35823;&#24046;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#24182;&#19981;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;AIEE 2023&#24180;&#21516;&#21517;&#20250;&#35758;&#30340;&#20027;&#39064;&#28436;&#35762;&#30340;&#38468;&#24102;&#35770;&#25991;&#65292;&#23646;&#20110;&#19968;&#31687;&#29702;&#35770;&#24615;&#25991;&#31456;&#12290;&#25991;&#31456;&#35299;&#37322;&#20102;AI&#39033;&#30446;&#20013;&#25152;&#35859;&#30340;&#8220;&#28145;&#24230;&#23398;&#20064;&#8221;&#32473;&#20986;&#20102;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20294;&#36825;&#20123;&#34920;&#29616;&#25968;&#25454;&#23454;&#38469;&#19978;&#34987;&#38169;&#35823;&#22320;&#22840;&#22823;&#20102;&#12290;&#25991;&#31456;&#38416;&#26126;&#20102;&#8220;&#25968;&#25454;&#21024;&#38500;&#8221;&#21644;&#8220;&#22312;&#35757;&#32451;&#38598;&#19978;&#27979;&#35797;&#8221;&#30340;&#35823;&#23548;&#24615;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#31867;&#26041;&#27861;&#8220;&#26368;&#36817;&#37051;&#21152;&#38408;&#20540;&#8221;&#65288;NNWT&#65289;&#12290;&#25991;&#31456;&#24314;&#31435;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#21363;&#22312;&#20855;&#26377;&#26377;&#38480;&#20294;&#26080;&#38480;&#30340;&#23384;&#20648;&#31354;&#38388;&#21644;&#35757;&#32451;&#26102;&#38388;&#65288;&#19982;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#31867;&#20284;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;NNWT&#26041;&#27861;&#20351;&#29992;&#36825;&#20004;&#31181;&#19981;&#24403;&#34892;&#20026;&#21487;&#20197;&#22312;&#20219;&#20309;&#39564;&#35777;&#38598;&#21644;&#20219;&#20309;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#38646;&#35823;&#24046;&#65292;&#20294;&#26159;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;NNWT&#26041;&#27861;&#22312;&#20869;&#65292;&#24182;&#19981;&#20855;&#26377;&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is a theoretical paper, as a companion paper of the keynote talk at the same conference AIEE 2023. In contrast to conscious learning, many projects in AI have employed so-called "deep learning" many of which seemed to give impressive performance. This paper explains that such performance data are deceptively inflated due to two misconducts: "data deletion" and "test on training set". This paper clarifies "data deletion" and "test on training set" in deep learning and why they are misconducts. A simple classification method is defined, called Nearest Neighbor With Threshold (NNWT). A theorem is established that the NNWT method reaches a zero error on any validation set and any test set using the two misconducts, as long as the test set is in the possession of the author and both the amount of storage space and the time of training are finite but unbounded like with many deep learning methods. However, many deep learning methods, like the NNWT method, are all not generalizable since
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#32447;&#24615;&#26102;&#19981;&#21464;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;&#22522;&#20110;&#23376;&#31354;&#38388;&#30340;&#26080;&#30417;&#30563;&#38477;&#38454;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27169;&#22411;&#22312;&#20849;&#20139;&#23376;&#31354;&#38388;&#20013;&#30340;&#34920;&#31034;&#24046;&#24322;&#65292;&#23558;&#22312;&#19968;&#20010;&#39046;&#22495;&#19978;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#65292;&#29992;&#20110;&#24314;&#31569;&#33021;&#37327;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.09456</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#29992;&#20110;&#24314;&#31569;&#33021;&#37327;&#31995;&#32479;&#24314;&#27169;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A physics-based domain adaptation framework for modelling and forecasting building energy systems. (arXiv:2208.09456v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#32447;&#24615;&#26102;&#19981;&#21464;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;&#22522;&#20110;&#23376;&#31354;&#38388;&#30340;&#26080;&#30417;&#30563;&#38477;&#38454;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27169;&#22411;&#22312;&#20849;&#20139;&#23376;&#31354;&#38388;&#20013;&#30340;&#34920;&#31034;&#24046;&#24322;&#65292;&#23558;&#22312;&#19968;&#20010;&#39046;&#22495;&#19978;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#65292;&#29992;&#20110;&#24314;&#31569;&#33021;&#37327;&#31995;&#32479;&#30340;&#24314;&#27169;&#21644;&#39044;&#27979;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#24314;&#31569;&#33021;&#28304;&#34892;&#20026;&#24314;&#27169;&#21644;&#39044;&#27979;&#26041;&#38754;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#27969;&#34892;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32467;&#26500;&#36890;&#24120;&#24182;&#19981;&#20855;&#26377;&#19982;&#29289;&#29702;&#29616;&#35937;&#30456;&#20851;&#30340;&#26426;&#26800;&#32467;&#26500;&#30456;&#23545;&#24212;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#25104;&#21151;&#22320;&#27867;&#21270;&#20026;&#26410;&#35266;&#27979;&#21040;&#30340;&#26102;&#38388;&#27493;&#21462;&#20915;&#20110;&#25152;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#21160;&#24577;&#22312;&#25968;&#25454;&#20013;&#34920;&#29616;&#30340;&#20195;&#34920;&#24615;&#65292;&#22312;&#25968;&#23383;&#23402;&#29983;&#25511;&#21046;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#30495;&#23454;&#19990;&#30028;&#30340;&#24037;&#31243;&#38382;&#39064;&#20013;&#24456;&#38590;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#32447;&#24615;&#26102;&#19981;&#21464;&#65288;LTI&#65289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#25104;&#25209;&#21442;&#25968;&#27169;&#22411;&#19982;&#22522;&#20110;&#23376;&#31354;&#38388;&#30340;&#26080;&#30417;&#30563;&#38477;&#38454;&#24314;&#27169;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#23376;&#31354;&#38388;&#23548;&#21521;&#30340;&#39046;&#22495;&#36866;&#24212;&#65288;SDA&#65289;&#26694;&#26550;&#12290;SDA&#26159;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#20849;&#20139;&#23376;&#31354;&#38388;&#20013;&#30340;&#27169;&#22411;&#34920;&#31034;&#30340;&#24046;&#24322;&#26469;&#23558;&#22312;&#19968;&#20010;&#39046;&#22495;&#19978;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#12290;&#22312;&#19968;&#20010;&#24314;&#31569;&#33021;&#37327;&#31995;&#32479;&#26696;&#20363;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#33021;&#37327;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art machine-learning-based models are a popular choice for modeling and forecasting energy behavior in buildings because given enough data, they are good at finding spatiotemporal patterns and structures even in scenarios where the complexity prohibits analytical descriptions. However, their architecture typically does not hold physical correspondence to mechanistic structures linked with governing physical phenomena. As a result, their ability to successfully generalize for unobserved timesteps depends on the representativeness of the dynamics underlying the observed system in the data, which is difficult to guarantee in real-world engineering problems such as control and energy management in digital twins. In response, we present a framework that combines lumped-parameter models in the form of linear time-invariant (LTI) state-space models (SSMs) with unsupervised reduced-order modeling in a subspace-based domain adaptation (SDA) framework. SDA is a type of transfer-lear
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#22522;&#30784;&#23398;&#20064;&#22120;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#38469;&#25928;&#30410;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24212;&#29992;&#33021;&#21147;&#20248;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#31616;&#21333;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.01148</link><description>&lt;p&gt;
&#22522;&#20110;Boosting&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Boosted Off-Policy Learning. (arXiv:2208.01148v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.01148
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#22522;&#30784;&#23398;&#20064;&#22120;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#33719;&#24471;&#20102;&#24191;&#27867;&#30340;&#23454;&#38469;&#25928;&#30410;&#65307;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24212;&#29992;&#33021;&#21147;&#20248;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#31616;&#21333;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26469;&#33258;&#35760;&#24405;&#24335;&#36172;&#21338;&#21453;&#39304;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;Boosting&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#30417;&#30563;&#23398;&#20064;&#30340;Boosting&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30452;&#25509;&#20248;&#21270;&#20102;&#31574;&#30053;&#39044;&#26399;&#25910;&#30410;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#22914;&#26524;&#22522;&#30784;&#23398;&#20064;&#22120;&#28385;&#36275;&#8220;&#24369;&#8221;&#23398;&#20064;&#26465;&#20214;&#65292;&#37027;&#20040;&#27599;&#19968;&#36718;Boosting&#37117;&#20250;&#20943;&#23567;&#36807;&#37327;&#32463;&#39564;&#39118;&#38505;&#65288;&#21487;&#33021;&#26159;&#25351;&#25968;&#32423;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22522;&#30784;&#23398;&#20064;&#22120;&#31616;&#21270;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#25171;&#24320;&#20102;&#24191;&#27867;&#30340;&#22522;&#30784;&#23398;&#20064;&#22120;&#28304;&#65292;&#22914;&#20915;&#31574;&#26641;&#31561;&#65292;&#20855;&#26377;&#23454;&#38469;&#30410;&#22788;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#32487;&#25215;&#20102;&#35768;&#22810;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;Boosting&#31639;&#27861;&#30340;&#20248;&#33391;&#24615;&#36136;&#65288;&#20363;&#22914;&#23545;&#29305;&#24449;&#32553;&#25918;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#40065;&#26834;&#24615;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#32988;&#36807;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#21644;&#21482;&#26159;&#22238;&#24402;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first boosting algorithm for off-policy learning from logged bandit feedback. Unlike existing boosting methods for supervised learning, our algorithm directly optimizes an estimate of the policy's expected reward. We analyze this algorithm and prove that the excess empirical risk decreases (possibly exponentially fast) with each round of boosting, provided a ''weak'' learning condition is satisfied by the base learner. We further show how to reduce the base learner to supervised learning, which opens up a broad range of readily available base learners with practical benefits, such as decision trees. Experiments indicate that our algorithm inherits many desirable properties of tree-based boosting algorithms (e.g., robustness to feature scaling and hyperparameter tuning), and that it can outperform off-policy learning with deep neural networks as well as methods that simply regress on the observed rewards.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#30001;&#20248;&#21270;&#22120;&#39537;&#21160;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#26367;&#20195;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#30446;&#21069;&#20351;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#20026;&#22797;&#26434;&#31995;&#32479;&#27169;&#25311;&#25552;&#20379;&#20102;&#26356;&#21152;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.12855</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#31995;&#32479;&#27169;&#25311;&#30340;&#26377;&#25928;&#23398;&#20064;&#20934;&#30830;&#26367;&#20195;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Accurate Surrogates for Simulations of Complex Systems. (arXiv:2207.12855v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#30001;&#20248;&#21270;&#22120;&#39537;&#21160;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#26367;&#20195;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#30446;&#21069;&#20351;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#20026;&#22797;&#26434;&#31995;&#32479;&#27169;&#25311;&#25552;&#20379;&#20102;&#26356;&#21152;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#26500;&#24314;&#35745;&#31639;&#19978;&#24265;&#20215;&#30340;&#22797;&#26434;&#29289;&#29702;&#27169;&#22411;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#24403;&#25968;&#25454;&#19981;&#31283;&#23450;&#12289;&#31232;&#30095;&#25110;&#26102;&#38388;&#30456;&#20851;&#26102;&#65292;&#36825;&#20123;&#26367;&#20195;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#25214;&#21040;&#19968;&#31181;&#21487;&#20197;&#25552;&#20379;&#20219;&#20309;&#28508;&#22312;&#26410;&#26469;&#27169;&#22411;&#35780;&#20272;&#30340;&#26377;&#25928;&#39044;&#27979;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30001;&#20248;&#21270;&#22120;&#39537;&#21160;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26377;&#20004;&#20010;&#20248;&#28857;&#65292;&#39318;&#20808;&#23427;&#30830;&#20445;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#27169;&#22411;&#21709;&#24212;&#26354;&#38754;&#19978;&#30340;&#25152;&#26377;&#25296;&#28857;&#12290;&#20854;&#27425;&#65292;&#22312;&#20219;&#20309;&#26032;&#30340;&#27169;&#22411;&#35780;&#20272;&#20043;&#21518;&#65292;&#22914;&#26524;&#8220;&#24471;&#20998;&#8221;&#19979;&#38477;&#21040;&#19979;&#38480;&#65292;&#21017;&#20250;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#37325;&#26032;&#35757;&#32451;&#8221;&#65288;&#26356;&#26032;&#65289;&#12290;&#22312;&#22522;&#20934;&#20989;&#25968;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;&#30001;&#20248;&#21270;&#22120;&#24341;&#23548;&#30340;&#37319;&#26679;&#36890;&#24120;&#22312;&#23616;&#37096;&#26497;&#20540;&#38468;&#36817;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#36234;&#65292;&#21363;&#20351;&#35780;&#20998;&#25351;&#26631;&#25903;&#25345;&#25972;&#20307;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#26680;&#29289;&#36136;&#30340;&#27169;&#25311;&#20013;&#65292;&#20197;&#35777;&#26126;&#23427;&#22788;&#29702;&#22024;&#26434;&#12289;&#31232;&#30095;&#21644;&#26102;&#38388;&#30456;&#20851;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#31435;&#30340;&#26367;&#20195;&#27169;&#22411;&#27604;&#21516;&#26679;&#25968;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#30340;&#30446;&#21069;&#20351;&#29992;&#30340;&#26367;&#20195;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#32780;&#20256;&#32479;&#30340;&#26367;&#20195;&#27169;&#22411;&#21017;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods are increasingly used to build computationally inexpensive surrogates for complex physical models. The predictive capability of these surrogates suffers when data are noisy, sparse, or time-dependent. As we are interested in finding a surrogate that provides valid predictions of any potential future model evaluations, we introduce an online learning method empowered by optimizer-driven sampling. The method has two advantages over current approaches. First, it ensures that all turning points on the model response surface are included in the training data. Second, after any new model evaluations, surrogates are tested and "retrained" (updated) if the "score" drops below a validity threshold. Tests on benchmark functions reveal that optimizer-directed sampling generally outperforms traditional sampling methods in terms of accuracy around local extrema, even when the scoring metric favors overall accuracy. We apply our method to simulations of nuclear matter to dem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;$L^2$&#27491;&#21017;&#21270;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;Stein&#35780;&#20215;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38454;&#27573;&#24615;&#30340;&#26435;&#37325;&#35843;&#25972;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#32479;&#35745;&#27979;&#35797;&#20013;&#23454;&#29616;&#23545;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#19982;&#21517;&#20041;&#27169;&#22411;&#20998;&#24067;&#30340;&#21306;&#20998;&#12290;</title><link>http://arxiv.org/abs/2207.03406</link><description>&lt;p&gt;
&#37319;&#29992;&#38454;&#27573;$L^2$&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;Stein&#35780;&#20215;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural Stein critics with staged $L^2$-regularization. (arXiv:2207.03406v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;$L^2$&#27491;&#21017;&#21270;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;Stein&#35780;&#20215;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38454;&#27573;&#24615;&#30340;&#26435;&#37325;&#35843;&#25972;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#32479;&#35745;&#27979;&#35797;&#20013;&#23454;&#29616;&#23545;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#19982;&#21517;&#20041;&#27169;&#22411;&#20998;&#24067;&#30340;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21306;&#20998;&#27169;&#22411;&#20998;&#24067;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#20998;&#24067;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#39640;&#32500;&#25968;&#25454;&#23545;&#36825;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#12290;&#37327;&#21270;&#27010;&#29575;&#20998;&#24067;&#24046;&#24322;&#30340;&#24230;&#37327;&#65292;&#20363;&#22914;Stein&#24046;&#24322;&#65292;&#22312;&#39640;&#32500;&#32479;&#35745;&#27979;&#35797;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;Stein&#35780;&#20215;&#22120;&#26102;&#37319;&#29992;$L^2$&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;&#65292;&#20197;&#21306;&#20998;&#20174;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#21644;&#21517;&#20041;&#27169;&#22411;&#20998;&#24067;&#12290;&#36890;&#36807;&#19982;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#29702;&#35770;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26435;&#37325;&#27491;&#21017;&#21270;&#30340;&#38454;&#27573;&#36807;&#31243;&#65292;&#21033;&#29992;&#20102;&#22312;&#26089;&#26399;&#26102;&#26399;&#39640;&#24230;&#27491;&#21017;&#21270;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#24403;$L^2$&#27491;&#21017;&#21270;&#26435;&#37325;&#36739;&#22823;&#26102;&#65292;&#35757;&#32451;&#21160;&#24577;&#30340;&#36817;&#20284;&#24615;&#36136;&#36890;&#36807;&#26680;&#20248;&#21270;&#21363;&#23454;&#29616;&#20102;&#25042;&#24816;(&#8220;lazy training&#8221;)&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to differentiate model distributions from observed data is a fundamental problem in statistics and machine learning, and high-dimensional data remains a challenging setting for such problems. Metrics that quantify the disparity in probability distributions, such as the Stein discrepancy, play an important role in high-dimensional statistical testing. In this paper, we investigate the role of $L^2$ regularization in training a neural network Stein critic so as to distinguish between data sampled from an unknown probability distribution and a nominal model distribution. Making a connection to the Neural Tangent Kernel (NTK) theory, we develop a novel staging procedure for the weight of regularization over training time, which leverages the advantages of highly-regularized training at early times. Theoretically, we prove the approximation of the training dynamic by the kernel optimization, namely the ``lazy training'', when the $L^2$ regularization weight is large, and training o
&lt;/p&gt;</description></item><item><title>LogGENE&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26694;&#26550;&#39044;&#27979;&#22522;&#22240;&#34920;&#36798;&#27700;&#24179;&#30340;&#23436;&#25972;&#26465;&#20214;&#20998;&#20301;&#25968;&#65292;&#20174;&#32780;&#20026;&#39640;&#36890;&#37327;&#22522;&#22240;&#32452;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#25552;&#20379;&#35299;&#37322;&#21644;&#25253;&#21578;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#24615;&#24378;&#30340;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.09333</link><description>&lt;p&gt;
LogGENE: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#21307;&#30103;&#25512;&#29702;&#20219;&#21153;&#30340;&#24179;&#28369;&#26816;&#26597;&#25439;&#22833;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks. (arXiv:2206.09333v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09333
&lt;/p&gt;
&lt;p&gt;
LogGENE&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26694;&#26550;&#39044;&#27979;&#22522;&#22240;&#34920;&#36798;&#27700;&#24179;&#30340;&#23436;&#25972;&#26465;&#20214;&#20998;&#20301;&#25968;&#65292;&#20174;&#32780;&#20026;&#39640;&#36890;&#37327;&#22522;&#22240;&#32452;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#25552;&#20379;&#35299;&#37322;&#21644;&#25253;&#21578;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12289;&#40065;&#26834;&#24615;&#24378;&#30340;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#25366;&#25496;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#20174;&#20013;&#33719;&#24471;&#26657;&#20934;&#30340;&#39044;&#27979;&#20855;&#26377;&#21363;&#26102;&#30456;&#20851;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#34920;&#36798;&#31561;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#19982;&#20856;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#25512;&#26029;&#25216;&#26415;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#25552;&#20379;&#35299;&#37322;&#21644;&#25253;&#21578;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26694;&#26550;&#26469;&#39044;&#27979;&#32473;&#23450;&#19968;&#32452;&#22522;&#22240;&#34920;&#36798;&#30340;&#23436;&#25972;&#26465;&#20214;&#20998;&#20301;&#25968;&#12290;&#26465;&#20214;&#20998;&#20301;&#25968;&#38500;&#20102;&#26377;&#21161;&#20110;&#25552;&#20379;&#39044;&#27979;&#30340;&#20016;&#23500;&#35299;&#37322;&#22806;&#65292;&#36824;&#33021;&#22815;&#25269;&#25239;&#27979;&#37327;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#39640;&#36890;&#37327;&#22522;&#22240;&#32452;&#23398;&#20013;&#29305;&#21035;&#37325;&#35201;&#65292;&#36825;&#26159;&#19968;&#20010;&#27491;&#22312;&#24341;&#39046;&#20010;&#24615;&#21270;&#21307;&#30103;&#12289;&#38774;&#21521;&#33647;&#29289;&#35774;&#35745;&#21644;&#20256;&#36882;&#30340;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#39537;&#21160;&#20272;&#35745;&#36807;&#31243;&#30340;&#26816;&#26597;&#25439;&#22833;&#65292;&#22312;&#20998;&#20301;&#25968;&#22238;&#24402;&#20013;&#24182;&#26080;&#19981;&#21516;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining large datasets and obtaining calibrated predictions from tem is of immediate relevance and utility in reliable deep learning. In our work, we develop methods for Deep neural networks based inferences in such datasets like the Gene Expression. However, unlike typical Deep learning methods, our inferential technique, while achieving state-of-the-art performance in terms of accuracy, can also provide explanations, and report uncertainty estimates. We adopt the Quantile Regression framework to predict full conditional quantiles for a given set of housekeeping gene expressions. Conditional quantiles, in addition to being useful in providing rich interpretations of the predictions, are also robust to measurement noise. Our technique is particularly consequential in High-throughput Genomics, an area which is ushering a new era in personalized health care, and targeted drug design and delivery. However, check loss, used in quantile regression to drive the estimation process is not diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#22270;&#20687;&#21442;&#29031;&#28216;&#25103;&#33258;&#36866;&#24212;&#19981;&#21516;&#21548;&#20247;&#30340;&#30446;&#26631;&#20219;&#21153;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843; CLIP &#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#36866;&#24212;&#21548;&#20247;&#30340;&#35821;&#35328;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#33258;&#28982;&#35821;&#35328;&#19987;&#19994;&#21270;&#12290;</title><link>http://arxiv.org/abs/2206.08349</link><description>&lt;p&gt;
&#20102;&#35299;&#20320;&#30340;&#21548;&#20247;&#65306;&#29992;&#21548;&#20247;&#20943;&#27861;&#19987;&#38376;&#21270;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Know your audience: specializing grounded language models with listener subtraction. (arXiv:2206.08349v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#22270;&#20687;&#21442;&#29031;&#28216;&#25103;&#33258;&#36866;&#24212;&#19981;&#21516;&#21548;&#20247;&#30340;&#30446;&#26631;&#20219;&#21153;&#25551;&#36848;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843; CLIP &#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#36866;&#24212;&#21548;&#20247;&#30340;&#35821;&#35328;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#33258;&#28982;&#35821;&#35328;&#19987;&#19994;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#27807;&#36890;&#38656;&#35201;&#36866;&#24212;&#27599;&#20010;&#20132;&#38469;&#24773;&#22659;&#30340;&#29305;&#27530;&#24615;&#65292;&#27604;&#22914;&#19982;&#27599;&#20010;&#20132;&#20114;&#20249;&#20276;&#20998;&#20139;&#30340;&#20849;&#21516;&#35821;&#22659;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20511;&#37492;&#23545;&#35805;&#28216;&#25103; Dixit &#30340;&#24605;&#24819;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#22270;&#20687;&#21442;&#29031;&#28216;&#25103;&#65292;&#35757;&#32451;&#19968;&#20010;&#35828;&#35805;&#32773;&#27169;&#22411;&#26469;&#25551;&#36848;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#20351;&#24471;&#19968;&#20010;&#21548;&#32773;&#33021;&#22815;&#22312;&#24178;&#25200;&#39033;&#20013;&#27491;&#30830;&#22320;&#35782;&#21035;&#20986;&#30446;&#26631;&#22270;&#20687;&#65292;&#32780;&#21478;&#19968;&#20010;&#21548;&#32773;&#21017;&#19981;&#33021;&#12290;&#36825;&#35201;&#27714;&#35828;&#35805;&#32773;&#21033;&#29992;&#23427;&#19982;&#19981;&#21516;&#21548;&#32773;&#30340;&#20849;&#21516;&#30693;&#35782;&#24046;&#24322;&#36827;&#34892;&#36866;&#24212;&#12290;&#26412;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#23545;&#27604;&#12289;&#22810;&#26234;&#33021;&#20307;&#30340;&#35821;&#22659;&#19979;&#24494;&#35843; CLIP &#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#36866;&#37197;&#22120;&#20250;&#33258;&#28982;&#22320;&#20135;&#29983;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;&#33258;&#28982;&#35821;&#35328;&#19987;&#19994;&#21270;&#65292;&#19988;&#21482;&#38656;&#35201;&#36890;&#36807;&#22870;&#21169;&#32780;&#26080;&#38656;&#30452;&#25509;&#30417;&#30563;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#29992;&#20004;&#20010;&#21548;&#32773;&#26469;&#35757;&#32451;&#35828;&#35805;&#32773;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective communication requires adapting to the idiosyncrasies of each communicative context--such as the common ground shared with each partner. Humans demonstrate this ability to specialize to their audience in many contexts, such as the popular game Dixit. We take inspiration from Dixit to formulate a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it among distractors, but another listener cannot. To adapt, the speaker must exploit differences in the knowledge it shares with the different listeners. We show that finetuning an attention-based adapter between a CLIP vision encoder and a large language model in this contrastive, multi-agent setting gives rise to context-dependent natural language specialization from rewards only, without direct supervision. Through controlled experiments, we show that training a speaker with two listeners that perceive different
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20302;&#20445;&#30495;&#27169;&#22411;&#21644;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#23398;&#20064;&#65292; &#24182;&#24320;&#21457;&#20102;&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#26694;&#26550;&#20801;&#35768;&#22810;&#20010;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2206.08201</link><description>&lt;p&gt;
&#36890;&#36807;&#20302;&#20445;&#30495;&#27169;&#22411;&#21644;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#39640;&#26031;&#36807;&#31243;&#23398;&#20064;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning Physics between Digital Twins with Low-Fidelity Models and Physics-Informed Gaussian Processes. (arXiv:2206.08201v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20302;&#20445;&#30495;&#27169;&#22411;&#21644;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#23398;&#20064;&#65292; &#24182;&#24320;&#21457;&#20102;&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#26694;&#26550;&#20801;&#35768;&#22810;&#20010;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#31181;&#20195;&#34920;&#20010;&#20307;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#65292;&#20363;&#22914;&#32452;&#20214;&#12289;&#24739;&#32773;&#25110;&#36807;&#31243;&#12290; &#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24819;&#20174;&#25968;&#25454;&#20013;&#33719;&#21462;&#26377;&#20851;&#20010;&#20307;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#32467;&#21512;&#19981;&#23436;&#32654;&#30340;&#29289;&#29702;&#30693;&#35782;&#65292;&#24182;&#20174;&#20854;&#20182;&#20010;&#20307;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#20010;&#20307;&#30340;&#29289;&#29702;&#21442;&#25968;&#24341;&#36215;&#20852;&#36259;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290; &#22312;&#20010;&#24615;&#21270;&#27169;&#22411;&#30340;&#27169;&#22411;&#20844;&#24335;&#20013;&#24341;&#20837;&#20102;&#27169;&#22411;&#24046;&#24322;&#39033;&#65292;&#20197;&#35299;&#37322;&#20302;&#20445;&#30495;&#27169;&#22411;&#20013;&#32570;&#22833;&#30340;&#29289;&#29702;&#29616;&#35937;&#12290; &#20026;&#20102;&#20801;&#35768;&#20010;&#20307;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#20998;&#23618;&#24314;&#27169;&#26694;&#26550;&#65292;&#20854;&#20013;&#36890;&#36807;&#26032;&#23618;&#23558;&#20010;&#20307;&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#19968;&#20010;&#26159;&#20808;&#21069;&#22312;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#29609;&#20855;&#31034;&#20363;&#65292;&#25193;&#23637;&#21040;&#26356;&#22810;&#20010;&#20307;&#30340;&#24773;&#20917;&#65292;&#19968;&#20010;&#26159;&#19982;&#27835;&#30103;&#39640;&#34880;&#21387;&#30456;&#20851;&#30340;&#24515;&#34880;&#31649;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A digital twin is a computer model that represents an individual, for example, a component, a patient or a process. In many situations, we want to gain knowledge about an individual from its data while incorporating imperfect physical knowledge and also learn from data from other individuals. In this paper, we introduce a fully Bayesian methodology for learning between digital twins in a setting where the physical parameters of each individual are of interest. A model discrepancy term is incorporated in the model formulation of each personalized model to account for the missing physics of the low-fidelity model. To allow sharing of information between individuals, we introduce a Bayesian Hierarchical modelling framework where the individual models are connected through a new level in the hierarchy. Our methodology is demonstrated in two case studies, a toy example previously used in the literature extended to more individuals and a cardiovascular model relevant for the treatment of Hyp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25506;&#32034;&#30340;Stein&#21464;&#20998;&#30446;&#26631;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#31890;&#23376;&#24314;&#27169;&#24182;&#24341;&#20837;&#36866;&#24403;&#38590;&#24230;&#21306;&#22495;&#65292;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30446;&#26631;&#36798;&#25104;&#30340;&#25104;&#21151;&#35206;&#30422;&#29575;&#65292;&#21516;&#26102;&#22312;&#29615;&#22659;&#21464;&#21270;&#26102;&#34920;&#29616;&#20986;&#26377;&#29992;&#30340;&#24674;&#22797;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.06719</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25506;&#32034;&#30340;Stein&#21464;&#20998;&#30446;&#26631;&#29983;&#25104;&#22312;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Goal Generation for adaptive Exploration in Multi-Goal Reinforcement Learning. (arXiv:2206.06719v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25506;&#32034;&#30340;Stein&#21464;&#20998;&#30446;&#26631;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#31890;&#23376;&#24314;&#27169;&#24182;&#24341;&#20837;&#36866;&#24403;&#38590;&#24230;&#21306;&#22495;&#65292;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#30446;&#26631;&#36798;&#25104;&#30340;&#25104;&#21151;&#35206;&#30422;&#29575;&#65292;&#21516;&#26102;&#22312;&#29615;&#22659;&#21464;&#21270;&#26102;&#34920;&#29616;&#20986;&#26377;&#29992;&#30340;&#24674;&#22797;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#30456;&#20851;&#30340;&#35757;&#32451;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#32463;&#39564;&#65292;&#20174;&#32780;&#22312;&#27979;&#35797;&#26102;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#20294;&#26159;&#65292;&#22312;&#30446;&#26631;&#31354;&#38388;&#20855;&#26377;&#19981;&#36830;&#32493;&#24615;&#19988;&#22870;&#21169;&#31232;&#30095;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22810;&#25968;&#30446;&#26631;&#38590;&#20197;&#36798;&#25104;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#30340;&#35838;&#31243;&#26377;&#21161;&#20110;&#26234;&#33021;&#20307;&#36890;&#36807;&#36866;&#24212;&#20854;&#24403;&#21069;&#33021;&#21147;&#26469;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stein Variational Goal Generation (SVGG)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#30446;&#26631;&#21040;&#36798;&#33021;&#21147;&#30340;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#65292;&#20026;&#26234;&#33021;&#20307;&#37319;&#26679;&#20013;&#31561;&#38590;&#24230;&#30340;&#30446;&#26631;&#12290;&#30446;&#26631;&#20998;&#24067;&#37319;&#29992;&#31890;&#23376;&#26469;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#23558;&#31890;&#23376;&#21560;&#24341;&#21040;&#36866;&#24403;&#38590;&#24230;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;SVGG&#22312;&#38590;&#24230;&#36739;&#39640;&#30340;&#25506;&#32034;&#38382;&#39064;&#20013;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#29615;&#22659;&#21457;&#29983;&#21464;&#21270;&#26102;&#20855;&#26377;&#26377;&#29992;&#30340;&#24674;&#22797;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-goal Reinforcement Learning, an agent can share experience between related training tasks, resulting in better generalization for new tasks at test time. However, when the goal space has discontinuities and the reward is sparse, a majority of goals are difficult to reach. In this context, a curriculum over goals helps agents learn by adapting training tasks to their current capabilities. In this work we propose Stein Variational Goal Generation (SVGG), which samples goals of intermediate difficulty for the agent, by leveraging a learned predictive model of its goal reaching capabilities. The distribution of goals is modeled with particles that are attracted in areas of appropriate difficulty using Stein Variational Gradient Descent. We show that SVGG outperforms state-of-the-art multi-goal Reinforcement Learning methods in terms of success coverage in hard exploration problems, and demonstrate that it is endowed with a useful recovery property when the environment changes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#31616;&#21333;&#31163;&#25955;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;Value Memory Graph&#65292;VMG&#65289;&#26469;&#25277;&#35937;&#21407;&#22987;&#22797;&#26434;&#29615;&#22659;&#65292;&#20174;&#32780;&#31616;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2206.04384</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#35760;&#24518;&#22270;
&lt;/p&gt;
&lt;p&gt;
Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning. (arXiv:2206.04384v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#31616;&#21333;&#31163;&#25955;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;Value Memory Graph&#65292;VMG&#65289;&#26469;&#25277;&#35937;&#21407;&#22987;&#22797;&#26434;&#29615;&#22659;&#65292;&#20174;&#32780;&#31616;&#21270;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#36890;&#24120;&#30452;&#25509;&#24212;&#29992;&#20110;&#29615;&#22659;&#26469;&#23398;&#20064;&#31574;&#30053;&#12290;&#22312;&#19968;&#20123;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#12289;&#31232;&#30095;&#22870;&#21169;&#21644;/&#25110;&#38271;&#26102;&#38388;&#38388;&#38548;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#22312;&#21407;&#22987;&#29615;&#22659;&#20013;&#23398;&#20064;&#19968;&#20010;&#22909;&#30340;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#31163;&#32447;RL&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#31616;&#21333;&#19988;&#31163;&#25955;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#25277;&#35937;&#20986;&#21407;&#22987;&#29615;&#22659;&#12290;RL&#26041;&#27861;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#19990;&#30028;&#27169;&#22411;&#32780;&#38750;&#29615;&#22659;&#25968;&#25454;&#36827;&#34892;&#31616;&#21270;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#19990;&#30028;&#27169;&#22411;&#31216;&#20026;&#20215;&#20540;&#35760;&#24518;&#22270;&#65288;VMG&#65289;&#65292;&#23427;&#34987;&#35774;&#35745;&#20026;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20854;&#20013;&#39030;&#28857;&#21644;&#26377;&#21521;&#36793;&#20998;&#21035;&#20195;&#34920;&#22270;&#29366;&#24577;&#21644;&#22270;&#21160;&#20316;&#12290;&#30001;&#20110;VMG&#30340;&#29366;&#24577;&#31354;&#38388;&#21644;&#21160;&#20316;&#31354;&#38388;&#30456;&#23545;&#20110;&#21407;&#22987;&#29615;&#22659;&#32780;&#35328;&#26159;&#26377;&#38480;&#19988;&#30456;&#23545;&#36739;&#23567;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#30452;&#25509;&#22312;VMG&#19978;&#24212;&#29992;&#20215;&#20540;&#36845;&#20195;&#31639;&#27861;&#26469;&#20272;&#31639;&#22270;&#29366;&#24577;&#20540;&#24182;&#30830;&#23450;&#26368;&#20339;&#30340;&#22270;&#21160;&#20316;&#12290;VMG&#26159;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#21644;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20026;&#26368;&#20248;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.03353</link><description>&lt;p&gt;
&#22312;&#19981;&#31283;&#20581;&#26679;&#26412;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving adversarial robustness by putting more regularizations on less robust samples. (arXiv:2206.03353v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20026;&#26368;&#20248;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20154;&#31867;&#35270;&#35273;&#26080;&#27861;&#23519;&#35273;&#30340;&#25968;&#25454;&#25200;&#21160;&#19979;&#65292;&#20351;&#32473;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#35823;&#21028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#35777;&#26126;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#19968;&#20010;&#26032;&#30340;&#29305;&#28857;&#26159;&#65306;&#23545;&#20110;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#65292;&#27604;&#20854;&#20182;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#31639;&#27861;&#26356;&#22810;&#22320;&#24212;&#29992;&#27491;&#21017;&#21270;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#19968;&#20010;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#30340;&#27491;&#21017;&#21270;&#31639;&#27861;&#65292;&#23427;&#26469;&#33258;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#39118;&#38505;&#19978;&#30028;&#30340;&#21160;&#26426;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;(&#22312;&#20363;&#23376;&#19978;&#30340;&#20934;&#30830;&#24615;)&#21644;&#40065;&#26834;&#24615;(&#22312;&#23545;&#25239;&#25915;&#20987;&#19978;&#30340;&#20934;&#30830;&#24615;)&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing the regularized empirical risk motivated from a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22242;&#20307;&#20844;&#24179;&#25351;&#26631;&#26694;&#26550;&#65292;&#23558;&#20854;&#19982;&#26356;&#22810;&#30340;&#20998;&#37197;&#20844;&#27491;&#29702;&#35770;&#32852;&#31995;&#36215;&#26469;&#65292;&#25581;&#31034;&#20102;&#26631;&#20934;&#22242;&#20307;&#20844;&#24179;&#25351;&#26631;&#25152;&#28041;&#21450;&#30340;&#35268;&#33539;&#36873;&#25321;&#24182;&#20801;&#35768;&#35299;&#37322;&#36947;&#24503;&#23454;&#36136;&#12290;</title><link>http://arxiv.org/abs/2206.02897</link><description>&lt;p&gt;
&#20998;&#24067;&#20844;&#27491;&#20316;&#20026;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#21069;&#25552;&#65306;&#22242;&#20307;&#20844;&#24179;&#25351;&#26631;&#30340;&#32479;&#19968;&#12289;&#25299;&#23637;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Distributive Justice as the Foundational Premise of Fair ML: Unification, Extension, and Interpretation of Group Fairness Metrics. (arXiv:2206.02897v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22242;&#20307;&#20844;&#24179;&#25351;&#26631;&#26694;&#26550;&#65292;&#23558;&#20854;&#19982;&#26356;&#22810;&#30340;&#20998;&#37197;&#20844;&#27491;&#29702;&#35770;&#32852;&#31995;&#36215;&#26469;&#65292;&#25581;&#31034;&#20102;&#26631;&#20934;&#22242;&#20307;&#20844;&#24179;&#25351;&#26631;&#25152;&#28041;&#21450;&#30340;&#35268;&#33539;&#36873;&#25321;&#24182;&#20801;&#35768;&#35299;&#37322;&#36947;&#24503;&#23454;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22242;&#20307;&#20844;&#24179;&#25351;&#26631;&#26159;&#35780;&#20272;&#22522;&#20110;&#39044;&#27979;&#30340;&#20915;&#31574;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#19968;&#31181;&#24050;&#32463;&#30830;&#31435;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#26631;&#20173;&#28982;&#19981;&#36275;&#20197;&#19982;&#21746;&#23398;&#29702;&#35770;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36947;&#24503;&#21547;&#20041;&#32463;&#24120;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22242;&#20307;&#20844;&#24179;&#25351;&#26631;&#26694;&#26550;&#65292;&#23558;&#23427;&#20204;&#19982;&#26356;&#22810;&#30340;&#20998;&#37197;&#20844;&#27491;&#29702;&#35770;&#32852;&#31995;&#36215;&#26469;&#12290;&#19981;&#21516;&#30340;&#22242;&#20307;&#20844;&#24179;&#25351;&#26631;&#22312;&#34913;&#37327;&#20915;&#31574;&#23545;&#21463;&#24433;&#21709;&#20154;&#21592;&#30340;&#21033;&#30410;&#25110;&#20260;&#23475;&#20197;&#21450;&#25152;&#20551;&#35774;&#30340;&#36947;&#24503;&#26435;&#21033;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#32479;&#19968;&#26694;&#26550;&#25581;&#31034;&#20102;&#26631;&#20934;&#22242;&#20307;&#20844;&#24179;&#25351;&#26631;&#25152;&#28041;&#21450;&#30340;&#35268;&#33539;&#36873;&#25321;&#65292;&#24182;&#20801;&#35768;&#35299;&#37322;&#23427;&#20204;&#30340;&#36947;&#24503;&#23454;&#36136;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#20026;&#25105;&#20204;&#22312;&#25991;&#29486;&#20013;&#25214;&#21040;&#30340;&#26631;&#20934;&#20844;&#24179;&#25351;&#26631;&#30340;&#25193;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#12290;&#36825;&#20010;&#25193;&#23637;&#20801;&#35768;&#35299;&#20915;&#26631;&#20934;&#22242;&#20307;&#20844;&#24179;&#25351;&#26631;&#30340;&#33509;&#24178;&#25209;&#35780;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#26159;&#22522;&#20110;&#24179;&#31561;&#30340;&#65292;&#21363;&#23427;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Group fairness metrics are an established way of assessing the fairness of prediction-based decision-making systems. However, these metrics are still insufficiently linked to philosophical theories, and their moral meaning is often unclear. In this paper, we propose a comprehensive framework for group fairness metrics, which links them to more theories of distributive justice. The different group fairness metrics differ in their choices about how to measure the benefit or harm of a decision for the affected individuals, and what moral claims to benefits are assumed. Our unifying framework reveals the normative choices associated with standard group fairness metrics and allows an interpretation of their moral substance. In addition, this broader view provides a structure for the expansion of standard fairness metrics that we find in the literature. This expansion allows addressing several criticisms of standard group fairness metrics, specifically: (1) they are parity-based, i.e., they 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#27491;&#19982;&#25928;&#29992;&#24182;&#37325;&#30340;&#31639;&#27861;&#20915;&#31574;&#26694;&#26550;&#65292;&#33021;&#22815;&#24179;&#34913;&#20915;&#31574;&#21046;&#23450;&#32773;&#21644;&#20915;&#31574;&#23545;&#35937;&#30340;&#35266;&#28857;&#65292;&#20854;&#20013;&#20844;&#27491;&#24615;&#35780;&#20272;&#22522;&#20110;&#20844;&#27491;&#30340;&#20998;&#37197;&#29702;&#35770;&#21644;&#31639;&#27861;&#20844;&#27491;&#30340;&#25991;&#29486;&#30740;&#31350;&#65292;&#25928;&#29992;&#35780;&#20272;&#22522;&#20110;&#22522;&#25968;&#25928;&#29992;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2206.02891</link><description>&lt;p&gt;
&#22522;&#20110;&#20844;&#27491;&#30340;&#31639;&#27861;&#26435;&#34913;-&#25928;&#29992;&#20132;&#26131;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Justice-Based Framework for the Analysis of Algorithmic Fairness-Utility Trade-Offs. (arXiv:2206.02891v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#27491;&#19982;&#25928;&#29992;&#24182;&#37325;&#30340;&#31639;&#27861;&#20915;&#31574;&#26694;&#26550;&#65292;&#33021;&#22815;&#24179;&#34913;&#20915;&#31574;&#21046;&#23450;&#32773;&#21644;&#20915;&#31574;&#23545;&#35937;&#30340;&#35266;&#28857;&#65292;&#20854;&#20013;&#20844;&#27491;&#24615;&#35780;&#20272;&#22522;&#20110;&#20844;&#27491;&#30340;&#20998;&#37197;&#29702;&#35770;&#21644;&#31639;&#27861;&#20844;&#27491;&#30340;&#25991;&#29486;&#30740;&#31350;&#65292;&#25928;&#29992;&#35780;&#20272;&#22522;&#20110;&#22522;&#25968;&#25928;&#29992;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#39044;&#27979;&#30340;&#20915;&#31574;&#31995;&#32479;&#20013;&#65292;&#19981;&#21516;&#30340;&#35266;&#28857;&#24120;&#24120;&#23384;&#22312;&#30683;&#30462;&#65306;&#20915;&#31574;&#21046;&#23450;&#32773;&#30340;&#30701;&#26399;&#21830;&#19994;&#30446;&#26631;&#19982;&#20915;&#31574;&#23545;&#35937;&#24076;&#26395;&#24471;&#21040;&#20844;&#27491;&#24453;&#36935;&#30340;&#24895;&#26395;&#24448;&#24448;&#30456;&#20914;&#31361;&#12290;&#24179;&#34913;&#36825;&#20004;&#31181;&#35266;&#28857;&#26159;&#19968;&#20010;&#20851;&#20046;&#20215;&#20540;&#35266;&#30340;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#20215;&#20540;&#35266;&#24120;&#24120;&#38544;&#34255;&#22312;&#20915;&#31574;&#31995;&#32479;&#23454;&#26045;&#30340;&#25216;&#26415;&#32454;&#33410;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#36825;&#20123;&#20215;&#20540;&#21547;&#20041;&#26126;&#30830;&#21487;&#35265;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#20010;&#25105;&#20204;&#24819;&#25214;&#21040;&#33021;&#22815;&#24179;&#34913;&#20915;&#31574;&#21046;&#23450;&#32773;&#21644;&#20915;&#31574;&#23545;&#35937;&#35266;&#28857;&#30340;&#20915;&#31574;&#35268;&#21017;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#24418;&#24335;&#21270;&#21644;&#35780;&#20272;&#20915;&#31574;&#21046;&#23450;&#32773;&#21644;&#20915;&#31574;&#23545;&#35937;&#30340;&#25928;&#29992;&#21644;&#20844;&#27491;&#24615;&#12290;&#23545;&#20110;&#20844;&#27491;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#22522;&#20110;&#20844;&#27491;&#30340;&#20998;&#37197;&#29702;&#35770;&#21644;&#31639;&#27861;&#20844;&#27491;&#30340;&#25991;&#29486;&#30740;&#31350;&#12290;&#23545;&#20110;&#25928;&#29992;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#25968;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#25991;&#29486;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#33879;&#21517;&#20363;&#23376;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In prediction-based decision-making systems, different perspectives can be at odds: The short-term business goals of the decision makers are often in conflict with the decision subjects' wish to be treated fairly. Balancing these two perspectives is a question of values. However, these values are often hidden in the technicalities of the implementation of the decision-making system. In this paper, we propose a framework to make these value-laden choices clearly visible. We focus on a setting in which we want to find decision rules that balance the perspective of the decision maker and of the decision subjects. We provide an approach to formalize both perspectives, i.e., to assess the utility of the decision maker and the fairness towards the decision subjects. In both cases, the idea is to elicit values from decision makers and decision subjects that are then turned into something measurable. For the fairness evaluation, we build on well-known theories of distributive justice and on th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#20551;&#35774;&#28608;&#27963;&#20989;&#25968;&#36830;&#32493;&#21487;&#23548;&#65292;&#21017;&#21487;&#20197;&#36890;&#36807;&#36138;&#24515;&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23547;&#25214;&#21040;&#26368;&#23567;&#36755;&#20837;&#29305;&#24449;&#38598;&#21512;&#65292;&#35299;&#20915;&#38590;&#20197;&#22788;&#29702;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.09901</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#22522;&#25968;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cardinality-Minimal Explanations for Monotonic Neural Networks. (arXiv:2205.09901v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#20551;&#35774;&#28608;&#27963;&#20989;&#25968;&#36830;&#32493;&#21487;&#23548;&#65292;&#21017;&#21487;&#20197;&#36890;&#36807;&#36138;&#24515;&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23547;&#25214;&#21040;&#26368;&#23567;&#36755;&#20837;&#29305;&#24449;&#38598;&#21512;&#65292;&#35299;&#20915;&#38590;&#20197;&#22788;&#29702;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#30340;&#35299;&#37322;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20854;&#20013;&#26368;&#20026;&#20934;&#30830;&#19988;&#24418;&#24335;&#21270;&#30340;&#26041;&#27861;&#21253;&#25324;&#23547;&#25214;&#23545;&#20110;&#39044;&#27979;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#26368;&#23567;&#36755;&#20837;&#29305;&#24449;&#38598;&#21512;&#65292;&#20197;&#21450;&#25913;&#21464;&#39044;&#27979;&#32467;&#26524;&#30340;&#26368;&#23567;&#36755;&#20837;&#29305;&#24449;&#38598;&#21512;&#12290;&#20294;&#36825;&#20123;&#30456;&#24212;&#30340;&#20915;&#31574;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35843;&#20989;&#25968;&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#27492;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#65292;&#21457;&#29616;&#22914;&#26524;&#20551;&#35774;&#28608;&#27963;&#20989;&#25968;&#22312;&#21508;&#22788;&#22343;&#36830;&#32493;&#19988;&#20960;&#20046;&#26080;&#22788;&#19981;&#21487;&#23548;&#65292;&#21017;&#21487;&#20197;&#36890;&#36807;&#36138;&#24515;&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35299;&#20915;&#30456;&#20851;&#30340;&#38590;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been increasing interest in explanation methods for neural model predictions that offer precise formal guarantees. These include abductive (respectively, contrastive) methods, which aim to compute minimal subsets of input features that are sufficient for a given prediction to hold (respectively, to change a given prediction). The corresponding decision problems are, however, known to be intractable. In this paper, we investigate whether tractability can be regained by focusing on neural models implementing a monotonic function. Although the relevant decision problems remain intractable, we can show that they become solvable in polynomial time by means of greedy algorithms if we additionally assume that the activation functions are continuous everywhere and differentiable almost everywhere. Our experiments suggest favourable performance of our algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;GLCC&#65289;&#20195;&#30721;&#65292;&#35813;&#20195;&#30721;&#21487;&#20197;&#25552;&#20379;&#40065;&#26834;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#22810;&#20010;&#32452;&#65292;&#20351;&#29992;&#25554;&#20540;&#22810;&#39033;&#24335;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#20998;&#20139;&#32534;&#30721;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#22312;&#20027;&#33410;&#28857;&#19978;&#28040;&#38500;&#36328;&#32452;&#30340;&#24178;&#25200;&#35745;&#31639;&#32467;&#26524;&#12290;GLCC&#20195;&#30721;&#26159;&#29616;&#26377;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;LCC&#65289;&#20195;&#30721;&#30340;&#19968;&#31181;&#29305;&#20363;&#65292;&#20855;&#26377;&#26356;&#28789;&#27963;&#30340;&#25240;&#34935;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2204.11168</link><description>&lt;p&gt;
&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65306;&#29992;&#20110;&#20855;&#26377;&#40065;&#26834;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35745;&#31639;&#36890;&#20449;&#25240;&#34935;&#30340;&#28789;&#27963;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Generalized Lagrange Coded Computing: A Flexible Computation-Communication Tradeoff for Resilient, Secure, and Private Computation. (arXiv:2204.11168v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;GLCC&#65289;&#20195;&#30721;&#65292;&#35813;&#20195;&#30721;&#21487;&#20197;&#25552;&#20379;&#40065;&#26834;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#22810;&#20010;&#32452;&#65292;&#20351;&#29992;&#25554;&#20540;&#22810;&#39033;&#24335;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#20998;&#20139;&#32534;&#30721;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#22312;&#20027;&#33410;&#28857;&#19978;&#28040;&#38500;&#36328;&#32452;&#30340;&#24178;&#25200;&#35745;&#31639;&#32467;&#26524;&#12290;GLCC&#20195;&#30721;&#26159;&#29616;&#26377;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;LCC&#65289;&#20195;&#30721;&#30340;&#19968;&#31181;&#29305;&#20363;&#65292;&#20855;&#26377;&#26356;&#28789;&#27963;&#30340;&#25240;&#34935;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21253;&#21547;&#22810;&#20010;&#36755;&#20837;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20219;&#24847;&#22810;&#20803;&#22810;&#39033;&#24335;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#20010;&#20027;&#33410;&#28857;&#21644;&#22810;&#20010;&#24037;&#20316;&#33410;&#28857;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#31995;&#32479;&#12290;&#25552;&#20986;&#20102;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;GLCC&#65289;&#20195;&#30721;&#65292;&#21516;&#26102;&#25552;&#20379;&#38024;&#23545;&#19981;&#21450;&#26102;&#36820;&#22238;&#35745;&#31639;&#32467;&#26524;&#30340;&#33853;&#21518;&#32773;&#30340;&#40065;&#26834;&#24615;&#65292;&#38024;&#23545;&#24694;&#24847;&#24037;&#20154;&#25925;&#24847;&#20462;&#25913;&#32467;&#26524;&#20197;&#33719;&#21462;&#22909;&#22788;&#30340;&#23433;&#20840;&#24615;&#65292;&#20197;&#21450;&#22312;&#24037;&#20154;&#21487;&#33021;&#20849;&#35851;&#30340;&#24773;&#20917;&#19979;&#32500;&#25252;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#12290;GLCC&#20195;&#30721;&#39318;&#20808;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#22810;&#20010;&#32452;&#65292;&#28982;&#21518;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25554;&#20540;&#22810;&#39033;&#24335;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#23558;&#22810;&#20010;&#32534;&#30721;&#25968;&#25454;&#28857;&#20998;&#20139;&#32473;&#27599;&#20010;&#24037;&#20316;&#22120;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#20027;&#33410;&#28857;&#19978;&#28040;&#38500;&#36328;&#32452;&#30340;&#24178;&#25200;&#35745;&#31639;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;GLCC&#20195;&#30721;&#21253;&#25324;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;LCC&#65289;&#20195;&#30721;&#20316;&#20026;&#19968;&#31181;&#29305;&#20363;&#65292;&#26174;&#31034;&#20986;&#26356;&#28789;&#27963;&#30340;&#25240;&#34935;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of evaluating arbitrary multivariate polynomials over a massive dataset containing multiple inputs, on a distributed computing system with a master node and multiple worker nodes. Generalized Lagrange Coded Computing (GLCC) codes are proposed to simultaneously provide resiliency against stragglers who do not return computation results in time, security against adversarial workers who deliberately modify results for their benefit, and information-theoretic privacy of the dataset amidst possible collusion of workers. GLCC codes are constructed by first partitioning the dataset into multiple groups, then encoding the dataset using carefully designed interpolation polynomials, and sharing multiple encoded data points to each worker, such that interference computation results across groups can be eliminated at the master. Particularly, GLCC codes include the state-of-the-art Lagrange Coded Computing (LCC) codes as a special case, and exhibit a more flexible tradeoff 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#21644;&#25506;&#35752;&#20102;&#36793;&#32536;&#20284;&#28982;&#22312;&#26500;&#36896;&#32422;&#26463;&#21644;&#20551;&#35774;&#27979;&#35797;&#26041;&#38754;&#30340;&#26377;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36793;&#32536;&#20284;&#28982;&#20316;&#20026;&#27867;&#21270;&#30340;&#20195;&#29702;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22914;&#20309;&#19982;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30456;&#20851;&#65292;&#21487;&#33021;&#23548;&#33268;&#36229;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2202.11678</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27169;&#22411;&#36873;&#25321;&#12289;&#36793;&#38469;&#20284;&#28982;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Model Selection, the Marginal Likelihood, and Generalization. (arXiv:2202.11678v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#21644;&#25506;&#35752;&#20102;&#36793;&#32536;&#20284;&#28982;&#22312;&#26500;&#36896;&#32422;&#26463;&#21644;&#20551;&#35774;&#27979;&#35797;&#26041;&#38754;&#30340;&#26377;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36793;&#32536;&#20284;&#28982;&#20316;&#20026;&#27867;&#21270;&#30340;&#20195;&#29702;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22914;&#20309;&#19982;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30456;&#20851;&#65292;&#21487;&#33021;&#23548;&#33268;&#36229;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#27604;&#36739;&#19982;&#35266;&#27979;&#23436;&#20840;&#19968;&#33268;&#30340;&#20551;&#35774;&#20043;&#38388;&#30340;&#21306;&#21035;&#65311;&#36793;&#38469;&#20284;&#28982;&#65288;&#20134;&#31216;&#20026;&#36125;&#21494;&#26031;&#35777;&#25454;&#65289;&#20316;&#20026;&#29983;&#25104;&#30001;&#20808;&#39564;&#24471;&#21040;&#35266;&#27979;&#32467;&#26524;&#30340;&#27010;&#29575;&#65292;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#32534;&#30721;&#22885;&#21345;&#22982;&#21059;&#20992;&#21407;&#29702;&#12290;&#23613;&#31649;&#24050;&#32463;&#35266;&#23519;&#21040;&#36793;&#38469;&#20284;&#28982;&#21487;&#33021;&#36807;&#25311;&#21512;&#24182;&#19988;&#23545;&#20808;&#39564;&#20551;&#35774;&#24456;&#25935;&#24863;&#65292;&#20294;&#20854;&#22312;&#36229;&#21442;&#25968;&#23398;&#20064;&#21644;&#31163;&#25955;&#27169;&#22411;&#27604;&#36739;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#20808;&#37325;&#28201;&#20102;&#36793;&#38469;&#20284;&#28982;&#30340;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#23398;&#20064;&#32422;&#26463;&#21644;&#20551;&#35774;&#27979;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20351;&#29992;&#36793;&#38469;&#20284;&#28982;&#20316;&#20026;&#27867;&#21270;&#30340;&#20195;&#29702;&#23384;&#22312;&#30340;&#27010;&#24565;&#21644;&#23454;&#38469;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36793;&#38469;&#20284;&#28982;&#22914;&#20309;&#19982;&#27867;&#21270;&#21576;&#36127;&#30456;&#20851;&#65292;&#24182;&#23545;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20135;&#29983;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#36229;&#21442;&#25968;&#23398;&#20064;&#20013;&#21487;&#33021;&#23548;&#33268;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We also re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38381;&#21512;&#24314;&#27169;&#26041;&#27861;CD-ROM&#65292;&#29992;&#20110;&#32463;&#20856;&#30340;POD-Galerkin&#38477;&#38454;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.10746</link><description>&lt;p&gt;
CD-ROM&#65306;&#34917;&#20805;&#28145;&#24230;&#20943;&#23569;&#38454;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CD-ROM: Complemented Deep-Reduced Order Model. (arXiv:2202.10746v4 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38381;&#21512;&#24314;&#27169;&#26041;&#27861;CD-ROM&#65292;&#29992;&#20110;&#32463;&#20856;&#30340;POD-Galerkin&#38477;&#38454;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;POD-Galerkin&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#38454;&#25968;&#32422;&#31616;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#27714;&#35299;&#29289;&#29702;&#38382;&#39064;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#39640;&#32500;&#21160;&#21147;&#31995;&#32479;&#22914;Navier-Stokes&#26041;&#31243;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#20135;&#29983;&#19981;&#20934;&#30830;&#19988;&#26377;&#26102;&#19981;&#31283;&#23450;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38381;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#32463;&#20856;&#30340;POD-Galerkin&#38477;&#38454;&#27169;&#22411;(ROM)&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#26159;&#26377;&#22522;&#30784;&#30340;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#30740;&#31350;&#24471;&#24403;&#30340;&#36816;&#31639;&#31526;&#12290;&#19982;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#26412;&#25991;&#20013;&#30340;CD-ROM&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#36830;&#32493;&#35760;&#24518;&#24418;&#24335;&#65292;&#30001;&#20851;&#20110;&#37096;&#20998;&#35266;&#27979;&#21040;&#30340;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#31616;&#21333;&#20551;&#35774;&#23548;&#20986;&#12290;&#22240;&#27492;&#65292;&#20462;&#27491;&#21518;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#22823;&#22810;&#25968;&#32463;&#20856;&#30340;&#26102;&#38388;&#27493;&#36827;&#27169;&#24335;&#36827;&#34892;&#27169;&#25311;&#12290;CD-ROM&#26041;&#27861;&#30340;&#33021;&#21147;&#22312;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#30340;&#20004;&#20010;&#32463;&#20856;&#26696;&#20363;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#38477;&#38454;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model order reduction through the POD-Galerkin method can lead to dramatic gains in terms of computational efficiency in solving physical problems. However, the applicability of the method to non linear high-dimensional dynamical systems such as the Navier-Stokes equations has been shown to be limited, producing inaccurate and sometimes unstable models. This paper proposes a deep learning based closure modeling approach for classical POD-Galerkin reduced order models (ROM). The proposed approach is theoretically grounded, using neural networks to approximate well studied operators. In contrast with most previous works, the present CD-ROM approach is based on an interpretable continuous memory formulation, derived from simple hypotheses on the behavior of partially observed dynamical systems. The final corrected models can hence be simulated using most classical time stepping schemes. The capabilities of the CD-ROM approach are demonstrated on two classical examples from Computational F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#22238;&#35793;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#26469;&#26377;&#25928;&#22320;&#20943;&#23569;&#20004;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#38388;&#31163;&#25955;&#23646;&#24615;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#31471;&#21040;&#31471;&#24335;&#30340;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#27604;&#20197;&#21069;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#30340;BLEU&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2202.08465</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#22238;&#35793;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
End-to-End Training for Back-Translation with Categorical Reparameterization Trick. (arXiv:2202.08465v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#22238;&#35793;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#26469;&#26377;&#25928;&#22320;&#20943;&#23569;&#20004;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#38388;&#31163;&#25955;&#23646;&#24615;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#31471;&#21040;&#31471;&#24335;&#30340;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#27604;&#20197;&#21069;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#30340;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#35793;&#26159;&#19968;&#31181;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;&#39044;&#20808;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#32763;&#35793;&#21333;&#35821;&#21477;&#23376;&#24182;&#29983;&#25104;&#21512;&#25104;&#30340;&#21452;&#35821;&#21477;&#23545;&#20197;&#35757;&#32451;&#21478;&#19968;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#23558;&#20004;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20998;&#21035;&#29702;&#35299;&#20026;&#25512;&#29702;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#22521;&#35757;&#26694;&#26550;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32763;&#35793;&#21477;&#23376;&#30340;&#31163;&#25955;&#23646;&#24615;&#20351;&#24471;&#26799;&#24230;&#20449;&#24687;&#26080;&#27861;&#22312;&#20004;&#20010;NMT&#27169;&#22411;&#20043;&#38388;&#27969;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#65292;&#20351;&#24471;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21487;&#24494;&#20998;&#30340;&#21477;&#23376;&#65292;&#20351;&#24471;VAE&#30340;&#35757;&#32451;&#26694;&#26550;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#26041;&#24335;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35757;&#32451;&#20102;NMT&#27169;&#22411;&#65292;&#24182;&#22312;WMT&#32763;&#35793;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#27604;&#20197;&#21069;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#30340;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Back-translation is an effective semi-supervised learning framework in neural machine translation (NMT). A pre-trained NMT model translates monolingual sentences and makes synthetic bilingual sentence pairs for the training of the other NMT model, and vice versa. Understanding the two NMT models as inference and generation models, respectively, previous works applied the training framework of variational auto-encoder (VAE). However, the discrete property of translated sentences prevents gradient information from flowing between the two NMT models. In this paper, we propose a categorical reparameterization trick that makes NMT models generate differentiable sentences so that the VAE's training framework can work in the end-to-end fashion. Our experiments demonstrate that our method effectively trains the NMT models and achieves better BLEU scores than the previous baseline on the datasets of the WMT translation task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#25209;&#27425;&#22823;&#23567;&#19982;&#35757;&#32451;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20026;&#20102;&#25214;&#21040;&#31283;&#23450;&#28857;&#65292;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20250;&#20943;&#23569;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2201.11989</link><description>&lt;p&gt;
&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#30340;&#23384;&#22312;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26102;&#25209;&#27425;&#22823;&#23567;&#19982;&#35757;&#32451;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20026;&#20102;&#25214;&#21040;&#31283;&#23450;&#28857;&#65292;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20250;&#20943;&#23569;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#65292;&#22914;&#19981;&#21516;&#30340;&#24658;&#23450;&#29575;&#25110;&#19981;&#21516;&#30340;&#34928;&#20943;&#29575;&#31561;&#65292;&#20351;&#29992;&#20004;&#26102;&#38388;&#23610;&#24230;&#26356;&#26032;&#35268;&#21017;&#65288;TTUR&#65289;&#26377;&#21161;&#20110;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#27492;&#22806;&#65292;&#25209;&#27425;&#22823;&#23567;&#23545;&#20110;&#20351;&#29992;TTUR&#35757;&#32451;GANs&#20063;&#24456;&#37325;&#35201;&#65292;&#20004;&#32773;&#37117;&#24433;&#21709;&#20102;&#35757;&#32451;&#25152;&#38656;&#30340;&#27493;&#39588;&#25968;&#37327;&#12290;&#26412;&#25991;&#22522;&#20110;&#24658;&#23450;&#23398;&#20064;&#29575;&#30740;&#31350;&#20102;&#25209;&#27425;&#22823;&#23567;&#19982;&#20351;&#29992;TTUR&#35757;&#32451;GANs&#25152;&#38656;&#27493;&#39588;&#25968;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#24658;&#23450;&#23398;&#20064;&#29575;&#30340;TTUR&#65292;&#20026;&#20102;&#25214;&#21040;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#65292;&#25152;&#38656;&#27493;&#39588;&#25968;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#26368;&#23567;&#21270;&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25209;&#27425;&#22823;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Fr'echet Inception Distance&#65288;FID&#65289;&#20316;&#20026;&#35757;&#32451;&#30340;&#24615;&#33021;&#27979;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu
&lt;/p&gt;</description></item><item><title>MassFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#39044;&#27979;&#23567;&#20998;&#23376;&#30340;&#20018;&#32852;&#36136;&#35889;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#22270;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#32771;&#34385;&#20998;&#23376;&#20013;&#21407;&#23376;&#20043;&#38388;&#30340;&#36828;&#36317;&#31163;&#20851;&#31995;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20987;&#36133;&#20854;&#20182;&#26041;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#20851;&#20110;&#30896;&#25758;&#33021;&#37327;&#23545;&#20809;&#35889;&#30340;&#24433;&#21709;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2111.04824</link><description>&lt;p&gt;
MassFormer&#65306;&#20351;&#29992;&#22270;&#21464;&#25442;&#22120;&#39044;&#27979;&#23567;&#20998;&#23376;&#20018;&#32852;&#36136;&#35889;
&lt;/p&gt;
&lt;p&gt;
MassFormer: Tandem Mass Spectrum Prediction for Small Molecules using Graph Transformers. (arXiv:2111.04824v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.04824
&lt;/p&gt;
&lt;p&gt;
MassFormer&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#39044;&#27979;&#23567;&#20998;&#23376;&#30340;&#20018;&#32852;&#36136;&#35889;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#22270;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#32771;&#34385;&#20998;&#23376;&#20013;&#21407;&#23376;&#20043;&#38388;&#30340;&#36828;&#36317;&#31163;&#20851;&#31995;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20987;&#36133;&#20854;&#20182;&#26041;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#20851;&#20110;&#30896;&#25758;&#33021;&#37327;&#23545;&#20809;&#35889;&#30340;&#24433;&#21709;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20018;&#32852;&#36136;&#35889;&#33021;&#22815;&#25429;&#25417;&#20998;&#23376;&#30340;&#30862;&#29255;&#27169;&#24335;&#65292;&#25552;&#20379;&#20851;&#20110;&#20998;&#23376;&#32467;&#26500;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#23613;&#31649;&#36136;&#35889;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#24212;&#29992;&#65292;&#20294;&#32477;&#22823;&#22810;&#25968;&#23567;&#20998;&#23376;&#32570;&#20047;&#23454;&#39564;&#21442;&#32771;&#35889;&#12290;&#19971;&#21313;&#22810;&#24180;&#26469;&#65292;&#35889;&#39044;&#27979;&#19968;&#30452;&#26159;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#20998;&#23376;&#30340;&#20840;&#23616;&#32467;&#26500;&#65292;&#21487;&#33021;&#23548;&#33268;&#38590;&#20197;&#25512;&#24191;&#21040;&#26032;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;MassFormer&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#20018;&#32852;&#36136;&#35889;&#12290;MassFormer&#20351;&#29992;&#22270;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#27169;&#25311;&#20998;&#23376;&#20013;&#21407;&#23376;&#20043;&#38388;&#30340;&#36828;&#36317;&#31163;&#20851;&#31995;&#12290;&#21464;&#24418;&#22120;&#27169;&#22359;&#20351;&#29992;&#21270;&#23398;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#33719;&#24471;&#30340;&#21442;&#25968;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#28982;&#21518;&#22312;&#35889;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;MassFormer&#22312;&#22810;&#32452;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#65292;&#33021;&#22815;&#24674;&#22797;&#20851;&#20110;&#30896;&#25758;&#33021;&#37327;&#23545;&#20809;&#35889;&#30340;&#24433;&#21709;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tandem mass spectra capture fragmentation patterns that provide key structural information about a molecule. Although mass spectrometry is applied in many areas, the vast majority of small molecules lack experimental reference spectra. For over seventy years, spectrum prediction has remained a key challenge in the field. Existing deep learning methods do not leverage global structure in the molecule, potentially resulting in difficulties when generalizing to new data. In this work we propose a new model, MassFormer, for accurately predicting tandem mass spectra. MassFormer uses a graph transformer architecture to model long-distance relationships between atoms in the molecule. The transformer module is initialized with parameters obtained through a chemical pre-training task, then fine-tuned on spectral data. MassFormer outperforms competing approaches for spectrum prediction on multiple datasets, and is able to recover prior knowledge about the effect of collision energy on the spectr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26234;&#33021;&#20307;&#38388;&#30340;&#28040;&#24687;&#20256;&#36882;&#29615;&#22659;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#36830;&#32493;&#22768;&#23398;&#36890;&#36947;&#36827;&#34892;&#36890;&#35759;&#24182;&#35266;&#23519;&#21040;&#26032;&#20852;&#35821;&#35328;&#30340;&#20135;&#29983;&#19982;&#29305;&#28857;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#19982;&#31163;&#25955;&#22411;&#20449;&#21495;&#19981;&#21516;&#65292;&#22768;&#23398;&#35762;&#35805;&#32773;&#23398;&#20064;&#20351;&#29992;&#20887;&#20313;&#20449;&#24687;&#20197;&#25552;&#39640;&#20390;&#21548;&#32773;&#30340;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.02827</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#32493;&#22768;&#23398;&#36890;&#36947;&#36827;&#34892;&#22810;&#26234;&#33021;&#20307;&#36890;&#35759;&#23398;&#20064;&#21548;&#35828;&#33021;&#21147;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Towards Learning to Speak and Hear Through Multi-Agent Communication over a Continuous Acoustic Channel. (arXiv:2111.02827v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26234;&#33021;&#20307;&#38388;&#30340;&#28040;&#24687;&#20256;&#36882;&#29615;&#22659;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#36830;&#32493;&#22768;&#23398;&#36890;&#36947;&#36827;&#34892;&#36890;&#35759;&#24182;&#35266;&#23519;&#21040;&#26032;&#20852;&#35821;&#35328;&#30340;&#20135;&#29983;&#19982;&#29305;&#28857;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#19982;&#31163;&#25955;&#22411;&#20449;&#21495;&#19981;&#21516;&#65292;&#22768;&#23398;&#35762;&#35805;&#32773;&#23398;&#20064;&#20351;&#29992;&#20887;&#20313;&#20449;&#24687;&#20197;&#25552;&#39640;&#20390;&#21548;&#32773;&#30340;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#20026;&#30740;&#31350;&#26234;&#33021;&#20307;&#38388;&#26032;&#20852;&#36890;&#35759;&#30340;&#26377;&#25928;&#25163;&#27573;&#65292;&#20294;&#23545;&#20110;&#36830;&#32493;&#22768;&#23398;&#36890;&#35759;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#36825;&#26356;&#31867;&#20284;&#20110;&#20154;&#31867;&#33719;&#24471;&#35821;&#35328;&#30340;&#26041;&#24335;&#65307;&#20154;&#31867;&#23156;&#20799;&#20027;&#35201;&#36890;&#36807;&#19982;&#30475;&#25252;&#32773;&#30340;&#36830;&#32493;&#20449;&#21495;&#20132;&#20114;&#26469;&#20064;&#24471;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#29616;&#22312;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#24179;&#21488;&#65292;&#20197;&#24320;&#22987;&#22635;&#34917;&#20154;&#31867;&#21644;&#26234;&#33021;&#20307;&#36890;&#20449;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35753;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#36830;&#32493;&#20449;&#21495;&#20197;&#21450;&#23427;&#20204;&#20135;&#29983;&#30340;&#26041;&#24335;&#65292;&#23427;&#20204;&#30340;&#29305;&#24449;&#20197;&#21450;&#23427;&#20204;&#19982;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning has been used as an effective means to study emergent communication between agents, yet little focus has been given to continuous acoustic communication. This would be more akin to human language acquisition; human infants acquire language in large part through continuous signalling with their caregivers. We therefore ask: Are we able to observe emergent language between agents with a continuous communication channel? Our goal is to provide a platform to begin bridging the gap between human and agent communication, allowing us to analyse continuous signals, how they emerge, their characteristics, and how they relate to human language acquisition. We propose a messaging environment where a Speaker agent needs to convey a set of attributes to a Listener over a noisy acoustic channel. Using DQN to train our agents, we show that: (1) unlike the discrete case, the acoustic Speaker learns redundancy to improve Listener coherency, (2) the acoustic Speaker de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;TUSLA&#31639;&#27861;&#22312;Wasserstein-1&#21644;Wasserstein-2&#36317;&#31163;&#19978;&#30340;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#38480;&#65292;&#36827;&#32780;&#25512;&#23548;&#20102;&#26399;&#26395;&#36807;&#37327;&#39118;&#38505;&#30340;&#38750;&#28176;&#36827;&#20272;&#35745;&#20540;&#12290;&#22312;ReLU&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29702;&#35770;&#21644;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;TUSLA&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#19988;&#31934;&#30830;&#22320;&#35299;&#20915;&#27492;&#31867;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2107.08649</link><description>&lt;p&gt;
&#38750;&#20984;&#23398;&#20064;&#20013;TUSLA&#31639;&#27861;&#30340;&#38750;&#28176;&#36827;&#20272;&#35745;&#21450;&#20854;&#22312;ReLU&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic estimates for TUSLA algorithm for non-convex learning with applications to neural networks with ReLU activation function. (arXiv:2107.08649v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;TUSLA&#31639;&#27861;&#22312;Wasserstein-1&#21644;Wasserstein-2&#36317;&#31163;&#19978;&#30340;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#38480;&#65292;&#36827;&#32780;&#25512;&#23548;&#20102;&#26399;&#26395;&#36807;&#37327;&#39118;&#38505;&#30340;&#38750;&#28176;&#36827;&#20272;&#35745;&#20540;&#12290;&#22312;ReLU&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29702;&#35770;&#21644;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;TUSLA&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#19988;&#31934;&#30830;&#22320;&#35299;&#20915;&#27492;&#31867;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#30446;&#26631;&#20989;&#25968;&#20855;&#26377;&#36229;&#32447;&#24615;&#22686;&#21152;&#21644;&#19981;&#36830;&#32493;&#38543;&#26426;&#26799;&#24230;&#30340;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#23545;Lovas&#31561;&#20154;&#65288;2020&#24180;&#65289;&#24341;&#20837;&#30340;tamed unadjusted stochastic Langevin algorithm&#65288;TUSLA&#65289;&#36827;&#34892;&#20102;&#38750;&#28176;&#36827;&#24615;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;Wasserstein-1&#21644;Wasserstein-2&#36317;&#31163;&#19978;&#24314;&#31435;&#20102;TUSLA&#31639;&#27861;&#30340;&#38750;&#28176;&#36827;&#35823;&#24046;&#30028;&#38480;&#12290;&#21518;&#19968;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#19968;&#27493;&#25512;&#23548;&#26399;&#26395;&#36807;&#37327;&#39118;&#38505;&#30340;&#38750;&#28176;&#36827;&#20272;&#35745;&#20540;&#12290;&#20026;&#20102;&#35828;&#26126;&#20027;&#35201;&#32467;&#26524;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21253;&#21547;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36801;&#31227;&#23398;&#20064;&#31034;&#20363;&#65292;&#35813;&#31034;&#20363;&#20195;&#34920;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#33539;&#20363;&#12290;&#25105;&#20204;&#20026;&#19978;&#36848;&#31034;&#20363;&#21576;&#29616;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#37117;&#35777;&#26126;&#20102;TUSLA&#31639;&#27861;&#33021;&#22815;&#39640;&#25928;&#19988;&#31934;&#30830;&#22320;&#35299;&#20915;&#28041;&#21450;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider non-convex stochastic optimization problems where the objective functions have super-linearly growing and discontinuous stochastic gradients. In such a setting, we provide a non-asymptotic analysis for the tamed unadjusted stochastic Langevin algorithm (TUSLA) introduced in Lovas et al. (2020). In particular, we establish non-asymptotic error bounds for the TUSLA algorithm in Wasserstein-1 and Wasserstein-2 distances. The latter result enables us to further derive non-asymptotic estimates for the expected excess risk. To illustrate the applicability of the main results, we consider an example from transfer learning with ReLU neural networks, which represents a key paradigm in machine learning. Numerical experiments are presented for the aforementioned example which support our theoretical findings. Hence, in this setting, we demonstrate both theoretically and numerically that the TUSLA algorithm can solve the optimization problem involving neural networks with ReLU activati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#19968;&#20123;&#20027;&#35201;&#30340;&#35789;&#21521;&#37327;&#26500;&#24314;&#31574;&#30053;&#65292;&#31216;&#20026;word embeddings&#65292;&#36825;&#20123;&#31574;&#30053;&#22522;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#32534;&#30721;&#20102;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#34987;&#35777;&#26126;&#22312;&#24456;&#22810;NLP&#20219;&#21153;&#20013;&#26159;&#26377;&#29992;&#30340;&#39069;&#22806;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/1901.09069</link><description>&lt;p&gt;
&#35789;&#21521;&#37327;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Word Embeddings: A Survey. (arXiv:1901.09069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.09069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#19968;&#20123;&#20027;&#35201;&#30340;&#35789;&#21521;&#37327;&#26500;&#24314;&#31574;&#30053;&#65292;&#31216;&#20026;word embeddings&#65292;&#36825;&#20123;&#31574;&#30053;&#22522;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#32534;&#30721;&#20102;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#34987;&#35777;&#26126;&#22312;&#24456;&#22810;NLP&#20219;&#21153;&#20013;&#26159;&#26377;&#29992;&#30340;&#39069;&#22806;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#21015;&#20986;&#24182;&#25551;&#36848;&#20102;&#36817;&#26399;&#20027;&#35201;&#30340;&#31574;&#30053;&#65292;&#22522;&#20110;&#20998;&#24067;&#20551;&#35774;&#65292;&#29992;&#20110;&#26500;&#24314;&#21333;&#35789;&#30340;&#22266;&#23450;&#38271;&#24230;&#12289;&#23494;&#38598;&#21644;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290; &#36825;&#20123;&#34920;&#31034;&#29616;&#22312;&#36890;&#24120;&#34987;&#31216;&#20026;&#35789;&#21521;&#37327;&#65292;&#24182;&#19988;&#38500;&#20102;&#32534;&#30721;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#20449;&#24687;&#22806;&#65292;&#22312;&#35768;&#22810;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#39069;&#22806;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.
&lt;/p&gt;</description></item></channel></rss>