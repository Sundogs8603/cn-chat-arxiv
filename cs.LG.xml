<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#65292;&#35752;&#35770;&#20102;&#33258;&#21160;&#35774;&#35745;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06532</link><description>&lt;p&gt;
&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Automated Design of Metaheuristic Algorithms. (arXiv:2303.06532v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#65292;&#35752;&#35770;&#20102;&#33258;&#21160;&#35774;&#35745;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, and discusses the potential future directions and open issues in this field.
&lt;/p&gt;
&lt;p&gt;
&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30001;&#20110;&#20854;&#33021;&#22815;&#29420;&#31435;&#20110;&#38382;&#39064;&#32467;&#26500;&#21644;&#38382;&#39064;&#39046;&#22495;&#36827;&#34892;&#25628;&#32034;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36890;&#24120;&#65292;&#38656;&#35201;&#20154;&#31867;&#19987;&#23478;&#25163;&#21160;&#35843;&#25972;&#31639;&#27861;&#20197;&#36866;&#24212;&#35299;&#20915;&#30446;&#26631;&#38382;&#39064;&#12290;&#25163;&#21160;&#35843;&#25972;&#36807;&#31243;&#21487;&#33021;&#26159;&#36153;&#21147;&#30340;&#12289;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#24341;&#36215;&#20102;&#23545;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#21644;&#38656;&#27714;&#65292;&#20197;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#33258;&#21160;&#35774;&#35745;&#21487;&#20197;&#20351;&#39640;&#24615;&#33021;&#31639;&#27861;&#23545;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21487;&#29992;&#65307;&#36890;&#36807;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#20805;&#20998;&#25506;&#32034;&#28508;&#22312;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#33258;&#21160;&#35774;&#35745;&#21487;&#20197;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#30340;&#35774;&#35745;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#24037;&#20316;&#30340;&#20849;&#21516;&#28857;&#21644;&#24046;&#24322;&#36827;&#34892;&#35843;&#26597;&#65292;&#25552;&#20986;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaheuristic algorithms have attracted wide attention from academia and industry due to their capability of conducting search independent of problem structures and problem domains. Often, human experts are requested to manually tailor algorithms to fit for solving a targeted problem. The manual tailoring process may be laborious, error-prone, and require intensive specialized knowledge. This gives rise to increasing interests and demands for automated design of metaheuristic algorithms with less human intervention. The automated design could make high-performance algorithms accessible to a much broader range of researchers and practitioners; and by leveraging computing power to fully explore the potential design choices, automated design could reach or even surpass human-level design. This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06530</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#25209;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.
&lt;/p&gt;
&lt;p&gt;
&#25209;&#26631;&#20934;&#21270;&#65288;BN&#65289;&#36890;&#24120;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#24182;&#21152;&#36895;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#38750;IID&#20998;&#25955;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20351;&#29992;BN&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;BN&#32479;&#35745;&#19981;&#21305;&#37197;&#32780;&#38459;&#30861;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#32676;&#32452;&#24402;&#19968;&#21270;&#65288;GN&#65289;&#26356;&#24120;&#29992;&#20110;FL&#20316;&#20026;BN&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;BN&#21644;GN&#20043;&#38388;&#27809;&#26377;&#19968;&#33268;&#30340;&#20248;&#32988;&#32773;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;FL&#20013;&#24402;&#19968;&#21270;&#23618;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;BN&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;FL&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#25104;&#20026;FL&#26410;&#26469;&#23454;&#38469;&#20351;&#29992;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20840;&#19987;&#23478;&#21453;&#39304;&#21644;Bandit&#21453;&#39304;&#35774;&#32622;&#20013;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.06526</link><description>&lt;p&gt;
&#25968;&#25454;&#30456;&#20851;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Data Dependent Regret Guarantees Against General Comparators for Full or Bandit Feedback. (arXiv:2303.06526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20840;&#19987;&#23478;&#21453;&#39304;&#21644;Bandit&#21453;&#39304;&#35774;&#32622;&#20013;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a data-dependent online learning algorithm framework that has data-dependent regret guarantees in both full expert feedback and bandit feedback settings, applicable for a wide variety of problem scenarios.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#23436;&#20840;&#22312;&#32447;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20855;&#26377;&#22312;&#20840;&#19987;&#23478;&#21453;&#39304;&#21644;Bandit&#21453;&#39304;&#35774;&#32622;&#20013;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#19968;&#33324;&#27604;&#36739;&#22120;&#30340;&#39044;&#26399;&#24615;&#33021;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#36890;&#29992;&#39044;&#27979;&#35282;&#24230;&#24037;&#20316;&#65292;&#20351;&#29992;&#30340;&#24615;&#33021;&#24230;&#37327;&#26159;&#23545;&#20219;&#24847;&#27604;&#36739;&#22120;&#24207;&#21015;&#30340;&#39044;&#26399;&#36951;&#25022;&#65292;&#21363;&#25105;&#20204;&#30340;&#25439;&#22833;&#19982;&#31454;&#20105;&#25439;&#22833;&#24207;&#21015;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#31454;&#20105;&#31867;&#21487;&#20197;&#35774;&#35745;&#20026;&#21253;&#25324;&#22266;&#23450;&#33218;&#36873;&#25321;&#12289;&#20999;&#25442;Bandit&#12289;&#19978;&#19979;&#25991;Bandit&#12289;&#21608;&#26399;Bandit&#25110;&#20219;&#20309;&#20854;&#20182;&#24863;&#20852;&#36259;&#30340;&#31454;&#20105;&#12290;&#31454;&#20105;&#31867;&#20013;&#30340;&#24207;&#21015;&#36890;&#24120;&#30001;&#20855;&#20307;&#24212;&#29992;&#31243;&#24207;&#30830;&#23450;&#65292;&#24182;&#24212;&#30456;&#24212;&#22320;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26082;&#19981;&#20351;&#29992;&#20063;&#19981;&#38656;&#35201;&#20219;&#20309;&#26377;&#20851;&#25439;&#22833;&#24207;&#21015;&#30340;&#21021;&#27493;&#20449;&#24687;&#65292;&#23436;&#20840;&#22312;&#32447;&#12290;&#20854;
&lt;/p&gt;
&lt;p&gt;
We study the adversarial online learning problem and create a completely online algorithmic framework that has data dependent regret guarantees in both full expert feedback and bandit feedback settings. We study the expected performance of our algorithm against general comparators, which makes it applicable for a wide variety of problem scenarios. Our algorithm works from a universal prediction perspective and the performance measure used is the expected regret against arbitrary comparator sequences, which is the difference between our losses and a competing loss sequence. The competition class can be designed to include fixed arm selections, switching bandits, contextual bandits, periodic bandits or any other competition of interest. The sequences in the competition class are generally determined by the specific application at hand and should be designed accordingly. Our algorithm neither uses nor needs any preliminary information about the loss sequences and is completely online. Its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#30340;&#39640;&#25928;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06519</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#26465;&#20214;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26080;&#25439;&#28857;&#20113;&#20960;&#20309;&#21644;&#23646;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossless Point Cloud Geometry and Attribute Compression Using a Learned Conditional Probability Model. (arXiv:2303.06519v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#30340;&#39640;&#25928;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient lossless point cloud compression method that uses sparse tensor-based deep neural networks to learn point cloud geometry and color probability distributions, achieving higher compression ratio and faster compression speed compared to the state-of-the-art method from Moving Pict.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25105;&#20204;&#22312;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#35265;&#35777;&#20102;&#28857;&#20113;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#20174;&#27785;&#28024;&#24335;&#23186;&#20307;&#12289;&#33258;&#21160;&#39550;&#39542;&#21040;&#21307;&#30103;&#20445;&#20581;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32479;&#19968;&#30340;&#31232;&#30095;&#34920;&#31034;&#23558;&#28857;&#20113;&#34920;&#31034;&#20026;&#20855;&#26377;&#19981;&#21516;&#20301;&#28145;&#24230;&#30340;&#21344;&#29992;&#29305;&#24449;&#21644;&#19977;&#20010;&#23646;&#24615;&#29305;&#24449;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#21033;&#29992;&#28857;&#20113;&#20869;&#30340;&#29305;&#24449;&#21644;&#28857;&#20869;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#20026;&#31639;&#26415;&#32534;&#30721;&#22120;&#26500;&#24314;&#20934;&#30830;&#30340;&#33258;&#22238;&#24402;&#19978;&#19979;&#25991;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#25439;&#28857;&#20113;&#20960;&#20309;&#21644;&#23646;&#24615;&#21387;&#32553;&#26041;&#27861;&#12290;&#19982;Moving Pict&#30340;&#26368;&#26032;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#26080;&#25439;&#21387;&#32553;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, we have witnessed the presence of point cloud data in many aspects of our life, from immersive media, autonomous driving to healthcare, although at the cost of a tremendous amount of data. In this paper, we present an efficient lossless point cloud compression method that uses sparse tensor-based deep neural networks to learn point cloud geometry and color probability distributions. Our method represents a point cloud with both occupancy feature and three attribute features at different bit depths in a unified sparse representation. This allows us to efficiently exploit feature-wise and point-wise dependencies within point clouds using a sparse tensor-based neural network and thus build an accurate auto-regressive context model for an arithmetic coder. To the best of our knowledge, this is the first learning-based lossless point cloud geometry and attribute compression approach. Compared with the-state-of-the-art lossless point cloud compression method from Moving Pict
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26080;&#25439;&#21487;&#25193;&#23637;&#28857;&#20113;&#23646;&#24615;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#26550;&#26500;&#25552;&#20379;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#32534;&#30721;&#27604;&#29305;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#20174;&#26080;&#25439;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#20013;&#36731;&#26494;&#25552;&#21462;&#36739;&#20302;&#36136;&#37327;&#30340;&#29256;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;G-PCC&#29256;&#26412;14&#30456;&#24403;&#65292;&#19988;&#32534;&#30721;&#26102;&#38388;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2303.06517</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#26080;&#25439;&#21487;&#25193;&#23637;&#28857;&#20113;&#23646;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Deep probabilistic model for lossless scalable point cloud attribute compression. (arXiv:2303.06517v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26080;&#25439;&#21487;&#25193;&#23637;&#28857;&#20113;&#23646;&#24615;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#26550;&#26500;&#25552;&#20379;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#32534;&#30721;&#27604;&#29305;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#20174;&#26080;&#25439;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#20013;&#36731;&#26494;&#25552;&#21462;&#36739;&#20302;&#36136;&#37327;&#30340;&#29256;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;G-PCC&#29256;&#26412;14&#30456;&#24403;&#65292;&#19988;&#32534;&#30721;&#26102;&#38388;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a deep probabilistic model for lossless scalable point cloud attribute compression, which utilizes a multiscale architecture to provide accurate context for attribute probability modeling and allows for easily extracting lower quality versions from the losslessly compressed bitstream. The method outperforms recently proposed methods and is on par with the latest G-PCC version 14, with substantially faster coding time.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#28857;&#20113;&#20960;&#20309;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#26159;&#20851;&#20110;&#23646;&#24615;&#21387;&#32553;&#65292;&#29305;&#21035;&#26159;&#26080;&#25439;&#21387;&#32553;&#30340;&#24037;&#20316;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#23610;&#24230;&#28857;&#20113;&#23646;&#24615;&#32534;&#30721;&#26041;&#27861;&#65288;MNeT&#65289;&#65292;&#35813;&#26041;&#27861;&#36880;&#27493;&#23558;&#23646;&#24615;&#25237;&#24433;&#21040;&#22810;&#23610;&#24230;&#28508;&#22312;&#31354;&#38388;&#19978;&#12290;&#22810;&#23610;&#24230;&#26550;&#26500;&#20026;&#23646;&#24615;&#27010;&#29575;&#24314;&#27169;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#36890;&#36807;&#21333;&#20010;&#32593;&#32476;&#39044;&#27979;&#26368;&#23567;&#21270;&#32534;&#30721;&#27604;&#29305;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#21487;&#25193;&#23637;&#32534;&#30721;&#65292;&#21487;&#20197;&#20174;&#26080;&#25439;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#20013;&#36731;&#26494;&#25552;&#21462;&#36739;&#20302;&#36136;&#37327;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;MVUB&#21644;MPEG&#30340;&#19968;&#32452;&#28857;&#20113;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;G-PCC&#29256;&#26412;14&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32534;&#30721;&#26102;&#38388;&#27604;G-PCC&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, several point cloud geometry compression methods that utilize advanced deep learning techniques have been proposed, but there are limited works on attribute compression, especially lossless compression. In this work, we build an end-to-end multiscale point cloud attribute coding method (MNeT) that progressively projects the attributes onto multiscale latent spaces. The multiscale architecture provides an accurate context for the attribute probability modeling and thus minimizes the coding bitrate with a single network prediction. Besides, our method allows scalable coding that lower quality versions can be easily extracted from the losslessly compressed bitstream. We validate our method on a set of point clouds from MVUB and MPEG and show that our method outperforms recently proposed methods and on par with the latest G-PCC version 14. Besides, our coding time is substantially faster than G-PCC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.06516</link><description>&lt;p&gt;
&#25171;&#24320;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20197;&#35745;&#31639;Shap&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#39640;&#25928;&#35745;&#31639;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36716;&#25442;&#20026;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#12290;&#25152;&#24471;&#21040;&#30340;&#30005;&#36335;&#34987;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#30456;&#27604;&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#32500;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#35268;&#27169;&#36739;&#22823;&#26102;&#20173;&#28982;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2303.06515</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multistage Stochastic Optimization via Kernels. (arXiv:2303.06515v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06515
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#32500;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#35268;&#27169;&#36739;&#22823;&#26102;&#20173;&#28982;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#12289;&#25968;&#25454;&#39537;&#21160;&#12289;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#19981;&#24433;&#21709;&#19981;&#30830;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#20915;&#31574;&#21464;&#37327;&#34920;&#31034;&#20026;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#20803;&#32032;&#65292;&#24182;&#25191;&#34892;&#20989;&#25968;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#32463;&#39564;&#27491;&#21017;&#21270;&#25439;&#22833;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#20989;&#25968;&#23376;&#31354;&#38388;&#25237;&#24433;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#26631;&#20934;&#26680;&#26041;&#27861;&#24341;&#20837;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#25968;&#25454;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#36741;&#21161;&#20449;&#24687;&#30340;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#20013;&#26159;&#28176;&#36817;&#26368;&#20248;&#30340;&#12290;&#22312;&#21508;&#31181;&#38543;&#26426;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#30340;&#35745;&#31639;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#32500;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#35268;&#27169;&#36739;&#22823;&#26102;&#20173;&#28982;&#21487;&#34892;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35745;&#31639;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#30340;&#26368;&#20248;&#25439;&#22833;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a non-parametric, data-driven, tractable approach for solving multistage stochastic optimization problems in which decisions do not affect the uncertainty. The proposed framework represents the decision variables as elements of a reproducing kernel Hilbert space and performs functional stochastic gradient descent to minimize the empirical regularized loss. By incorporating sparsification techniques based on function subspace projections we are able to overcome the computational complexity that standard kernel methods introduce as the data size increases. We prove that the proposed approach is asymptotically optimal for multistage stochastic optimization with side information. Across various computational experiments on stochastic inventory management problems, {our method performs well in multidimensional settings} and remains tractable when the data size is large. Lastly, by computing lower bounds for the optimal loss of the inventory control problem, we show that the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#29615;&#22659;&#20013;&#26816;&#27979;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#27979;&#35797;&#22235;&#31181;&#31639;&#27861;&#65292;&#20854;&#20013;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.06513</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#30340;DDoS&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Detection of DDoS Attacks in Software Defined Networking Using Machine Learning Models. (arXiv:2303.06513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#29615;&#22659;&#20013;&#26816;&#27979;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#27979;&#35797;&#22235;&#31181;&#31639;&#27861;&#65292;&#20854;&#20013;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effectiveness of using machine learning algorithms to detect distributed denial-of-service (DDoS) attacks in software-defined networking (SDN) environments, and tests four algorithms on the CICDDoS2019 dataset, with Random Forest performing the best.
&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#30340;&#27010;&#24565;&#20195;&#34920;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#25277;&#35937;&#23558;&#25511;&#21046;&#24179;&#38754;&#19982;&#25968;&#25454;&#24179;&#38754;&#20998;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#19982;&#20256;&#32479;&#32593;&#32476;&#30456;&#27604;&#26356;&#28789;&#27963;&#12289;&#21487;&#32534;&#31243;&#21644;&#21160;&#24577;&#30340;&#26550;&#26500;&#12290;&#25511;&#21046;&#24179;&#38754;&#21644;&#25968;&#25454;&#24179;&#38754;&#30340;&#20998;&#31163;&#23548;&#33268;&#20102;&#39640;&#24230;&#30340;&#32593;&#32476;&#24377;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#36825;&#22312;SDN&#29615;&#22659;&#20013;&#26500;&#25104;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#29615;&#22659;&#20013;&#26816;&#27979;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;CICDDoS2019&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#22235;&#31181;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#12289;&#20915;&#31574;&#26641;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;XGBoost&#65292;&#20854;&#20013;&#26102;&#38388;&#25139;&#29305;&#24449;&#34987;&#21024;&#38500;&#31561;&#12290;&#36890;&#36807;&#20934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#35780;&#20272;&#20102;&#24615;&#33021;&#65292;&#20854;&#20013;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of Software Defined Networking (SDN) represents a modern approach to networking that separates the control plane from the data plane through network abstraction, resulting in a flexible, programmable and dynamic architecture compared to traditional networks. The separation of control and data planes has led to a high degree of network resilience, but has also given rise to new security risks, including the threat of distributed denial-of-service (DDoS) attacks, which pose a new challenge in the SDN environment. In this paper, the effectiveness of using machine learning algorithms to detect distributed denial-of-service (DDoS) attacks in software-defined networking (SDN) environments is investigated. Four algorithms, including Random Forest, Decision Tree, Support Vector Machine, and XGBoost, were tested on the CICDDoS2019 dataset, with the timestamp feature dropped among others. Performance was assessed by measures of accuracy, recall, accuracy, and F1 score, with the Rando
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.06484</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#22635;&#34917;&#31070;&#32463;&#22349;&#22604;&#30340;&#27867;&#21270;&#21644;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap. (arXiv:2303.06484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a generalized neural collapse hypothesis that effectively subsumes the original neural collapse and decomposes it into two objectives: minimizing intra-class variability and maximizing inter-class separability. The authors use hyperspherical uniformity as a unified framework to quantify these objectives and propose a general objective, hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity.
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22349;&#22604;&#29616;&#35937;&#25551;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24213;&#23618;&#20960;&#20309;&#23545;&#31216;&#24615;&#65292;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#37117;&#25910;&#25947;&#20110;&#19968;&#20010;&#31561;&#35282;&#32039;&#26694;&#26550;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22343;&#26041;&#35823;&#24046;&#37117;&#21487;&#20197;&#23548;&#33268;&#31070;&#32463;&#22349;&#22604;&#12290;&#25105;&#20204;&#28040;&#38500;&#20102;&#31070;&#32463;&#22349;&#22604;&#23545;&#29305;&#24449;&#32500;&#24230;&#21644;&#31867;&#21035;&#25968;&#37327;&#30340;&#20851;&#38190;&#20551;&#35774;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#12290;&#21463;&#31070;&#32463;&#22349;&#22604;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30446;&#26631;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#65288;&#23427;&#25551;&#36848;&#20102;&#21333;&#20301;&#36229;&#29699;&#19978;&#22343;&#21248;&#24615;&#30340;&#31243;&#24230;&#65289;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both cross-entropy loss and mean square error can provably lead to NC. We remove NC's key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective -- hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20808;&#21069;&#36816;&#34892;&#20013;&#30340;&#35745;&#31639;&#26469;&#20943;&#23569;&#26410;&#26469;&#36816;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#65292;&#36890;&#36807;&#23558;&#26410;&#26469;&#36816;&#34892;&#19982;&#26469;&#33258;&#20808;&#21069;&#36816;&#34892;&#30340;KD&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;KD&#30340;&#24320;&#38144;&#38477;&#20302;&#20102;80-90&#65285;&#65292;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#65292;&#24182;&#22312;&#25972;&#20307;&#25104;&#26412;&#26041;&#38754;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.06480</link><description>&lt;p&gt;
&#39640;&#25928;&#35757;&#32451;&#24207;&#21015;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation for Efficient Sequences of Training Runs. (arXiv:2303.06480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20808;&#21069;&#36816;&#34892;&#20013;&#30340;&#35745;&#31639;&#26469;&#20943;&#23569;&#26410;&#26469;&#36816;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#65292;&#36890;&#36807;&#23558;&#26410;&#26469;&#36816;&#34892;&#19982;&#26469;&#33258;&#20808;&#21069;&#36816;&#34892;&#30340;KD&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;KD&#30340;&#24320;&#38144;&#38477;&#20302;&#20102;80-90&#65285;&#65292;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#65292;&#24182;&#22312;&#25972;&#20307;&#25104;&#26412;&#26041;&#38754;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies how to reduce the cost of future runs by utilizing the computation invested in previous runs using knowledge distillation (KD). Augmenting future runs with KD from previous runs dramatically reduces the time necessary to train these models, and the overhead of KD can be reduced by 80-90% with minimal effect on accuracy, resulting in vast pareto-improvements in overall cost.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22914;&#36229;&#21442;&#25968;&#25628;&#32034;&#25110;&#20351;&#29992;&#26032;&#25968;&#25454;&#36827;&#34892;&#25345;&#32493;&#37325;&#26032;&#35757;&#32451;&#65292;&#30456;&#20851;&#30340;&#35757;&#32451;&#36816;&#34892;&#20250;&#25353;&#39034;&#24207;&#25191;&#34892;&#22810;&#27425;&#12290;&#30446;&#21069;&#30340;&#20570;&#27861;&#26159;&#20174;&#22836;&#24320;&#22987;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#20808;&#21069;&#36816;&#34892;&#20013;&#30340;&#35745;&#31639;&#26469;&#20943;&#23569;&#26410;&#26469;&#36816;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#26410;&#26469;&#36816;&#34892;&#19982;&#26469;&#33258;&#20808;&#21069;&#36816;&#34892;&#30340;KD&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#21363;&#20351;&#32771;&#34385;&#21040;KD&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#25913;&#36827;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#23558;KD&#30340;&#24320;&#38144;&#38477;&#20302;&#20102;80-90&#65285;&#65292;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#65292;&#24182;&#22312;&#25972;&#20307;&#25104;&#26412;&#26041;&#38754;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;KD&#26159;&#20943;&#23569;&#23454;&#36341;&#20013;&#35757;&#32451;&#26368;&#32456;&#27169;&#22411;&#20043;&#21069;&#26114;&#36149;&#30340;&#20934;&#22791;&#24037;&#20316;&#25104;&#26412;&#30340;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many practical scenarios -- like hyperparameter search or continual retraining with new data -- related training runs are performed many times in sequence. Current practice is to train each of these models independently from scratch. We study the problem of exploiting the computation invested in previous runs to reduce the cost of future runs using knowledge distillation (KD). We find that augmenting future runs with KD from previous runs dramatically reduces the time necessary to train these models, even taking into account the overhead of KD. We improve on these results with two strategies that reduce the overhead of KD by 80-90% with minimal effect on accuracy and vast pareto-improvements in overall cost. We conclude that KD is a promising avenue for reducing the cost of the expensive preparatory work that precedes training final models in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06471</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#20195;&#30340;&#32959;&#30244;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review. (arXiv:2303.06471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review article analyzes the application of deep neural networks in multimodal data integration to improve the accuracy and reliability of cancer diagnosis and treatment.
&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#22312;&#19981;&#21516;&#23610;&#24230;&#12289;&#27169;&#24577;&#21644;&#20998;&#36776;&#29575;&#30340;&#33719;&#21462;&#25968;&#25454;&#20013;&#20855;&#26377;&#20851;&#31995;&#20449;&#24687;&#65292;&#20363;&#22914;&#25918;&#23556;&#23398;&#12289;&#30149;&#29702;&#23398;&#12289;&#22522;&#22240;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#21644;&#20020;&#24202;&#35760;&#24405;&#12290;&#25972;&#21512;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#21487;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#21487;&#33021;&#23384;&#22312;&#20154;&#31867;&#25110;&#29616;&#26377;&#25216;&#26415;&#24037;&#20855;&#26080;&#27861;&#35270;&#35273;&#19978;&#21306;&#20998;&#30340;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#21333;&#20010;&#23610;&#24230;&#30340;&#29983;&#29289;&#31995;&#32479;&#30340;&#37096;&#20998;&#25110;&#21333;&#19968;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#26410;&#28085;&#30422;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#23436;&#25972;&#20809;&#35889;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20419;&#36827;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#21644;&#25972;&#21512;&#30456;&#20851;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#22914;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21464;&#21387;&#22120;&#65292;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#27861;&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer has relational information residing at varying scales, modalities, and resolutions of the acquired data, such as radiology, pathology, genomics, proteomics, and clinical records. Integrating diverse data types can improve the accuracy and reliability of cancer diagnosis and treatment. There can be disease-related information that is too subtle for humans or existing technological tools to discern visually. Traditional methods typically focus on partial or unimodal information about biological systems at individual scales and fail to encapsulate the complete spectrum of the heterogeneous nature of data. Deep neural networks have facilitated the development of sophisticated multimodal data fusion approaches that can extract and integrate relevant information from multiple sources. Recent deep learning frameworks such as Graph Neural Networks (GNNs) and Transformers have shown remarkable success in multimodal learning. This review article provides an in-depth analysis of the state-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06470</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20998;&#23376;&#36136;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prefix-tree Decoding for Predicting Mass Spectra from Molecules. (arXiv:2303.06470v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms, and decoding the formula set using a prefix tree structure, atom-type by atom-type, overcoming the combinatorial possibilities for chemical subformulae.
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#24050;&#32463;&#23454;&#29616;&#20102;&#20020;&#24202;&#30456;&#20851;&#20195;&#35874;&#29289;&#30340;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#39044;&#27979;&#24037;&#20855;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21344;&#25454;&#20102;&#20004;&#20010;&#26497;&#31471;&#65292;&#35201;&#20040;&#36890;&#36807;&#36807;&#24230;&#21018;&#24615;&#30340;&#32422;&#26463;&#21644;&#36739;&#24046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#32452;&#21512;&#20998;&#23376;&#26469;&#36827;&#34892;&#25805;&#20316;&#65292;&#35201;&#20040;&#36890;&#36807;&#35299;&#30721;&#26377;&#25439;&#21644;&#38750;&#29289;&#29702;&#31163;&#25955;&#21270;&#30340;&#20809;&#35889;&#21521;&#37327;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#36825;&#20123;&#21270;&#23398;&#20844;&#24335;&#26412;&#36523;&#26159;&#21407;&#23376;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;&#22312;&#39318;&#20808;&#23545;&#36755;&#20837;&#20998;&#23376;&#22270;&#36827;&#34892;&#32534;&#30721;&#21518;&#65292;&#25105;&#20204;&#35299;&#30721;&#19968;&#32452;&#21270;&#23398;&#23376;&#20844;&#24335;&#65292;&#27599;&#20010;&#21270;&#23398;&#23376;&#20844;&#24335;&#25351;&#23450;&#36136;&#35889;&#20013;&#30340;&#19968;&#20010;&#39044;&#27979;&#23792;&#65292;&#20854;&#24378;&#24230;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#36890;&#36807;&#20351;&#29992;&#21069;&#32512;&#26641;&#32467;&#26500;&#65292;&#36880;&#20010;&#21407;&#23376;&#31867;&#22411;&#22320;&#35299;&#30721;&#20844;&#24335;&#38598;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational predictions of mass spectra from molecules have enabled the discovery of clinically relevant metabolites. However, such predictive tools are still limited as they occupy one of two extremes, either operating (a) by fragmenting molecules combinatorially with overly rigid constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and nonphysical discretized spectra vectors. In this work, we introduce a new intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms. After first encoding an input molecular graph, we decode a set of chemical subformulae, each of which specify a predicted peak in the mass spectra, the intensities of which are predicted by a second model. Our key insight is to overcome the combinatorial possibilities for chemical subformulae by decoding the formula set using a prefix tree structure, atom-type by atom-type, represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06455</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#22312;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep learning model based on Graph Neural Network (GNN) with Interaction Network (IN) for contextual embedding, which outperforms the recent DL benchmark on five public datasets and achieves competitive results compared to boosted-tree solutions in tabular data processing.
&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#34892;&#19994;&#37117;&#35797;&#22270;&#21033;&#29992;&#29616;&#26377;&#30340;&#22823;&#25968;&#25454;&#36827;&#34892;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#20197;&#25152;&#35859;&#30340;&#34920;&#26684;&#24418;&#24335;&#23384;&#22312;&#65292;&#20854;&#20013;&#27599;&#20010;&#35760;&#24405;&#30001;&#35768;&#22810;&#24322;&#26500;&#30340;&#36830;&#32493;&#21644;&#20998;&#31867;&#21015;&#32452;&#25104;&#65292;&#20063;&#31216;&#20026;&#29305;&#24449;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#19982;&#20154;&#31867;&#25216;&#33021;&#30456;&#20851;&#30340;&#39046;&#22495;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#20854;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26356;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#20132;&#20114;&#32593;&#32476;&#65288;IN&#65289;&#65292;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#20854;&#32467;&#26524;&#20248;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#22522;&#20110;&#20116;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
All industries are trying to leverage Artificial Intelligence (AI) based on their existing big data which is available in so called tabular form, where each record is composed of a number of heterogeneous continuous and categorical columns also known as features. Deep Learning (DL) has consituted a major breathrough for AI in fields related to human skills like natural language processing, but its applicability to tabular data has been more challenging. More classical Machine Learning (ML) models like tree-based ensemble ones usually perform better. In this manuscript a novel DL model that uses Graph Neural Network (GNN), more specifically Interaction Network (IN), for contextual embedding is introduced. Its results outperform those of the recently published survey with DL benchmark based on five public datasets, achieving also competitive results when compared to boosted-tree solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#29699;&#35270;&#39057;&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;DECOMPL&#65292;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#20998;&#25903;&#32452;&#25104;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#25552;&#21462;&#29305;&#24449;&#65292;&#32771;&#34385;&#21442;&#19982;&#32773;&#30340;&#24403;&#21069;&#37197;&#32622;&#65292;&#24182;&#20174;&#26694;&#22352;&#26631;&#20013;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#25490;&#29699;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#26041;&#26696;&#38477;&#20302;&#20102;&#27963;&#21160;&#20013;&#30340;&#22242;&#20307;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2303.06439</link><description>&lt;p&gt;
DECOMPL: &#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#27744;&#21270;&#30340;&#20998;&#35299;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;&#25490;&#29699;&#22270;&#20687;&#20013;&#35782;&#21035;&#22242;&#20307;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
DECOMPL: Decompositional Learning with Attention Pooling for Group Activity Recognition from a Single Volleyball Image. (arXiv:2303.06439v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#29699;&#35270;&#39057;&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;DECOMPL&#65292;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#20998;&#25903;&#32452;&#25104;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#25552;&#21462;&#29305;&#24449;&#65292;&#32771;&#34385;&#21442;&#19982;&#32773;&#30340;&#24403;&#21069;&#37197;&#32622;&#65292;&#24182;&#20174;&#26694;&#22352;&#26631;&#20013;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#25490;&#29699;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#26041;&#26696;&#38477;&#20302;&#20102;&#27963;&#21160;&#20013;&#30340;&#22242;&#20307;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel GAR technique for volleyball videos, DECOMPL, which consists of two complementary branches, using selective attention pooling to extract features, considering the current configuration of actors and extracting spatial information from box coordinates. The paper also reveals that the labeling scheme of the Volleyball dataset degrades the group concept in activities.
&lt;/p&gt;
&lt;p&gt;
&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#26088;&#22312;&#26816;&#27979;&#22330;&#26223;&#20013;&#22810;&#20010;&#21442;&#19982;&#32773;&#25191;&#34892;&#30340;&#27963;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#22522;&#20110;RGB&#12289;&#20809;&#27969;&#25110;&#20851;&#38190;&#28857;&#25968;&#25454;&#31867;&#22411;&#23545;&#26102;&#31354;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#20351;&#29992;&#26102;&#38388;&#24615;&#21644;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#20165;&#20351;&#29992;RGB&#25968;&#25454;&#32780;&#19981;&#32771;&#34385;&#26102;&#38388;&#24615;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#29699;&#35270;&#39057;&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;DECOMPL&#65292;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#20998;&#25903;&#32452;&#25104;&#12290;&#22312;&#35270;&#35273;&#20998;&#25903;&#20013;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#25552;&#21462;&#29305;&#24449;&#12290;&#22312;&#22352;&#26631;&#20998;&#25903;&#20013;&#65292;&#23427;&#32771;&#34385;&#21442;&#19982;&#32773;&#30340;&#24403;&#21069;&#37197;&#32622;&#65292;&#24182;&#20174;&#26694;&#22352;&#26631;&#20013;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25490;&#29699;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#26631;&#31614;&#26041;&#26696;&#38477;&#20302;&#20102;&#27963;&#21160;&#20013;&#30340;&#22242;&#20307;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group Activity Recognition (GAR) aims to detect the activity performed by multiple actors in a scene. Prior works model the spatio-temporal features based on the RGB, optical flow or keypoint data types. However, using both the temporality and these data types altogether increase the computational complexity significantly. Our hypothesis is that by only using the RGB data without temporality, the performance can be maintained with a negligible loss in accuracy. To that end, we propose a novel GAR technique for volleyball videos, DECOMPL, which consists of two complementary branches. In the visual branch, it extracts the features using attention pooling in a selective way. In the coordinate branch, it considers the current configuration of the actors and extracts the spatial information from the box coordinates. Moreover, we analyzed the Volleyball dataset that the recent literature is mostly based on, and realized that its labeling scheme degrades the group concept in the activities to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;OFDM&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36890;&#36807;&#21407;&#22411;&#38382;&#39064;&#35780;&#20272;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.06438</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#28304;&#20998;&#31163;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65306;&#20849;&#20449;&#36947;OFDM&#20449;&#21495;&#30340;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
On Neural Architectures for Deep Learning-based Source Separation of Co-Channel OFDM Signals. (arXiv:2303.06438v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;OFDM&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36890;&#36807;&#21407;&#22411;&#38382;&#39064;&#35780;&#20272;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the single-channel source separation problem involving OFDM signals and evaluates the efficacy of using audio-oriented neural architectures in separating co-channel OFDM waveforms. Critical domain-informed modifications to the network parameterization are proposed based on insights from OFDM structures.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36825;&#31181;&#20449;&#21495;&#22312;&#35768;&#22810;&#29616;&#20195;&#25968;&#23383;&#36890;&#20449;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#21333;&#22768;&#36947;&#28304;&#20998;&#31163;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#21162;&#21147;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#38899;&#39057;&#20449;&#21495;&#20998;&#31163;&#22120;&#65288;&#20316;&#20026;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#65289;&#12290;&#36890;&#36807;&#22522;&#20110;OFDM&#28304;&#27169;&#22411;&#30340;&#21407;&#22411;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#24182;&#36136;&#30097;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#22522;&#20110;&#36890;&#20449;&#27874;&#24418;&#30456;&#20851;&#29305;&#24449;&#20998;&#31163;&#20449;&#21495;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#37197;&#32622;&#20013;&#65292;&#21363;&#20351;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#23454;&#29616;&#23436;&#32654;&#20998;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#22522;&#20110;OFDM&#32467;&#26500;&#30340;&#27934;&#23519;&#65292;&#21487;&#20197;&#20849;&#21516;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the single-channel source separation problem involving orthogonal frequency-division multiplexing (OFDM) signals, which are ubiquitous in many modern-day digital communication systems. Related efforts have been pursued in monaural source separation, where state-of-the-art neural architectures have been adopted to train an end-to-end separator for audio signals (as 1-dimensional time series). In this work, through a prototype problem based on the OFDM source model, we assess -- and question -- the efficacy of using audio-oriented neural architectures in separating signals based on features pertinent to communication waveforms. Perhaps surprisingly, we demonstrate that in some configurations, where perfect separation is theoretically attainable, these audio-oriented neural architectures perform poorly in separating co-channel OFDM waveforms. Yet, we propose critical domain-informed modifications to the network parameterization, based on insights from OFDM structures, that can co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21019;&#24314;&#20102;&#21453;&#34394;&#20551;&#20449;&#24687;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#24110;&#21161;&#26222;&#36890;&#29992;&#25143;&#26377;&#25928;&#32416;&#27491;&#34394;&#20551;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.06433</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#34394;&#20551;&#20449;&#24687;&#21709;&#24212;&#29983;&#25104;&#65306;&#20197;COVID-19&#30123;&#33495;&#34394;&#20551;&#20449;&#24687;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-based Counter-Misinformation Response Generation: A Case Study of COVID-19 Vaccine Misinformation. (arXiv:2303.06433v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21019;&#24314;&#20102;&#21453;&#34394;&#20551;&#20449;&#24687;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#24110;&#21161;&#26222;&#36890;&#29992;&#25143;&#26377;&#25928;&#32416;&#27491;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study creates a counter-misinformation response generation model using reinforcement learning algorithm to empower ordinary users to effectively correct misinformation.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#23041;&#32961;&#30528;&#20844;&#20849;&#21355;&#29983;&#12289;&#27665;&#20027;&#21644;&#26356;&#24191;&#27867;&#30340;&#31038;&#20250;&#12290;&#26412;&#25991;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#21453;&#34394;&#20551;&#20449;&#24687;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#36171;&#20104;&#29992;&#25143;&#26377;&#25928;&#32416;&#27491;&#34394;&#20551;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#20004;&#20010;&#34394;&#20551;&#20449;&#24687;&#21644;&#21453;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26377;&#25928;&#30340;&#21453;&#34394;&#20551;&#20449;&#24687;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of online misinformation threatens public health, democracy, and the broader society. While professional fact-checkers form the first line of defense by fact-checking popular false claims, they do not engage directly in conversations with misinformation spreaders. On the other hand, non-expert ordinary users act as eyes-on-the-ground who proactively counter misinformation -- recent research has shown that 96% counter-misinformation responses are made by ordinary users. However, research also found that 2/3 times, these responses are rude and lack evidence. This work seeks to create a counter-misinformation response generation model to empower users to effectively correct misinformation. This objective is challenging due to the absence of datasets containing ground-truth of ideal counter-misinformation responses, and the lack of models that can generate responses backed by communication theories. In this work, we create two novel datasets of misinformation and counter-misinfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#27491;&#24120;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#23558;&#36755;&#20837;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#20174;&#28508;&#22312;&#21521;&#37327;&#20013;&#37325;&#26500;&#36755;&#20986;&#26679;&#26412;&#65292;&#26368;&#32456;&#23558;&#37325;&#26500;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#32534;&#30721;&#26469;&#20248;&#21270;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.06431</link><description>&lt;p&gt;
&#24102;&#26377;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection with Ensemble of Encoder and Decoder. (arXiv:2303.06431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#27491;&#24120;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#23558;&#36755;&#20837;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#20174;&#28508;&#22312;&#21521;&#37327;&#20013;&#37325;&#26500;&#36755;&#20986;&#26679;&#26412;&#65292;&#26368;&#32456;&#23558;&#37325;&#26500;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#32534;&#30721;&#26469;&#20248;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel anomaly detection method by modeling the data distribution of normal samples via multiple encoders and decoders, mapping input samples into a latent space, reconstructing output samples from latent vectors, and mapping reconstructed samples to latent representations. Parameters are optimized during the training phase by minimizing the reconstruction loss and encoding.
&lt;/p&gt;
&lt;p&gt;
&#40657;&#23458;&#21644;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#21487;&#33021;&#20250;&#23041;&#32961;&#30005;&#32593;&#30340;&#26085;&#24120;&#36816;&#33829;&#24182;&#36896;&#25104;&#37325;&#22823;&#32463;&#27982;&#25439;&#22833;&#12290;&#30005;&#32593;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#26816;&#27979;&#21644;&#21306;&#20998;&#30001;&#38024;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#32593;&#32476;&#25915;&#20987;&#24341;&#36215;&#30340;&#24322;&#24120;&#65292;&#36825;&#23545;&#20110;&#20445;&#25345;&#30005;&#32593;&#30340;&#27491;&#24120;&#36816;&#34892;&#21644;&#39640;&#25928;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24212;&#29992;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#20363;&#22914;&#32479;&#35745;&#26041;&#27861;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#38656;&#35201;&#23545;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#27491;&#24120;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#36755;&#20837;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#20174;&#28508;&#22312;&#21521;&#37327;&#20013;&#37325;&#26500;&#36755;&#20986;&#26679;&#26412;&#12290;&#39069;&#22806;&#30340;&#32534;&#30721;&#22120;&#26368;&#32456;&#23558;&#37325;&#26500;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#32534;&#30721;&#26469;&#20248;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hacking and false data injection from adversaries can threaten power grids' everyday operations and cause significant economic loss. Anomaly detection in power grids aims to detect and discriminate anomalies caused by cyber attacks against the power system, which is essential for keeping power grids working correctly and efficiently. Different methods have been applied for anomaly detection, such as statistical methods and machine learning-based methods. Usually, machine learning-based methods need to model the normal data distribution. In this work, we propose a novel anomaly detection method by modeling the data distribution of normal samples via multiple encoders and decoders. Specifically, the proposed method maps input samples into a latent space and then reconstructs output samples from latent vectors. The extra encoder finally maps reconstructed samples to latent representations. During the training phase, we optimize parameters by minimizing the reconstruction loss and encoding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#24182;&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20854;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.06423</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#32593;&#32476;&#65292;&#20197;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;40&#19975;&#20221;&#21307;&#30103;&#35760;&#24405;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable causal networks from very large datasets, application to 400,000 medical records of breast cancer patients. (arXiv:2303.06423v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#24182;&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20854;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a more reliable and scalable causal discovery method (iMIIC) and showcases its unique capabilities on healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. Over 90% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity.
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#22240;&#26524;&#25928;&#24212;&#26159;&#31185;&#23398;&#30740;&#31350;&#30340;&#26680;&#24515;&#65292;&#20294;&#24403;&#21482;&#26377;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#36825;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22240;&#26524;&#32593;&#32476;&#38590;&#20197;&#23398;&#20064;&#21644;&#35299;&#37322;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#22522;&#20110;&#19968;&#33324;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21407;&#21017;&#65292;&#23427;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25512;&#26029;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#21306;&#20998;&#20102;&#30495;&#27491;&#30340;&#21407;&#22240;&#21644;&#20551;&#23450;&#30340;&#21644;&#28508;&#22312;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;iMIIC&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21512;&#25104;&#21644;&#29616;&#23454;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;iMIIC&#30340;&#29420;&#29305;&#33021;&#21147;&#24320;&#36767;&#20102;&#21457;&#29616;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#32593;&#32476;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal effects is at the core of scientific investigation but remains challenging when only observational data is available. In practice, causal networks are difficult to learn and interpret, and limited to relatively small datasets. We report a more reliable and scalable causal discovery method (iMIIC), based on a general mutual information supremum principle, which greatly improves the precision of inferred causal relations while distinguishing genuine causes from putative and latent causal effects. We showcase iMIIC on synthetic and real-life healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. More than 90\% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity. iMIIC's unique capabilities open up new avenues to discover reliable and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#25351;&#23450;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06419</link><description>&lt;p&gt;
&#20174;&#35299;&#37322;&#20013;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning from Explanations. (arXiv:2303.06419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#25351;&#23450;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new machine learning approach, recasting machine learning from explanations (MLX) as an adversarial robustness problem, which specifies a lower dimensional manifold from which perturbations can be drawn based on human-provided annotations, and shows improved performance over prior MLX methods on both synthetic and real-world benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20154;&#31867;&#25552;&#20379;&#30340;&#26377;&#20851;&#27599;&#20010;&#36755;&#20837;&#30340;&#30456;&#20851;&#29305;&#24449;&#30340;&#27880;&#37322;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#39044;&#27979;&#30340;&#21407;&#22240;&#27491;&#30830;&#12290;&#29616;&#26377;&#30340;MLX&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#38656;&#35201;&#24378;&#22823;&#30340;&#21442;&#25968;&#27491;&#21017;&#21270;&#26469;&#23545;&#40784;&#27169;&#22411;&#21644;&#20154;&#31867;&#35299;&#37322;&#65292;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;MLX&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#20154;&#31867;&#35299;&#37322;&#25351;&#23450;&#20102;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#21487;&#20197;&#20174;&#20013;&#32472;&#21046;&#25200;&#21160;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20943;&#36731;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20808;&#21069;MLX&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#24615;&#19982;&#26089;&#26399;&#30340;MLX&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning from explanations (MLX) is an approach to learning that uses human-provided annotations of relevant features for each input to ensure that model predictions are right for the right reasons. Existing MLX approaches rely heavily on a specific model interpretation approach and require strong parameter regularization to align model and human explanations, leading to sub-optimal performance. We recast MLX as an adversarial robustness problem, where human explanations specify a lower dimensional manifold from which perturbations can be drawn, and show both theoretically and empirically how this approach alleviates the need for strong parameter regularization. We consider various approaches to achieving robustness, leading to improved performance over prior MLX methods. Finally, we combine robustness with an earlier MLX method, yielding state-of-the-art results on both synthetic and real-world benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;ADNI&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06410</link><description>&lt;p&gt;
Brain Diffuser&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#33041;&#22270;&#20687;&#21040;&#33041;&#32593;&#32476;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline. (arXiv:2303.06410v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;ADNI&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
&lt;/p&gt;
&lt;p&gt;
&#33041;&#32593;&#32476;&#20998;&#26512;&#23545;&#20110;&#35786;&#26029;&#21644;&#24178;&#39044;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#32791;&#26102;&#21644;&#20027;&#35266;&#30340;&#24037;&#20855;&#21253;&#12290;&#21482;&#26377;&#23569;&#25968;&#24037;&#20855;&#21487;&#20197;&#20174;&#33041;&#25193;&#25955;&#24352;&#37327;&#22270;&#20687;&#65288;DTI&#65289;&#20013;&#33719;&#21462;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#19982;&#29616;&#26377;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;Brain Diffuser&#36890;&#36807;&#20998;&#26512;&#21463;&#35797;&#32773;&#20043;&#38388;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#30340;&#24046;&#24322;&#65292;&#21033;&#29992;&#26356;&#22810;&#30340;&#32467;&#26500;&#36830;&#25509;&#29305;&#24449;&#21644;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#65288;ADNI&#65289;&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain network analysis is essential for diagnosing and intervention for Alzheimer's disease (AD). However, previous research relied primarily on specific time-consuming and subjective toolkits. Only few tools can obtain the structural brain networks from brain diffusion tensor images (DTI). In this paper, we propose a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. Compared to existing toolkits, Brain Diffuser exploits more structural connectivity features and disease-related information by analyzing disparities in structural brain networks across subjects. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;&#65292;&#20197;&#25552;&#39640;&#30315;&#30187;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06407</link><description>&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Automatic Detection of Signalling Behaviour from Assistance Dogs as they Forecast the Onset of Epileptic Seizures in Humans. (arXiv:2303.06407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;&#65292;&#20197;&#25552;&#39640;&#30315;&#30187;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores how to automatically detect signalling behaviour from assistance dogs as they forecast the onset of epileptic seizures in humans, to improve the quality of life for epilepsy patients.
&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19990;&#30028;&#19978;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20043;&#19968;&#65292;&#24433;&#21709;&#30528;&#25968;&#30334;&#19975;&#20154;&#12290;&#30315;&#30187;&#21457;&#20316;&#36890;&#24120;&#26159;&#30001;&#20110;&#20154;&#33041;&#20013;&#30340;&#38750;&#21327;&#35843;&#30005;&#25918;&#30005;&#24341;&#36215;&#30340;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#65292;&#21253;&#25324;&#20498;&#22320;&#21644;&#22833;&#21435;&#24847;&#35782;&#12290;&#22914;&#26524;&#33021;&#22815;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#24320;&#22987;&#65292;&#37027;&#20040;&#21487;&#20197;&#23558;&#21463;&#35797;&#32773;&#32622;&#20110;&#23433;&#20840;&#30340;&#29615;&#22659;&#25110;&#20301;&#32622;&#65292;&#20197;&#26368;&#23567;&#21270;&#30001;&#20110;&#20498;&#22320;&#32780;&#23548;&#33268;&#30340;&#33258;&#25105;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#22312;&#26085;&#24120;&#30340;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23456;&#29289;&#29399;&#26377;&#33021;&#21147;&#36890;&#36807;&#21957;&#25506;&#21463;&#35797;&#32773;&#22312;&#30315;&#30187;&#21457;&#20316;&#21069;&#30382;&#32932;&#25955;&#21457;&#30340;&#29305;&#24449;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#26469;&#26816;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#24320;&#22987;&#65292;&#26377;&#20123;&#36741;&#21161;&#29356;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#21521;&#20854;&#20027;&#20154;/&#35757;&#32451;&#21592;&#21457;&#20986;&#20449;&#21495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#20449;&#21495;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy or the occurrence of epileptic seizures, is one of the world's most well-known neurological disorders affecting millions of people. Seizures mostly occur due to non-coordinated electrical discharges in the human brain and may cause damage, including collapse and loss of consciousness. If the onset of a seizure can be forecast then the subject can be placed into a safe environment or position so that self-injury as a result of a collapse can be minimised. However there are no definitive methods to predict seizures in an everyday, uncontrolled environment. Previous studies have shown that pet dogs have the ability to detect the onset of an epileptic seizure by scenting the characteristic volatile organic compounds exuded through the skin by a subject prior a seizure occurring and there are cases where assistance dogs, trained to scent the onset of a seizure, can signal this to their owner/trainer. In this work we identify how we can automatically detect the signalling behaviours
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#36951;&#25022;&#31639;&#27861;&#65292;&#29992;&#20110;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;$c_\alpha$-&#36817;&#20284;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#20854;&#20013;&#36817;&#20284;&#22240;&#23376;$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$&#65292;&#23545;&#20110;$0\leq \alpha &lt; 1$&#12290;</title><link>http://arxiv.org/abs/2303.06396</link><description>&lt;p&gt;
&#26080;&#36951;&#25022;&#31639;&#27861;&#29992;&#20110;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
No-regret Algorithms for Fair Resource Allocation. (arXiv:2303.06396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#36951;&#25022;&#31639;&#27861;&#65292;&#29992;&#20110;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;$c_\alpha$-&#36817;&#20284;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#20854;&#20013;&#36817;&#20284;&#22240;&#23376;$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$&#65292;&#23545;&#20110;$0\leq \alpha &lt; 1$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a no-regret algorithm for fair resource allocation, which achieves $c_\alpha$-approximate sublinear regret with the approximation factor $c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445,$ for $0\leq \alpha &lt; 1$.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#26080;&#36951;&#25022;&#35774;&#32622;&#19979;&#38024;&#23545;&#26080;&#38480;&#21046;&#30340;&#23545;&#25163;&#12290;&#30446;&#26631;&#26159;&#20197;&#22312;&#32447;&#26041;&#24335;&#20844;&#24179;&#22320;&#20998;&#37197;&#22810;&#20010;&#20195;&#29702;&#30340;&#36164;&#28304;&#65292;&#20351;&#24471;&#26368;&#20248;&#38745;&#24577;&#39044;&#30693;&#20998;&#37197;&#21644;&#22312;&#32447;&#31574;&#30053;&#30340;&#20195;&#29702;&#30340;&#32858;&#21512;&#945;-&#20844;&#24179;&#25928;&#29992;&#20043;&#24046;&#38543;&#26102;&#38388;&#22686;&#38271;&#30340;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;&#30001;&#20110;&#945;-&#20844;&#24179;&#24615;&#20989;&#25968;&#30340;&#38750;&#21152;&#24615;&#29305;&#24615;&#65292;&#35813;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#38382;&#39064;&#19981;&#23384;&#22312;&#20855;&#26377;&#27425;&#32447;&#24615;&#26631;&#20934;&#36951;&#25022;&#30340;&#22312;&#32447;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#31216;&#20026;&#22312;&#32447;&#27604;&#20363;&#20844;&#24179;&#65288;OPF&#65289;&#65292;&#35813;&#31574;&#30053;&#23454;&#29616;&#20102;$c_\alpha$-&#36817;&#20284;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#20854;&#20013;&#36817;&#20284;&#22240;&#23376;$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$&#65292;&#23545;&#20110;$0\leq \alpha &lt; 1$&#12290;&#35813;&#38382;&#39064;&#30340;$c_\alpha$-&#36951;&#25022;&#19978;&#30028;&#23637;&#29616;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#30456;&#21464;&#29616;&#35937;&#12290;&#36951;&#25022;&#19978;&#30028;&#20174;&#19968;&#20010;&#24130;&#20989;&#25968;&#21464;&#20026;&#19968;&#20010;&#23545;&#25968;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a fair resource allocation problem in the no-regret setting against an unrestricted adversary. The objective is to allocate resources equitably among several agents in an online fashion so that the difference of the aggregate $\alpha$-fair utilities of the agents between an optimal static clairvoyant allocation and that of the online policy grows sub-linearly with time. The problem is challenging due to the non-additive nature of the $\alpha$-fairness function. Previously, it was shown that no online policy can exist for this problem with a sublinear standard regret. In this paper, we propose an efficient online resource allocation policy, called Online Proportional Fair (OPF), that achieves $c_\alpha$-approximate sublinear regret with the approximation factor $c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445,$ for $0\leq \alpha &lt; 1$. The upper bound to the $c_\alpha$-regret for this problem exhibits a surprising phase transition phenomenon. The regret bound changes from a power
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#21069;&#27839;&#26041;&#27861;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#12290;</title><link>http://arxiv.org/abs/2303.06394</link><description>&lt;p&gt;
&#19968;&#31181;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
A Novel Method Combines Moving Fronts, Data Decomposition and Deep Learning to Forecast Intricate Time Series. (arXiv:2303.06394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#21069;&#27839;&#26041;&#27861;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel method that combines moving fronts, data decomposition, and deep learning to forecast intricate time series. The method decomposes the time series into simpler constituent series using empirical wavelet transform and prevents data leakage using the moving front method.
&lt;/p&gt;
&lt;p&gt;
&#39640;&#21464;&#24322;&#24615;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29978;&#33267;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20063;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34987;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#23427;&#20204;&#30340;&#24635;&#21644;&#31561;&#20110;&#21407;&#22987;&#24207;&#21015;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#20256;&#32479;&#30340;&#19968;&#27425;&#20998;&#35299;&#25216;&#26415;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31227;&#21160;&#21069;&#27839;&#65288;MF&#65289;&#26041;&#27861;&#26469;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#65292;&#20351;&#20998;&#35299;&#21518;&#30340;&#24207;&#21015;&#21487;&#20197;&#20687;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#19968;&#26679;&#22788;&#29702;&#12290;&#21360;&#24230;&#22799;&#23395;&#23395;&#39118;&#38477;&#38632;&#65288;ISMR&#65289;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#23545;DNN&#26500;&#25104;&#25361;&#25112;&#65292;&#22240;&#27492;&#34987;&#36873;&#20026;&#31034;&#20363;&#12290;&#20174;&#20247;&#22810;&#21487;&#29992;&#30340;&#20449;&#21495;&#22788;&#29702;&#24037;&#20855;&#20013;&#65292;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#65288;EWT&#65289;&#34987;&#36873;&#25321;&#29992;&#20110;&#23558;ISMR&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#22240;&#20026;&#23427;&#34987;&#21457;&#29616;&#27604;&#20854;&#20182;&#27969;&#34892;&#30340;&#31639;&#27861;&#65292;&#22914;&#33258;&#36866;&#24212;&#22122;&#22768;&#23436;&#20840;&#38598;&#21512;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#65288;CEEMDAN&#65289;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
A univariate time series with high variability can pose a challenge even to Deep Neural Network (DNN). To overcome this, a univariate time series is decomposed into simpler constituent series, whose sum equals the original series. As demonstrated in this article, the conventional one-time decomposition technique suffers from a leak of information from the future, referred to as a data leak. In this work, a novel Moving Front (MF) method is proposed to prevent data leakage, so that the decomposed series can be treated like other time series. Indian Summer Monsoon Rainfall (ISMR) is a very complex time series, which poses a challenge to DNN and is therefore selected as an example. From the many signal processing tools available, Empirical Wavelet Transform (EWT) was chosen for decomposing the ISMR into simpler constituent series, as it was found to be more effective than the other popular algorithm, Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN). The propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06389</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#31163;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Off-Policy Learning. (arXiv:2303.06389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, which explicitly models the uncertainty in the estimated logging policy and demonstrates advantageous sample efficiency against an extensive list of state-of-the-art baselines on synthetic and three real-world recommendation datasets.
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23398;&#20064;&#26159;&#25351;&#20165;&#36890;&#36807;&#35760;&#24405;&#30340;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#37325;&#35201;&#24615;&#65292;&#20363;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#12290;&#34429;&#28982;&#29983;&#25104;&#35760;&#24405;&#25968;&#25454;&#30340;&#30495;&#23454;&#35760;&#24405;&#31574;&#30053;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#20197;&#21069;&#30340;&#24037;&#20316;&#20165;&#22312;&#31163;&#32447;&#23398;&#20064;&#20013;&#37319;&#29992;&#20854;&#20272;&#35745;&#20540;&#65292;&#24573;&#30053;&#20102;&#30001;&#20110;&#36825;&#31181;&#20272;&#35745;&#22120;&#23548;&#33268;&#30340;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#23567;&#19988;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#35760;&#24405;&#27010;&#29575;&#30340;&#26679;&#26412;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#26469;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#12290;&#22312;&#21512;&#25104;&#21644;&#19977;&#20010;&#30495;&#23454;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;UIPS&#20272;&#35745;&#22120;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines, recommender systems, and etc. While the ground-truth logging policy, which generates the logged data, is usually unknown, previous work simply takes its estimated value in off-policy learning, ignoring both high bias and high variance resulted from such an estimator, especially on samples with small and inaccurately estimated logging probabilities. In this work, we explicitly model the uncertainty in the estimated logging policy and propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning. Experiment results on synthetic and three real-world recommendation datasets demonstrate the advantageous sample efficiency of the proposed UIPS estimator against an extensive list of state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20020;&#24202;&#33041;&#30005;&#22270;&#20998;&#31867;&#20013;&#31383;&#21475;&#26631;&#31614;&#21487;&#33021;&#35823;&#23548;&#30340;&#38382;&#39064;&#65306;&#22686;&#21152;&#31383;&#21475;&#38271;&#24230;&#21644;&#24341;&#20837;&#31532;&#20108;&#38454;&#27573;&#27169;&#22411;&#26469;&#20210;&#35009;&#35760;&#24405;&#20869;&#30340;&#31383;&#21475;&#29305;&#23450;&#39044;&#27979;&#12290;&#22312;Temple&#22823;&#23398;&#21307;&#38498;&#24322;&#24120;&#33041;&#30005;&#22270;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#26368;&#20808;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20174;89.8&#65285;&#26174;&#30528;&#25552;&#39640;&#21040;93.3&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.06386</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20020;&#24202;&#33041;&#30005;&#22270;&#20998;&#31867;&#20013;&#30340;&#33539;&#22260;&#21644;&#20210;&#35009;
&lt;/p&gt;
&lt;p&gt;
Scope and Arbitration in Machine Learning Clinical EEG Classification. (arXiv:2303.06386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20020;&#24202;&#33041;&#30005;&#22270;&#20998;&#31867;&#20013;&#31383;&#21475;&#26631;&#31614;&#21487;&#33021;&#35823;&#23548;&#30340;&#38382;&#39064;&#65306;&#22686;&#21152;&#31383;&#21475;&#38271;&#24230;&#21644;&#24341;&#20837;&#31532;&#20108;&#38454;&#27573;&#27169;&#22411;&#26469;&#20210;&#35009;&#35760;&#24405;&#20869;&#30340;&#31383;&#21475;&#29305;&#23450;&#39044;&#27979;&#12290;&#22312;Temple&#22823;&#23398;&#21307;&#38498;&#24322;&#24120;&#33041;&#30005;&#22270;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#26368;&#20808;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20174;89.8&#65285;&#26174;&#30528;&#25552;&#39640;&#21040;93.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two methods to address the problem of potentially misleading window labels in machine learning clinical EEG classification: increasing window length and introducing a second-stage model to arbitrate between window-specific predictions within a recording. Evaluating these methods on the Temple University Hospital Abnormal EEG Corpus, the state-of-the-art average accuracy was significantly improved from 89.8% to 93.3%.
&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#33041;&#30005;&#22270;&#35299;&#35835;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26159;&#23558;&#35760;&#24405;&#25110;&#20250;&#35805;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20026;&#20102;&#23454;&#38469;&#25805;&#20316;&#30340;&#21407;&#22240;&#65292;&#36890;&#24120;&#23558;&#35760;&#24405;&#20998;&#25104;&#36739;&#30701;&#30340;&#31383;&#21475;&#65292;&#24182;&#19988;&#36825;&#20123;&#31383;&#21475;&#32487;&#25215;&#20854;&#29238;&#35760;&#24405;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#20551;&#35774;&#20197;&#36825;&#31181;&#26041;&#24335;&#27966;&#29983;&#30340;&#31383;&#21475;&#26631;&#31614;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#20363;&#22914;&#65292;&#27809;&#26377;&#26126;&#26174;&#24322;&#24120;&#30340;&#31383;&#21475;&#21487;&#33021;&#20250;&#34987;&#26631;&#35760;&#20026;&#8220;&#24322;&#24120;&#8221;&#65292;&#20174;&#32780;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#24182;&#38477;&#20302;&#24615;&#33021;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#21487;&#20998;&#31163;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65306;&#22686;&#21152;&#31383;&#21475;&#38271;&#24230;&#21644;&#24341;&#20837;&#31532;&#20108;&#38454;&#27573;&#27169;&#22411;&#26469;&#20210;&#35009;&#35760;&#24405;&#20869;&#30340;&#31383;&#21475;&#29305;&#23450;&#39044;&#27979;&#12290;&#22312;Temple&#22823;&#23398;&#21307;&#38498;&#24322;&#24120;&#33041;&#30005;&#22270;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#26368;&#20808;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20174;89.8&#65285;&#26174;&#30528;&#25552;&#39640;&#21040;93.3&#65285;&#12290;&#36825;&#20010;&#32467;&#26524;&#25361;&#25112;&#20102;&#20808;&#21069;&#23545;&#35813;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#38480;&#30340;&#20272;&#35745;&#65292;&#20195;&#34920;&#20102;&#36808;&#21521;&#26356;&#22909;&#24615;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key task in clinical EEG interpretation is to classify a recording or session as normal or abnormal. In machine learning approaches to this task, recordings are typically divided into shorter windows for practical reasons, and these windows inherit the label of their parent recording. We hypothesised that window labels derived in this manner can be misleading for example, windows without evident abnormalities can be labelled `abnormal' disrupting the learning process and degrading performance. We explored two separable approaches to mitigate this problem: increasing the window length and introducing a second-stage model to arbitrate between the window-specific predictions within a recording. Evaluating these methods on the Temple University Hospital Abnormal EEG Corpus, we significantly improved state-of-the-art average accuracy from 89.8 percent to 93.3 percent. This result defies previous estimates of the upper limit for performance on this dataset and represents a major step towar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#30340;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#65292;&#20197;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#29031;&#26126;&#21151;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#26368;&#23567;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23384;&#22312;&#20449;&#36947;&#20272;&#35745;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20135;&#29983;&#36739;&#23567;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06381</link><description>&lt;p&gt;
&#23398;&#20064;&#39044;&#32534;&#30721;&#29992;&#20110;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning to Precode for Integrated Sensing and Communications Systems. (arXiv:2303.06381v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#30340;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#65292;&#20197;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#29031;&#26126;&#21151;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#26368;&#23567;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23384;&#22312;&#20449;&#36947;&#20272;&#35745;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20135;&#29983;&#36739;&#23567;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an unsupervised learning neural model to design transmit precoders for integrated sensing and communication (ISAC) systems to maximize the worst-case target illumination power while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for all the users. The proposed method outperforms traditional optimization-based methods in presence of channel estimation errors while incurring lesser computational complexity and generalizing well across different channel conditions that were not shown during training.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#30340;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#65292;&#20197;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#29031;&#26126;&#21151;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#26368;&#23567;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#12290;&#20174;&#19978;&#34892;&#23548;&#39057;&#21644;&#22238;&#27874;&#20013;&#23398;&#20064;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#30340;&#38382;&#39064;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#21442;&#25968;&#21270;&#20989;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#20010;&#20989;&#25968;&#12290;&#20026;&#20102;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#32435;&#20837;SINR&#21644;&#21151;&#29575;&#32422;&#26463;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#20449;&#36947;&#20272;&#35745;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20135;&#29983;&#36739;&#23567;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#20123;&#26465;&#20214;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#26174;&#31034;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an unsupervised learning neural model to design transmit precoders for integrated sensing and communication (ISAC) systems to maximize the worst-case target illumination power while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for all the users. The problem of learning transmit precoders from uplink pilots and echoes can be viewed as a parameterized function estimation problem and we propose to learn this function using a neural network model. To learn the neural network parameters, we develop a novel loss function based on the first-order optimality conditions to incorporate the SINR and power constraints. Through numerical simulations, we demonstrate that the proposed method outperforms traditional optimization-based methods in presence of channel estimation errors while incurring lesser computational complexity and generalizing well across different channel conditions that were not shown during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22810;&#20013;&#24515;&#29615;&#22659;&#20013;&#23545;&#22522;&#20110;EEG&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#24615;&#21035;&#23376;&#32452;&#32676;&#30340;&#26816;&#27979;&#33021;&#21147;&#20998;&#26512;&#65292;&#21457;&#29616;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;PD&#26816;&#27979;&#33021;&#21147;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06376</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;EEG&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#22810;&#20013;&#24515;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing gender fairness in EEG-based machine learning detection of Parkinson's disease: A multi-center study. (arXiv:2303.06376v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22810;&#20013;&#24515;&#29615;&#22659;&#20013;&#23545;&#22522;&#20110;EEG&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#24615;&#21035;&#23376;&#32452;&#32676;&#30340;&#26816;&#27979;&#33021;&#21147;&#20998;&#26512;&#65292;&#21457;&#29616;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;PD&#26816;&#27979;&#33021;&#21147;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study analyzed the detection ability of gender sub-groups in a multi-center setting of a previously developed machine learning algorithm based on EEG, finding significant differences in Parkinson's disease detection ability between males and females.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#38745;&#24687;&#24577;&#33041;&#30005;&#22270;&#65288;rs-EEG&#65289;&#30340;&#33258;&#21160;&#24037;&#20855;&#22312;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26816;&#27979;&#20013;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#36890;&#36807;&#20844;&#24179;&#24615;&#21644;&#20559;&#24046;&#20998;&#26512;&#35780;&#20272;&#21487;&#33021;&#21152;&#21095;&#20581;&#24247;&#24046;&#24322;&#30340;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#21463;&#20445;&#25252;&#30340;&#23646;&#24615;&#65292;&#22914;&#24615;&#21035;&#65292;&#22312;PD&#35786;&#26029;&#24320;&#21457;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#19981;&#21516;&#24615;&#21035;&#30340;&#23376;&#32452;&#32676;&#20307;&#30340;&#20998;&#26512;&#24456;&#23569;&#22312;ML&#27169;&#22411;&#30340;&#24320;&#21457;&#25110;PD&#26816;&#27979;&#30340;&#24615;&#33021;&#35780;&#20272;&#20013;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#38745;&#24687;&#24577;&#33041;&#30005;&#22270;&#21151;&#29575;&#35889;&#23494;&#24230;&#65288;PSD&#65289;&#29305;&#24449;&#30340;&#20808;&#21069;&#24320;&#21457;&#30340;ML&#31639;&#27861;&#22312;&#22810;&#20013;&#24515;&#29615;&#22659;&#20013;&#30340;&#24615;&#21035;&#23376;&#32452;&#32676;&#30340;&#26816;&#27979;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27979;&#35797;&#26102;&#38388;&#65288;80.5&#65285;&#23545;63.7&#65285;&#30340;&#20934;&#30830;&#24615;&#65289;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;PD&#26816;&#27979;&#33021;&#21147;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#24182;&#19988;&#19968;&#32452;&#39030;&#37096;&#21644;&#21069;&#39069;&#33041;&#30005;&#22270;&#36890;&#36947;&#21644;&#39057;&#29575;&#23384;&#22312;&#26174;&#30528;&#26356;&#39640;&#30340;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of automatic tools based on machine learning (ML) and resting-state electroencephalography (rs-EEG) for Parkinson's disease (PD) detection keeps growing, the assessment of possible exacerbation of health disparities by means of fairness and bias analysis becomes more relevant. Protected attributes, such as gender, play an important role in PD diagnosis development. However, analysis of sub-group populations stemming from different genders is seldom taken into consideration in ML models' development or the performance assessment for PD detection. In this work, we perform a systematic analysis of the detection ability for gender sub-groups in a multi-center setting of a previously developed ML algorithm based on power spectral density (PSD) features of rs-EEG. We find significant differences in the PD detection ability for males and females at testing time (80.5% vs. 63.7% accuracy) and significantly higher activity for a set of parietal and frontal EEG channels and frequen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;</title><link>http://arxiv.org/abs/2303.06365</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#26816;&#26597;&#23618;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Time Series via Virtual Inspection Layers. (arXiv:2303.06365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a virtual inspection layer that transforms time series into an interpretable representation and allows for relevance attributions to be propagated to this representation via local XAI methods. The applicability of a family of XAI methods is extended to domains where the input is only interpretable after a transformation. The usefulness of DFT-LRP is demonstrated in various time series classification settings, such as audio and electronic health records.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#12290;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#30001;&#20110;&#36755;&#20837;&#36890;&#24120;&#19981;&#21487;&#35299;&#37322;&#65292;&#22240;&#27492;&#21482;&#26377;&#26377;&#38480;&#30340;XAI&#30740;&#31350;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#65288;&#22914;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#65289;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#35821;&#38899;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#36825;&#22312;&#26102;&#38388;&#24207;&#21015;&#35299;&#37322;&#21644;LRP&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;DFT-LRP&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;DFT-LRP&#26469;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of eXplainable Artificial Intelligence (XAI) has greatly advanced in recent years, but progress has mainly been made in computer vision and natural language processing. For time series, where the input is often not interpretable, only limited research on XAI is available. In this work, we put forward a virtual inspection layer, that transforms the time series to an interpretable representation and allows to propagate relevance attributions to this representation via local XAI methods like layer-wise relevance propagation (LRP). In this way, we extend the applicability of a family of XAI methods to domains (e.g. speech) where the input is only interpretable after a transformation. Here, we focus on the Fourier transformation which is prominently applied in the interpretation of time series and LRP and refer to our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time series classification settings like audio and electronic health records. We showcase how 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06361</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#38745;&#24577;&#29615;&#22659;&#30340;&#38544;&#31169;&#20445;&#25252;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#65306;&#32852;&#37030;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective. (arXiv:2303.06361v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a cooperative visible light positioning scheme based on federated learning, which improves the positioning accuracy and generalization capability in nonstationary environments by jointly training a global model adaptive to environmental changes without sharing private data of users.
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35265;&#20809;&#23450;&#20301;&#65288;VLP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23460;&#20869;&#23450;&#20301;&#25216;&#26415;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#65292;&#30001;&#20110;&#39640;&#24230;&#26102;&#21464;&#30340;&#20449;&#36947;&#65292;VLP&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#25552;&#39640;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21512;&#20316;VLP&#26041;&#26696;&#12290;&#21033;&#29992;FL&#26694;&#26550;&#65292;&#29992;&#25143;&#21487;&#20197;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#32593;&#32476;&#65288;CVPosNet&#65289;&#65292;&#20197;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visible light positioning (VLP) has drawn plenty of attention as a promising indoor positioning technique. However, in nonstationary environments, the performance of VLP is limited because of the highly time-varying channels. To improve the positioning accuracy and generalization capability in nonstationary environments, a cooperative VLP scheme based on federated learning (FL) is proposed in this paper. Exploiting the FL framework, a global model adaptive to environmental changes can be jointly trained by users without sharing private data of users. Moreover, a Cooperative Visible-light Positioning Network (CVPosNet) is proposed to accelerate the convergence rate and improve the positioning accuracy. Simulation results show that the proposed scheme outperforms the benchmark schemes, especially in nonstationary environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.06360</link><description>&lt;p&gt;
FedLP: &#19968;&#31181;&#29992;&#20110;&#36890;&#20449;&#35745;&#31639;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#23618;&#27425;&#21098;&#26525;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning. (arXiv:2303.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explicit FL pruning framework, FedLP, which adopts layer-wise pruning in local training and federated updating, and is model-agnostic and universal for different types of deep learning models. FedLP can relieve the system bottlenecks of communication and computation with marginal performance decay.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#39640;&#25928;&#19988;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65288;Federated Layer-wise Pruning&#65289;&#65292;&#35813;&#26694;&#26550;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#20026;&#20855;&#26377;&#21516;&#36136;&#26412;&#22320;&#27169;&#22411;&#21644;&#24322;&#36136;&#26412;&#22320;&#27169;&#22411;&#30340;&#22330;&#26223;&#35774;&#35745;&#20102;&#20004;&#31181;&#29305;&#23450;&#30340;FedLP&#26041;&#26696;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FedLP&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FedLP&#26159;&#31532;&#19968;&#20010;&#27491;&#24335;&#23558;&#23618;&#27425;&#21098;&#26525;&#24341;&#20837;FL&#30340;&#26694;&#26550;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#22260;&#20869;&#65292;&#21487;&#20197;&#22522;&#20110;FedLP&#36827;&#19968;&#27493;&#35774;&#35745;&#26356;&#22810;&#30340;&#21464;&#20307;&#21644;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has prevailed as an efficient and privacy-preserved scheme for distributed learning. In this work, we mainly focus on the optimization of computation and communication in FL from a view of pruning. By adopting layer-wise pruning in local training and federated updating, we formulate an explicit FL pruning framework, FedLP (Federated Layer-wise Pruning), which is model-agnostic and universal for different types of deep learning models. Two specific schemes of FedLP are designed for scenarios with homogeneous local models and heterogeneous ones. Both theoretical and experimental evaluations are developed to verify that FedLP relieves the system bottlenecks of communication and computation with marginal performance decay. To the best of our knowledge, FedLP is the first framework that formally introduces the layer-wise pruning into FL. Within the scope of federated learning, more variants and combinations can be further designed based on FedLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#26631;&#20934;RNN&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#65292;&#21253;&#25324;&#32447;&#24615;&#21270;&#21644;&#23545;&#35282;&#21270;&#24490;&#29615;&#12289;&#20351;&#29992;&#26356;&#22909;&#30340;&#21442;&#25968;&#21270;&#21644;&#21021;&#22987;&#21270;&#20197;&#21450;&#30830;&#20445;&#27491;&#24120;&#21270;&#21069;&#21521;&#20256;&#36882;&#31561;&#19968;&#31995;&#21015;&#25913;&#21464;&#65292;&#26469;&#24674;&#22797;&#28145;&#24230;SSM&#22312;&#38271;&#36317;&#31163;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#21305;&#37197;&#23427;&#20204;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06349</link><description>&lt;p&gt;
&#22797;&#27963;&#38271;&#24207;&#21015;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Resurrecting Recurrent Neural Networks for Long Sequences. (arXiv:2303.06349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#26631;&#20934;RNN&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#65292;&#21253;&#25324;&#32447;&#24615;&#21270;&#21644;&#23545;&#35282;&#21270;&#24490;&#29615;&#12289;&#20351;&#29992;&#26356;&#22909;&#30340;&#21442;&#25968;&#21270;&#21644;&#21021;&#22987;&#21270;&#20197;&#21450;&#30830;&#20445;&#27491;&#24120;&#21270;&#21069;&#21521;&#20256;&#36882;&#31561;&#19968;&#31995;&#21015;&#25913;&#21464;&#65292;&#26469;&#24674;&#22797;&#28145;&#24230;SSM&#22312;&#38271;&#36317;&#31163;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#21305;&#37197;&#23427;&#20204;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores how to recover the impressive performance of deep state-space models (SSMs) on long-range reasoning tasks by carefully designing deep RNNs using standard signal propagation arguments, including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass.
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#22312;&#38271;&#24207;&#21015;&#19978;&#25552;&#20379;&#24555;&#36895;&#25512;&#29702;&#65292;&#20294;&#38590;&#20197;&#20248;&#21270;&#19988;&#35757;&#32451;&#36895;&#24230;&#24930;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#21487;&#24182;&#34892;&#21270;&#30340;&#35757;&#32451;&#21644;&#31867;&#20284;RNN&#30340;&#24555;&#36895;&#25512;&#29702;&#30340;&#39069;&#22806;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;SSM&#19982;RNN&#22312;&#34920;&#38754;&#19978;&#30456;&#20284;&#65292;&#20294;&#23384;&#22312;&#37325;&#35201;&#24046;&#24322;&#65292;&#20351;&#24471;&#19981;&#28165;&#26970;&#23427;&#20204;&#22312;RNN&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#26469;&#33258;&#20309;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26631;&#20934;&#20449;&#21495;&#20256;&#25773;&#35770;&#25454;&#31934;&#24515;&#35774;&#35745;&#30340;&#28145;&#24230;RNN&#21487;&#20197;&#24674;&#22797;&#28145;&#24230;SSM&#22312;&#38271;&#36317;&#31163;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#21305;&#37197;&#23427;&#20204;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20998;&#26512;&#21644;&#28040;&#34701;&#20102;&#19968;&#31995;&#21015;&#23545;&#26631;&#20934;RNN&#30340;&#26356;&#25913;&#65292;&#21253;&#25324;&#32447;&#24615;&#21270;&#21644;&#23545;&#35282;&#21270;&#24490;&#29615;&#65292;&#20351;&#29992;&#26356;&#22909;&#30340;&#21442;&#25968;&#21270;&#21644;&#21021;&#22987;&#21270;&#65292;&#24182;&#30830;&#20445;&#27491;&#24120;&#21270;&#21069;&#21521;&#20256;&#36882;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#28145;&#24230;RNN&#21644;&#28145;&#24230;SSM&#24615;&#33021;&#24046;&#24322;&#26469;&#28304;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#24322;&#36136;&#24615;&#19979;&#26080;&#27861;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06344</link><description>&lt;p&gt;
&#24322;&#36136;&#24615;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning under Heterophily. (arXiv:2303.06344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#24322;&#36136;&#24615;&#19979;&#26080;&#27861;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the first graph contrastive learning method to address the problem that existing graph contrastive learning methods cannot learn high-quality representations under heterophily.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#29305;&#23450;&#20219;&#21153;&#33410;&#28857;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20026;&#22270;&#24418;&#33719;&#21462;&#26631;&#31614;&#26159;&#26114;&#36149;&#30340;&#12290;&#36825;&#22312;&#22823;&#22411;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#24037;&#20316;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#22312;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#29305;&#21035;&#21463;&#27426;&#36814;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;CL&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#21516;&#31034;&#20363;&#30340;&#22686;&#24378;&#35270;&#22270;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26368;&#23567;&#21270;&#19981;&#21516;&#31034;&#20363;&#30340;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24418;CL&#26041;&#27861;&#19981;&#33021;&#22312;&#24322;&#36136;&#24615;&#19979;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#24322;&#36136;&#24615;&#19979;&#65292;&#21516;&#19968;&#31034;&#20363;&#30340;&#22686;&#24378;&#21487;&#33021;&#24444;&#27492;&#19981;&#30456;&#20284;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#31532;&#19968;&#20010;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are powerful tools for learning node representations when task-specific node labels are available. However, obtaining labels for graphs is expensive in many applications. This is particularly the case for large graphs. To address this, there has been a body of work to learn node representations in a self-supervised manner without labels. Contrastive learning (CL), has been particularly popular to learn representations in a self-supervised manner. In general, CL methods work by maximizing the similarity between representations of augmented views of the same example, and minimizing the similarity between augmented views of different examples. However, existing graph CL methods cannot learn high-quality representations under heterophily, where connected nodes tend to belong to different classes. This is because under heterophily, augmentations of the same example may not be similar to each other. In this work, we address the above problem by proposing the first graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2303.06340</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#32954;&#30284;&#26234;&#33021;&#35786;&#26029;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Intelligent diagnostic scheme for lung cancer screening with Raman spectra data by tensor network machine learning. (arXiv:2303.06340v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a tensor-network machine learning method to reliably predict lung cancer patients and their stages via screening Raman spectra data of Volatile organic compounds (VOCs) in exhaled breath.
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22312;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#20013;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#65292;&#20174;&#23398;&#26415;&#30740;&#31350;&#21040;&#20020;&#24202;&#24212;&#29992;&#65292;&#20363;&#22914;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#26816;&#27979;&#21644;&#35786;&#26029;&#12289;&#27835;&#30103;&#20248;&#21270;&#20197;&#21450;&#33647;&#29289;&#21457;&#29616;&#20013;&#26032;&#30340;&#27835;&#30103;&#38774;&#28857;&#30340;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;AI&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#20005;&#37325;&#21463;&#21040;&#38750;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#19981;&#21487;&#25511;&#22320;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#23545;&#20110;ML&#30340;&#21487;&#35299;&#37322;&#24615;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#28040;&#36153;&#32773;&#24517;&#39035;&#20174;&#22362;&#23454;&#30340;&#22522;&#30784;&#25110;&#20196;&#20154;&#20449;&#26381;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#24517;&#35201;&#30340;&#23433;&#20840;&#24863;&#21644;&#20449;&#20219;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;-ML&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#36866;&#29992;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#38750;&#20405;&#20837;&#24615;&#32954;&#30284;&#31579;&#26597;&#30340;&#29702;&#24819;&#26041;&#24335;&#12290;TN-ML&#30340;&#39044;&#27979;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has brought tremendous impacts on biomedical sciences from academic researches to clinical applications, such as in biomarkers' detection and diagnosis, optimization of treatment, and identification of new therapeutic targets in drug discovery. However, the contemporary AI technologies, particularly deep machine learning (ML), severely suffer from non-interpretability, which might uncontrollably lead to incorrect predictions. Interpretability is particularly crucial to ML for clinical diagnosis as the consumers must gain necessary sense of security and trust from firm grounds or convincing interpretations. In this work, we propose a tensor-network (TN)-ML method to reliably predict lung cancer patients and their stages via screening Raman spectra data of Volatile organic compounds (VOCs) in exhaled breath, which are generally suitable as biomarkers and are considered to be an ideal way for non-invasive lung cancer screening. The prediction of TN-ML is based
&lt;/p&gt;</description></item><item><title>AutoMLP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06337</link><description>&lt;p&gt;
AutoMLP: &#33258;&#21160;&#21270;MLP&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
AutoMLP: Automated MLP for Sequential Recommendations. (arXiv:2303.06337v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06337
&lt;/p&gt;
&lt;p&gt;
AutoMLP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoMLP is a novel sequential recommender system that models users' long/short-term interests through an automated and adaptive search algorithm, achieving better recommendation performance.
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#26469;&#39044;&#27979;&#20182;&#20204;&#19979;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#36825;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#24182;&#23545;&#19979;&#19968;&#20010;&#25512;&#33616;&#20135;&#29983;&#19981;&#21516;&#30340;&#36129;&#29486;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#31351;&#20030;&#25628;&#32034;&#25110;&#32463;&#39564;&#32463;&#39564;&#35774;&#32622;&#39044;&#23450;&#20041;&#30340;&#30701;&#26399;&#20852;&#36259;&#38271;&#24230;&#65292;&#36825;&#26082;&#39640;&#24230;&#20302;&#25928;&#21448;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23384;&#22312;&#19978;&#36848;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#23545;&#36755;&#20837;&#24207;&#21015;&#30340;&#38271;&#24230;&#20855;&#26377;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;AutoMLP&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#20248;&#21270;&#33719;&#24471;&#26356;&#22909;&#30340;&#30701;&#26399;&#20852;&#36259;&#38271;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AutoMLP&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommender systems aim to predict users' next interested item given their historical interactions. However, a long-standing issue is how to distinguish between users' long/short-term interests, which may be heterogeneous and contribute differently to the next recommendation. Existing approaches usually set pre-defined short-term interest length by exhaustive search or empirical experience, which is either highly inefficient or yields subpar results. The recent advanced transformer-based models can achieve state-of-the-art performances despite the aforementioned issue, but they have a quadratic computational complexity to the length of the input sequence. To this end, this paper proposes a novel sequential recommender system, AutoMLP, aiming for better modeling users' long/short-term interests from their historical interactions. In addition, we design an automated and adaptive search algorithm for preferable short-term interest length via end-to-end optimization. Through ext
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;MetaViewer&#65292;&#36890;&#36807;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#39044;&#20808;&#25351;&#23450;&#30340;&#34701;&#21512;&#20989;&#25968;&#21644;&#28151;&#21512;&#22312;&#29305;&#24449;&#20013;&#30340;&#35270;&#22270;&#19987;&#29992;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#20250;&#38477;&#20302;&#25152;&#24471;&#34920;&#31034;&#30340;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06329</link><description>&lt;p&gt;
MetaViewer: &#26397;&#30528;&#32479;&#19968;&#30340;&#22810;&#35270;&#22270;&#34920;&#31034;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
MetaViewer: Towards A Unified Multi-View Representation. (arXiv:2303.06329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;MetaViewer&#65292;&#36890;&#36807;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#39044;&#20808;&#25351;&#23450;&#30340;&#34701;&#21512;&#20989;&#25968;&#21644;&#28151;&#21512;&#22312;&#29305;&#24449;&#20013;&#30340;&#35270;&#22270;&#19987;&#29992;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#20250;&#38477;&#20302;&#25152;&#24471;&#34920;&#31034;&#30340;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel bi-level-optimization-based multi-view learning framework, MetaViewer, which learns the representation in a uniform-to-specific manner, avoiding the problem of manually pre-specify fusion functions and view-private redundant information mixed in features that potentially degrade the quality of the derived representation.
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36981;&#24490;&#29305;&#23450;&#21040;&#32479;&#19968;&#30340;&#27969;&#31243;&#65292;&#20174;&#27599;&#20010;&#35270;&#22270;&#20013;&#25552;&#21462;&#28508;&#22312;&#29305;&#24449;&#65292;&#28982;&#21518;&#34701;&#21512;&#25110;&#23545;&#40784;&#23427;&#20204;&#20197;&#33719;&#24471;&#32479;&#19968;&#30340;&#23545;&#35937;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#39044;&#20808;&#25351;&#23450;&#30340;&#34701;&#21512;&#20989;&#25968;&#21644;&#28151;&#21512;&#22312;&#29305;&#24449;&#20013;&#30340;&#35270;&#22270;&#19987;&#29992;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#20250;&#38477;&#20302;&#25152;&#24471;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#34920;&#31034;&#26159;&#20197;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#26041;&#24335;&#23398;&#20064;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20803;&#23398;&#20064;&#22120;&#65292;&#21363;MetaViewer&#65292;&#22312;&#22806;&#23618;&#20248;&#21270;&#20013;&#23398;&#20064;&#34701;&#21512;&#21644;&#24314;&#27169;&#35270;&#22270;&#20849;&#20139;&#30340;&#20803;&#34920;&#31034;&#12290;&#20174;&#36825;&#20010;&#20803;&#34920;&#31034;&#24320;&#22987;&#65292;&#38656;&#35201;&#22312;&#20869;&#23618;&#35757;&#32451;&#35270;&#22270;&#29305;&#23450;&#30340;&#22522;&#23398;&#20064;&#22120;&#65292;&#20197;&#24555;&#36895;&#37325;&#26500;&#30456;&#24212;&#30340;&#35270;&#22270;&#12290;MetaViewer&#26368;&#32456;&#36890;&#36807;&#35266;&#23519;&#25152;&#26377;&#35270;&#22270;&#19978;&#20174;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#37325;&#26500;&#36807;&#31243;&#26469;&#26356;&#26032;&#65292;&#24182;&#23398;&#20064;&#26368;&#20339;&#34701;&#21512;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multi-view representation learning methods typically follow a specific-to-uniform pipeline, extracting latent features from each view and then fusing or aligning them to obtain the unified object representation. However, the manually pre-specify fusion functions and view-private redundant information mixed in features potentially degrade the quality of the derived representation. To overcome them, we propose a novel bi-level-optimization-based multi-view learning framework, where the representation is learned in a uniform-to-specific manner. Specifically, we train a meta-learner, namely MetaViewer, to learn fusion and model the view-shared meta representation in outer-level optimization. Start with this meta representation, view-specific base-learners are then required to rapidly reconstruct the corresponding view in inner-level. MetaViewer eventually updates by observing reconstruction processes from uniform to specific over all views, and learns an optimal fusion scheme that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.06318</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#19987;&#23478;&#28151;&#21512;&#24182;&#34892;&#26041;&#27861;&#26469;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixture-of-Experts&#65288;MoE&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#28155;&#21152;&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#22359;&#26469;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#65288;&#22522;&#26412;&#27169;&#22411;&#65289;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#25913;&#21464;&#35757;&#32451;&#25110;&#25512;&#29702;&#30340;&#24635;&#28014;&#28857;&#25805;&#20316;&#25968;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#31181;&#26550;&#26500;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#20219;&#24847;&#22823;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19982;&#22522;&#26412;&#27169;&#22411;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;64&#21040;128&#20010;&#19987;&#23478;&#22359;&#20043;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35266;&#23519;&#21040;&#36825;&#20123;MoE&#27169;&#22411;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#36882;&#20943;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;MoE&#27169;&#22411;&#38656;&#35201;&#25105;&#20204;&#25193;&#23637;&#22522;&#26412;&#27169;&#22411;&#30340;&#22823;&#23567;&#20197;&#21450;&#19987;&#23478;&#22359;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#32500;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#27493;&#39588;&#20013;&#25552;&#20986;&#20102;&#20869;&#23384;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#25928;&#29575;&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#24615;&#33021;&#26159;&#30001;&#20110;&#20869;&#22312;&#34920;&#36798;&#33021;&#21147;&#32780;&#38750;&#21442;&#25968;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2303.06316</link><description>&lt;p&gt;
&#19968;&#20010;&#31070;&#32463;&#20803;&#30340;&#33410;&#30465;&#23601;&#26159;&#19968;&#20010;&#31070;&#32463;&#20803;&#30340;&#25910;&#30410;&#65306;&#20851;&#20110;&#20108;&#27425;&#32593;&#32476;&#21442;&#25968;&#25928;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
One Neuron Saved Is One Neuron Earned: On Parametric Efficiency of Quadratic Networks. (arXiv:2303.06316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#25928;&#29575;&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#24615;&#33021;&#26159;&#30001;&#20110;&#20869;&#22312;&#34920;&#36798;&#33021;&#21147;&#32780;&#38750;&#21442;&#25968;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the parametric efficiency of quadratic neurons and confirms that their superior performance is due to intrinsic expressive capability rather than increased parameters.
&lt;/p&gt;
&lt;p&gt;
&#21463;&#29983;&#29289;&#31070;&#32463;&#31995;&#32479;&#20013;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#30340;&#21551;&#21457;&#65292;&#22823;&#37327;&#30740;&#31350;&#25552;&#20986;&#20102;&#35774;&#35745;&#26032;&#22411;&#20154;&#24037;&#31070;&#32463;&#20803;&#24182;&#23558;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#24341;&#20837;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#65292;&#23558;&#20256;&#32479;&#31070;&#32463;&#20803;&#20013;&#30340;&#20869;&#31215;&#25805;&#20316;&#26367;&#25442;&#20026;&#20108;&#27425;&#25805;&#20316;&#65292;&#22312;&#35768;&#22810;&#37325;&#35201;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#32467;&#26524;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#20108;&#27425;&#32593;&#32476;&#30340;&#21331;&#36234;&#24615;&#33021;&#20165;&#20165;&#26159;&#30001;&#20110;&#21442;&#25968;&#22686;&#21152;&#36824;&#26159;&#30001;&#20110;&#20869;&#22312;&#34920;&#36798;&#33021;&#21147;&#65311;&#22312;&#26410;&#28548;&#28165;&#36825;&#20010;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#20108;&#27425;&#32593;&#32476;&#30340;&#24615;&#33021;&#24635;&#26159;&#20196;&#20154;&#24576;&#30097;&#12290;&#27492;&#22806;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23601;&#26159;&#25214;&#21040;&#20108;&#27425;&#32593;&#32476;&#30340;&#26432;&#25163;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#27425;&#32593;&#32476;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#65292;&#20174;&#32780;&#30830;&#35748;&#20102;&#20108;&#27425;&#32593;&#32476;&#30340;&#21331;&#36234;&#24615;&#33021;&#26159;&#30001;&#20110;&#20854;&#20869;&#22312;&#34920;&#36798;&#33021;&#21147;&#32780;&#38750;&#21442;&#25968;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by neuronal diversity in the biological neural system, a plethora of studies proposed to design novel types of artificial neurons and introduce neuronal diversity into artificial neural networks. Recently proposed quadratic neuron, which replaces the inner-product operation in conventional neurons with a quadratic one, have achieved great success in many essential tasks. Despite the promising results of quadratic neurons, there is still an unresolved issue: \textit{Is the superior performance of quadratic networks simply due to the increased parameters or due to the intrinsic expressive capability?} Without clarifying this issue, the performance of quadratic networks is always suspicious. Additionally, resolving this issue is reduced to finding killer applications of quadratic networks. In this paper, with theoretical and empirical studies, we show that quadratic networks enjoy parametric efficiency, thereby confirming that the superior performance of quadratic networks is due
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#65292;&#20197;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.06314</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#36890;&#36807;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#23458;&#25143;&#31471;dropout&#26469;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stabilizing and Improving Federated Learning with Non-IID Data and Client Dropout in IoT Systems. (arXiv:2303.06314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#65292;&#20197;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple yet effective framework to stabilize and improve federated learning by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based feature extraction to maintain a balanced classifier head.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#19981;&#26292;&#38706;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22312;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#19978;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#28982;&#32780;&#23427;&#21463;&#21040;&#26631;&#31614;&#20998;&#24067;&#20559;&#26012;&#30340;&#24433;&#21709;&#65292;&#36890;&#24120;&#23548;&#33268;&#25910;&#25947;&#32531;&#24930;&#21644;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#24403;&#21442;&#19982;&#30340;&#23458;&#25143;&#31471;&#22788;&#20110;&#19981;&#31283;&#23450;&#30340;&#29615;&#22659;&#24182;&#32463;&#24120;&#25481;&#32447;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21487;&#33021;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an emerging technique for training deep models over decentralized clients without exposing private data, which however suffers from label distribution skew and usually results in slow convergence and degraded model performance. This challenge could be more serious when the participating clients are in unstable circumstances and dropout frequently. Previous work and our empirical observations demonstrate that the classifier head for classification task is more sensitive to label skew and the unstable performance of FedAvg mainly lies in the imbalanced training samples across different classes. The biased classifier head will also impact the learning of feature representations. Therefore, maintaining a balanced classifier head is of significant importance for building a better global model. To tackle this issue, we propose a simple yet effective framework by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.06311</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;EXO-200&#38378;&#28865;&#20449;&#21495;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks for Scintillation Signal Simulation in EXO-200. (arXiv:2303.06311v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach using Generative Adversarial Networks to simulate photodetector signals from the time projection chamber of the EXO-200 experiment. The method is able to produce high-quality simulated waveforms an order of magnitude faster than traditional simulation methods and can generalize from the training sample and discern salient high-level features of the data.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#25110;&#23454;&#38469;&#20107;&#20214;&#26679;&#26412;&#35757;&#32451;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#20026;&#20195;&#20215;&#29983;&#25104;&#22823;&#35268;&#27169;&#27169;&#25311;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20801;&#35768;&#23545;&#32473;&#23450;&#23545;&#35937;&#38598;&#30340;&#24635;&#20307;&#20998;&#24067;&#36827;&#34892;&#38544;&#24335;&#38750;&#21442;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#20351;&#29992;&#21407;&#22987;&#38378;&#28865;&#27874;&#24418;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#26657;&#20934;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23427;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#32593;&#32476;&#27491;&#30830;&#25512;&#26029;&#20986;&#25506;&#27979;&#22120;&#20013;&#38378;&#28865;&#20809;&#21709;&#24212;&#30340;&#20301;&#32622;&#20381;&#36182;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks trained on samples of simulated or actual events have been proposed as a way of generating large simulated datasets at a reduced computational cost. In this work, a novel approach to perform the simulation of photodetector signals from the time projection chamber of the EXO-200 experiment is demonstrated. The method is based on a Wasserstein Generative Adversarial Network - a deep learning technique allowing for implicit non-parametric estimation of the population distribution for a given set of objects. Our network is trained on real calibration data using raw scintillation waveforms as input. We find that it is able to produce high-quality simulated waveforms an order of magnitude faster than the traditional simulation approach and, importantly, generalize from the training sample and discern salient high-level features of the data. In particular, the network correctly deduces position dependency of scintillation light response in the detector and corr
&lt;/p&gt;</description></item><item><title>&#39550;&#39542;&#21592;&#30130;&#21171;&#26159;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#19968;&#31181;&#23454;&#26102;&#26816;&#27979;&#39550;&#39542;&#21592;&#30130;&#21171;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06310</link><description>&lt;p&gt;
&#39550;&#39542;&#21592;&#30130;&#21171;&#26816;&#27979;&#31995;&#32479;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Driver Drowsiness Detection System: An Approach By Machine Learning Application. (arXiv:2303.06310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06310
&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#21592;&#30130;&#21171;&#26159;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#19968;&#31181;&#23454;&#26102;&#26816;&#27979;&#39550;&#39542;&#21592;&#30130;&#21171;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driver drowsiness is one of the main causes of traffic accidents. This study aims to develop a real-time detection system for driver drowsiness using machine learning algorithms.
&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20107;&#25925;&#23548;&#33268;&#22823;&#22810;&#25968;&#20154;&#30340;&#27515;&#20129;&#21644;&#20260;&#23475;&#12290;&#27599;&#24180;&#26377;&#25968;&#30334;&#19975;&#20154;&#22240;&#20132;&#36890;&#20107;&#25925;&#21463;&#20260;&#25110;&#27515;&#20129;&#65292;&#36825;&#19982;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#30340;&#25968;&#25454;&#19968;&#33268;&#12290;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30561;&#30496;&#12289;&#20241;&#24687;&#25110;&#24863;&#21040;&#30130;&#20518;&#30340;&#39550;&#39542;&#21592;&#21487;&#33021;&#20250;&#22312;&#39550;&#39542;&#36807;&#31243;&#20013;&#30561;&#30528;&#65292;&#21361;&#21450;&#33258;&#24049;&#21644;&#20854;&#20182;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23433;&#20840;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#30130;&#21171;&#39550;&#39542;&#32780;&#23548;&#33268;&#30340;&#37325;&#22823;&#36947;&#36335;&#20107;&#25925;&#12290;&#29616;&#22312;&#65292;&#30130;&#21171;&#39550;&#39542;&#25104;&#20026;&#21457;&#29983;&#30130;&#21171;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#29616;&#22312;&#65292;&#30130;&#21171;&#25104;&#20026;&#22686;&#21152;&#36947;&#36335;&#20107;&#25925;&#25968;&#37327;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#36825;&#25104;&#20026;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23613;&#24555;&#35299;&#20915;&#12290;&#25152;&#26377;&#35774;&#22791;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#23454;&#26102;&#26816;&#27979;&#30130;&#21171;&#30340;&#24615;&#33021;&#12290;&#35768;&#22810;&#35774;&#22791;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#29992;&#20110;&#26816;&#27979;&#30130;&#21171;&#65292;&#36825;&#20123;&#35774;&#22791;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20063;&#19982;&#39550;&#39542;&#21592;&#30130;&#21171;&#26816;&#27979;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of human deaths and injuries are caused by traffic accidents. A million people worldwide die each year due to traffic accident injuries, consistent with the World Health Organization. Drivers who do not receive enough sleep, rest, or who feel weary may fall asleep behind the wheel, endangering both themselves and other road users. The research on road accidents specified that major road accidents occur due to drowsiness while driving. These days, it is observed that tired driving is the main reason to occur drowsiness. Now, drowsiness becomes the main principle for to increase in the number of road accidents. This becomes a major issue in a world which is very important to resolve as soon as possible. The predominant goal of all devices is to improve the performance to detect drowsiness in real time. Many devices were developed to detect drowsiness, which depend on different artificial intelligence algorithms. So, our research is also related to driver drowsiness detection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#34394;&#25311;&#21161;&#25163;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34394;&#25311;&#21161;&#25163;&#26159;&#19968;&#31181;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#35821;&#38899;&#21629;&#20196;&#24182;&#33021;&#20195;&#34920;&#24744;&#25191;&#34892;&#20219;&#21153;&#30340;&#36719;&#20214;&#65292;&#21487;&#20197;&#23436;&#25104;&#20960;&#20046;&#20219;&#20309;&#24744;&#33258;&#24049;&#21487;&#20197;&#23436;&#25104;&#30340;&#29305;&#23450;&#26234;&#33021;&#25163;&#26426;&#25110;PC&#27963;&#21160;&#65292;&#32780;&#19988;&#21015;&#34920;&#19981;&#26029;&#25193;&#22823;&#12290;</title><link>http://arxiv.org/abs/2303.06309</link><description>&lt;p&gt;
&#34394;&#25311;&#40736;&#26631;&#21644;&#21161;&#25163;&#65306;&#20154;&#24037;&#26234;&#33021;&#30340;&#25216;&#26415;&#38761;&#21629;
&lt;/p&gt;
&lt;p&gt;
Virtual Mouse And Assistant: A Technological Revolution Of Artificial Intelligence. (arXiv:2303.06309v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#34394;&#25311;&#21161;&#25163;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34394;&#25311;&#21161;&#25163;&#26159;&#19968;&#31181;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#35821;&#38899;&#21629;&#20196;&#24182;&#33021;&#20195;&#34920;&#24744;&#25191;&#34892;&#20219;&#21153;&#30340;&#36719;&#20214;&#65292;&#21487;&#20197;&#23436;&#25104;&#20960;&#20046;&#20219;&#20309;&#24744;&#33258;&#24049;&#21487;&#20197;&#23436;&#25104;&#30340;&#29305;&#23450;&#26234;&#33021;&#25163;&#26426;&#25110;PC&#27963;&#21160;&#65292;&#32780;&#19988;&#21015;&#34920;&#19981;&#26029;&#25193;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the performance improvement of virtual assistants, which are software that understands natural language voice commands and can perform tasks on your behalf. They can complete practically any specific smartphone or PC activity that you can complete on your own, and the list is continually expanding.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#34394;&#25311;&#21161;&#25163;&#30340;&#24615;&#33021;&#12290;&#37027;&#20040;&#20160;&#20040;&#26159;&#34394;&#25311;&#21161;&#25163;&#65311;&#24212;&#29992;&#36719;&#20214;&#65292;&#36890;&#24120;&#31216;&#20026;&#34394;&#25311;&#21161;&#25163;&#65292;&#20063;&#31216;&#20026;AI&#21161;&#25163;&#25110;&#25968;&#23383;&#21161;&#25163;&#65292;&#26159;&#19968;&#31181;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#35821;&#38899;&#21629;&#20196;&#24182;&#33021;&#20195;&#34920;&#24744;&#25191;&#34892;&#20219;&#21153;&#30340;&#36719;&#20214;&#12290;&#34394;&#25311;&#21161;&#25163;&#21487;&#20197;&#23436;&#25104;&#20960;&#20046;&#20219;&#20309;&#24744;&#33258;&#24049;&#21487;&#20197;&#23436;&#25104;&#30340;&#29305;&#23450;&#26234;&#33021;&#25163;&#26426;&#25110;PC&#27963;&#21160;&#65292;&#32780;&#19988;&#21015;&#34920;&#19981;&#26029;&#25193;&#22823;&#12290;&#34394;&#25311;&#21161;&#25163;&#36890;&#24120;&#21487;&#20197;&#23436;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#23433;&#25490;&#20250;&#35758;&#12289;&#21457;&#36865;&#28040;&#24687;&#21644;&#30417;&#25511;&#22825;&#27668;&#12290;&#20197;&#21069;&#30340;&#34394;&#25311;&#21161;&#25163;&#65292;&#22914;Google&#21161;&#25163;&#21644;Cortana&#65292;&#22312;&#26576;&#20123;&#26041;&#38754;&#26377;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#25191;&#34892;&#25628;&#32034;&#65292;&#32780;&#19981;&#26159;&#23436;&#20840;&#33258;&#21160;&#21270;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#24341;&#25806;&#27809;&#26377;&#33021;&#21147;&#21069;&#36827;&#21644;&#20498;&#24102;&#27468;&#26354;&#65292;&#20197;&#20445;&#25345;&#27468;&#26354;&#30340;&#25511;&#21046;&#21151;&#33021;&#65307;&#23427;&#20204;&#21482;&#33021;&#20855;&#26377;&#25628;&#32034;&#27468;&#26354;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this paper is to enhance the performance of the virtual assistant. So, what exactly is a virtual assistant. Application software, often called virtual assistants, also known as AI assistants or digital assistants, is software that understands natural language voice commands and can perform tasks on your behalf. What does a virtual assistant do. Virtual assistants can complete practically any specific smartphone or PC activity that you can complete on your own, and the list is continually expanding. Virtual assistants typically do an impressive variety of tasks, including scheduling meetings, delivering messages, and monitoring the weather. Previous virtual assistants, like Google Assistant and Cortana, had limits in that they could only perform searches and were not entirely automated. For instance, these engines do not have the ability to forward and rewind the song in order to maintain the control function of the song; they can only have the module to search for songs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21435;&#20013;&#24515;&#21270;&#25237;&#31080;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36523;&#20221;&#35782;&#21035;&#26041;&#24335;&#65292;&#20351;&#24471;&#27599;&#20010;&#20154;&#37117;&#33021;&#36861;&#36394;&#25237;&#31080;&#27450;&#35784;&#65292;&#31995;&#32479;&#38750;&#24120;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2303.06306</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21435;&#20013;&#24515;&#21270;&#25237;&#31080;&#31995;&#32479;&#23433;&#20840;&#35270;&#35282;&#65306;&#25968;&#23383;&#25237;&#31080;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
Blockchain-based decentralized voting system security Perspective: Safe and secure for digital voting system. (arXiv:2303.06306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21435;&#20013;&#24515;&#21270;&#25237;&#31080;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36523;&#20221;&#35782;&#21035;&#26041;&#24335;&#65292;&#20351;&#24471;&#27599;&#20010;&#20154;&#37117;&#33021;&#36861;&#36394;&#25237;&#31080;&#27450;&#35784;&#65292;&#31995;&#32479;&#38750;&#24120;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the blockchain-based decentralized voting system and proposes a unique identification method that enables everyone to trace vote fraud, making the system incredibly safe.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#25237;&#31080;&#31995;&#32479;&#65292;&#20026;&#36873;&#27665;&#12289;&#20505;&#36873;&#20154;&#21644;&#23448;&#21592;&#21442;&#19982;&#21644;&#31649;&#29702;&#25237;&#31080;&#25552;&#20379;&#20415;&#21033;&#12290;&#30001;&#20110;&#25105;&#20204;&#22312;&#21518;&#31471;&#20351;&#29992;&#20102;&#21306;&#22359;&#38142;&#65292;&#20351;&#24471;&#27599;&#20010;&#20154;&#37117;&#33021;&#36861;&#36394;&#25237;&#31080;&#27450;&#35784;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#31995;&#32479;&#38750;&#24120;&#23433;&#20840;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36523;&#20221;&#35782;&#21035;&#26041;&#24335;&#65292;&#21363;&#20351;&#29992;Aadhar&#21345;&#21495;&#25110;OTP&#29983;&#25104;&#65292;&#28982;&#21518;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#25237;&#31080;&#31995;&#32479;&#25237;&#31080;&#12290;&#25552;&#20986;&#20102;&#27604;&#29305;&#24065;&#30340;&#24314;&#35758;&#65292;&#27604;&#29305;&#24065;&#26159;&#19968;&#31181;&#34394;&#25311;&#36135;&#24065;&#31995;&#32479;&#65292;&#30001;&#20013;&#22830;&#26426;&#26500;&#20915;&#23450;&#29983;&#20135;&#36135;&#24065;&#12289;&#36716;&#31227;&#25152;&#26377;&#26435;&#21644;&#39564;&#35777;&#20132;&#26131;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#32593;&#32476;&#22312;&#21306;&#22359;&#38142;&#31995;&#32479;&#20013;&#65292;&#36134;&#26412;&#22312;&#22810;&#20010;&#30456;&#21516;&#30340;&#25968;&#25454;&#24211;&#20013;&#22797;&#21046;&#65292;&#30001;&#19981;&#21516;&#30340;&#36827;&#31243;&#25176;&#31649;&#21644;&#26356;&#26032;&#65292;&#22914;&#26524;&#23545;&#19968;&#20010;&#33410;&#28857;&#36827;&#34892;&#26356;&#25913;&#24182;&#21457;&#29983;&#20132;&#26131;&#65292;&#21017;&#25152;&#26377;&#20854;&#20182;&#33410;&#28857;&#20250;&#21516;&#26102;&#26356;&#26032;&#65292;&#20215;&#20540;&#21644;&#36164;&#20135;&#30340;&#35760;&#24405;&#23558;&#27704;&#20037;&#20132;&#25442;&#65292;&#21482;&#26377;&#29992;&#25143;&#21644;&#31995;&#32479;&#38656;&#35201;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research study focuses primarily on Block-Chain-based voting systems, which facilitate participation in and administration of voting for voters, candidates, and officials. Because we used Block-Chain in the backend, which enables everyone to trace vote fraud, our system is incredibly safe. This paper approach any unique identification the Aadhar Card number or an OTP will be generated then user can utilise the voting system to cast his/her vote. A proposal for Bit-coin, a virtual currency system that is decided by a central authority for producing money, transferring ownership, and validating transactions, included the peer-to-peer network in a Block-Chain system, the ledger is duplicated across several, identical databases which is hosted and updated by a different process and all other nodes are updated concurrently if changes made to one node and a transaction occurs, the records of the values and assets are permanently exchanged, Only the user and the system need to be verifie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.06302</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#29616;&#20195;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey. (arXiv:2303.06302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey provides a comprehensive overview of recent advancements in adversarial attack and defense techniques in machine learning and deep neural networks, with a focus on deep neural network-based classification models. The methods are classified into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-based attacks.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#20114;&#32852;&#32593;&#21644;&#30456;&#20851;&#22330;&#26223;&#20013;&#30340;&#24555;&#36895;&#22686;&#38271;&#24212;&#29992;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#25915;&#20987;&#21407;&#29702;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#36825;&#26159;&#22522;&#20110;&#23545;&#29616;&#26377;&#24037;&#20316;&#30340;&#20005;&#26684;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#23558;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks and defenses in machine learning and deep neural network have been gaining significant attention due to the rapidly growing applications of deep learning in the Internet and relevant scenarios. This survey provides a comprehensive overview of the recent advancements in the field of adversarial attack and defense techniques, with a focus on deep neural network-based classification models. Specifically, we conduct a comprehensive classification of recent adversarial attack methods and state-of-the-art adversarial defense techniques based on attack principles, and present them in visually appealing tables and tree diagrams. This is based on a rigorous evaluation of the existing works, including an analysis of their strengths and limitations. We also categorize the methods into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-base
&lt;/p&gt;</description></item><item><title>MLP-SRGAN&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN&#65292;&#20351;&#29992;MLP-Mixer&#21644;&#21367;&#31215;&#23618;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#21487;&#29992;&#20110;FLAIR MRI&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.06298</link><description>&lt;p&gt;
MLP-SRGAN: &#20351;&#29992;MLP-Mixer&#30340;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer. (arXiv:2303.06298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06298
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN&#65292;&#20351;&#29992;MLP-Mixer&#21644;&#21367;&#31215;&#23618;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#21487;&#29992;&#20110;FLAIR MRI&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN is a single-dimension Super Resolution GAN that utilizes MLP-Mixers and convolutional layers for upsampling, and can be used for super-resolution reconstruction of FLAIR MRI images. New image quality metrics were proposed.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;MLP-SRGAN&#65292;&#23427;&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SRGAN&#65289;&#65292;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#28151;&#21512;&#22120;&#65288;MLP-Mixer&#65289;&#20197;&#21450;&#21367;&#31215;&#23618;&#22312;&#20999;&#29255;&#26041;&#21521;&#19978;&#36827;&#34892;&#19978;&#37319;&#26679;&#12290; MLP-SRGAN&#20351;&#29992;MSSEG2&#25361;&#25112;&#25968;&#25454;&#38598;&#20013;&#30340;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;FLAIR MRI&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#20302;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20013;&#24515;FLAIR&#25968;&#25454;&#38598;&#65288;CAIN&#65292;ADNI&#65292;CCNA&#65289;&#30340;&#22270;&#20687;&#65292;&#20197;&#26816;&#26597;&#22312;&#20445;&#30041;&#65288;&#26410;&#35265;&#65289;&#20020;&#24202;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#23558;&#19978;&#37319;&#26679;&#32467;&#26524;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;SR&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22522;&#26412;&#20107;&#23454;&#30340;&#22270;&#20687;&#65292;&#20351;&#29992;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#26469;&#34913;&#37327;&#19978;&#37319;&#26679;&#24615;&#33021;&#12290;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#32467;&#26500;&#65292;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#65292;&#20197;&#22312;&#32570;&#20047;&#22522;&#30784;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#37327;&#21270;&#38160;&#24230;&#65288;&#36793;&#32536;&#24378;&#24230;&#65289;&#65292;&#22122;&#22768;&#65288;&#29109;&#65289;&#21644;&#27169;&#31946;&#24230;&#65288;&#20302;&#39057;&#20449;&#24687;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel architecture called MLP-SRGAN, which is a single-dimension Super Resolution Generative Adversarial Network (SRGAN) that utilizes Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to upsample in the slice direction. MLP-SRGAN is trained and validated using high resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with low spatial resolution in the slice dimension to examine performance on held-out (unseen) clinical data. Upsampled results are compared to several state-of-the-art SR networks. For images with high resolution (HR) ground truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) are used to measure upsampling performance. Several new structural, no-reference image quality metrics were proposed to quantify sharpness (edge strength), noise (entropy), and blurriness (low frequency information) in the absence of groun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.06296</link><description>&lt;p&gt;
&#38450;&#27490;&#27880;&#24847;&#21147;&#29109;&#23849;&#28291;&#30340;Transformer&#35757;&#32451;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31283;&#23450;&#24615;&#23545;&#20110;Transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27880;&#24847;&#21147;&#23618;&#30340;&#28436;&#21464;&#26469;&#25506;&#31350;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36319;&#36394;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#30340;&#27880;&#24847;&#21147;&#29109;&#65292;&#36825;&#26159;&#27169;&#22411;&#38160;&#24230;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23384;&#22312;&#19968;&#31181;&#24120;&#35265;&#27169;&#24335;&#65292;&#21363;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#37319;&#21462;&#25391;&#33633;&#25439;&#22833;&#25110;&#21457;&#25955;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#23558;&#30149;&#24577;&#20302;&#27880;&#24847;&#21147;&#29109;&#65292;&#23545;&#24212;&#39640;&#24230;&#38598;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#31216;&#20026;$\textit{&#29109;&#23849;&#28291;}$&#12290;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\sigma$Reparam&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;&#21644;&#39069;&#22806;&#30340;&#23398;&#20064;&#26631;&#37327;&#37325;&#26032;&#21442;&#25968;&#21270;&#25152;&#26377;&#32447;&#24615;&#23618;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#22823;&#26032;&#33410;&#28857;&#25968;&#37327;&#30340;&#38408;&#20540;&#65292;&#35813;&#38408;&#20540;&#20351;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20445;&#25345;&#36817;&#20284;&#31561;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#31354;&#38388;&#19981;&#21464;&#25237;&#24433;&#65288;SIP&#65289;&#65292;&#20351;&#20219;&#24847;&#38745;&#24577;MF&#23884;&#20837;&#26041;&#26696;&#33021;&#22815;&#24555;&#36895;&#23884;&#20837;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#26032;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.06293</link><description>&lt;p&gt;
&#27969;&#24335;&#32593;&#32476;&#23884;&#20837;&#20013;&#30340;&#31354;&#38388;&#19981;&#21464;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Space-Invariant Projection in Streaming Network Embedding. (arXiv:2303.06293v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#22823;&#26032;&#33410;&#28857;&#25968;&#37327;&#30340;&#38408;&#20540;&#65292;&#35813;&#38408;&#20540;&#20351;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20445;&#25345;&#36817;&#20284;&#31561;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#31354;&#38388;&#19981;&#21464;&#25237;&#24433;&#65288;SIP&#65289;&#65292;&#20351;&#20219;&#24847;&#38745;&#24577;MF&#23884;&#20837;&#26041;&#26696;&#33021;&#22815;&#24555;&#36895;&#23884;&#20837;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#26032;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a threshold for the maximum number of new nodes that keep the node embedding space approximately equivalent, and proposes a generation framework called Space-Invariant Projection (SIP) to enable fast embedding of new nodes in dynamic networks using any static MF-based embedding scheme.
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32593;&#32476;&#20013;&#26032;&#21040;&#36798;&#30340;&#33410;&#28857;&#20250;&#36880;&#28176;&#20351;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#28418;&#31227;&#65292;&#22240;&#27492;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#33410;&#28857;&#23884;&#20837;&#21644;&#19979;&#28216;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#20154;&#22312;&#29702;&#35770;&#25110;&#23454;&#39564;&#20013;&#32771;&#34385;&#36825;&#20123;&#26032;&#33410;&#28857;&#30340;&#30830;&#20999;&#38408;&#20540;&#22823;&#23567;&#65292;&#21363;&#20351;&#36825;&#20123;&#26032;&#33410;&#28857;&#30340;&#22823;&#23567;&#20302;&#20110;&#26576;&#20010;&#38408;&#20540;&#65292;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20063;&#24456;&#38590;&#34987;&#32500;&#25252;&#12290;&#26412;&#25991;&#20174;&#30697;&#38453;&#25200;&#21160;&#29702;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#22823;&#26032;&#33410;&#28857;&#25968;&#37327;&#30340;&#38408;&#20540;&#65292;&#35813;&#38408;&#20540;&#20351;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20445;&#25345;&#36817;&#20284;&#31561;&#25928;&#65292;&#24182;&#32463;&#36807;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#22240;&#27492;&#65292;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;&#24403;&#26032;&#21040;&#36798;&#33410;&#28857;&#30340;&#25968;&#37327;&#20302;&#20110;&#27492;&#38408;&#20540;&#26102;&#65292;&#36825;&#20123;&#26032;&#33410;&#28857;&#30340;&#23884;&#20837;&#21487;&#20197;&#24555;&#36895;&#20174;&#21407;&#22987;&#33410;&#28857;&#30340;&#23884;&#20837;&#20013;&#23548;&#20986;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#31354;&#38388;&#19981;&#21464;&#25237;&#24433;&#65288;SIP&#65289;&#65292;&#20351;&#20219;&#24847;&#38745;&#24577;MF&#23884;&#20837;&#26041;&#26696;&#33021;&#22815;&#24555;&#36895;&#23884;&#20837;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#26032;&#33410;&#28857;&#12290;SIP&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19982;&#32593;&#32476;&#22823;&#23567;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Newly arriving nodes in dynamics networks would gradually make the node embedding space drifted and the retraining of node embedding and downstream models indispensable. An exact threshold size of these new nodes, below which the node embedding space will be predicatively maintained, however, is rarely considered in either theory or experiment. From the view of matrix perturbation theory, a threshold of the maximum number of new nodes that keep the node embedding space approximately equivalent is analytically provided and empirically validated. It is therefore theoretically guaranteed that as the size of newly arriving nodes is below this threshold, embeddings of these new nodes can be quickly derived from embeddings of original nodes. A generation framework, Space-Invariant Projection (SIP), is accordingly proposed to enables arbitrary static MF-based embedding schemes to embed new nodes in dynamics networks fast. The time complexity of SIP is linear with the network size. By combinin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DLHDMD&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.06289</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;Hankel&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Enhanced Hankel Dynamic-Mode Decomposition. (arXiv:2303.06289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DLHDMD&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a deep learning DMD based method, called DLHDMD, which uses the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics, and is able to generate accurate dynamics for chaotic time series.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#21464;&#24471;&#36234;&#26469;&#36234;&#31616;&#21333;&#21644;&#22797;&#26434;&#65292;&#20294;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#24320;&#21457;&#21160;&#24577;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#38382;&#39064;&#39046;&#22495;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#24050;&#32463;&#19982;&#25152;&#35859;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#36890;&#29992;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#29305;&#21035;&#26377;&#21069;&#36884;&#30340;&#31934;&#23494;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#24320;&#21457;&#36884;&#24452;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#28145;&#24230;&#23398;&#20064;Hankel DMD&#65288;DLHDMD&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DLHDMD&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#65292;&#24182;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#23398;&#20064;&#26144;&#23556;&#65292;&#36825;&#20123;&#26144;&#23556;&#22312;&#25104;&#21151;&#35757;&#32451;&#21518;&#24448;&#24448;&#36235;&#21521;&#20110;&#26174;&#33879;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the acquisition of time series has become increasingly more straightforward and sophisticated, developing dynamical models from time series is still a challenging and ever evolving problem domain. Within the last several years, to address this problem, there has been a merging of machine learning tools with what is called the dynamic mode decomposition (DMD). This general approach has been shown to be an especially promising avenue for sophisticated and accurate model development. Building on this prior body of work, we develop a deep learning DMD based method which makes use of the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). We show that the DLHDMD is able to generate accurate dynamics for chaotic time series, and we likewise explore how our method learns mappings which tend, after successful training, to significant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;82.2&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;76.5&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06280</link><description>&lt;p&gt;
&#25506;&#31350;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Investigating Stateful Defenses Against Black-Box Adversarial Examples. (arXiv:2303.06280v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;82.2&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;76.5&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates stateful defenses against black-box adversarial examples and proposes a new stateful defense model that achieves 82.2% accuracy on the CIFAR10 dataset and 76.5% accuracy on the ImageNet dataset.
&lt;/p&gt;
&lt;p&gt;
&#38450;&#24481;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20813;&#21463;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#26497;&#20026;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#65292;&#35797;&#22270;&#38450;&#24481;&#26356;&#21463;&#38480;&#21046;&#30340;&#40657;&#30418;&#25915;&#20987;&#32773;&#12290;&#36825;&#20123;&#38450;&#24481;&#36890;&#36807;&#36319;&#36394;&#20256;&#20837;&#27169;&#22411;&#26597;&#35810;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#25298;&#32477;&#37027;&#20123;&#21487;&#30097;&#22320;&#30456;&#20284;&#30340;&#26597;&#35810;&#26469;&#25805;&#20316;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;Blacklight&#26159;&#22312;USENIX Security '22&#19978;&#25552;&#20986;&#30340;&#65292;&#22768;&#31216;&#21487;&#20197;&#38450;&#27490;&#20960;&#20046;100&#65285;&#30340;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#35843;&#25972;&#29616;&#26377;&#40657;&#30418;&#25915;&#20987;&#30340;&#21442;&#25968;&#65292;&#26174;&#33879;&#38477;&#20302;&#21463;Blacklight&#20445;&#25252;&#30340;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#65288;&#20363;&#22914;&#65292;&#22312;CIFAR10&#19978;&#20174;82.2&#65285;&#38477;&#33267;6.4&#65285;&#65289;&#12290;&#21463;&#21040;&#36825;&#19968;&#24778;&#20154;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#30340;&#31995;&#32479;&#21270;&#65292;&#20197;&#20102;&#35299;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#20250;&#22833;&#36133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#20026;82.2&#65285;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#20026;76.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defending machine-learning (ML) models against white-box adversarial attacks has proven to be extremely difficult. Instead, recent work has proposed stateful defenses in an attempt to defend against a more restricted black-box attacker. These defenses operate by tracking a history of incoming model queries, and rejecting those that are suspiciously similar. The current state-of-the-art stateful defense Blacklight was proposed at USENIX Security '22 and claims to prevent nearly 100% of attacks on both the CIFAR10 and ImageNet datasets. In this paper, we observe that an attacker can significantly reduce the accuracy of a Blacklight-protected classifier (e.g., from 82.2% to 6.4% on CIFAR10) by simply adjusting the parameters of an existing black-box attack. Motivated by this surprising observation, since existing attacks were evaluated by the Blacklight authors, we provide a systematization of stateful defenses to understand why existing stateful defense models fail. Finally, we propose a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06275</link><description>&lt;p&gt;
&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhancing Protein Language Models with Structure-based Encoder and Pre-training. (arXiv:2303.06275v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes enhancing protein language models with structure-based encoder and pre-training to explicitly encode protein structures for better structure-aware protein representations, and empirically verifies its effectiveness.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#34507;&#30333;&#36136;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#23613;&#31649;&#33021;&#22815;&#38544;&#24335;&#22320;&#25429;&#33719;&#27531;&#22522;&#38388;&#30340;&#25509;&#35302;&#20449;&#24687;&#65292;&#20294;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;PLMs&#19981;&#33021;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#32467;&#26500;&#23545;&#20110;&#30830;&#23450;&#21151;&#33021;&#24456;&#37325;&#35201;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#22312;&#21487;&#29992;&#34507;&#30333;&#36136;&#32467;&#26500;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#36825;&#20123;PLMs&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;PLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models (PLMs) pre-trained on large-scale protein sequence corpora have achieved impressive performance on various downstream protein understanding tasks. Despite the ability to implicitly capture inter-residue contact information, transformer-based PLMs cannot encode protein structures explicitly for better structure-aware protein representations. Besides, the power of pre-training on available protein structures has not been explored for improving these PLMs, though structures are important to determine functions. To tackle these limitations, in this work, we enhance the PLMs with structure-based encoder and pre-training. We first explore feasible model architectures to combine the advantages of a state-of-the-art PLM (i.e., ESM-1b1) and a state-of-the-art protein structure encoder (i.e., GearNet). We empirically verify the ESM-GearNet that connects two encoders in a series way as the most effective combination model. To further improve the effectiveness of ESM-GearNe
&lt;/p&gt;</description></item><item><title>CoNIC&#25361;&#25112;&#20351;&#29992;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#21457;&#29616;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06274</link><description>&lt;p&gt;
CoNIC&#25361;&#25112;&#65306;&#25512;&#21160;&#26680;&#26816;&#27979;&#12289;&#20998;&#21106;&#12289;&#20998;&#31867;&#21644;&#35745;&#25968;&#30340;&#21069;&#27839;&#65288;arXiv:2303.06274v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
CoNIC Challenge: Pushing the Frontiers of Nuclear Detection, Segmentation, Classification and Counting. (arXiv:2303.06274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06274
&lt;/p&gt;
&lt;p&gt;
CoNIC&#25361;&#25112;&#20351;&#29992;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#21457;&#29616;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CoNIC challenge used the largest dataset to evaluate nuclear segmentation and cellular composition, stimulated the development of reproducible algorithms for cellular recognition, and found that eosinophils and neutrophils play an important role in tumors.
&lt;/p&gt;
&lt;p&gt;
&#26680;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#24418;&#24577;&#27979;&#37327;&#26159;&#24110;&#21161;&#25105;&#20204;&#36827;&#19968;&#27493;&#20102;&#35299;&#32452;&#32455;&#23398;&#21644;&#24739;&#32773;&#39044;&#21518;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#21019;&#26032;&#65292;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35774;&#32622;&#20102;&#19968;&#20010;&#31038;&#21306;&#24191;&#27867;&#30340;&#25361;&#25112;&#65292;&#20197;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#25361;&#25112;&#21517;&#20026;CoNIC&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#20849;&#25490;&#34892;&#27036;&#19978;&#36827;&#34892;&#23454;&#26102;&#32467;&#26524;&#26816;&#26597;&#12290;&#25105;&#20204;&#22522;&#20110;1,658&#20010;&#32467;&#32928;&#32452;&#32455;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21518;&#25361;&#25112;&#20998;&#26512;&#12290;&#27599;&#20010;&#27169;&#22411;&#26816;&#27979;&#21040;&#32422;7&#20159;&#20010;&#32454;&#32990;&#26680;&#65292;&#30456;&#20851;&#29305;&#24449;&#29992;&#20110;&#19981;&#33391;&#22686;&#29983;&#20998;&#32423;&#21644;&#29983;&#23384;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25361;&#25112;&#23545;&#20808;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#25913;&#36827;&#23548;&#33268;&#20102;&#19979;&#28216;&#24615;&#33021;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#34920;&#26126;&#65292;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclear detection, segmentation and morphometric profiling are essential in helping us further understand the relationship between histology and patient outcome. To drive innovation in this area, we setup a community-wide challenge using the largest available dataset of its kind to assess nuclear segmentation and cellular composition. Our challenge, named CoNIC, stimulated the development of reproducible algorithms for cellular recognition with real-time result inspection on public leaderboards. We conducted an extensive post-challenge analysis based on the top-performing models using 1,658 whole-slide images of colon tissue. With around 700 million detected nuclei per model, associated features were used for dysplasia grading and survival analysis, where we demonstrated that the challenge's improvement over the previous state-of-the-art led to significant boosts in downstream performance. Our findings also suggest that eosinophils and neutrophils play an important role in the tumour m
&lt;/p&gt;</description></item></channel></rss>